{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Data\\\\X.npy', allow_pickle=True)\n",
    "y = np.load('Data\\\\y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc11 = nn.Linear(468, 24, bias=True)\n",
    "        self.fc12 = nn.Linear(24, 13, bias=True) \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = torch.tanh(self.fc11(x))\n",
    "        x1 = self.fc12(x1)     \n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data\\\\model_checkpoint.pt'\n",
    "device = torch.device('cpu')\n",
    "net = Net1()\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    inputs = torch.FloatTensor(X)\n",
    "    labels = torch.tensor(y, dtype=torch.long)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net.forward(inputs)\n",
    "    loss = error(outputs, labels) \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "for v,p in enumerate(net.parameters()):\n",
    "    param_dict[v] = p.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = param_dict[0].T\n",
    "b0 = param_dict[1]\n",
    "W1 = param_dict[2].T\n",
    "b1 = param_dict[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = np.matmul(X, W0) + b0\n",
    "h1 = np.tanh(h0)\n",
    "h2 = np.matmul(h1, W1) + b1\n",
    "h3 = np.exp(h2)\n",
    "o = h3/np.sum(h3,axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data\\\\param_dict.npy', param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pattern.es import parsetree\n",
    "import unidecode\n",
    "import re\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_funcionales = nltk.corpus.stopwords.words(\"spanish\")    \n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def trim_sent(sentence):    \n",
    "    return ' '.join(sentence.split())\n",
    "\n",
    "def prepare_text(text): \n",
    "    try:\n",
    "        text = trim_sent(text).lower()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('Exception en prepare_text: {0}'.format(e))\n",
    "        return None       \n",
    "\n",
    "\n",
    "def hasNumbers(string):\n",
    "    return bool(re.search(r'\\d', string))\n",
    "\n",
    "\n",
    "def hasBC(string):\n",
    "    i = string.find('/')\n",
    "    return bool(i != -1)\n",
    "\n",
    "\n",
    "def other_check(token):    \n",
    "    b1 = not hasNumbers(token)\n",
    "    b2 = not hasBC(token)\n",
    "    return (b1 and b2)\n",
    "\n",
    "def remove_accent(word):\n",
    "    return unidecode.unidecode(word)\n",
    "\n",
    "def stem_lemma(word):     \n",
    "    word = parsetree(word, lemmata=True)[0].lemmata[0]\n",
    "    word = stemmer.stem(word) \n",
    "    return word\n",
    "\n",
    "\n",
    "def token_and_clean(texto): \n",
    "    tokens = nltk.word_tokenize(texto, \"spanish\")\n",
    "    token_list = []\n",
    "    for token in tokens:        \n",
    "        if token not in palabras_funcionales:\n",
    "            token = stem_lemma(token)\n",
    "            token = remove_accent(token)\n",
    "            if len(token) >= 2 and other_check(token):\n",
    "                token_list.append(token)                \n",
    "        \n",
    "    return token_list   \n",
    "\n",
    "\n",
    "def vectorize_phrase(texto, vocab):\n",
    "    try:\n",
    "        tokens = token_and_clean(texto)\n",
    "        vector = np.zeros(len(vocab))\n",
    "        for t in tokens:\n",
    "            if t in vocab:\n",
    "                vector[vocab.index(t)] = 1\n",
    "        return vector\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception en vectorize_phrase: {0}'.format(e))\n",
    "        return None  \n",
    "    \n",
    "\n",
    "def n_token(sentence):\n",
    "    token_list = token_and_clean(sentence) \n",
    "    return len(token_list) \n",
    "\n",
    "\n",
    "def polarity_and_lang(message): #blob has a limit on api calls\n",
    "    \n",
    "    try:\n",
    "        if len(message) > 2:\n",
    "    \n",
    "            blob = TextBlob(message)    \n",
    "        \n",
    "            leng = blob.detect_language()\n",
    "            text = ''\n",
    "            if leng == 'es':\n",
    "                blob = blob.translate(to='en').lower() \n",
    "                text = message\n",
    "            else:\n",
    "                blob = blob.lower() \n",
    "                text = blob.translate(to='es').lower().raw \n",
    "            \n",
    "            pol = blob.sentiment[0]        \n",
    "        else:\n",
    "            print('Se paso a polarity_and_lang un texto menor que 3 caracters')\n",
    "            pol = 0\n",
    "            text = message            \n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "            print('Exception en polarity_and_lang: {0}'.format(e))\n",
    "            pol = 0\n",
    "            text = None            \n",
    "    \n",
    "    return (pol, text)\n",
    "\n",
    "\n",
    "def percent_greet(sentence):\n",
    "    tgreet = ['hol', 'buen', 'tard', 'dia', 'noch']\n",
    "    count = 0\n",
    "    tokens = token_and_clean(sentence)\n",
    "    for w in tokens:       \n",
    "        if w in tgreet:\n",
    "            count += 1  \n",
    "    if len(tokens) > 0:\n",
    "        return count/len(tokens)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def pred_prob(text):\n",
    "    try:        \n",
    "    \n",
    "        vocab = np.load('Data\\\\vocab.npy', allow_pickle=True)\n",
    "        vocab = list(vocab)\n",
    "    \n",
    "        ldata = np.load('Data\\\\param_dict.npy', allow_pickle=True)\n",
    "        param_dict = ldata.item() \n",
    "        W0 = param_dict[0].T\n",
    "        b0 = param_dict[1]\n",
    "        W1 = param_dict[2].T\n",
    "        b1 = param_dict[3]\n",
    "        \n",
    "        pol, text = polarity_and_lang(text)\n",
    "        \n",
    "        if text:\n",
    "    \n",
    "            x = vectorize_phrase(text, vocab)\n",
    "            if x.any():\n",
    "                x = np.append(x, n_token(text))\n",
    "                x = np.append(x, percent_greet(text))\n",
    "                x = np.append(x, pol)   \n",
    "    \n",
    "                h0 = np.matmul(x, W0) + b0\n",
    "                h1 = np.tanh(h0)\n",
    "                h2 = np.matmul(h1, W1) + b1\n",
    "                h3 = np.exp(h2)\n",
    "                prob = h3/np.sum(h3)\n",
    "        \n",
    "                return (prob, pol, text)  \n",
    "            else:\n",
    "                return(None, None, None)\n",
    "        \n",
    "        else:\n",
    "            return (None, None, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception en predTop_prob: {0}'.format(e))\n",
    "        return (None, None, None)\n",
    "    \n",
    "    \n",
    "def predict_topic(sentence):\n",
    "    topics = ['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
    "    \n",
    "    sentence = prepare_text(sentence)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        if sentence:\n",
    "            prob, pol, text = pred_prob(sentence)\n",
    "            if prob.all():\n",
    "                return (topics[np.argmax(prob)], pol, text)\n",
    "            else:\n",
    "                return('No Topic', 0, None) \n",
    "        else:\n",
    "            return('No Topic', 0, None) \n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Exception en predict_topic: {0}'.format(e))\n",
    "        return('No Topic', 0, None)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Estan buscando perfiles, donde puedo enviar mi cv?'\n",
    "text = 'Hello my name is Roberto Valdez'\n",
    "text = 12\n",
    "text = 'si'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se paso a polarity_and_lang un texto menor que 3 caracters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('No Topic', 0, 'si')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_topic(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
