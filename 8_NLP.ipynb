{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data\\\\preguntas.csv\", sep=',', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Data\\\\X.npy', allow_pickle=True)\n",
    "y = np.load('Data\\\\y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_oh = np.zeros((y.shape[0], max(y)+1))\n",
    "#y_oh[np.arange(y.shape[0]), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "idx = np.arange(X.shape[0])\n",
    "random.shuffle(idx)  \n",
    "btra = np.random.choice(idx, int(0.8*X.shape[0]), replace=False)\n",
    "btes = [i for i in idx if i not in btra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtra = X[btra]\n",
    "ytra = y[btra]\n",
    "\n",
    "Xtes = X[btes]\n",
    "ytes = y[btes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 576)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, n_batches):  \n",
    "    \n",
    "    random.seed(123)\n",
    "    \n",
    "    batch_size = X.shape[0] // n_batches\n",
    "    \n",
    "    idx = np.arange(X.shape[0])\n",
    "    random.shuffle(idx)    \n",
    "    idx = idx[:n_batches*batch_size]\n",
    "        \n",
    "    for i in range(n_batches):            \n",
    "        bi = np.random.choice(idx, batch_size, replace=False)\n",
    "        X_batch = X[bi]\n",
    "        Y_batch = Y[bi]\n",
    "        idx = [i for i in idx if i not in bi]\n",
    "        yield (X_batch,Y_batch)       \n",
    "        \n",
    "        \n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc11 = nn.Linear(576, 24, bias=True)\n",
    "        self.fc12 = nn.Linear(24, 13, bias=True) \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = torch.tanh(self.fc11(x))\n",
    "        x1 = self.fc12(x1)     \n",
    "        return x1\n",
    "    \n",
    "    \n",
    "def fit(X, Y, Xt, Yt, net, optimizer, error, n_epochs, n_batches, device, PATH, min_val_loss = float('inf')):\n",
    "    \n",
    "    net = net.to(device)    \n",
    "    losses = []    \n",
    "    val_losses = []\n",
    "\n",
    "    val_inputs = torch.FloatTensor(Xt)\n",
    "    val_labels = torch.tensor(Yt, dtype=torch.long)\n",
    "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)  \n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "        running_loss = 0  \n",
    "         \n",
    "        \n",
    "        for batch_x, batch_y in batch_generator(X, Y, n_batches):  \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the inputs\n",
    "            inputs = torch.FloatTensor(batch_x)\n",
    "            labels = torch.tensor(batch_y, dtype=torch.long)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)             \n",
    "                \n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = error(outputs, labels)\n",
    "                        \n",
    "            loss.backward()    #obtain gradients      \n",
    "            optimizer.step()   #optimize\n",
    "                \n",
    "            running_loss += loss.item()      \n",
    "                \n",
    "        running_loss = running_loss/n_batches    \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_outputs = net.forward(val_inputs)\n",
    "            val_loss = error(val_outputs, val_labels) \n",
    "        \n",
    "        losses.append(running_loss)   \n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        \n",
    "        print('Epoch {0}: Training Loss: {1} Validation Loss: {2}'.format(epoch+1, running_loss, val_loss.item()))\n",
    "        \n",
    "        if val_loss.item() < min_val_loss:\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            print('New Checkpoint Saved into PATH')\n",
    "            min_val_loss = val_loss.item()\n",
    "        \n",
    "        \n",
    "def fweights_init_normal(m):     \n",
    "    classname = m.__class__.__name__\n",
    "    torch.manual_seed(0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n-1)\n",
    "        #y = 0.0001\n",
    "        m.weight.data.normal_(0, y)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.normal_(0, y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15000\n",
    "lr = 0.001\n",
    "n_batches = 3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH = 'Data\\\\model_checkpoint.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.sum(np.unique(ytes, return_counts=True)[1])/np.unique(ytes, return_counts=True)[1]\n",
    "class_weights = torch.FloatTensor(weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net1()\n",
    "net.apply(fweights_init_normal)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 2.6303813457489014 Validation Loss: 2.617147922515869\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2: Training Loss: 2.6187355518341064 Validation Loss: 2.6034090518951416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3: Training Loss: 2.598628362019857 Validation Loss: 2.5871829986572266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4: Training Loss: 2.5775736967722573 Validation Loss: 2.574556350708008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5: Training Loss: 2.5616602102915444 Validation Loss: 2.5686254501342773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 6: Training Loss: 2.55218513806661 Validation Loss: 2.565044641494751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 7: Training Loss: 2.546469767888387 Validation Loss: 2.5621063709259033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 8: Training Loss: 2.541802088419596 Validation Loss: 2.5596210956573486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 9: Training Loss: 2.5379830996195474 Validation Loss: 2.5571510791778564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 10: Training Loss: 2.53488818804423 Validation Loss: 2.554636001586914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 11: Training Loss: 2.531289498011271 Validation Loss: 2.5524020195007324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 12: Training Loss: 2.529526710510254 Validation Loss: 2.5502326488494873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 13: Training Loss: 2.527231136957804 Validation Loss: 2.548214912414551\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 14: Training Loss: 2.5255932013193765 Validation Loss: 2.54624342918396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 15: Training Loss: 2.523238261540731 Validation Loss: 2.5444397926330566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 16: Training Loss: 2.5213045279184976 Validation Loss: 2.5428178310394287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 17: Training Loss: 2.51900847752889 Validation Loss: 2.5413570404052734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 18: Training Loss: 2.517191171646118 Validation Loss: 2.5400586128234863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 19: Training Loss: 2.5153611501057944 Validation Loss: 2.538794994354248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 20: Training Loss: 2.5126880009969077 Validation Loss: 2.5376715660095215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 21: Training Loss: 2.5115233262379966 Validation Loss: 2.5366220474243164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 22: Training Loss: 2.509624401728312 Validation Loss: 2.5354950428009033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 23: Training Loss: 2.507537047068278 Validation Loss: 2.5344831943511963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 24: Training Loss: 2.5057950019836426 Validation Loss: 2.533376932144165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 25: Training Loss: 2.504351536432902 Validation Loss: 2.5323519706726074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 26: Training Loss: 2.5032832622528076 Validation Loss: 2.531235933303833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 27: Training Loss: 2.5005443890889487 Validation Loss: 2.5301413536071777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 28: Training Loss: 2.499627113342285 Validation Loss: 2.5289313793182373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 29: Training Loss: 2.496300379435221 Validation Loss: 2.527676582336426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 30: Training Loss: 2.4961324532826743 Validation Loss: 2.5264999866485596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 31: Training Loss: 2.4938496748606362 Validation Loss: 2.5251972675323486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 32: Training Loss: 2.4926795164744058 Validation Loss: 2.5237936973571777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 33: Training Loss: 2.490913470586141 Validation Loss: 2.5224266052246094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 34: Training Loss: 2.4888593355814614 Validation Loss: 2.521055221557617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 35: Training Loss: 2.48724897702535 Validation Loss: 2.5198020935058594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 36: Training Loss: 2.4851059118906655 Validation Loss: 2.518481731414795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 37: Training Loss: 2.4837448596954346 Validation Loss: 2.517167806625366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 38: Training Loss: 2.4827780723571777 Validation Loss: 2.5159225463867188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 39: Training Loss: 2.4793264071146646 Validation Loss: 2.5146422386169434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 40: Training Loss: 2.4786020119984946 Validation Loss: 2.5134005546569824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 41: Training Loss: 2.477874835332235 Validation Loss: 2.5122311115264893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 42: Training Loss: 2.4762724240620932 Validation Loss: 2.5110795497894287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 43: Training Loss: 2.474688450495402 Validation Loss: 2.5099096298217773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 44: Training Loss: 2.47300918896993 Validation Loss: 2.50878643989563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 45: Training Loss: 2.471926689147949 Validation Loss: 2.5076870918273926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 46: Training Loss: 2.4687615235646567 Validation Loss: 2.5065901279449463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 47: Training Loss: 2.4688875675201416 Validation Loss: 2.5055091381073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 48: Training Loss: 2.4668869972229004 Validation Loss: 2.504457473754883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 49: Training Loss: 2.465773900349935 Validation Loss: 2.503464460372925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 50: Training Loss: 2.4646114508310952 Validation Loss: 2.5025031566619873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 51: Training Loss: 2.462522109349569 Validation Loss: 2.501422882080078\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 52: Training Loss: 2.4606711069742837 Validation Loss: 2.500333786010742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 53: Training Loss: 2.4597107569376626 Validation Loss: 2.499345541000366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 54: Training Loss: 2.4585262139638266 Validation Loss: 2.498291015625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 55: Training Loss: 2.4566550254821777 Validation Loss: 2.497256278991699\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 56: Training Loss: 2.4548635482788086 Validation Loss: 2.496321439743042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 57: Training Loss: 2.4534138838450112 Validation Loss: 2.4954161643981934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 58: Training Loss: 2.452470382054647 Validation Loss: 2.49428391456604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 59: Training Loss: 2.4505180517832437 Validation Loss: 2.493241310119629\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 60: Training Loss: 2.4494760036468506 Validation Loss: 2.492218255996704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 61: Training Loss: 2.447889725367228 Validation Loss: 2.491220235824585\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 62: Training Loss: 2.446697394053141 Validation Loss: 2.49033522605896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 63: Training Loss: 2.444908062616984 Validation Loss: 2.4892935752868652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 64: Training Loss: 2.443471988042196 Validation Loss: 2.4883854389190674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 65: Training Loss: 2.4421639442443848 Validation Loss: 2.48747181892395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 66: Training Loss: 2.439137617746989 Validation Loss: 2.4864730834960938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 67: Training Loss: 2.4375980695088706 Validation Loss: 2.4854869842529297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 68: Training Loss: 2.4375840028127036 Validation Loss: 2.4844655990600586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 69: Training Loss: 2.4362200101216636 Validation Loss: 2.4833953380584717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 70: Training Loss: 2.4347230593363443 Validation Loss: 2.482351541519165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 71: Training Loss: 2.4330809116363525 Validation Loss: 2.4813332557678223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 72: Training Loss: 2.43181578318278 Validation Loss: 2.4804000854492188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 73: Training Loss: 2.429655392964681 Validation Loss: 2.479407548904419\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 74: Training Loss: 2.428299347559611 Validation Loss: 2.4785375595092773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 75: Training Loss: 2.4278058211008706 Validation Loss: 2.477559804916382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 76: Training Loss: 2.426262299219767 Validation Loss: 2.47663950920105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 77: Training Loss: 2.424465815226237 Validation Loss: 2.4755194187164307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 78: Training Loss: 2.4227632681528726 Validation Loss: 2.474470615386963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 79: Training Loss: 2.4216310183207193 Validation Loss: 2.4734604358673096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 80: Training Loss: 2.4199535846710205 Validation Loss: 2.4724295139312744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 81: Training Loss: 2.417672554651896 Validation Loss: 2.4712893962860107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 82: Training Loss: 2.417555014292399 Validation Loss: 2.470165967941284\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 83: Training Loss: 2.4156695206960044 Validation Loss: 2.4692115783691406\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 84: Training Loss: 2.4132705529530845 Validation Loss: 2.468064546585083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 85: Training Loss: 2.4123123486836753 Validation Loss: 2.467118978500366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 86: Training Loss: 2.4106053511301675 Validation Loss: 2.4661953449249268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 87: Training Loss: 2.4099458853403726 Validation Loss: 2.465155839920044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 88: Training Loss: 2.408094803492228 Validation Loss: 2.464038133621216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 89: Training Loss: 2.4064382712046304 Validation Loss: 2.463050603866577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 90: Training Loss: 2.4052122433980307 Validation Loss: 2.4619133472442627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 91: Training Loss: 2.403202215830485 Validation Loss: 2.4608373641967773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 92: Training Loss: 2.4019546508789062 Validation Loss: 2.459852695465088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 93: Training Loss: 2.4004432360331216 Validation Loss: 2.4588029384613037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 94: Training Loss: 2.399402618408203 Validation Loss: 2.457751989364624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 95: Training Loss: 2.398021936416626 Validation Loss: 2.45662260055542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 96: Training Loss: 2.396788994471232 Validation Loss: 2.4554357528686523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 97: Training Loss: 2.3952616850535073 Validation Loss: 2.454368829727173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 98: Training Loss: 2.3939178784688315 Validation Loss: 2.453392267227173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 99: Training Loss: 2.3927032152811685 Validation Loss: 2.4522476196289062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 100: Training Loss: 2.3911311626434326 Validation Loss: 2.4510843753814697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 101: Training Loss: 2.390213886896769 Validation Loss: 2.4500367641448975\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 102: Training Loss: 2.3887596925099692 Validation Loss: 2.448939561843872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 103: Training Loss: 2.3867541948954263 Validation Loss: 2.4477317333221436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 104: Training Loss: 2.386061429977417 Validation Loss: 2.4465484619140625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 105: Training Loss: 2.3841048081715903 Validation Loss: 2.4455037117004395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 106: Training Loss: 2.3826672236124673 Validation Loss: 2.444321870803833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 107: Training Loss: 2.3809526761372886 Validation Loss: 2.443065643310547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 108: Training Loss: 2.380685806274414 Validation Loss: 2.4419827461242676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 109: Training Loss: 2.3792478243509927 Validation Loss: 2.4410483837127686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 110: Training Loss: 2.377411444981893 Validation Loss: 2.4399592876434326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 111: Training Loss: 2.375629266103109 Validation Loss: 2.4388351440429688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 112: Training Loss: 2.3749327659606934 Validation Loss: 2.437803030014038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 113: Training Loss: 2.3732438882191977 Validation Loss: 2.43686842918396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 114: Training Loss: 2.371601104736328 Validation Loss: 2.4359097480773926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 115: Training Loss: 2.3703485329945884 Validation Loss: 2.43477725982666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 116: Training Loss: 2.3696930408477783 Validation Loss: 2.433596611022949\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 117: Training Loss: 2.3677282333374023 Validation Loss: 2.432450771331787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 118: Training Loss: 2.3668246269226074 Validation Loss: 2.431340217590332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 119: Training Loss: 2.365813732147217 Validation Loss: 2.4302968978881836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 120: Training Loss: 2.363802115122477 Validation Loss: 2.429171562194824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 121: Training Loss: 2.3628222942352295 Validation Loss: 2.428201198577881\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 122: Training Loss: 2.3611061573028564 Validation Loss: 2.4270617961883545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 123: Training Loss: 2.358794848124186 Validation Loss: 2.4259212017059326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 124: Training Loss: 2.3589146931966147 Validation Loss: 2.4247782230377197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 125: Training Loss: 2.3576536178588867 Validation Loss: 2.423583745956421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 126: Training Loss: 2.3568119208017984 Validation Loss: 2.4225308895111084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 127: Training Loss: 2.3549387454986572 Validation Loss: 2.421494245529175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 128: Training Loss: 2.353142738342285 Validation Loss: 2.420379161834717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 129: Training Loss: 2.3519140084584556 Validation Loss: 2.4192798137664795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 130: Training Loss: 2.3507120609283447 Validation Loss: 2.4183177947998047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 131: Training Loss: 2.349119265874227 Validation Loss: 2.4172019958496094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 132: Training Loss: 2.34735099474589 Validation Loss: 2.416067123413086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 133: Training Loss: 2.346790313720703 Validation Loss: 2.414968490600586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 134: Training Loss: 2.3452230294545493 Validation Loss: 2.413907051086426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 135: Training Loss: 2.3443988958994546 Validation Loss: 2.412778615951538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 136: Training Loss: 2.3422513008117676 Validation Loss: 2.4117631912231445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 137: Training Loss: 2.3406031131744385 Validation Loss: 2.41074538230896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 138: Training Loss: 2.3393234411875405 Validation Loss: 2.409628391265869\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 139: Training Loss: 2.3388710816701255 Validation Loss: 2.4084837436676025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 140: Training Loss: 2.3374670346577964 Validation Loss: 2.4074649810791016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 141: Training Loss: 2.3356217543284097 Validation Loss: 2.4063830375671387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 142: Training Loss: 2.3348985513051352 Validation Loss: 2.405275344848633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 143: Training Loss: 2.333406686782837 Validation Loss: 2.4042508602142334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 144: Training Loss: 2.331664800643921 Validation Loss: 2.40317702293396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 145: Training Loss: 2.3300986289978027 Validation Loss: 2.401998519897461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 146: Training Loss: 2.329493999481201 Validation Loss: 2.400792121887207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 147: Training Loss: 2.3273538748423257 Validation Loss: 2.399806261062622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 148: Training Loss: 2.3262430826822915 Validation Loss: 2.3986363410949707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 149: Training Loss: 2.325176556905111 Validation Loss: 2.397548198699951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 150: Training Loss: 2.3241063753763833 Validation Loss: 2.3964784145355225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 151: Training Loss: 2.3223888874053955 Validation Loss: 2.3953518867492676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 152: Training Loss: 2.3217126528422036 Validation Loss: 2.3941810131073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 153: Training Loss: 2.3190733591715493 Validation Loss: 2.3932294845581055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 154: Training Loss: 2.318544069925944 Validation Loss: 2.3921051025390625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 155: Training Loss: 2.317312161127726 Validation Loss: 2.3909215927124023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 156: Training Loss: 2.3150134881337485 Validation Loss: 2.3896846771240234\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157: Training Loss: 2.3148081302642822 Validation Loss: 2.388615846633911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 158: Training Loss: 2.3127084573109946 Validation Loss: 2.387582778930664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 159: Training Loss: 2.311711072921753 Validation Loss: 2.386451244354248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 160: Training Loss: 2.3092899322509766 Validation Loss: 2.3855361938476562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 161: Training Loss: 2.3090540568033853 Validation Loss: 2.3843936920166016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 162: Training Loss: 2.3069239457448325 Validation Loss: 2.3832709789276123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 163: Training Loss: 2.3061808745066323 Validation Loss: 2.3822782039642334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 164: Training Loss: 2.3044306437174478 Validation Loss: 2.3811872005462646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 165: Training Loss: 2.303008794784546 Validation Loss: 2.379986047744751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 166: Training Loss: 2.3025078773498535 Validation Loss: 2.37892484664917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 167: Training Loss: 2.300691763559977 Validation Loss: 2.3778247833251953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 168: Training Loss: 2.2991626262664795 Validation Loss: 2.376666784286499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 169: Training Loss: 2.2981350421905518 Validation Loss: 2.375425100326538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 170: Training Loss: 2.2973148028055825 Validation Loss: 2.3742470741271973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 171: Training Loss: 2.2954240640004477 Validation Loss: 2.3731446266174316\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 172: Training Loss: 2.29410457611084 Validation Loss: 2.371967077255249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 173: Training Loss: 2.292382796605428 Validation Loss: 2.3709876537323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 174: Training Loss: 2.2914364337921143 Validation Loss: 2.370022773742676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 175: Training Loss: 2.289799769719442 Validation Loss: 2.368882179260254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 176: Training Loss: 2.2879697481791177 Validation Loss: 2.3677408695220947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 177: Training Loss: 2.286804437637329 Validation Loss: 2.3665812015533447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 178: Training Loss: 2.2848880290985107 Validation Loss: 2.3655755519866943\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 179: Training Loss: 2.283541202545166 Validation Loss: 2.3644940853118896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 180: Training Loss: 2.282522678375244 Validation Loss: 2.3633365631103516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 181: Training Loss: 2.2818082173665366 Validation Loss: 2.3621292114257812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 182: Training Loss: 2.2800607681274414 Validation Loss: 2.36091947555542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 183: Training Loss: 2.2787839571634927 Validation Loss: 2.3597047328948975\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 184: Training Loss: 2.2771592934926352 Validation Loss: 2.358619213104248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 185: Training Loss: 2.2758737405141196 Validation Loss: 2.3575518131256104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 186: Training Loss: 2.274369398752848 Validation Loss: 2.3564260005950928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 187: Training Loss: 2.273237864176432 Validation Loss: 2.3553788661956787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 188: Training Loss: 2.271771192550659 Validation Loss: 2.354325532913208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 189: Training Loss: 2.270423650741577 Validation Loss: 2.3532204627990723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 190: Training Loss: 2.26938263575236 Validation Loss: 2.3520755767822266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 191: Training Loss: 2.2678585847218833 Validation Loss: 2.350994825363159\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 192: Training Loss: 2.2663007577260337 Validation Loss: 2.349666118621826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 193: Training Loss: 2.265120029449463 Validation Loss: 2.3485300540924072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 194: Training Loss: 2.262002388636271 Validation Loss: 2.347304344177246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 195: Training Loss: 2.2624531586964927 Validation Loss: 2.346240997314453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 196: Training Loss: 2.260575850804647 Validation Loss: 2.345078229904175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 197: Training Loss: 2.2594570318857827 Validation Loss: 2.3441038131713867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 198: Training Loss: 2.258122205734253 Validation Loss: 2.3428356647491455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 199: Training Loss: 2.25493852297465 Validation Loss: 2.341905355453491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 200: Training Loss: 2.2553606033325195 Validation Loss: 2.3406755924224854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 201: Training Loss: 2.2533093293507895 Validation Loss: 2.3395602703094482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 202: Training Loss: 2.252321640650431 Validation Loss: 2.338303804397583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 203: Training Loss: 2.250635306040446 Validation Loss: 2.3371026515960693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 204: Training Loss: 2.249582131703695 Validation Loss: 2.335862398147583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 205: Training Loss: 2.247592051823934 Validation Loss: 2.3347694873809814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 206: Training Loss: 2.2460520267486572 Validation Loss: 2.3337128162384033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 207: Training Loss: 2.245195468266805 Validation Loss: 2.3325982093811035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 208: Training Loss: 2.2438579400380454 Validation Loss: 2.3314247131347656\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 209: Training Loss: 2.2414515018463135 Validation Loss: 2.3303933143615723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 210: Training Loss: 2.2409958044687905 Validation Loss: 2.329179048538208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 211: Training Loss: 2.2391202449798584 Validation Loss: 2.3279285430908203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 212: Training Loss: 2.237316608428955 Validation Loss: 2.32678484916687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 213: Training Loss: 2.2368388175964355 Validation Loss: 2.3257298469543457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 214: Training Loss: 2.2352596124013266 Validation Loss: 2.3246042728424072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 215: Training Loss: 2.232983191808065 Validation Loss: 2.3233039379119873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 216: Training Loss: 2.2324164708455405 Validation Loss: 2.322200059890747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 217: Training Loss: 2.2299181620279946 Validation Loss: 2.3209805488586426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 218: Training Loss: 2.2294348875681558 Validation Loss: 2.3198766708374023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 219: Training Loss: 2.2278764247894287 Validation Loss: 2.318551778793335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 220: Training Loss: 2.226837158203125 Validation Loss: 2.317415952682495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 221: Training Loss: 2.2246755758921304 Validation Loss: 2.316267251968384\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 222: Training Loss: 2.2241074244181314 Validation Loss: 2.3150177001953125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 223: Training Loss: 2.2221941153208413 Validation Loss: 2.3139569759368896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 224: Training Loss: 2.221256971359253 Validation Loss: 2.3128905296325684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 225: Training Loss: 2.2193472385406494 Validation Loss: 2.311779260635376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 226: Training Loss: 2.2179644107818604 Validation Loss: 2.3106393814086914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 227: Training Loss: 2.216444651285807 Validation Loss: 2.3094065189361572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 228: Training Loss: 2.214991807937622 Validation Loss: 2.3081631660461426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 229: Training Loss: 2.2132513523101807 Validation Loss: 2.30702805519104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 230: Training Loss: 2.2119195461273193 Validation Loss: 2.3058135509490967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 231: Training Loss: 2.210867007573446 Validation Loss: 2.304785966873169\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 232: Training Loss: 2.210096995035807 Validation Loss: 2.3036792278289795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 233: Training Loss: 2.2081968784332275 Validation Loss: 2.3024208545684814\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234: Training Loss: 2.206247170766195 Validation Loss: 2.3013272285461426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 235: Training Loss: 2.2051886717478433 Validation Loss: 2.300062894821167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 236: Training Loss: 2.2034106254577637 Validation Loss: 2.2988884449005127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 237: Training Loss: 2.20242969195048 Validation Loss: 2.2977964878082275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 238: Training Loss: 2.200162649154663 Validation Loss: 2.296495199203491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 239: Training Loss: 2.199256102244059 Validation Loss: 2.2952780723571777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 240: Training Loss: 2.1983326276143393 Validation Loss: 2.293999433517456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 241: Training Loss: 2.1962428092956543 Validation Loss: 2.2926225662231445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 242: Training Loss: 2.1921302477518716 Validation Loss: 2.2914276123046875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 243: Training Loss: 2.1936493714650473 Validation Loss: 2.2903523445129395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 244: Training Loss: 2.1920429865519204 Validation Loss: 2.289234161376953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 245: Training Loss: 2.1907499631245932 Validation Loss: 2.288374900817871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 246: Training Loss: 2.1885066827138266 Validation Loss: 2.2871830463409424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 247: Training Loss: 2.188096761703491 Validation Loss: 2.2859995365142822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 248: Training Loss: 2.186549425125122 Validation Loss: 2.2847020626068115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 249: Training Loss: 2.1846936543782554 Validation Loss: 2.283477783203125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 250: Training Loss: 2.1828578313191733 Validation Loss: 2.28234601020813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 251: Training Loss: 2.1813987096150718 Validation Loss: 2.281094789505005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 252: Training Loss: 2.1802217165629068 Validation Loss: 2.2798266410827637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 253: Training Loss: 2.1792944272359214 Validation Loss: 2.2785563468933105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 254: Training Loss: 2.1776095231374106 Validation Loss: 2.2772603034973145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 255: Training Loss: 2.175250848134359 Validation Loss: 2.276036262512207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 256: Training Loss: 2.1746393839518228 Validation Loss: 2.2749650478363037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 257: Training Loss: 2.1726982593536377 Validation Loss: 2.2738280296325684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 258: Training Loss: 2.1719090143839517 Validation Loss: 2.2728240489959717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 259: Training Loss: 2.170066754023234 Validation Loss: 2.2716474533081055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 260: Training Loss: 2.1684619585673013 Validation Loss: 2.2704715728759766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 261: Training Loss: 2.167051076889038 Validation Loss: 2.2691264152526855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 262: Training Loss: 2.164344549179077 Validation Loss: 2.2679035663604736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 263: Training Loss: 2.1640822092692056 Validation Loss: 2.26667857170105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 264: Training Loss: 2.1614534854888916 Validation Loss: 2.265415906906128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 265: Training Loss: 2.161043405532837 Validation Loss: 2.26418399810791\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 266: Training Loss: 2.1594111919403076 Validation Loss: 2.2630863189697266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 267: Training Loss: 2.1581249237060547 Validation Loss: 2.261897087097168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 268: Training Loss: 2.157202402750651 Validation Loss: 2.2606961727142334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 269: Training Loss: 2.1548167069753013 Validation Loss: 2.2595865726470947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 270: Training Loss: 2.153662999471029 Validation Loss: 2.2581708431243896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 271: Training Loss: 2.151172081629435 Validation Loss: 2.257089614868164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 272: Training Loss: 2.1507508754730225 Validation Loss: 2.2559986114501953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 273: Training Loss: 2.149468183517456 Validation Loss: 2.2546322345733643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 274: Training Loss: 2.1481522719065347 Validation Loss: 2.253218650817871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 275: Training Loss: 2.146584908167521 Validation Loss: 2.2519173622131348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 276: Training Loss: 2.144786834716797 Validation Loss: 2.250784158706665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 277: Training Loss: 2.1433726151784263 Validation Loss: 2.249441146850586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 278: Training Loss: 2.1419535477956138 Validation Loss: 2.248464584350586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 279: Training Loss: 2.138862371444702 Validation Loss: 2.2473130226135254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 280: Training Loss: 2.1384224891662598 Validation Loss: 2.2461206912994385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 281: Training Loss: 2.1366270383199057 Validation Loss: 2.2449660301208496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 282: Training Loss: 2.135362386703491 Validation Loss: 2.243593454360962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 283: Training Loss: 2.1340920130411782 Validation Loss: 2.242465019226074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 284: Training Loss: 2.1326462427775064 Validation Loss: 2.2411816120147705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 285: Training Loss: 2.1311022440592446 Validation Loss: 2.24005126953125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 286: Training Loss: 2.12900439898173 Validation Loss: 2.2388501167297363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 287: Training Loss: 2.1285201708475747 Validation Loss: 2.2375993728637695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 288: Training Loss: 2.1266578833262124 Validation Loss: 2.236266613006592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 289: Training Loss: 2.1248668829600015 Validation Loss: 2.234936237335205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 290: Training Loss: 2.1245850721995034 Validation Loss: 2.2338075637817383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 291: Training Loss: 2.120671510696411 Validation Loss: 2.2325191497802734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 292: Training Loss: 2.1205854415893555 Validation Loss: 2.2314653396606445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 293: Training Loss: 2.119499444961548 Validation Loss: 2.2302074432373047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 294: Training Loss: 2.116763194402059 Validation Loss: 2.2290287017822266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 295: Training Loss: 2.1164552370707193 Validation Loss: 2.2280585765838623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 296: Training Loss: 2.114410161972046 Validation Loss: 2.2268335819244385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 297: Training Loss: 2.113647222518921 Validation Loss: 2.2254889011383057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 298: Training Loss: 2.111337184906006 Validation Loss: 2.224118709564209\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 299: Training Loss: 2.1096699237823486 Validation Loss: 2.2226691246032715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 300: Training Loss: 2.1078224182128906 Validation Loss: 2.2214150428771973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 301: Training Loss: 2.1074089209238687 Validation Loss: 2.2201950550079346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 302: Training Loss: 2.1065917015075684 Validation Loss: 2.2190399169921875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 303: Training Loss: 2.1037606398264566 Validation Loss: 2.2177906036376953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 304: Training Loss: 2.1021726926167807 Validation Loss: 2.2163877487182617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 305: Training Loss: 2.0995985666910806 Validation Loss: 2.2152435779571533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 306: Training Loss: 2.099527279535929 Validation Loss: 2.2141332626342773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 307: Training Loss: 2.096036990483602 Validation Loss: 2.2129018306732178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 308: Training Loss: 2.095939874649048 Validation Loss: 2.211651563644409\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 309: Training Loss: 2.095059394836426 Validation Loss: 2.210465431213379\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 310: Training Loss: 2.0917258262634277 Validation Loss: 2.2094266414642334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 311: Training Loss: 2.0913123289744058 Validation Loss: 2.2081456184387207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 312: Training Loss: 2.08979598681132 Validation Loss: 2.2069592475891113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 313: Training Loss: 2.088326374689738 Validation Loss: 2.2056543827056885\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314: Training Loss: 2.086108605066935 Validation Loss: 2.2043941020965576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 315: Training Loss: 2.0848239262898765 Validation Loss: 2.2031989097595215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 316: Training Loss: 2.084248701731364 Validation Loss: 2.2020344734191895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 317: Training Loss: 2.082228342692057 Validation Loss: 2.2009875774383545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 318: Training Loss: 2.080556790033976 Validation Loss: 2.1995391845703125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 319: Training Loss: 2.07952610651652 Validation Loss: 2.198244333267212\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 320: Training Loss: 2.077337106068929 Validation Loss: 2.196901798248291\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 321: Training Loss: 2.076007604598999 Validation Loss: 2.195645809173584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 322: Training Loss: 2.074878772099813 Validation Loss: 2.194072723388672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 323: Training Loss: 2.073453426361084 Validation Loss: 2.1929304599761963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 324: Training Loss: 2.0716206232706704 Validation Loss: 2.1917824745178223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 325: Training Loss: 2.070206960042318 Validation Loss: 2.190460443496704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 326: Training Loss: 2.068580945332845 Validation Loss: 2.1891674995422363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 327: Training Loss: 2.0664242108662925 Validation Loss: 2.187894105911255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 328: Training Loss: 2.0660961469014487 Validation Loss: 2.186525583267212\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 329: Training Loss: 2.0642968018849692 Validation Loss: 2.1855108737945557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 330: Training Loss: 2.062119722366333 Validation Loss: 2.184115171432495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 331: Training Loss: 2.060415824254354 Validation Loss: 2.1829118728637695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 332: Training Loss: 2.059342543284098 Validation Loss: 2.181809902191162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 333: Training Loss: 2.058010737101237 Validation Loss: 2.180441379547119\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 334: Training Loss: 2.055027484893799 Validation Loss: 2.179333448410034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 335: Training Loss: 2.0546089013417563 Validation Loss: 2.178208589553833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 336: Training Loss: 2.053182601928711 Validation Loss: 2.1768076419830322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 337: Training Loss: 2.0515729586283364 Validation Loss: 2.1756372451782227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 338: Training Loss: 2.0477609634399414 Validation Loss: 2.174382448196411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 339: Training Loss: 2.048242966334025 Validation Loss: 2.173189878463745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 340: Training Loss: 2.046990712483724 Validation Loss: 2.171787738800049\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 341: Training Loss: 2.0452748934427896 Validation Loss: 2.1705424785614014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 342: Training Loss: 2.043055454889933 Validation Loss: 2.169288158416748\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 343: Training Loss: 2.0421880880991616 Validation Loss: 2.1678643226623535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 344: Training Loss: 2.040414730707804 Validation Loss: 2.166616678237915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 345: Training Loss: 2.0391833782196045 Validation Loss: 2.1653146743774414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 346: Training Loss: 2.037803332010905 Validation Loss: 2.164099931716919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 347: Training Loss: 2.0354181925455728 Validation Loss: 2.1628921031951904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 348: Training Loss: 2.0338998635609946 Validation Loss: 2.1616671085357666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 349: Training Loss: 2.0324788093566895 Validation Loss: 2.160372734069824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 350: Training Loss: 2.0311455726623535 Validation Loss: 2.159214735031128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 351: Training Loss: 2.028173883756002 Validation Loss: 2.1578402519226074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 352: Training Loss: 2.02874485651652 Validation Loss: 2.1565046310424805\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 353: Training Loss: 2.026280721028646 Validation Loss: 2.155285120010376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 354: Training Loss: 2.0242834091186523 Validation Loss: 2.154041290283203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 355: Training Loss: 2.0232131481170654 Validation Loss: 2.152867078781128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 356: Training Loss: 2.02171790599823 Validation Loss: 2.151435136795044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 357: Training Loss: 2.021233876546224 Validation Loss: 2.1501476764678955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 358: Training Loss: 2.018629233042399 Validation Loss: 2.1487371921539307\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 359: Training Loss: 2.016864617665609 Validation Loss: 2.1475229263305664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 360: Training Loss: 2.015175183614095 Validation Loss: 2.146458148956299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 361: Training Loss: 2.013252774874369 Validation Loss: 2.14518404006958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 362: Training Loss: 2.011965831120809 Validation Loss: 2.1440417766571045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 363: Training Loss: 2.0107526779174805 Validation Loss: 2.1430625915527344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 364: Training Loss: 2.0099300940831504 Validation Loss: 2.141843795776367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 365: Training Loss: 2.0074042876561484 Validation Loss: 2.1403331756591797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 366: Training Loss: 2.0058550437291465 Validation Loss: 2.138941764831543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 367: Training Loss: 2.0038610696792603 Validation Loss: 2.1375222206115723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 368: Training Loss: 2.0025519927342734 Validation Loss: 2.1359634399414062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 369: Training Loss: 2.000987966855367 Validation Loss: 2.1345651149749756\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 370: Training Loss: 1.9993787209192913 Validation Loss: 2.133457899093628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 371: Training Loss: 1.9979086716969807 Validation Loss: 2.1324472427368164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 372: Training Loss: 1.9957844416300456 Validation Loss: 2.131174325942993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 373: Training Loss: 1.9943502346674602 Validation Loss: 2.1300048828125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 374: Training Loss: 1.9929275115331013 Validation Loss: 2.1288228034973145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 375: Training Loss: 1.9914042154947917 Validation Loss: 2.1275289058685303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 376: Training Loss: 1.9902487595876057 Validation Loss: 2.1262011528015137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 377: Training Loss: 1.9881802399953206 Validation Loss: 2.1248586177825928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 378: Training Loss: 1.9861849943796794 Validation Loss: 2.123655080795288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 379: Training Loss: 1.9852892557779949 Validation Loss: 2.1220991611480713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 380: Training Loss: 1.9834508895874023 Validation Loss: 2.1208348274230957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 381: Training Loss: 1.9810627698898315 Validation Loss: 2.1196045875549316\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 382: Training Loss: 1.979990005493164 Validation Loss: 2.1183862686157227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 383: Training Loss: 1.9784412781397502 Validation Loss: 2.1170949935913086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 384: Training Loss: 1.9775650103886921 Validation Loss: 2.1156842708587646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 385: Training Loss: 1.9759759505589802 Validation Loss: 2.1143689155578613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 386: Training Loss: 1.973362962404887 Validation Loss: 2.1131539344787598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 387: Training Loss: 1.9720137119293213 Validation Loss: 2.111989974975586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 388: Training Loss: 1.9710792303085327 Validation Loss: 2.1105942726135254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 389: Training Loss: 1.969254692395528 Validation Loss: 2.1093361377716064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 390: Training Loss: 1.967785636583964 Validation Loss: 2.108247995376587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 391: Training Loss: 1.9644118944803874 Validation Loss: 2.1069493293762207\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392: Training Loss: 1.9646466573079426 Validation Loss: 2.1056466102600098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 393: Training Loss: 1.962651252746582 Validation Loss: 2.104787588119507\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 394: Training Loss: 1.9611986478169758 Validation Loss: 2.103365182876587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 395: Training Loss: 1.9586401383082073 Validation Loss: 2.1019184589385986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 396: Training Loss: 1.9577593008677165 Validation Loss: 2.1007206439971924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 397: Training Loss: 1.9557841221491497 Validation Loss: 2.099320888519287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 398: Training Loss: 1.9546318848927815 Validation Loss: 2.097912549972534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 399: Training Loss: 1.9525945981343586 Validation Loss: 2.0965099334716797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 400: Training Loss: 1.9508407513300579 Validation Loss: 2.0950875282287598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 401: Training Loss: 1.9505401849746704 Validation Loss: 2.0939080715179443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 402: Training Loss: 1.948503851890564 Validation Loss: 2.092731237411499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 403: Training Loss: 1.946410894393921 Validation Loss: 2.0914559364318848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 404: Training Loss: 1.9455794095993042 Validation Loss: 2.0901482105255127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 405: Training Loss: 1.943100929260254 Validation Loss: 2.088961362838745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 406: Training Loss: 1.9418627421061199 Validation Loss: 2.0875244140625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 407: Training Loss: 1.9390581051508586 Validation Loss: 2.086385726928711\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 408: Training Loss: 1.937816858291626 Validation Loss: 2.0849721431732178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 409: Training Loss: 1.9363913536071777 Validation Loss: 2.083544969558716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 410: Training Loss: 1.9361050526301067 Validation Loss: 2.0823445320129395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 411: Training Loss: 1.9332270622253418 Validation Loss: 2.0812392234802246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 412: Training Loss: 1.9321145216623943 Validation Loss: 2.0801079273223877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 413: Training Loss: 1.9307600657145183 Validation Loss: 2.0788862705230713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 414: Training Loss: 1.9289725224177043 Validation Loss: 2.0773043632507324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 415: Training Loss: 1.9279379844665527 Validation Loss: 2.076223611831665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 416: Training Loss: 1.9260194698969524 Validation Loss: 2.0747532844543457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 417: Training Loss: 1.9243052005767822 Validation Loss: 2.073356866836548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 418: Training Loss: 1.9232240517934163 Validation Loss: 2.0719847679138184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 419: Training Loss: 1.9210678339004517 Validation Loss: 2.070557117462158\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 420: Training Loss: 1.9202131430308025 Validation Loss: 2.06935453414917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 421: Training Loss: 1.9171544313430786 Validation Loss: 2.0682411193847656\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 422: Training Loss: 1.915663202603658 Validation Loss: 2.0668604373931885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 423: Training Loss: 1.9143275022506714 Validation Loss: 2.0656278133392334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 424: Training Loss: 1.9126112461090088 Validation Loss: 2.064362049102783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 425: Training Loss: 1.9112488428751628 Validation Loss: 2.063101053237915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 426: Training Loss: 1.9103459517161052 Validation Loss: 2.061798334121704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 427: Training Loss: 1.9080411990483601 Validation Loss: 2.0604593753814697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 428: Training Loss: 1.9065396388371785 Validation Loss: 2.0591609477996826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 429: Training Loss: 1.904902497927348 Validation Loss: 2.057866096496582\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 430: Training Loss: 1.9034078121185303 Validation Loss: 2.0566394329071045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 431: Training Loss: 1.9021928707758586 Validation Loss: 2.055478096008301\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 432: Training Loss: 1.9002056519190471 Validation Loss: 2.0541422367095947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 433: Training Loss: 1.8990219036738079 Validation Loss: 2.0528392791748047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 434: Training Loss: 1.897406538327535 Validation Loss: 2.051445722579956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 435: Training Loss: 1.8956708113352458 Validation Loss: 2.049787759780884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 436: Training Loss: 1.8934932549794514 Validation Loss: 2.0484459400177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 437: Training Loss: 1.8920290072758992 Validation Loss: 2.0471537113189697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 438: Training Loss: 1.8901845216751099 Validation Loss: 2.0459096431732178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 439: Training Loss: 1.8883448044459026 Validation Loss: 2.0445685386657715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 440: Training Loss: 1.8871570428212483 Validation Loss: 2.0433104038238525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 441: Training Loss: 1.8856600920359294 Validation Loss: 2.042402505874634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 442: Training Loss: 1.8835850954055786 Validation Loss: 2.0411267280578613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 443: Training Loss: 1.8814318974812825 Validation Loss: 2.0397002696990967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 444: Training Loss: 1.8812692960103352 Validation Loss: 2.0386414527893066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 445: Training Loss: 1.880002776781718 Validation Loss: 2.0373709201812744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 446: Training Loss: 1.8778244654337566 Validation Loss: 2.036039352416992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 447: Training Loss: 1.8757291634877522 Validation Loss: 2.034571886062622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 448: Training Loss: 1.8744817972183228 Validation Loss: 2.033252477645874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 449: Training Loss: 1.8718039989471436 Validation Loss: 2.0318572521209717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 450: Training Loss: 1.8715210755666096 Validation Loss: 2.0304758548736572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 451: Training Loss: 1.8685415188471477 Validation Loss: 2.0291545391082764\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 452: Training Loss: 1.8674702246983845 Validation Loss: 2.027815580368042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 453: Training Loss: 1.866358796755473 Validation Loss: 2.0266051292419434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 454: Training Loss: 1.8646727403004963 Validation Loss: 2.025325059890747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 455: Training Loss: 1.863139271736145 Validation Loss: 2.024137496948242\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 456: Training Loss: 1.861138145128886 Validation Loss: 2.022820472717285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 457: Training Loss: 1.8599785168965657 Validation Loss: 2.021498203277588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 458: Training Loss: 1.8585606813430786 Validation Loss: 2.020200729370117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 459: Training Loss: 1.8565853834152222 Validation Loss: 2.0188510417938232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 460: Training Loss: 1.8552403450012207 Validation Loss: 2.0177533626556396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 461: Training Loss: 1.853157599767049 Validation Loss: 2.0164072513580322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 462: Training Loss: 1.851657549540202 Validation Loss: 2.01483416557312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 463: Training Loss: 1.8500433762868245 Validation Loss: 2.013659715652466\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 464: Training Loss: 1.848642349243164 Validation Loss: 2.01225209236145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 465: Training Loss: 1.8472981850306194 Validation Loss: 2.0110599994659424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 466: Training Loss: 1.845379908879598 Validation Loss: 2.0095255374908447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 467: Training Loss: 1.8436278502146404 Validation Loss: 2.008697748184204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 468: Training Loss: 1.8417295217514038 Validation Loss: 2.0074808597564697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 469: Training Loss: 1.8391300042470295 Validation Loss: 2.006105422973633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 470: Training Loss: 1.8390729029973347 Validation Loss: 2.0048553943634033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 471: Training Loss: 1.8367153803507488 Validation Loss: 2.003436326980591\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 472: Training Loss: 1.8358005285263062 Validation Loss: 2.0021023750305176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 473: Training Loss: 1.8335540692011516 Validation Loss: 2.0007095336914062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 474: Training Loss: 1.8325621287027996 Validation Loss: 1.9995166063308716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 475: Training Loss: 1.8306748867034912 Validation Loss: 1.9981764554977417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 476: Training Loss: 1.8286486466725667 Validation Loss: 1.9968453645706177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 477: Training Loss: 1.8274671236673992 Validation Loss: 1.9958088397979736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 478: Training Loss: 1.8258822758992512 Validation Loss: 1.9943798780441284\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 479: Training Loss: 1.8245623111724854 Validation Loss: 1.9928810596466064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 480: Training Loss: 1.823143442471822 Validation Loss: 1.9912660121917725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 481: Training Loss: 1.8210825522740681 Validation Loss: 1.9901012182235718\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 482: Training Loss: 1.8192871411641438 Validation Loss: 1.9888834953308105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 483: Training Loss: 1.817902406056722 Validation Loss: 1.9876985549926758\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 484: Training Loss: 1.8165355523427327 Validation Loss: 1.98659348487854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 485: Training Loss: 1.8147639036178589 Validation Loss: 1.9851855039596558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 486: Training Loss: 1.8127928177515666 Validation Loss: 1.9839564561843872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 487: Training Loss: 1.8112557729085286 Validation Loss: 1.9829012155532837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 488: Training Loss: 1.8097378412882488 Validation Loss: 1.981403112411499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 489: Training Loss: 1.8082705736160278 Validation Loss: 1.9802045822143555\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 490: Training Loss: 1.8065808216730754 Validation Loss: 1.9789530038833618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 491: Training Loss: 1.8070175250371296 Validation Loss: 1.9775316715240479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 492: Training Loss: 1.8031782309214275 Validation Loss: 1.9763295650482178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 493: Training Loss: 1.800964077313741 Validation Loss: 1.9747651815414429\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 494: Training Loss: 1.8001746733983357 Validation Loss: 1.9737367630004883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 495: Training Loss: 1.7995702028274536 Validation Loss: 1.97222101688385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 496: Training Loss: 1.797218124071757 Validation Loss: 1.9706944227218628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 497: Training Loss: 1.7965562740961711 Validation Loss: 1.9694936275482178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 498: Training Loss: 1.7928556601206462 Validation Loss: 1.9681988954544067\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 499: Training Loss: 1.792377233505249 Validation Loss: 1.9670116901397705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 500: Training Loss: 1.790205677350362 Validation Loss: 1.965808629989624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 501: Training Loss: 1.7887444496154785 Validation Loss: 1.96451997756958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 502: Training Loss: 1.7887758016586304 Validation Loss: 1.9631733894348145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 503: Training Loss: 1.7847891251246135 Validation Loss: 1.9618988037109375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 504: Training Loss: 1.7827799320220947 Validation Loss: 1.9607117176055908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 505: Training Loss: 1.7825604677200317 Validation Loss: 1.9592739343643188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 506: Training Loss: 1.7807539304097493 Validation Loss: 1.9579702615737915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 507: Training Loss: 1.7794190645217896 Validation Loss: 1.9565881490707397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 508: Training Loss: 1.778462251027425 Validation Loss: 1.9553996324539185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 509: Training Loss: 1.7760374943415325 Validation Loss: 1.9544168710708618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 510: Training Loss: 1.7745120922724407 Validation Loss: 1.9529379606246948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 511: Training Loss: 1.7721118132273357 Validation Loss: 1.9514329433441162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 512: Training Loss: 1.7716981172561646 Validation Loss: 1.9501903057098389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 513: Training Loss: 1.7705103158950806 Validation Loss: 1.9490147829055786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 514: Training Loss: 1.767768661181132 Validation Loss: 1.9477734565734863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 515: Training Loss: 1.7657265265782673 Validation Loss: 1.9464279413223267\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 516: Training Loss: 1.7645747264226277 Validation Loss: 1.9450981616973877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 517: Training Loss: 1.76314644018809 Validation Loss: 1.9440003633499146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 518: Training Loss: 1.7626996437708538 Validation Loss: 1.9425222873687744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 519: Training Loss: 1.7600118319193523 Validation Loss: 1.9409973621368408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 520: Training Loss: 1.758048415184021 Validation Loss: 1.9397298097610474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 521: Training Loss: 1.7556987603505452 Validation Loss: 1.938488483428955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 522: Training Loss: 1.7569456100463867 Validation Loss: 1.9372919797897339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 523: Training Loss: 1.75258473555247 Validation Loss: 1.9360028505325317\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 524: Training Loss: 1.7513298988342285 Validation Loss: 1.9347842931747437\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 525: Training Loss: 1.7504629691441853 Validation Loss: 1.9335269927978516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 526: Training Loss: 1.748474399248759 Validation Loss: 1.9322105646133423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 527: Training Loss: 1.7469685077667236 Validation Loss: 1.9309494495391846\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 528: Training Loss: 1.7455750306447346 Validation Loss: 1.9295456409454346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 529: Training Loss: 1.7445167303085327 Validation Loss: 1.9284569025039673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 530: Training Loss: 1.7434632778167725 Validation Loss: 1.927308440208435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 531: Training Loss: 1.7404521703720093 Validation Loss: 1.9257762432098389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 532: Training Loss: 1.7395577430725098 Validation Loss: 1.924594759941101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 533: Training Loss: 1.7397655248641968 Validation Loss: 1.9231966733932495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 534: Training Loss: 1.7360687255859375 Validation Loss: 1.9220446348190308\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 535: Training Loss: 1.734265883763631 Validation Loss: 1.92087721824646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 536: Training Loss: 1.7334558566411336 Validation Loss: 1.919433832168579\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 537: Training Loss: 1.7298046747843425 Validation Loss: 1.9180771112442017\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 538: Training Loss: 1.7301586071650188 Validation Loss: 1.916547179222107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 539: Training Loss: 1.728952964146932 Validation Loss: 1.9150853157043457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 540: Training Loss: 1.725567619005839 Validation Loss: 1.9136977195739746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 541: Training Loss: 1.7247078816095989 Validation Loss: 1.912793517112732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 542: Training Loss: 1.7234108845392864 Validation Loss: 1.9116867780685425\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 543: Training Loss: 1.7217452923456829 Validation Loss: 1.9104937314987183\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 544: Training Loss: 1.7197903394699097 Validation Loss: 1.9090590476989746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 545: Training Loss: 1.719133694966634 Validation Loss: 1.907872200012207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 546: Training Loss: 1.7169861793518066 Validation Loss: 1.9066894054412842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 547: Training Loss: 1.7151952584584553 Validation Loss: 1.905306339263916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 548: Training Loss: 1.712281346321106 Validation Loss: 1.9039229154586792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 549: Training Loss: 1.7123546997706096 Validation Loss: 1.9028147459030151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 550: Training Loss: 1.7119500239690144 Validation Loss: 1.9015684127807617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 551: Training Loss: 1.7090612649917603 Validation Loss: 1.9001308679580688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 552: Training Loss: 1.7071718374888103 Validation Loss: 1.8985716104507446\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 553: Training Loss: 1.705946723620097 Validation Loss: 1.8973239660263062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 554: Training Loss: 1.704391598701477 Validation Loss: 1.8962286710739136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 555: Training Loss: 1.7026944955190022 Validation Loss: 1.8948066234588623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 556: Training Loss: 1.7004083395004272 Validation Loss: 1.8935304880142212\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 557: Training Loss: 1.6995800336201985 Validation Loss: 1.892332673072815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 558: Training Loss: 1.6966572602589924 Validation Loss: 1.8907287120819092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 559: Training Loss: 1.6975492238998413 Validation Loss: 1.8897364139556885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 560: Training Loss: 1.694112499554952 Validation Loss: 1.888543963432312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 561: Training Loss: 1.6925290425618489 Validation Loss: 1.887162446975708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 562: Training Loss: 1.691243290901184 Validation Loss: 1.8858126401901245\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 563: Training Loss: 1.6899396181106567 Validation Loss: 1.8845806121826172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 564: Training Loss: 1.6888874371846516 Validation Loss: 1.8833342790603638\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 565: Training Loss: 1.6870024998982747 Validation Loss: 1.8820116519927979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 566: Training Loss: 1.6845768690109253 Validation Loss: 1.8809278011322021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 567: Training Loss: 1.6832595268885295 Validation Loss: 1.8796076774597168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 568: Training Loss: 1.6824968258539836 Validation Loss: 1.8784743547439575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 569: Training Loss: 1.6797551314036052 Validation Loss: 1.877013921737671\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 570: Training Loss: 1.679243524869283 Validation Loss: 1.8755788803100586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 571: Training Loss: 1.677643855412801 Validation Loss: 1.8744388818740845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 572: Training Loss: 1.6761172612508137 Validation Loss: 1.8731495141983032\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 573: Training Loss: 1.6743086576461792 Validation Loss: 1.8719472885131836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 574: Training Loss: 1.6728326479593914 Validation Loss: 1.8708449602127075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 575: Training Loss: 1.6721973816553752 Validation Loss: 1.8695409297943115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 576: Training Loss: 1.670490066210429 Validation Loss: 1.8682258129119873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 577: Training Loss: 1.668649713198344 Validation Loss: 1.86700439453125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 578: Training Loss: 1.6673214832941692 Validation Loss: 1.8657047748565674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 579: Training Loss: 1.6653997898101807 Validation Loss: 1.8644722700119019\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 580: Training Loss: 1.662939190864563 Validation Loss: 1.8632481098175049\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 581: Training Loss: 1.6615811586380005 Validation Loss: 1.8617562055587769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 582: Training Loss: 1.6603238185246785 Validation Loss: 1.860416293144226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 583: Training Loss: 1.6593687136967976 Validation Loss: 1.85912024974823\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 584: Training Loss: 1.6582578420639038 Validation Loss: 1.8580363988876343\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 585: Training Loss: 1.6565786600112915 Validation Loss: 1.8567813634872437\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 586: Training Loss: 1.6547330220540364 Validation Loss: 1.8553292751312256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 587: Training Loss: 1.6526760657628377 Validation Loss: 1.8541805744171143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 588: Training Loss: 1.650421182314555 Validation Loss: 1.85298752784729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 589: Training Loss: 1.6485389073689778 Validation Loss: 1.8516911268234253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 590: Training Loss: 1.647746205329895 Validation Loss: 1.8505371809005737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 591: Training Loss: 1.6463345686594646 Validation Loss: 1.8490322828292847\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 592: Training Loss: 1.6443182229995728 Validation Loss: 1.8476439714431763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 593: Training Loss: 1.6433136065800984 Validation Loss: 1.8464089632034302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 594: Training Loss: 1.6411271492640178 Validation Loss: 1.8451529741287231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 595: Training Loss: 1.6405948003133137 Validation Loss: 1.8439842462539673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 596: Training Loss: 1.6378628810246785 Validation Loss: 1.8428465127944946\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 597: Training Loss: 1.637598713239034 Validation Loss: 1.8415504693984985\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 598: Training Loss: 1.6357276439666748 Validation Loss: 1.8406546115875244\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 599: Training Loss: 1.635947624842326 Validation Loss: 1.8391377925872803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 600: Training Loss: 1.633579691251119 Validation Loss: 1.837934136390686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 601: Training Loss: 1.6309020121892293 Validation Loss: 1.8368266820907593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 602: Training Loss: 1.6291869481404622 Validation Loss: 1.8354278802871704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 603: Training Loss: 1.627820571263631 Validation Loss: 1.8341038227081299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 604: Training Loss: 1.62682048479716 Validation Loss: 1.8326705694198608\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 605: Training Loss: 1.6248238881429036 Validation Loss: 1.831511378288269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 606: Training Loss: 1.6240232785542805 Validation Loss: 1.830445408821106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 607: Training Loss: 1.6218543847401936 Validation Loss: 1.829321026802063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 608: Training Loss: 1.6189209620157878 Validation Loss: 1.8276453018188477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 609: Training Loss: 1.6204415559768677 Validation Loss: 1.8263734579086304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 610: Training Loss: 1.617697834968567 Validation Loss: 1.8252222537994385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 611: Training Loss: 1.6155815919240315 Validation Loss: 1.8240349292755127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 612: Training Loss: 1.614249308904012 Validation Loss: 1.8229522705078125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 613: Training Loss: 1.61166516939799 Validation Loss: 1.8217805624008179\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 614: Training Loss: 1.6108696460723877 Validation Loss: 1.8206591606140137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 615: Training Loss: 1.6106199026107788 Validation Loss: 1.8191957473754883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 616: Training Loss: 1.6078206300735474 Validation Loss: 1.817908525466919\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 617: Training Loss: 1.6062829494476318 Validation Loss: 1.81675124168396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 618: Training Loss: 1.6052828232447307 Validation Loss: 1.8153897523880005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 619: Training Loss: 1.602477749188741 Validation Loss: 1.8137283325195312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 620: Training Loss: 1.6034311850865681 Validation Loss: 1.8125759363174438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 621: Training Loss: 1.6001387039820354 Validation Loss: 1.8114773035049438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 622: Training Loss: 1.5998161633809407 Validation Loss: 1.8103469610214233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 623: Training Loss: 1.5982505877812703 Validation Loss: 1.8092254400253296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 624: Training Loss: 1.595900336901347 Validation Loss: 1.8081300258636475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 625: Training Loss: 1.5924829244613647 Validation Loss: 1.806773066520691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 626: Training Loss: 1.5936459302902222 Validation Loss: 1.805747151374817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 627: Training Loss: 1.5910309553146362 Validation Loss: 1.8045552968978882\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 628: Training Loss: 1.5905893643697102 Validation Loss: 1.8031648397445679\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 629: Training Loss: 1.5879265467325847 Validation Loss: 1.8018325567245483\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 630: Training Loss: 1.58597465356191 Validation Loss: 1.8005108833312988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 631: Training Loss: 1.5855899254480998 Validation Loss: 1.799423336982727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 632: Training Loss: 1.5839866797129314 Validation Loss: 1.7981492280960083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 633: Training Loss: 1.583109458287557 Validation Loss: 1.7969034910202026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 634: Training Loss: 1.5807857116063435 Validation Loss: 1.7956377267837524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 635: Training Loss: 1.5795750220616658 Validation Loss: 1.79446542263031\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 636: Training Loss: 1.5778115193049114 Validation Loss: 1.7932344675064087\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 637: Training Loss: 1.5771499872207642 Validation Loss: 1.7921329736709595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 638: Training Loss: 1.575333038965861 Validation Loss: 1.7909122705459595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 639: Training Loss: 1.5737601518630981 Validation Loss: 1.7895228862762451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 640: Training Loss: 1.5721811056137085 Validation Loss: 1.7880420684814453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 641: Training Loss: 1.5693678855895996 Validation Loss: 1.7865374088287354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 642: Training Loss: 1.569257418314616 Validation Loss: 1.7854026556015015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 643: Training Loss: 1.5688813924789429 Validation Loss: 1.784244179725647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 644: Training Loss: 1.565065860748291 Validation Loss: 1.7830839157104492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 645: Training Loss: 1.5643103122711182 Validation Loss: 1.782170295715332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 646: Training Loss: 1.5622563362121582 Validation Loss: 1.7810858488082886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 647: Training Loss: 1.5618291298548381 Validation Loss: 1.7800037860870361\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 648: Training Loss: 1.55988605817159 Validation Loss: 1.7788852453231812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 649: Training Loss: 1.559215744336446 Validation Loss: 1.7774386405944824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 650: Training Loss: 1.5583237012227376 Validation Loss: 1.7760831117630005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 651: Training Loss: 1.55548894405365 Validation Loss: 1.7748315334320068\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 652: Training Loss: 1.5539186000823975 Validation Loss: 1.773819088935852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 653: Training Loss: 1.5518823862075806 Validation Loss: 1.7726407051086426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 654: Training Loss: 1.5512272119522095 Validation Loss: 1.7714139223098755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 655: Training Loss: 1.5473291873931885 Validation Loss: 1.7700740098953247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 656: Training Loss: 1.5474373896916707 Validation Loss: 1.7688202857971191\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 657: Training Loss: 1.546707034111023 Validation Loss: 1.767703890800476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 658: Training Loss: 1.545121431350708 Validation Loss: 1.7662317752838135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 659: Training Loss: 1.5442996422449748 Validation Loss: 1.7650121450424194\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 660: Training Loss: 1.541957934697469 Validation Loss: 1.7638163566589355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 661: Training Loss: 1.5416515270868938 Validation Loss: 1.7629234790802002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 662: Training Loss: 1.5391441583633423 Validation Loss: 1.7617220878601074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 663: Training Loss: 1.538131316502889 Validation Loss: 1.760601282119751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 664: Training Loss: 1.535969654719035 Validation Loss: 1.7593042850494385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 665: Training Loss: 1.5359385013580322 Validation Loss: 1.7580400705337524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 666: Training Loss: 1.5336947441101074 Validation Loss: 1.7569986581802368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 667: Training Loss: 1.5306023756663005 Validation Loss: 1.755861759185791\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 668: Training Loss: 1.5312196811040242 Validation Loss: 1.7545812129974365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 669: Training Loss: 1.528182586034139 Validation Loss: 1.7531780004501343\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 670: Training Loss: 1.5279972155888875 Validation Loss: 1.7519124746322632\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 671: Training Loss: 1.5255845785140991 Validation Loss: 1.7508138418197632\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 672: Training Loss: 1.524451494216919 Validation Loss: 1.7495781183242798\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 673: Training Loss: 1.5230806271235149 Validation Loss: 1.7484632730484009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 674: Training Loss: 1.5221821069717407 Validation Loss: 1.7471216917037964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 675: Training Loss: 1.5209279457728069 Validation Loss: 1.7460196018218994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 676: Training Loss: 1.5204323927561443 Validation Loss: 1.7448195219039917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 677: Training Loss: 1.5179125467936199 Validation Loss: 1.7435157299041748\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 678: Training Loss: 1.5162543058395386 Validation Loss: 1.742441177368164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 679: Training Loss: 1.5145383675893147 Validation Loss: 1.7413427829742432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 680: Training Loss: 1.5122924248377483 Validation Loss: 1.7399858236312866\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 681: Training Loss: 1.5122934182484944 Validation Loss: 1.7389591932296753\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 682: Training Loss: 1.5098930597305298 Validation Loss: 1.7377995252609253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 683: Training Loss: 1.5074316263198853 Validation Loss: 1.736954927444458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 684: Training Loss: 1.506731629371643 Validation Loss: 1.7357441186904907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 685: Training Loss: 1.5052495002746582 Validation Loss: 1.7345412969589233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 686: Training Loss: 1.502912958463033 Validation Loss: 1.733202338218689\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 687: Training Loss: 1.502286434173584 Validation Loss: 1.7318263053894043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 688: Training Loss: 1.503353516260783 Validation Loss: 1.7306164503097534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 689: Training Loss: 1.500263770421346 Validation Loss: 1.7295727729797363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 690: Training Loss: 1.4985116322835286 Validation Loss: 1.7284265756607056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 691: Training Loss: 1.4974204301834106 Validation Loss: 1.727517008781433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 692: Training Loss: 1.4962591727574666 Validation Loss: 1.7262910604476929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 693: Training Loss: 1.4945039351781209 Validation Loss: 1.724902629852295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 694: Training Loss: 1.4922797282536824 Validation Loss: 1.7236335277557373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 695: Training Loss: 1.4912007252375286 Validation Loss: 1.7222929000854492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 696: Training Loss: 1.490152359008789 Validation Loss: 1.721200942993164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 697: Training Loss: 1.48862620194753 Validation Loss: 1.7200506925582886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 698: Training Loss: 1.4871703386306763 Validation Loss: 1.7190243005752563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 699: Training Loss: 1.4857620000839233 Validation Loss: 1.717738151550293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 700: Training Loss: 1.4847033421198528 Validation Loss: 1.7167041301727295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 701: Training Loss: 1.482731779416402 Validation Loss: 1.715397834777832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 702: Training Loss: 1.4815773566563923 Validation Loss: 1.7143043279647827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 703: Training Loss: 1.4805608987808228 Validation Loss: 1.7130624055862427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 704: Training Loss: 1.4788754383722942 Validation Loss: 1.7117679119110107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 705: Training Loss: 1.4771989583969116 Validation Loss: 1.710526943206787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 706: Training Loss: 1.4761120478312175 Validation Loss: 1.7091692686080933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 707: Training Loss: 1.472157597541809 Validation Loss: 1.7084366083145142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 708: Training Loss: 1.4730298519134521 Validation Loss: 1.7076045274734497\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 709: Training Loss: 1.471449573834737 Validation Loss: 1.706656575202942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 710: Training Loss: 1.470382293065389 Validation Loss: 1.7054671049118042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 711: Training Loss: 1.469573179880778 Validation Loss: 1.704014778137207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 712: Training Loss: 1.4677141904830933 Validation Loss: 1.7027980089187622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 713: Training Loss: 1.4666632413864136 Validation Loss: 1.7015358209609985\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 714: Training Loss: 1.4646073977152507 Validation Loss: 1.7002218961715698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 715: Training Loss: 1.4648075501124065 Validation Loss: 1.6992602348327637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 716: Training Loss: 1.4622379938761394 Validation Loss: 1.6983206272125244\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 717: Training Loss: 1.4609163204828899 Validation Loss: 1.6971884965896606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 718: Training Loss: 1.4596249262491863 Validation Loss: 1.69582200050354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 719: Training Loss: 1.4571201801300049 Validation Loss: 1.6944537162780762\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 720: Training Loss: 1.4563775459925334 Validation Loss: 1.693472981452942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 721: Training Loss: 1.4555178085962932 Validation Loss: 1.6925013065338135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 722: Training Loss: 1.4541374444961548 Validation Loss: 1.6915823221206665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 723: Training Loss: 1.4524723291397095 Validation Loss: 1.6902518272399902\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 724: Training Loss: 1.4505879878997803 Validation Loss: 1.6889082193374634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 725: Training Loss: 1.4496991634368896 Validation Loss: 1.6877521276474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 726: Training Loss: 1.4472775061925252 Validation Loss: 1.6866871118545532\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 727: Training Loss: 1.4481863975524902 Validation Loss: 1.6857181787490845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 728: Training Loss: 1.445573131243388 Validation Loss: 1.6845134496688843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 729: Training Loss: 1.4439611434936523 Validation Loss: 1.6834999322891235\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 730: Training Loss: 1.4417386054992676 Validation Loss: 1.682305932044983\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 731: Training Loss: 1.440978209177653 Validation Loss: 1.6813814640045166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 732: Training Loss: 1.4400173425674438 Validation Loss: 1.6799986362457275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 733: Training Loss: 1.4396655956904094 Validation Loss: 1.6787745952606201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 734: Training Loss: 1.4374627669652302 Validation Loss: 1.6775097846984863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 735: Training Loss: 1.4355120658874512 Validation Loss: 1.676465392112732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 736: Training Loss: 1.4340058167775471 Validation Loss: 1.675455093383789\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 737: Training Loss: 1.433574636777242 Validation Loss: 1.6744152307510376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 738: Training Loss: 1.4314812024434407 Validation Loss: 1.6734975576400757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 739: Training Loss: 1.429830551147461 Validation Loss: 1.6723512411117554\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 740: Training Loss: 1.4287246068318684 Validation Loss: 1.6711066961288452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 741: Training Loss: 1.4268544912338257 Validation Loss: 1.6698129177093506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 742: Training Loss: 1.4263319571812947 Validation Loss: 1.6685669422149658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 743: Training Loss: 1.4254467884699504 Validation Loss: 1.6674940586090088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 744: Training Loss: 1.4242627223332722 Validation Loss: 1.666621446609497\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 745: Training Loss: 1.4220900535583496 Validation Loss: 1.6653552055358887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 746: Training Loss: 1.4225938717524211 Validation Loss: 1.6641641855239868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 747: Training Loss: 1.4190610647201538 Validation Loss: 1.6632704734802246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 748: Training Loss: 1.4178969462712605 Validation Loss: 1.6623880863189697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 749: Training Loss: 1.4169564247131348 Validation Loss: 1.6613317728042603\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 750: Training Loss: 1.417022744814555 Validation Loss: 1.6605786085128784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 751: Training Loss: 1.4143259127934773 Validation Loss: 1.6589792966842651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 752: Training Loss: 1.4120575189590454 Validation Loss: 1.6575473546981812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 753: Training Loss: 1.4114738702774048 Validation Loss: 1.6564021110534668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 754: Training Loss: 1.4103411038716633 Validation Loss: 1.6553823947906494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 755: Training Loss: 1.4087905486424763 Validation Loss: 1.6545147895812988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 756: Training Loss: 1.4056343237559001 Validation Loss: 1.653555989265442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 757: Training Loss: 1.407042105992635 Validation Loss: 1.6524244546890259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 758: Training Loss: 1.4043840567270915 Validation Loss: 1.6514219045639038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 759: Training Loss: 1.4035007953643799 Validation Loss: 1.6499583721160889\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 760: Training Loss: 1.4026329517364502 Validation Loss: 1.6487441062927246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 761: Training Loss: 1.4013357957204182 Validation Loss: 1.647679090499878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 762: Training Loss: 1.4003703991572063 Validation Loss: 1.6465078592300415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 763: Training Loss: 1.3982892036437988 Validation Loss: 1.6455140113830566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 764: Training Loss: 1.3966941436131795 Validation Loss: 1.6444910764694214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 765: Training Loss: 1.3961726427078247 Validation Loss: 1.6435930728912354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 766: Training Loss: 1.3950714270273845 Validation Loss: 1.642615556716919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 767: Training Loss: 1.3924925724665325 Validation Loss: 1.6411536931991577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 768: Training Loss: 1.391922950744629 Validation Loss: 1.6401947736740112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 769: Training Loss: 1.3901323080062866 Validation Loss: 1.639315128326416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 770: Training Loss: 1.388685981432597 Validation Loss: 1.6380484104156494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 771: Training Loss: 1.3872400919596355 Validation Loss: 1.6367406845092773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 772: Training Loss: 1.3858384291330974 Validation Loss: 1.6359875202178955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 773: Training Loss: 1.3844366073608398 Validation Loss: 1.6347339153289795\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 774: Training Loss: 1.3837064107259114 Validation Loss: 1.6337864398956299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 775: Training Loss: 1.3800902764002483 Validation Loss: 1.6327755451202393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 776: Training Loss: 1.3811931610107422 Validation Loss: 1.6318904161453247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 777: Training Loss: 1.3809396425882976 Validation Loss: 1.6307013034820557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 778: Training Loss: 1.379209081331889 Validation Loss: 1.6297082901000977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 779: Training Loss: 1.378788908322652 Validation Loss: 1.6286438703536987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 780: Training Loss: 1.375354290008545 Validation Loss: 1.6271111965179443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 781: Training Loss: 1.3753610452016194 Validation Loss: 1.625784158706665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 782: Training Loss: 1.3734493652979534 Validation Loss: 1.6247050762176514\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 783: Training Loss: 1.372916539510091 Validation Loss: 1.6236906051635742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 784: Training Loss: 1.3706785440444946 Validation Loss: 1.6229323148727417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 785: Training Loss: 1.3698185682296753 Validation Loss: 1.6217302083969116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 786: Training Loss: 1.369482119878133 Validation Loss: 1.620802402496338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 787: Training Loss: 1.3662283420562744 Validation Loss: 1.6199610233306885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 788: Training Loss: 1.3663569688796997 Validation Loss: 1.6190907955169678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 789: Training Loss: 1.3650702635447185 Validation Loss: 1.6178849935531616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 790: Training Loss: 1.3629796107610066 Validation Loss: 1.6167980432510376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 791: Training Loss: 1.3623273770014446 Validation Loss: 1.6158050298690796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 792: Training Loss: 1.364184816678365 Validation Loss: 1.6147063970565796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 793: Training Loss: 1.3590684334437053 Validation Loss: 1.613600730895996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 794: Training Loss: 1.3587534030278523 Validation Loss: 1.6122792959213257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 795: Training Loss: 1.3565514882405598 Validation Loss: 1.6109373569488525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 796: Training Loss: 1.3546920617421467 Validation Loss: 1.609976053237915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 797: Training Loss: 1.3539393345514934 Validation Loss: 1.6092348098754883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 798: Training Loss: 1.3521135648091633 Validation Loss: 1.6082502603530884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 799: Training Loss: 1.3515664339065552 Validation Loss: 1.6072056293487549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 800: Training Loss: 1.3515240748723347 Validation Loss: 1.6058449745178223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 801: Training Loss: 1.348005731900533 Validation Loss: 1.605130910873413\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 802: Training Loss: 1.3474218845367432 Validation Loss: 1.603919506072998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 803: Training Loss: 1.3472137848536174 Validation Loss: 1.6030256748199463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 804: Training Loss: 1.3452632427215576 Validation Loss: 1.6018531322479248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 805: Training Loss: 1.3428267240524292 Validation Loss: 1.6008256673812866\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 806: Training Loss: 1.3435618082682292 Validation Loss: 1.5998859405517578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 807: Training Loss: 1.342547337214152 Validation Loss: 1.5989675521850586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 808: Training Loss: 1.3389434019724529 Validation Loss: 1.5984735488891602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 809: Training Loss: 1.338849663734436 Validation Loss: 1.5970951318740845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 810: Training Loss: 1.3397655487060547 Validation Loss: 1.5958210229873657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 811: Training Loss: 1.337407112121582 Validation Loss: 1.595191478729248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 812: Training Loss: 1.3354808886845906 Validation Loss: 1.5939874649047852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 813: Training Loss: 1.3342526356379192 Validation Loss: 1.5926032066345215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 814: Training Loss: 1.3330464760462444 Validation Loss: 1.591230034828186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 815: Training Loss: 1.3315261602401733 Validation Loss: 1.5902808904647827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 816: Training Loss: 1.3296814759572346 Validation Loss: 1.5893776416778564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 817: Training Loss: 1.3289472659428914 Validation Loss: 1.5884343385696411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 818: Training Loss: 1.3285047610600789 Validation Loss: 1.5876692533493042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 819: Training Loss: 1.3264659245808919 Validation Loss: 1.5867934226989746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 820: Training Loss: 1.324442187945048 Validation Loss: 1.5855262279510498\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 821: Training Loss: 1.3261757294336955 Validation Loss: 1.5846151113510132\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 822: Training Loss: 1.3234627644220989 Validation Loss: 1.5834721326828003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 823: Training Loss: 1.3212072451909382 Validation Loss: 1.5825227499008179\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 824: Training Loss: 1.3202344973882039 Validation Loss: 1.5812925100326538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 825: Training Loss: 1.3185564279556274 Validation Loss: 1.5806605815887451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 826: Training Loss: 1.318244496981303 Validation Loss: 1.5796717405319214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 827: Training Loss: 1.317645271619161 Validation Loss: 1.5786855220794678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 828: Training Loss: 1.3162970940272014 Validation Loss: 1.5773440599441528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 829: Training Loss: 1.3138041098912556 Validation Loss: 1.57631254196167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 830: Training Loss: 1.3129756848017375 Validation Loss: 1.574988603591919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 831: Training Loss: 1.313830852508545 Validation Loss: 1.573811411857605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 832: Training Loss: 1.3109335501988728 Validation Loss: 1.5731648206710815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 833: Training Loss: 1.3092292149861653 Validation Loss: 1.572477102279663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 834: Training Loss: 1.3087350130081177 Validation Loss: 1.5717154741287231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 835: Training Loss: 1.307096004486084 Validation Loss: 1.5705187320709229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 836: Training Loss: 1.3050072590510051 Validation Loss: 1.569643497467041\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 837: Training Loss: 1.304736812909444 Validation Loss: 1.5686044692993164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 838: Training Loss: 1.302579681078593 Validation Loss: 1.567581295967102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 839: Training Loss: 1.3037786483764648 Validation Loss: 1.5662891864776611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 840: Training Loss: 1.300329367319743 Validation Loss: 1.5655096769332886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 841: Training Loss: 1.296520749727885 Validation Loss: 1.5644214153289795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 842: Training Loss: 1.2994894981384277 Validation Loss: 1.5634918212890625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 843: Training Loss: 1.2970417737960815 Validation Loss: 1.562567114830017\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 844: Training Loss: 1.2963701883951824 Validation Loss: 1.56108558177948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 845: Training Loss: 1.2946148316065471 Validation Loss: 1.5599838495254517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 846: Training Loss: 1.294420599937439 Validation Loss: 1.558912754058838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 847: Training Loss: 1.2936828931172688 Validation Loss: 1.5580583810806274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 848: Training Loss: 1.2907987038294475 Validation Loss: 1.5573047399520874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 849: Training Loss: 1.290071964263916 Validation Loss: 1.5566338300704956\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 850: Training Loss: 1.2898656924565632 Validation Loss: 1.5557231903076172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 851: Training Loss: 1.2869032621383667 Validation Loss: 1.5547829866409302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 852: Training Loss: 1.285022536913554 Validation Loss: 1.5535821914672852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 853: Training Loss: 1.2838460604349773 Validation Loss: 1.552833080291748\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 854: Training Loss: 1.2845370372136433 Validation Loss: 1.5517547130584717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 855: Training Loss: 1.2823102076848347 Validation Loss: 1.5506935119628906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 856: Training Loss: 1.2813597917556763 Validation Loss: 1.5494877099990845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 857: Training Loss: 1.2825090090433757 Validation Loss: 1.5483237504959106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 858: Training Loss: 1.2792274157206218 Validation Loss: 1.547581434249878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 859: Training Loss: 1.2780309518178303 Validation Loss: 1.5468336343765259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 860: Training Loss: 1.276858647664388 Validation Loss: 1.545677900314331\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 861: Training Loss: 1.275865077972412 Validation Loss: 1.544335961341858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 862: Training Loss: 1.274701992670695 Validation Loss: 1.5436745882034302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 863: Training Loss: 1.2733185688654582 Validation Loss: 1.5426448583602905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 864: Training Loss: 1.272618571917216 Validation Loss: 1.541898250579834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 865: Training Loss: 1.2721611658732097 Validation Loss: 1.5406746864318848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 866: Training Loss: 1.2696071863174438 Validation Loss: 1.5398303270339966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 867: Training Loss: 1.2697800397872925 Validation Loss: 1.5390551090240479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 868: Training Loss: 1.2680268287658691 Validation Loss: 1.5381512641906738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 869: Training Loss: 1.2660574913024902 Validation Loss: 1.5370882749557495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 870: Training Loss: 1.265239993731181 Validation Loss: 1.536131501197815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 871: Training Loss: 1.264056921005249 Validation Loss: 1.5351688861846924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 872: Training Loss: 1.261038859685262 Validation Loss: 1.5339118242263794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 873: Training Loss: 1.261466145515442 Validation Loss: 1.533112645149231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 874: Training Loss: 1.260972261428833 Validation Loss: 1.532346487045288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 875: Training Loss: 1.2580069303512573 Validation Loss: 1.531258225440979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 876: Training Loss: 1.258107860883077 Validation Loss: 1.5302586555480957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 877: Training Loss: 1.256951928138733 Validation Loss: 1.529187798500061\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 878: Training Loss: 1.255720814069112 Validation Loss: 1.5281766653060913\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 879: Training Loss: 1.25434414545695 Validation Loss: 1.5275455713272095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 880: Training Loss: 1.2537883122762044 Validation Loss: 1.5265796184539795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 881: Training Loss: 1.2525824308395386 Validation Loss: 1.5255687236785889\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 882: Training Loss: 1.2522532145182292 Validation Loss: 1.5244921445846558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 883: Training Loss: 1.2496311664581299 Validation Loss: 1.523479700088501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 884: Training Loss: 1.249876578648885 Validation Loss: 1.5228604078292847\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 885: Training Loss: 1.2482244968414307 Validation Loss: 1.5222246646881104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 886: Training Loss: 1.245567758878072 Validation Loss: 1.5210500955581665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 887: Training Loss: 1.245883544286092 Validation Loss: 1.5199791193008423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 888: Training Loss: 1.2466482718785603 Validation Loss: 1.5192347764968872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 889: Training Loss: 1.2436140378316243 Validation Loss: 1.5183124542236328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 890: Training Loss: 1.2429355382919312 Validation Loss: 1.517149806022644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 891: Training Loss: 1.2422997156778972 Validation Loss: 1.5165619850158691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 892: Training Loss: 1.240296721458435 Validation Loss: 1.51579749584198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 893: Training Loss: 1.2375153303146362 Validation Loss: 1.5145058631896973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 894: Training Loss: 1.2370123863220215 Validation Loss: 1.513468861579895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 895: Training Loss: 1.237602432568868 Validation Loss: 1.5122238397598267\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 896: Training Loss: 1.2353458801905315 Validation Loss: 1.5112885236740112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 897: Training Loss: 1.2344797054926555 Validation Loss: 1.5104516744613647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 898: Training Loss: 1.2329370578130086 Validation Loss: 1.509932041168213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 899: Training Loss: 1.2320140997568767 Validation Loss: 1.509008526802063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 900: Training Loss: 1.2314860026041667 Validation Loss: 1.5077131986618042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 901: Training Loss: 1.230239709218343 Validation Loss: 1.506945252418518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 902: Training Loss: 1.2289127906163533 Validation Loss: 1.5060501098632812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 903: Training Loss: 1.227488915125529 Validation Loss: 1.5051236152648926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 904: Training Loss: 1.2264894644419353 Validation Loss: 1.504217267036438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 905: Training Loss: 1.2253858248392742 Validation Loss: 1.5032126903533936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 906: Training Loss: 1.224820892016093 Validation Loss: 1.5024386644363403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 907: Training Loss: 1.2251269419987996 Validation Loss: 1.5015552043914795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 908: Training Loss: 1.221813718477885 Validation Loss: 1.5006229877471924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 909: Training Loss: 1.2212292750676472 Validation Loss: 1.4997731447219849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 910: Training Loss: 1.2188963492711384 Validation Loss: 1.4986486434936523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 911: Training Loss: 1.2186822891235352 Validation Loss: 1.49761164188385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 912: Training Loss: 1.2170192797978718 Validation Loss: 1.4969700574874878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 913: Training Loss: 1.2169302304585774 Validation Loss: 1.4962118864059448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 914: Training Loss: 1.2156272729237874 Validation Loss: 1.4952291250228882\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 915: Training Loss: 1.214243729909261 Validation Loss: 1.4942550659179688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 916: Training Loss: 1.2144307692845662 Validation Loss: 1.4932363033294678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 917: Training Loss: 1.2124911546707153 Validation Loss: 1.4923577308654785\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 918: Training Loss: 1.2122892141342163 Validation Loss: 1.4912015199661255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 919: Training Loss: 1.2101388374964397 Validation Loss: 1.4903780221939087\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 920: Training Loss: 1.211625337600708 Validation Loss: 1.489430546760559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 921: Training Loss: 1.2066506544748943 Validation Loss: 1.4887069463729858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 922: Training Loss: 1.206595500310262 Validation Loss: 1.4879897832870483\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 923: Training Loss: 1.2059718370437622 Validation Loss: 1.4872393608093262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 924: Training Loss: 1.2045348087946575 Validation Loss: 1.486567735671997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 925: Training Loss: 1.204559286435445 Validation Loss: 1.4855923652648926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 926: Training Loss: 1.2022331555684407 Validation Loss: 1.4843257665634155\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 927: Training Loss: 1.2021830479303997 Validation Loss: 1.483408808708191\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 928: Training Loss: 1.199442744255066 Validation Loss: 1.4824069738388062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 929: Training Loss: 1.1993769804636638 Validation Loss: 1.4815605878829956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 930: Training Loss: 1.1980470816294353 Validation Loss: 1.480635643005371\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 931: Training Loss: 1.1966731945673625 Validation Loss: 1.4797979593276978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 932: Training Loss: 1.1957839727401733 Validation Loss: 1.4786953926086426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 933: Training Loss: 1.194606900215149 Validation Loss: 1.477798342704773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 934: Training Loss: 1.1937673091888428 Validation Loss: 1.4772109985351562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 935: Training Loss: 1.1923163731892903 Validation Loss: 1.4763894081115723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 936: Training Loss: 1.1915703614552815 Validation Loss: 1.475329041481018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 937: Training Loss: 1.1909640630086262 Validation Loss: 1.4747288227081299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 938: Training Loss: 1.1894986629486084 Validation Loss: 1.4737498760223389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 939: Training Loss: 1.1877118349075317 Validation Loss: 1.4728871583938599\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 940: Training Loss: 1.1883209546407063 Validation Loss: 1.4719891548156738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 941: Training Loss: 1.1862115065256755 Validation Loss: 1.4711947441101074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 942: Training Loss: 1.1838497718175252 Validation Loss: 1.470413088798523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 943: Training Loss: 1.18398384253184 Validation Loss: 1.4696053266525269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 944: Training Loss: 1.1836962302525837 Validation Loss: 1.468768835067749\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 945: Training Loss: 1.182173490524292 Validation Loss: 1.4674314260482788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 946: Training Loss: 1.1810218890508015 Validation Loss: 1.4665769338607788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 947: Training Loss: 1.1797486543655396 Validation Loss: 1.4657083749771118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 948: Training Loss: 1.1793640454610188 Validation Loss: 1.4649007320404053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 949: Training Loss: 1.1774590412775676 Validation Loss: 1.464296579360962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 950: Training Loss: 1.1771049896876018 Validation Loss: 1.4633944034576416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 951: Training Loss: 1.1767891645431519 Validation Loss: 1.4624863862991333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 952: Training Loss: 1.1744536558787029 Validation Loss: 1.4615486860275269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 953: Training Loss: 1.173966884613037 Validation Loss: 1.4607338905334473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 954: Training Loss: 1.172896146774292 Validation Loss: 1.4596672058105469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 955: Training Loss: 1.1715832948684692 Validation Loss: 1.4587794542312622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 956: Training Loss: 1.1714831193288167 Validation Loss: 1.4580509662628174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 957: Training Loss: 1.1691062053044636 Validation Loss: 1.4571778774261475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 958: Training Loss: 1.1687217553456624 Validation Loss: 1.4564244747161865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 959: Training Loss: 1.1665307680765789 Validation Loss: 1.4555718898773193\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 960: Training Loss: 1.1682648261388142 Validation Loss: 1.4545589685440063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 961: Training Loss: 1.1652212142944336 Validation Loss: 1.4535795450210571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 962: Training Loss: 1.1648223400115967 Validation Loss: 1.4529836177825928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 963: Training Loss: 1.1630105177561443 Validation Loss: 1.4519376754760742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 964: Training Loss: 1.161457896232605 Validation Loss: 1.4512628316879272\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 965: Training Loss: 1.160554051399231 Validation Loss: 1.4504474401474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 966: Training Loss: 1.1601381301879883 Validation Loss: 1.4496735334396362\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 967: Training Loss: 1.159222960472107 Validation Loss: 1.449141025543213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 968: Training Loss: 1.1583200295766194 Validation Loss: 1.4482752084732056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 969: Training Loss: 1.157057523727417 Validation Loss: 1.4472975730895996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 970: Training Loss: 1.1556340058644612 Validation Loss: 1.446394443511963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 971: Training Loss: 1.154699683189392 Validation Loss: 1.445549726486206\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 972: Training Loss: 1.1538280646006267 Validation Loss: 1.444551944732666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 973: Training Loss: 1.1516809463500977 Validation Loss: 1.4438855648040771\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 974: Training Loss: 1.1526087919871013 Validation Loss: 1.443077802658081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 975: Training Loss: 1.1524712244669597 Validation Loss: 1.4421809911727905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 976: Training Loss: 1.1513408422470093 Validation Loss: 1.4411991834640503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 977: Training Loss: 1.1493609348932903 Validation Loss: 1.440294623374939\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 978: Training Loss: 1.1485449075698853 Validation Loss: 1.439379334449768\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 979: Training Loss: 1.1498841047286987 Validation Loss: 1.4386464357376099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 980: Training Loss: 1.1467976172765095 Validation Loss: 1.4378372430801392\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 981: Training Loss: 1.1457267204920452 Validation Loss: 1.4369027614593506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 982: Training Loss: 1.1419918139775593 Validation Loss: 1.4361765384674072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 983: Training Loss: 1.1448577642440796 Validation Loss: 1.4352350234985352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 984: Training Loss: 1.1420783201853435 Validation Loss: 1.4345242977142334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 985: Training Loss: 1.1405520836512248 Validation Loss: 1.4339103698730469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 986: Training Loss: 1.139722228050232 Validation Loss: 1.433132529258728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 987: Training Loss: 1.1389827330907185 Validation Loss: 1.43240487575531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 988: Training Loss: 1.1381810903549194 Validation Loss: 1.431601881980896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 989: Training Loss: 1.1366486152013142 Validation Loss: 1.4307178258895874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 990: Training Loss: 1.1351289351781209 Validation Loss: 1.4297711849212646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 991: Training Loss: 1.1354405482610066 Validation Loss: 1.4288668632507324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 992: Training Loss: 1.135131041208903 Validation Loss: 1.427924633026123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 993: Training Loss: 1.1328253746032715 Validation Loss: 1.4270738363265991\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 994: Training Loss: 1.1318230628967285 Validation Loss: 1.4266091585159302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 995: Training Loss: 1.131383975346883 Validation Loss: 1.425985336303711\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 996: Training Loss: 1.13003937403361 Validation Loss: 1.4252759218215942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 997: Training Loss: 1.1289042234420776 Validation Loss: 1.4243632555007935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 998: Training Loss: 1.1277774572372437 Validation Loss: 1.4234861135482788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 999: Training Loss: 1.1272437175114949 Validation Loss: 1.4225455522537231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 1000: Training Loss: 1.1260402997334797 Validation Loss: 1.421531319618225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1001: Training Loss: 1.124565561612447 Validation Loss: 1.4205493927001953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1002: Training Loss: 1.1261845032374065 Validation Loss: 1.4198176860809326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1003: Training Loss: 1.1237794558207195 Validation Loss: 1.41909658908844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1004: Training Loss: 1.1262895663579304 Validation Loss: 1.4181047677993774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1005: Training Loss: 1.123844305674235 Validation Loss: 1.4173961877822876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1006: Training Loss: 1.121416171391805 Validation Loss: 1.4168527126312256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1007: Training Loss: 1.1191614866256714 Validation Loss: 1.4160197973251343\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1008: Training Loss: 1.117954134941101 Validation Loss: 1.4154529571533203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1009: Training Loss: 1.1175365845362346 Validation Loss: 1.4146021604537964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1010: Training Loss: 1.117030660311381 Validation Loss: 1.4140005111694336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1011: Training Loss: 1.115021030108134 Validation Loss: 1.4131466150283813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1012: Training Loss: 1.114679177602132 Validation Loss: 1.4121427536010742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1013: Training Loss: 1.11407474676768 Validation Loss: 1.4112201929092407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1014: Training Loss: 1.111955205599467 Validation Loss: 1.4103329181671143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1015: Training Loss: 1.1119816700617473 Validation Loss: 1.4097007513046265\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1016: Training Loss: 1.110534389813741 Validation Loss: 1.4086027145385742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1017: Training Loss: 1.1094996531804402 Validation Loss: 1.4079331159591675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1018: Training Loss: 1.1088056166966755 Validation Loss: 1.407127857208252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1019: Training Loss: 1.1074068546295166 Validation Loss: 1.406566858291626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1020: Training Loss: 1.1070812145868938 Validation Loss: 1.405782699584961\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1021: Training Loss: 1.1056830088297527 Validation Loss: 1.4050875902175903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1022: Training Loss: 1.1052396694819133 Validation Loss: 1.404266119003296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1023: Training Loss: 1.1036312580108643 Validation Loss: 1.4033012390136719\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1024: Training Loss: 1.1031017700831096 Validation Loss: 1.4022949934005737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1025: Training Loss: 1.1036696036656697 Validation Loss: 1.4016201496124268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1026: Training Loss: 1.0990486939748128 Validation Loss: 1.4010072946548462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1027: Training Loss: 1.1012182633082073 Validation Loss: 1.4001446962356567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1028: Training Loss: 1.0976088444391887 Validation Loss: 1.3992046117782593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1029: Training Loss: 1.0971681674321492 Validation Loss: 1.398415207862854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1030: Training Loss: 1.0988364219665527 Validation Loss: 1.3977158069610596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1031: Training Loss: 1.0963265498479207 Validation Loss: 1.3971015214920044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1032: Training Loss: 1.095897873242696 Validation Loss: 1.3964953422546387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1033: Training Loss: 1.094663103421529 Validation Loss: 1.3958914279937744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1034: Training Loss: 1.0940277973810832 Validation Loss: 1.394944429397583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1035: Training Loss: 1.0924493471781414 Validation Loss: 1.3941946029663086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1036: Training Loss: 1.0923852523167927 Validation Loss: 1.3931974172592163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1037: Training Loss: 1.091090162595113 Validation Loss: 1.3923475742340088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1038: Training Loss: 1.0910449822743733 Validation Loss: 1.3916336297988892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1039: Training Loss: 1.0895986557006836 Validation Loss: 1.3908357620239258\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1040: Training Loss: 1.0876304705937703 Validation Loss: 1.3899319171905518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1041: Training Loss: 1.086918830871582 Validation Loss: 1.3891488313674927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1042: Training Loss: 1.0867114464441936 Validation Loss: 1.3887637853622437\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1043: Training Loss: 1.0857830444971721 Validation Loss: 1.3877863883972168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1044: Training Loss: 1.0855943361918132 Validation Loss: 1.386918544769287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1045: Training Loss: 1.083294113477071 Validation Loss: 1.3861979246139526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1046: Training Loss: 1.083199421564738 Validation Loss: 1.3857274055480957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1047: Training Loss: 1.0817190210024517 Validation Loss: 1.3846564292907715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1048: Training Loss: 1.0800252358118694 Validation Loss: 1.3839839696884155\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1049: Training Loss: 1.07755442460378 Validation Loss: 1.383266568183899\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1050: Training Loss: 1.0773663520812988 Validation Loss: 1.3825680017471313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1051: Training Loss: 1.0782156785329182 Validation Loss: 1.3816099166870117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1052: Training Loss: 1.0766252279281616 Validation Loss: 1.3810025453567505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1053: Training Loss: 1.0770198504130046 Validation Loss: 1.3801602125167847\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1054: Training Loss: 1.0752017895380657 Validation Loss: 1.3792399168014526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1055: Training Loss: 1.0742891629536946 Validation Loss: 1.3786505460739136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1056: Training Loss: 1.0729340314865112 Validation Loss: 1.377919316291809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1057: Training Loss: 1.0760776201883953 Validation Loss: 1.377109408378601\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1058: Training Loss: 1.0707716941833496 Validation Loss: 1.3765605688095093\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1059: Training Loss: 1.0728950301806133 Validation Loss: 1.3758878707885742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1060: Training Loss: 1.0696262121200562 Validation Loss: 1.3748317956924438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1061: Training Loss: 1.0682399670283 Validation Loss: 1.3739668130874634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1062: Training Loss: 1.067020098368327 Validation Loss: 1.3734073638916016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1063: Training Loss: 1.067065914471944 Validation Loss: 1.3726387023925781\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1064: Training Loss: 1.069433828194936 Validation Loss: 1.3716784715652466\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1065: Training Loss: 1.064662257830302 Validation Loss: 1.3713974952697754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1066: Training Loss: 1.064408540725708 Validation Loss: 1.3703233003616333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1067: Training Loss: 1.0641648769378662 Validation Loss: 1.3694058656692505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1068: Training Loss: 1.062615950902303 Validation Loss: 1.3689442873001099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1069: Training Loss: 1.063558836778005 Validation Loss: 1.3681623935699463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1070: Training Loss: 1.0603948036829631 Validation Loss: 1.3678046464920044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1071: Training Loss: 1.0598748127619426 Validation Loss: 1.3674595355987549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1072: Training Loss: 1.057530442873637 Validation Loss: 1.3663403987884521\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1073: Training Loss: 1.0566060940424602 Validation Loss: 1.3654109239578247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1074: Training Loss: 1.0573570330937703 Validation Loss: 1.3641470670700073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1075: Training Loss: 1.0557358264923096 Validation Loss: 1.3635343313217163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1076: Training Loss: 1.0553836822509766 Validation Loss: 1.3630242347717285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1077: Training Loss: 1.05482812722524 Validation Loss: 1.3624722957611084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1078: Training Loss: 1.0535443623860676 Validation Loss: 1.3619815111160278\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1079: Training Loss: 1.051682452360789 Validation Loss: 1.3609883785247803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1080: Training Loss: 1.052065372467041 Validation Loss: 1.3603862524032593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1081: Training Loss: 1.0496578613917034 Validation Loss: 1.3595584630966187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1082: Training Loss: 1.0507673422495525 Validation Loss: 1.3587055206298828\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1083: Training Loss: 1.0484864314397175 Validation Loss: 1.3580290079116821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1084: Training Loss: 1.048738678296407 Validation Loss: 1.3569139242172241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1085: Training Loss: 1.0478405952453613 Validation Loss: 1.3562098741531372\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1086: Training Loss: 1.0466028849283855 Validation Loss: 1.3555376529693604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1087: Training Loss: 1.0467269817988079 Validation Loss: 1.354987382888794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1088: Training Loss: 1.044721523920695 Validation Loss: 1.3543422222137451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1089: Training Loss: 1.0427927176157634 Validation Loss: 1.3535517454147339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1090: Training Loss: 1.0443514188130696 Validation Loss: 1.3530887365341187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1091: Training Loss: 1.0410881439844768 Validation Loss: 1.3523774147033691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1092: Training Loss: 1.0414451360702515 Validation Loss: 1.3517998456954956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1093: Training Loss: 1.039883812268575 Validation Loss: 1.3511608839035034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1094: Training Loss: 1.0402249495188396 Validation Loss: 1.350630760192871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1095: Training Loss: 1.0379573504130046 Validation Loss: 1.3499709367752075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1096: Training Loss: 1.0376631418863933 Validation Loss: 1.3490116596221924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1097: Training Loss: 1.0379788875579834 Validation Loss: 1.3480862379074097\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1098: Training Loss: 1.0361303488413494 Validation Loss: 1.3471081256866455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1099: Training Loss: 1.0342878699302673 Validation Loss: 1.3466378450393677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1100: Training Loss: 1.033922255039215 Validation Loss: 1.3455489873886108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1101: Training Loss: 1.0350996851921082 Validation Loss: 1.3450168371200562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1102: Training Loss: 1.0330790678660076 Validation Loss: 1.3443983793258667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1103: Training Loss: 1.033102552096049 Validation Loss: 1.3436585664749146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1104: Training Loss: 1.0309908787409465 Validation Loss: 1.3426920175552368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1105: Training Loss: 1.0299582282702129 Validation Loss: 1.342024564743042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1106: Training Loss: 1.0299508770306904 Validation Loss: 1.3413336277008057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1107: Training Loss: 1.0292439659436543 Validation Loss: 1.3406398296356201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1108: Training Loss: 1.0273913343747456 Validation Loss: 1.3400485515594482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1109: Training Loss: 1.0276122490564983 Validation Loss: 1.3395814895629883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1110: Training Loss: 1.0272124807039897 Validation Loss: 1.3392499685287476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1111: Training Loss: 1.0245513916015625 Validation Loss: 1.3384445905685425\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1112: Training Loss: 1.0271272659301758 Validation Loss: 1.3374361991882324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1113: Training Loss: 1.0265609224637349 Validation Loss: 1.3366577625274658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1114: Training Loss: 1.0226465463638306 Validation Loss: 1.3358564376831055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1115: Training Loss: 1.0213670134544373 Validation Loss: 1.335214614868164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1116: Training Loss: 1.0216586391131084 Validation Loss: 1.334245204925537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1117: Training Loss: 1.020094911257426 Validation Loss: 1.3335380554199219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1118: Training Loss: 1.0195454756418865 Validation Loss: 1.3330096006393433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1119: Training Loss: 1.018201192220052 Validation Loss: 1.3324891328811646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1120: Training Loss: 1.0179484883944194 Validation Loss: 1.3315932750701904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1121: Training Loss: 1.0168219010035198 Validation Loss: 1.3308954238891602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1122: Training Loss: 1.0154484510421753 Validation Loss: 1.3301172256469727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1123: Training Loss: 1.0152783393859863 Validation Loss: 1.3295079469680786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1124: Training Loss: 1.0138572057088215 Validation Loss: 1.3292248249053955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1125: Training Loss: 1.0135184129079182 Validation Loss: 1.3285773992538452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1126: Training Loss: 1.0126347343126934 Validation Loss: 1.3279612064361572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1127: Training Loss: 1.0132220387458801 Validation Loss: 1.3273440599441528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1128: Training Loss: 1.0099988182385762 Validation Loss: 1.3266295194625854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1129: Training Loss: 1.0105289419492085 Validation Loss: 1.325757384300232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1130: Training Loss: 1.0104002753893535 Validation Loss: 1.3246248960494995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1131: Training Loss: 1.0095276435216267 Validation Loss: 1.3237395286560059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1132: Training Loss: 1.007706383864085 Validation Loss: 1.3231632709503174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1133: Training Loss: 1.0073395768801372 Validation Loss: 1.3222769498825073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1134: Training Loss: 1.0059633255004883 Validation Loss: 1.321716547012329\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1135: Training Loss: 1.0075725317001343 Validation Loss: 1.3214906454086304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1136: Training Loss: 1.0041072964668274 Validation Loss: 1.3209278583526611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1137: Training Loss: 1.0039955973625183 Validation Loss: 1.3202577829360962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1138: Training Loss: 1.0026288032531738 Validation Loss: 1.3197767734527588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1139: Training Loss: 1.001367171605428 Validation Loss: 1.3189009428024292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1140: Training Loss: 0.9993758797645569 Validation Loss: 1.3186606168746948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1141: Training Loss: 1.0002639691034954 Validation Loss: 1.3178764581680298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1142: Training Loss: 1.002412458260854 Validation Loss: 1.316950798034668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1143: Training Loss: 0.9989532033602396 Validation Loss: 1.3159682750701904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1144: Training Loss: 0.9976746241251627 Validation Loss: 1.3151522874832153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1145: Training Loss: 0.995834469795227 Validation Loss: 1.3145664930343628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1146: Training Loss: 0.9957283933957418 Validation Loss: 1.3137794733047485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1147: Training Loss: 0.9959460496902466 Validation Loss: 1.3133677244186401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 1148: Training Loss: 0.9956218600273132 Validation Loss: 1.3130472898483276\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1149: Training Loss: 0.993498424688975 Validation Loss: 1.3124080896377563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1150: Training Loss: 0.9950626889864603 Validation Loss: 1.31166672706604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1151: Training Loss: 0.9926162958145142 Validation Loss: 1.3108688592910767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1152: Training Loss: 0.9916222294171652 Validation Loss: 1.3100658655166626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1153: Training Loss: 0.9905814727147421 Validation Loss: 1.3092715740203857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1154: Training Loss: 0.989610493183136 Validation Loss: 1.3085929155349731\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1155: Training Loss: 0.9896618723869324 Validation Loss: 1.3078029155731201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1156: Training Loss: 0.9882007837295532 Validation Loss: 1.307196021080017\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1157: Training Loss: 0.9874862432479858 Validation Loss: 1.3067399263381958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1158: Training Loss: 0.9864853421847025 Validation Loss: 1.3059167861938477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1159: Training Loss: 0.9859318534533182 Validation Loss: 1.3053513765335083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1160: Training Loss: 0.9869212110837301 Validation Loss: 1.3044928312301636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1161: Training Loss: 0.9830964406331381 Validation Loss: 1.3040497303009033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1162: Training Loss: 0.9826509356498718 Validation Loss: 1.303406000137329\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1163: Training Loss: 0.9816293915112814 Validation Loss: 1.30280339717865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1164: Training Loss: 0.9817668000857035 Validation Loss: 1.3022998571395874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1165: Training Loss: 0.981195867061615 Validation Loss: 1.301712989807129\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1166: Training Loss: 0.9803792039553324 Validation Loss: 1.3009302616119385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1167: Training Loss: 0.9807478388150533 Validation Loss: 1.3002562522888184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1168: Training Loss: 0.9794352849324545 Validation Loss: 1.2995623350143433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1169: Training Loss: 0.9773696660995483 Validation Loss: 1.2989346981048584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1170: Training Loss: 0.9798397819201151 Validation Loss: 1.2981852293014526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1171: Training Loss: 0.9758404493331909 Validation Loss: 1.2974838018417358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1172: Training Loss: 0.975466807683309 Validation Loss: 1.2968858480453491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1173: Training Loss: 0.9746125936508179 Validation Loss: 1.2960882186889648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1174: Training Loss: 0.9734907547632853 Validation Loss: 1.295580267906189\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1175: Training Loss: 0.9735009670257568 Validation Loss: 1.29500412940979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1176: Training Loss: 0.9725373784701029 Validation Loss: 1.294160008430481\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1177: Training Loss: 0.9713532527287801 Validation Loss: 1.2935521602630615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1178: Training Loss: 0.9692468643188477 Validation Loss: 1.2933344841003418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1179: Training Loss: 0.970636785030365 Validation Loss: 1.2926894426345825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1180: Training Loss: 0.9701743920644125 Validation Loss: 1.292020320892334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1181: Training Loss: 0.9690887331962585 Validation Loss: 1.291228175163269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1182: Training Loss: 0.9696900248527527 Validation Loss: 1.2904680967330933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1183: Training Loss: 0.9657072226206461 Validation Loss: 1.290099859237671\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1184: Training Loss: 0.9653616150220236 Validation Loss: 1.2896467447280884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1185: Training Loss: 0.9656724731127421 Validation Loss: 1.2887487411499023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1186: Training Loss: 0.9636904994646708 Validation Loss: 1.2884066104888916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1187: Training Loss: 0.966305156548818 Validation Loss: 1.2877223491668701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1188: Training Loss: 0.9629865884780884 Validation Loss: 1.2869048118591309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1189: Training Loss: 0.9626098473866781 Validation Loss: 1.2861393690109253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1190: Training Loss: 0.9615244468053182 Validation Loss: 1.285198450088501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1191: Training Loss: 0.9606935183207194 Validation Loss: 1.2845159769058228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1192: Training Loss: 0.9603492021560669 Validation Loss: 1.2839170694351196\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1193: Training Loss: 0.9593308170636495 Validation Loss: 1.2833220958709717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1194: Training Loss: 0.9584152301152548 Validation Loss: 1.28262197971344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1195: Training Loss: 0.9576512972513834 Validation Loss: 1.2820374965667725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1196: Training Loss: 0.9583185911178589 Validation Loss: 1.2816543579101562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1197: Training Loss: 0.9553081194559733 Validation Loss: 1.281337022781372\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1198: Training Loss: 0.9557605385780334 Validation Loss: 1.2805347442626953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1199: Training Loss: 0.9554573893547058 Validation Loss: 1.279899001121521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1200: Training Loss: 0.9541891614596049 Validation Loss: 1.2790312767028809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1201: Training Loss: 0.9536926945050558 Validation Loss: 1.2784780263900757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1202: Training Loss: 0.9529287616411845 Validation Loss: 1.2779622077941895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1203: Training Loss: 0.9519829750061035 Validation Loss: 1.2771867513656616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1204: Training Loss: 0.9525971015294393 Validation Loss: 1.2767122983932495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1205: Training Loss: 0.9505791266759237 Validation Loss: 1.2761695384979248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1206: Training Loss: 0.9505026936531067 Validation Loss: 1.2754452228546143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1207: Training Loss: 0.9484658241271973 Validation Loss: 1.2747774124145508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1208: Training Loss: 0.9486708839734396 Validation Loss: 1.27428138256073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1209: Training Loss: 0.9475891788800558 Validation Loss: 1.2736166715621948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1210: Training Loss: 0.9467007716496786 Validation Loss: 1.2728718519210815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1211: Training Loss: 0.9468744993209839 Validation Loss: 1.2721813917160034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1212: Training Loss: 0.9448510408401489 Validation Loss: 1.2717905044555664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1213: Training Loss: 0.9437499841054281 Validation Loss: 1.2711527347564697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1214: Training Loss: 0.9441188971201578 Validation Loss: 1.2704389095306396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1215: Training Loss: 0.9423478643099467 Validation Loss: 1.2700356245040894\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1216: Training Loss: 0.9419388175010681 Validation Loss: 1.2693122625350952\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1217: Training Loss: 0.9388986229896545 Validation Loss: 1.268562912940979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1218: Training Loss: 0.9403267304102579 Validation Loss: 1.2678418159484863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1219: Training Loss: 0.9398667017618815 Validation Loss: 1.2675113677978516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1220: Training Loss: 0.9393097162246704 Validation Loss: 1.2669187784194946\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1221: Training Loss: 0.9396347403526306 Validation Loss: 1.2662160396575928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1222: Training Loss: 0.9380200902620951 Validation Loss: 1.2657294273376465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1223: Training Loss: 0.9370771050453186 Validation Loss: 1.2651118040084839\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1224: Training Loss: 0.9386870463689169 Validation Loss: 1.2646945714950562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1225: Training Loss: 0.9346661766370138 Validation Loss: 1.2638742923736572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1226: Training Loss: 0.9339235623677572 Validation Loss: 1.26310133934021\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1227: Training Loss: 0.9374723633130392 Validation Loss: 1.2625679969787598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1228: Training Loss: 0.933296283086141 Validation Loss: 1.262115478515625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1229: Training Loss: 0.9330416123072306 Validation Loss: 1.261397361755371\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1230: Training Loss: 0.9323400855064392 Validation Loss: 1.2606536149978638\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1231: Training Loss: 0.9312492609024048 Validation Loss: 1.259928822517395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1232: Training Loss: 0.9311797618865967 Validation Loss: 1.259438395500183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1233: Training Loss: 0.9302738904953003 Validation Loss: 1.259082317352295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1234: Training Loss: 0.9318243861198425 Validation Loss: 1.2583355903625488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1235: Training Loss: 0.9281080961227417 Validation Loss: 1.2579046487808228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1236: Training Loss: 0.927823543548584 Validation Loss: 1.257211446762085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1237: Training Loss: 0.9277373949686686 Validation Loss: 1.2566550970077515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1238: Training Loss: 0.9261762499809265 Validation Loss: 1.2558488845825195\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1239: Training Loss: 0.9253917336463928 Validation Loss: 1.2553472518920898\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1240: Training Loss: 0.9253092010815939 Validation Loss: 1.2545782327651978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1241: Training Loss: 0.9245401422182719 Validation Loss: 1.2542383670806885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1242: Training Loss: 0.9232833385467529 Validation Loss: 1.2535219192504883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1243: Training Loss: 0.9222545425097147 Validation Loss: 1.2528815269470215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1244: Training Loss: 0.9215057492256165 Validation Loss: 1.252461314201355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1245: Training Loss: 0.9211286902427673 Validation Loss: 1.2520229816436768\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1246: Training Loss: 0.9205377101898193 Validation Loss: 1.251426339149475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1247: Training Loss: 0.9202303489049276 Validation Loss: 1.2505921125411987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1248: Training Loss: 0.9183519879976908 Validation Loss: 1.2501312494277954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1249: Training Loss: 0.9185288747151693 Validation Loss: 1.2494555711746216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1250: Training Loss: 0.9174118041992188 Validation Loss: 1.248714566230774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1251: Training Loss: 0.9157009919484457 Validation Loss: 1.2482151985168457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1252: Training Loss: 0.9165000518163046 Validation Loss: 1.2475295066833496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1253: Training Loss: 0.9153494437535604 Validation Loss: 1.2470020055770874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1254: Training Loss: 0.9145065546035767 Validation Loss: 1.246472954750061\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1255: Training Loss: 0.9124067425727844 Validation Loss: 1.2458466291427612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1256: Training Loss: 0.9126264055569967 Validation Loss: 1.2453724145889282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1257: Training Loss: 0.9128741025924683 Validation Loss: 1.2447963953018188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1258: Training Loss: 0.9119133750597636 Validation Loss: 1.2442938089370728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1259: Training Loss: 0.913355807463328 Validation Loss: 1.2442961931228638\n",
      "Epoch 1260: Training Loss: 0.9108846187591553 Validation Loss: 1.2436572313308716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1261: Training Loss: 0.909909168879191 Validation Loss: 1.2428175210952759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1262: Training Loss: 0.9091448187828064 Validation Loss: 1.2420387268066406\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1263: Training Loss: 0.9084058205286661 Validation Loss: 1.241186261177063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1264: Training Loss: 0.9089740912119547 Validation Loss: 1.2406651973724365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1265: Training Loss: 0.9075033863385519 Validation Loss: 1.240110158920288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1266: Training Loss: 0.9059959252675375 Validation Loss: 1.2393665313720703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1267: Training Loss: 0.905697743097941 Validation Loss: 1.2390176057815552\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1268: Training Loss: 0.905222753683726 Validation Loss: 1.2386164665222168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1269: Training Loss: 0.9043054382006327 Validation Loss: 1.2382690906524658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1270: Training Loss: 0.9048319657643636 Validation Loss: 1.2372726202011108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1271: Training Loss: 0.9033634861310323 Validation Loss: 1.2367808818817139\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1272: Training Loss: 0.9026850461959839 Validation Loss: 1.236176609992981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1273: Training Loss: 0.9013170997301737 Validation Loss: 1.2356688976287842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1274: Training Loss: 0.9012569387753805 Validation Loss: 1.235016107559204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1275: Training Loss: 0.8996916015942892 Validation Loss: 1.234646201133728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1276: Training Loss: 0.8998998800913492 Validation Loss: 1.2343031167984009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1277: Training Loss: 0.8991473913192749 Validation Loss: 1.2336169481277466\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1278: Training Loss: 0.8986417055130005 Validation Loss: 1.2326682806015015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1279: Training Loss: 0.8971828023592631 Validation Loss: 1.2321048974990845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1280: Training Loss: 0.8974677522977194 Validation Loss: 1.2315593957901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1281: Training Loss: 0.8982384999593099 Validation Loss: 1.2308876514434814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1282: Training Loss: 0.8962200482686361 Validation Loss: 1.230272889137268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1283: Training Loss: 0.8943385481834412 Validation Loss: 1.2298038005828857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1284: Training Loss: 0.896193782488505 Validation Loss: 1.229196548461914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1285: Training Loss: 0.8945138653119405 Validation Loss: 1.2286560535430908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1286: Training Loss: 0.8927929202715555 Validation Loss: 1.2281588315963745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1287: Training Loss: 0.8916786511739095 Validation Loss: 1.2278692722320557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1288: Training Loss: 0.8909285068511963 Validation Loss: 1.2273736000061035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1289: Training Loss: 0.889880100886027 Validation Loss: 1.2266690731048584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1290: Training Loss: 0.8894888758659363 Validation Loss: 1.226146936416626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1291: Training Loss: 0.8900651335716248 Validation Loss: 1.225398063659668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1292: Training Loss: 0.888807475566864 Validation Loss: 1.224768877029419\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1293: Training Loss: 0.8883614341417948 Validation Loss: 1.2244676351547241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1294: Training Loss: 0.8884591658910116 Validation Loss: 1.2239875793457031\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1295: Training Loss: 0.8867851297060648 Validation Loss: 1.2234790325164795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1296: Training Loss: 0.8856920798619589 Validation Loss: 1.2228566408157349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1297: Training Loss: 0.8851927717526754 Validation Loss: 1.2225282192230225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1298: Training Loss: 0.8848406672477722 Validation Loss: 1.2219642400741577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1299: Training Loss: 0.883634090423584 Validation Loss: 1.2209786176681519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1300: Training Loss: 0.8830331762631735 Validation Loss: 1.220305323600769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1301: Training Loss: 0.8827911019325256 Validation Loss: 1.2195559740066528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1302: Training Loss: 0.8806877334912618 Validation Loss: 1.219218134880066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1303: Training Loss: 0.8818062543869019 Validation Loss: 1.2190377712249756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 1304: Training Loss: 0.8812854687372843 Validation Loss: 1.2182204723358154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1305: Training Loss: 0.8775299191474915 Validation Loss: 1.2175369262695312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1306: Training Loss: 0.8795679211616516 Validation Loss: 1.2170525789260864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1307: Training Loss: 0.881591260433197 Validation Loss: 1.2166162729263306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1308: Training Loss: 0.878173847993215 Validation Loss: 1.216044306755066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1309: Training Loss: 0.8790044784545898 Validation Loss: 1.2156603336334229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1310: Training Loss: 0.8760585784912109 Validation Loss: 1.2152763605117798\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1311: Training Loss: 0.876651406288147 Validation Loss: 1.214956283569336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1312: Training Loss: 0.8766981164614359 Validation Loss: 1.2143839597702026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1313: Training Loss: 0.8743619521458944 Validation Loss: 1.213340401649475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1314: Training Loss: 0.8740915457407633 Validation Loss: 1.2124075889587402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1315: Training Loss: 0.8736102183659872 Validation Loss: 1.2117986679077148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1316: Training Loss: 0.8724415103594462 Validation Loss: 1.2114551067352295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1317: Training Loss: 0.8704670866330465 Validation Loss: 1.2111616134643555\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1318: Training Loss: 0.8729185461997986 Validation Loss: 1.2107293605804443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1319: Training Loss: 0.8709906140963236 Validation Loss: 1.2104613780975342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1320: Training Loss: 0.8704875906308492 Validation Loss: 1.2100334167480469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1321: Training Loss: 0.869020402431488 Validation Loss: 1.2093427181243896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1322: Training Loss: 0.8700150648752848 Validation Loss: 1.208763837814331\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1323: Training Loss: 0.8674585620562235 Validation Loss: 1.2082901000976562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1324: Training Loss: 0.8677833278973898 Validation Loss: 1.2076243162155151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1325: Training Loss: 0.8664614955584208 Validation Loss: 1.2070698738098145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1326: Training Loss: 0.8658459782600403 Validation Loss: 1.2062054872512817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1327: Training Loss: 0.8652183214823405 Validation Loss: 1.2054808139801025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1328: Training Loss: 0.8641093770662943 Validation Loss: 1.204994797706604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1329: Training Loss: 0.8650187452634176 Validation Loss: 1.2045905590057373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1330: Training Loss: 0.8637866775194804 Validation Loss: 1.2039844989776611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1331: Training Loss: 0.8626490632692972 Validation Loss: 1.2035757303237915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1332: Training Loss: 0.863108774026235 Validation Loss: 1.203162670135498\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1333: Training Loss: 0.8608381549517313 Validation Loss: 1.2026010751724243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1334: Training Loss: 0.8603832721710205 Validation Loss: 1.2022545337677002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1335: Training Loss: 0.8606908321380615 Validation Loss: 1.2017629146575928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1336: Training Loss: 0.8599815964698792 Validation Loss: 1.2011052370071411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1337: Training Loss: 0.8589989145596822 Validation Loss: 1.200531244277954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1338: Training Loss: 0.8581748803456625 Validation Loss: 1.1999415159225464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1339: Training Loss: 0.8568389614423116 Validation Loss: 1.1992974281311035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1340: Training Loss: 0.8580058018366495 Validation Loss: 1.19873046875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1341: Training Loss: 0.8563924630482992 Validation Loss: 1.198323130607605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1342: Training Loss: 0.8559654752413431 Validation Loss: 1.1980013847351074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1343: Training Loss: 0.8557937542597452 Validation Loss: 1.1974750757217407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1344: Training Loss: 0.8551274736722311 Validation Loss: 1.1969797611236572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1345: Training Loss: 0.8547830382982889 Validation Loss: 1.1966155767440796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1346: Training Loss: 0.8557857672373453 Validation Loss: 1.1960241794586182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1347: Training Loss: 0.8528334299723307 Validation Loss: 1.1952558755874634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1348: Training Loss: 0.8517351945241293 Validation Loss: 1.1948126554489136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1349: Training Loss: 0.8527421355247498 Validation Loss: 1.1945406198501587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1350: Training Loss: 0.8510617812474569 Validation Loss: 1.1939284801483154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1351: Training Loss: 0.8517693281173706 Validation Loss: 1.1933391094207764\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1352: Training Loss: 0.8495936195055643 Validation Loss: 1.192819595336914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1353: Training Loss: 0.848572293917338 Validation Loss: 1.1922659873962402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1354: Training Loss: 0.8500635027885437 Validation Loss: 1.1917320489883423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1355: Training Loss: 0.847705622514089 Validation Loss: 1.1913038492202759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1356: Training Loss: 0.8474801182746887 Validation Loss: 1.1909431219100952\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1357: Training Loss: 0.8474881251653036 Validation Loss: 1.1905142068862915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1358: Training Loss: 0.8456530372301737 Validation Loss: 1.1898070573806763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1359: Training Loss: 0.8449060320854187 Validation Loss: 1.1893706321716309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1360: Training Loss: 0.845353345076243 Validation Loss: 1.1885056495666504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1361: Training Loss: 0.8447524110476176 Validation Loss: 1.1878912448883057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1362: Training Loss: 0.8443454106648763 Validation Loss: 1.1873815059661865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1363: Training Loss: 0.8443832993507385 Validation Loss: 1.1867754459381104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1364: Training Loss: 0.8421561519304911 Validation Loss: 1.186123013496399\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1365: Training Loss: 0.8420217434565226 Validation Loss: 1.1857728958129883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1366: Training Loss: 0.8420574069023132 Validation Loss: 1.1853846311569214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1367: Training Loss: 0.8409373760223389 Validation Loss: 1.184996247291565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1368: Training Loss: 0.8382964531580607 Validation Loss: 1.1846046447753906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1369: Training Loss: 0.8391758600870768 Validation Loss: 1.1839687824249268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1370: Training Loss: 0.8386149803797404 Validation Loss: 1.1836103200912476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1371: Training Loss: 0.838246484597524 Validation Loss: 1.1829808950424194\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1372: Training Loss: 0.8377886017163595 Validation Loss: 1.1824365854263306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1373: Training Loss: 0.8373404343922933 Validation Loss: 1.1821411848068237\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1374: Training Loss: 0.8357961773872375 Validation Loss: 1.1817049980163574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1375: Training Loss: 0.8348044157028198 Validation Loss: 1.1812728643417358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1376: Training Loss: 0.8348244627316793 Validation Loss: 1.180505394935608\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1377: Training Loss: 0.8359933296839396 Validation Loss: 1.1799068450927734\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1378: Training Loss: 0.8343564867973328 Validation Loss: 1.1792819499969482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1379: Training Loss: 0.8334973255793253 Validation Loss: 1.1788175106048584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1380: Training Loss: 0.8324579199155172 Validation Loss: 1.178248643875122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1381: Training Loss: 0.8323206702868143 Validation Loss: 1.1778124570846558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1382: Training Loss: 0.8328844706217448 Validation Loss: 1.1776546239852905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1383: Training Loss: 0.832359512646993 Validation Loss: 1.177402377128601\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1384: Training Loss: 0.8302842775980631 Validation Loss: 1.1768213510513306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1385: Training Loss: 0.8295108079910278 Validation Loss: 1.1763935089111328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1386: Training Loss: 0.8293171922365824 Validation Loss: 1.1756618022918701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1387: Training Loss: 0.8286468187967936 Validation Loss: 1.1749669313430786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1388: Training Loss: 0.8292463223139445 Validation Loss: 1.1745339632034302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1389: Training Loss: 0.8271329800287882 Validation Loss: 1.1740059852600098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1390: Training Loss: 0.8262345790863037 Validation Loss: 1.173458218574524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1391: Training Loss: 0.8254293004671732 Validation Loss: 1.1728562116622925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1392: Training Loss: 0.8251640796661377 Validation Loss: 1.1723954677581787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1393: Training Loss: 0.8256709973017374 Validation Loss: 1.1718770265579224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1394: Training Loss: 0.823193887869517 Validation Loss: 1.1716290712356567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1395: Training Loss: 0.8236515124638876 Validation Loss: 1.1712884902954102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1396: Training Loss: 0.8231048583984375 Validation Loss: 1.1708300113677979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1397: Training Loss: 0.8228808840115865 Validation Loss: 1.1700514554977417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1398: Training Loss: 0.8210387229919434 Validation Loss: 1.169497013092041\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1399: Training Loss: 0.8221966425577799 Validation Loss: 1.1691107749938965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1400: Training Loss: 0.8205659985542297 Validation Loss: 1.1686668395996094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1401: Training Loss: 0.8200122316678365 Validation Loss: 1.168199896812439\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1402: Training Loss: 0.8206362922986349 Validation Loss: 1.1676568984985352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1403: Training Loss: 0.819979707400004 Validation Loss: 1.167451024055481\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1404: Training Loss: 0.8181111017862955 Validation Loss: 1.1671178340911865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1405: Training Loss: 0.8182859222094218 Validation Loss: 1.1665297746658325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1406: Training Loss: 0.816776176293691 Validation Loss: 1.1659725904464722\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1407: Training Loss: 0.8174448609352112 Validation Loss: 1.1652036905288696\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1408: Training Loss: 0.8170386354128519 Validation Loss: 1.1645146608352661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1409: Training Loss: 0.8167734543482462 Validation Loss: 1.1638349294662476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1410: Training Loss: 0.814884344736735 Validation Loss: 1.1634632349014282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1411: Training Loss: 0.814992884794871 Validation Loss: 1.1632148027420044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1412: Training Loss: 0.8159775733947754 Validation Loss: 1.1630231142044067\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1413: Training Loss: 0.8127530813217163 Validation Loss: 1.1624571084976196\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1414: Training Loss: 0.812201182047526 Validation Loss: 1.1619298458099365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1415: Training Loss: 0.811754564444224 Validation Loss: 1.1614755392074585\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1416: Training Loss: 0.8114010294278463 Validation Loss: 1.1609183549880981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1417: Training Loss: 0.810753067334493 Validation Loss: 1.1604212522506714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1418: Training Loss: 0.8098329504330953 Validation Loss: 1.1600912809371948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1419: Training Loss: 0.8093380729357401 Validation Loss: 1.1595319509506226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1420: Training Loss: 0.8091998298962911 Validation Loss: 1.159137487411499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1421: Training Loss: 0.8081881205240885 Validation Loss: 1.1585594415664673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1422: Training Loss: 0.8076248367627462 Validation Loss: 1.1579937934875488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1423: Training Loss: 0.8061511516571045 Validation Loss: 1.1576921939849854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1424: Training Loss: 0.808683971563975 Validation Loss: 1.1573179960250854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1425: Training Loss: 0.8072781364123026 Validation Loss: 1.1567237377166748\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1426: Training Loss: 0.8048760096232096 Validation Loss: 1.1560957431793213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1427: Training Loss: 0.8038652340571085 Validation Loss: 1.155776023864746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1428: Training Loss: 0.8031971255938212 Validation Loss: 1.155083179473877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1429: Training Loss: 0.8037349581718445 Validation Loss: 1.154684066772461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1430: Training Loss: 0.8027150829633077 Validation Loss: 1.154049038887024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1431: Training Loss: 0.8034304976463318 Validation Loss: 1.153667688369751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1432: Training Loss: 0.8018990159034729 Validation Loss: 1.153394341468811\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1433: Training Loss: 0.8021440307299296 Validation Loss: 1.1528726816177368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1434: Training Loss: 0.8014868497848511 Validation Loss: 1.1525046825408936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1435: Training Loss: 0.8004695971806844 Validation Loss: 1.1521317958831787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1436: Training Loss: 0.7996356089909872 Validation Loss: 1.1516642570495605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1437: Training Loss: 0.8005611101786295 Validation Loss: 1.1511256694793701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1438: Training Loss: 0.7989440162976583 Validation Loss: 1.1505274772644043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1439: Training Loss: 0.7986667354901632 Validation Loss: 1.1500858068466187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1440: Training Loss: 0.7973941365877787 Validation Loss: 1.1495012044906616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1441: Training Loss: 0.7980015476544698 Validation Loss: 1.14911687374115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1442: Training Loss: 0.7955756386121114 Validation Loss: 1.1486823558807373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1443: Training Loss: 0.7960291703542074 Validation Loss: 1.1482868194580078\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1444: Training Loss: 0.7962636947631836 Validation Loss: 1.1476316452026367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1445: Training Loss: 0.7939091523488363 Validation Loss: 1.1470850706100464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1446: Training Loss: 0.7952068249384562 Validation Loss: 1.1465226411819458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1447: Training Loss: 0.79468834400177 Validation Loss: 1.1462745666503906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1448: Training Loss: 0.7930391828219095 Validation Loss: 1.146068811416626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1449: Training Loss: 0.7926440834999084 Validation Loss: 1.145870327949524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1450: Training Loss: 0.7921139995257059 Validation Loss: 1.1454391479492188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1451: Training Loss: 0.7936494747797648 Validation Loss: 1.1448341608047485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1452: Training Loss: 0.7907843788464864 Validation Loss: 1.1442530155181885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1453: Training Loss: 0.7903066674868265 Validation Loss: 1.1438030004501343\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1454: Training Loss: 0.790216326713562 Validation Loss: 1.1431865692138672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1455: Training Loss: 0.7885346412658691 Validation Loss: 1.1427820920944214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1456: Training Loss: 0.7890488306681315 Validation Loss: 1.1422227621078491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1457: Training Loss: 0.7888072729110718 Validation Loss: 1.1416854858398438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1458: Training Loss: 0.7875680724779764 Validation Loss: 1.1413034200668335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1459: Training Loss: 0.7869329849878947 Validation Loss: 1.1407655477523804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1460: Training Loss: 0.7874346375465393 Validation Loss: 1.1405115127563477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1461: Training Loss: 0.7856732606887817 Validation Loss: 1.1403605937957764\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1462: Training Loss: 0.7853928009668986 Validation Loss: 1.1397885084152222\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1463: Training Loss: 0.7846618493398031 Validation Loss: 1.1394565105438232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1464: Training Loss: 0.7842147946357727 Validation Loss: 1.1388919353485107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1465: Training Loss: 0.7843537529309591 Validation Loss: 1.1379956007003784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1466: Training Loss: 0.7836261789004008 Validation Loss: 1.1373947858810425\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1467: Training Loss: 0.7835614879926046 Validation Loss: 1.1369662284851074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1468: Training Loss: 0.784001092116038 Validation Loss: 1.1366909742355347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1469: Training Loss: 0.7808584372202555 Validation Loss: 1.1362824440002441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1470: Training Loss: 0.7809148033459982 Validation Loss: 1.135974645614624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1471: Training Loss: 0.7803434729576111 Validation Loss: 1.1355699300765991\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1472: Training Loss: 0.7798304359118143 Validation Loss: 1.1350213289260864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1473: Training Loss: 0.7791680296262106 Validation Loss: 1.1345889568328857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1474: Training Loss: 0.7797564069430033 Validation Loss: 1.1341034173965454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1475: Training Loss: 0.7787571549415588 Validation Loss: 1.1336299180984497\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1476: Training Loss: 0.7792796095212301 Validation Loss: 1.1332645416259766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1477: Training Loss: 0.7770102421442667 Validation Loss: 1.1329535245895386\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1478: Training Loss: 0.7762559254964193 Validation Loss: 1.1327582597732544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1479: Training Loss: 0.7764617800712585 Validation Loss: 1.1322184801101685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1480: Training Loss: 0.7755566835403442 Validation Loss: 1.1314505338668823\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1481: Training Loss: 0.7752262552579244 Validation Loss: 1.1310436725616455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1482: Training Loss: 0.7747019728024801 Validation Loss: 1.1306504011154175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1483: Training Loss: 0.7731367150942484 Validation Loss: 1.1300996541976929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1484: Training Loss: 0.7731016079584757 Validation Loss: 1.129783272743225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1485: Training Loss: 0.7741266489028931 Validation Loss: 1.1291868686676025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1486: Training Loss: 0.771648089090983 Validation Loss: 1.1287636756896973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1487: Training Loss: 0.7720649043718973 Validation Loss: 1.1284292936325073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1488: Training Loss: 0.7714111606280009 Validation Loss: 1.127927303314209\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1489: Training Loss: 0.7708721359570821 Validation Loss: 1.127691626548767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1490: Training Loss: 0.7713884711265564 Validation Loss: 1.1272540092468262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1491: Training Loss: 0.7700339158376058 Validation Loss: 1.1266875267028809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1492: Training Loss: 0.7695542375246683 Validation Loss: 1.1263446807861328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1493: Training Loss: 0.7685701251029968 Validation Loss: 1.1258548498153687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1494: Training Loss: 0.7694758971532186 Validation Loss: 1.1252797842025757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1495: Training Loss: 0.7693224350611368 Validation Loss: 1.124609351158142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1496: Training Loss: 0.7672257622083029 Validation Loss: 1.1241624355316162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1497: Training Loss: 0.7666758100191752 Validation Loss: 1.1239020824432373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1498: Training Loss: 0.7661545674006144 Validation Loss: 1.123779058456421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1499: Training Loss: 0.7656003832817078 Validation Loss: 1.1232390403747559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1500: Training Loss: 0.7649146119753519 Validation Loss: 1.1227154731750488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1501: Training Loss: 0.7643695076306661 Validation Loss: 1.122096061706543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1502: Training Loss: 0.7637686729431152 Validation Loss: 1.1216493844985962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1503: Training Loss: 0.7655450900395712 Validation Loss: 1.1214216947555542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1504: Training Loss: 0.7632139523824056 Validation Loss: 1.1208715438842773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1505: Training Loss: 0.7619802753130595 Validation Loss: 1.1207468509674072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1506: Training Loss: 0.7620538671811422 Validation Loss: 1.1203200817108154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1507: Training Loss: 0.7639555732409159 Validation Loss: 1.1198605298995972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1508: Training Loss: 0.7615954279899597 Validation Loss: 1.1194144487380981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1509: Training Loss: 0.7604623238245646 Validation Loss: 1.1190204620361328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1510: Training Loss: 0.7602895498275757 Validation Loss: 1.1187036037445068\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1511: Training Loss: 0.7591448823610941 Validation Loss: 1.118038296699524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1512: Training Loss: 0.7589205900828043 Validation Loss: 1.1174582242965698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1513: Training Loss: 0.7573379675547282 Validation Loss: 1.1167491674423218\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1514: Training Loss: 0.7575434645016988 Validation Loss: 1.1162725687026978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1515: Training Loss: 0.7571139931678772 Validation Loss: 1.1159220933914185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1516: Training Loss: 0.7568232615788778 Validation Loss: 1.1157586574554443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1517: Training Loss: 0.756655236085256 Validation Loss: 1.1153485774993896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1518: Training Loss: 0.7549583315849304 Validation Loss: 1.1150915622711182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1519: Training Loss: 0.7534585992495219 Validation Loss: 1.1146727800369263\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1520: Training Loss: 0.7547723452250162 Validation Loss: 1.1142174005508423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1521: Training Loss: 0.7542779048283895 Validation Loss: 1.113876223564148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1522: Training Loss: 0.7537640929222107 Validation Loss: 1.1132529973983765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1523: Training Loss: 0.7525808811187744 Validation Loss: 1.1128828525543213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1524: Training Loss: 0.7527047197024027 Validation Loss: 1.1124480962753296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1525: Training Loss: 0.7515265742937723 Validation Loss: 1.1119731664657593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1526: Training Loss: 0.7518064379692078 Validation Loss: 1.1118261814117432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1527: Training Loss: 0.7501035928726196 Validation Loss: 1.111541986465454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1528: Training Loss: 0.7504902482032776 Validation Loss: 1.110982894897461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1529: Training Loss: 0.7497910062472025 Validation Loss: 1.110409140586853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1530: Training Loss: 0.7482941945393881 Validation Loss: 1.1098365783691406\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1531: Training Loss: 0.7492560744285583 Validation Loss: 1.1096131801605225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1532: Training Loss: 0.7489032745361328 Validation Loss: 1.1089186668395996\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1533: Training Loss: 0.7471248706181844 Validation Loss: 1.1086833477020264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1534: Training Loss: 0.7475249568621317 Validation Loss: 1.1082934141159058\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1535: Training Loss: 0.747120996316274 Validation Loss: 1.107988715171814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1536: Training Loss: 0.7469273408253988 Validation Loss: 1.1076191663742065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1537: Training Loss: 0.7452112436294556 Validation Loss: 1.107156753540039\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1538: Training Loss: 0.7448252240816752 Validation Loss: 1.1065765619277954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1539: Training Loss: 0.7458641529083252 Validation Loss: 1.1060646772384644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1540: Training Loss: 0.7452432910601298 Validation Loss: 1.1057101488113403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1541: Training Loss: 0.7448012034098307 Validation Loss: 1.1055480241775513\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1542: Training Loss: 0.7436576882998148 Validation Loss: 1.105247139930725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1543: Training Loss: 0.7428414225578308 Validation Loss: 1.104680061340332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1544: Training Loss: 0.7436057329177856 Validation Loss: 1.1044285297393799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1545: Training Loss: 0.7417408426602682 Validation Loss: 1.1036279201507568\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1546: Training Loss: 0.7427790760993958 Validation Loss: 1.1033732891082764\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1547: Training Loss: 0.7406497200330099 Validation Loss: 1.102813482284546\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1548: Training Loss: 0.7417996923128763 Validation Loss: 1.1023317575454712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1549: Training Loss: 0.7394137779871622 Validation Loss: 1.1017396450042725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1550: Training Loss: 0.738402803738912 Validation Loss: 1.1014684438705444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1551: Training Loss: 0.7387134432792664 Validation Loss: 1.1011419296264648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1552: Training Loss: 0.7384286920229594 Validation Loss: 1.1007529497146606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1553: Training Loss: 0.7386354804039001 Validation Loss: 1.1002936363220215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1554: Training Loss: 0.7375379800796509 Validation Loss: 1.1001770496368408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1555: Training Loss: 0.7366707921028137 Validation Loss: 1.1001825332641602\n",
      "Epoch 1556: Training Loss: 0.7369777162869772 Validation Loss: 1.0996484756469727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1557: Training Loss: 0.7359510064125061 Validation Loss: 1.099189281463623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1558: Training Loss: 0.7364752292633057 Validation Loss: 1.0986248254776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1559: Training Loss: 0.7349085211753845 Validation Loss: 1.0982574224472046\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1560: Training Loss: 0.7346121271451315 Validation Loss: 1.0976730585098267\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1561: Training Loss: 0.7341482043266296 Validation Loss: 1.0973976850509644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1562: Training Loss: 0.7344261606534322 Validation Loss: 1.0971559286117554\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1563: Training Loss: 0.7357662320137024 Validation Loss: 1.096899151802063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1564: Training Loss: 0.7329180637995402 Validation Loss: 1.0963371992111206\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1565: Training Loss: 0.7328939437866211 Validation Loss: 1.0955562591552734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1566: Training Loss: 0.7311567664146423 Validation Loss: 1.0950785875320435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1567: Training Loss: 0.7304998238881429 Validation Loss: 1.0945100784301758\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1568: Training Loss: 0.7307647665341696 Validation Loss: 1.0942659378051758\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1569: Training Loss: 0.7302515506744385 Validation Loss: 1.09389066696167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1570: Training Loss: 0.7294351855913798 Validation Loss: 1.0935609340667725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1571: Training Loss: 0.7294953465461731 Validation Loss: 1.0933552980422974\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1572: Training Loss: 0.7282358805338541 Validation Loss: 1.092856764793396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1573: Training Loss: 0.7282857100168864 Validation Loss: 1.0924471616744995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1574: Training Loss: 0.7273895939191183 Validation Loss: 1.092132806777954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1575: Training Loss: 0.7275325059890747 Validation Loss: 1.091865062713623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1576: Training Loss: 0.7257589101791382 Validation Loss: 1.0913304090499878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1577: Training Loss: 0.7257275978724161 Validation Loss: 1.0910720825195312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1578: Training Loss: 0.7264781792958578 Validation Loss: 1.0908710956573486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1579: Training Loss: 0.7250615159670512 Validation Loss: 1.0903083086013794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1580: Training Loss: 0.7245493332544962 Validation Loss: 1.0896435976028442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1581: Training Loss: 0.7265985210736593 Validation Loss: 1.0891335010528564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1582: Training Loss: 0.72445148229599 Validation Loss: 1.088623046875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1583: Training Loss: 0.7234004537264506 Validation Loss: 1.0882446765899658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1584: Training Loss: 0.7219999035199484 Validation Loss: 1.0880061388015747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1585: Training Loss: 0.7226572235425314 Validation Loss: 1.0879322290420532\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1586: Training Loss: 0.7215133905410767 Validation Loss: 1.0877227783203125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1587: Training Loss: 0.7219115893046061 Validation Loss: 1.0871652364730835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1588: Training Loss: 0.7220683693885803 Validation Loss: 1.0865665674209595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1589: Training Loss: 0.7207798759142557 Validation Loss: 1.0862061977386475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1590: Training Loss: 0.7202847401301066 Validation Loss: 1.085614562034607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1591: Training Loss: 0.7196542819341024 Validation Loss: 1.0853641033172607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1592: Training Loss: 0.7191576361656189 Validation Loss: 1.0850390195846558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1593: Training Loss: 0.7194215655326843 Validation Loss: 1.08497953414917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1594: Training Loss: 0.7185230453809103 Validation Loss: 1.0845431089401245\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1595: Training Loss: 0.7175026933352152 Validation Loss: 1.084100604057312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1596: Training Loss: 0.7183857758839926 Validation Loss: 1.083598256111145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1597: Training Loss: 0.7185449202855428 Validation Loss: 1.0831984281539917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1598: Training Loss: 0.7157780329386393 Validation Loss: 1.0825355052947998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1599: Training Loss: 0.7174765666325887 Validation Loss: 1.0820033550262451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1600: Training Loss: 0.7128394643465678 Validation Loss: 1.0816673040390015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1601: Training Loss: 0.7149138251940409 Validation Loss: 1.0814796686172485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1602: Training Loss: 0.7152060667673746 Validation Loss: 1.081290364265442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1603: Training Loss: 0.7146172523498535 Validation Loss: 1.0811094045639038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1604: Training Loss: 0.7133847077687582 Validation Loss: 1.0805758237838745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1605: Training Loss: 0.7129886349042257 Validation Loss: 1.0799460411071777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1606: Training Loss: 0.7125759124755859 Validation Loss: 1.0794624090194702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1607: Training Loss: 0.7118849754333496 Validation Loss: 1.0791988372802734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1608: Training Loss: 0.7116617361704508 Validation Loss: 1.0787824392318726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1609: Training Loss: 0.7115763227144877 Validation Loss: 1.0784540176391602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1610: Training Loss: 0.7102010250091553 Validation Loss: 1.0787314176559448\n",
      "Epoch 1611: Training Loss: 0.71107417345047 Validation Loss: 1.0783030986785889\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1612: Training Loss: 0.7103356122970581 Validation Loss: 1.0777758359909058\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1613: Training Loss: 0.7090066274007162 Validation Loss: 1.0772639513015747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1614: Training Loss: 0.7091996868451437 Validation Loss: 1.0766960382461548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1615: Training Loss: 0.7080210645993551 Validation Loss: 1.0760337114334106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1616: Training Loss: 0.7080835700035095 Validation Loss: 1.0755724906921387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1617: Training Loss: 0.7076229651769003 Validation Loss: 1.0752406120300293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1618: Training Loss: 0.7065569957097372 Validation Loss: 1.0746759176254272\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1619: Training Loss: 0.7054982781410217 Validation Loss: 1.0742231607437134\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1620: Training Loss: 0.7057712276776632 Validation Loss: 1.0739319324493408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1621: Training Loss: 0.7068928480148315 Validation Loss: 1.0740265846252441\n",
      "Epoch 1622: Training Loss: 0.7047611872355143 Validation Loss: 1.0738860368728638\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1623: Training Loss: 0.70527317126592 Validation Loss: 1.07366943359375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1624: Training Loss: 0.7038228511810303 Validation Loss: 1.0732190608978271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1625: Training Loss: 0.7029938300450643 Validation Loss: 1.0727055072784424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1626: Training Loss: 0.7029414772987366 Validation Loss: 1.0720158815383911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1627: Training Loss: 0.7035367290178934 Validation Loss: 1.0714733600616455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1628: Training Loss: 0.7021742065747579 Validation Loss: 1.0711146593093872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1629: Training Loss: 0.7016403079032898 Validation Loss: 1.070807933807373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1630: Training Loss: 0.7013753453890482 Validation Loss: 1.0704598426818848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1631: Training Loss: 0.7013483246167501 Validation Loss: 1.0700689554214478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1632: Training Loss: 0.7035757501920065 Validation Loss: 1.0697646141052246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1633: Training Loss: 0.701299250125885 Validation Loss: 1.0692896842956543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1634: Training Loss: 0.6995673775672913 Validation Loss: 1.0690146684646606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1635: Training Loss: 0.6991696357727051 Validation Loss: 1.0684833526611328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1636: Training Loss: 0.6981908877690634 Validation Loss: 1.0683536529541016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1637: Training Loss: 0.699147621790568 Validation Loss: 1.068396806716919\n",
      "Epoch 1638: Training Loss: 0.6971436937650045 Validation Loss: 1.0681439638137817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1639: Training Loss: 0.6976476709047953 Validation Loss: 1.067427396774292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1640: Training Loss: 0.6961231827735901 Validation Loss: 1.0671017169952393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1641: Training Loss: 0.6960217157999674 Validation Loss: 1.0664557218551636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1642: Training Loss: 0.69511479139328 Validation Loss: 1.0660287141799927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1643: Training Loss: 0.695533553759257 Validation Loss: 1.0657533407211304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1644: Training Loss: 0.6944687962532043 Validation Loss: 1.0656659603118896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1645: Training Loss: 0.6940062046051025 Validation Loss: 1.0651798248291016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1646: Training Loss: 0.6940904657046 Validation Loss: 1.0648658275604248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1647: Training Loss: 0.6938191850980123 Validation Loss: 1.0644922256469727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1648: Training Loss: 0.6942507028579712 Validation Loss: 1.0638974905014038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1649: Training Loss: 0.692887008190155 Validation Loss: 1.0635323524475098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1650: Training Loss: 0.6924403707186381 Validation Loss: 1.0629407167434692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1651: Training Loss: 0.6919091542561849 Validation Loss: 1.0629066228866577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1652: Training Loss: 0.69243852297465 Validation Loss: 1.0626684427261353\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1653: Training Loss: 0.6901612281799316 Validation Loss: 1.0624192953109741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1654: Training Loss: 0.6903937458992004 Validation Loss: 1.0620155334472656\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1655: Training Loss: 0.6897852222124735 Validation Loss: 1.0614333152770996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1656: Training Loss: 0.689582367738088 Validation Loss: 1.0610086917877197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1657: Training Loss: 0.6887535452842712 Validation Loss: 1.0607175827026367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1658: Training Loss: 0.688695232073466 Validation Loss: 1.060448408126831\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1659: Training Loss: 0.6869654854138693 Validation Loss: 1.0601369142532349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1660: Training Loss: 0.687848170598348 Validation Loss: 1.0598632097244263\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1661: Training Loss: 0.6874507069587708 Validation Loss: 1.0593582391738892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1662: Training Loss: 0.6868789792060852 Validation Loss: 1.0590232610702515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1663: Training Loss: 0.6863887111345927 Validation Loss: 1.0588001012802124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1664: Training Loss: 0.6871041258176168 Validation Loss: 1.0582562685012817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1665: Training Loss: 0.6857196489969889 Validation Loss: 1.057855248451233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1666: Training Loss: 0.6861460208892822 Validation Loss: 1.057706594467163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1667: Training Loss: 0.6854954759279887 Validation Loss: 1.0570120811462402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1668: Training Loss: 0.6839837431907654 Validation Loss: 1.0567500591278076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1669: Training Loss: 0.6833936174710592 Validation Loss: 1.056265115737915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1670: Training Loss: 0.6832435329755148 Validation Loss: 1.0559911727905273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1671: Training Loss: 0.683214525381724 Validation Loss: 1.0555124282836914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1672: Training Loss: 0.6819760799407959 Validation Loss: 1.0550050735473633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1673: Training Loss: 0.6821629007657369 Validation Loss: 1.054661512374878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1674: Training Loss: 0.6819839477539062 Validation Loss: 1.05452299118042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1675: Training Loss: 0.6800696055094401 Validation Loss: 1.0540121793746948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1676: Training Loss: 0.6809494495391846 Validation Loss: 1.0539835691452026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1677: Training Loss: 0.6806281407674154 Validation Loss: 1.0538712739944458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1678: Training Loss: 0.6803294022878011 Validation Loss: 1.0533605813980103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1679: Training Loss: 0.6801026264826456 Validation Loss: 1.0529903173446655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1680: Training Loss: 0.6786122719446818 Validation Loss: 1.0526272058486938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1681: Training Loss: 0.6784410874048868 Validation Loss: 1.0522602796554565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1682: Training Loss: 0.6781677007675171 Validation Loss: 1.0516469478607178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1683: Training Loss: 0.6768050193786621 Validation Loss: 1.0513650178909302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1684: Training Loss: 0.6779032548268636 Validation Loss: 1.051522970199585\n",
      "Epoch 1685: Training Loss: 0.6772121588389078 Validation Loss: 1.0511562824249268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1686: Training Loss: 0.6768990953763326 Validation Loss: 1.0508077144622803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1687: Training Loss: 0.6762254436810812 Validation Loss: 1.0501960515975952\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1688: Training Loss: 0.6752789616584778 Validation Loss: 1.0497275590896606\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1689: Training Loss: 0.6743640700976054 Validation Loss: 1.049328327178955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1690: Training Loss: 0.6747744480768839 Validation Loss: 1.0489956140518188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1691: Training Loss: 0.6738935907681783 Validation Loss: 1.048421025276184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1692: Training Loss: 0.6760478417078654 Validation Loss: 1.0480635166168213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1693: Training Loss: 0.6731312870979309 Validation Loss: 1.0476151704788208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1694: Training Loss: 0.6736352642377218 Validation Loss: 1.047661542892456\n",
      "Epoch 1695: Training Loss: 0.6724154949188232 Validation Loss: 1.047662615776062\n",
      "Epoch 1696: Training Loss: 0.6727574467658997 Validation Loss: 1.0472469329833984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1697: Training Loss: 0.6709582805633545 Validation Loss: 1.0468816757202148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1698: Training Loss: 0.6725504597028097 Validation Loss: 1.0463544130325317\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1699: Training Loss: 0.6711564461390177 Validation Loss: 1.0459705591201782\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1700: Training Loss: 0.6710825761159261 Validation Loss: 1.045605182647705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1701: Training Loss: 0.6702289978663126 Validation Loss: 1.0452964305877686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1702: Training Loss: 0.6699308355649313 Validation Loss: 1.044811725616455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1703: Training Loss: 0.6696277260780334 Validation Loss: 1.0445297956466675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1704: Training Loss: 0.6685470342636108 Validation Loss: 1.0442328453063965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1705: Training Loss: 0.6687448422114054 Validation Loss: 1.0439214706420898\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1706: Training Loss: 0.6696225802103678 Validation Loss: 1.0434141159057617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1707: Training Loss: 0.6681003570556641 Validation Loss: 1.042816162109375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1708: Training Loss: 0.667720099290212 Validation Loss: 1.0426326990127563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1709: Training Loss: 0.6668620109558105 Validation Loss: 1.0424126386642456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1710: Training Loss: 0.6681750615437826 Validation Loss: 1.042202353477478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1711: Training Loss: 0.6658605138460795 Validation Loss: 1.041946530342102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1712: Training Loss: 0.6647167801856995 Validation Loss: 1.0420671701431274\n",
      "Epoch 1713: Training Loss: 0.6653911670049032 Validation Loss: 1.0418407917022705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1714: Training Loss: 0.6648580233256022 Validation Loss: 1.041433334350586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1715: Training Loss: 0.6645147800445557 Validation Loss: 1.0410315990447998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1716: Training Loss: 0.6626957853635153 Validation Loss: 1.0402748584747314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1717: Training Loss: 0.6642507910728455 Validation Loss: 1.0398871898651123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1718: Training Loss: 0.6638451218605042 Validation Loss: 1.0391845703125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1719: Training Loss: 0.6632829904556274 Validation Loss: 1.0388153791427612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1720: Training Loss: 0.6633477409680685 Validation Loss: 1.038593053817749\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1721: Training Loss: 0.6630884607632955 Validation Loss: 1.038405179977417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1722: Training Loss: 0.6622406244277954 Validation Loss: 1.0382323265075684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1723: Training Loss: 0.6613921920458475 Validation Loss: 1.0377211570739746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1724: Training Loss: 0.6600409348805746 Validation Loss: 1.0375021696090698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1725: Training Loss: 0.6600094040234884 Validation Loss: 1.0370702743530273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1726: Training Loss: 0.6595576008160909 Validation Loss: 1.036802887916565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1727: Training Loss: 0.6603201826413473 Validation Loss: 1.0366170406341553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1728: Training Loss: 0.6587581038475037 Validation Loss: 1.0363929271697998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1729: Training Loss: 0.6585520704587301 Validation Loss: 1.0360902547836304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1730: Training Loss: 0.6593315998713175 Validation Loss: 1.0355331897735596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1731: Training Loss: 0.657690167427063 Validation Loss: 1.0349761247634888\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1732: Training Loss: 0.6571349700291952 Validation Loss: 1.0347789525985718\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1733: Training Loss: 0.657558262348175 Validation Loss: 1.0344908237457275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1734: Training Loss: 0.6575984358787537 Validation Loss: 1.0341386795043945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1735: Training Loss: 0.6558804710706075 Validation Loss: 1.0339834690093994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1736: Training Loss: 0.6582279404004415 Validation Loss: 1.033889651298523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1737: Training Loss: 0.6550214091936747 Validation Loss: 1.0335471630096436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1738: Training Loss: 0.6554277141888937 Validation Loss: 1.0330427885055542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1739: Training Loss: 0.6550706823666891 Validation Loss: 1.032625436782837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1740: Training Loss: 0.6541664997736613 Validation Loss: 1.0322493314743042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1741: Training Loss: 0.654460628827413 Validation Loss: 1.0319340229034424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1742: Training Loss: 0.6536753177642822 Validation Loss: 1.031338095664978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1743: Training Loss: 0.6534636815388998 Validation Loss: 1.031251072883606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1744: Training Loss: 0.6524583101272583 Validation Loss: 1.0307234525680542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1745: Training Loss: 0.6521446506182352 Validation Loss: 1.0305999517440796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1746: Training Loss: 0.6510293285051981 Validation Loss: 1.0302190780639648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1747: Training Loss: 0.6511487166086832 Validation Loss: 1.0301074981689453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1748: Training Loss: 0.6513407627741495 Validation Loss: 1.0297739505767822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1749: Training Loss: 0.6500821709632874 Validation Loss: 1.0292277336120605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1750: Training Loss: 0.6495303908983866 Validation Loss: 1.0290018320083618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1751: Training Loss: 0.6493963599205017 Validation Loss: 1.0287916660308838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1752: Training Loss: 0.6493793328603109 Validation Loss: 1.0287429094314575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1753: Training Loss: 0.6514344612757365 Validation Loss: 1.028204083442688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1754: Training Loss: 0.6500148971875509 Validation Loss: 1.0277119874954224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1755: Training Loss: 0.6479172309239706 Validation Loss: 1.0275483131408691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1756: Training Loss: 0.647519071896871 Validation Loss: 1.0271292924880981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1757: Training Loss: 0.6477924982706705 Validation Loss: 1.0265660285949707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1758: Training Loss: 0.6467035214106241 Validation Loss: 1.0262051820755005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1759: Training Loss: 0.6469208598136902 Validation Loss: 1.025924563407898\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1760: Training Loss: 0.6475439667701721 Validation Loss: 1.025365948677063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1761: Training Loss: 0.6453524827957153 Validation Loss: 1.0252221822738647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1762: Training Loss: 0.6455478072166443 Validation Loss: 1.0249227285385132\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1763: Training Loss: 0.6456841826438904 Validation Loss: 1.0249828100204468\n",
      "Epoch 1764: Training Loss: 0.6448623935381571 Validation Loss: 1.0248122215270996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1765: Training Loss: 0.6431258320808411 Validation Loss: 1.02438485622406\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1766: Training Loss: 0.6421553492546082 Validation Loss: 1.02396559715271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1767: Training Loss: 0.6431290904680887 Validation Loss: 1.0234769582748413\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1768: Training Loss: 0.6418370803197225 Validation Loss: 1.0229690074920654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1769: Training Loss: 0.6424198547999064 Validation Loss: 1.022762656211853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1770: Training Loss: 0.6418441931406657 Validation Loss: 1.0224499702453613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1771: Training Loss: 0.6423099239667257 Validation Loss: 1.0222601890563965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1772: Training Loss: 0.6416554053624471 Validation Loss: 1.021981120109558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1773: Training Loss: 0.6410738428433737 Validation Loss: 1.021541714668274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1774: Training Loss: 0.6402741273244222 Validation Loss: 1.021125078201294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1775: Training Loss: 0.6399457454681396 Validation Loss: 1.0208728313446045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1776: Training Loss: 0.6407979925473531 Validation Loss: 1.020567536354065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1777: Training Loss: 0.6398152510325114 Validation Loss: 1.020203709602356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1778: Training Loss: 0.6390289664268494 Validation Loss: 1.019696831703186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1779: Training Loss: 0.6389856139818827 Validation Loss: 1.0195163488388062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1780: Training Loss: 0.6384168465932211 Validation Loss: 1.0192501544952393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1781: Training Loss: 0.6374448537826538 Validation Loss: 1.0192182064056396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1782: Training Loss: 0.6376920541127523 Validation Loss: 1.0188813209533691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1783: Training Loss: 0.6369396448135376 Validation Loss: 1.0185425281524658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1784: Training Loss: 0.6364915569623312 Validation Loss: 1.0184109210968018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1785: Training Loss: 0.6355042854944865 Validation Loss: 1.0179439783096313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1786: Training Loss: 0.6365892092386881 Validation Loss: 1.0174390077590942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1787: Training Loss: 0.6352817416191101 Validation Loss: 1.0171524286270142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1788: Training Loss: 0.6355506579081217 Validation Loss: 1.016943097114563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1789: Training Loss: 0.6346602638562521 Validation Loss: 1.0165221691131592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1790: Training Loss: 0.6339694062868754 Validation Loss: 1.01616370677948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1791: Training Loss: 0.6330298781394958 Validation Loss: 1.0158743858337402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1792: Training Loss: 0.6333414514859518 Validation Loss: 1.0154279470443726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1793: Training Loss: 0.6333699822425842 Validation Loss: 1.0150665044784546\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1794: Training Loss: 0.6332146724065145 Validation Loss: 1.014835000038147\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1795: Training Loss: 0.6314635475476583 Validation Loss: 1.0145882368087769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1796: Training Loss: 0.6320951382319132 Validation Loss: 1.0142992734909058\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1797: Training Loss: 0.6304775476455688 Validation Loss: 1.014101266860962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1798: Training Loss: 0.6315824786822001 Validation Loss: 1.0136640071868896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1799: Training Loss: 0.6305034160614014 Validation Loss: 1.0133183002471924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1800: Training Loss: 0.6305099924405416 Validation Loss: 1.0130749940872192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1801: Training Loss: 0.6324332157770792 Validation Loss: 1.0128240585327148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1802: Training Loss: 0.6312402089436849 Validation Loss: 1.0127170085906982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1803: Training Loss: 0.631195863087972 Validation Loss: 1.012449860572815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1804: Training Loss: 0.6300400495529175 Validation Loss: 1.0123218297958374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1805: Training Loss: 0.6287965575853983 Validation Loss: 1.011952519416809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1806: Training Loss: 0.6283295353253683 Validation Loss: 1.011569619178772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1807: Training Loss: 0.6272545059521993 Validation Loss: 1.0112251043319702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1808: Training Loss: 0.6270939707756042 Validation Loss: 1.0109047889709473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1809: Training Loss: 0.6273934443791708 Validation Loss: 1.0106905698776245\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1810: Training Loss: 0.6265635291735331 Validation Loss: 1.0104445219039917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1811: Training Loss: 0.6261399388313293 Validation Loss: 1.00987708568573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1812: Training Loss: 0.626314381758372 Validation Loss: 1.0093252658843994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1813: Training Loss: 0.6278203328450521 Validation Loss: 1.0090272426605225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1814: Training Loss: 0.6260628898938497 Validation Loss: 1.0086818933486938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1815: Training Loss: 0.6256804863611857 Validation Loss: 1.0083810091018677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1816: Training Loss: 0.6247332493464152 Validation Loss: 1.008614182472229\n",
      "Epoch 1817: Training Loss: 0.6243322292963663 Validation Loss: 1.0086086988449097\n",
      "Epoch 1818: Training Loss: 0.623757521311442 Validation Loss: 1.0081276893615723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1819: Training Loss: 0.6242886980374655 Validation Loss: 1.0075151920318604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1820: Training Loss: 0.6222196022669474 Validation Loss: 1.006860613822937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1821: Training Loss: 0.6222459276517233 Validation Loss: 1.006617784500122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1822: Training Loss: 0.6220091581344604 Validation Loss: 1.0065118074417114\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1823: Training Loss: 0.6211433211962382 Validation Loss: 1.0063859224319458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1824: Training Loss: 0.623199999332428 Validation Loss: 1.0064222812652588\n",
      "Epoch 1825: Training Loss: 0.6210770606994629 Validation Loss: 1.0059401988983154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1826: Training Loss: 0.6206178665161133 Validation Loss: 1.005759835243225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1827: Training Loss: 0.6204833388328552 Validation Loss: 1.0051426887512207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1828: Training Loss: 0.6203559041023254 Validation Loss: 1.0046296119689941\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1829: Training Loss: 0.6190792322158813 Validation Loss: 1.0042401552200317\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1830: Training Loss: 0.6207718253135681 Validation Loss: 1.0038732290267944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1831: Training Loss: 0.6183478037516276 Validation Loss: 1.0035187005996704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1832: Training Loss: 0.6182148456573486 Validation Loss: 1.0031609535217285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1833: Training Loss: 0.6180069843928019 Validation Loss: 1.0034360885620117\n",
      "Epoch 1834: Training Loss: 0.6185034116109213 Validation Loss: 1.003008484840393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1835: Training Loss: 0.6175915797551473 Validation Loss: 1.002747893333435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1836: Training Loss: 0.617650032043457 Validation Loss: 1.0022861957550049\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1837: Training Loss: 0.6163183252016703 Validation Loss: 1.0023510456085205\n",
      "Epoch 1838: Training Loss: 0.6161384582519531 Validation Loss: 1.0019574165344238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1839: Training Loss: 0.6163356502850851 Validation Loss: 1.0016225576400757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1840: Training Loss: 0.615967333316803 Validation Loss: 1.0010439157485962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1841: Training Loss: 0.6155738234519958 Validation Loss: 1.0008867979049683\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1842: Training Loss: 0.6149289011955261 Validation Loss: 1.0005995035171509\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1843: Training Loss: 0.6153185764948527 Validation Loss: 1.0001599788665771\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1844: Training Loss: 0.614729126294454 Validation Loss: 0.9998412132263184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1845: Training Loss: 0.6136066118876139 Validation Loss: 0.9996291399002075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1846: Training Loss: 0.6132511695226034 Validation Loss: 0.9994316697120667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1847: Training Loss: 0.6142768661181132 Validation Loss: 0.9992230534553528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1848: Training Loss: 0.6117501258850098 Validation Loss: 0.9988877177238464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1849: Training Loss: 0.6121740539868673 Validation Loss: 0.998223066329956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1850: Training Loss: 0.611785352230072 Validation Loss: 0.9979575276374817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1851: Training Loss: 0.6114631493886312 Validation Loss: 0.9976741671562195\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1852: Training Loss: 0.611493984858195 Validation Loss: 0.9976453185081482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1853: Training Loss: 0.6108695467313131 Validation Loss: 0.9973495006561279\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1854: Training Loss: 0.6104269822438558 Validation Loss: 0.9969154000282288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1855: Training Loss: 0.6100014050801595 Validation Loss: 0.9967120885848999\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1856: Training Loss: 0.6103385090827942 Validation Loss: 0.9964934587478638\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1857: Training Loss: 0.6093678871790568 Validation Loss: 0.996337354183197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1858: Training Loss: 0.6096619963645935 Validation Loss: 0.9962043166160583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1859: Training Loss: 0.6100644667943319 Validation Loss: 0.9960371255874634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1860: Training Loss: 0.6105562647183737 Validation Loss: 0.9956030249595642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1861: Training Loss: 0.6076033512751261 Validation Loss: 0.9952766299247742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1862: Training Loss: 0.6071354349454244 Validation Loss: 0.9950780272483826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1863: Training Loss: 0.6070545315742493 Validation Loss: 0.9945579767227173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1864: Training Loss: 0.6063981850941976 Validation Loss: 0.9943746328353882\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1865: Training Loss: 0.6065987745920817 Validation Loss: 0.9942505955696106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1866: Training Loss: 0.6060456236203512 Validation Loss: 0.9939391613006592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1867: Training Loss: 0.6056095163027445 Validation Loss: 0.9935619831085205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1868: Training Loss: 0.605363686879476 Validation Loss: 0.9930959343910217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1869: Training Loss: 0.6054955323537191 Validation Loss: 0.992431104183197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1870: Training Loss: 0.6054983933766683 Validation Loss: 0.9922059774398804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1871: Training Loss: 0.6026118993759155 Validation Loss: 0.9918090105056763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1872: Training Loss: 0.6032722393671671 Validation Loss: 0.9918115139007568\n",
      "Epoch 1873: Training Loss: 0.6032570401827494 Validation Loss: 0.991788387298584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1874: Training Loss: 0.6049944758415222 Validation Loss: 0.9914859533309937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1875: Training Loss: 0.6022850672403971 Validation Loss: 0.9914035797119141\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1876: Training Loss: 0.6026603182156881 Validation Loss: 0.9913250207901001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1877: Training Loss: 0.6023114720980326 Validation Loss: 0.9909567832946777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1878: Training Loss: 0.6025407910346985 Validation Loss: 0.9907641410827637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1879: Training Loss: 0.6017675399780273 Validation Loss: 0.9903441071510315\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1880: Training Loss: 0.6014329195022583 Validation Loss: 0.9900087118148804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1881: Training Loss: 0.6012489795684814 Validation Loss: 0.9893800020217896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1882: Training Loss: 0.6002612511316935 Validation Loss: 0.9888773560523987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1883: Training Loss: 0.5998925964037577 Validation Loss: 0.9886336922645569\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1884: Training Loss: 0.5986539721488953 Validation Loss: 0.9881571531295776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1885: Training Loss: 0.5993480285008749 Validation Loss: 0.9877671003341675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1886: Training Loss: 0.5997277498245239 Validation Loss: 0.9876476526260376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1887: Training Loss: 0.598429282506307 Validation Loss: 0.9876096248626709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1888: Training Loss: 0.5982022285461426 Validation Loss: 0.9876112341880798\n",
      "Epoch 1889: Training Loss: 0.5980927149454752 Validation Loss: 0.9873771071434021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1890: Training Loss: 0.5979938705762228 Validation Loss: 0.9873993992805481\n",
      "Epoch 1891: Training Loss: 0.5972369909286499 Validation Loss: 0.9874128103256226\n",
      "Epoch 1892: Training Loss: 0.5975717107454935 Validation Loss: 0.9869778156280518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1893: Training Loss: 0.5967589219411215 Validation Loss: 0.986316978931427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1894: Training Loss: 0.5968844890594482 Validation Loss: 0.9859110713005066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1895: Training Loss: 0.5959364374478658 Validation Loss: 0.9854281544685364\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1896: Training Loss: 0.5957889954249064 Validation Loss: 0.9852187037467957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1897: Training Loss: 0.5952306588490804 Validation Loss: 0.9846744537353516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1898: Training Loss: 0.5939937432607015 Validation Loss: 0.9845404624938965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1899: Training Loss: 0.5944686532020569 Validation Loss: 0.984389066696167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1900: Training Loss: 0.5942107439041138 Validation Loss: 0.9840680956840515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1901: Training Loss: 0.5940114657084147 Validation Loss: 0.9839388132095337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1902: Training Loss: 0.5934096574783325 Validation Loss: 0.9836617112159729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1903: Training Loss: 0.5932628313700358 Validation Loss: 0.9832565784454346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1904: Training Loss: 0.593175987402598 Validation Loss: 0.9829165935516357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1905: Training Loss: 0.5930000344912211 Validation Loss: 0.9828981757164001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1906: Training Loss: 0.5920683542887369 Validation Loss: 0.9826900959014893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1907: Training Loss: 0.5917732318242391 Validation Loss: 0.9824344515800476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1908: Training Loss: 0.5917685031890869 Validation Loss: 0.9819349050521851\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1909: Training Loss: 0.5904870430628458 Validation Loss: 0.9815932512283325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1910: Training Loss: 0.591094454129537 Validation Loss: 0.9813340306282043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1911: Training Loss: 0.5902770757675171 Validation Loss: 0.981101393699646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1912: Training Loss: 0.5903470913569132 Validation Loss: 0.9809203743934631\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1913: Training Loss: 0.5896371603012085 Validation Loss: 0.9806646704673767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1914: Training Loss: 0.5890578031539917 Validation Loss: 0.9805343151092529\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1915: Training Loss: 0.5890597701072693 Validation Loss: 0.9800452589988708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1916: Training Loss: 0.5889559785525004 Validation Loss: 0.9799101948738098\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1917: Training Loss: 0.5882976651191711 Validation Loss: 0.9794862270355225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1918: Training Loss: 0.588785986105601 Validation Loss: 0.9791121482849121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1919: Training Loss: 0.5872310400009155 Validation Loss: 0.9790180325508118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1920: Training Loss: 0.58879687388738 Validation Loss: 0.979050874710083\n",
      "Epoch 1921: Training Loss: 0.5866979559262594 Validation Loss: 0.9786511659622192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1922: Training Loss: 0.5871969064076742 Validation Loss: 0.9785085320472717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1923: Training Loss: 0.5873732765515646 Validation Loss: 0.9783639907836914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1924: Training Loss: 0.5859530170758566 Validation Loss: 0.977600634098053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1925: Training Loss: 0.5855077703793844 Validation Loss: 0.9773826599121094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1926: Training Loss: 0.5854288736979166 Validation Loss: 0.976936399936676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1927: Training Loss: 0.5849813222885132 Validation Loss: 0.976657509803772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1928: Training Loss: 0.5854068994522095 Validation Loss: 0.9765563011169434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1929: Training Loss: 0.5840368072191874 Validation Loss: 0.9761961102485657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1930: Training Loss: 0.5855627457300822 Validation Loss: 0.9759845733642578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1931: Training Loss: 0.5835814078648885 Validation Loss: 0.9757809042930603\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1932: Training Loss: 0.5820090373357137 Validation Loss: 0.9756699204444885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1933: Training Loss: 0.5832282304763794 Validation Loss: 0.9755964279174805\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1934: Training Loss: 0.5818109114964803 Validation Loss: 0.9751784205436707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1935: Training Loss: 0.5822505156199137 Validation Loss: 0.9749090075492859\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1936: Training Loss: 0.5820524493853251 Validation Loss: 0.9745913147926331\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1937: Training Loss: 0.5823188026746114 Validation Loss: 0.9740564823150635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1938: Training Loss: 0.5813509027163187 Validation Loss: 0.9737469553947449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1939: Training Loss: 0.5819321870803833 Validation Loss: 0.9734293818473816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1940: Training Loss: 0.5809824665387472 Validation Loss: 0.9731442332267761\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1941: Training Loss: 0.5804497400919596 Validation Loss: 0.9730874300003052\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1942: Training Loss: 0.5804355144500732 Validation Loss: 0.972960352897644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1943: Training Loss: 0.5797905524571737 Validation Loss: 0.9727544188499451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1944: Training Loss: 0.5796166261037191 Validation Loss: 0.9721230864524841\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1945: Training Loss: 0.5785236756006876 Validation Loss: 0.9721243977546692\n",
      "Epoch 1946: Training Loss: 0.5785687565803528 Validation Loss: 0.971781313419342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1947: Training Loss: 0.5780420899391174 Validation Loss: 0.9714470505714417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1948: Training Loss: 0.5778703490893046 Validation Loss: 0.9712774157524109\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1949: Training Loss: 0.5774710973103842 Validation Loss: 0.9710196256637573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1950: Training Loss: 0.5771850148836771 Validation Loss: 0.9708195328712463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1951: Training Loss: 0.5767684578895569 Validation Loss: 0.9704828858375549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1952: Training Loss: 0.5768850445747375 Validation Loss: 0.970253586769104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1953: Training Loss: 0.5785078803698221 Validation Loss: 0.9702805876731873\n",
      "Epoch 1954: Training Loss: 0.575964351495107 Validation Loss: 0.9701706767082214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1955: Training Loss: 0.5759304960568746 Validation Loss: 0.969871997833252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1956: Training Loss: 0.5761293172836304 Validation Loss: 0.969687819480896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1957: Training Loss: 0.5749144951502482 Validation Loss: 0.9689763188362122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1958: Training Loss: 0.5779762864112854 Validation Loss: 0.9687740802764893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1959: Training Loss: 0.5747844576835632 Validation Loss: 0.9685665965080261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1960: Training Loss: 0.5746034979820251 Validation Loss: 0.968410074710846\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1961: Training Loss: 0.573702335357666 Validation Loss: 0.9679853916168213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1962: Training Loss: 0.5749242901802063 Validation Loss: 0.9677470326423645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1963: Training Loss: 0.5726041197776794 Validation Loss: 0.9673494100570679\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1964: Training Loss: 0.5726772944132487 Validation Loss: 0.9669764041900635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1965: Training Loss: 0.5726660490036011 Validation Loss: 0.9670140743255615\n",
      "Epoch 1966: Training Loss: 0.5720322330792745 Validation Loss: 0.9668452143669128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1967: Training Loss: 0.5730128884315491 Validation Loss: 0.9664590358734131\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1968: Training Loss: 0.571499764919281 Validation Loss: 0.9660555124282837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1969: Training Loss: 0.5713441570599874 Validation Loss: 0.9658620357513428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1970: Training Loss: 0.5717374285062155 Validation Loss: 0.9657950401306152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1971: Training Loss: 0.5706529418627421 Validation Loss: 0.9655281901359558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1972: Training Loss: 0.5715397596359253 Validation Loss: 0.9652268886566162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1973: Training Loss: 0.5700177550315857 Validation Loss: 0.9649819731712341\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1974: Training Loss: 0.5703401366869608 Validation Loss: 0.9648920893669128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1975: Training Loss: 0.5677852829297384 Validation Loss: 0.964652955532074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1976: Training Loss: 0.5686646302541097 Validation Loss: 0.9642336964607239\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1977: Training Loss: 0.5684757033983866 Validation Loss: 0.9639731049537659\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1978: Training Loss: 0.5704602599143982 Validation Loss: 0.9634939432144165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1979: Training Loss: 0.5687065323193868 Validation Loss: 0.963051438331604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1980: Training Loss: 0.5680500864982605 Validation Loss: 0.9629598259925842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1981: Training Loss: 0.5672528346379598 Validation Loss: 0.9627977609634399\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1982: Training Loss: 0.5694494048754374 Validation Loss: 0.9627726674079895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1983: Training Loss: 0.5682316521803538 Validation Loss: 0.9626567959785461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1984: Training Loss: 0.5661907990773519 Validation Loss: 0.9624429941177368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1985: Training Loss: 0.5660658677419027 Validation Loss: 0.9620856046676636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1986: Training Loss: 0.565745492776235 Validation Loss: 0.9617031812667847\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1987: Training Loss: 0.5649657845497131 Validation Loss: 0.9614153504371643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1988: Training Loss: 0.5654176672299703 Validation Loss: 0.9612719416618347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1989: Training Loss: 0.5645051797231039 Validation Loss: 0.9608723521232605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1990: Training Loss: 0.565526286760966 Validation Loss: 0.9603053331375122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1991: Training Loss: 0.5655679106712341 Validation Loss: 0.9601343274116516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1992: Training Loss: 0.5639739036560059 Validation Loss: 0.9598217606544495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1993: Training Loss: 0.5631895263989767 Validation Loss: 0.9598350524902344\n",
      "Epoch 1994: Training Loss: 0.56342480580012 Validation Loss: 0.9598852396011353\n",
      "Epoch 1995: Training Loss: 0.5627896984418234 Validation Loss: 0.9593175649642944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1996: Training Loss: 0.5629720489184061 Validation Loss: 0.9592357873916626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1997: Training Loss: 0.5624483426411947 Validation Loss: 0.9590249061584473\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1998: Training Loss: 0.5621733864148458 Validation Loss: 0.9586750864982605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1999: Training Loss: 0.5616795023282369 Validation Loss: 0.9585241079330444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2000: Training Loss: 0.5608602166175842 Validation Loss: 0.9580813646316528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2001: Training Loss: 0.560425341129303 Validation Loss: 0.9577515125274658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2002: Training Loss: 0.5611601074536642 Validation Loss: 0.9573403596878052\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2003: Training Loss: 0.5613277753194174 Validation Loss: 0.957455575466156\n",
      "Epoch 2004: Training Loss: 0.5599591732025146 Validation Loss: 0.9574388265609741\n",
      "Epoch 2005: Training Loss: 0.5598093668619791 Validation Loss: 0.9571593403816223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2006: Training Loss: 0.5610817670822144 Validation Loss: 0.9567744731903076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2007: Training Loss: 0.5596727331479391 Validation Loss: 0.9565059542655945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2008: Training Loss: 0.5582659045855204 Validation Loss: 0.956375241279602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2009: Training Loss: 0.5581127007802328 Validation Loss: 0.9556440114974976\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2010: Training Loss: 0.5586846868197123 Validation Loss: 0.9553758502006531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2011: Training Loss: 0.5584035913149515 Validation Loss: 0.9555068016052246\n",
      "Epoch 2012: Training Loss: 0.5574591755867004 Validation Loss: 0.955251157283783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2013: Training Loss: 0.5576040148735046 Validation Loss: 0.9550246000289917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2014: Training Loss: 0.5572558442751566 Validation Loss: 0.9549218416213989\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2015: Training Loss: 0.5554915467898051 Validation Loss: 0.9547067284584045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2016: Training Loss: 0.557567318280538 Validation Loss: 0.9544049501419067\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2017: Training Loss: 0.5554582675298055 Validation Loss: 0.9540655016899109\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2018: Training Loss: 0.5561670859654745 Validation Loss: 0.9540570378303528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2019: Training Loss: 0.5557118455568949 Validation Loss: 0.9538136124610901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2020: Training Loss: 0.5562299092610677 Validation Loss: 0.9534083008766174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2021: Training Loss: 0.5551795562108358 Validation Loss: 0.9531142115592957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2022: Training Loss: 0.5544333259264628 Validation Loss: 0.9529792070388794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2023: Training Loss: 0.5536500612894694 Validation Loss: 0.952777087688446\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2024: Training Loss: 0.5531563957532247 Validation Loss: 0.9523274898529053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2025: Training Loss: 0.5535742044448853 Validation Loss: 0.952110230922699\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2026: Training Loss: 0.5532170335451762 Validation Loss: 0.9518187046051025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2027: Training Loss: 0.5526838997999827 Validation Loss: 0.951482892036438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2028: Training Loss: 0.5534898241360983 Validation Loss: 0.9512509703636169\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2029: Training Loss: 0.5524824659029642 Validation Loss: 0.9509394764900208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2030: Training Loss: 0.5518927574157715 Validation Loss: 0.9508975148200989\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2031: Training Loss: 0.5518300930658976 Validation Loss: 0.9503421187400818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2032: Training Loss: 0.5515583753585815 Validation Loss: 0.9503822922706604\n",
      "Epoch 2033: Training Loss: 0.5509410301844279 Validation Loss: 0.9502026438713074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2034: Training Loss: 0.5508880019187927 Validation Loss: 0.9498860836029053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2035: Training Loss: 0.5505022803942362 Validation Loss: 0.9497493505477905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2036: Training Loss: 0.5505522886912028 Validation Loss: 0.9494606852531433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2037: Training Loss: 0.5491911768913269 Validation Loss: 0.949338972568512\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2038: Training Loss: 0.5495662689208984 Validation Loss: 0.9492335319519043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2039: Training Loss: 0.5496073961257935 Validation Loss: 0.9490720629692078\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2040: Training Loss: 0.5494901537895203 Validation Loss: 0.9487234354019165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2041: Training Loss: 0.5483976801236471 Validation Loss: 0.9482610821723938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2042: Training Loss: 0.547496368487676 Validation Loss: 0.9478110074996948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2043: Training Loss: 0.5478337208429972 Validation Loss: 0.9478240013122559\n",
      "Epoch 2044: Training Loss: 0.5501052439212799 Validation Loss: 0.9476057887077332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2045: Training Loss: 0.5479021271069845 Validation Loss: 0.947409451007843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2046: Training Loss: 0.5474457144737244 Validation Loss: 0.9472858309745789\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2047: Training Loss: 0.5474582115809122 Validation Loss: 0.9472156167030334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2048: Training Loss: 0.5464568932851156 Validation Loss: 0.9466394782066345\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2049: Training Loss: 0.5467051068941752 Validation Loss: 0.9461207985877991\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2050: Training Loss: 0.5458334485689799 Validation Loss: 0.9459888935089111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2051: Training Loss: 0.5458256403605143 Validation Loss: 0.9457854628562927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2052: Training Loss: 0.5455161929130554 Validation Loss: 0.9456028342247009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2053: Training Loss: 0.5450861652692159 Validation Loss: 0.9457787275314331\n",
      "Epoch 2054: Training Loss: 0.546531875928243 Validation Loss: 0.9451013803482056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2055: Training Loss: 0.5445784131685892 Validation Loss: 0.9447675347328186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2056: Training Loss: 0.5440981388092041 Validation Loss: 0.944866418838501\n",
      "Epoch 2057: Training Loss: 0.5441680153210958 Validation Loss: 0.9447075724601746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2058: Training Loss: 0.5433297554651896 Validation Loss: 0.9445824027061462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2059: Training Loss: 0.5434321165084839 Validation Loss: 0.9442682862281799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2060: Training Loss: 0.5440338452657064 Validation Loss: 0.9442120790481567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2061: Training Loss: 0.54317773381869 Validation Loss: 0.9437941312789917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2062: Training Loss: 0.5433342953523 Validation Loss: 0.9437220692634583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2063: Training Loss: 0.5425645510355631 Validation Loss: 0.9434131383895874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2064: Training Loss: 0.5417729417483012 Validation Loss: 0.9431519508361816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2065: Training Loss: 0.5417713522911072 Validation Loss: 0.943210244178772\n",
      "Epoch 2066: Training Loss: 0.540992279847463 Validation Loss: 0.942794144153595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2067: Training Loss: 0.5417588253815969 Validation Loss: 0.9424773454666138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2068: Training Loss: 0.5406106313069662 Validation Loss: 0.9419609308242798\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2069: Training Loss: 0.5415618717670441 Validation Loss: 0.9418051242828369\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2070: Training Loss: 0.5399303237597147 Validation Loss: 0.9413815140724182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2071: Training Loss: 0.5402669707934061 Validation Loss: 0.9411584138870239\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2072: Training Loss: 0.5393807490666708 Validation Loss: 0.9410451650619507\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2073: Training Loss: 0.5392338633537292 Validation Loss: 0.9407737851142883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2074: Training Loss: 0.5386002659797668 Validation Loss: 0.9405485391616821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2075: Training Loss: 0.5387402772903442 Validation Loss: 0.9403687715530396\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2076: Training Loss: 0.5394736925760905 Validation Loss: 0.940012514591217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2077: Training Loss: 0.5377073486646017 Validation Loss: 0.9396843314170837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2078: Training Loss: 0.5371827185153961 Validation Loss: 0.9398720264434814\n",
      "Epoch 2079: Training Loss: 0.5372067292531332 Validation Loss: 0.9399911761283875\n",
      "Epoch 2080: Training Loss: 0.5382696092128754 Validation Loss: 0.939866304397583\n",
      "Epoch 2081: Training Loss: 0.5365307927131653 Validation Loss: 0.9395743608474731\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2082: Training Loss: 0.5373222629229227 Validation Loss: 0.9393013119697571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2083: Training Loss: 0.5365618467330933 Validation Loss: 0.9388301372528076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2084: Training Loss: 0.5355609357357025 Validation Loss: 0.9386240243911743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2085: Training Loss: 0.5353711843490601 Validation Loss: 0.9382131695747375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2086: Training Loss: 0.5357473889986674 Validation Loss: 0.9378165006637573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2087: Training Loss: 0.5360682507356008 Validation Loss: 0.9375579953193665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2088: Training Loss: 0.5346342325210571 Validation Loss: 0.9373745918273926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2089: Training Loss: 0.5350993573665619 Validation Loss: 0.9373172521591187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2090: Training Loss: 0.5356705884138743 Validation Loss: 0.9370946288108826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2091: Training Loss: 0.5340613126754761 Validation Loss: 0.9369552731513977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2092: Training Loss: 0.5350057681401571 Validation Loss: 0.9365296959877014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2093: Training Loss: 0.5323498050371805 Validation Loss: 0.9365780353546143\n",
      "Epoch 2094: Training Loss: 0.5329206387201945 Validation Loss: 0.9360767006874084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2095: Training Loss: 0.5327460964520773 Validation Loss: 0.9361041188240051\n",
      "Epoch 2096: Training Loss: 0.5323592225710551 Validation Loss: 0.9358607530593872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2097: Training Loss: 0.5326298375924429 Validation Loss: 0.9355334043502808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2098: Training Loss: 0.531549314657847 Validation Loss: 0.9354299902915955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2099: Training Loss: 0.5319447020689646 Validation Loss: 0.9352542161941528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2100: Training Loss: 0.5318670570850372 Validation Loss: 0.9347388744354248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2101: Training Loss: 0.5309764345486959 Validation Loss: 0.9342771768569946\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2102: Training Loss: 0.5315819680690765 Validation Loss: 0.9341921806335449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2103: Training Loss: 0.5303812126318613 Validation Loss: 0.9339790344238281\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2104: Training Loss: 0.5288074215253195 Validation Loss: 0.9339596033096313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2105: Training Loss: 0.5290913780530294 Validation Loss: 0.9340190291404724\n",
      "Epoch 2106: Training Loss: 0.5291473070780436 Validation Loss: 0.9336095452308655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2107: Training Loss: 0.5292742153008779 Validation Loss: 0.9334843158721924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2108: Training Loss: 0.5286597808202108 Validation Loss: 0.933533787727356\n",
      "Epoch 2109: Training Loss: 0.5292131106058756 Validation Loss: 0.9335397481918335\n",
      "Epoch 2110: Training Loss: 0.5284635325272878 Validation Loss: 0.9330471754074097\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2111: Training Loss: 0.5278142988681793 Validation Loss: 0.9328842163085938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2112: Training Loss: 0.5280797978242239 Validation Loss: 0.9324504137039185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2113: Training Loss: 0.5273582339286804 Validation Loss: 0.9321649074554443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2114: Training Loss: 0.5272378226121267 Validation Loss: 0.9318724274635315\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2115: Training Loss: 0.5266062716643015 Validation Loss: 0.9315922856330872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2116: Training Loss: 0.5269806385040283 Validation Loss: 0.9314011335372925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2117: Training Loss: 0.5267490347226461 Validation Loss: 0.9309613704681396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2118: Training Loss: 0.5257675250371298 Validation Loss: 0.9306711554527283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2119: Training Loss: 0.5257128477096558 Validation Loss: 0.9305147528648376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2120: Training Loss: 0.5256043672561646 Validation Loss: 0.9302712678909302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2121: Training Loss: 0.5254578292369843 Validation Loss: 0.9302151799201965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2122: Training Loss: 0.5248016913731893 Validation Loss: 0.9301038384437561\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2123: Training Loss: 0.5244800349076589 Validation Loss: 0.9299098253250122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2124: Training Loss: 0.5246572494506836 Validation Loss: 0.9297121167182922\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2125: Training Loss: 0.52432252963384 Validation Loss: 0.9294965267181396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2126: Training Loss: 0.5240395466486613 Validation Loss: 0.9293922185897827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2127: Training Loss: 0.5238264600435892 Validation Loss: 0.9289188385009766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2128: Training Loss: 0.5232863823572794 Validation Loss: 0.9286280870437622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2129: Training Loss: 0.5229499936103821 Validation Loss: 0.9285700917243958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2130: Training Loss: 0.5224721332391103 Validation Loss: 0.9284390807151794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2131: Training Loss: 0.5229009588559469 Validation Loss: 0.9279205799102783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2132: Training Loss: 0.5220483442147573 Validation Loss: 0.927626371383667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2133: Training Loss: 0.5227209726969401 Validation Loss: 0.9277717471122742\n",
      "Epoch 2134: Training Loss: 0.5210125048955282 Validation Loss: 0.9275277256965637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2135: Training Loss: 0.5209076404571533 Validation Loss: 0.9274035692214966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2136: Training Loss: 0.5202651023864746 Validation Loss: 0.9273661971092224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2137: Training Loss: 0.5207901199658712 Validation Loss: 0.9269647598266602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2138: Training Loss: 0.5207554300626119 Validation Loss: 0.9265563488006592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2139: Training Loss: 0.5203902224699656 Validation Loss: 0.9264057278633118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2140: Training Loss: 0.5196004112561544 Validation Loss: 0.9264530539512634\n",
      "Epoch 2141: Training Loss: 0.5192961990833282 Validation Loss: 0.9263772964477539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2142: Training Loss: 0.5198298394680023 Validation Loss: 0.9260446429252625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2143: Training Loss: 0.520029733578364 Validation Loss: 0.9258946776390076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2144: Training Loss: 0.5192908843358358 Validation Loss: 0.9256301522254944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2145: Training Loss: 0.5180966456731161 Validation Loss: 0.925467312335968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2146: Training Loss: 0.5187437931696574 Validation Loss: 0.9249067902565002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2147: Training Loss: 0.5190056264400482 Validation Loss: 0.9243507385253906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2148: Training Loss: 0.5179984072844187 Validation Loss: 0.9240365028381348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2149: Training Loss: 0.5173723300298055 Validation Loss: 0.9238892197608948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2150: Training Loss: 0.517206867535909 Validation Loss: 0.9239058494567871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2151: Training Loss: 0.5184013148148855 Validation Loss: 0.9238694310188293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2152: Training Loss: 0.5158734122912089 Validation Loss: 0.923599898815155\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2153: Training Loss: 0.5162754356861115 Validation Loss: 0.9233795404434204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2154: Training Loss: 0.5160730282465616 Validation Loss: 0.9230777025222778\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2155: Training Loss: 0.5159238974253336 Validation Loss: 0.9231330752372742\n",
      "Epoch 2156: Training Loss: 0.515523225069046 Validation Loss: 0.9227815866470337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2157: Training Loss: 0.5161543488502502 Validation Loss: 0.9226970672607422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2158: Training Loss: 0.5156289239724478 Validation Loss: 0.9225500822067261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2159: Training Loss: 0.5145725607872009 Validation Loss: 0.9225157499313354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2160: Training Loss: 0.5138559738794962 Validation Loss: 0.9221973419189453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2161: Training Loss: 0.5156302154064178 Validation Loss: 0.9218771457672119\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2162: Training Loss: 0.5166600346565247 Validation Loss: 0.9218536019325256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2163: Training Loss: 0.5128444135189056 Validation Loss: 0.9215369820594788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2164: Training Loss: 0.5130609671274821 Validation Loss: 0.9212863445281982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2165: Training Loss: 0.5135217308998108 Validation Loss: 0.9211199879646301\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2166: Training Loss: 0.5132855276266733 Validation Loss: 0.9204616546630859\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2167: Training Loss: 0.5126168529192606 Validation Loss: 0.9202171564102173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2168: Training Loss: 0.5126435160636902 Validation Loss: 0.9200114011764526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2169: Training Loss: 0.5122824311256409 Validation Loss: 0.9199778437614441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2170: Training Loss: 0.5126822392145792 Validation Loss: 0.9195933938026428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2171: Training Loss: 0.5109118521213531 Validation Loss: 0.9197611212730408\n",
      "Epoch 2172: Training Loss: 0.5108188490072886 Validation Loss: 0.9197154641151428\n",
      "Epoch 2173: Training Loss: 0.5108935832977295 Validation Loss: 0.9194847345352173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2174: Training Loss: 0.5112743775049845 Validation Loss: 0.9192265272140503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2175: Training Loss: 0.5106199185053507 Validation Loss: 0.9193877577781677\n",
      "Epoch 2176: Training Loss: 0.5106524229049683 Validation Loss: 0.9191455841064453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2177: Training Loss: 0.5104873478412628 Validation Loss: 0.9187353849411011\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2178: Training Loss: 0.5102702081203461 Validation Loss: 0.9184285998344421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2179: Training Loss: 0.5103477338949839 Validation Loss: 0.9183748364448547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2180: Training Loss: 0.5095368027687073 Validation Loss: 0.9180919528007507\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2181: Training Loss: 0.5080619355042776 Validation Loss: 0.9179273843765259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2182: Training Loss: 0.5086007912953695 Validation Loss: 0.917434811592102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2183: Training Loss: 0.5082628925641378 Validation Loss: 0.917141854763031\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2184: Training Loss: 0.5090634723504385 Validation Loss: 0.9168263077735901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2185: Training Loss: 0.5084697703520457 Validation Loss: 0.9166741371154785\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2186: Training Loss: 0.5074368119239807 Validation Loss: 0.9165100455284119\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2187: Training Loss: 0.5073869923750559 Validation Loss: 0.9164238572120667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2188: Training Loss: 0.5082809428373972 Validation Loss: 0.9165492057800293\n",
      "Epoch 2189: Training Loss: 0.5075456400712332 Validation Loss: 0.916261613368988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2190: Training Loss: 0.5077386101086935 Validation Loss: 0.9159362316131592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2191: Training Loss: 0.5082990129788717 Validation Loss: 0.9157405495643616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2192: Training Loss: 0.5062748789787292 Validation Loss: 0.9155758023262024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2193: Training Loss: 0.5061678687731425 Validation Loss: 0.9154861569404602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2194: Training Loss: 0.5059537490208944 Validation Loss: 0.9153429269790649\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2195: Training Loss: 0.5050662358601888 Validation Loss: 0.9151782393455505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2196: Training Loss: 0.5073195596536001 Validation Loss: 0.9151133298873901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2197: Training Loss: 0.5043107668558756 Validation Loss: 0.9148563742637634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2198: Training Loss: 0.504829486211141 Validation Loss: 0.9144172072410583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2199: Training Loss: 0.5047589937845866 Validation Loss: 0.9139289855957031\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2200: Training Loss: 0.5047069291273752 Validation Loss: 0.9137299656867981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2201: Training Loss: 0.5041140019893646 Validation Loss: 0.9134036302566528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2202: Training Loss: 0.5031381547451019 Validation Loss: 0.9129447340965271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2203: Training Loss: 0.5033616622289022 Validation Loss: 0.9126825332641602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2204: Training Loss: 0.5031488140424093 Validation Loss: 0.9127484560012817\n",
      "Epoch 2205: Training Loss: 0.5021888514359792 Validation Loss: 0.9129241108894348\n",
      "Epoch 2206: Training Loss: 0.5027431348959605 Validation Loss: 0.9128945469856262\n",
      "Epoch 2207: Training Loss: 0.5024337271849314 Validation Loss: 0.912453830242157\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2208: Training Loss: 0.5022520323594412 Validation Loss: 0.9123783707618713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2209: Training Loss: 0.5015937189261118 Validation Loss: 0.9119588732719421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2210: Training Loss: 0.5015497505664825 Validation Loss: 0.9115889072418213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2211: Training Loss: 0.500563790400823 Validation Loss: 0.9114705920219421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2212: Training Loss: 0.5007631480693817 Validation Loss: 0.9113447070121765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2213: Training Loss: 0.500424732764562 Validation Loss: 0.9114317893981934\n",
      "Epoch 2214: Training Loss: 0.500453511873881 Validation Loss: 0.9115055203437805\n",
      "Epoch 2215: Training Loss: 0.5012114147345225 Validation Loss: 0.9113870859146118\n",
      "Epoch 2216: Training Loss: 0.5009786089261373 Validation Loss: 0.9111060500144958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2217: Training Loss: 0.4995614190896352 Validation Loss: 0.9107564687728882\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2218: Training Loss: 0.4985516170660655 Validation Loss: 0.9104930758476257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2219: Training Loss: 0.49852243065834045 Validation Loss: 0.9101589322090149\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2220: Training Loss: 0.4987255831559499 Validation Loss: 0.909990131855011\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2221: Training Loss: 0.4984104533990224 Validation Loss: 0.9097763895988464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2222: Training Loss: 0.49838077028592426 Validation Loss: 0.9093775153160095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2223: Training Loss: 0.4995608429114024 Validation Loss: 0.9091099500656128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2224: Training Loss: 0.49871740738550824 Validation Loss: 0.9088082909584045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2225: Training Loss: 0.4972939093907674 Validation Loss: 0.9086317420005798\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2226: Training Loss: 0.49770177404085797 Validation Loss: 0.9085545539855957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2227: Training Loss: 0.4979969263076782 Validation Loss: 0.9085467457771301\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2228: Training Loss: 0.4962375561396281 Validation Loss: 0.9083359241485596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2229: Training Loss: 0.49647851785024005 Validation Loss: 0.908204972743988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2230: Training Loss: 0.4963980217774709 Validation Loss: 0.9083170294761658\n",
      "Epoch 2231: Training Loss: 0.49640435973803204 Validation Loss: 0.9083484411239624\n",
      "Epoch 2232: Training Loss: 0.4955727756023407 Validation Loss: 0.9080669283866882\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2233: Training Loss: 0.4957130551338196 Validation Loss: 0.9076259732246399\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2234: Training Loss: 0.4950632353623708 Validation Loss: 0.9072695970535278\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2235: Training Loss: 0.49494024117787677 Validation Loss: 0.9071305394172668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2236: Training Loss: 0.49431222677230835 Validation Loss: 0.9068239331245422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2237: Training Loss: 0.49481714765230816 Validation Loss: 0.9065976142883301\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2238: Training Loss: 0.4942064881324768 Validation Loss: 0.9064093828201294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2239: Training Loss: 0.4939065674940745 Validation Loss: 0.9059906005859375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2240: Training Loss: 0.49411208430926007 Validation Loss: 0.9060993194580078\n",
      "Epoch 2241: Training Loss: 0.49347541729609173 Validation Loss: 0.9057950973510742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2242: Training Loss: 0.493525892496109 Validation Loss: 0.9053986668586731\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2243: Training Loss: 0.49265698591868085 Validation Loss: 0.9053887128829956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2244: Training Loss: 0.49318045377731323 Validation Loss: 0.9053240418434143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2245: Training Loss: 0.4916453957557678 Validation Loss: 0.9048894047737122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2246: Training Loss: 0.4934018353621165 Validation Loss: 0.9048474431037903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2247: Training Loss: 0.4914382994174957 Validation Loss: 0.904694676399231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2248: Training Loss: 0.49229711294174194 Validation Loss: 0.9046087861061096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2249: Training Loss: 0.4909987847010295 Validation Loss: 0.9044636487960815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2250: Training Loss: 0.49110734462738037 Validation Loss: 0.9040940999984741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2251: Training Loss: 0.4909765422344208 Validation Loss: 0.9036436080932617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2252: Training Loss: 0.49476097027460736 Validation Loss: 0.9036175608634949\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2253: Training Loss: 0.48981013894081116 Validation Loss: 0.9034290313720703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2254: Training Loss: 0.4930903911590576 Validation Loss: 0.9032241702079773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2255: Training Loss: 0.4908415774504344 Validation Loss: 0.9030922651290894\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2256: Training Loss: 0.48936017354329425 Validation Loss: 0.9029914736747742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2257: Training Loss: 0.4900560478369395 Validation Loss: 0.9030013680458069\n",
      "Epoch 2258: Training Loss: 0.4895869791507721 Validation Loss: 0.9026258587837219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2259: Training Loss: 0.48971159259478253 Validation Loss: 0.9026200771331787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2260: Training Loss: 0.48873775204022724 Validation Loss: 0.9022279381752014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2261: Training Loss: 0.4881983200709025 Validation Loss: 0.9022547602653503\n",
      "Epoch 2262: Training Loss: 0.48830724755922955 Validation Loss: 0.9020417928695679\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2263: Training Loss: 0.4886213342348735 Validation Loss: 0.901759922504425\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2264: Training Loss: 0.4880981544653575 Validation Loss: 0.9017946720123291\n",
      "Epoch 2265: Training Loss: 0.4883084297180176 Validation Loss: 0.9014638066291809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2266: Training Loss: 0.48922059933344525 Validation Loss: 0.9013139009475708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2267: Training Loss: 0.48746705055236816 Validation Loss: 0.9013090133666992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2268: Training Loss: 0.4862480064233144 Validation Loss: 0.9007086157798767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2269: Training Loss: 0.4862760206063588 Validation Loss: 0.900390625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2270: Training Loss: 0.4864463011423747 Validation Loss: 0.9002258777618408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2271: Training Loss: 0.4872005681196849 Validation Loss: 0.9002875089645386\n",
      "Epoch 2272: Training Loss: 0.4857734143733978 Validation Loss: 0.9002259969711304\n",
      "Epoch 2273: Training Loss: 0.4856135845184326 Validation Loss: 0.8999468684196472\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2274: Training Loss: 0.48511529962221783 Validation Loss: 0.8996842503547668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2275: Training Loss: 0.4849542776743571 Validation Loss: 0.8993168473243713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2276: Training Loss: 0.4841867685317993 Validation Loss: 0.8991180658340454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2277: Training Loss: 0.48455748955408734 Validation Loss: 0.8989858627319336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2278: Training Loss: 0.4844210048516591 Validation Loss: 0.8990250825881958\n",
      "Epoch 2279: Training Loss: 0.48574065168698627 Validation Loss: 0.8989447355270386\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2280: Training Loss: 0.48357362548510235 Validation Loss: 0.8984856009483337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2281: Training Loss: 0.48334283630053204 Validation Loss: 0.8984048962593079\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2282: Training Loss: 0.48203455408414203 Validation Loss: 0.8979872465133667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2283: Training Loss: 0.4833581745624542 Validation Loss: 0.8977389335632324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2284: Training Loss: 0.4832132160663605 Validation Loss: 0.8977887630462646\n",
      "Epoch 2285: Training Loss: 0.4826064209143321 Validation Loss: 0.897724986076355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2286: Training Loss: 0.48306257526079815 Validation Loss: 0.8977639675140381\n",
      "Epoch 2287: Training Loss: 0.4815194805463155 Validation Loss: 0.8972315192222595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2288: Training Loss: 0.481556236743927 Validation Loss: 0.8970261216163635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2289: Training Loss: 0.4818800489107768 Validation Loss: 0.8968222141265869\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2290: Training Loss: 0.4812072316805522 Validation Loss: 0.8967592716217041\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2291: Training Loss: 0.4815862774848938 Validation Loss: 0.8966127634048462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2292: Training Loss: 0.4807424545288086 Validation Loss: 0.8966255187988281\n",
      "Epoch 2293: Training Loss: 0.4802875916163127 Validation Loss: 0.8962624669075012\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2294: Training Loss: 0.48026859760284424 Validation Loss: 0.8958605527877808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2295: Training Loss: 0.47981294989585876 Validation Loss: 0.895698606967926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2296: Training Loss: 0.4792662064234416 Validation Loss: 0.8955846428871155\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2297: Training Loss: 0.479701966047287 Validation Loss: 0.8955504894256592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2298: Training Loss: 0.4787616829077403 Validation Loss: 0.8953356146812439\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2299: Training Loss: 0.4793979624907176 Validation Loss: 0.8952388763427734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2300: Training Loss: 0.4788747827212016 Validation Loss: 0.8950383067131042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2301: Training Loss: 0.47832831740379333 Validation Loss: 0.8946813941001892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2302: Training Loss: 0.4789709548155467 Validation Loss: 0.8941996097564697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2303: Training Loss: 0.47935035824775696 Validation Loss: 0.8940809369087219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2304: Training Loss: 0.47788063685099286 Validation Loss: 0.8939386606216431\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2305: Training Loss: 0.47806376218795776 Validation Loss: 0.893815279006958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2306: Training Loss: 0.47661129633585614 Validation Loss: 0.8941847681999207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2307: Training Loss: 0.47678276896476746 Validation Loss: 0.8941039443016052\n",
      "Epoch 2308: Training Loss: 0.4763946334520976 Validation Loss: 0.8938217759132385\n",
      "Epoch 2309: Training Loss: 0.47738399108250934 Validation Loss: 0.893415629863739\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2310: Training Loss: 0.47795162598292035 Validation Loss: 0.8928360342979431\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2311: Training Loss: 0.47604356209437054 Validation Loss: 0.8925970196723938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2312: Training Loss: 0.47576869527498883 Validation Loss: 0.8924885988235474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2313: Training Loss: 0.475896159807841 Validation Loss: 0.8925586938858032\n",
      "Epoch 2314: Training Loss: 0.47594157854715985 Validation Loss: 0.8926287293434143\n",
      "Epoch 2315: Training Loss: 0.47468923528989154 Validation Loss: 0.8923389911651611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2316: Training Loss: 0.47510380546251935 Validation Loss: 0.8918428421020508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2317: Training Loss: 0.4750884771347046 Validation Loss: 0.8918296098709106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2318: Training Loss: 0.47368433078130084 Validation Loss: 0.8916719555854797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2319: Training Loss: 0.47410764296849567 Validation Loss: 0.8914699554443359\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2320: Training Loss: 0.47391751408576965 Validation Loss: 0.8911026120185852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2321: Training Loss: 0.47380032142003375 Validation Loss: 0.8911702632904053\n",
      "Epoch 2322: Training Loss: 0.4739254017670949 Validation Loss: 0.8908300995826721\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2323: Training Loss: 0.4737546543280284 Validation Loss: 0.8908166885375977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2324: Training Loss: 0.47381913661956787 Validation Loss: 0.890468180179596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2325: Training Loss: 0.47366949915885925 Validation Loss: 0.8903178572654724\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2326: Training Loss: 0.47434672713279724 Validation Loss: 0.8905590772628784\n",
      "Epoch 2327: Training Loss: 0.472057044506073 Validation Loss: 0.8904690742492676\n",
      "Epoch 2328: Training Loss: 0.47186176975568134 Validation Loss: 0.890285849571228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2329: Training Loss: 0.47165773312250775 Validation Loss: 0.8899739384651184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2330: Training Loss: 0.4715603490670522 Validation Loss: 0.8899499773979187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2331: Training Loss: 0.4714500904083252 Validation Loss: 0.8899953365325928\n",
      "Epoch 2332: Training Loss: 0.4714645842711131 Validation Loss: 0.8898288011550903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2333: Training Loss: 0.4710116982460022 Validation Loss: 0.8894304037094116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2334: Training Loss: 0.46982205907503766 Validation Loss: 0.8888739347457886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2335: Training Loss: 0.47027815381685895 Validation Loss: 0.8885617256164551\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2336: Training Loss: 0.4697972238063812 Validation Loss: 0.8883878588676453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2337: Training Loss: 0.470065842072169 Validation Loss: 0.8884257674217224\n",
      "Epoch 2338: Training Loss: 0.46947548786799115 Validation Loss: 0.8883300423622131\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2339: Training Loss: 0.46923593680063885 Validation Loss: 0.8882633447647095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2340: Training Loss: 0.4693262080351512 Validation Loss: 0.8879698514938354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2341: Training Loss: 0.4691472351551056 Validation Loss: 0.8877586126327515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2342: Training Loss: 0.46864203612009686 Validation Loss: 0.8874728679656982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2343: Training Loss: 0.46888891855875653 Validation Loss: 0.8873852491378784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2344: Training Loss: 0.4691709876060486 Validation Loss: 0.8872302174568176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2345: Training Loss: 0.4678209920724233 Validation Loss: 0.8871437311172485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2346: Training Loss: 0.467047522465388 Validation Loss: 0.8873001933097839\n",
      "Epoch 2347: Training Loss: 0.4673769374688466 Validation Loss: 0.8868395090103149\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2348: Training Loss: 0.46705759565035504 Validation Loss: 0.8867636919021606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2349: Training Loss: 0.467005451520284 Validation Loss: 0.8865556716918945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2350: Training Loss: 0.4672105809052785 Validation Loss: 0.8866833448410034\n",
      "Epoch 2351: Training Loss: 0.4662281970183055 Validation Loss: 0.8862287402153015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2352: Training Loss: 0.46615925431251526 Validation Loss: 0.8860005736351013\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2353: Training Loss: 0.46595874428749084 Validation Loss: 0.8858215808868408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2354: Training Loss: 0.46648065249125165 Validation Loss: 0.8856278657913208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2355: Training Loss: 0.46535106499989826 Validation Loss: 0.885428249835968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2356: Training Loss: 0.4662730892499288 Validation Loss: 0.8851120471954346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2357: Training Loss: 0.4657701551914215 Validation Loss: 0.8849622011184692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2358: Training Loss: 0.46506984035174054 Validation Loss: 0.8848786950111389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2359: Training Loss: 0.46489569544792175 Validation Loss: 0.8845898509025574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2360: Training Loss: 0.4655201733112335 Validation Loss: 0.884369969367981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2361: Training Loss: 0.4629720250765483 Validation Loss: 0.8843604922294617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2362: Training Loss: 0.46386754512786865 Validation Loss: 0.8842851519584656\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2363: Training Loss: 0.4636032283306122 Validation Loss: 0.8843163847923279\n",
      "Epoch 2364: Training Loss: 0.46377137303352356 Validation Loss: 0.8841519355773926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2365: Training Loss: 0.4638917048772176 Validation Loss: 0.8840333223342896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2366: Training Loss: 0.4630322257677714 Validation Loss: 0.884005069732666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2367: Training Loss: 0.46302353342374164 Validation Loss: 0.883603036403656\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2368: Training Loss: 0.4626819888750712 Validation Loss: 0.8828902244567871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2369: Training Loss: 0.46238664786020917 Validation Loss: 0.8826864361763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2370: Training Loss: 0.4631807903448741 Validation Loss: 0.8830743432044983\n",
      "Epoch 2371: Training Loss: 0.46164413293202716 Validation Loss: 0.8828415274620056\n",
      "Epoch 2372: Training Loss: 0.4618329207102458 Validation Loss: 0.8826498985290527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2373: Training Loss: 0.46129416426022846 Validation Loss: 0.8826042413711548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2374: Training Loss: 0.46095962325731915 Validation Loss: 0.8825423121452332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2375: Training Loss: 0.46177205443382263 Validation Loss: 0.8825721740722656\n",
      "Epoch 2376: Training Loss: 0.4604331354300181 Validation Loss: 0.8820544481277466\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2377: Training Loss: 0.4605698088804881 Validation Loss: 0.8818785548210144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2378: Training Loss: 0.45968876282374066 Validation Loss: 0.8818458318710327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2379: Training Loss: 0.46097039182980853 Validation Loss: 0.8817402124404907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2380: Training Loss: 0.46111324429512024 Validation Loss: 0.8813863396644592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2381: Training Loss: 0.4595682422320048 Validation Loss: 0.881161093711853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2382: Training Loss: 0.461161067088445 Validation Loss: 0.8808806538581848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2383: Training Loss: 0.4587528010209401 Validation Loss: 0.8808789253234863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2384: Training Loss: 0.45806803305943805 Validation Loss: 0.8808422088623047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2385: Training Loss: 0.4585926632086436 Validation Loss: 0.8808301687240601\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2386: Training Loss: 0.4583962559700012 Validation Loss: 0.8807872533798218\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2387: Training Loss: 0.4582029680411021 Validation Loss: 0.8804993033409119\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2388: Training Loss: 0.45802391568819684 Validation Loss: 0.8801875710487366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2389: Training Loss: 0.4589765767256419 Validation Loss: 0.8799650073051453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2390: Training Loss: 0.457461675008138 Validation Loss: 0.8800520300865173\n",
      "Epoch 2391: Training Loss: 0.45730366309483844 Validation Loss: 0.8796995878219604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2392: Training Loss: 0.45678970217704773 Validation Loss: 0.879572331905365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2393: Training Loss: 0.4565938313802083 Validation Loss: 0.8793329000473022\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2394: Training Loss: 0.45694491267204285 Validation Loss: 0.8791033625602722\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2395: Training Loss: 0.45673781633377075 Validation Loss: 0.8789818286895752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2396: Training Loss: 0.45631158351898193 Validation Loss: 0.8787139058113098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2397: Training Loss: 0.4567863841851552 Validation Loss: 0.8787546157836914\n",
      "Epoch 2398: Training Loss: 0.45571304361025494 Validation Loss: 0.8781393766403198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2399: Training Loss: 0.45504292845726013 Validation Loss: 0.8781400918960571\n",
      "Epoch 2400: Training Loss: 0.4557954668998718 Validation Loss: 0.878035306930542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2401: Training Loss: 0.454997460047404 Validation Loss: 0.8777962923049927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2402: Training Loss: 0.45566898584365845 Validation Loss: 0.8778026103973389\n",
      "Epoch 2403: Training Loss: 0.45569052298863727 Validation Loss: 0.8779333233833313\n",
      "Epoch 2404: Training Loss: 0.45445196827252704 Validation Loss: 0.8777826428413391\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2405: Training Loss: 0.454032023747762 Validation Loss: 0.8777218461036682\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2406: Training Loss: 0.45399028062820435 Validation Loss: 0.8776386976242065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2407: Training Loss: 0.4543417990207672 Validation Loss: 0.8773545026779175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2408: Training Loss: 0.4543125629425049 Validation Loss: 0.8770219087600708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2409: Training Loss: 0.4537867307662964 Validation Loss: 0.8765762448310852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2410: Training Loss: 0.45366738239924115 Validation Loss: 0.876447856426239\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2411: Training Loss: 0.4525616268316905 Validation Loss: 0.8763896226882935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2412: Training Loss: 0.4534241457780202 Validation Loss: 0.8764585256576538\n",
      "Epoch 2413: Training Loss: 0.454148530960083 Validation Loss: 0.8763465285301208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2414: Training Loss: 0.4515167872111003 Validation Loss: 0.876114010810852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2415: Training Loss: 0.45193397998809814 Validation Loss: 0.8758627772331238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2416: Training Loss: 0.45121344923973083 Validation Loss: 0.8753973841667175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2417: Training Loss: 0.4515769879023234 Validation Loss: 0.8750903010368347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2418: Training Loss: 0.452226201693217 Validation Loss: 0.8750506043434143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2419: Training Loss: 0.4507620632648468 Validation Loss: 0.8748176693916321\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2420: Training Loss: 0.4508766730626424 Validation Loss: 0.8747568726539612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2421: Training Loss: 0.4505645235379537 Validation Loss: 0.8749659061431885\n",
      "Epoch 2422: Training Loss: 0.4505303204059601 Validation Loss: 0.8751218318939209\n",
      "Epoch 2423: Training Loss: 0.4504708747069041 Validation Loss: 0.8751682639122009\n",
      "Epoch 2424: Training Loss: 0.4496682782967885 Validation Loss: 0.8749123215675354\n",
      "Epoch 2425: Training Loss: 0.4499608079592387 Validation Loss: 0.8747900128364563\n",
      "Epoch 2426: Training Loss: 0.4505518575509389 Validation Loss: 0.8743277192115784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2427: Training Loss: 0.4494628806908925 Validation Loss: 0.87419593334198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2428: Training Loss: 0.44926632444063824 Validation Loss: 0.8736878037452698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2429: Training Loss: 0.45038848121960956 Validation Loss: 0.8737093806266785\n",
      "Epoch 2430: Training Loss: 0.4488863945007324 Validation Loss: 0.8738840222358704\n",
      "Epoch 2431: Training Loss: 0.4488667647043864 Validation Loss: 0.8733896613121033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2432: Training Loss: 0.44803447524706524 Validation Loss: 0.8734145760536194\n",
      "Epoch 2433: Training Loss: 0.449909249941508 Validation Loss: 0.8728939294815063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2434: Training Loss: 0.44770633180936176 Validation Loss: 0.8725597858428955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2435: Training Loss: 0.44745386640230816 Validation Loss: 0.8724037408828735\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2436: Training Loss: 0.4503372808297475 Validation Loss: 0.8725371956825256\n",
      "Epoch 2437: Training Loss: 0.4471432864665985 Validation Loss: 0.8726230263710022\n",
      "Epoch 2438: Training Loss: 0.44778935114542645 Validation Loss: 0.8725829720497131\n",
      "Epoch 2439: Training Loss: 0.447959840297699 Validation Loss: 0.8722783923149109\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2440: Training Loss: 0.44720345735549927 Validation Loss: 0.8724777102470398\n",
      "Epoch 2441: Training Loss: 0.4460141360759735 Validation Loss: 0.8720045685768127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2442: Training Loss: 0.44685372710227966 Validation Loss: 0.8717648386955261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2443: Training Loss: 0.44626252849896747 Validation Loss: 0.8718149065971375\n",
      "Epoch 2444: Training Loss: 0.44625643889109295 Validation Loss: 0.8717758655548096\n",
      "Epoch 2445: Training Loss: 0.44642869631449383 Validation Loss: 0.8714922070503235\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2446: Training Loss: 0.44542328516642254 Validation Loss: 0.8710432648658752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2447: Training Loss: 0.44586246212323505 Validation Loss: 0.8707170486450195\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2448: Training Loss: 0.44441555937131244 Validation Loss: 0.8704183101654053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2449: Training Loss: 0.44485190510749817 Validation Loss: 0.8702868819236755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2450: Training Loss: 0.44434194763501483 Validation Loss: 0.8703246116638184\n",
      "Epoch 2451: Training Loss: 0.4445921679337819 Validation Loss: 0.8701308369636536\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2452: Training Loss: 0.4452986717224121 Validation Loss: 0.8699355125427246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2453: Training Loss: 0.4440537889798482 Validation Loss: 0.8700338006019592\n",
      "Epoch 2454: Training Loss: 0.4443760613600413 Validation Loss: 0.8698955774307251\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2455: Training Loss: 0.44405336181322735 Validation Loss: 0.8697775602340698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2456: Training Loss: 0.44302334388097125 Validation Loss: 0.8695166110992432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2457: Training Loss: 0.44311489661534625 Validation Loss: 0.8693641424179077\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2458: Training Loss: 0.4427010516325633 Validation Loss: 0.8692497611045837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2459: Training Loss: 0.4429582754770915 Validation Loss: 0.8690326809883118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2460: Training Loss: 0.4422460397084554 Validation Loss: 0.8688328266143799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2461: Training Loss: 0.44210996230443317 Validation Loss: 0.8688132166862488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2462: Training Loss: 0.44207386175791424 Validation Loss: 0.8685899972915649\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2463: Training Loss: 0.4417988459269206 Validation Loss: 0.8683822751045227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2464: Training Loss: 0.44106783469518024 Validation Loss: 0.8684077262878418\n",
      "Epoch 2465: Training Loss: 0.4413638412952423 Validation Loss: 0.8683950901031494\n",
      "Epoch 2466: Training Loss: 0.44226281841595966 Validation Loss: 0.868311882019043\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2467: Training Loss: 0.44084758559862774 Validation Loss: 0.8680588603019714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2468: Training Loss: 0.44129165013631183 Validation Loss: 0.8677896857261658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2469: Training Loss: 0.44157100717226666 Validation Loss: 0.8676103353500366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2470: Training Loss: 0.440263311068217 Validation Loss: 0.8674523830413818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2471: Training Loss: 0.43967822194099426 Validation Loss: 0.8673922419548035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2472: Training Loss: 0.43976373473803204 Validation Loss: 0.8675053715705872\n",
      "Epoch 2473: Training Loss: 0.4387611746788025 Validation Loss: 0.8670825362205505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2474: Training Loss: 0.43950602412223816 Validation Loss: 0.8667375445365906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2475: Training Loss: 0.43870503703753155 Validation Loss: 0.8666453957557678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2476: Training Loss: 0.4390667180220286 Validation Loss: 0.8664055466651917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2477: Training Loss: 0.4404407739639282 Validation Loss: 0.8662134408950806\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2478: Training Loss: 0.4385053912798564 Validation Loss: 0.8660942912101746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2479: Training Loss: 0.4386575520038605 Validation Loss: 0.8661893606185913\n",
      "Epoch 2480: Training Loss: 0.43846144278844196 Validation Loss: 0.8662055730819702\n",
      "Epoch 2481: Training Loss: 0.43784376978874207 Validation Loss: 0.8660079836845398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2482: Training Loss: 0.4373640716075897 Validation Loss: 0.8656668663024902\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2483: Training Loss: 0.4376530746618907 Validation Loss: 0.8654760122299194\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2484: Training Loss: 0.43695196509361267 Validation Loss: 0.8651878237724304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2485: Training Loss: 0.43706415096918744 Validation Loss: 0.8655171394348145\n",
      "Epoch 2486: Training Loss: 0.43888161579767865 Validation Loss: 0.8654392957687378\n",
      "Epoch 2487: Training Loss: 0.43635563055674237 Validation Loss: 0.8652659058570862\n",
      "Epoch 2488: Training Loss: 0.43899574875831604 Validation Loss: 0.8649186491966248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2489: Training Loss: 0.43857253591219586 Validation Loss: 0.8649287223815918\n",
      "Epoch 2490: Training Loss: 0.436733881632487 Validation Loss: 0.8645238280296326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2491: Training Loss: 0.4365653892358144 Validation Loss: 0.8643894791603088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2492: Training Loss: 0.43607159455617267 Validation Loss: 0.8639287948608398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2493: Training Loss: 0.43744270006815594 Validation Loss: 0.8639312386512756\n",
      "Epoch 2494: Training Loss: 0.4349737564722697 Validation Loss: 0.8641559481620789\n",
      "Epoch 2495: Training Loss: 0.4339994589487712 Validation Loss: 0.8640142679214478\n",
      "Epoch 2496: Training Loss: 0.43532484769821167 Validation Loss: 0.8637259602546692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2497: Training Loss: 0.43492189049720764 Validation Loss: 0.8633772730827332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2498: Training Loss: 0.4343022207419078 Validation Loss: 0.8634942770004272\n",
      "Epoch 2499: Training Loss: 0.43462173144022626 Validation Loss: 0.8628857731819153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2500: Training Loss: 0.4341131846110026 Validation Loss: 0.8628975749015808\n",
      "Epoch 2501: Training Loss: 0.4339730242888133 Validation Loss: 0.8627708554267883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2502: Training Loss: 0.4350542724132538 Validation Loss: 0.8633003234863281\n",
      "Epoch 2503: Training Loss: 0.43345699707667035 Validation Loss: 0.8632810711860657\n",
      "Epoch 2504: Training Loss: 0.4329957862695058 Validation Loss: 0.8629888892173767\n",
      "Epoch 2505: Training Loss: 0.43281644582748413 Validation Loss: 0.8629956841468811\n",
      "Epoch 2506: Training Loss: 0.433471143245697 Validation Loss: 0.8626625537872314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2507: Training Loss: 0.43295108278592426 Validation Loss: 0.8621309399604797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2508: Training Loss: 0.4328693350156148 Validation Loss: 0.861783504486084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2509: Training Loss: 0.4320211013158162 Validation Loss: 0.8616628050804138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2510: Training Loss: 0.43168559670448303 Validation Loss: 0.8613031506538391\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2511: Training Loss: 0.4324771463871002 Validation Loss: 0.8612958788871765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2512: Training Loss: 0.43198129534721375 Validation Loss: 0.8613166809082031\n",
      "Epoch 2513: Training Loss: 0.43172112107276917 Validation Loss: 0.8612717986106873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2514: Training Loss: 0.4312226176261902 Validation Loss: 0.8613740801811218\n",
      "Epoch 2515: Training Loss: 0.43143269419670105 Validation Loss: 0.8611490726470947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2516: Training Loss: 0.4304063816865285 Validation Loss: 0.8607928156852722\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2517: Training Loss: 0.4328347643216451 Validation Loss: 0.8607720732688904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2518: Training Loss: 0.4307175278663635 Validation Loss: 0.8606798648834229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2519: Training Loss: 0.4311923086643219 Validation Loss: 0.8600996136665344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2520: Training Loss: 0.4300327996412913 Validation Loss: 0.8598708510398865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2521: Training Loss: 0.430446724096934 Validation Loss: 0.8599459528923035\n",
      "Epoch 2522: Training Loss: 0.43071630597114563 Validation Loss: 0.8599802255630493\n",
      "Epoch 2523: Training Loss: 0.4282185236612956 Validation Loss: 0.8596547842025757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2524: Training Loss: 0.4291183054447174 Validation Loss: 0.8596897125244141\n",
      "Epoch 2525: Training Loss: 0.42927975455919903 Validation Loss: 0.8596146702766418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2526: Training Loss: 0.42822081844011944 Validation Loss: 0.859353244304657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2527: Training Loss: 0.42969590425491333 Validation Loss: 0.8590363264083862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2528: Training Loss: 0.4295395414034526 Validation Loss: 0.8594047427177429\n",
      "Epoch 2529: Training Loss: 0.4285646180311839 Validation Loss: 0.8591808676719666\n",
      "Epoch 2530: Training Loss: 0.42866163452466327 Validation Loss: 0.8594023585319519\n",
      "Epoch 2531: Training Loss: 0.42773672938346863 Validation Loss: 0.859377920627594\n",
      "Epoch 2532: Training Loss: 0.42710455258687335 Validation Loss: 0.8588621020317078\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2533: Training Loss: 0.42774882912635803 Validation Loss: 0.858496367931366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2534: Training Loss: 0.42717064420382184 Validation Loss: 0.8583132028579712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2535: Training Loss: 0.42711228132247925 Validation Loss: 0.858168363571167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2536: Training Loss: 0.42807162801424664 Validation Loss: 0.8579107522964478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2537: Training Loss: 0.427137811978658 Validation Loss: 0.8578073382377625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2538: Training Loss: 0.4258398612340291 Validation Loss: 0.8578484654426575\n",
      "Epoch 2539: Training Loss: 0.4269094069798787 Validation Loss: 0.8576478958129883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2540: Training Loss: 0.4273085395495097 Validation Loss: 0.8571913242340088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2541: Training Loss: 0.4260164499282837 Validation Loss: 0.8567201495170593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2542: Training Loss: 0.42566848794619244 Validation Loss: 0.8567278385162354\n",
      "Epoch 2543: Training Loss: 0.42666911085446674 Validation Loss: 0.8569478988647461\n",
      "Epoch 2544: Training Loss: 0.4252539873123169 Validation Loss: 0.85695481300354\n",
      "Epoch 2545: Training Loss: 0.42519257465998334 Validation Loss: 0.8567038774490356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2546: Training Loss: 0.42460641264915466 Validation Loss: 0.85645991563797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2547: Training Loss: 0.4238276183605194 Validation Loss: 0.8563721776008606\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2548: Training Loss: 0.4249597390492757 Validation Loss: 0.8560139536857605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2549: Training Loss: 0.42446215947469074 Validation Loss: 0.8558251261711121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2550: Training Loss: 0.4240002930164337 Validation Loss: 0.8559536337852478\n",
      "Epoch 2551: Training Loss: 0.4231110413869222 Validation Loss: 0.8559865355491638\n",
      "Epoch 2552: Training Loss: 0.4237349331378937 Validation Loss: 0.8561000227928162\n",
      "Epoch 2553: Training Loss: 0.4236113727092743 Validation Loss: 0.855889618396759\n",
      "Epoch 2554: Training Loss: 0.4231812258561452 Validation Loss: 0.855677604675293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2555: Training Loss: 0.42333577076594037 Validation Loss: 0.8550078272819519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2556: Training Loss: 0.4228683412075043 Validation Loss: 0.8549754619598389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2557: Training Loss: 0.42311616738637287 Validation Loss: 0.8551445007324219\n",
      "Epoch 2558: Training Loss: 0.42267051339149475 Validation Loss: 0.8549641966819763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2559: Training Loss: 0.4220587412516276 Validation Loss: 0.8549229502677917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2560: Training Loss: 0.4229095975557963 Validation Loss: 0.8550094962120056\n",
      "Epoch 2561: Training Loss: 0.42241740226745605 Validation Loss: 0.854810357093811\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2562: Training Loss: 0.4242374897003174 Validation Loss: 0.8549516797065735\n",
      "Epoch 2563: Training Loss: 0.4219282964865367 Validation Loss: 0.8543531894683838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2564: Training Loss: 0.4210752646128337 Validation Loss: 0.8541319370269775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2565: Training Loss: 0.4210517505804698 Validation Loss: 0.8537883162498474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2566: Training Loss: 0.4238194425900777 Validation Loss: 0.8533457517623901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2567: Training Loss: 0.4202846984068553 Validation Loss: 0.8533704280853271\n",
      "Epoch 2568: Training Loss: 0.42293665806452435 Validation Loss: 0.8533542156219482\n",
      "Epoch 2569: Training Loss: 0.41999102632204693 Validation Loss: 0.8534684181213379\n",
      "Epoch 2570: Training Loss: 0.4197254578272502 Validation Loss: 0.8533387184143066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2571: Training Loss: 0.4198829034964244 Validation Loss: 0.8531250953674316\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2572: Training Loss: 0.41950350999832153 Validation Loss: 0.8528769016265869\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2573: Training Loss: 0.4194822708765666 Validation Loss: 0.8527024388313293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2574: Training Loss: 0.41950992743174237 Validation Loss: 0.8526052832603455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2575: Training Loss: 0.4191122353076935 Validation Loss: 0.8527821898460388\n",
      "Epoch 2576: Training Loss: 0.4194924831390381 Validation Loss: 0.8526799082756042\n",
      "Epoch 2577: Training Loss: 0.41881871223449707 Validation Loss: 0.852737307548523\n",
      "Epoch 2578: Training Loss: 0.4184735417366028 Validation Loss: 0.8526725769042969\n",
      "Epoch 2579: Training Loss: 0.41856513420740765 Validation Loss: 0.852155864238739\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2580: Training Loss: 0.4190268615881602 Validation Loss: 0.8517038226127625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2581: Training Loss: 0.41856370369593304 Validation Loss: 0.8515564799308777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2582: Training Loss: 0.4196820855140686 Validation Loss: 0.8515415191650391\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2583: Training Loss: 0.4176582296689351 Validation Loss: 0.8512756824493408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2584: Training Loss: 0.417844424645106 Validation Loss: 0.8513586521148682\n",
      "Epoch 2585: Training Loss: 0.4181820551554362 Validation Loss: 0.8513598442077637\n",
      "Epoch 2586: Training Loss: 0.41706445813179016 Validation Loss: 0.8512251377105713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2587: Training Loss: 0.41764317949612934 Validation Loss: 0.8509871363639832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2588: Training Loss: 0.41569047172864276 Validation Loss: 0.8506832122802734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2589: Training Loss: 0.41849063833554584 Validation Loss: 0.850818395614624\n",
      "Epoch 2590: Training Loss: 0.4167988101641337 Validation Loss: 0.8508240580558777\n",
      "Epoch 2591: Training Loss: 0.41634322206179303 Validation Loss: 0.8504349589347839\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2592: Training Loss: 0.41582690676053363 Validation Loss: 0.8502827882766724\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2593: Training Loss: 0.4167613089084625 Validation Loss: 0.8500532507896423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2594: Training Loss: 0.4152032236258189 Validation Loss: 0.8499094843864441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2595: Training Loss: 0.41552674770355225 Validation Loss: 0.8496441841125488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2596: Training Loss: 0.41529348492622375 Validation Loss: 0.8494282960891724\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2597: Training Loss: 0.4159227212270101 Validation Loss: 0.8494661450386047\n",
      "Epoch 2598: Training Loss: 0.41510483622550964 Validation Loss: 0.8496208190917969\n",
      "Epoch 2599: Training Loss: 0.4150720139344533 Validation Loss: 0.8496755957603455\n",
      "Epoch 2600: Training Loss: 0.4146624406178792 Validation Loss: 0.8494511246681213\n",
      "Epoch 2601: Training Loss: 0.4150535762310028 Validation Loss: 0.8492106199264526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2602: Training Loss: 0.4145330588022868 Validation Loss: 0.8490941524505615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2603: Training Loss: 0.4142308533191681 Validation Loss: 0.8488948941230774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2604: Training Loss: 0.41510653495788574 Validation Loss: 0.8484064340591431\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2605: Training Loss: 0.41311901807785034 Validation Loss: 0.8485594987869263\n",
      "Epoch 2606: Training Loss: 0.4127141733964284 Validation Loss: 0.8485841751098633\n",
      "Epoch 2607: Training Loss: 0.4131890833377838 Validation Loss: 0.8482993245124817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2608: Training Loss: 0.4128921727339427 Validation Loss: 0.8484374284744263\n",
      "Epoch 2609: Training Loss: 0.41312630971272785 Validation Loss: 0.8482574820518494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2610: Training Loss: 0.41528183221817017 Validation Loss: 0.8481617569923401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2611: Training Loss: 0.41347674528757733 Validation Loss: 0.8479287624359131\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2612: Training Loss: 0.41146833697954815 Validation Loss: 0.8477708101272583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2613: Training Loss: 0.41239585479100543 Validation Loss: 0.847553551197052\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2614: Training Loss: 0.41210055351257324 Validation Loss: 0.8474107980728149\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2615: Training Loss: 0.4116702775160472 Validation Loss: 0.8472162485122681\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2616: Training Loss: 0.41204161445299786 Validation Loss: 0.8471179604530334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2617: Training Loss: 0.4115708569685618 Validation Loss: 0.847023606300354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2618: Training Loss: 0.41110434134801227 Validation Loss: 0.8468345999717712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2619: Training Loss: 0.41132497787475586 Validation Loss: 0.8466240763664246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2620: Training Loss: 0.411119947830836 Validation Loss: 0.8464792370796204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2621: Training Loss: 0.41105515758196515 Validation Loss: 0.8463454842567444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2622: Training Loss: 0.4104847311973572 Validation Loss: 0.8463689088821411\n",
      "Epoch 2623: Training Loss: 0.4104238748550415 Validation Loss: 0.8463475108146667\n",
      "Epoch 2624: Training Loss: 0.41064082582791644 Validation Loss: 0.8461639285087585\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2625: Training Loss: 0.41051965951919556 Validation Loss: 0.8457575440406799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2626: Training Loss: 0.4104672968387604 Validation Loss: 0.845551609992981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2627: Training Loss: 0.4098055164019267 Validation Loss: 0.8456080555915833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2628: Training Loss: 0.4092276593049367 Validation Loss: 0.8452737927436829\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2629: Training Loss: 0.4099919895331065 Validation Loss: 0.8455645442008972\n",
      "Epoch 2630: Training Loss: 0.4094160695870717 Validation Loss: 0.8453627824783325\n",
      "Epoch 2631: Training Loss: 0.4103494882583618 Validation Loss: 0.8453544974327087\n",
      "Epoch 2632: Training Loss: 0.4088662366072337 Validation Loss: 0.8453020453453064\n",
      "Epoch 2633: Training Loss: 0.40887131293614704 Validation Loss: 0.8451334238052368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2634: Training Loss: 0.40951239069302875 Validation Loss: 0.845068633556366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2635: Training Loss: 0.4099508921305339 Validation Loss: 0.8447202444076538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2636: Training Loss: 0.4079635242621104 Validation Loss: 0.8447265028953552\n",
      "Epoch 2637: Training Loss: 0.4085979461669922 Validation Loss: 0.844491183757782\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2638: Training Loss: 0.4075607160727183 Validation Loss: 0.8440595865249634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2639: Training Loss: 0.4072616795698802 Validation Loss: 0.8437017798423767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2640: Training Loss: 0.4070316056410472 Validation Loss: 0.8436459898948669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2641: Training Loss: 0.40712328751881915 Validation Loss: 0.8435370326042175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2642: Training Loss: 0.40710264444351196 Validation Loss: 0.8435894250869751\n",
      "Epoch 2643: Training Loss: 0.4067157208919525 Validation Loss: 0.8435174822807312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2644: Training Loss: 0.40626553694407147 Validation Loss: 0.8435919880867004\n",
      "Epoch 2645: Training Loss: 0.40590181946754456 Validation Loss: 0.843294084072113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2646: Training Loss: 0.4065786898136139 Validation Loss: 0.8432844281196594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2647: Training Loss: 0.40580983956654865 Validation Loss: 0.8431864976882935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2648: Training Loss: 0.4059131145477295 Validation Loss: 0.8430160284042358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2649: Training Loss: 0.4050367971261342 Validation Loss: 0.8428637385368347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2650: Training Loss: 0.4050591289997101 Validation Loss: 0.8428148031234741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2651: Training Loss: 0.4056590000788371 Validation Loss: 0.8427038192749023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2652: Training Loss: 0.40529865026474 Validation Loss: 0.8426591157913208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2653: Training Loss: 0.40522100528081256 Validation Loss: 0.8423734903335571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2654: Training Loss: 0.4032915234565735 Validation Loss: 0.8421816229820251\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2655: Training Loss: 0.40535857280095416 Validation Loss: 0.8422393202781677\n",
      "Epoch 2656: Training Loss: 0.4043245812257131 Validation Loss: 0.8421741724014282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2657: Training Loss: 0.4040638705094655 Validation Loss: 0.8417001366615295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2658: Training Loss: 0.40420523285865784 Validation Loss: 0.8415751457214355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2659: Training Loss: 0.4039488037427266 Validation Loss: 0.8414452075958252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2660: Training Loss: 0.4038378596305847 Validation Loss: 0.8413422703742981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2661: Training Loss: 0.4032673239707947 Validation Loss: 0.8411052823066711\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2662: Training Loss: 0.4027656714121501 Validation Loss: 0.8410729169845581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2663: Training Loss: 0.40462546547253925 Validation Loss: 0.8413972854614258\n",
      "Epoch 2664: Training Loss: 0.4027971227963765 Validation Loss: 0.841051459312439\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2665: Training Loss: 0.4028763771057129 Validation Loss: 0.8410152196884155\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2666: Training Loss: 0.40287800629933673 Validation Loss: 0.8408403396606445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2667: Training Loss: 0.4021461208661397 Validation Loss: 0.8408932089805603\n",
      "Epoch 2668: Training Loss: 0.4024361769358317 Validation Loss: 0.8406133055686951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2669: Training Loss: 0.40183547139167786 Validation Loss: 0.8402314782142639\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2670: Training Loss: 0.4024743934472402 Validation Loss: 0.8400264382362366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2671: Training Loss: 0.4027503927548726 Validation Loss: 0.8397286534309387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2672: Training Loss: 0.4041963716348012 Validation Loss: 0.8399398326873779\n",
      "Epoch 2673: Training Loss: 0.40108880400657654 Validation Loss: 0.8400536179542542\n",
      "Epoch 2674: Training Loss: 0.40113574266433716 Validation Loss: 0.8402479290962219\n",
      "Epoch 2675: Training Loss: 0.4012755751609802 Validation Loss: 0.840255618095398\n",
      "Epoch 2676: Training Loss: 0.40086646874745685 Validation Loss: 0.8398354053497314\n",
      "Epoch 2677: Training Loss: 0.40065468351046246 Validation Loss: 0.8395296335220337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2678: Training Loss: 0.4010404348373413 Validation Loss: 0.8393802046775818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2679: Training Loss: 0.4001537263393402 Validation Loss: 0.8392273783683777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2680: Training Loss: 0.400496244430542 Validation Loss: 0.83878093957901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2681: Training Loss: 0.39995434880256653 Validation Loss: 0.838574230670929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2682: Training Loss: 0.3996141354242961 Validation Loss: 0.8386092185974121\n",
      "Epoch 2683: Training Loss: 0.40034733215967816 Validation Loss: 0.8382013440132141\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2684: Training Loss: 0.3993913233280182 Validation Loss: 0.838231086730957\n",
      "Epoch 2685: Training Loss: 0.39941181739171344 Validation Loss: 0.8381461501121521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2686: Training Loss: 0.3991013964017232 Validation Loss: 0.8378927111625671\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2687: Training Loss: 0.39978983004887897 Validation Loss: 0.8382719159126282\n",
      "Epoch 2688: Training Loss: 0.39879316091537476 Validation Loss: 0.8383827209472656\n",
      "Epoch 2689: Training Loss: 0.3987066348393758 Validation Loss: 0.8382831811904907\n",
      "Epoch 2690: Training Loss: 0.39856473604838055 Validation Loss: 0.8380665183067322\n",
      "Epoch 2691: Training Loss: 0.3980270226796468 Validation Loss: 0.8378376364707947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2692: Training Loss: 0.39793333411216736 Validation Loss: 0.8376697301864624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2693: Training Loss: 0.3994126319885254 Validation Loss: 0.8373470306396484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2694: Training Loss: 0.39814894398053485 Validation Loss: 0.8371357321739197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2695: Training Loss: 0.39860297242800397 Validation Loss: 0.8369246125221252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2696: Training Loss: 0.39746323227882385 Validation Loss: 0.8367047309875488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2697: Training Loss: 0.39720505475997925 Validation Loss: 0.836844265460968\n",
      "Epoch 2698: Training Loss: 0.3969538112481435 Validation Loss: 0.8370952606201172\n",
      "Epoch 2699: Training Loss: 0.39696286122004193 Validation Loss: 0.8368279933929443\n",
      "Epoch 2700: Training Loss: 0.39662718772888184 Validation Loss: 0.8364373445510864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2701: Training Loss: 0.395480473836263 Validation Loss: 0.8361901044845581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2702: Training Loss: 0.39647047718365985 Validation Loss: 0.8357052803039551\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2703: Training Loss: 0.3960566719373067 Validation Loss: 0.8359024524688721\n",
      "Epoch 2704: Training Loss: 0.39970187346140545 Validation Loss: 0.8360161781311035\n",
      "Epoch 2705: Training Loss: 0.3962251643339793 Validation Loss: 0.8358323574066162\n",
      "Epoch 2706: Training Loss: 0.3961422344048818 Validation Loss: 0.8357253670692444\n",
      "Epoch 2707: Training Loss: 0.3957907756169637 Validation Loss: 0.8360369801521301\n",
      "Epoch 2708: Training Loss: 0.39560718337694806 Validation Loss: 0.8359434604644775\n",
      "Epoch 2709: Training Loss: 0.3953547378381093 Validation Loss: 0.8355088233947754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2710: Training Loss: 0.3952298363049825 Validation Loss: 0.8352877497673035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2711: Training Loss: 0.39469196399052936 Validation Loss: 0.835336446762085\n",
      "Epoch 2712: Training Loss: 0.3953853249549866 Validation Loss: 0.8350449204444885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2713: Training Loss: 0.39512675007184345 Validation Loss: 0.8347727656364441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2714: Training Loss: 0.3954979379971822 Validation Loss: 0.8348067998886108\n",
      "Epoch 2715: Training Loss: 0.39383355776468915 Validation Loss: 0.8345485329627991\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2716: Training Loss: 0.3936629096666972 Validation Loss: 0.8344718217849731\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2717: Training Loss: 0.393815815448761 Validation Loss: 0.8348529934883118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2718: Training Loss: 0.3943727910518646 Validation Loss: 0.8343747854232788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2719: Training Loss: 0.39378999670346576 Validation Loss: 0.833797812461853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2720: Training Loss: 0.3942004144191742 Validation Loss: 0.833882212638855\n",
      "Epoch 2721: Training Loss: 0.39333392182985943 Validation Loss: 0.8339448571205139\n",
      "Epoch 2722: Training Loss: 0.39332003394762677 Validation Loss: 0.834122359752655\n",
      "Epoch 2723: Training Loss: 0.3944612244764964 Validation Loss: 0.8342564105987549\n",
      "Epoch 2724: Training Loss: 0.39227717121442157 Validation Loss: 0.834116518497467\n",
      "Epoch 2725: Training Loss: 0.3930703004201253 Validation Loss: 0.8340027332305908\n",
      "Epoch 2726: Training Loss: 0.3927191495895386 Validation Loss: 0.8337277770042419\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2727: Training Loss: 0.39264806111653644 Validation Loss: 0.8332730531692505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2728: Training Loss: 0.39172306656837463 Validation Loss: 0.8329049944877625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2729: Training Loss: 0.39281604687372845 Validation Loss: 0.8326812386512756\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2730: Training Loss: 0.38973313570022583 Validation Loss: 0.8325576782226562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2731: Training Loss: 0.3914884626865387 Validation Loss: 0.8325761556625366\n",
      "Epoch 2732: Training Loss: 0.39115973313649494 Validation Loss: 0.8328790664672852\n",
      "Epoch 2733: Training Loss: 0.3907579481601715 Validation Loss: 0.832821249961853\n",
      "Epoch 2734: Training Loss: 0.39072343707084656 Validation Loss: 0.8327836990356445\n",
      "Epoch 2735: Training Loss: 0.39156806468963623 Validation Loss: 0.832549512386322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2736: Training Loss: 0.3902894854545593 Validation Loss: 0.8322381377220154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2737: Training Loss: 0.39065665006637573 Validation Loss: 0.8318066596984863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2738: Training Loss: 0.39081700642903644 Validation Loss: 0.8318565487861633\n",
      "Epoch 2739: Training Loss: 0.3901737531026204 Validation Loss: 0.8318463563919067\n",
      "Epoch 2740: Training Loss: 0.39001696308453876 Validation Loss: 0.8316311240196228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2741: Training Loss: 0.38980670770009357 Validation Loss: 0.8313575983047485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2742: Training Loss: 0.39018791913986206 Validation Loss: 0.8311237096786499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2743: Training Loss: 0.3895770112673442 Validation Loss: 0.8310825228691101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2744: Training Loss: 0.389946977297465 Validation Loss: 0.8309776186943054\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2745: Training Loss: 0.3884902099768321 Validation Loss: 0.8311113119125366\n",
      "Epoch 2746: Training Loss: 0.39001408219337463 Validation Loss: 0.8310540318489075\n",
      "Epoch 2747: Training Loss: 0.3880918125311534 Validation Loss: 0.8308958411216736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2748: Training Loss: 0.3889029026031494 Validation Loss: 0.8312305212020874\n",
      "Epoch 2749: Training Loss: 0.38901352882385254 Validation Loss: 0.8313052654266357\n",
      "Epoch 2750: Training Loss: 0.38919687271118164 Validation Loss: 0.8314542770385742\n",
      "Epoch 2751: Training Loss: 0.3904304504394531 Validation Loss: 0.8315029144287109\n",
      "Epoch 2752: Training Loss: 0.3884895046552022 Validation Loss: 0.8310540318489075\n",
      "Epoch 2753: Training Loss: 0.38841121395428974 Validation Loss: 0.8306156992912292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2754: Training Loss: 0.38798073927561444 Validation Loss: 0.830384373664856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2755: Training Loss: 0.3871018588542938 Validation Loss: 0.8300409317016602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2756: Training Loss: 0.388434499502182 Validation Loss: 0.8297156691551208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2757: Training Loss: 0.3872788151105245 Validation Loss: 0.8297218084335327\n",
      "Epoch 2758: Training Loss: 0.3871944149335225 Validation Loss: 0.8294081687927246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2759: Training Loss: 0.38788684209187824 Validation Loss: 0.8295876979827881\n",
      "Epoch 2760: Training Loss: 0.38700493176778156 Validation Loss: 0.829558253288269\n",
      "Epoch 2761: Training Loss: 0.3872450391451518 Validation Loss: 0.8290607929229736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2762: Training Loss: 0.3865099052588145 Validation Loss: 0.8290494084358215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2763: Training Loss: 0.38560714324315387 Validation Loss: 0.8288758397102356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2764: Training Loss: 0.38611756761868793 Validation Loss: 0.8288424015045166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2765: Training Loss: 0.38599270582199097 Validation Loss: 0.8289699554443359\n",
      "Epoch 2766: Training Loss: 0.38530447085698444 Validation Loss: 0.828813374042511\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2767: Training Loss: 0.3863256573677063 Validation Loss: 0.8287575840950012\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2768: Training Loss: 0.3867190480232239 Validation Loss: 0.828825056552887\n",
      "Epoch 2769: Training Loss: 0.3851526578267415 Validation Loss: 0.8287285566329956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2770: Training Loss: 0.3870855967203776 Validation Loss: 0.828835666179657\n",
      "Epoch 2771: Training Loss: 0.3850855628649394 Validation Loss: 0.8283310532569885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2772: Training Loss: 0.3850114941596985 Validation Loss: 0.8279183506965637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2773: Training Loss: 0.38565873106320697 Validation Loss: 0.8279145956039429\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2774: Training Loss: 0.38471295436223346 Validation Loss: 0.8278428316116333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2775: Training Loss: 0.38412344455718994 Validation Loss: 0.8280381560325623\n",
      "Epoch 2776: Training Loss: 0.38385026653607685 Validation Loss: 0.8279266953468323\n",
      "Epoch 2777: Training Loss: 0.38387178381284076 Validation Loss: 0.8275539875030518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2778: Training Loss: 0.3840423325697581 Validation Loss: 0.8269740343093872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2779: Training Loss: 0.384013166030248 Validation Loss: 0.8271600604057312\n",
      "Epoch 2780: Training Loss: 0.3835209409395854 Validation Loss: 0.8271656632423401\n",
      "Epoch 2781: Training Loss: 0.3844946026802063 Validation Loss: 0.8273136615753174\n",
      "Epoch 2782: Training Loss: 0.3829314410686493 Validation Loss: 0.8271732926368713\n",
      "Epoch 2783: Training Loss: 0.3830716609954834 Validation Loss: 0.8271741271018982\n",
      "Epoch 2784: Training Loss: 0.38436047236124676 Validation Loss: 0.826878547668457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2785: Training Loss: 0.3833594024181366 Validation Loss: 0.8267584443092346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2786: Training Loss: 0.3827304045359294 Validation Loss: 0.8264254927635193\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2787: Training Loss: 0.38219772775967914 Validation Loss: 0.8259623646736145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2788: Training Loss: 0.3822128673394521 Validation Loss: 0.8261359333992004\n",
      "Epoch 2789: Training Loss: 0.38222623864809674 Validation Loss: 0.8264228105545044\n",
      "Epoch 2790: Training Loss: 0.3817945718765259 Validation Loss: 0.826180636882782\n",
      "Epoch 2791: Training Loss: 0.3817487955093384 Validation Loss: 0.8261724710464478\n",
      "Epoch 2792: Training Loss: 0.381806343793869 Validation Loss: 0.825782060623169\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2793: Training Loss: 0.38168104489644367 Validation Loss: 0.8256260752677917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2794: Training Loss: 0.3814318577448527 Validation Loss: 0.8256238102912903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2795: Training Loss: 0.3810123801231384 Validation Loss: 0.8255445957183838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2796: Training Loss: 0.38048456112543744 Validation Loss: 0.8254653215408325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2797: Training Loss: 0.3814383347829183 Validation Loss: 0.8250638842582703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2798: Training Loss: 0.38068800171216327 Validation Loss: 0.8249570727348328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2799: Training Loss: 0.38059860467910767 Validation Loss: 0.8249755501747131\n",
      "Epoch 2800: Training Loss: 0.38081632057825726 Validation Loss: 0.8249993324279785\n",
      "Epoch 2801: Training Loss: 0.3802947799364726 Validation Loss: 0.8248178958892822\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2802: Training Loss: 0.3808778425057729 Validation Loss: 0.8250710368156433\n",
      "Epoch 2803: Training Loss: 0.3811010817686717 Validation Loss: 0.825036346912384\n",
      "Epoch 2804: Training Loss: 0.37921247879664105 Validation Loss: 0.8245745897293091\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2805: Training Loss: 0.37923310200373334 Validation Loss: 0.8243160843849182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2806: Training Loss: 0.38032613197962445 Validation Loss: 0.8241671919822693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2807: Training Loss: 0.3795536359151204 Validation Loss: 0.824240505695343\n",
      "Epoch 2808: Training Loss: 0.3795755406220754 Validation Loss: 0.8242303133010864\n",
      "Epoch 2809: Training Loss: 0.37906330823898315 Validation Loss: 0.8240246176719666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2810: Training Loss: 0.37735747297604877 Validation Loss: 0.8237916827201843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2811: Training Loss: 0.3791053593158722 Validation Loss: 0.8238046765327454\n",
      "Epoch 2812: Training Loss: 0.3786594768365224 Validation Loss: 0.8234716653823853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2813: Training Loss: 0.3784889181454976 Validation Loss: 0.8234395980834961\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2814: Training Loss: 0.3783634503682454 Validation Loss: 0.8236916065216064\n",
      "Epoch 2815: Training Loss: 0.3793097833792369 Validation Loss: 0.8234203457832336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2816: Training Loss: 0.37782947222391766 Validation Loss: 0.8233430981636047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2817: Training Loss: 0.37732144196828205 Validation Loss: 0.8231942057609558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2818: Training Loss: 0.37743974725405377 Validation Loss: 0.8229787945747375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2819: Training Loss: 0.37730294466018677 Validation Loss: 0.8231199383735657\n",
      "Epoch 2820: Training Loss: 0.37762006123860675 Validation Loss: 0.8230432271957397\n",
      "Epoch 2821: Training Loss: 0.37739553054173786 Validation Loss: 0.8229370713233948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2822: Training Loss: 0.3768955171108246 Validation Loss: 0.8229165077209473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2823: Training Loss: 0.37726740042368573 Validation Loss: 0.8227993845939636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2824: Training Loss: 0.37717122832934064 Validation Loss: 0.8225671648979187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2825: Training Loss: 0.37652642528216046 Validation Loss: 0.8224977850914001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2826: Training Loss: 0.37622661391894024 Validation Loss: 0.8224260807037354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2827: Training Loss: 0.37623244524002075 Validation Loss: 0.8218551874160767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2828: Training Loss: 0.37679827213287354 Validation Loss: 0.8214781880378723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2829: Training Loss: 0.37440284093221027 Validation Loss: 0.8213156461715698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2830: Training Loss: 0.3761804401874542 Validation Loss: 0.8213990926742554\n",
      "Epoch 2831: Training Loss: 0.3758050203323364 Validation Loss: 0.8215952515602112\n",
      "Epoch 2832: Training Loss: 0.37441614270210266 Validation Loss: 0.8217875957489014\n",
      "Epoch 2833: Training Loss: 0.37441999713579815 Validation Loss: 0.8216643333435059\n",
      "Epoch 2834: Training Loss: 0.375255823135376 Validation Loss: 0.8217315077781677\n",
      "Epoch 2835: Training Loss: 0.3750949601332347 Validation Loss: 0.821671187877655\n",
      "Epoch 2836: Training Loss: 0.37465718388557434 Validation Loss: 0.8213813900947571\n",
      "Epoch 2837: Training Loss: 0.3748516837755839 Validation Loss: 0.8210350871086121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2838: Training Loss: 0.37491996089617413 Validation Loss: 0.8205063343048096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2839: Training Loss: 0.3744937876860301 Validation Loss: 0.8206952810287476\n",
      "Epoch 2840: Training Loss: 0.37450377146402997 Validation Loss: 0.8204179406166077\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2841: Training Loss: 0.37461015582084656 Validation Loss: 0.8204197883605957\n",
      "Epoch 2842: Training Loss: 0.3745697836081187 Validation Loss: 0.8204256892204285\n",
      "Epoch 2843: Training Loss: 0.3751697738965352 Validation Loss: 0.8204684257507324\n",
      "Epoch 2844: Training Loss: 0.3732438286145528 Validation Loss: 0.8202977180480957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2845: Training Loss: 0.3737216889858246 Validation Loss: 0.820317268371582\n",
      "Epoch 2846: Training Loss: 0.3743795057137807 Validation Loss: 0.8202224969863892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2847: Training Loss: 0.37321870525677997 Validation Loss: 0.8203704357147217\n",
      "Epoch 2848: Training Loss: 0.3731811046600342 Validation Loss: 0.8201342821121216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2849: Training Loss: 0.37212778131167096 Validation Loss: 0.819865882396698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2850: Training Loss: 0.37297843893369037 Validation Loss: 0.8197238445281982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2851: Training Loss: 0.3724546233812968 Validation Loss: 0.8195095062255859\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2852: Training Loss: 0.37341463565826416 Validation Loss: 0.8191238045692444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2853: Training Loss: 0.3725247581799825 Validation Loss: 0.819202721118927\n",
      "Epoch 2854: Training Loss: 0.37194321552912396 Validation Loss: 0.8193907141685486\n",
      "Epoch 2855: Training Loss: 0.3715364336967468 Validation Loss: 0.8190982341766357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2856: Training Loss: 0.3722633123397827 Validation Loss: 0.8193058371543884\n",
      "Epoch 2857: Training Loss: 0.3720866839090983 Validation Loss: 0.8190874457359314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2858: Training Loss: 0.372381071249644 Validation Loss: 0.8190017938613892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2859: Training Loss: 0.3712778389453888 Validation Loss: 0.8185359239578247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2860: Training Loss: 0.37126025557518005 Validation Loss: 0.8181847929954529\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2861: Training Loss: 0.37086909015973407 Validation Loss: 0.8183595538139343\n",
      "Epoch 2862: Training Loss: 0.3708001871903737 Validation Loss: 0.8180251717567444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2863: Training Loss: 0.3706921339035034 Validation Loss: 0.8178497552871704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2864: Training Loss: 0.3716619710127513 Validation Loss: 0.8179101347923279\n",
      "Epoch 2865: Training Loss: 0.3708273470401764 Validation Loss: 0.818193256855011\n",
      "Epoch 2866: Training Loss: 0.37078435222307843 Validation Loss: 0.8180130124092102\n",
      "Epoch 2867: Training Loss: 0.37023454904556274 Validation Loss: 0.8183141350746155\n",
      "Epoch 2868: Training Loss: 0.3716284930706024 Validation Loss: 0.818082869052887\n",
      "Epoch 2869: Training Loss: 0.36971471707026166 Validation Loss: 0.817951500415802\n",
      "Epoch 2870: Training Loss: 0.37017303705215454 Validation Loss: 0.8175097107887268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2871: Training Loss: 0.36940498153368634 Validation Loss: 0.8172007203102112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2872: Training Loss: 0.3695196310679118 Validation Loss: 0.8173200488090515\n",
      "Epoch 2873: Training Loss: 0.3690396845340729 Validation Loss: 0.8172522187232971\n",
      "Epoch 2874: Training Loss: 0.36947205662727356 Validation Loss: 0.8172581791877747\n",
      "Epoch 2875: Training Loss: 0.3694251875082652 Validation Loss: 0.8171308040618896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2876: Training Loss: 0.3685200313727061 Validation Loss: 0.817249596118927\n",
      "Epoch 2877: Training Loss: 0.3692333996295929 Validation Loss: 0.8170059323310852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2878: Training Loss: 0.368358850479126 Validation Loss: 0.8169745802879333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2879: Training Loss: 0.36870097120602924 Validation Loss: 0.8164841532707214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2880: Training Loss: 0.3688683907190959 Validation Loss: 0.8163073658943176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2881: Training Loss: 0.3680013318856557 Validation Loss: 0.8163294792175293\n",
      "Epoch 2882: Training Loss: 0.36874335010846454 Validation Loss: 0.8161431550979614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2883: Training Loss: 0.36777442693710327 Validation Loss: 0.8161357045173645\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2884: Training Loss: 0.36669973532358807 Validation Loss: 0.8160801529884338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2885: Training Loss: 0.3678291141986847 Validation Loss: 0.8160743713378906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2886: Training Loss: 0.36787177125612897 Validation Loss: 0.8159119486808777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2887: Training Loss: 0.36766384045283 Validation Loss: 0.8155724406242371\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2888: Training Loss: 0.36730705698331195 Validation Loss: 0.8154665231704712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2889: Training Loss: 0.3675787349541982 Validation Loss: 0.8153162002563477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2890: Training Loss: 0.366335650285085 Validation Loss: 0.8154885768890381\n",
      "Epoch 2891: Training Loss: 0.36644012729326886 Validation Loss: 0.8155432343482971\n",
      "Epoch 2892: Training Loss: 0.36625227332115173 Validation Loss: 0.8155562281608582\n",
      "Epoch 2893: Training Loss: 0.36768784125645954 Validation Loss: 0.8156395554542542\n",
      "Epoch 2894: Training Loss: 0.36633936564127606 Validation Loss: 0.8159593343734741\n",
      "Epoch 2895: Training Loss: 0.3663231035073598 Validation Loss: 0.8156407475471497\n",
      "Epoch 2896: Training Loss: 0.3662824233373006 Validation Loss: 0.8152698874473572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2897: Training Loss: 0.3675660490989685 Validation Loss: 0.814739465713501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2898: Training Loss: 0.3658442695935567 Validation Loss: 0.8143567442893982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2899: Training Loss: 0.36528633038202923 Validation Loss: 0.814211905002594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2900: Training Loss: 0.3658105432987213 Validation Loss: 0.8147212862968445\n",
      "Epoch 2901: Training Loss: 0.3644104103247325 Validation Loss: 0.8148274421691895\n",
      "Epoch 2902: Training Loss: 0.3666679362456004 Validation Loss: 0.8150837421417236\n",
      "Epoch 2903: Training Loss: 0.36470721165339154 Validation Loss: 0.8146161437034607\n",
      "Epoch 2904: Training Loss: 0.3640786012013753 Validation Loss: 0.8141021132469177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2905: Training Loss: 0.3640841742356618 Validation Loss: 0.8139365315437317\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2906: Training Loss: 0.36519739031791687 Validation Loss: 0.8139680027961731\n",
      "Epoch 2907: Training Loss: 0.3643205463886261 Validation Loss: 0.8138203024864197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2908: Training Loss: 0.3641191323598226 Validation Loss: 0.8136670589447021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2909: Training Loss: 0.3642921845118205 Validation Loss: 0.8134626150131226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2910: Training Loss: 0.36389121413230896 Validation Loss: 0.8134971261024475\n",
      "Epoch 2911: Training Loss: 0.36460907260576886 Validation Loss: 0.8134751915931702\n",
      "Epoch 2912: Training Loss: 0.3636053403218587 Validation Loss: 0.8130490779876709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2913: Training Loss: 0.363912969827652 Validation Loss: 0.8133001923561096\n",
      "Epoch 2914: Training Loss: 0.36386000116666156 Validation Loss: 0.8131124377250671\n",
      "Epoch 2915: Training Loss: 0.363136351108551 Validation Loss: 0.8133421540260315\n",
      "Epoch 2916: Training Loss: 0.3629758556683858 Validation Loss: 0.8131814002990723\n",
      "Epoch 2917: Training Loss: 0.361858993768692 Validation Loss: 0.8130063414573669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2918: Training Loss: 0.3626304566860199 Validation Loss: 0.8126924633979797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2919: Training Loss: 0.3628871937592824 Validation Loss: 0.8123986721038818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2920: Training Loss: 0.3621418575445811 Validation Loss: 0.812277615070343\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2921: Training Loss: 0.36266738176345825 Validation Loss: 0.8127599358558655\n",
      "Epoch 2922: Training Loss: 0.3630457321802775 Validation Loss: 0.8128421306610107\n",
      "Epoch 2923: Training Loss: 0.36159375309944153 Validation Loss: 0.8126652240753174\n",
      "Epoch 2924: Training Loss: 0.36182260513305664 Validation Loss: 0.8123988509178162\n",
      "Epoch 2925: Training Loss: 0.3629351456960042 Validation Loss: 0.8118425011634827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2926: Training Loss: 0.36246904730796814 Validation Loss: 0.8117676973342896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2927: Training Loss: 0.3606833517551422 Validation Loss: 0.8115954399108887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2928: Training Loss: 0.3618062535921733 Validation Loss: 0.8118851184844971\n",
      "Epoch 2929: Training Loss: 0.36153075098991394 Validation Loss: 0.8118906617164612\n",
      "Epoch 2930: Training Loss: 0.3608566224575043 Validation Loss: 0.8116492629051208\n",
      "Epoch 2931: Training Loss: 0.3619101444880168 Validation Loss: 0.8118871450424194\n",
      "Epoch 2932: Training Loss: 0.36031126976013184 Validation Loss: 0.8114660978317261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2933: Training Loss: 0.3606252272923787 Validation Loss: 0.8111103177070618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2934: Training Loss: 0.36027079820632935 Validation Loss: 0.8111878037452698\n",
      "Epoch 2935: Training Loss: 0.36056933800379437 Validation Loss: 0.810610294342041\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2936: Training Loss: 0.3597279687722524 Validation Loss: 0.8108698725700378\n",
      "Epoch 2937: Training Loss: 0.3598225613435109 Validation Loss: 0.8110862374305725\n",
      "Epoch 2938: Training Loss: 0.36085734764734906 Validation Loss: 0.8110179901123047\n",
      "Epoch 2939: Training Loss: 0.3593785564104716 Validation Loss: 0.8107160329818726\n",
      "Epoch 2940: Training Loss: 0.35982775688171387 Validation Loss: 0.8103751540184021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2941: Training Loss: 0.3592732946077983 Validation Loss: 0.8106399774551392\n",
      "Epoch 2942: Training Loss: 0.3593375086784363 Validation Loss: 0.8110021948814392\n",
      "Epoch 2943: Training Loss: 0.35916760563850403 Validation Loss: 0.8107534646987915\n",
      "Epoch 2944: Training Loss: 0.35900922616322833 Validation Loss: 0.8105655908584595\n",
      "Epoch 2945: Training Loss: 0.359299639860789 Validation Loss: 0.8101994395256042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2946: Training Loss: 0.35869212945302326 Validation Loss: 0.8101863265037537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2947: Training Loss: 0.35867932438850403 Validation Loss: 0.8098005056381226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2948: Training Loss: 0.35822543501853943 Validation Loss: 0.809739351272583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2949: Training Loss: 0.35886964201927185 Validation Loss: 0.8096821904182434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2950: Training Loss: 0.35792750120162964 Validation Loss: 0.8096767067909241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2951: Training Loss: 0.3577161729335785 Validation Loss: 0.809754490852356\n",
      "Epoch 2952: Training Loss: 0.3579898774623871 Validation Loss: 0.8099383115768433\n",
      "Epoch 2953: Training Loss: 0.357470840215683 Validation Loss: 0.8096299171447754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2954: Training Loss: 0.357395201921463 Validation Loss: 0.809421181678772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2955: Training Loss: 0.35837037364641827 Validation Loss: 0.8091098666191101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2956: Training Loss: 0.3571934799353282 Validation Loss: 0.808916449546814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2957: Training Loss: 0.3573554257551829 Validation Loss: 0.808679461479187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2958: Training Loss: 0.35711055000623065 Validation Loss: 0.8086928725242615\n",
      "Epoch 2959: Training Loss: 0.35694454113642377 Validation Loss: 0.8087626099586487\n",
      "Epoch 2960: Training Loss: 0.3557329773902893 Validation Loss: 0.8087732791900635\n",
      "Epoch 2961: Training Loss: 0.35746949911117554 Validation Loss: 0.8085522055625916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2962: Training Loss: 0.35760998725891113 Validation Loss: 0.808536946773529\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2963: Training Loss: 0.3558780550956726 Validation Loss: 0.8086252212524414\n",
      "Epoch 2964: Training Loss: 0.35565683245658875 Validation Loss: 0.8086121082305908\n",
      "Epoch 2965: Training Loss: 0.3557006319363912 Validation Loss: 0.8085640072822571\n",
      "Epoch 2966: Training Loss: 0.35696398218472797 Validation Loss: 0.8081740140914917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2967: Training Loss: 0.356477548678716 Validation Loss: 0.8080755472183228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2968: Training Loss: 0.3556138873100281 Validation Loss: 0.8080205321311951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2969: Training Loss: 0.35534430543581647 Validation Loss: 0.8075956106185913\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2970: Training Loss: 0.35503652691841125 Validation Loss: 0.8075646758079529\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2971: Training Loss: 0.3549614151318868 Validation Loss: 0.8079444766044617\n",
      "Epoch 2972: Training Loss: 0.3546122908592224 Validation Loss: 0.8077521920204163\n",
      "Epoch 2973: Training Loss: 0.3549606005350749 Validation Loss: 0.8078624606132507\n",
      "Epoch 2974: Training Loss: 0.3572100301583608 Validation Loss: 0.807441234588623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2975: Training Loss: 0.35487528642018634 Validation Loss: 0.807460367679596\n",
      "Epoch 2976: Training Loss: 0.35479649901390076 Validation Loss: 0.8077124953269958\n",
      "Epoch 2977: Training Loss: 0.3547794719537099 Validation Loss: 0.8074482083320618\n",
      "Epoch 2978: Training Loss: 0.35416383544603985 Validation Loss: 0.8072046041488647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2979: Training Loss: 0.35524951418240863 Validation Loss: 0.80718994140625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2980: Training Loss: 0.35386838515599567 Validation Loss: 0.8068779706954956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2981: Training Loss: 0.354963352282842 Validation Loss: 0.8067037463188171\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2982: Training Loss: 0.35372920831044513 Validation Loss: 0.8068938255310059\n",
      "Epoch 2983: Training Loss: 0.3535550534725189 Validation Loss: 0.8069567680358887\n",
      "Epoch 2984: Training Loss: 0.35292256871859234 Validation Loss: 0.8067394495010376\n",
      "Epoch 2985: Training Loss: 0.35276662309964496 Validation Loss: 0.8065563440322876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2986: Training Loss: 0.35314255952835083 Validation Loss: 0.8064025640487671\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2987: Training Loss: 0.3528907497723897 Validation Loss: 0.8059438467025757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2988: Training Loss: 0.3530118962128957 Validation Loss: 0.8057945966720581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2989: Training Loss: 0.3535504837830861 Validation Loss: 0.8062017560005188\n",
      "Epoch 2990: Training Loss: 0.3527355094750722 Validation Loss: 0.8060747981071472\n",
      "Epoch 2991: Training Loss: 0.35301536321640015 Validation Loss: 0.8060646057128906\n",
      "Epoch 2992: Training Loss: 0.3524521092573802 Validation Loss: 0.8058950901031494\n",
      "Epoch 2993: Training Loss: 0.3524678548177083 Validation Loss: 0.8058918714523315\n",
      "Epoch 2994: Training Loss: 0.3520915110905965 Validation Loss: 0.8058896064758301\n",
      "Epoch 2995: Training Loss: 0.3523606558640798 Validation Loss: 0.8055698275566101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2996: Training Loss: 0.35416290163993835 Validation Loss: 0.8052345514297485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2997: Training Loss: 0.35165685415267944 Validation Loss: 0.8053157925605774\n",
      "Epoch 2998: Training Loss: 0.3524041374524434 Validation Loss: 0.805252194404602\n",
      "Epoch 2999: Training Loss: 0.3513057430585225 Validation Loss: 0.8051415681838989\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3000: Training Loss: 0.35154011845588684 Validation Loss: 0.8051244020462036\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3001: Training Loss: 0.3517843981583913 Validation Loss: 0.8053059577941895\n",
      "Epoch 3002: Training Loss: 0.35101068019866943 Validation Loss: 0.8052074313163757\n",
      "Epoch 3003: Training Loss: 0.3509914775689443 Validation Loss: 0.8049208521842957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3004: Training Loss: 0.3510998487472534 Validation Loss: 0.8043707609176636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3005: Training Loss: 0.350851575533549 Validation Loss: 0.8042252659797668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3006: Training Loss: 0.3500594099362691 Validation Loss: 0.8043200969696045\n",
      "Epoch 3007: Training Loss: 0.35032451152801514 Validation Loss: 0.8044077754020691\n",
      "Epoch 3008: Training Loss: 0.3504272798697154 Validation Loss: 0.8045870065689087\n",
      "Epoch 3009: Training Loss: 0.35018567244211835 Validation Loss: 0.8044087886810303\n",
      "Epoch 3010: Training Loss: 0.35033201177914935 Validation Loss: 0.8042349219322205\n",
      "Epoch 3011: Training Loss: 0.35041417678197223 Validation Loss: 0.804111123085022\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3012: Training Loss: 0.349194198846817 Validation Loss: 0.8042595982551575\n",
      "Epoch 3013: Training Loss: 0.349581519762675 Validation Loss: 0.8041518926620483\n",
      "Epoch 3014: Training Loss: 0.3494827449321747 Validation Loss: 0.8041262626647949\n",
      "Epoch 3015: Training Loss: 0.3492639462153117 Validation Loss: 0.8034800887107849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3016: Training Loss: 0.34839247663815814 Validation Loss: 0.8033114671707153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3017: Training Loss: 0.34858091672261554 Validation Loss: 0.8032772541046143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3018: Training Loss: 0.34944166739781696 Validation Loss: 0.8031888604164124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3019: Training Loss: 0.3487993081410726 Validation Loss: 0.8031231164932251\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3020: Training Loss: 0.34853847821553546 Validation Loss: 0.8029235601425171\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3021: Training Loss: 0.34862369298934937 Validation Loss: 0.8029525279998779\n",
      "Epoch 3022: Training Loss: 0.34836237629254657 Validation Loss: 0.8031539916992188\n",
      "Epoch 3023: Training Loss: 0.3480182985464732 Validation Loss: 0.8032799363136292\n",
      "Epoch 3024: Training Loss: 0.3487139741579692 Validation Loss: 0.8034752011299133\n",
      "Epoch 3025: Training Loss: 0.3481480876604716 Validation Loss: 0.8035470247268677\n",
      "Epoch 3026: Training Loss: 0.3479758898417155 Validation Loss: 0.802657425403595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3027: Training Loss: 0.3479253649711609 Validation Loss: 0.8026102781295776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3028: Training Loss: 0.3477492829163869 Validation Loss: 0.8021590113639832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3029: Training Loss: 0.3476512630780538 Validation Loss: 0.8022507429122925\n",
      "Epoch 3030: Training Loss: 0.34729722142219543 Validation Loss: 0.8025818467140198\n",
      "Epoch 3031: Training Loss: 0.34692633152008057 Validation Loss: 0.8024993538856506\n",
      "Epoch 3032: Training Loss: 0.34731701016426086 Validation Loss: 0.802319347858429\n",
      "Epoch 3033: Training Loss: 0.3476555844148 Validation Loss: 0.8021770119667053\n",
      "Epoch 3034: Training Loss: 0.3479355076948802 Validation Loss: 0.8019558787345886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3035: Training Loss: 0.34752367933591205 Validation Loss: 0.8018364906311035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3036: Training Loss: 0.3464278479417165 Validation Loss: 0.8017805218696594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3037: Training Loss: 0.34656022985776264 Validation Loss: 0.8016994595527649\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3038: Training Loss: 0.34687532981236774 Validation Loss: 0.8017245531082153\n",
      "Epoch 3039: Training Loss: 0.34630540013313293 Validation Loss: 0.8020834922790527\n",
      "Epoch 3040: Training Loss: 0.34670450290044147 Validation Loss: 0.8018702268600464\n",
      "Epoch 3041: Training Loss: 0.3452688157558441 Validation Loss: 0.8017075061798096\n",
      "Epoch 3042: Training Loss: 0.34822327891985577 Validation Loss: 0.8017480969429016\n",
      "Epoch 3043: Training Loss: 0.34605327248573303 Validation Loss: 0.8012266755104065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3044: Training Loss: 0.3460353414217631 Validation Loss: 0.8009627461433411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3045: Training Loss: 0.3462195098400116 Validation Loss: 0.8011871576309204\n",
      "Epoch 3046: Training Loss: 0.345304399728775 Validation Loss: 0.8013057112693787\n",
      "Epoch 3047: Training Loss: 0.3459007243315379 Validation Loss: 0.8010120987892151\n",
      "Epoch 3048: Training Loss: 0.34512556592623395 Validation Loss: 0.8007612824440002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3049: Training Loss: 0.34570540984471637 Validation Loss: 0.8004183173179626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3050: Training Loss: 0.3453512489795685 Validation Loss: 0.8005480766296387\n",
      "Epoch 3051: Training Loss: 0.34494688113530475 Validation Loss: 0.8008922934532166\n",
      "Epoch 3052: Training Loss: 0.3442182739575704 Validation Loss: 0.8006714582443237\n",
      "Epoch 3053: Training Loss: 0.3442783057689667 Validation Loss: 0.8004326224327087\n",
      "Epoch 3054: Training Loss: 0.34423355261484784 Validation Loss: 0.8006290793418884\n",
      "Epoch 3055: Training Loss: 0.3431540826956431 Validation Loss: 0.8005688190460205\n",
      "Epoch 3056: Training Loss: 0.34415168563524884 Validation Loss: 0.8003261089324951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3057: Training Loss: 0.34370139241218567 Validation Loss: 0.8002695441246033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3058: Training Loss: 0.3432873884836833 Validation Loss: 0.8000838756561279\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3059: Training Loss: 0.34352415800094604 Validation Loss: 0.7999305129051208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3060: Training Loss: 0.3436331550280253 Validation Loss: 0.7994737029075623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3061: Training Loss: 0.3439255356788635 Validation Loss: 0.7990636229515076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3062: Training Loss: 0.3432360390822093 Validation Loss: 0.7994301319122314\n",
      "Epoch 3063: Training Loss: 0.34344521164894104 Validation Loss: 0.7993674874305725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3064: Training Loss: 0.3434472680091858 Validation Loss: 0.7996259331703186\n",
      "Epoch 3065: Training Loss: 0.3435063660144806 Validation Loss: 0.7998262047767639\n",
      "Epoch 3066: Training Loss: 0.3429239292939504 Validation Loss: 0.79957115650177\n",
      "Epoch 3067: Training Loss: 0.34258901079495746 Validation Loss: 0.7994397282600403\n",
      "Epoch 3068: Training Loss: 0.3430334726969401 Validation Loss: 0.7994073629379272\n",
      "Epoch 3069: Training Loss: 0.3425697088241577 Validation Loss: 0.7992356419563293\n",
      "Epoch 3070: Training Loss: 0.3422112961610158 Validation Loss: 0.7990858554840088\n",
      "Epoch 3071: Training Loss: 0.3419266839822133 Validation Loss: 0.7989957928657532\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3072: Training Loss: 0.341904620329539 Validation Loss: 0.7987107038497925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3073: Training Loss: 0.34160951773325604 Validation Loss: 0.7984675765037537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3074: Training Loss: 0.34147652983665466 Validation Loss: 0.7984384894371033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3075: Training Loss: 0.3415822684764862 Validation Loss: 0.7985288500785828\n",
      "Epoch 3076: Training Loss: 0.341252197821935 Validation Loss: 0.7985631823539734\n",
      "Epoch 3077: Training Loss: 0.3413265446821849 Validation Loss: 0.798549234867096\n",
      "Epoch 3078: Training Loss: 0.3410838842391968 Validation Loss: 0.7984399795532227\n",
      "Epoch 3079: Training Loss: 0.3410058418909709 Validation Loss: 0.798215389251709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3080: Training Loss: 0.3404577175776164 Validation Loss: 0.7982407808303833\n",
      "Epoch 3081: Training Loss: 0.34091628591219586 Validation Loss: 0.7981564402580261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3082: Training Loss: 0.34152887264887494 Validation Loss: 0.7983255386352539\n",
      "Epoch 3083: Training Loss: 0.34041215976079303 Validation Loss: 0.798438310623169\n",
      "Epoch 3084: Training Loss: 0.3402519126733144 Validation Loss: 0.7982855439186096\n",
      "Epoch 3085: Training Loss: 0.34104519089063007 Validation Loss: 0.7981476783752441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3086: Training Loss: 0.34070496757825214 Validation Loss: 0.7980732321739197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3087: Training Loss: 0.3401952385902405 Validation Loss: 0.7978581786155701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3088: Training Loss: 0.3401185969511668 Validation Loss: 0.7975150942802429\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3089: Training Loss: 0.34001537164052326 Validation Loss: 0.7973107695579529\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3090: Training Loss: 0.3397439221541087 Validation Loss: 0.7973245978355408\n",
      "Epoch 3091: Training Loss: 0.3389644920825958 Validation Loss: 0.7970736026763916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3092: Training Loss: 0.3393535514672597 Validation Loss: 0.7970065474510193\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3093: Training Loss: 0.33934112389882404 Validation Loss: 0.7967603802680969\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3094: Training Loss: 0.33925242225329083 Validation Loss: 0.7967778444290161\n",
      "Epoch 3095: Training Loss: 0.33922308683395386 Validation Loss: 0.79673832654953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3096: Training Loss: 0.33934661746025085 Validation Loss: 0.7969566583633423\n",
      "Epoch 3097: Training Loss: 0.3391544421513875 Validation Loss: 0.7966877818107605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3098: Training Loss: 0.33860985438028973 Validation Loss: 0.7970219850540161\n",
      "Epoch 3099: Training Loss: 0.3388736844062805 Validation Loss: 0.7965490818023682\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3100: Training Loss: 0.33948618173599243 Validation Loss: 0.7964546084403992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3101: Training Loss: 0.33806824684143066 Validation Loss: 0.796491801738739\n",
      "Epoch 3102: Training Loss: 0.3391243815422058 Validation Loss: 0.7965779900550842\n",
      "Epoch 3103: Training Loss: 0.3379824956258138 Validation Loss: 0.7968500852584839\n",
      "Epoch 3104: Training Loss: 0.33806342879931134 Validation Loss: 0.7965084314346313\n",
      "Epoch 3105: Training Loss: 0.33794527252515155 Validation Loss: 0.7964773178100586\n",
      "Epoch 3106: Training Loss: 0.33817702531814575 Validation Loss: 0.7963120937347412\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3107: Training Loss: 0.3374197979768117 Validation Loss: 0.7961096167564392\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3108: Training Loss: 0.33747108777364093 Validation Loss: 0.7958012223243713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3109: Training Loss: 0.33741770188013714 Validation Loss: 0.7958314418792725\n",
      "Epoch 3110: Training Loss: 0.33739907542864483 Validation Loss: 0.7960384488105774\n",
      "Epoch 3111: Training Loss: 0.3372228443622589 Validation Loss: 0.7959057092666626\n",
      "Epoch 3112: Training Loss: 0.3369091947873433 Validation Loss: 0.7954217195510864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3113: Training Loss: 0.33759693304697674 Validation Loss: 0.7951803803443909\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3114: Training Loss: 0.3373543818791707 Validation Loss: 0.7951209545135498\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3115: Training Loss: 0.3364991843700409 Validation Loss: 0.7953869104385376\n",
      "Epoch 3116: Training Loss: 0.336781640847524 Validation Loss: 0.7950708270072937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3117: Training Loss: 0.3364071349302928 Validation Loss: 0.7951010465621948\n",
      "Epoch 3118: Training Loss: 0.33620137969652814 Validation Loss: 0.7953687906265259\n",
      "Epoch 3119: Training Loss: 0.3359777530034383 Validation Loss: 0.7952940464019775\n",
      "Epoch 3120: Training Loss: 0.3363467951615651 Validation Loss: 0.7951116561889648\n",
      "Epoch 3121: Training Loss: 0.3365885217984517 Validation Loss: 0.7952084541320801\n",
      "Epoch 3122: Training Loss: 0.33631474773089093 Validation Loss: 0.7950359582901001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3123: Training Loss: 0.3354286154111226 Validation Loss: 0.7951498627662659\n",
      "Epoch 3124: Training Loss: 0.335471252600352 Validation Loss: 0.7951513528823853\n",
      "Epoch 3125: Training Loss: 0.33541996280352276 Validation Loss: 0.7945460081100464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3126: Training Loss: 0.336960107088089 Validation Loss: 0.7942630052566528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3127: Training Loss: 0.3352187971274058 Validation Loss: 0.7945324182510376\n",
      "Epoch 3128: Training Loss: 0.33484697341918945 Validation Loss: 0.7943843603134155\n",
      "Epoch 3129: Training Loss: 0.33478492498397827 Validation Loss: 0.7942008376121521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3130: Training Loss: 0.33475956320762634 Validation Loss: 0.7939873337745667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3131: Training Loss: 0.33435898025830585 Validation Loss: 0.7941418886184692\n",
      "Epoch 3132: Training Loss: 0.33589765429496765 Validation Loss: 0.7944181561470032\n",
      "Epoch 3133: Training Loss: 0.3347405691941579 Validation Loss: 0.794059693813324\n",
      "Epoch 3134: Training Loss: 0.33444368839263916 Validation Loss: 0.7940142154693604\n",
      "Epoch 3135: Training Loss: 0.33414556582768756 Validation Loss: 0.7940381169319153\n",
      "Epoch 3136: Training Loss: 0.3337985674540202 Validation Loss: 0.7934882044792175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3137: Training Loss: 0.3340799808502197 Validation Loss: 0.7934377789497375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3138: Training Loss: 0.33433110515276593 Validation Loss: 0.7934490442276001\n",
      "Epoch 3139: Training Loss: 0.33375778794288635 Validation Loss: 0.7933003902435303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3140: Training Loss: 0.33344800273577374 Validation Loss: 0.793238639831543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3141: Training Loss: 0.33344754576683044 Validation Loss: 0.7932605743408203\n",
      "Epoch 3142: Training Loss: 0.3337721526622772 Validation Loss: 0.7935919761657715\n",
      "Epoch 3143: Training Loss: 0.3330237865447998 Validation Loss: 0.793595552444458\n",
      "Epoch 3144: Training Loss: 0.33367887139320374 Validation Loss: 0.7935722470283508\n",
      "Epoch 3145: Training Loss: 0.33378591140111286 Validation Loss: 0.792862594127655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3146: Training Loss: 0.33234352866808575 Validation Loss: 0.7930231690406799\n",
      "Epoch 3147: Training Loss: 0.3319029112656911 Validation Loss: 0.7927657961845398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3148: Training Loss: 0.33458565672238666 Validation Loss: 0.7924445271492004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3149: Training Loss: 0.3327510158220927 Validation Loss: 0.7923256754875183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3150: Training Loss: 0.33304961522420246 Validation Loss: 0.792464017868042\n",
      "Epoch 3151: Training Loss: 0.3323623339335124 Validation Loss: 0.7924415469169617\n",
      "Epoch 3152: Training Loss: 0.33219342430432636 Validation Loss: 0.7927792072296143\n",
      "Epoch 3153: Training Loss: 0.33205848932266235 Validation Loss: 0.7928984761238098\n",
      "Epoch 3154: Training Loss: 0.3319332003593445 Validation Loss: 0.7926416993141174\n",
      "Epoch 3155: Training Loss: 0.3319970667362213 Validation Loss: 0.7923678159713745\n",
      "Epoch 3156: Training Loss: 0.33266714215278625 Validation Loss: 0.7924213409423828\n",
      "Epoch 3157: Training Loss: 0.3319433530171712 Validation Loss: 0.7921411395072937\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3158: Training Loss: 0.3319046000639598 Validation Loss: 0.7921448349952698\n",
      "Epoch 3159: Training Loss: 0.3309975067774455 Validation Loss: 0.7923772931098938\n",
      "Epoch 3160: Training Loss: 0.3315585156281789 Validation Loss: 0.792072057723999\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3161: Training Loss: 0.33198704322179157 Validation Loss: 0.7919567823410034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3162: Training Loss: 0.33147892355918884 Validation Loss: 0.7917828559875488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3163: Training Loss: 0.330841064453125 Validation Loss: 0.7916575074195862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3164: Training Loss: 0.3296179970105489 Validation Loss: 0.7914663553237915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3165: Training Loss: 0.33126771450042725 Validation Loss: 0.7915610074996948\n",
      "Epoch 3166: Training Loss: 0.33023707071940106 Validation Loss: 0.7915167808532715\n",
      "Epoch 3167: Training Loss: 0.33001284797986347 Validation Loss: 0.791120171546936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3168: Training Loss: 0.3301855226357778 Validation Loss: 0.7912306189537048\n",
      "Epoch 3169: Training Loss: 0.3302149176597595 Validation Loss: 0.7914254665374756\n",
      "Epoch 3170: Training Loss: 0.3298754294713338 Validation Loss: 0.7910996675491333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3171: Training Loss: 0.32978875438372296 Validation Loss: 0.7911964058876038\n",
      "Epoch 3172: Training Loss: 0.33096712827682495 Validation Loss: 0.7911564707756042\n",
      "Epoch 3173: Training Loss: 0.32972385485967 Validation Loss: 0.790838360786438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3174: Training Loss: 0.32968997955322266 Validation Loss: 0.7907369136810303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3175: Training Loss: 0.329630712668101 Validation Loss: 0.7907465100288391\n",
      "Epoch 3176: Training Loss: 0.3291811943054199 Validation Loss: 0.7903575301170349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3177: Training Loss: 0.3289923469225566 Validation Loss: 0.7902668714523315\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3178: Training Loss: 0.3291325569152832 Validation Loss: 0.790344774723053\n",
      "Epoch 3179: Training Loss: 0.32920854290326435 Validation Loss: 0.790492594242096\n",
      "Epoch 3180: Training Loss: 0.3298398454984029 Validation Loss: 0.7902074456214905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3181: Training Loss: 0.3293036421140035 Validation Loss: 0.7903384566307068\n",
      "Epoch 3182: Training Loss: 0.3282683591047923 Validation Loss: 0.790259599685669\n",
      "Epoch 3183: Training Loss: 0.3284921546777089 Validation Loss: 0.7902371287345886\n",
      "Epoch 3184: Training Loss: 0.3281907339890798 Validation Loss: 0.7902313470840454\n",
      "Epoch 3185: Training Loss: 0.3283173243204753 Validation Loss: 0.7901038527488708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3186: Training Loss: 0.3290039698282878 Validation Loss: 0.7900398969650269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3187: Training Loss: 0.32771749297777814 Validation Loss: 0.7900109887123108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3188: Training Loss: 0.32825950781504315 Validation Loss: 0.7899292707443237\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3189: Training Loss: 0.3278117875258128 Validation Loss: 0.7897418737411499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3190: Training Loss: 0.327430784702301 Validation Loss: 0.7898229956626892\n",
      "Epoch 3191: Training Loss: 0.32764256993929547 Validation Loss: 0.7896274924278259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3192: Training Loss: 0.32730453213055927 Validation Loss: 0.7895471453666687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3193: Training Loss: 0.3274416426817576 Validation Loss: 0.7896642088890076\n",
      "Epoch 3194: Training Loss: 0.32831624150276184 Validation Loss: 0.7894693613052368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3195: Training Loss: 0.32759760816891986 Validation Loss: 0.7896286249160767\n",
      "Epoch 3196: Training Loss: 0.3269791007041931 Validation Loss: 0.7892992496490479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3197: Training Loss: 0.3263164162635803 Validation Loss: 0.7890394926071167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3198: Training Loss: 0.32627057035764057 Validation Loss: 0.7891740798950195\n",
      "Epoch 3199: Training Loss: 0.32704849044481915 Validation Loss: 0.7890611886978149\n",
      "Epoch 3200: Training Loss: 0.3267502784729004 Validation Loss: 0.7889949083328247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3201: Training Loss: 0.32654311259587604 Validation Loss: 0.7887653708457947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3202: Training Loss: 0.32648026943206787 Validation Loss: 0.7885222434997559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3203: Training Loss: 0.3271339734395345 Validation Loss: 0.7884451150894165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3204: Training Loss: 0.32670549551645917 Validation Loss: 0.7884200215339661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3205: Training Loss: 0.326041042804718 Validation Loss: 0.7887610197067261\n",
      "Epoch 3206: Training Loss: 0.32639065384864807 Validation Loss: 0.7888322472572327\n",
      "Epoch 3207: Training Loss: 0.3264051079750061 Validation Loss: 0.7887452840805054\n",
      "Epoch 3208: Training Loss: 0.3268643120924632 Validation Loss: 0.7886461019515991\n",
      "Epoch 3209: Training Loss: 0.32547178864479065 Validation Loss: 0.788399338722229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3210: Training Loss: 0.325650413831075 Validation Loss: 0.7880757451057434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3211: Training Loss: 0.3261820276578267 Validation Loss: 0.7881760597229004\n",
      "Epoch 3212: Training Loss: 0.32605454325675964 Validation Loss: 0.7882626056671143\n",
      "Epoch 3213: Training Loss: 0.3250419696172078 Validation Loss: 0.7881566882133484\n",
      "Epoch 3214: Training Loss: 0.3264415959517161 Validation Loss: 0.7878048419952393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3215: Training Loss: 0.32523496945699054 Validation Loss: 0.7877179980278015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3216: Training Loss: 0.3252459168434143 Validation Loss: 0.7877949476242065\n",
      "Epoch 3217: Training Loss: 0.32481299837430316 Validation Loss: 0.7875868082046509\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3218: Training Loss: 0.3243313233057658 Validation Loss: 0.7873960137367249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3219: Training Loss: 0.3241189817587535 Validation Loss: 0.787276566028595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3220: Training Loss: 0.3241294225056966 Validation Loss: 0.7875596284866333\n",
      "Epoch 3221: Training Loss: 0.323848436276118 Validation Loss: 0.787753701210022\n",
      "Epoch 3222: Training Loss: 0.32376458247502643 Validation Loss: 0.7878053784370422\n",
      "Epoch 3223: Training Loss: 0.3240739305814107 Validation Loss: 0.7873232364654541\n",
      "Epoch 3224: Training Loss: 0.3236876626809438 Validation Loss: 0.7872016429901123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3225: Training Loss: 0.32526538769404095 Validation Loss: 0.7867839932441711\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3226: Training Loss: 0.3238444924354553 Validation Loss: 0.7866063117980957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3227: Training Loss: 0.3237216571966807 Validation Loss: 0.7865684032440186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3228: Training Loss: 0.32348612944285077 Validation Loss: 0.7866131067276001\n",
      "Epoch 3229: Training Loss: 0.3233582178751628 Validation Loss: 0.7868333458900452\n",
      "Epoch 3230: Training Loss: 0.32328540086746216 Validation Loss: 0.7871812582015991\n",
      "Epoch 3231: Training Loss: 0.32295677065849304 Validation Loss: 0.7872734069824219\n",
      "Epoch 3232: Training Loss: 0.32293501496315 Validation Loss: 0.7874411940574646\n",
      "Epoch 3233: Training Loss: 0.32293155789375305 Validation Loss: 0.787129282951355\n",
      "Epoch 3234: Training Loss: 0.32343510786692303 Validation Loss: 0.7866635322570801\n",
      "Epoch 3235: Training Loss: 0.3219558894634247 Validation Loss: 0.7861944437026978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3236: Training Loss: 0.3225974937280019 Validation Loss: 0.7862196564674377\n",
      "Epoch 3237: Training Loss: 0.3224806487560272 Validation Loss: 0.7861387133598328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3238: Training Loss: 0.32251734534899396 Validation Loss: 0.7860211730003357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3239: Training Loss: 0.3235805730024974 Validation Loss: 0.7859255075454712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3240: Training Loss: 0.32199687759081524 Validation Loss: 0.7860122323036194\n",
      "Epoch 3241: Training Loss: 0.32179319858551025 Validation Loss: 0.7859821915626526\n",
      "Epoch 3242: Training Loss: 0.32228758931159973 Validation Loss: 0.786054253578186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3243: Training Loss: 0.32159749666849774 Validation Loss: 0.7857006788253784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3244: Training Loss: 0.3215753436088562 Validation Loss: 0.7858125567436218\n",
      "Epoch 3245: Training Loss: 0.32138145963350934 Validation Loss: 0.7858354449272156\n",
      "Epoch 3246: Training Loss: 0.32236896951993305 Validation Loss: 0.7857416868209839\n",
      "Epoch 3247: Training Loss: 0.32145534952481586 Validation Loss: 0.7857058048248291\n",
      "Epoch 3248: Training Loss: 0.32107776403427124 Validation Loss: 0.7856978178024292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3249: Training Loss: 0.32113581895828247 Validation Loss: 0.7853317260742188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3250: Training Loss: 0.3208487530549367 Validation Loss: 0.7854434847831726\n",
      "Epoch 3251: Training Loss: 0.3214009900887807 Validation Loss: 0.785149097442627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3252: Training Loss: 0.3207612733046214 Validation Loss: 0.7849934101104736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3253: Training Loss: 0.3201052049795787 Validation Loss: 0.7852434515953064\n",
      "Epoch 3254: Training Loss: 0.3206477959950765 Validation Loss: 0.785521924495697\n",
      "Epoch 3255: Training Loss: 0.32057039936383563 Validation Loss: 0.7852071523666382\n",
      "Epoch 3256: Training Loss: 0.32028623421986896 Validation Loss: 0.7847028374671936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3257: Training Loss: 0.32083268960316974 Validation Loss: 0.7847262620925903\n",
      "Epoch 3258: Training Loss: 0.32000353932380676 Validation Loss: 0.7847000360488892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3259: Training Loss: 0.31999971469243366 Validation Loss: 0.7845722436904907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3260: Training Loss: 0.31974830230077106 Validation Loss: 0.7847383618354797\n",
      "Epoch 3261: Training Loss: 0.3195776144663493 Validation Loss: 0.7851024270057678\n",
      "Epoch 3262: Training Loss: 0.32026879986127216 Validation Loss: 0.7847169637680054\n",
      "Epoch 3263: Training Loss: 0.3196270366509755 Validation Loss: 0.7845246195793152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3264: Training Loss: 0.3196806311607361 Validation Loss: 0.784371018409729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3265: Training Loss: 0.3190135657787323 Validation Loss: 0.7842392921447754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3266: Training Loss: 0.3190193970998128 Validation Loss: 0.7842183113098145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3267: Training Loss: 0.3192793031533559 Validation Loss: 0.7844070196151733\n",
      "Epoch 3268: Training Loss: 0.3190821409225464 Validation Loss: 0.7840954661369324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3269: Training Loss: 0.31864287455876666 Validation Loss: 0.7843020558357239\n",
      "Epoch 3270: Training Loss: 0.3190113703409831 Validation Loss: 0.7842376232147217\n",
      "Epoch 3271: Training Loss: 0.3188769320646922 Validation Loss: 0.7842749357223511\n",
      "Epoch 3272: Training Loss: 0.3187155524889628 Validation Loss: 0.7840381860733032\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3273: Training Loss: 0.31858423352241516 Validation Loss: 0.783864438533783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3274: Training Loss: 0.31811116139094037 Validation Loss: 0.7834811210632324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3275: Training Loss: 0.31850773096084595 Validation Loss: 0.78334641456604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3276: Training Loss: 0.31833377480506897 Validation Loss: 0.7831517457962036\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3277: Training Loss: 0.31795672575632733 Validation Loss: 0.7836172580718994\n",
      "Epoch 3278: Training Loss: 0.31770986318588257 Validation Loss: 0.7836378812789917\n",
      "Epoch 3279: Training Loss: 0.31803058584531146 Validation Loss: 0.783683180809021\n",
      "Epoch 3280: Training Loss: 0.3174521028995514 Validation Loss: 0.7835186123847961\n",
      "Epoch 3281: Training Loss: 0.317580113808314 Validation Loss: 0.7834497094154358\n",
      "Epoch 3282: Training Loss: 0.31738783915837604 Validation Loss: 0.7833225131034851\n",
      "Epoch 3283: Training Loss: 0.3172033131122589 Validation Loss: 0.7830210328102112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3284: Training Loss: 0.3168949484825134 Validation Loss: 0.7831793427467346\n",
      "Epoch 3285: Training Loss: 0.3175049126148224 Validation Loss: 0.783056914806366\n",
      "Epoch 3286: Training Loss: 0.3169233202934265 Validation Loss: 0.7830891609191895\n",
      "Epoch 3287: Training Loss: 0.3160865207513173 Validation Loss: 0.7829673886299133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3288: Training Loss: 0.31686753034591675 Validation Loss: 0.7828413248062134\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3289: Training Loss: 0.31857969363530475 Validation Loss: 0.7823166847229004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3290: Training Loss: 0.3167959153652191 Validation Loss: 0.7826566696166992\n",
      "Epoch 3291: Training Loss: 0.3160903851191203 Validation Loss: 0.7824987173080444\n",
      "Epoch 3292: Training Loss: 0.31603172421455383 Validation Loss: 0.782566487789154\n",
      "Epoch 3293: Training Loss: 0.31612274050712585 Validation Loss: 0.7825273275375366\n",
      "Epoch 3294: Training Loss: 0.3145272632439931 Validation Loss: 0.7827357649803162\n",
      "Epoch 3295: Training Loss: 0.31610527634620667 Validation Loss: 0.7827054858207703\n",
      "Epoch 3296: Training Loss: 0.3159225483735402 Validation Loss: 0.7825875878334045\n",
      "Epoch 3297: Training Loss: 0.3161267340183258 Validation Loss: 0.7820332050323486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3298: Training Loss: 0.31695016225179035 Validation Loss: 0.7821702361106873\n",
      "Epoch 3299: Training Loss: 0.3158603310585022 Validation Loss: 0.7818834185600281\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3300: Training Loss: 0.315679927666982 Validation Loss: 0.7817445397377014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3301: Training Loss: 0.3150855799516042 Validation Loss: 0.7815264463424683\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3302: Training Loss: 0.31525643666585285 Validation Loss: 0.7817116379737854\n",
      "Epoch 3303: Training Loss: 0.31491348147392273 Validation Loss: 0.7818849086761475\n",
      "Epoch 3304: Training Loss: 0.31529362003008526 Validation Loss: 0.7824203968048096\n",
      "Epoch 3305: Training Loss: 0.31484370430310565 Validation Loss: 0.7823857665061951\n",
      "Epoch 3306: Training Loss: 0.31487274169921875 Validation Loss: 0.7823848128318787\n",
      "Epoch 3307: Training Loss: 0.314796378215154 Validation Loss: 0.7821011543273926\n",
      "Epoch 3308: Training Loss: 0.31496532758076984 Validation Loss: 0.7816556692123413\n",
      "Epoch 3309: Training Loss: 0.3143995702266693 Validation Loss: 0.781341016292572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3310: Training Loss: 0.31451061367988586 Validation Loss: 0.781122624874115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3311: Training Loss: 0.3146929045518239 Validation Loss: 0.7810705900192261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3312: Training Loss: 0.31391308705012005 Validation Loss: 0.7813020944595337\n",
      "Epoch 3313: Training Loss: 0.3149934808413188 Validation Loss: 0.7813803553581238\n",
      "Epoch 3314: Training Loss: 0.3142017622788747 Validation Loss: 0.7813486456871033\n",
      "Epoch 3315: Training Loss: 0.3138742744922638 Validation Loss: 0.7812318801879883\n",
      "Epoch 3316: Training Loss: 0.3146454493204753 Validation Loss: 0.7812777161598206\n",
      "Epoch 3317: Training Loss: 0.3155619204044342 Validation Loss: 0.7811204195022583\n",
      "Epoch 3318: Training Loss: 0.3134235938390096 Validation Loss: 0.7808035016059875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3319: Training Loss: 0.31345997254053753 Validation Loss: 0.780540943145752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3320: Training Loss: 0.313022901614507 Validation Loss: 0.7805814743041992\n",
      "Epoch 3321: Training Loss: 0.3130561908086141 Validation Loss: 0.7807427048683167\n",
      "Epoch 3322: Training Loss: 0.3129321138064067 Validation Loss: 0.7805771827697754\n",
      "Epoch 3323: Training Loss: 0.31269801656405133 Validation Loss: 0.7809435129165649\n",
      "Epoch 3324: Training Loss: 0.31259875496228534 Validation Loss: 0.7805606722831726\n",
      "Epoch 3325: Training Loss: 0.3124123414357503 Validation Loss: 0.7804830074310303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3326: Training Loss: 0.31369055310885113 Validation Loss: 0.7803764939308167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3327: Training Loss: 0.31236783663431805 Validation Loss: 0.7801883816719055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3328: Training Loss: 0.3143475254376729 Validation Loss: 0.7799538969993591\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3329: Training Loss: 0.3123779396216075 Validation Loss: 0.7797454595565796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3330: Training Loss: 0.3121784031391144 Validation Loss: 0.7797616720199585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3331: Training Loss: 0.31194812059402466 Validation Loss: 0.7800126671791077\n",
      "Epoch 3332: Training Loss: 0.31189456582069397 Validation Loss: 0.7802007794380188\n",
      "Epoch 3333: Training Loss: 0.312518447637558 Validation Loss: 0.7802102565765381\n",
      "Epoch 3334: Training Loss: 0.3126090367635091 Validation Loss: 0.7800324559211731\n",
      "Epoch 3335: Training Loss: 0.3117157320181529 Validation Loss: 0.780190646648407\n",
      "Epoch 3336: Training Loss: 0.31104708711306256 Validation Loss: 0.7798486948013306\n",
      "Epoch 3337: Training Loss: 0.3117194175720215 Validation Loss: 0.7795143723487854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3338: Training Loss: 0.3116358121236165 Validation Loss: 0.7795135974884033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3339: Training Loss: 0.31147779027620953 Validation Loss: 0.7795342803001404\n",
      "Epoch 3340: Training Loss: 0.31101447343826294 Validation Loss: 0.7791655659675598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3341: Training Loss: 0.3121352692445119 Validation Loss: 0.7789549231529236\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3342: Training Loss: 0.3106343746185303 Validation Loss: 0.7790666818618774\n",
      "Epoch 3343: Training Loss: 0.31116631627082825 Validation Loss: 0.7790176868438721\n",
      "Epoch 3344: Training Loss: 0.31052030126253766 Validation Loss: 0.7792091965675354\n",
      "Epoch 3345: Training Loss: 0.3110840519269307 Validation Loss: 0.7793843746185303\n",
      "Epoch 3346: Training Loss: 0.3101401627063751 Validation Loss: 0.7791718244552612\n",
      "Epoch 3347: Training Loss: 0.3110276659329732 Validation Loss: 0.7792641520500183\n",
      "Epoch 3348: Training Loss: 0.310054083665212 Validation Loss: 0.7791410684585571\n",
      "Epoch 3349: Training Loss: 0.3105862538019816 Validation Loss: 0.7790901064872742\n",
      "Epoch 3350: Training Loss: 0.31007834275563556 Validation Loss: 0.7792934775352478\n",
      "Epoch 3351: Training Loss: 0.3103736639022827 Validation Loss: 0.77884441614151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3352: Training Loss: 0.3095715244611104 Validation Loss: 0.7787691950798035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3353: Training Loss: 0.30976179242134094 Validation Loss: 0.7784885168075562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3354: Training Loss: 0.3092171251773834 Validation Loss: 0.7784098386764526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3355: Training Loss: 0.31014062960942584 Validation Loss: 0.7781789302825928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3356: Training Loss: 0.3102570076783498 Validation Loss: 0.7780318856239319\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3357: Training Loss: 0.3093940814336141 Validation Loss: 0.7784805297851562\n",
      "Epoch 3358: Training Loss: 0.3096245229244232 Validation Loss: 0.7790455222129822\n",
      "Epoch 3359: Training Loss: 0.3094586233297984 Validation Loss: 0.7787670493125916\n",
      "Epoch 3360: Training Loss: 0.3090068797270457 Validation Loss: 0.7788959741592407\n",
      "Epoch 3361: Training Loss: 0.3085130751132965 Validation Loss: 0.7784097790718079\n",
      "Epoch 3362: Training Loss: 0.30875447392463684 Validation Loss: 0.7781388759613037\n",
      "Epoch 3363: Training Loss: 0.3087139427661896 Validation Loss: 0.7780633568763733\n",
      "Epoch 3364: Training Loss: 0.30925925572713214 Validation Loss: 0.7778365015983582\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3365: Training Loss: 0.30878978967666626 Validation Loss: 0.7777716517448425\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3366: Training Loss: 0.30841102202733356 Validation Loss: 0.777482271194458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3367: Training Loss: 0.30898796518643695 Validation Loss: 0.7774167656898499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3368: Training Loss: 0.30830833315849304 Validation Loss: 0.7772905826568604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3369: Training Loss: 0.3083907961845398 Validation Loss: 0.7772631049156189\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3370: Training Loss: 0.3083315889040629 Validation Loss: 0.7775163650512695\n",
      "Epoch 3371: Training Loss: 0.30909894903500873 Validation Loss: 0.7780660390853882\n",
      "Epoch 3372: Training Loss: 0.30779846509297687 Validation Loss: 0.7778621912002563\n",
      "Epoch 3373: Training Loss: 0.3078560133775075 Validation Loss: 0.7782942056655884\n",
      "Epoch 3374: Training Loss: 0.30765606959660846 Validation Loss: 0.7778210639953613\n",
      "Epoch 3375: Training Loss: 0.30760733286539715 Validation Loss: 0.7773441672325134\n",
      "Epoch 3376: Training Loss: 0.3080426553885142 Validation Loss: 0.7772231698036194\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3377: Training Loss: 0.30828073620796204 Validation Loss: 0.7775795459747314\n",
      "Epoch 3378: Training Loss: 0.3069921632607778 Validation Loss: 0.7775554060935974\n",
      "Epoch 3379: Training Loss: 0.3070130944252014 Validation Loss: 0.7774197459220886\n",
      "Epoch 3380: Training Loss: 0.30843591690063477 Validation Loss: 0.7768555283546448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3381: Training Loss: 0.3087550501028697 Validation Loss: 0.7765456438064575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3382: Training Loss: 0.3069714605808258 Validation Loss: 0.7767922282218933\n",
      "Epoch 3383: Training Loss: 0.30641697843869525 Validation Loss: 0.776915967464447\n",
      "Epoch 3384: Training Loss: 0.3070678611596425 Validation Loss: 0.7768236398696899\n",
      "Epoch 3385: Training Loss: 0.3067193229993184 Validation Loss: 0.7767022252082825\n",
      "Epoch 3386: Training Loss: 0.3066183527310689 Validation Loss: 0.7767801880836487\n",
      "Epoch 3387: Training Loss: 0.30830390254656476 Validation Loss: 0.7767071723937988\n",
      "Epoch 3388: Training Loss: 0.30655426780382794 Validation Loss: 0.7765443921089172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3389: Training Loss: 0.3061276574929555 Validation Loss: 0.776585578918457\n",
      "Epoch 3390: Training Loss: 0.30567067861557007 Validation Loss: 0.7766782641410828\n",
      "Epoch 3391: Training Loss: 0.3060118655363719 Validation Loss: 0.7764171957969666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3392: Training Loss: 0.30536548296610516 Validation Loss: 0.7765882611274719\n",
      "Epoch 3393: Training Loss: 0.3057522972424825 Validation Loss: 0.7766001224517822\n",
      "Epoch 3394: Training Loss: 0.3060254454612732 Validation Loss: 0.777079164981842\n",
      "Epoch 3395: Training Loss: 0.30562958121299744 Validation Loss: 0.7767072916030884\n",
      "Epoch 3396: Training Loss: 0.30596543351809186 Validation Loss: 0.7763285040855408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3397: Training Loss: 0.3052265743414561 Validation Loss: 0.7759029865264893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3398: Training Loss: 0.30539459983507794 Validation Loss: 0.7757842540740967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3399: Training Loss: 0.305696835120519 Validation Loss: 0.7753806114196777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3400: Training Loss: 0.30494893590609234 Validation Loss: 0.7754932641983032\n",
      "Epoch 3401: Training Loss: 0.30538735787073773 Validation Loss: 0.7759644389152527\n",
      "Epoch 3402: Training Loss: 0.30472563703854877 Validation Loss: 0.7762017250061035\n",
      "Epoch 3403: Training Loss: 0.30434895555178326 Validation Loss: 0.7759582996368408\n",
      "Epoch 3404: Training Loss: 0.30496011177698773 Validation Loss: 0.7759198546409607\n",
      "Epoch 3405: Training Loss: 0.3046730657418569 Validation Loss: 0.7759917974472046\n",
      "Epoch 3406: Training Loss: 0.3047206203142802 Validation Loss: 0.7754596471786499\n",
      "Epoch 3407: Training Loss: 0.3044114609559377 Validation Loss: 0.7755579948425293\n",
      "Epoch 3408: Training Loss: 0.3050941526889801 Validation Loss: 0.7753605246543884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3409: Training Loss: 0.30387863516807556 Validation Loss: 0.7751874923706055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3410: Training Loss: 0.30431809027989704 Validation Loss: 0.7748780250549316\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3411: Training Loss: 0.30532190203666687 Validation Loss: 0.7748815417289734\n",
      "Epoch 3412: Training Loss: 0.30359559257825214 Validation Loss: 0.774859607219696\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3413: Training Loss: 0.30372849106788635 Validation Loss: 0.7747837901115417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3414: Training Loss: 0.3036488691965739 Validation Loss: 0.774747908115387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3415: Training Loss: 0.30357326070467633 Validation Loss: 0.7749865651130676\n",
      "Epoch 3416: Training Loss: 0.30358588695526123 Validation Loss: 0.7751628160476685\n",
      "Epoch 3417: Training Loss: 0.30327069759368896 Validation Loss: 0.775314450263977\n",
      "Epoch 3418: Training Loss: 0.3031477729479472 Validation Loss: 0.7752287983894348\n",
      "Epoch 3419: Training Loss: 0.30352985858917236 Validation Loss: 0.7745962142944336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3420: Training Loss: 0.3031255801518758 Validation Loss: 0.7747324109077454\n",
      "Epoch 3421: Training Loss: 0.3032169242699941 Validation Loss: 0.7742839455604553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3422: Training Loss: 0.3021292785803477 Validation Loss: 0.7744755744934082\n",
      "Epoch 3423: Training Loss: 0.3021388351917267 Validation Loss: 0.7744855880737305\n",
      "Epoch 3424: Training Loss: 0.3021332522233327 Validation Loss: 0.7745867967605591\n",
      "Epoch 3425: Training Loss: 0.30216891566912335 Validation Loss: 0.7744028568267822\n",
      "Epoch 3426: Training Loss: 0.30251172184944153 Validation Loss: 0.7744511365890503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3427: Training Loss: 0.303716907898585 Validation Loss: 0.7740474343299866\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3428: Training Loss: 0.30217887957890827 Validation Loss: 0.7746124267578125\n",
      "Epoch 3429: Training Loss: 0.3036750753720601 Validation Loss: 0.774546205997467\n",
      "Epoch 3430: Training Loss: 0.3040585021177928 Validation Loss: 0.7743200063705444\n",
      "Epoch 3431: Training Loss: 0.30173205335934955 Validation Loss: 0.7742785215377808\n",
      "Epoch 3432: Training Loss: 0.3022565146287282 Validation Loss: 0.7744642496109009\n",
      "Epoch 3433: Training Loss: 0.3017372985680898 Validation Loss: 0.774247944355011\n",
      "Epoch 3434: Training Loss: 0.3015511830647786 Validation Loss: 0.7744659781455994\n",
      "Epoch 3435: Training Loss: 0.3017156223456065 Validation Loss: 0.7740564942359924\n",
      "Epoch 3436: Training Loss: 0.30168447891871136 Validation Loss: 0.7736091613769531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3437: Training Loss: 0.3015046815077464 Validation Loss: 0.7736456394195557\n",
      "Epoch 3438: Training Loss: 0.30120564500490826 Validation Loss: 0.7733980417251587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3439: Training Loss: 0.30106909076372784 Validation Loss: 0.7733087539672852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3440: Training Loss: 0.30177165071169537 Validation Loss: 0.7733175158500671\n",
      "Epoch 3441: Training Loss: 0.30112480123837787 Validation Loss: 0.7730646133422852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3442: Training Loss: 0.300808846950531 Validation Loss: 0.7729681730270386\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3443: Training Loss: 0.30085549751917523 Validation Loss: 0.7728144526481628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3444: Training Loss: 0.3010615011056264 Validation Loss: 0.7733429670333862\n",
      "Epoch 3445: Training Loss: 0.3014705975850423 Validation Loss: 0.7734391689300537\n",
      "Epoch 3446: Training Loss: 0.30041887362798053 Validation Loss: 0.7733387351036072\n",
      "Epoch 3447: Training Loss: 0.30041465163230896 Validation Loss: 0.7731713056564331\n",
      "Epoch 3448: Training Loss: 0.3011932373046875 Validation Loss: 0.7730024456977844\n",
      "Epoch 3449: Training Loss: 0.3004944125811259 Validation Loss: 0.773163378238678\n",
      "Epoch 3450: Training Loss: 0.29996105035146076 Validation Loss: 0.7731516361236572\n",
      "Epoch 3451: Training Loss: 0.30044495066006977 Validation Loss: 0.773366391658783\n",
      "Epoch 3452: Training Loss: 0.3007263243198395 Validation Loss: 0.7732415199279785\n",
      "Epoch 3453: Training Loss: 0.2997923493385315 Validation Loss: 0.7728562951087952\n",
      "Epoch 3454: Training Loss: 0.2999375859896342 Validation Loss: 0.7728148698806763\n",
      "Epoch 3455: Training Loss: 0.2998849352200826 Validation Loss: 0.7726596593856812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3456: Training Loss: 0.2993810574213664 Validation Loss: 0.7727580070495605\n",
      "Epoch 3457: Training Loss: 0.2995556791623433 Validation Loss: 0.7723333835601807\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3458: Training Loss: 0.29971033334732056 Validation Loss: 0.7724275588989258\n",
      "Epoch 3459: Training Loss: 0.29895084102948505 Validation Loss: 0.772483229637146\n",
      "Epoch 3460: Training Loss: 0.2994187871615092 Validation Loss: 0.7725697159767151\n",
      "Epoch 3461: Training Loss: 0.30072592695554096 Validation Loss: 0.7726970911026001\n",
      "Epoch 3462: Training Loss: 0.29938871661822003 Validation Loss: 0.7724328637123108\n",
      "Epoch 3463: Training Loss: 0.2990330457687378 Validation Loss: 0.772252082824707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3464: Training Loss: 0.3001048068205516 Validation Loss: 0.7719940543174744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3465: Training Loss: 0.29883722464243573 Validation Loss: 0.7719848155975342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3466: Training Loss: 0.29867810010910034 Validation Loss: 0.7718684077262878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3467: Training Loss: 0.29880322019259137 Validation Loss: 0.7717106938362122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3468: Training Loss: 0.29882601896921795 Validation Loss: 0.7716756463050842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3469: Training Loss: 0.29803510506947833 Validation Loss: 0.7722199559211731\n",
      "Epoch 3470: Training Loss: 0.29836271206537884 Validation Loss: 0.772835373878479\n",
      "Epoch 3471: Training Loss: 0.29804064830144245 Validation Loss: 0.7725656628608704\n",
      "Epoch 3472: Training Loss: 0.29991841812928516 Validation Loss: 0.7721836566925049\n",
      "Epoch 3473: Training Loss: 0.2984259823958079 Validation Loss: 0.7717102766036987\n",
      "Epoch 3474: Training Loss: 0.29761173327763873 Validation Loss: 0.7715054154396057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3475: Training Loss: 0.29823259512583417 Validation Loss: 0.7715238332748413\n",
      "Epoch 3476: Training Loss: 0.29762523372968036 Validation Loss: 0.7713744044303894\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3477: Training Loss: 0.29792292912801105 Validation Loss: 0.771312415599823\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3478: Training Loss: 0.2973125179608663 Validation Loss: 0.771314799785614\n",
      "Epoch 3479: Training Loss: 0.29782824714978534 Validation Loss: 0.7714667320251465\n",
      "Epoch 3480: Training Loss: 0.29782938957214355 Validation Loss: 0.771448016166687\n",
      "Epoch 3481: Training Loss: 0.2982675035794576 Validation Loss: 0.771481990814209\n",
      "Epoch 3482: Training Loss: 0.2972395718097687 Validation Loss: 0.7713343501091003\n",
      "Epoch 3483: Training Loss: 0.2962803542613983 Validation Loss: 0.7713544368743896\n",
      "Epoch 3484: Training Loss: 0.29691489537556964 Validation Loss: 0.7714509963989258\n",
      "Epoch 3485: Training Loss: 0.29599708318710327 Validation Loss: 0.7712609767913818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3486: Training Loss: 0.29661887884140015 Validation Loss: 0.7708795666694641\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3487: Training Loss: 0.2963615159193675 Validation Loss: 0.7706954479217529\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3488: Training Loss: 0.2964046001434326 Validation Loss: 0.7709081768989563\n",
      "Epoch 3489: Training Loss: 0.29654685656229657 Validation Loss: 0.7706375122070312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3490: Training Loss: 0.29538219173749286 Validation Loss: 0.7707908153533936\n",
      "Epoch 3491: Training Loss: 0.29634807507197064 Validation Loss: 0.7708030343055725\n",
      "Epoch 3492: Training Loss: 0.29668540755907696 Validation Loss: 0.7704979777336121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3493: Training Loss: 0.2970546980698903 Validation Loss: 0.7703306078910828\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3494: Training Loss: 0.29673192898432416 Validation Loss: 0.7706350684165955\n",
      "Epoch 3495: Training Loss: 0.2967681686083476 Validation Loss: 0.7706953287124634\n",
      "Epoch 3496: Training Loss: 0.29592469334602356 Validation Loss: 0.7701901793479919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3497: Training Loss: 0.2962052623430888 Validation Loss: 0.7704640626907349\n",
      "Epoch 3498: Training Loss: 0.29584861795107525 Validation Loss: 0.7706788182258606\n",
      "Epoch 3499: Training Loss: 0.29567622145016986 Validation Loss: 0.7705009579658508\n",
      "Epoch 3500: Training Loss: 0.2951761285463969 Validation Loss: 0.7705923318862915\n",
      "Epoch 3501: Training Loss: 0.2954845428466797 Validation Loss: 0.770427405834198\n",
      "Epoch 3502: Training Loss: 0.2951960066954295 Validation Loss: 0.7701103091239929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3503: Training Loss: 0.29499993721644086 Validation Loss: 0.7701565623283386\n",
      "Epoch 3504: Training Loss: 0.29489704966545105 Validation Loss: 0.7700880765914917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3505: Training Loss: 0.29495319724082947 Validation Loss: 0.7701547145843506\n",
      "Epoch 3506: Training Loss: 0.294891099135081 Validation Loss: 0.7701385021209717\n",
      "Epoch 3507: Training Loss: 0.29458622137705487 Validation Loss: 0.7697246670722961\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3508: Training Loss: 0.2945067584514618 Validation Loss: 0.7697494626045227\n",
      "Epoch 3509: Training Loss: 0.2944653332233429 Validation Loss: 0.769582211971283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3510: Training Loss: 0.29452993472417194 Validation Loss: 0.7694592475891113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3511: Training Loss: 0.29493866364161175 Validation Loss: 0.7695322036743164\n",
      "Epoch 3512: Training Loss: 0.29550207654635113 Validation Loss: 0.7693196535110474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3513: Training Loss: 0.2940519154071808 Validation Loss: 0.7694490551948547\n",
      "Epoch 3514: Training Loss: 0.2942849397659302 Validation Loss: 0.7694371938705444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3515: Training Loss: 0.29404595494270325 Validation Loss: 0.7697587609291077\n",
      "Epoch 3516: Training Loss: 0.2937587797641754 Validation Loss: 0.7697396278381348\n",
      "Epoch 3517: Training Loss: 0.2941344380378723 Validation Loss: 0.7694808840751648\n",
      "Epoch 3518: Training Loss: 0.2936403155326843 Validation Loss: 0.7694351077079773\n",
      "Epoch 3519: Training Loss: 0.29375986258188885 Validation Loss: 0.7695063948631287\n",
      "Epoch 3520: Training Loss: 0.2935878535111745 Validation Loss: 0.7695938944816589\n",
      "Epoch 3521: Training Loss: 0.2945738633473714 Validation Loss: 0.7695019245147705\n",
      "Epoch 3522: Training Loss: 0.2934160828590393 Validation Loss: 0.7696057558059692\n",
      "Epoch 3523: Training Loss: 0.29329806566238403 Validation Loss: 0.769527792930603\n",
      "Epoch 3524: Training Loss: 0.292988916238149 Validation Loss: 0.7690976858139038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3525: Training Loss: 0.29305041829744977 Validation Loss: 0.768932580947876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3526: Training Loss: 0.29296990235646564 Validation Loss: 0.7688373327255249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3527: Training Loss: 0.2930840651194255 Validation Loss: 0.7687641382217407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3528: Training Loss: 0.2922257085641225 Validation Loss: 0.7687937021255493\n",
      "Epoch 3529: Training Loss: 0.29301635424296063 Validation Loss: 0.7685316801071167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3530: Training Loss: 0.2926097611586253 Validation Loss: 0.7687496542930603\n",
      "Epoch 3531: Training Loss: 0.2922791043917338 Validation Loss: 0.7688593864440918\n",
      "Epoch 3532: Training Loss: 0.2927717864513397 Validation Loss: 0.7685399055480957\n",
      "Epoch 3533: Training Loss: 0.2921398381392161 Validation Loss: 0.7683840990066528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3534: Training Loss: 0.29328277707099915 Validation Loss: 0.7684325575828552\n",
      "Epoch 3535: Training Loss: 0.2913204828898112 Validation Loss: 0.7685531973838806\n",
      "Epoch 3536: Training Loss: 0.29246681928634644 Validation Loss: 0.7685770988464355\n",
      "Epoch 3537: Training Loss: 0.29190828402837116 Validation Loss: 0.7684915065765381\n",
      "Epoch 3538: Training Loss: 0.29206039508183795 Validation Loss: 0.7681609988212585\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3539: Training Loss: 0.29163933793703717 Validation Loss: 0.7681886553764343\n",
      "Epoch 3540: Training Loss: 0.2916666567325592 Validation Loss: 0.7682287693023682\n",
      "Epoch 3541: Training Loss: 0.2914896011352539 Validation Loss: 0.7680357694625854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3542: Training Loss: 0.29144128163655597 Validation Loss: 0.7676029801368713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3543: Training Loss: 0.29146020611127216 Validation Loss: 0.7679293155670166\n",
      "Epoch 3544: Training Loss: 0.29111339648564655 Validation Loss: 0.7680071592330933\n",
      "Epoch 3545: Training Loss: 0.290811151266098 Validation Loss: 0.7678678631782532\n",
      "Epoch 3546: Training Loss: 0.29221972823143005 Validation Loss: 0.7682270407676697\n",
      "Epoch 3547: Training Loss: 0.2911601662635803 Validation Loss: 0.7683630585670471\n",
      "Epoch 3548: Training Loss: 0.29205196102460224 Validation Loss: 0.7684827446937561\n",
      "Epoch 3549: Training Loss: 0.2908213833967845 Validation Loss: 0.7679924964904785\n",
      "Epoch 3550: Training Loss: 0.29058114687601727 Validation Loss: 0.767595648765564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3551: Training Loss: 0.29057586193084717 Validation Loss: 0.7672750949859619\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3552: Training Loss: 0.29057807723681134 Validation Loss: 0.7671781182289124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3553: Training Loss: 0.2915198902289073 Validation Loss: 0.7670356631278992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3554: Training Loss: 0.29013777772585553 Validation Loss: 0.7676970958709717\n",
      "Epoch 3555: Training Loss: 0.2904931902885437 Validation Loss: 0.7676365971565247\n",
      "Epoch 3556: Training Loss: 0.29020286599795025 Validation Loss: 0.7674230337142944\n",
      "Epoch 3557: Training Loss: 0.290195494890213 Validation Loss: 0.7671516537666321\n",
      "Epoch 3558: Training Loss: 0.29041193922360736 Validation Loss: 0.7670097351074219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3559: Training Loss: 0.2901342809200287 Validation Loss: 0.7670890092849731\n",
      "Epoch 3560: Training Loss: 0.28959917028745014 Validation Loss: 0.7669026851654053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3561: Training Loss: 0.28994391361872357 Validation Loss: 0.7669252157211304\n",
      "Epoch 3562: Training Loss: 0.2895508408546448 Validation Loss: 0.7668941020965576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3563: Training Loss: 0.2899990677833557 Validation Loss: 0.767112672328949\n",
      "Epoch 3564: Training Loss: 0.2901502052942912 Validation Loss: 0.766823947429657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3565: Training Loss: 0.28995731472969055 Validation Loss: 0.7672510743141174\n",
      "Epoch 3566: Training Loss: 0.2897501587867737 Validation Loss: 0.7669816613197327\n",
      "Epoch 3567: Training Loss: 0.2894110480944316 Validation Loss: 0.7667842507362366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3568: Training Loss: 0.28906165560086566 Validation Loss: 0.7669230103492737\n",
      "Epoch 3569: Training Loss: 0.2886798580487569 Validation Loss: 0.7670294046401978\n",
      "Epoch 3570: Training Loss: 0.2893453737099965 Validation Loss: 0.767082154750824\n",
      "Epoch 3571: Training Loss: 0.289028803507487 Validation Loss: 0.7673500776290894\n",
      "Epoch 3572: Training Loss: 0.28946031133333844 Validation Loss: 0.7669721841812134\n",
      "Epoch 3573: Training Loss: 0.2886808713277181 Validation Loss: 0.7664152979850769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3574: Training Loss: 0.2887924512227376 Validation Loss: 0.7663393020629883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3575: Training Loss: 0.2888002594312032 Validation Loss: 0.7660930752754211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3576: Training Loss: 0.2879926959673564 Validation Loss: 0.7659769058227539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3577: Training Loss: 0.2885020077228546 Validation Loss: 0.7660790681838989\n",
      "Epoch 3578: Training Loss: 0.2883220116297404 Validation Loss: 0.7661102414131165\n",
      "Epoch 3579: Training Loss: 0.288011759519577 Validation Loss: 0.7663147449493408\n",
      "Epoch 3580: Training Loss: 0.28874437014261883 Validation Loss: 0.7663203477859497\n",
      "Epoch 3581: Training Loss: 0.28945105771223706 Validation Loss: 0.7664875388145447\n",
      "Epoch 3582: Training Loss: 0.28795631726582843 Validation Loss: 0.7663982510566711\n",
      "Epoch 3583: Training Loss: 0.28807637095451355 Validation Loss: 0.7661217451095581\n",
      "Epoch 3584: Training Loss: 0.28862400849660236 Validation Loss: 0.7660802602767944\n",
      "Epoch 3585: Training Loss: 0.28819799423217773 Validation Loss: 0.7660388350486755\n",
      "Epoch 3586: Training Loss: 0.2876805861790975 Validation Loss: 0.7661363482475281\n",
      "Epoch 3587: Training Loss: 0.28828604022661847 Validation Loss: 0.7659396529197693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3588: Training Loss: 0.28746076424916583 Validation Loss: 0.7658572793006897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3589: Training Loss: 0.2871174116929372 Validation Loss: 0.7659043073654175\n",
      "Epoch 3590: Training Loss: 0.2875632146994273 Validation Loss: 0.7658693194389343\n",
      "Epoch 3591: Training Loss: 0.2876887917518616 Validation Loss: 0.7654705047607422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3592: Training Loss: 0.28802720705668133 Validation Loss: 0.7653990983963013\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3593: Training Loss: 0.2869093318780263 Validation Loss: 0.7653997540473938\n",
      "Epoch 3594: Training Loss: 0.28676604231198627 Validation Loss: 0.7655813097953796\n",
      "Epoch 3595: Training Loss: 0.28727656602859497 Validation Loss: 0.7654299736022949\n",
      "Epoch 3596: Training Loss: 0.2864884634812673 Validation Loss: 0.765622615814209\n",
      "Epoch 3597: Training Loss: 0.28734809160232544 Validation Loss: 0.7658033967018127\n",
      "Epoch 3598: Training Loss: 0.28656458854675293 Validation Loss: 0.7650820016860962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3599: Training Loss: 0.28728307286898297 Validation Loss: 0.7649643421173096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3600: Training Loss: 0.28757651646931964 Validation Loss: 0.7650330066680908\n",
      "Epoch 3601: Training Loss: 0.28589220841725665 Validation Loss: 0.7653375864028931\n",
      "Epoch 3602: Training Loss: 0.2856653829415639 Validation Loss: 0.7653282880783081\n",
      "Epoch 3603: Training Loss: 0.28571897745132446 Validation Loss: 0.7651292681694031\n",
      "Epoch 3604: Training Loss: 0.28614404300848645 Validation Loss: 0.7651508450508118\n",
      "Epoch 3605: Training Loss: 0.2865099608898163 Validation Loss: 0.7651495337486267\n",
      "Epoch 3606: Training Loss: 0.2856926421324412 Validation Loss: 0.7650324702262878\n",
      "Epoch 3607: Training Loss: 0.28511329491933185 Validation Loss: 0.7649498581886292\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3608: Training Loss: 0.28557243943214417 Validation Loss: 0.7647414207458496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3609: Training Loss: 0.2853187322616577 Validation Loss: 0.7647085785865784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3610: Training Loss: 0.2859046856562297 Validation Loss: 0.7648258209228516\n",
      "Epoch 3611: Training Loss: 0.2858688731988271 Validation Loss: 0.7646885514259338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3612: Training Loss: 0.2852216064929962 Validation Loss: 0.7646394371986389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3613: Training Loss: 0.2851130465666453 Validation Loss: 0.7645568251609802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3614: Training Loss: 0.28516440590222675 Validation Loss: 0.764587938785553\n",
      "Epoch 3615: Training Loss: 0.2858487069606781 Validation Loss: 0.7644912004470825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3616: Training Loss: 0.2853593130906423 Validation Loss: 0.7644955515861511\n",
      "Epoch 3617: Training Loss: 0.2848212420940399 Validation Loss: 0.7641672492027283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3618: Training Loss: 0.28505117694536847 Validation Loss: 0.7640818357467651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3619: Training Loss: 0.28522467613220215 Validation Loss: 0.7642753720283508\n",
      "Epoch 3620: Training Loss: 0.2844744523366292 Validation Loss: 0.7645593881607056\n",
      "Epoch 3621: Training Loss: 0.284150833884875 Validation Loss: 0.7643672227859497\n",
      "Epoch 3622: Training Loss: 0.2847926914691925 Validation Loss: 0.7641157507896423\n",
      "Epoch 3623: Training Loss: 0.2843111952145894 Validation Loss: 0.7643829584121704\n",
      "Epoch 3624: Training Loss: 0.2840827703475952 Validation Loss: 0.7639994025230408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3625: Training Loss: 0.2841523090998332 Validation Loss: 0.7642167806625366\n",
      "Epoch 3626: Training Loss: 0.28499868015448254 Validation Loss: 0.7646207213401794\n",
      "Epoch 3627: Training Loss: 0.28486864765485126 Validation Loss: 0.7644060254096985\n",
      "Epoch 3628: Training Loss: 0.2839538057645162 Validation Loss: 0.7641780972480774\n",
      "Epoch 3629: Training Loss: 0.2837578058242798 Validation Loss: 0.7641075849533081\n",
      "Epoch 3630: Training Loss: 0.283370574315389 Validation Loss: 0.7640507221221924\n",
      "Epoch 3631: Training Loss: 0.2838203012943268 Validation Loss: 0.7638269066810608\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3632: Training Loss: 0.28328414758046466 Validation Loss: 0.7636339068412781\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3633: Training Loss: 0.2837175627549489 Validation Loss: 0.763556182384491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3634: Training Loss: 0.2835165758927663 Validation Loss: 0.7634828686714172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3635: Training Loss: 0.2831348677476247 Validation Loss: 0.7632610201835632\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3636: Training Loss: 0.28355251749356586 Validation Loss: 0.763408362865448\n",
      "Epoch 3637: Training Loss: 0.283912127216657 Validation Loss: 0.7632915377616882\n",
      "Epoch 3638: Training Loss: 0.283553014198939 Validation Loss: 0.7634678483009338\n",
      "Epoch 3639: Training Loss: 0.2825877567132314 Validation Loss: 0.7631776332855225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3640: Training Loss: 0.28351739545663196 Validation Loss: 0.7633814215660095\n",
      "Epoch 3641: Training Loss: 0.28315259516239166 Validation Loss: 0.7633416056632996\n",
      "Epoch 3642: Training Loss: 0.28322989741961163 Validation Loss: 0.7635831832885742\n",
      "Epoch 3643: Training Loss: 0.28182313839594525 Validation Loss: 0.7632691264152527\n",
      "Epoch 3644: Training Loss: 0.2823970913887024 Validation Loss: 0.7634799480438232\n",
      "Epoch 3645: Training Loss: 0.28269097208976746 Validation Loss: 0.7636737823486328\n",
      "Epoch 3646: Training Loss: 0.2829606781403224 Validation Loss: 0.7633040547370911\n",
      "Epoch 3647: Training Loss: 0.28172362844149273 Validation Loss: 0.7628121376037598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3648: Training Loss: 0.28184203306833905 Validation Loss: 0.7624965310096741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3649: Training Loss: 0.28211172421773273 Validation Loss: 0.7626720666885376\n",
      "Epoch 3650: Training Loss: 0.28168676296869916 Validation Loss: 0.7627878189086914\n",
      "Epoch 3651: Training Loss: 0.2817052900791168 Validation Loss: 0.76285320520401\n",
      "Epoch 3652: Training Loss: 0.2817159692446391 Validation Loss: 0.7628933191299438\n",
      "Epoch 3653: Training Loss: 0.2817109326521556 Validation Loss: 0.763005256652832\n",
      "Epoch 3654: Training Loss: 0.281684935092926 Validation Loss: 0.7633228302001953\n",
      "Epoch 3655: Training Loss: 0.28124282757441205 Validation Loss: 0.7631499767303467\n",
      "Epoch 3656: Training Loss: 0.28215837478637695 Validation Loss: 0.7630084156990051\n",
      "Epoch 3657: Training Loss: 0.28219160437583923 Validation Loss: 0.7631552815437317\n",
      "Epoch 3658: Training Loss: 0.2812297244866689 Validation Loss: 0.762672483921051\n",
      "Epoch 3659: Training Loss: 0.28099604447682697 Validation Loss: 0.7625183463096619\n",
      "Epoch 3660: Training Loss: 0.2809334695339203 Validation Loss: 0.7626893520355225\n",
      "Epoch 3661: Training Loss: 0.28071141242980957 Validation Loss: 0.7624577879905701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3662: Training Loss: 0.2808241844177246 Validation Loss: 0.7624708414077759\n",
      "Epoch 3663: Training Loss: 0.280655841032664 Validation Loss: 0.762376070022583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3664: Training Loss: 0.28072946270306903 Validation Loss: 0.7625234127044678\n",
      "Epoch 3665: Training Loss: 0.2813788155714671 Validation Loss: 0.762559175491333\n",
      "Epoch 3666: Training Loss: 0.2803148378928502 Validation Loss: 0.7620850801467896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3667: Training Loss: 0.28101325035095215 Validation Loss: 0.7616965174674988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3668: Training Loss: 0.2804182469844818 Validation Loss: 0.7616693377494812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3669: Training Loss: 0.28147462010383606 Validation Loss: 0.7618913054466248\n",
      "Epoch 3670: Training Loss: 0.2802390158176422 Validation Loss: 0.7618329524993896\n",
      "Epoch 3671: Training Loss: 0.28015554944674176 Validation Loss: 0.7621594667434692\n",
      "Epoch 3672: Training Loss: 0.2796915074189504 Validation Loss: 0.7619001865386963\n",
      "Epoch 3673: Training Loss: 0.27992215752601624 Validation Loss: 0.7622501254081726\n",
      "Epoch 3674: Training Loss: 0.27983008821805316 Validation Loss: 0.7617799639701843\n",
      "Epoch 3675: Training Loss: 0.28025534252325696 Validation Loss: 0.7616486549377441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3676: Training Loss: 0.27956749002138775 Validation Loss: 0.7614221572875977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3677: Training Loss: 0.27988840142885846 Validation Loss: 0.7612714171409607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3678: Training Loss: 0.27930023272832233 Validation Loss: 0.7614805698394775\n",
      "Epoch 3679: Training Loss: 0.27996806303660077 Validation Loss: 0.7616885304450989\n",
      "Epoch 3680: Training Loss: 0.279296080271403 Validation Loss: 0.7618868350982666\n",
      "Epoch 3681: Training Loss: 0.2792799274126689 Validation Loss: 0.7617859244346619\n",
      "Epoch 3682: Training Loss: 0.2796769142150879 Validation Loss: 0.7617028951644897\n",
      "Epoch 3683: Training Loss: 0.2796513835589091 Validation Loss: 0.7617896795272827\n",
      "Epoch 3684: Training Loss: 0.27903549869855243 Validation Loss: 0.7618407011032104\n",
      "Epoch 3685: Training Loss: 0.2792082627614339 Validation Loss: 0.7613682150840759\n",
      "Epoch 3686: Training Loss: 0.2798265864451726 Validation Loss: 0.7614525556564331\n",
      "Epoch 3687: Training Loss: 0.27842362721761066 Validation Loss: 0.7614246010780334\n",
      "Epoch 3688: Training Loss: 0.2794222782055537 Validation Loss: 0.7617029547691345\n",
      "Epoch 3689: Training Loss: 0.27906903127829236 Validation Loss: 0.7619473934173584\n",
      "Epoch 3690: Training Loss: 0.2784590770800908 Validation Loss: 0.7615772485733032\n",
      "Epoch 3691: Training Loss: 0.27858027815818787 Validation Loss: 0.7615193724632263\n",
      "Epoch 3692: Training Loss: 0.27829254666964215 Validation Loss: 0.7610956430435181\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3693: Training Loss: 0.2780008713404338 Validation Loss: 0.7605281472206116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3694: Training Loss: 0.27921396990617114 Validation Loss: 0.7602492570877075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3695: Training Loss: 0.2798302471637726 Validation Loss: 0.7601683735847473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3696: Training Loss: 0.2780907154083252 Validation Loss: 0.7607394456863403\n",
      "Epoch 3697: Training Loss: 0.27824831008911133 Validation Loss: 0.7610549330711365\n",
      "Epoch 3698: Training Loss: 0.2779518961906433 Validation Loss: 0.7608953714370728\n",
      "Epoch 3699: Training Loss: 0.2775014638900757 Validation Loss: 0.7608088850975037\n",
      "Epoch 3700: Training Loss: 0.27720850706100464 Validation Loss: 0.761238694190979\n",
      "Epoch 3701: Training Loss: 0.27791792651017505 Validation Loss: 0.7614060044288635\n",
      "Epoch 3702: Training Loss: 0.2780289053916931 Validation Loss: 0.7607389688491821\n",
      "Epoch 3703: Training Loss: 0.2781005452076594 Validation Loss: 0.7607468366622925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3704: Training Loss: 0.2775778571764628 Validation Loss: 0.760925829410553\n",
      "Epoch 3705: Training Loss: 0.2779255658388138 Validation Loss: 0.760659396648407\n",
      "Epoch 3706: Training Loss: 0.2768577039241791 Validation Loss: 0.7606684565544128\n",
      "Epoch 3707: Training Loss: 0.2774196267127991 Validation Loss: 0.7602261900901794\n",
      "Epoch 3708: Training Loss: 0.2765875260035197 Validation Loss: 0.760179340839386\n",
      "Epoch 3709: Training Loss: 0.2769087056318919 Validation Loss: 0.7600149512290955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3710: Training Loss: 0.27881339689095813 Validation Loss: 0.7600616812705994\n",
      "Epoch 3711: Training Loss: 0.27733931442101795 Validation Loss: 0.7598254084587097\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3712: Training Loss: 0.27680302659670514 Validation Loss: 0.7597177028656006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3713: Training Loss: 0.2769976456960042 Validation Loss: 0.7605147957801819\n",
      "Epoch 3714: Training Loss: 0.27679165204366046 Validation Loss: 0.760445237159729\n",
      "Epoch 3715: Training Loss: 0.27663010358810425 Validation Loss: 0.7601824998855591\n",
      "Epoch 3716: Training Loss: 0.27651549379030865 Validation Loss: 0.760187566280365\n",
      "Epoch 3717: Training Loss: 0.27743316690127057 Validation Loss: 0.7607353925704956\n",
      "Epoch 3718: Training Loss: 0.27721773584683734 Validation Loss: 0.7609549760818481\n",
      "Epoch 3719: Training Loss: 0.276442289352417 Validation Loss: 0.7603527307510376\n",
      "Epoch 3720: Training Loss: 0.27583374579747516 Validation Loss: 0.759993314743042\n",
      "Epoch 3721: Training Loss: 0.2760815421740214 Validation Loss: 0.7599444389343262\n",
      "Epoch 3722: Training Loss: 0.2761074850956599 Validation Loss: 0.7595964074134827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3723: Training Loss: 0.27652926246325177 Validation Loss: 0.7592743039131165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3724: Training Loss: 0.2762290636698405 Validation Loss: 0.759215235710144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3725: Training Loss: 0.27557377020517987 Validation Loss: 0.7592583298683167\n",
      "Epoch 3726: Training Loss: 0.2756168842315674 Validation Loss: 0.75909823179245\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3727: Training Loss: 0.2756100594997406 Validation Loss: 0.759239912033081\n",
      "Epoch 3728: Training Loss: 0.2758607864379883 Validation Loss: 0.7597042918205261\n",
      "Epoch 3729: Training Loss: 0.2757441500822703 Validation Loss: 0.7598200440406799\n",
      "Epoch 3730: Training Loss: 0.2753748893737793 Validation Loss: 0.7598203420639038\n",
      "Epoch 3731: Training Loss: 0.27555208404858905 Validation Loss: 0.7599068880081177\n",
      "Epoch 3732: Training Loss: 0.27544404069582623 Validation Loss: 0.7595300078392029\n",
      "Epoch 3733: Training Loss: 0.27450316150983173 Validation Loss: 0.7592429518699646\n",
      "Epoch 3734: Training Loss: 0.275104284286499 Validation Loss: 0.7593294382095337\n",
      "Epoch 3735: Training Loss: 0.2753744125366211 Validation Loss: 0.7594048380851746\n",
      "Epoch 3736: Training Loss: 0.27480945984522503 Validation Loss: 0.7594171762466431\n",
      "Epoch 3737: Training Loss: 0.2747272551059723 Validation Loss: 0.7590468525886536\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3738: Training Loss: 0.2748982807000478 Validation Loss: 0.7585985064506531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3739: Training Loss: 0.27726968626181286 Validation Loss: 0.7582576274871826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3740: Training Loss: 0.2755362590154012 Validation Loss: 0.7587612271308899\n",
      "Epoch 3741: Training Loss: 0.2747706075509389 Validation Loss: 0.7589046359062195\n",
      "Epoch 3742: Training Loss: 0.2745811839898427 Validation Loss: 0.7587774395942688\n",
      "Epoch 3743: Training Loss: 0.27429558833440143 Validation Loss: 0.7592481970787048\n",
      "Epoch 3744: Training Loss: 0.2744381328423818 Validation Loss: 0.7590190768241882\n",
      "Epoch 3745: Training Loss: 0.27419015765190125 Validation Loss: 0.7589189410209656\n",
      "Epoch 3746: Training Loss: 0.2742391526699066 Validation Loss: 0.7586994171142578\n",
      "Epoch 3747: Training Loss: 0.27431078751881915 Validation Loss: 0.7588411569595337\n",
      "Epoch 3748: Training Loss: 0.27380354205767315 Validation Loss: 0.7584954500198364\n",
      "Epoch 3749: Training Loss: 0.2748886396487554 Validation Loss: 0.7587877511978149\n",
      "Epoch 3750: Training Loss: 0.2739296307166417 Validation Loss: 0.7591124176979065\n",
      "Epoch 3751: Training Loss: 0.27418647706508636 Validation Loss: 0.7592857480049133\n",
      "Epoch 3752: Training Loss: 0.2737853229045868 Validation Loss: 0.7590325474739075\n",
      "Epoch 3753: Training Loss: 0.2733973066012065 Validation Loss: 0.7587979435920715\n",
      "Epoch 3754: Training Loss: 0.27340858181317645 Validation Loss: 0.7583661079406738\n",
      "Epoch 3755: Training Loss: 0.27364790439605713 Validation Loss: 0.758441150188446\n",
      "Epoch 3756: Training Loss: 0.27498210966587067 Validation Loss: 0.7583144903182983\n",
      "Epoch 3757: Training Loss: 0.2730210522810618 Validation Loss: 0.758371114730835\n",
      "Epoch 3758: Training Loss: 0.2749725927909215 Validation Loss: 0.7586448788642883\n",
      "Epoch 3759: Training Loss: 0.27260417739550274 Validation Loss: 0.7584213614463806\n",
      "Epoch 3760: Training Loss: 0.27239539722601575 Validation Loss: 0.7582927942276001\n",
      "Epoch 3761: Training Loss: 0.27288714051246643 Validation Loss: 0.7586789131164551\n",
      "Epoch 3762: Training Loss: 0.2730070650577545 Validation Loss: 0.7587284445762634\n",
      "Epoch 3763: Training Loss: 0.2731289565563202 Validation Loss: 0.7586570382118225\n",
      "Epoch 3764: Training Loss: 0.2730747361977895 Validation Loss: 0.7581007480621338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3765: Training Loss: 0.27279558777809143 Validation Loss: 0.7577058672904968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3766: Training Loss: 0.27303792039553326 Validation Loss: 0.757698655128479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3767: Training Loss: 0.2713690648476283 Validation Loss: 0.7577943205833435\n",
      "Epoch 3768: Training Loss: 0.2721559504667918 Validation Loss: 0.7578722834587097\n",
      "Epoch 3769: Training Loss: 0.2721621592839559 Validation Loss: 0.7581183314323425\n",
      "Epoch 3770: Training Loss: 0.27258284389972687 Validation Loss: 0.7583627104759216\n",
      "Epoch 3771: Training Loss: 0.27177201211452484 Validation Loss: 0.7581756711006165\n",
      "Epoch 3772: Training Loss: 0.271669402718544 Validation Loss: 0.7579420208930969\n",
      "Epoch 3773: Training Loss: 0.27189121146996814 Validation Loss: 0.7578257322311401\n",
      "Epoch 3774: Training Loss: 0.2723356286684672 Validation Loss: 0.7576731443405151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3775: Training Loss: 0.27161111931006116 Validation Loss: 0.7574994564056396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3776: Training Loss: 0.2713750898838043 Validation Loss: 0.7576261758804321\n",
      "Epoch 3777: Training Loss: 0.27165470520655316 Validation Loss: 0.7576934099197388\n",
      "Epoch 3778: Training Loss: 0.2715241114298503 Validation Loss: 0.7575597167015076\n",
      "Epoch 3779: Training Loss: 0.27210157612959546 Validation Loss: 0.757634699344635\n",
      "Epoch 3780: Training Loss: 0.2712725102901459 Validation Loss: 0.7577195763587952\n",
      "Epoch 3781: Training Loss: 0.27113935351371765 Validation Loss: 0.7573808431625366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3782: Training Loss: 0.27220553656419116 Validation Loss: 0.7570021152496338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3783: Training Loss: 0.27130195995171863 Validation Loss: 0.7570199966430664\n",
      "Epoch 3784: Training Loss: 0.27069540818532306 Validation Loss: 0.7571976780891418\n",
      "Epoch 3785: Training Loss: 0.2707523802916209 Validation Loss: 0.757094144821167\n",
      "Epoch 3786: Training Loss: 0.2725915461778641 Validation Loss: 0.7578864097595215\n",
      "Epoch 3787: Training Loss: 0.27070632576942444 Validation Loss: 0.7579818964004517\n",
      "Epoch 3788: Training Loss: 0.2714174687862396 Validation Loss: 0.7577326893806458\n",
      "Epoch 3789: Training Loss: 0.2711840867996216 Validation Loss: 0.7579911351203918\n",
      "Epoch 3790: Training Loss: 0.27163125574588776 Validation Loss: 0.7579696774482727\n",
      "Epoch 3791: Training Loss: 0.2698901295661926 Validation Loss: 0.7573024034500122\n",
      "Epoch 3792: Training Loss: 0.2702861527601878 Validation Loss: 0.7570987343788147\n",
      "Epoch 3793: Training Loss: 0.27034125725428265 Validation Loss: 0.7568226456642151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3794: Training Loss: 0.2702803711096446 Validation Loss: 0.7569329142570496\n",
      "Epoch 3795: Training Loss: 0.2704546054204305 Validation Loss: 0.7569566965103149\n",
      "Epoch 3796: Training Loss: 0.2706451416015625 Validation Loss: 0.7569672465324402\n",
      "Epoch 3797: Training Loss: 0.26997258762518567 Validation Loss: 0.7570290565490723\n",
      "Epoch 3798: Training Loss: 0.2706252286831538 Validation Loss: 0.756869912147522\n",
      "Epoch 3799: Training Loss: 0.2698606550693512 Validation Loss: 0.756826639175415\n",
      "Epoch 3800: Training Loss: 0.2705146074295044 Validation Loss: 0.7566336989402771\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3801: Training Loss: 0.2693791488806407 Validation Loss: 0.7568641304969788\n",
      "Epoch 3802: Training Loss: 0.2700100888808568 Validation Loss: 0.7568061947822571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3803: Training Loss: 0.2696334818998973 Validation Loss: 0.75677889585495\n",
      "Epoch 3804: Training Loss: 0.2703786790370941 Validation Loss: 0.7561944723129272\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3805: Training Loss: 0.26957522829373676 Validation Loss: 0.7561318874359131\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3806: Training Loss: 0.2693709433078766 Validation Loss: 0.7563930153846741\n",
      "Epoch 3807: Training Loss: 0.26838502287864685 Validation Loss: 0.7562323808670044\n",
      "Epoch 3808: Training Loss: 0.26911670962969464 Validation Loss: 0.756171464920044\n",
      "Epoch 3809: Training Loss: 0.26931773622830707 Validation Loss: 0.7564771175384521\n",
      "Epoch 3810: Training Loss: 0.26927676796913147 Validation Loss: 0.7562059164047241\n",
      "Epoch 3811: Training Loss: 0.26891674598058063 Validation Loss: 0.7565935850143433\n",
      "Epoch 3812: Training Loss: 0.2688976029555003 Validation Loss: 0.7570395469665527\n",
      "Epoch 3813: Training Loss: 0.26975008348623913 Validation Loss: 0.7568578124046326\n",
      "Epoch 3814: Training Loss: 0.269724835952123 Validation Loss: 0.7567685842514038\n",
      "Epoch 3815: Training Loss: 0.2683773785829544 Validation Loss: 0.7567630410194397\n",
      "Epoch 3816: Training Loss: 0.268450270096461 Validation Loss: 0.7563570737838745\n",
      "Epoch 3817: Training Loss: 0.268347571293513 Validation Loss: 0.7560520768165588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3818: Training Loss: 0.2686343292395274 Validation Loss: 0.755750834941864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3819: Training Loss: 0.2682717740535736 Validation Loss: 0.7555043697357178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3820: Training Loss: 0.2688174396753311 Validation Loss: 0.7555661201477051\n",
      "Epoch 3821: Training Loss: 0.2682429800430934 Validation Loss: 0.7554526925086975\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3822: Training Loss: 0.26857949296633404 Validation Loss: 0.7556197643280029\n",
      "Epoch 3823: Training Loss: 0.26787803570429486 Validation Loss: 0.7556615471839905\n",
      "Epoch 3824: Training Loss: 0.2680843323469162 Validation Loss: 0.7560445070266724\n",
      "Epoch 3825: Training Loss: 0.2682043065627416 Validation Loss: 0.7562077045440674\n",
      "Epoch 3826: Training Loss: 0.268727367122968 Validation Loss: 0.7559062242507935\n",
      "Epoch 3827: Training Loss: 0.2679538478453954 Validation Loss: 0.7557860612869263\n",
      "Epoch 3828: Training Loss: 0.2669161061445872 Validation Loss: 0.7556665539741516\n",
      "Epoch 3829: Training Loss: 0.26745548844337463 Validation Loss: 0.7557123303413391\n",
      "Epoch 3830: Training Loss: 0.2679218202829361 Validation Loss: 0.7554990649223328\n",
      "Epoch 3831: Training Loss: 0.267713263630867 Validation Loss: 0.7555232048034668\n",
      "Epoch 3832: Training Loss: 0.26752251386642456 Validation Loss: 0.7558364272117615\n",
      "Epoch 3833: Training Loss: 0.2676004469394684 Validation Loss: 0.7558996081352234\n",
      "Epoch 3834: Training Loss: 0.2670524666706721 Validation Loss: 0.7554302215576172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3835: Training Loss: 0.26801501711209613 Validation Loss: 0.7551787495613098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3836: Training Loss: 0.2670811315377553 Validation Loss: 0.7550920844078064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3837: Training Loss: 0.2671239674091339 Validation Loss: 0.7550061345100403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3838: Training Loss: 0.26657341917355853 Validation Loss: 0.7551584243774414\n",
      "Epoch 3839: Training Loss: 0.2670375108718872 Validation Loss: 0.7551287412643433\n",
      "Epoch 3840: Training Loss: 0.2669035494327545 Validation Loss: 0.7549599409103394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3841: Training Loss: 0.26583271225293476 Validation Loss: 0.7551561594009399\n",
      "Epoch 3842: Training Loss: 0.2665632863839467 Validation Loss: 0.7551306486129761\n",
      "Epoch 3843: Training Loss: 0.26658230026563007 Validation Loss: 0.7553337812423706\n",
      "Epoch 3844: Training Loss: 0.2662063539028168 Validation Loss: 0.7553793787956238\n",
      "Epoch 3845: Training Loss: 0.26648162802060443 Validation Loss: 0.7550479173660278\n",
      "Epoch 3846: Training Loss: 0.2667516569296519 Validation Loss: 0.7549734115600586\n",
      "Epoch 3847: Training Loss: 0.26653896768887836 Validation Loss: 0.7548545598983765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3848: Training Loss: 0.26579123735427856 Validation Loss: 0.7547378540039062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3849: Training Loss: 0.26644983887672424 Validation Loss: 0.754747748374939\n",
      "Epoch 3850: Training Loss: 0.2655131667852402 Validation Loss: 0.7545977234840393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3851: Training Loss: 0.26710021992524463 Validation Loss: 0.7544789910316467\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3852: Training Loss: 0.26530252893765766 Validation Loss: 0.754644513130188\n",
      "Epoch 3853: Training Loss: 0.26642315089702606 Validation Loss: 0.7549470663070679\n",
      "Epoch 3854: Training Loss: 0.26586322983105976 Validation Loss: 0.7549706697463989\n",
      "Epoch 3855: Training Loss: 0.26662424206733704 Validation Loss: 0.7554640173912048\n",
      "Epoch 3856: Training Loss: 0.26605187853177387 Validation Loss: 0.7547362446784973\n",
      "Epoch 3857: Training Loss: 0.2650502969821294 Validation Loss: 0.7547338604927063\n",
      "Epoch 3858: Training Loss: 0.2658921182155609 Validation Loss: 0.7544450163841248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3859: Training Loss: 0.26541657249132794 Validation Loss: 0.7543349862098694\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3860: Training Loss: 0.26503580808639526 Validation Loss: 0.7540535926818848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3861: Training Loss: 0.2661225348711014 Validation Loss: 0.754491925239563\n",
      "Epoch 3862: Training Loss: 0.2654430866241455 Validation Loss: 0.754402220249176\n",
      "Epoch 3863: Training Loss: 0.2650434573491414 Validation Loss: 0.7544021606445312\n",
      "Epoch 3864: Training Loss: 0.2657509694496791 Validation Loss: 0.7550239562988281\n",
      "Epoch 3865: Training Loss: 0.26519911487897235 Validation Loss: 0.7549828886985779\n",
      "Epoch 3866: Training Loss: 0.26605947812398273 Validation Loss: 0.7546185851097107\n",
      "Epoch 3867: Training Loss: 0.26560308039188385 Validation Loss: 0.7542720437049866\n",
      "Epoch 3868: Training Loss: 0.2645214597384135 Validation Loss: 0.7542515397071838\n",
      "Epoch 3869: Training Loss: 0.2644134412209193 Validation Loss: 0.7539566159248352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3870: Training Loss: 0.2646471858024597 Validation Loss: 0.7535555362701416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3871: Training Loss: 0.2651187678178151 Validation Loss: 0.7538888454437256\n",
      "Epoch 3872: Training Loss: 0.26379800339539844 Validation Loss: 0.7541743516921997\n",
      "Epoch 3873: Training Loss: 0.26385413110256195 Validation Loss: 0.7541949152946472\n",
      "Epoch 3874: Training Loss: 0.2653263509273529 Validation Loss: 0.7541833519935608\n",
      "Epoch 3875: Training Loss: 0.2641788423061371 Validation Loss: 0.7538849115371704\n",
      "Epoch 3876: Training Loss: 0.2638312876224518 Validation Loss: 0.7542641758918762\n",
      "Epoch 3877: Training Loss: 0.2648889869451523 Validation Loss: 0.7541294693946838\n",
      "Epoch 3878: Training Loss: 0.26377106706301373 Validation Loss: 0.7537488341331482\n",
      "Epoch 3879: Training Loss: 0.26396939158439636 Validation Loss: 0.7535743713378906\n",
      "Epoch 3880: Training Loss: 0.2640339980522792 Validation Loss: 0.7534443736076355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3881: Training Loss: 0.26396653552850086 Validation Loss: 0.753187358379364\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3882: Training Loss: 0.2642241617043813 Validation Loss: 0.753385066986084\n",
      "Epoch 3883: Training Loss: 0.2648581614096959 Validation Loss: 0.7536700963973999\n",
      "Epoch 3884: Training Loss: 0.2633758435646693 Validation Loss: 0.753568708896637\n",
      "Epoch 3885: Training Loss: 0.2644082009792328 Validation Loss: 0.7535848021507263\n",
      "Epoch 3886: Training Loss: 0.26333598295847577 Validation Loss: 0.7540509104728699\n",
      "Epoch 3887: Training Loss: 0.26326342423756915 Validation Loss: 0.7539179921150208\n",
      "Epoch 3888: Training Loss: 0.2640112489461899 Validation Loss: 0.754216194152832\n",
      "Epoch 3889: Training Loss: 0.26327380041281384 Validation Loss: 0.7540782690048218\n",
      "Epoch 3890: Training Loss: 0.26321841776371 Validation Loss: 0.7539560198783875\n",
      "Epoch 3891: Training Loss: 0.26412364343802136 Validation Loss: 0.7541646361351013\n",
      "Epoch 3892: Training Loss: 0.26276809970537823 Validation Loss: 0.753667414188385\n",
      "Epoch 3893: Training Loss: 0.2626856565475464 Validation Loss: 0.7531334757804871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3894: Training Loss: 0.2624303748210271 Validation Loss: 0.7529436945915222\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3895: Training Loss: 0.2632635881503423 Validation Loss: 0.7527387738227844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3896: Training Loss: 0.262801726659139 Validation Loss: 0.7529416680335999\n",
      "Epoch 3897: Training Loss: 0.26230989893277484 Validation Loss: 0.7528988718986511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3898: Training Loss: 0.26261604328950244 Validation Loss: 0.7529953122138977\n",
      "Epoch 3899: Training Loss: 0.26246946056683856 Validation Loss: 0.7532832622528076\n",
      "Epoch 3900: Training Loss: 0.26223838329315186 Validation Loss: 0.75346839427948\n",
      "Epoch 3901: Training Loss: 0.26269061863422394 Validation Loss: 0.7533668875694275\n",
      "Epoch 3902: Training Loss: 0.2622087448835373 Validation Loss: 0.7529789805412292\n",
      "Epoch 3903: Training Loss: 0.2618791460990906 Validation Loss: 0.7526273727416992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3904: Training Loss: 0.262914980451266 Validation Loss: 0.7525451183319092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3905: Training Loss: 0.2620092531045278 Validation Loss: 0.7527428269386292\n",
      "Epoch 3906: Training Loss: 0.2615944842497508 Validation Loss: 0.7525591850280762\n",
      "Epoch 3907: Training Loss: 0.26210661232471466 Validation Loss: 0.752846896648407\n",
      "Epoch 3908: Training Loss: 0.26187219222386676 Validation Loss: 0.753045916557312\n",
      "Epoch 3909: Training Loss: 0.2619498570760091 Validation Loss: 0.7533206343650818\n",
      "Epoch 3910: Training Loss: 0.26144029200077057 Validation Loss: 0.7531006336212158\n",
      "Epoch 3911: Training Loss: 0.26269034047921497 Validation Loss: 0.7530954480171204\n",
      "Epoch 3912: Training Loss: 0.2637544423341751 Validation Loss: 0.7529465556144714\n",
      "Epoch 3913: Training Loss: 0.2618199984232585 Validation Loss: 0.7532710433006287\n",
      "Epoch 3914: Training Loss: 0.2627748101949692 Validation Loss: 0.7535114884376526\n",
      "Epoch 3915: Training Loss: 0.2611914078394572 Validation Loss: 0.7529626488685608\n",
      "Epoch 3916: Training Loss: 0.2604696750640869 Validation Loss: 0.752493679523468\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3917: Training Loss: 0.26108978192011517 Validation Loss: 0.7525928616523743\n",
      "Epoch 3918: Training Loss: 0.26112914582093555 Validation Loss: 0.7523658275604248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3919: Training Loss: 0.261355256040891 Validation Loss: 0.7526276707649231\n",
      "Epoch 3920: Training Loss: 0.26146270831425983 Validation Loss: 0.752598226070404\n",
      "Epoch 3921: Training Loss: 0.2606556912263234 Validation Loss: 0.7522823810577393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3922: Training Loss: 0.2610770563284556 Validation Loss: 0.7519758939743042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3923: Training Loss: 0.2606613487005234 Validation Loss: 0.7518154978752136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3924: Training Loss: 0.2607917785644531 Validation Loss: 0.7520807981491089\n",
      "Epoch 3925: Training Loss: 0.26010899742444354 Validation Loss: 0.7520581483840942\n",
      "Epoch 3926: Training Loss: 0.2604132493336995 Validation Loss: 0.7520508170127869\n",
      "Epoch 3927: Training Loss: 0.2601276735464732 Validation Loss: 0.7524047493934631\n",
      "Epoch 3928: Training Loss: 0.2598813623189926 Validation Loss: 0.7520462870597839\n",
      "Epoch 3929: Training Loss: 0.2602010468641917 Validation Loss: 0.7520098686218262\n",
      "Epoch 3930: Training Loss: 0.2598884304364522 Validation Loss: 0.7521706223487854\n",
      "Epoch 3931: Training Loss: 0.2602234333753586 Validation Loss: 0.7525257468223572\n",
      "Epoch 3932: Training Loss: 0.2607335150241852 Validation Loss: 0.7519978880882263\n",
      "Epoch 3933: Training Loss: 0.26117858787377674 Validation Loss: 0.7516897320747375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3934: Training Loss: 0.25994107127189636 Validation Loss: 0.7521138191223145\n",
      "Epoch 3935: Training Loss: 0.2598610321680705 Validation Loss: 0.7523969411849976\n",
      "Epoch 3936: Training Loss: 0.2597655306259791 Validation Loss: 0.7525172829627991\n",
      "Epoch 3937: Training Loss: 0.2596152226130168 Validation Loss: 0.7524234652519226\n",
      "Epoch 3938: Training Loss: 0.2594712475935618 Validation Loss: 0.7522832155227661\n",
      "Epoch 3939: Training Loss: 0.25942428906758624 Validation Loss: 0.7524783611297607\n",
      "Epoch 3940: Training Loss: 0.2594446043173472 Validation Loss: 0.751899242401123\n",
      "Epoch 3941: Training Loss: 0.2594413757324219 Validation Loss: 0.751547634601593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3942: Training Loss: 0.25979584952195484 Validation Loss: 0.7511872053146362\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3943: Training Loss: 0.2591915726661682 Validation Loss: 0.751001238822937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3944: Training Loss: 0.25955676039059955 Validation Loss: 0.7512456178665161\n",
      "Epoch 3945: Training Loss: 0.25943103929360706 Validation Loss: 0.751457929611206\n",
      "Epoch 3946: Training Loss: 0.25988365709781647 Validation Loss: 0.7516655921936035\n",
      "Epoch 3947: Training Loss: 0.2605046033859253 Validation Loss: 0.7514247298240662\n",
      "Epoch 3948: Training Loss: 0.25858167310555774 Validation Loss: 0.7515096664428711\n",
      "Epoch 3949: Training Loss: 0.2598727097113927 Validation Loss: 0.7517291307449341\n",
      "Epoch 3950: Training Loss: 0.26042569677035016 Validation Loss: 0.7516778707504272\n",
      "Epoch 3951: Training Loss: 0.2589467018842697 Validation Loss: 0.7514049410820007\n",
      "Epoch 3952: Training Loss: 0.25842827558517456 Validation Loss: 0.7511917352676392\n",
      "Epoch 3953: Training Loss: 0.25888749460379284 Validation Loss: 0.7509192824363708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3954: Training Loss: 0.25940783818562824 Validation Loss: 0.751068115234375\n",
      "Epoch 3955: Training Loss: 0.25826974709828693 Validation Loss: 0.7513201236724854\n",
      "Epoch 3956: Training Loss: 0.2582281430562337 Validation Loss: 0.751400351524353\n",
      "Epoch 3957: Training Loss: 0.258183886607488 Validation Loss: 0.7517625093460083\n",
      "Epoch 3958: Training Loss: 0.25805368026097614 Validation Loss: 0.751558244228363\n",
      "Epoch 3959: Training Loss: 0.25820739567279816 Validation Loss: 0.7517935037612915\n",
      "Epoch 3960: Training Loss: 0.2585501968860626 Validation Loss: 0.7511317729949951\n",
      "Epoch 3961: Training Loss: 0.25822093586126965 Validation Loss: 0.7511993646621704\n",
      "Epoch 3962: Training Loss: 0.25789612034956616 Validation Loss: 0.750923752784729\n",
      "Epoch 3963: Training Loss: 0.2576678991317749 Validation Loss: 0.7507756948471069\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3964: Training Loss: 0.25765689214070636 Validation Loss: 0.7505444884300232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3965: Training Loss: 0.2577921152114868 Validation Loss: 0.7506476640701294\n",
      "Epoch 3966: Training Loss: 0.25762325525283813 Validation Loss: 0.750727117061615\n",
      "Epoch 3967: Training Loss: 0.25790949662526447 Validation Loss: 0.750566840171814\n",
      "Epoch 3968: Training Loss: 0.25771403312683105 Validation Loss: 0.750742495059967\n",
      "Epoch 3969: Training Loss: 0.2572656174500783 Validation Loss: 0.7507935166358948\n",
      "Epoch 3970: Training Loss: 0.25726426641146344 Validation Loss: 0.751063346862793\n",
      "Epoch 3971: Training Loss: 0.25715629756450653 Validation Loss: 0.7511054277420044\n",
      "Epoch 3972: Training Loss: 0.2570187697807948 Validation Loss: 0.7509264945983887\n",
      "Epoch 3973: Training Loss: 0.257393479347229 Validation Loss: 0.7506346702575684\n",
      "Epoch 3974: Training Loss: 0.257039338350296 Validation Loss: 0.7504254579544067\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3975: Training Loss: 0.2576214522123337 Validation Loss: 0.7504450082778931\n",
      "Epoch 3976: Training Loss: 0.258665790160497 Validation Loss: 0.7507462501525879\n",
      "Epoch 3977: Training Loss: 0.25708748896916706 Validation Loss: 0.7509828209877014\n",
      "Epoch 3978: Training Loss: 0.256632203857104 Validation Loss: 0.751207172870636\n",
      "Epoch 3979: Training Loss: 0.2573586205641429 Validation Loss: 0.7510101795196533\n",
      "Epoch 3980: Training Loss: 0.2565438648064931 Validation Loss: 0.7505070567131042\n",
      "Epoch 3981: Training Loss: 0.25636446475982666 Validation Loss: 0.7503237128257751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3982: Training Loss: 0.25721461077531177 Validation Loss: 0.7504136562347412\n",
      "Epoch 3983: Training Loss: 0.25612760583559674 Validation Loss: 0.7501783967018127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3984: Training Loss: 0.2563639183839162 Validation Loss: 0.7501264810562134\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3985: Training Loss: 0.2567172000805537 Validation Loss: 0.750037431716919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3986: Training Loss: 0.2559480567773183 Validation Loss: 0.7503306269645691\n",
      "Epoch 3987: Training Loss: 0.2562999228636424 Validation Loss: 0.7507526874542236\n",
      "Epoch 3988: Training Loss: 0.25628886620203656 Validation Loss: 0.7509410381317139\n",
      "Epoch 3989: Training Loss: 0.25595565636952716 Validation Loss: 0.7506371140480042\n",
      "Epoch 3990: Training Loss: 0.25617071489493054 Validation Loss: 0.7500897645950317\n",
      "Epoch 3991: Training Loss: 0.2569833646217982 Validation Loss: 0.7503218650817871\n",
      "Epoch 3992: Training Loss: 0.2558423578739166 Validation Loss: 0.7498352527618408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3993: Training Loss: 0.25568054616451263 Validation Loss: 0.7498944401741028\n",
      "Epoch 3994: Training Loss: 0.2558278739452362 Validation Loss: 0.7500201463699341\n",
      "Epoch 3995: Training Loss: 0.255456601579984 Validation Loss: 0.7498688101768494\n",
      "Epoch 3996: Training Loss: 0.2556822697321574 Validation Loss: 0.7497077584266663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3997: Training Loss: 0.2567079911629359 Validation Loss: 0.749821126461029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3998: Training Loss: 0.2556220044692357 Validation Loss: 0.7498611211776733\n",
      "Epoch 3999: Training Loss: 0.2558412452538808 Validation Loss: 0.7500536441802979\n",
      "Epoch 4000: Training Loss: 0.25503753622372943 Validation Loss: 0.7501674890518188\n",
      "Epoch 4001: Training Loss: 0.25506222744782764 Validation Loss: 0.7500558495521545\n",
      "Epoch 4002: Training Loss: 0.25544212758541107 Validation Loss: 0.7497954368591309\n",
      "Epoch 4003: Training Loss: 0.25531244774659473 Validation Loss: 0.7492263913154602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4004: Training Loss: 0.2548656513293584 Validation Loss: 0.7493789196014404\n",
      "Epoch 4005: Training Loss: 0.254833847284317 Validation Loss: 0.7496272921562195\n",
      "Epoch 4006: Training Loss: 0.25436640282471973 Validation Loss: 0.7499651908874512\n",
      "Epoch 4007: Training Loss: 0.2545928359031677 Validation Loss: 0.7496163249015808\n",
      "Epoch 4008: Training Loss: 0.2545940577983856 Validation Loss: 0.7494547367095947\n",
      "Epoch 4009: Training Loss: 0.2543133795261383 Validation Loss: 0.7495229244232178\n",
      "Epoch 4010: Training Loss: 0.25454436739285785 Validation Loss: 0.7497246265411377\n",
      "Epoch 4011: Training Loss: 0.2547539273897807 Validation Loss: 0.7497715950012207\n",
      "Epoch 4012: Training Loss: 0.25412868956724805 Validation Loss: 0.749765932559967\n",
      "Epoch 4013: Training Loss: 0.25431545078754425 Validation Loss: 0.749516487121582\n",
      "Epoch 4014: Training Loss: 0.2548772643009822 Validation Loss: 0.7495531439781189\n",
      "Epoch 4015: Training Loss: 0.25331270198027295 Validation Loss: 0.7495182156562805\n",
      "Epoch 4016: Training Loss: 0.25603975852330524 Validation Loss: 0.7490572929382324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4017: Training Loss: 0.2542770306269328 Validation Loss: 0.7491346001625061\n",
      "Epoch 4018: Training Loss: 0.2545069307088852 Validation Loss: 0.7492627501487732\n",
      "Epoch 4019: Training Loss: 0.25444063544273376 Validation Loss: 0.7493487000465393\n",
      "Epoch 4020: Training Loss: 0.2539123147726059 Validation Loss: 0.7492310404777527\n",
      "Epoch 4021: Training Loss: 0.2548215836286545 Validation Loss: 0.7491321563720703\n",
      "Epoch 4022: Training Loss: 0.2541099687417348 Validation Loss: 0.7491892576217651\n",
      "Epoch 4023: Training Loss: 0.2547263403733571 Validation Loss: 0.7494977712631226\n",
      "Epoch 4024: Training Loss: 0.2545188417037328 Validation Loss: 0.7497864961624146\n",
      "Epoch 4025: Training Loss: 0.25365229447682697 Validation Loss: 0.7501072287559509\n",
      "Epoch 4026: Training Loss: 0.2534249424934387 Validation Loss: 0.7497755885124207\n",
      "Epoch 4027: Training Loss: 0.2536672502756119 Validation Loss: 0.7491845488548279\n",
      "Epoch 4028: Training Loss: 0.2526552528142929 Validation Loss: 0.7489638924598694\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4029: Training Loss: 0.25391403834025067 Validation Loss: 0.7490574717521667\n",
      "Epoch 4030: Training Loss: 0.2530723661184311 Validation Loss: 0.7489358186721802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4031: Training Loss: 0.2537447313467662 Validation Loss: 0.7489067316055298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4032: Training Loss: 0.25325843691825867 Validation Loss: 0.7489001750946045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4033: Training Loss: 0.2527046451965968 Validation Loss: 0.749139666557312\n",
      "Epoch 4034: Training Loss: 0.25295528769493103 Validation Loss: 0.7490650415420532\n",
      "Epoch 4035: Training Loss: 0.253964826464653 Validation Loss: 0.7489022016525269\n",
      "Epoch 4036: Training Loss: 0.2525727649529775 Validation Loss: 0.7484205961227417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4037: Training Loss: 0.2532515376806259 Validation Loss: 0.7480217814445496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4038: Training Loss: 0.2528470704952876 Validation Loss: 0.7485925555229187\n",
      "Epoch 4039: Training Loss: 0.25317207475503284 Validation Loss: 0.7488039135932922\n",
      "Epoch 4040: Training Loss: 0.25227900842825574 Validation Loss: 0.7485396265983582\n",
      "Epoch 4041: Training Loss: 0.2524464428424835 Validation Loss: 0.7484591007232666\n",
      "Epoch 4042: Training Loss: 0.2522473881642024 Validation Loss: 0.7483057975769043\n",
      "Epoch 4043: Training Loss: 0.25202523171901703 Validation Loss: 0.7484211921691895\n",
      "Epoch 4044: Training Loss: 0.25201795995235443 Validation Loss: 0.7486949563026428\n",
      "Epoch 4045: Training Loss: 0.25196154912312824 Validation Loss: 0.7485013604164124\n",
      "Epoch 4046: Training Loss: 0.2526182333628337 Validation Loss: 0.7481955289840698\n",
      "Epoch 4047: Training Loss: 0.25151898960272473 Validation Loss: 0.7482031583786011\n",
      "Epoch 4048: Training Loss: 0.2519391675790151 Validation Loss: 0.7485499382019043\n",
      "Epoch 4049: Training Loss: 0.25231972833474475 Validation Loss: 0.7483577728271484\n",
      "Epoch 4050: Training Loss: 0.2524498701095581 Validation Loss: 0.7482231259346008\n",
      "Epoch 4051: Training Loss: 0.25141844650109607 Validation Loss: 0.7483332753181458\n",
      "Epoch 4052: Training Loss: 0.25158966581026715 Validation Loss: 0.7484938502311707\n",
      "Epoch 4053: Training Loss: 0.2517106582721074 Validation Loss: 0.7480853199958801\n",
      "Epoch 4054: Training Loss: 0.2516515503327052 Validation Loss: 0.748063862323761\n",
      "Epoch 4055: Training Loss: 0.2523457656304042 Validation Loss: 0.7483135461807251\n",
      "Epoch 4056: Training Loss: 0.251261830329895 Validation Loss: 0.7487545609474182\n",
      "Epoch 4057: Training Loss: 0.2527061551809311 Validation Loss: 0.7484847903251648\n",
      "Epoch 4058: Training Loss: 0.2514052391052246 Validation Loss: 0.748284637928009\n",
      "Epoch 4059: Training Loss: 0.25112833082675934 Validation Loss: 0.7482026815414429\n",
      "Epoch 4060: Training Loss: 0.25114094217618305 Validation Loss: 0.7480402588844299\n",
      "Epoch 4061: Training Loss: 0.2511350115140279 Validation Loss: 0.7482994794845581\n",
      "Epoch 4062: Training Loss: 0.2510024309158325 Validation Loss: 0.7485496997833252\n",
      "Epoch 4063: Training Loss: 0.25118254621823627 Validation Loss: 0.7486559748649597\n",
      "Epoch 4064: Training Loss: 0.2510717461506526 Validation Loss: 0.7483929991722107\n",
      "Epoch 4065: Training Loss: 0.2513372500737508 Validation Loss: 0.7480472922325134\n",
      "Epoch 4066: Training Loss: 0.25090861817200977 Validation Loss: 0.7478822469711304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4067: Training Loss: 0.2508687923351924 Validation Loss: 0.7474097609519958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4068: Training Loss: 0.2510182211796443 Validation Loss: 0.7471888065338135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4069: Training Loss: 0.2511047621568044 Validation Loss: 0.7478419542312622\n",
      "Epoch 4070: Training Loss: 0.2503247757752736 Validation Loss: 0.7481290698051453\n",
      "Epoch 4071: Training Loss: 0.2505267560482025 Validation Loss: 0.7481433749198914\n",
      "Epoch 4072: Training Loss: 0.25011033316453296 Validation Loss: 0.7482531666755676\n",
      "Epoch 4073: Training Loss: 0.2505074640115102 Validation Loss: 0.7485465407371521\n",
      "Epoch 4074: Training Loss: 0.2506081660588582 Validation Loss: 0.7486569881439209\n",
      "Epoch 4075: Training Loss: 0.2499876767396927 Validation Loss: 0.7479073405265808\n",
      "Epoch 4076: Training Loss: 0.25038841863473255 Validation Loss: 0.74749356508255\n",
      "Epoch 4077: Training Loss: 0.2500150104363759 Validation Loss: 0.7475944757461548\n",
      "Epoch 4078: Training Loss: 0.2504516790310542 Validation Loss: 0.7478840947151184\n",
      "Epoch 4079: Training Loss: 0.2504696150620778 Validation Loss: 0.7474014759063721\n",
      "Epoch 4080: Training Loss: 0.24994399646917978 Validation Loss: 0.7469395995140076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4081: Training Loss: 0.24993087848027548 Validation Loss: 0.747037410736084\n",
      "Epoch 4082: Training Loss: 0.24921029806137085 Validation Loss: 0.7472032904624939\n",
      "Epoch 4083: Training Loss: 0.25015680988629657 Validation Loss: 0.7473570108413696\n",
      "Epoch 4084: Training Loss: 0.25123514235019684 Validation Loss: 0.7475187182426453\n",
      "Epoch 4085: Training Loss: 0.24952454368273416 Validation Loss: 0.7474425435066223\n",
      "Epoch 4086: Training Loss: 0.24959346652030945 Validation Loss: 0.7475255131721497\n",
      "Epoch 4087: Training Loss: 0.24998796979586282 Validation Loss: 0.7472884058952332\n",
      "Epoch 4088: Training Loss: 0.24938817818959555 Validation Loss: 0.7473824620246887\n",
      "Epoch 4089: Training Loss: 0.24999764561653137 Validation Loss: 0.7470552921295166\n",
      "Epoch 4090: Training Loss: 0.24979453285535178 Validation Loss: 0.7467934489250183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4091: Training Loss: 0.2495020478963852 Validation Loss: 0.7477842569351196\n",
      "Epoch 4092: Training Loss: 0.2496443192164103 Validation Loss: 0.747714638710022\n",
      "Epoch 4093: Training Loss: 0.24947255353132883 Validation Loss: 0.7475570440292358\n",
      "Epoch 4094: Training Loss: 0.2494764725367228 Validation Loss: 0.747532308101654\n",
      "Epoch 4095: Training Loss: 0.24920419851938883 Validation Loss: 0.74725341796875\n",
      "Epoch 4096: Training Loss: 0.24879607558250427 Validation Loss: 0.7471392154693604\n",
      "Epoch 4097: Training Loss: 0.24875115354855856 Validation Loss: 0.7473171353340149\n",
      "Epoch 4098: Training Loss: 0.2489141176144282 Validation Loss: 0.7474856972694397\n",
      "Epoch 4099: Training Loss: 0.2487432062625885 Validation Loss: 0.7471092343330383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4100: Training Loss: 0.2487088938554128 Validation Loss: 0.7474905252456665\n",
      "Epoch 4101: Training Loss: 0.2484580526749293 Validation Loss: 0.747434675693512\n",
      "Epoch 4102: Training Loss: 0.2482762187719345 Validation Loss: 0.7469983100891113\n",
      "Epoch 4103: Training Loss: 0.24865709245204926 Validation Loss: 0.7470034956932068\n",
      "Epoch 4104: Training Loss: 0.24839653074741364 Validation Loss: 0.7464198470115662\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4105: Training Loss: 0.24841884275277457 Validation Loss: 0.7463487386703491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4106: Training Loss: 0.2478403995434443 Validation Loss: 0.7465163469314575\n",
      "Epoch 4107: Training Loss: 0.24813958505789438 Validation Loss: 0.7466141581535339\n",
      "Epoch 4108: Training Loss: 0.24832209944725037 Validation Loss: 0.7468334436416626\n",
      "Epoch 4109: Training Loss: 0.2479784538348516 Validation Loss: 0.7468607425689697\n",
      "Epoch 4110: Training Loss: 0.24795242647329965 Validation Loss: 0.7468388676643372\n",
      "Epoch 4111: Training Loss: 0.2481173574924469 Validation Loss: 0.7472078204154968\n",
      "Epoch 4112: Training Loss: 0.24816088378429413 Validation Loss: 0.7466931343078613\n",
      "Epoch 4113: Training Loss: 0.24778934816519418 Validation Loss: 0.7463699579238892\n",
      "Epoch 4114: Training Loss: 0.24764211972554526 Validation Loss: 0.7464569211006165\n",
      "Epoch 4115: Training Loss: 0.2482171654701233 Validation Loss: 0.7467926144599915\n",
      "Epoch 4116: Training Loss: 0.24765081703662872 Validation Loss: 0.7469527125358582\n",
      "Epoch 4117: Training Loss: 0.2478150576353073 Validation Loss: 0.746940553188324\n",
      "Epoch 4118: Training Loss: 0.24754034479459128 Validation Loss: 0.7465753555297852\n",
      "Epoch 4119: Training Loss: 0.24793440103530884 Validation Loss: 0.7464421987533569\n",
      "Epoch 4120: Training Loss: 0.24727614223957062 Validation Loss: 0.7466040253639221\n",
      "Epoch 4121: Training Loss: 0.2471767763296763 Validation Loss: 0.7465586066246033\n",
      "Epoch 4122: Training Loss: 0.2479047973950704 Validation Loss: 0.7464416027069092\n",
      "Epoch 4123: Training Loss: 0.2478958715995153 Validation Loss: 0.7462981939315796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4124: Training Loss: 0.2466712345679601 Validation Loss: 0.7462342381477356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4125: Training Loss: 0.24702726801236471 Validation Loss: 0.7462940216064453\n",
      "Epoch 4126: Training Loss: 0.24714510639508566 Validation Loss: 0.7464838624000549\n",
      "Epoch 4127: Training Loss: 0.2488589584827423 Validation Loss: 0.7460041642189026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4128: Training Loss: 0.24715863168239594 Validation Loss: 0.746677815914154\n",
      "Epoch 4129: Training Loss: 0.24746557076772055 Validation Loss: 0.7464139461517334\n",
      "Epoch 4130: Training Loss: 0.24645773073037466 Validation Loss: 0.7462474703788757\n",
      "Epoch 4131: Training Loss: 0.24663556118806204 Validation Loss: 0.7467429637908936\n",
      "Epoch 4132: Training Loss: 0.24658015867074332 Validation Loss: 0.7469208836555481\n",
      "Epoch 4133: Training Loss: 0.24644212424755096 Validation Loss: 0.7466067671775818\n",
      "Epoch 4134: Training Loss: 0.2466307282447815 Validation Loss: 0.7466115951538086\n",
      "Epoch 4135: Training Loss: 0.24601401388645172 Validation Loss: 0.7466709613800049\n",
      "Epoch 4136: Training Loss: 0.2464011957248052 Validation Loss: 0.7465579509735107\n",
      "Epoch 4137: Training Loss: 0.2465867946545283 Validation Loss: 0.7464640140533447\n",
      "Epoch 4138: Training Loss: 0.24562556544939676 Validation Loss: 0.7460148930549622\n",
      "Epoch 4139: Training Loss: 0.2457467863957087 Validation Loss: 0.7455760836601257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4140: Training Loss: 0.2465386539697647 Validation Loss: 0.7455892562866211\n",
      "Epoch 4141: Training Loss: 0.24588057895501456 Validation Loss: 0.7455657124519348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4142: Training Loss: 0.24578293661276499 Validation Loss: 0.7455717921257019\n",
      "Epoch 4143: Training Loss: 0.24581992626190186 Validation Loss: 0.7459269762039185\n",
      "Epoch 4144: Training Loss: 0.245635524392128 Validation Loss: 0.7459310293197632\n",
      "Epoch 4145: Training Loss: 0.24639332791169485 Validation Loss: 0.7456204891204834\n",
      "Epoch 4146: Training Loss: 0.24632413685321808 Validation Loss: 0.7459712624549866\n",
      "Epoch 4147: Training Loss: 0.24583456416924795 Validation Loss: 0.7457881569862366\n",
      "Epoch 4148: Training Loss: 0.2457942416270574 Validation Loss: 0.7456425428390503\n",
      "Epoch 4149: Training Loss: 0.245854119459788 Validation Loss: 0.7455747127532959\n",
      "Epoch 4150: Training Loss: 0.24498321115970612 Validation Loss: 0.7459385395050049\n",
      "Epoch 4151: Training Loss: 0.2465758522351583 Validation Loss: 0.74623042345047\n",
      "Epoch 4152: Training Loss: 0.24529099464416504 Validation Loss: 0.7464373707771301\n",
      "Epoch 4153: Training Loss: 0.2452490677436193 Validation Loss: 0.7460538148880005\n",
      "Epoch 4154: Training Loss: 0.24591072897116342 Validation Loss: 0.7459845542907715\n",
      "Epoch 4155: Training Loss: 0.24495694041252136 Validation Loss: 0.7459121942520142\n",
      "Epoch 4156: Training Loss: 0.24511131644248962 Validation Loss: 0.7454927563667297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4157: Training Loss: 0.24655622243881226 Validation Loss: 0.7453200817108154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4158: Training Loss: 0.24502315620581308 Validation Loss: 0.7454448342323303\n",
      "Epoch 4159: Training Loss: 0.24498211840788522 Validation Loss: 0.7456050515174866\n",
      "Epoch 4160: Training Loss: 0.24455155928929648 Validation Loss: 0.7458217144012451\n",
      "Epoch 4161: Training Loss: 0.244798481464386 Validation Loss: 0.7457898259162903\n",
      "Epoch 4162: Training Loss: 0.24644151826699576 Validation Loss: 0.7460007667541504\n",
      "Epoch 4163: Training Loss: 0.24467392762502035 Validation Loss: 0.745718240737915\n",
      "Epoch 4164: Training Loss: 0.24449507892131805 Validation Loss: 0.7450026869773865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4165: Training Loss: 0.24445908764998117 Validation Loss: 0.7448485493659973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4166: Training Loss: 0.24473976095517477 Validation Loss: 0.7451874613761902\n",
      "Epoch 4167: Training Loss: 0.24440977474053702 Validation Loss: 0.7451397776603699\n",
      "Epoch 4168: Training Loss: 0.2441627929608027 Validation Loss: 0.7450059652328491\n",
      "Epoch 4169: Training Loss: 0.24416557451089224 Validation Loss: 0.7447680830955505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4170: Training Loss: 0.24415894349416098 Validation Loss: 0.7448146939277649\n",
      "Epoch 4171: Training Loss: 0.24363982677459717 Validation Loss: 0.7450925707817078\n",
      "Epoch 4172: Training Loss: 0.24434374769528708 Validation Loss: 0.7453802227973938\n",
      "Epoch 4173: Training Loss: 0.24393694599469504 Validation Loss: 0.745918869972229\n",
      "Epoch 4174: Training Loss: 0.24435109396775564 Validation Loss: 0.7460561990737915\n",
      "Epoch 4175: Training Loss: 0.24550587932268778 Validation Loss: 0.7459830641746521\n",
      "Epoch 4176: Training Loss: 0.24439931412537894 Validation Loss: 0.745434045791626\n",
      "Epoch 4177: Training Loss: 0.24397254486878714 Validation Loss: 0.7452777028083801\n",
      "Epoch 4178: Training Loss: 0.2442640463511149 Validation Loss: 0.7454364895820618\n",
      "Epoch 4179: Training Loss: 0.2440958470106125 Validation Loss: 0.7454624772071838\n",
      "Epoch 4180: Training Loss: 0.24389528234799704 Validation Loss: 0.7450210452079773\n",
      "Epoch 4181: Training Loss: 0.24351882437864938 Validation Loss: 0.7451655864715576\n",
      "Epoch 4182: Training Loss: 0.24409701426823935 Validation Loss: 0.7448953986167908\n",
      "Epoch 4183: Training Loss: 0.24323343733946481 Validation Loss: 0.7446527481079102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4184: Training Loss: 0.24321964383125305 Validation Loss: 0.7446338534355164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4185: Training Loss: 0.24302751322587332 Validation Loss: 0.7446772456169128\n",
      "Epoch 4186: Training Loss: 0.24354296425978342 Validation Loss: 0.7446867823600769\n",
      "Epoch 4187: Training Loss: 0.24319510658582053 Validation Loss: 0.7448834180831909\n",
      "Epoch 4188: Training Loss: 0.24289963146050772 Validation Loss: 0.7450165152549744\n",
      "Epoch 4189: Training Loss: 0.24293427169322968 Validation Loss: 0.7452990412712097\n",
      "Epoch 4190: Training Loss: 0.24340601762135824 Validation Loss: 0.7446171641349792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4191: Training Loss: 0.24301816523075104 Validation Loss: 0.744428813457489\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4192: Training Loss: 0.24278783798217773 Validation Loss: 0.7444837093353271\n",
      "Epoch 4193: Training Loss: 0.24266672631104788 Validation Loss: 0.7448509335517883\n",
      "Epoch 4194: Training Loss: 0.2425386607646942 Validation Loss: 0.7449744343757629\n",
      "Epoch 4195: Training Loss: 0.24122894803682962 Validation Loss: 0.7449191808700562\n",
      "Epoch 4196: Training Loss: 0.24235310653845468 Validation Loss: 0.7448838949203491\n",
      "Epoch 4197: Training Loss: 0.2422458976507187 Validation Loss: 0.7451766133308411\n",
      "Epoch 4198: Training Loss: 0.2436068058013916 Validation Loss: 0.7448351383209229\n",
      "Epoch 4199: Training Loss: 0.2421822448571523 Validation Loss: 0.7447122931480408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4200: Training Loss: 0.24160032471021017 Validation Loss: 0.7443685531616211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4201: Training Loss: 0.2422291487455368 Validation Loss: 0.7444694638252258\n",
      "Epoch 4202: Training Loss: 0.2431268443663915 Validation Loss: 0.7445927262306213\n",
      "Epoch 4203: Training Loss: 0.24146880706151327 Validation Loss: 0.7444498538970947\n",
      "Epoch 4204: Training Loss: 0.2427046149969101 Validation Loss: 0.7443112730979919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4205: Training Loss: 0.24190534154574075 Validation Loss: 0.7444295287132263\n",
      "Epoch 4206: Training Loss: 0.24289959172407785 Validation Loss: 0.7447332739830017\n",
      "Epoch 4207: Training Loss: 0.24189206461111704 Validation Loss: 0.7446495890617371\n",
      "Epoch 4208: Training Loss: 0.24157809714476267 Validation Loss: 0.744565486907959\n",
      "Epoch 4209: Training Loss: 0.24154533445835114 Validation Loss: 0.7442647218704224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4210: Training Loss: 0.2415552387634913 Validation Loss: 0.744391143321991\n",
      "Epoch 4211: Training Loss: 0.24133860071500143 Validation Loss: 0.7446177005767822\n",
      "Epoch 4212: Training Loss: 0.24321054418881735 Validation Loss: 0.745044469833374\n",
      "Epoch 4213: Training Loss: 0.24156432847181955 Validation Loss: 0.7447985410690308\n",
      "Epoch 4214: Training Loss: 0.24141229192415872 Validation Loss: 0.7442991733551025\n",
      "Epoch 4215: Training Loss: 0.242411936322848 Validation Loss: 0.7437659502029419\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4216: Training Loss: 0.24114877978960672 Validation Loss: 0.7441065311431885\n",
      "Epoch 4217: Training Loss: 0.24145682156085968 Validation Loss: 0.7443790435791016\n",
      "Epoch 4218: Training Loss: 0.24131708840529123 Validation Loss: 0.7443970441818237\n",
      "Epoch 4219: Training Loss: 0.2413411339124044 Validation Loss: 0.7443265914916992\n",
      "Epoch 4220: Training Loss: 0.2407294660806656 Validation Loss: 0.7446466088294983\n",
      "Epoch 4221: Training Loss: 0.24096021056175232 Validation Loss: 0.7442678809165955\n",
      "Epoch 4222: Training Loss: 0.24211907386779785 Validation Loss: 0.7441101670265198\n",
      "Epoch 4223: Training Loss: 0.2409818967183431 Validation Loss: 0.743735134601593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4224: Training Loss: 0.24080023169517517 Validation Loss: 0.743679940700531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4225: Training Loss: 0.24096190432707468 Validation Loss: 0.7438669204711914\n",
      "Epoch 4226: Training Loss: 0.24065395692984262 Validation Loss: 0.7435595989227295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4227: Training Loss: 0.24060445527235666 Validation Loss: 0.7433034777641296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4228: Training Loss: 0.240889439980189 Validation Loss: 0.7438193559646606\n",
      "Epoch 4229: Training Loss: 0.2417337199052175 Validation Loss: 0.7442873120307922\n",
      "Epoch 4230: Training Loss: 0.24084603786468506 Validation Loss: 0.7441849708557129\n",
      "Epoch 4231: Training Loss: 0.24020926157633463 Validation Loss: 0.7443587779998779\n",
      "Epoch 4232: Training Loss: 0.24213027457396188 Validation Loss: 0.7442181706428528\n",
      "Epoch 4233: Training Loss: 0.24023890495300293 Validation Loss: 0.7442145347595215\n",
      "Epoch 4234: Training Loss: 0.24042280515034994 Validation Loss: 0.7440432906150818\n",
      "Epoch 4235: Training Loss: 0.240026926000913 Validation Loss: 0.7437262535095215\n",
      "Epoch 4236: Training Loss: 0.2403721958398819 Validation Loss: 0.7434496283531189\n",
      "Epoch 4237: Training Loss: 0.24012057979901633 Validation Loss: 0.7434375882148743\n",
      "Epoch 4238: Training Loss: 0.23901724815368652 Validation Loss: 0.7436763644218445\n",
      "Epoch 4239: Training Loss: 0.24006983637809753 Validation Loss: 0.7436919212341309\n",
      "Epoch 4240: Training Loss: 0.2400747388601303 Validation Loss: 0.7438247799873352\n",
      "Epoch 4241: Training Loss: 0.2401578724384308 Validation Loss: 0.7434875965118408\n",
      "Epoch 4242: Training Loss: 0.2396672467390696 Validation Loss: 0.7440073490142822\n",
      "Epoch 4243: Training Loss: 0.23979715506235758 Validation Loss: 0.743740975856781\n",
      "Epoch 4244: Training Loss: 0.2395453949769338 Validation Loss: 0.7436006665229797\n",
      "Epoch 4245: Training Loss: 0.23952536284923553 Validation Loss: 0.743474006652832\n",
      "Epoch 4246: Training Loss: 0.23950747648874918 Validation Loss: 0.7430619597434998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4247: Training Loss: 0.2396102100610733 Validation Loss: 0.7430869340896606\n",
      "Epoch 4248: Training Loss: 0.23887980977694193 Validation Loss: 0.7429842948913574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4249: Training Loss: 0.23978580037752786 Validation Loss: 0.7436968684196472\n",
      "Epoch 4250: Training Loss: 0.23936444024244943 Validation Loss: 0.7440118789672852\n",
      "Epoch 4251: Training Loss: 0.24001768231391907 Validation Loss: 0.7444626688957214\n",
      "Epoch 4252: Training Loss: 0.2399074286222458 Validation Loss: 0.743587851524353\n",
      "Epoch 4253: Training Loss: 0.23942061762015024 Validation Loss: 0.7432504892349243\n",
      "Epoch 4254: Training Loss: 0.239242821931839 Validation Loss: 0.7431598901748657\n",
      "Epoch 4255: Training Loss: 0.2387702465057373 Validation Loss: 0.7429672479629517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4256: Training Loss: 0.2386972258488337 Validation Loss: 0.7433677911758423\n",
      "Epoch 4257: Training Loss: 0.23914405206839243 Validation Loss: 0.7438119053840637\n",
      "Epoch 4258: Training Loss: 0.2384459525346756 Validation Loss: 0.7439883351325989\n",
      "Epoch 4259: Training Loss: 0.23898364106814066 Validation Loss: 0.7435294389724731\n",
      "Epoch 4260: Training Loss: 0.23878230651219687 Validation Loss: 0.7433707118034363\n",
      "Epoch 4261: Training Loss: 0.23928464452425638 Validation Loss: 0.7433989644050598\n",
      "Epoch 4262: Training Loss: 0.2385298510392507 Validation Loss: 0.7428774833679199\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4263: Training Loss: 0.2383376012245814 Validation Loss: 0.7428878545761108\n",
      "Epoch 4264: Training Loss: 0.23883617917696634 Validation Loss: 0.7427117228507996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4265: Training Loss: 0.2383801539738973 Validation Loss: 0.7427994608879089\n",
      "Epoch 4266: Training Loss: 0.23875311017036438 Validation Loss: 0.742944061756134\n",
      "Epoch 4267: Training Loss: 0.23810856540997824 Validation Loss: 0.7429443001747131\n",
      "Epoch 4268: Training Loss: 0.23819672067960104 Validation Loss: 0.7428199052810669\n",
      "Epoch 4269: Training Loss: 0.23828336596488953 Validation Loss: 0.7425649166107178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4270: Training Loss: 0.2380226751168569 Validation Loss: 0.7431520223617554\n",
      "Epoch 4271: Training Loss: 0.23807877798875174 Validation Loss: 0.7432718873023987\n",
      "Epoch 4272: Training Loss: 0.2380554030338923 Validation Loss: 0.7433763146400452\n",
      "Epoch 4273: Training Loss: 0.2384482075770696 Validation Loss: 0.7434096932411194\n",
      "Epoch 4274: Training Loss: 0.2375239133834839 Validation Loss: 0.743159830570221\n",
      "Epoch 4275: Training Loss: 0.2375305195649465 Validation Loss: 0.74288409948349\n",
      "Epoch 4276: Training Loss: 0.2380195011695226 Validation Loss: 0.7428217530250549\n",
      "Epoch 4277: Training Loss: 0.23777766029040018 Validation Loss: 0.742419421672821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4278: Training Loss: 0.23743638892968497 Validation Loss: 0.7421556115150452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4279: Training Loss: 0.23729809621969858 Validation Loss: 0.7423615455627441\n",
      "Epoch 4280: Training Loss: 0.23754067718982697 Validation Loss: 0.7424774169921875\n",
      "Epoch 4281: Training Loss: 0.23729689419269562 Validation Loss: 0.7427569627761841\n",
      "Epoch 4282: Training Loss: 0.23734821379184723 Validation Loss: 0.7429278492927551\n",
      "Epoch 4283: Training Loss: 0.23839687804381052 Validation Loss: 0.7431589961051941\n",
      "Epoch 4284: Training Loss: 0.23725228508313498 Validation Loss: 0.7428147792816162\n",
      "Epoch 4285: Training Loss: 0.23716020584106445 Validation Loss: 0.7429741621017456\n",
      "Epoch 4286: Training Loss: 0.23702661196390787 Validation Loss: 0.7425015568733215\n",
      "Epoch 4287: Training Loss: 0.23727282881736755 Validation Loss: 0.742458701133728\n",
      "Epoch 4288: Training Loss: 0.23707222938537598 Validation Loss: 0.7429383397102356\n",
      "Epoch 4289: Training Loss: 0.23741916318734488 Validation Loss: 0.7429836988449097\n",
      "Epoch 4290: Training Loss: 0.23647880057493845 Validation Loss: 0.7424671649932861\n",
      "Epoch 4291: Training Loss: 0.2376850595076879 Validation Loss: 0.7422893047332764\n",
      "Epoch 4292: Training Loss: 0.23676459987958273 Validation Loss: 0.7424337267875671\n",
      "Epoch 4293: Training Loss: 0.23685358464717865 Validation Loss: 0.7423382997512817\n",
      "Epoch 4294: Training Loss: 0.23702873289585114 Validation Loss: 0.7428933382034302\n",
      "Epoch 4295: Training Loss: 0.23665887614091238 Validation Loss: 0.7431209683418274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4296: Training Loss: 0.2377327928940455 Validation Loss: 0.7427478432655334\n",
      "Epoch 4297: Training Loss: 0.23655948539574942 Validation Loss: 0.7426267862319946\n",
      "Epoch 4298: Training Loss: 0.23637483020623526 Validation Loss: 0.7425214052200317\n",
      "Epoch 4299: Training Loss: 0.23618942499160767 Validation Loss: 0.7422916889190674\n",
      "Epoch 4300: Training Loss: 0.2367397447427114 Validation Loss: 0.7422529458999634\n",
      "Epoch 4301: Training Loss: 0.23647727072238922 Validation Loss: 0.7420653700828552\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4302: Training Loss: 0.23716764648755392 Validation Loss: 0.7418386340141296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4303: Training Loss: 0.23622493942578635 Validation Loss: 0.7423268556594849\n",
      "Epoch 4304: Training Loss: 0.2381711502869924 Validation Loss: 0.7424553036689758\n",
      "Epoch 4305: Training Loss: 0.23601862291495004 Validation Loss: 0.7426669001579285\n",
      "Epoch 4306: Training Loss: 0.23655191560586294 Validation Loss: 0.742327868938446\n",
      "Epoch 4307: Training Loss: 0.23541214565436044 Validation Loss: 0.7420022487640381\n",
      "Epoch 4308: Training Loss: 0.2356780618429184 Validation Loss: 0.741815984249115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4309: Training Loss: 0.2361077517271042 Validation Loss: 0.7422084808349609\n",
      "Epoch 4310: Training Loss: 0.23527325689792633 Validation Loss: 0.7423598170280457\n",
      "Epoch 4311: Training Loss: 0.23659725487232208 Validation Loss: 0.742351770401001\n",
      "Epoch 4312: Training Loss: 0.2352024664481481 Validation Loss: 0.7425504922866821\n",
      "Epoch 4313: Training Loss: 0.2356570909420649 Validation Loss: 0.7425757050514221\n",
      "Epoch 4314: Training Loss: 0.23560847342014313 Validation Loss: 0.7423061728477478\n",
      "Epoch 4315: Training Loss: 0.2355732023715973 Validation Loss: 0.7420304417610168\n",
      "Epoch 4316: Training Loss: 0.23547550539175668 Validation Loss: 0.742007851600647\n",
      "Epoch 4317: Training Loss: 0.2353744606177012 Validation Loss: 0.742068886756897\n",
      "Epoch 4318: Training Loss: 0.2352126936117808 Validation Loss: 0.7420220375061035\n",
      "Epoch 4319: Training Loss: 0.2356164405743281 Validation Loss: 0.7417111992835999\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4320: Training Loss: 0.23511299987634024 Validation Loss: 0.7416664958000183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4321: Training Loss: 0.23495442668596903 Validation Loss: 0.7419402003288269\n",
      "Epoch 4322: Training Loss: 0.236258864402771 Validation Loss: 0.7418299913406372\n",
      "Epoch 4323: Training Loss: 0.23491188883781433 Validation Loss: 0.7419266700744629\n",
      "Epoch 4324: Training Loss: 0.23550923665364584 Validation Loss: 0.7417582869529724\n",
      "Epoch 4325: Training Loss: 0.2351254572470983 Validation Loss: 0.7416375279426575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4326: Training Loss: 0.2346043586730957 Validation Loss: 0.7416504621505737\n",
      "Epoch 4327: Training Loss: 0.23470390339692435 Validation Loss: 0.7419030070304871\n",
      "Epoch 4328: Training Loss: 0.2346197416385015 Validation Loss: 0.742000937461853\n",
      "Epoch 4329: Training Loss: 0.23526199162006378 Validation Loss: 0.7422510385513306\n",
      "Epoch 4330: Training Loss: 0.23463169236977896 Validation Loss: 0.7424996495246887\n",
      "Epoch 4331: Training Loss: 0.23441367348035178 Validation Loss: 0.742185652256012\n",
      "Epoch 4332: Training Loss: 0.23435373604297638 Validation Loss: 0.7420143485069275\n",
      "Epoch 4333: Training Loss: 0.23454775412877402 Validation Loss: 0.7415979504585266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4334: Training Loss: 0.23418472707271576 Validation Loss: 0.7413862943649292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4335: Training Loss: 0.2360635052124659 Validation Loss: 0.7416131496429443\n",
      "Epoch 4336: Training Loss: 0.2338638405005137 Validation Loss: 0.7411808371543884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4337: Training Loss: 0.234671821196874 Validation Loss: 0.7416737675666809\n",
      "Epoch 4338: Training Loss: 0.2348840981721878 Validation Loss: 0.7417225241661072\n",
      "Epoch 4339: Training Loss: 0.2339858611424764 Validation Loss: 0.741273045539856\n",
      "Epoch 4340: Training Loss: 0.2337335248788198 Validation Loss: 0.7413767576217651\n",
      "Epoch 4341: Training Loss: 0.2347003718217214 Validation Loss: 0.7412057518959045\n",
      "Epoch 4342: Training Loss: 0.23419230182965597 Validation Loss: 0.74148029088974\n",
      "Epoch 4343: Training Loss: 0.23357359071572623 Validation Loss: 0.741413950920105\n",
      "Epoch 4344: Training Loss: 0.23372172812620798 Validation Loss: 0.7410294413566589\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4345: Training Loss: 0.23389037946859995 Validation Loss: 0.7413908839225769\n",
      "Epoch 4346: Training Loss: 0.23438823719819388 Validation Loss: 0.741615355014801\n",
      "Epoch 4347: Training Loss: 0.23419340451558432 Validation Loss: 0.7416486740112305\n",
      "Epoch 4348: Training Loss: 0.23377159734567007 Validation Loss: 0.7419692277908325\n",
      "Epoch 4349: Training Loss: 0.2334072639544805 Validation Loss: 0.7410632967948914\n",
      "Epoch 4350: Training Loss: 0.2331062157948812 Validation Loss: 0.7415229678153992\n",
      "Epoch 4351: Training Loss: 0.23342825969060263 Validation Loss: 0.7409718632698059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4352: Training Loss: 0.23394919435183206 Validation Loss: 0.7408972978591919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4353: Training Loss: 0.23336319625377655 Validation Loss: 0.7413368225097656\n",
      "Epoch 4354: Training Loss: 0.2338874638080597 Validation Loss: 0.7409666776657104\n",
      "Epoch 4355: Training Loss: 0.2332740475734075 Validation Loss: 0.7409911155700684\n",
      "Epoch 4356: Training Loss: 0.23346595962842306 Validation Loss: 0.7410253882408142\n",
      "Epoch 4357: Training Loss: 0.23294328649838766 Validation Loss: 0.7417058944702148\n",
      "Epoch 4358: Training Loss: 0.23286742468674979 Validation Loss: 0.7416480779647827\n",
      "Epoch 4359: Training Loss: 0.2327809731165568 Validation Loss: 0.7416083812713623\n",
      "Epoch 4360: Training Loss: 0.2326178401708603 Validation Loss: 0.7416759133338928\n",
      "Epoch 4361: Training Loss: 0.2336055487394333 Validation Loss: 0.7412939071655273\n",
      "Epoch 4362: Training Loss: 0.23293389876683554 Validation Loss: 0.7411724925041199\n",
      "Epoch 4363: Training Loss: 0.23267503082752228 Validation Loss: 0.7411748170852661\n",
      "Epoch 4364: Training Loss: 0.23270419736703238 Validation Loss: 0.7411408424377441\n",
      "Epoch 4365: Training Loss: 0.2331078698237737 Validation Loss: 0.7408561706542969\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4366: Training Loss: 0.23252361019452414 Validation Loss: 0.7410044074058533\n",
      "Epoch 4367: Training Loss: 0.23352170487244925 Validation Loss: 0.7406076788902283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4368: Training Loss: 0.2329122026761373 Validation Loss: 0.7413369417190552\n",
      "Epoch 4369: Training Loss: 0.23245400687058768 Validation Loss: 0.7417268753051758\n",
      "Epoch 4370: Training Loss: 0.2327912151813507 Validation Loss: 0.7417198419570923\n",
      "Epoch 4371: Training Loss: 0.23217564324537912 Validation Loss: 0.7412777543067932\n",
      "Epoch 4372: Training Loss: 0.2318821301062902 Validation Loss: 0.7410468459129333\n",
      "Epoch 4373: Training Loss: 0.23212056855360666 Validation Loss: 0.7408316135406494\n",
      "Epoch 4374: Training Loss: 0.23201918105284372 Validation Loss: 0.7403737902641296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4375: Training Loss: 0.23197788000106812 Validation Loss: 0.7405605912208557\n",
      "Epoch 4376: Training Loss: 0.2321928640206655 Validation Loss: 0.7411479949951172\n",
      "Epoch 4377: Training Loss: 0.23410764336585999 Validation Loss: 0.7413877844810486\n",
      "Epoch 4378: Training Loss: 0.23229903976122537 Validation Loss: 0.7411446571350098\n",
      "Epoch 4379: Training Loss: 0.23192247251669565 Validation Loss: 0.7413926720619202\n",
      "Epoch 4380: Training Loss: 0.23188521961371103 Validation Loss: 0.741485059261322\n",
      "Epoch 4381: Training Loss: 0.23153156538804373 Validation Loss: 0.7408303022384644\n",
      "Epoch 4382: Training Loss: 0.2315556208292643 Validation Loss: 0.7405232787132263\n",
      "Epoch 4383: Training Loss: 0.2316698133945465 Validation Loss: 0.7406245470046997\n",
      "Epoch 4384: Training Loss: 0.23164304594198862 Validation Loss: 0.7400940656661987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4385: Training Loss: 0.2319870889186859 Validation Loss: 0.7405598759651184\n",
      "Epoch 4386: Training Loss: 0.2315176179011663 Validation Loss: 0.7409912943840027\n",
      "Epoch 4387: Training Loss: 0.2300338645776113 Validation Loss: 0.7411038875579834\n",
      "Epoch 4388: Training Loss: 0.23270129164059958 Validation Loss: 0.7412317991256714\n",
      "Epoch 4389: Training Loss: 0.2313745121161143 Validation Loss: 0.740870475769043\n",
      "Epoch 4390: Training Loss: 0.23181844254334769 Validation Loss: 0.7403891682624817\n",
      "Epoch 4391: Training Loss: 0.2315935641527176 Validation Loss: 0.740143358707428\n",
      "Epoch 4392: Training Loss: 0.23135638733704886 Validation Loss: 0.7402840256690979\n",
      "Epoch 4393: Training Loss: 0.23190604150295258 Validation Loss: 0.7404565811157227\n",
      "Epoch 4394: Training Loss: 0.2318706512451172 Validation Loss: 0.7404991984367371\n",
      "Epoch 4395: Training Loss: 0.23109526435534158 Validation Loss: 0.7410933971405029\n",
      "Epoch 4396: Training Loss: 0.23122033973534903 Validation Loss: 0.7411788105964661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4397: Training Loss: 0.23141704499721527 Validation Loss: 0.7407731413841248\n",
      "Epoch 4398: Training Loss: 0.23108875254789987 Validation Loss: 0.7406383156776428\n",
      "Epoch 4399: Training Loss: 0.2310942163070043 Validation Loss: 0.7399763464927673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4400: Training Loss: 0.23077125350634256 Validation Loss: 0.7397412061691284\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4401: Training Loss: 0.23058498402436575 Validation Loss: 0.7399448156356812\n",
      "Epoch 4402: Training Loss: 0.23082423210144043 Validation Loss: 0.7404147982597351\n",
      "Epoch 4403: Training Loss: 0.23094066480795541 Validation Loss: 0.7410733103752136\n",
      "Epoch 4404: Training Loss: 0.23079739014307657 Validation Loss: 0.7409277558326721\n",
      "Epoch 4405: Training Loss: 0.23058480521043143 Validation Loss: 0.7406371831893921\n",
      "Epoch 4406: Training Loss: 0.23059149583180746 Validation Loss: 0.7406269311904907\n",
      "Epoch 4407: Training Loss: 0.22998015582561493 Validation Loss: 0.7406958341598511\n",
      "Epoch 4408: Training Loss: 0.2299852321545283 Validation Loss: 0.740734338760376\n",
      "Epoch 4409: Training Loss: 0.23040682077407837 Validation Loss: 0.7400882244110107\n",
      "Epoch 4410: Training Loss: 0.23016721506913504 Validation Loss: 0.7398537993431091\n",
      "Epoch 4411: Training Loss: 0.22977685928344727 Validation Loss: 0.7397738099098206\n",
      "Epoch 4412: Training Loss: 0.23111548026402792 Validation Loss: 0.7398459911346436\n",
      "Epoch 4413: Training Loss: 0.2295155475536982 Validation Loss: 0.7404186725616455\n",
      "Epoch 4414: Training Loss: 0.2298665096362432 Validation Loss: 0.7408145070075989\n",
      "Epoch 4415: Training Loss: 0.23051588237285614 Validation Loss: 0.7406781911849976\n",
      "Epoch 4416: Training Loss: 0.2292588104804357 Validation Loss: 0.7401806116104126\n",
      "Epoch 4417: Training Loss: 0.23007117211818695 Validation Loss: 0.7396854758262634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4418: Training Loss: 0.22994848589102426 Validation Loss: 0.7394039034843445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4419: Training Loss: 0.22976461052894592 Validation Loss: 0.7395530939102173\n",
      "Epoch 4420: Training Loss: 0.2295138289531072 Validation Loss: 0.7400251030921936\n",
      "Epoch 4421: Training Loss: 0.2295949955781301 Validation Loss: 0.7401866316795349\n",
      "Epoch 4422: Training Loss: 0.2293501595656077 Validation Loss: 0.7401378750801086\n",
      "Epoch 4423: Training Loss: 0.22955499092737833 Validation Loss: 0.7403151392936707\n",
      "Epoch 4424: Training Loss: 0.22957650323708853 Validation Loss: 0.7399505376815796\n",
      "Epoch 4425: Training Loss: 0.2296002060174942 Validation Loss: 0.7403756976127625\n",
      "Epoch 4426: Training Loss: 0.22947779794534048 Validation Loss: 0.7405651807785034\n",
      "Epoch 4427: Training Loss: 0.2298598736524582 Validation Loss: 0.740623950958252\n",
      "Epoch 4428: Training Loss: 0.22918794055779776 Validation Loss: 0.7404455542564392\n",
      "Epoch 4429: Training Loss: 0.2289506991704305 Validation Loss: 0.7400319576263428\n",
      "Epoch 4430: Training Loss: 0.22890004018942514 Validation Loss: 0.7400899529457092\n",
      "Epoch 4431: Training Loss: 0.22881434857845306 Validation Loss: 0.7401775121688843\n",
      "Epoch 4432: Training Loss: 0.22903513411680856 Validation Loss: 0.7400869131088257\n",
      "Epoch 4433: Training Loss: 0.23012369374434152 Validation Loss: 0.739969789981842\n",
      "Epoch 4434: Training Loss: 0.22907552619775137 Validation Loss: 0.7402524948120117\n",
      "Epoch 4435: Training Loss: 0.22864066064357758 Validation Loss: 0.7396231293678284\n",
      "Epoch 4436: Training Loss: 0.22878526151180267 Validation Loss: 0.7394542694091797\n",
      "Epoch 4437: Training Loss: 0.2285300592581431 Validation Loss: 0.7394226789474487\n",
      "Epoch 4438: Training Loss: 0.22869656483332315 Validation Loss: 0.739639163017273\n",
      "Epoch 4439: Training Loss: 0.23175438741842905 Validation Loss: 0.7404727339744568\n",
      "Epoch 4440: Training Loss: 0.2283529539903005 Validation Loss: 0.7407780289649963\n",
      "Epoch 4441: Training Loss: 0.2282817562421163 Validation Loss: 0.7400456070899963\n",
      "Epoch 4442: Training Loss: 0.22948291897773743 Validation Loss: 0.7396813631057739\n",
      "Epoch 4443: Training Loss: 0.22825205822785696 Validation Loss: 0.7394509315490723\n",
      "Epoch 4444: Training Loss: 0.2282916009426117 Validation Loss: 0.7395917177200317\n",
      "Epoch 4445: Training Loss: 0.22781676054000854 Validation Loss: 0.7397065162658691\n",
      "Epoch 4446: Training Loss: 0.22775773704051971 Validation Loss: 0.7395703792572021\n",
      "Epoch 4447: Training Loss: 0.22887146969636282 Validation Loss: 0.7401400804519653\n",
      "Epoch 4448: Training Loss: 0.2274290919303894 Validation Loss: 0.7399951219558716\n",
      "Epoch 4449: Training Loss: 0.22870572408040366 Validation Loss: 0.7398923635482788\n",
      "Epoch 4450: Training Loss: 0.22814484934012094 Validation Loss: 0.73964923620224\n",
      "Epoch 4451: Training Loss: 0.22935590147972107 Validation Loss: 0.739326536655426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4452: Training Loss: 0.2277547170718511 Validation Loss: 0.739505410194397\n",
      "Epoch 4453: Training Loss: 0.2280502716700236 Validation Loss: 0.739538311958313\n",
      "Epoch 4454: Training Loss: 0.2277744859457016 Validation Loss: 0.7395182251930237\n",
      "Epoch 4455: Training Loss: 0.22784083088239035 Validation Loss: 0.7398421764373779\n",
      "Epoch 4456: Training Loss: 0.22887117664019266 Validation Loss: 0.7397628426551819\n",
      "Epoch 4457: Training Loss: 0.22764137387275696 Validation Loss: 0.7392967939376831\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4458: Training Loss: 0.22714460889498392 Validation Loss: 0.7393399477005005\n",
      "Epoch 4459: Training Loss: 0.22782749931017557 Validation Loss: 0.7392943501472473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4460: Training Loss: 0.22729949156443277 Validation Loss: 0.739453911781311\n",
      "Epoch 4461: Training Loss: 0.22711540758609772 Validation Loss: 0.7402554750442505\n",
      "Epoch 4462: Training Loss: 0.22671428322792053 Validation Loss: 0.7399040460586548\n",
      "Epoch 4463: Training Loss: 0.2284237196048101 Validation Loss: 0.7403953075408936\n",
      "Epoch 4464: Training Loss: 0.22695394853750864 Validation Loss: 0.739730954170227\n",
      "Epoch 4465: Training Loss: 0.22737634678681692 Validation Loss: 0.739759624004364\n",
      "Epoch 4466: Training Loss: 0.22774715721607208 Validation Loss: 0.7393244504928589\n",
      "Epoch 4467: Training Loss: 0.22698302070299783 Validation Loss: 0.7388368844985962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4468: Training Loss: 0.22720004121462503 Validation Loss: 0.7389550805091858\n",
      "Epoch 4469: Training Loss: 0.22771054009596506 Validation Loss: 0.7391826510429382\n",
      "Epoch 4470: Training Loss: 0.22663181523482004 Validation Loss: 0.7392223477363586\n",
      "Epoch 4471: Training Loss: 0.2267492711544037 Validation Loss: 0.7391747236251831\n",
      "Epoch 4472: Training Loss: 0.2264675498008728 Validation Loss: 0.7392557859420776\n",
      "Epoch 4473: Training Loss: 0.22700515886147818 Validation Loss: 0.7388035655021667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4474: Training Loss: 0.22695613404115042 Validation Loss: 0.7385920286178589\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4475: Training Loss: 0.22661559780438742 Validation Loss: 0.739447295665741\n",
      "Epoch 4476: Training Loss: 0.2269352227449417 Validation Loss: 0.7396302819252014\n",
      "Epoch 4477: Training Loss: 0.22705053786436716 Validation Loss: 0.7392003536224365\n",
      "Epoch 4478: Training Loss: 0.22680910925070444 Validation Loss: 0.7392987012863159\n",
      "Epoch 4479: Training Loss: 0.22642216583093008 Validation Loss: 0.7396953701972961\n",
      "Epoch 4480: Training Loss: 0.22677766780058542 Validation Loss: 0.7396223545074463\n",
      "Epoch 4481: Training Loss: 0.2265353004137675 Validation Loss: 0.7394983768463135\n",
      "Epoch 4482: Training Loss: 0.2258798430363337 Validation Loss: 0.7387847900390625\n",
      "Epoch 4483: Training Loss: 0.22732373078664145 Validation Loss: 0.7387106418609619\n",
      "Epoch 4484: Training Loss: 0.22701452672481537 Validation Loss: 0.7386960387229919\n",
      "Epoch 4485: Training Loss: 0.22602291901906332 Validation Loss: 0.7389991283416748\n",
      "Epoch 4486: Training Loss: 0.22636437912782034 Validation Loss: 0.7388850450515747\n",
      "Epoch 4487: Training Loss: 0.22687961657842 Validation Loss: 0.7384312152862549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4488: Training Loss: 0.22604920963446298 Validation Loss: 0.7384929656982422\n",
      "Epoch 4489: Training Loss: 0.22591958940029144 Validation Loss: 0.7386454343795776\n",
      "Epoch 4490: Training Loss: 0.22560800115267435 Validation Loss: 0.738816499710083\n",
      "Epoch 4491: Training Loss: 0.22629532714684805 Validation Loss: 0.7390267848968506\n",
      "Epoch 4492: Training Loss: 0.2267071008682251 Validation Loss: 0.7391538619995117\n",
      "Epoch 4493: Training Loss: 0.22546308239301047 Validation Loss: 0.7389642596244812\n",
      "Epoch 4494: Training Loss: 0.22613648573557535 Validation Loss: 0.7391893267631531\n",
      "Epoch 4495: Training Loss: 0.2255885104338328 Validation Loss: 0.7392764687538147\n",
      "Epoch 4496: Training Loss: 0.22551455100377402 Validation Loss: 0.7385809421539307\n",
      "Epoch 4497: Training Loss: 0.2260593722263972 Validation Loss: 0.7384473085403442\n",
      "Epoch 4498: Training Loss: 0.22572669883569083 Validation Loss: 0.7387577295303345\n",
      "Epoch 4499: Training Loss: 0.2256152182817459 Validation Loss: 0.7390628457069397\n",
      "Epoch 4500: Training Loss: 0.22549685339132944 Validation Loss: 0.7392745614051819\n",
      "Epoch 4501: Training Loss: 0.22514407833417258 Validation Loss: 0.7391378879547119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4502: Training Loss: 0.2248938332001368 Validation Loss: 0.7393978238105774\n",
      "Epoch 4503: Training Loss: 0.2252935767173767 Validation Loss: 0.7394154071807861\n",
      "Epoch 4504: Training Loss: 0.2250081350406011 Validation Loss: 0.739091157913208\n",
      "Epoch 4505: Training Loss: 0.22486871480941772 Validation Loss: 0.7389714121818542\n",
      "Epoch 4506: Training Loss: 0.22591355443000793 Validation Loss: 0.7386181354522705\n",
      "Epoch 4507: Training Loss: 0.22505109012126923 Validation Loss: 0.7382696866989136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4508: Training Loss: 0.22507553299268088 Validation Loss: 0.7381591796875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4509: Training Loss: 0.22479389111200967 Validation Loss: 0.7382583618164062\n",
      "Epoch 4510: Training Loss: 0.22580586870511374 Validation Loss: 0.7388842701911926\n",
      "Epoch 4511: Training Loss: 0.22475751241048178 Validation Loss: 0.7385051250457764\n",
      "Epoch 4512: Training Loss: 0.22500910858313242 Validation Loss: 0.7383454442024231\n",
      "Epoch 4513: Training Loss: 0.22475033005078635 Validation Loss: 0.7383226156234741\n",
      "Epoch 4514: Training Loss: 0.22447845339775085 Validation Loss: 0.7381147742271423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4515: Training Loss: 0.22460761666297913 Validation Loss: 0.7383646965026855\n",
      "Epoch 4516: Training Loss: 0.2240520417690277 Validation Loss: 0.7385138869285583\n",
      "Epoch 4517: Training Loss: 0.22458485265572867 Validation Loss: 0.7386493682861328\n",
      "Epoch 4518: Training Loss: 0.22497072319189707 Validation Loss: 0.7389187812805176\n",
      "Epoch 4519: Training Loss: 0.22485718627770743 Validation Loss: 0.7387504577636719\n",
      "Epoch 4520: Training Loss: 0.2241650770107905 Validation Loss: 0.7386265397071838\n",
      "Epoch 4521: Training Loss: 0.2240604559580485 Validation Loss: 0.7382935285568237\n",
      "Epoch 4522: Training Loss: 0.2241443544626236 Validation Loss: 0.7384083867073059\n",
      "Epoch 4523: Training Loss: 0.2240752081076304 Validation Loss: 0.7382476329803467\n",
      "Epoch 4524: Training Loss: 0.223929430047671 Validation Loss: 0.7381291389465332\n",
      "Epoch 4525: Training Loss: 0.22467481096585593 Validation Loss: 0.737908661365509\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4526: Training Loss: 0.22396748264630637 Validation Loss: 0.7378711104393005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4527: Training Loss: 0.22392097115516663 Validation Loss: 0.7384443879127502\n",
      "Epoch 4528: Training Loss: 0.2252776175737381 Validation Loss: 0.7382848858833313\n",
      "Epoch 4529: Training Loss: 0.22415178517500559 Validation Loss: 0.7386762499809265\n",
      "Epoch 4530: Training Loss: 0.2235549638668696 Validation Loss: 0.7385940551757812\n",
      "Epoch 4531: Training Loss: 0.22330275177955627 Validation Loss: 0.7385659217834473\n",
      "Epoch 4532: Training Loss: 0.22387992839018503 Validation Loss: 0.7381740212440491\n",
      "Epoch 4533: Training Loss: 0.22499245901902518 Validation Loss: 0.7386907339096069\n",
      "Epoch 4534: Training Loss: 0.22337240477403006 Validation Loss: 0.7388186454772949\n",
      "Epoch 4535: Training Loss: 0.22366485993067423 Validation Loss: 0.7389686703681946\n",
      "Epoch 4536: Training Loss: 0.22339726984500885 Validation Loss: 0.7385910749435425\n",
      "Epoch 4537: Training Loss: 0.22321866949399313 Validation Loss: 0.7377291917800903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4538: Training Loss: 0.22348902622858682 Validation Loss: 0.7375494241714478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4539: Training Loss: 0.22334652145703635 Validation Loss: 0.7372502684593201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4540: Training Loss: 0.22292364140351614 Validation Loss: 0.7372316718101501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4541: Training Loss: 0.2231190005938212 Validation Loss: 0.7374226450920105\n",
      "Epoch 4542: Training Loss: 0.22349741061528525 Validation Loss: 0.7377180457115173\n",
      "Epoch 4543: Training Loss: 0.22356155018011728 Validation Loss: 0.7379767298698425\n",
      "Epoch 4544: Training Loss: 0.2228825589021047 Validation Loss: 0.7388141751289368\n",
      "Epoch 4545: Training Loss: 0.2232883721590042 Validation Loss: 0.7388935089111328\n",
      "Epoch 4546: Training Loss: 0.22443447013696036 Validation Loss: 0.7384628653526306\n",
      "Epoch 4547: Training Loss: 0.22250447670618692 Validation Loss: 0.7380661368370056\n",
      "Epoch 4548: Training Loss: 0.22308591504891714 Validation Loss: 0.7378145456314087\n",
      "Epoch 4549: Training Loss: 0.22281146049499512 Validation Loss: 0.7384073138237\n",
      "Epoch 4550: Training Loss: 0.22262285153071085 Validation Loss: 0.7382725477218628\n",
      "Epoch 4551: Training Loss: 0.22272109488646188 Validation Loss: 0.7383289337158203\n",
      "Epoch 4552: Training Loss: 0.22254366676012674 Validation Loss: 0.7386107444763184\n",
      "Epoch 4553: Training Loss: 0.2226984699567159 Validation Loss: 0.738212525844574\n",
      "Epoch 4554: Training Loss: 0.22346860667069754 Validation Loss: 0.7380459308624268\n",
      "Epoch 4555: Training Loss: 0.22291970252990723 Validation Loss: 0.7381842732429504\n",
      "Epoch 4556: Training Loss: 0.2224358767271042 Validation Loss: 0.737932562828064\n",
      "Epoch 4557: Training Loss: 0.2216750830411911 Validation Loss: 0.7375703454017639\n",
      "Epoch 4558: Training Loss: 0.22215664883454642 Validation Loss: 0.7375572323799133\n",
      "Epoch 4559: Training Loss: 0.22261595726013184 Validation Loss: 0.7381032109260559\n",
      "Epoch 4560: Training Loss: 0.22218327224254608 Validation Loss: 0.7386402487754822\n",
      "Epoch 4561: Training Loss: 0.22218690315882364 Validation Loss: 0.7388827204704285\n",
      "Epoch 4562: Training Loss: 0.22203834851582846 Validation Loss: 0.7385878562927246\n",
      "Epoch 4563: Training Loss: 0.22206888099511465 Validation Loss: 0.7378327250480652\n",
      "Epoch 4564: Training Loss: 0.22179837028185526 Validation Loss: 0.7373642325401306\n",
      "Epoch 4565: Training Loss: 0.22350499530633292 Validation Loss: 0.7370243072509766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4566: Training Loss: 0.22171072165171304 Validation Loss: 0.7372644543647766\n",
      "Epoch 4567: Training Loss: 0.22220681111017862 Validation Loss: 0.7379634380340576\n",
      "Epoch 4568: Training Loss: 0.22153416275978088 Validation Loss: 0.7380447387695312\n",
      "Epoch 4569: Training Loss: 0.22198117276032767 Validation Loss: 0.738024890422821\n",
      "Epoch 4570: Training Loss: 0.2218172699213028 Validation Loss: 0.738139808177948\n",
      "Epoch 4571: Training Loss: 0.22108294566472372 Validation Loss: 0.7383522391319275\n",
      "Epoch 4572: Training Loss: 0.221454789241155 Validation Loss: 0.7381229400634766\n",
      "Epoch 4573: Training Loss: 0.22147105634212494 Validation Loss: 0.7379175424575806\n",
      "Epoch 4574: Training Loss: 0.22149507204691568 Validation Loss: 0.7373802065849304\n",
      "Epoch 4575: Training Loss: 0.22141287724177042 Validation Loss: 0.7372686862945557\n",
      "Epoch 4576: Training Loss: 0.2214758644501368 Validation Loss: 0.737622082233429\n",
      "Epoch 4577: Training Loss: 0.22147517402966818 Validation Loss: 0.7382200956344604\n",
      "Epoch 4578: Training Loss: 0.22180686394373575 Validation Loss: 0.73822420835495\n",
      "Epoch 4579: Training Loss: 0.22151905298233032 Validation Loss: 0.7378392815589905\n",
      "Epoch 4580: Training Loss: 0.22161236902077994 Validation Loss: 0.7377522587776184\n",
      "Epoch 4581: Training Loss: 0.22177899380524954 Validation Loss: 0.7376735806465149\n",
      "Epoch 4582: Training Loss: 0.22101005911827087 Validation Loss: 0.7377143502235413\n",
      "Epoch 4583: Training Loss: 0.22090848286946616 Validation Loss: 0.7375580072402954\n",
      "Epoch 4584: Training Loss: 0.2210687200228373 Validation Loss: 0.7376179695129395\n",
      "Epoch 4585: Training Loss: 0.22079851229985556 Validation Loss: 0.7377286553382874\n",
      "Epoch 4586: Training Loss: 0.22173215448856354 Validation Loss: 0.7375009059906006\n",
      "Epoch 4587: Training Loss: 0.22103089094161987 Validation Loss: 0.737156093120575\n",
      "Epoch 4588: Training Loss: 0.2207997739315033 Validation Loss: 0.737253725528717\n",
      "Epoch 4589: Training Loss: 0.2207242101430893 Validation Loss: 0.7371624112129211\n",
      "Epoch 4590: Training Loss: 0.2209406942129135 Validation Loss: 0.7377340793609619\n",
      "Epoch 4591: Training Loss: 0.2204878826936086 Validation Loss: 0.7374451160430908\n",
      "Epoch 4592: Training Loss: 0.2203485369682312 Validation Loss: 0.7374607920646667\n",
      "Epoch 4593: Training Loss: 0.22044004996617636 Validation Loss: 0.7376746535301208\n",
      "Epoch 4594: Training Loss: 0.22044793764750162 Validation Loss: 0.7379149794578552\n",
      "Epoch 4595: Training Loss: 0.22050419449806213 Validation Loss: 0.7377489805221558\n",
      "Epoch 4596: Training Loss: 0.22061973810195923 Validation Loss: 0.7377768158912659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4597: Training Loss: 0.22029368579387665 Validation Loss: 0.7378464937210083\n",
      "Epoch 4598: Training Loss: 0.22061756253242493 Validation Loss: 0.7379702925682068\n",
      "Epoch 4599: Training Loss: 0.2211864540974299 Validation Loss: 0.7376915812492371\n",
      "Epoch 4600: Training Loss: 0.22083204487959543 Validation Loss: 0.737369179725647\n",
      "Epoch 4601: Training Loss: 0.22136166195074716 Validation Loss: 0.7369554042816162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4602: Training Loss: 0.21987566351890564 Validation Loss: 0.7370243072509766\n",
      "Epoch 4603: Training Loss: 0.2201524923245112 Validation Loss: 0.736973226070404\n",
      "Epoch 4604: Training Loss: 0.22082164386908212 Validation Loss: 0.737385630607605\n",
      "Epoch 4605: Training Loss: 0.21985353032747904 Validation Loss: 0.7372666597366333\n",
      "Epoch 4606: Training Loss: 0.22033518056074777 Validation Loss: 0.7373706102371216\n",
      "Epoch 4607: Training Loss: 0.22007185717423758 Validation Loss: 0.7375038862228394\n",
      "Epoch 4608: Training Loss: 0.2219213346640269 Validation Loss: 0.7372427582740784\n",
      "Epoch 4609: Training Loss: 0.2198575735092163 Validation Loss: 0.7370089888572693\n",
      "Epoch 4610: Training Loss: 0.21976986030737558 Validation Loss: 0.7371192574501038\n",
      "Epoch 4611: Training Loss: 0.2197484721740087 Validation Loss: 0.7374361753463745\n",
      "Epoch 4612: Training Loss: 0.21977023780345917 Validation Loss: 0.7378979921340942\n",
      "Epoch 4613: Training Loss: 0.21929773688316345 Validation Loss: 0.7383080720901489\n",
      "Epoch 4614: Training Loss: 0.21959356466929117 Validation Loss: 0.7378357648849487\n",
      "Epoch 4615: Training Loss: 0.21955410639444986 Validation Loss: 0.7373638153076172\n",
      "Epoch 4616: Training Loss: 0.21896126866340637 Validation Loss: 0.7371094226837158\n",
      "Epoch 4617: Training Loss: 0.21924927333990732 Validation Loss: 0.7368924617767334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4618: Training Loss: 0.21931265791257223 Validation Loss: 0.7370972633361816\n",
      "Epoch 4619: Training Loss: 0.2193758189678192 Validation Loss: 0.7371490001678467\n",
      "Epoch 4620: Training Loss: 0.21915985147158304 Validation Loss: 0.7368636131286621\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4621: Training Loss: 0.21938730279604593 Validation Loss: 0.7372076511383057\n",
      "Epoch 4622: Training Loss: 0.21994295716285706 Validation Loss: 0.737306535243988\n",
      "Epoch 4623: Training Loss: 0.21940818428993225 Validation Loss: 0.7374340891838074\n",
      "Epoch 4624: Training Loss: 0.21828418970108032 Validation Loss: 0.737575352191925\n",
      "Epoch 4625: Training Loss: 0.21880528827508292 Validation Loss: 0.7370136380195618\n",
      "Epoch 4626: Training Loss: 0.21902336180210114 Validation Loss: 0.7368286848068237\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4627: Training Loss: 0.2184271514415741 Validation Loss: 0.7368866205215454\n",
      "Epoch 4628: Training Loss: 0.21910908818244934 Validation Loss: 0.7368985414505005\n",
      "Epoch 4629: Training Loss: 0.21896573404471079 Validation Loss: 0.7368869781494141\n",
      "Epoch 4630: Training Loss: 0.21898715694745383 Validation Loss: 0.7368295192718506\n",
      "Epoch 4631: Training Loss: 0.21871993442376456 Validation Loss: 0.7368229031562805\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4632: Training Loss: 0.218433345357577 Validation Loss: 0.7371479868888855\n",
      "Epoch 4633: Training Loss: 0.21862173080444336 Validation Loss: 0.7374036908149719\n",
      "Epoch 4634: Training Loss: 0.21896142264207205 Validation Loss: 0.73704993724823\n",
      "Epoch 4635: Training Loss: 0.21854056417942047 Validation Loss: 0.7369312644004822\n",
      "Epoch 4636: Training Loss: 0.218772754073143 Validation Loss: 0.7372100949287415\n",
      "Epoch 4637: Training Loss: 0.219112495581309 Validation Loss: 0.7372172474861145\n",
      "Epoch 4638: Training Loss: 0.21823114653428397 Validation Loss: 0.736814558506012\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4639: Training Loss: 0.21873980263868967 Validation Loss: 0.7368847727775574\n",
      "Epoch 4640: Training Loss: 0.21798725922902426 Validation Loss: 0.7367815375328064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4641: Training Loss: 0.21840710441271463 Validation Loss: 0.7367933988571167\n",
      "Epoch 4642: Training Loss: 0.2181174506743749 Validation Loss: 0.7366585731506348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4643: Training Loss: 0.2187998741865158 Validation Loss: 0.7365856766700745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4644: Training Loss: 0.21876260638237 Validation Loss: 0.7368594408035278\n",
      "Epoch 4645: Training Loss: 0.2180044949054718 Validation Loss: 0.7366287708282471\n",
      "Epoch 4646: Training Loss: 0.21813605725765228 Validation Loss: 0.736316978931427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4647: Training Loss: 0.21830222010612488 Validation Loss: 0.7365055084228516\n",
      "Epoch 4648: Training Loss: 0.2194891224304835 Validation Loss: 0.7366065979003906\n",
      "Epoch 4649: Training Loss: 0.21766971548398337 Validation Loss: 0.7370619177818298\n",
      "Epoch 4650: Training Loss: 0.217709352572759 Validation Loss: 0.7371911406517029\n",
      "Epoch 4651: Training Loss: 0.217721755305926 Validation Loss: 0.736882746219635\n",
      "Epoch 4652: Training Loss: 0.21754472454388937 Validation Loss: 0.7366741299629211\n",
      "Epoch 4653: Training Loss: 0.21778018275896707 Validation Loss: 0.7363629937171936\n",
      "Epoch 4654: Training Loss: 0.21655331552028656 Validation Loss: 0.7364642024040222\n",
      "Epoch 4655: Training Loss: 0.21822838485240936 Validation Loss: 0.7368583083152771\n",
      "Epoch 4656: Training Loss: 0.21753371258576712 Validation Loss: 0.7368963360786438\n",
      "Epoch 4657: Training Loss: 0.21824252605438232 Validation Loss: 0.737298309803009\n",
      "Epoch 4658: Training Loss: 0.21732822060585022 Validation Loss: 0.737199068069458\n",
      "Epoch 4659: Training Loss: 0.21740621825059256 Validation Loss: 0.7362643480300903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4660: Training Loss: 0.21791974206765494 Validation Loss: 0.7361549139022827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4661: Training Loss: 0.2171720266342163 Validation Loss: 0.7364891767501831\n",
      "Epoch 4662: Training Loss: 0.2170660744110743 Validation Loss: 0.7368957996368408\n",
      "Epoch 4663: Training Loss: 0.21700547138849893 Validation Loss: 0.7370507121086121\n",
      "Epoch 4664: Training Loss: 0.2173274556795756 Validation Loss: 0.736405074596405\n",
      "Epoch 4665: Training Loss: 0.21749269465605417 Validation Loss: 0.7361944913864136\n",
      "Epoch 4666: Training Loss: 0.21704023579756418 Validation Loss: 0.7364645004272461\n",
      "Epoch 4667: Training Loss: 0.21651064852873483 Validation Loss: 0.7365354895591736\n",
      "Epoch 4668: Training Loss: 0.21691808104515076 Validation Loss: 0.736737072467804\n",
      "Epoch 4669: Training Loss: 0.21683179835478464 Validation Loss: 0.7361022233963013\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4670: Training Loss: 0.21659858028093973 Validation Loss: 0.7363883852958679\n",
      "Epoch 4671: Training Loss: 0.21690950294335684 Validation Loss: 0.736156702041626\n",
      "Epoch 4672: Training Loss: 0.2167945603529612 Validation Loss: 0.7363990545272827\n",
      "Epoch 4673: Training Loss: 0.2168315052986145 Validation Loss: 0.7362616658210754\n",
      "Epoch 4674: Training Loss: 0.2163219302892685 Validation Loss: 0.7363244891166687\n",
      "Epoch 4675: Training Loss: 0.21624508500099182 Validation Loss: 0.7362874150276184\n",
      "Epoch 4676: Training Loss: 0.21651883920033774 Validation Loss: 0.7364545464515686\n",
      "Epoch 4677: Training Loss: 0.21660534540812174 Validation Loss: 0.7362360954284668\n",
      "Epoch 4678: Training Loss: 0.21629883348941803 Validation Loss: 0.7365562915802002\n",
      "Epoch 4679: Training Loss: 0.21825096507867178 Validation Loss: 0.7366055846214294\n",
      "Epoch 4680: Training Loss: 0.21628118554751077 Validation Loss: 0.7366629242897034\n",
      "Epoch 4681: Training Loss: 0.21647544701894125 Validation Loss: 0.7368952035903931\n",
      "Epoch 4682: Training Loss: 0.21631111204624176 Validation Loss: 0.7371008396148682\n",
      "Epoch 4683: Training Loss: 0.2158463050921758 Validation Loss: 0.7363279461860657\n",
      "Epoch 4684: Training Loss: 0.21612741549809775 Validation Loss: 0.7358472347259521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4685: Training Loss: 0.21604637801647186 Validation Loss: 0.7355532646179199\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4686: Training Loss: 0.21596633394559225 Validation Loss: 0.7358335852622986\n",
      "Epoch 4687: Training Loss: 0.21579576035340628 Validation Loss: 0.7357630729675293\n",
      "Epoch 4688: Training Loss: 0.21590429544448853 Validation Loss: 0.735844612121582\n",
      "Epoch 4689: Training Loss: 0.21617057919502258 Validation Loss: 0.7362201809883118\n",
      "Epoch 4690: Training Loss: 0.21822407841682434 Validation Loss: 0.7361512780189514\n",
      "Epoch 4691: Training Loss: 0.21537659565607706 Validation Loss: 0.7363266348838806\n",
      "Epoch 4692: Training Loss: 0.21639446914196014 Validation Loss: 0.7363967895507812\n",
      "Epoch 4693: Training Loss: 0.21564799547195435 Validation Loss: 0.7362731695175171\n",
      "Epoch 4694: Training Loss: 0.21563513080279031 Validation Loss: 0.7364705801010132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4695: Training Loss: 0.2158763607343038 Validation Loss: 0.7363176941871643\n",
      "Epoch 4696: Training Loss: 0.21551277240117392 Validation Loss: 0.7363312840461731\n",
      "Epoch 4697: Training Loss: 0.21582074463367462 Validation Loss: 0.7359007596969604\n",
      "Epoch 4698: Training Loss: 0.21602985262870789 Validation Loss: 0.7362046241760254\n",
      "Epoch 4699: Training Loss: 0.21540353695551553 Validation Loss: 0.7362396121025085\n",
      "Epoch 4700: Training Loss: 0.2155175507068634 Validation Loss: 0.736719012260437\n",
      "Epoch 4701: Training Loss: 0.21529188255469003 Validation Loss: 0.736657440662384\n",
      "Epoch 4702: Training Loss: 0.21524669229984283 Validation Loss: 0.7366794347763062\n",
      "Epoch 4703: Training Loss: 0.21527135372161865 Validation Loss: 0.7359282970428467\n",
      "Epoch 4704: Training Loss: 0.2151496410369873 Validation Loss: 0.7358755469322205\n",
      "Epoch 4705: Training Loss: 0.2150335262219111 Validation Loss: 0.7358623147010803\n",
      "Epoch 4706: Training Loss: 0.21517849961916605 Validation Loss: 0.7360225319862366\n",
      "Epoch 4707: Training Loss: 0.21510031322638193 Validation Loss: 0.7355437874794006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4708: Training Loss: 0.21497179567813873 Validation Loss: 0.7353319525718689\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4709: Training Loss: 0.21623536944389343 Validation Loss: 0.7352274060249329\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4710: Training Loss: 0.21486952404181162 Validation Loss: 0.7359655499458313\n",
      "Epoch 4711: Training Loss: 0.21501917640368143 Validation Loss: 0.7364970445632935\n",
      "Epoch 4712: Training Loss: 0.21462434033552805 Validation Loss: 0.7366518974304199\n",
      "Epoch 4713: Training Loss: 0.21496539811293283 Validation Loss: 0.736404299736023\n",
      "Epoch 4714: Training Loss: 0.21460036436716715 Validation Loss: 0.7360660433769226\n",
      "Epoch 4715: Training Loss: 0.21446856359640756 Validation Loss: 0.7361684441566467\n",
      "Epoch 4716: Training Loss: 0.21447737514972687 Validation Loss: 0.7362338900566101\n",
      "Epoch 4717: Training Loss: 0.21483192344506583 Validation Loss: 0.7367008924484253\n",
      "Epoch 4718: Training Loss: 0.21485682825247446 Validation Loss: 0.7363032102584839\n",
      "Epoch 4719: Training Loss: 0.21485035121440887 Validation Loss: 0.7354559302330017\n",
      "Epoch 4720: Training Loss: 0.21437125404675803 Validation Loss: 0.7355331778526306\n",
      "Epoch 4721: Training Loss: 0.215093195438385 Validation Loss: 0.735112726688385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4722: Training Loss: 0.21458049615224203 Validation Loss: 0.7354762554168701\n",
      "Epoch 4723: Training Loss: 0.21434851984182993 Validation Loss: 0.7358657121658325\n",
      "Epoch 4724: Training Loss: 0.21430127322673798 Validation Loss: 0.7356255054473877\n",
      "Epoch 4725: Training Loss: 0.21394623816013336 Validation Loss: 0.7358362078666687\n",
      "Epoch 4726: Training Loss: 0.21402163803577423 Validation Loss: 0.7357585430145264\n",
      "Epoch 4727: Training Loss: 0.21412948270638785 Validation Loss: 0.736242413520813\n",
      "Epoch 4728: Training Loss: 0.21535016099611917 Validation Loss: 0.7359482645988464\n",
      "Epoch 4729: Training Loss: 0.21439423660437265 Validation Loss: 0.7359752058982849\n",
      "Epoch 4730: Training Loss: 0.21395177642504373 Validation Loss: 0.7363440990447998\n",
      "Epoch 4731: Training Loss: 0.21372906863689423 Validation Loss: 0.7362048625946045\n",
      "Epoch 4732: Training Loss: 0.2139342725276947 Validation Loss: 0.7355302572250366\n",
      "Epoch 4733: Training Loss: 0.2141212671995163 Validation Loss: 0.7354846596717834\n",
      "Epoch 4734: Training Loss: 0.21392200887203217 Validation Loss: 0.735256552696228\n",
      "Epoch 4735: Training Loss: 0.21355610092480978 Validation Loss: 0.7353001832962036\n",
      "Epoch 4736: Training Loss: 0.2137696494658788 Validation Loss: 0.7356840372085571\n",
      "Epoch 4737: Training Loss: 0.21356805165608725 Validation Loss: 0.7356030344963074\n",
      "Epoch 4738: Training Loss: 0.21303967634836832 Validation Loss: 0.7355683445930481\n",
      "Epoch 4739: Training Loss: 0.21384749313195547 Validation Loss: 0.7357644438743591\n",
      "Epoch 4740: Training Loss: 0.21350752313931784 Validation Loss: 0.7354984283447266\n",
      "Epoch 4741: Training Loss: 0.21357722580432892 Validation Loss: 0.7354924082756042\n",
      "Epoch 4742: Training Loss: 0.21364967028299967 Validation Loss: 0.7358096241950989\n",
      "Epoch 4743: Training Loss: 0.21322700877984366 Validation Loss: 0.7360717058181763\n",
      "Epoch 4744: Training Loss: 0.21314584712187448 Validation Loss: 0.736067533493042\n",
      "Epoch 4745: Training Loss: 0.2131934513648351 Validation Loss: 0.7364274263381958\n",
      "Epoch 4746: Training Loss: 0.21316391229629517 Validation Loss: 0.7359692454338074\n",
      "Epoch 4747: Training Loss: 0.21314934392770132 Validation Loss: 0.7352038621902466\n",
      "Epoch 4748: Training Loss: 0.2132021685441335 Validation Loss: 0.7352452874183655\n",
      "Epoch 4749: Training Loss: 0.21326802670955658 Validation Loss: 0.7352175712585449\n",
      "Epoch 4750: Training Loss: 0.2124867339928945 Validation Loss: 0.7356102466583252\n",
      "Epoch 4751: Training Loss: 0.21276971201101938 Validation Loss: 0.7358356714248657\n",
      "Epoch 4752: Training Loss: 0.2130638857682546 Validation Loss: 0.7361251711845398\n",
      "Epoch 4753: Training Loss: 0.21314716339111328 Validation Loss: 0.7357929348945618\n",
      "Epoch 4754: Training Loss: 0.2130855123202006 Validation Loss: 0.735286295413971\n",
      "Epoch 4755: Training Loss: 0.21296203136444092 Validation Loss: 0.7355849146842957\n",
      "Epoch 4756: Training Loss: 0.21279324094454447 Validation Loss: 0.7358766198158264\n",
      "Epoch 4757: Training Loss: 0.2126025309165319 Validation Loss: 0.735267162322998\n",
      "Epoch 4758: Training Loss: 0.21262086927890778 Validation Loss: 0.7352734208106995\n",
      "Epoch 4759: Training Loss: 0.21242630978425345 Validation Loss: 0.7351988554000854\n",
      "Epoch 4760: Training Loss: 0.21261743704477945 Validation Loss: 0.7353510856628418\n",
      "Epoch 4761: Training Loss: 0.2137596756219864 Validation Loss: 0.7360274195671082\n",
      "Epoch 4762: Training Loss: 0.2124804606040319 Validation Loss: 0.7357117533683777\n",
      "Epoch 4763: Training Loss: 0.21269941329956055 Validation Loss: 0.7354060411453247\n",
      "Epoch 4764: Training Loss: 0.2124397854010264 Validation Loss: 0.7350816130638123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4765: Training Loss: 0.21225241820017496 Validation Loss: 0.735485315322876\n",
      "Epoch 4766: Training Loss: 0.2123974213997523 Validation Loss: 0.7355564832687378\n",
      "Epoch 4767: Training Loss: 0.2121082991361618 Validation Loss: 0.7354493737220764\n",
      "Epoch 4768: Training Loss: 0.2120958318312963 Validation Loss: 0.7355847954750061\n",
      "Epoch 4769: Training Loss: 0.2115561862786611 Validation Loss: 0.7358525395393372\n",
      "Epoch 4770: Training Loss: 0.2124138375123342 Validation Loss: 0.7356537580490112\n",
      "Epoch 4771: Training Loss: 0.21191753447055817 Validation Loss: 0.7356500625610352\n",
      "Epoch 4772: Training Loss: 0.2119558056195577 Validation Loss: 0.7358494400978088\n",
      "Epoch 4773: Training Loss: 0.2118230164051056 Validation Loss: 0.7357701063156128\n",
      "Epoch 4774: Training Loss: 0.21282230814297995 Validation Loss: 0.7355018258094788\n",
      "Epoch 4775: Training Loss: 0.21210603415966034 Validation Loss: 0.7354342937469482\n",
      "Epoch 4776: Training Loss: 0.21200662354628244 Validation Loss: 0.7354505062103271\n",
      "Epoch 4777: Training Loss: 0.21190627415974936 Validation Loss: 0.735564649105072\n",
      "Epoch 4778: Training Loss: 0.21243161956469217 Validation Loss: 0.7352609634399414\n",
      "Epoch 4779: Training Loss: 0.2121443102757136 Validation Loss: 0.7354718446731567\n",
      "Epoch 4780: Training Loss: 0.21180772284666696 Validation Loss: 0.7348966002464294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4781: Training Loss: 0.21152990063031515 Validation Loss: 0.7343800067901611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4782: Training Loss: 0.21148171524206796 Validation Loss: 0.7349908351898193\n",
      "Epoch 4783: Training Loss: 0.21160888175169626 Validation Loss: 0.735593318939209\n",
      "Epoch 4784: Training Loss: 0.21162975331147513 Validation Loss: 0.7354064583778381\n",
      "Epoch 4785: Training Loss: 0.21137615044911703 Validation Loss: 0.735664963722229\n",
      "Epoch 4786: Training Loss: 0.21215098599592844 Validation Loss: 0.7356062531471252\n",
      "Epoch 4787: Training Loss: 0.21123938262462616 Validation Loss: 0.7354522943496704\n",
      "Epoch 4788: Training Loss: 0.21139115591843924 Validation Loss: 0.734989583492279\n",
      "Epoch 4789: Training Loss: 0.21221286555131277 Validation Loss: 0.735099196434021\n",
      "Epoch 4790: Training Loss: 0.2112179398536682 Validation Loss: 0.7351589202880859\n",
      "Epoch 4791: Training Loss: 0.21114827195803323 Validation Loss: 0.7351894378662109\n",
      "Epoch 4792: Training Loss: 0.21102191507816315 Validation Loss: 0.7351182103157043\n",
      "Epoch 4793: Training Loss: 0.21212512254714966 Validation Loss: 0.7355548143386841\n",
      "Epoch 4794: Training Loss: 0.21086944142977396 Validation Loss: 0.7352146506309509\n",
      "Epoch 4795: Training Loss: 0.21136816839377084 Validation Loss: 0.7347793579101562\n",
      "Epoch 4796: Training Loss: 0.21059344708919525 Validation Loss: 0.7347633242607117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4797: Training Loss: 0.21122892200946808 Validation Loss: 0.734697699546814\n",
      "Epoch 4798: Training Loss: 0.21108239392439523 Validation Loss: 0.734894335269928\n",
      "Epoch 4799: Training Loss: 0.21094477673371634 Validation Loss: 0.7350041270256042\n",
      "Epoch 4800: Training Loss: 0.211732750137647 Validation Loss: 0.735485315322876\n",
      "Epoch 4801: Training Loss: 0.21049659450848898 Validation Loss: 0.7352820634841919\n",
      "Epoch 4802: Training Loss: 0.21002961695194244 Validation Loss: 0.7352254986763\n",
      "Epoch 4803: Training Loss: 0.21100691457589468 Validation Loss: 0.735081672668457\n",
      "Epoch 4804: Training Loss: 0.2110989640156428 Validation Loss: 0.735306978225708\n",
      "Epoch 4805: Training Loss: 0.2104362646738688 Validation Loss: 0.7356220483779907\n",
      "Epoch 4806: Training Loss: 0.21029611925284067 Validation Loss: 0.7359157800674438\n",
      "Epoch 4807: Training Loss: 0.21152914563814798 Validation Loss: 0.7353667616844177\n",
      "Epoch 4808: Training Loss: 0.21033647656440735 Validation Loss: 0.7347702980041504\n",
      "Epoch 4809: Training Loss: 0.21034923692544302 Validation Loss: 0.734072744846344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4810: Training Loss: 0.21010931332906088 Validation Loss: 0.7337692975997925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4811: Training Loss: 0.2104631612698237 Validation Loss: 0.7340217232704163\n",
      "Epoch 4812: Training Loss: 0.21024891237417856 Validation Loss: 0.7345601320266724\n",
      "Epoch 4813: Training Loss: 0.2100790043671926 Validation Loss: 0.7350007891654968\n",
      "Epoch 4814: Training Loss: 0.21041562159856161 Validation Loss: 0.7355538010597229\n",
      "Epoch 4815: Training Loss: 0.2102393259604772 Validation Loss: 0.7356249094009399\n",
      "Epoch 4816: Training Loss: 0.20980226496855417 Validation Loss: 0.7351509928703308\n",
      "Epoch 4817: Training Loss: 0.20984665552775064 Validation Loss: 0.7354363203048706\n",
      "Epoch 4818: Training Loss: 0.21001282334327698 Validation Loss: 0.7351595759391785\n",
      "Epoch 4819: Training Loss: 0.20981108148892721 Validation Loss: 0.7348206043243408\n",
      "Epoch 4820: Training Loss: 0.2099998046954473 Validation Loss: 0.7353379130363464\n",
      "Epoch 4821: Training Loss: 0.21013176441192627 Validation Loss: 0.7347862124443054\n",
      "Epoch 4822: Training Loss: 0.21038449803988138 Validation Loss: 0.7348847389221191\n",
      "Epoch 4823: Training Loss: 0.21158209443092346 Validation Loss: 0.734847903251648\n",
      "Epoch 4824: Training Loss: 0.2103249728679657 Validation Loss: 0.7350614070892334\n",
      "Epoch 4825: Training Loss: 0.2094485561052958 Validation Loss: 0.7347877621650696\n",
      "Epoch 4826: Training Loss: 0.21045183638731638 Validation Loss: 0.7346785068511963\n",
      "Epoch 4827: Training Loss: 0.2091664324204127 Validation Loss: 0.7347732782363892\n",
      "Epoch 4828: Training Loss: 0.20936942100524902 Validation Loss: 0.7347452044487\n",
      "Epoch 4829: Training Loss: 0.2093478093544642 Validation Loss: 0.734602689743042\n",
      "Epoch 4830: Training Loss: 0.2094671130180359 Validation Loss: 0.7351146936416626\n",
      "Epoch 4831: Training Loss: 0.20940680305163065 Validation Loss: 0.7351534962654114\n",
      "Epoch 4832: Training Loss: 0.2095513939857483 Validation Loss: 0.734397828578949\n",
      "Epoch 4833: Training Loss: 0.2100120186805725 Validation Loss: 0.73422771692276\n",
      "Epoch 4834: Training Loss: 0.20914319654305777 Validation Loss: 0.7347658276557922\n",
      "Epoch 4835: Training Loss: 0.2088335007429123 Validation Loss: 0.7347002625465393\n",
      "Epoch 4836: Training Loss: 0.20986691613992056 Validation Loss: 0.7347487211227417\n",
      "Epoch 4837: Training Loss: 0.20954237381617227 Validation Loss: 0.7346767783164978\n",
      "Epoch 4838: Training Loss: 0.20880430936813354 Validation Loss: 0.7346158027648926\n",
      "Epoch 4839: Training Loss: 0.20898933708667755 Validation Loss: 0.7346377968788147\n",
      "Epoch 4840: Training Loss: 0.2094549039999644 Validation Loss: 0.7351167798042297\n",
      "Epoch 4841: Training Loss: 0.2093987117211024 Validation Loss: 0.7350802421569824\n",
      "Epoch 4842: Training Loss: 0.2094320406516393 Validation Loss: 0.7348406314849854\n",
      "Epoch 4843: Training Loss: 0.20873222251733145 Validation Loss: 0.7346749305725098\n",
      "Epoch 4844: Training Loss: 0.2091181973616282 Validation Loss: 0.7349114418029785\n",
      "Epoch 4845: Training Loss: 0.2094071408112844 Validation Loss: 0.7347966432571411\n",
      "Epoch 4846: Training Loss: 0.20883017281691232 Validation Loss: 0.7352237105369568\n",
      "Epoch 4847: Training Loss: 0.20980726182460785 Validation Loss: 0.735417366027832\n",
      "Epoch 4848: Training Loss: 0.20889627933502197 Validation Loss: 0.7355601787567139\n",
      "Epoch 4849: Training Loss: 0.20871242384115854 Validation Loss: 0.734862208366394\n",
      "Epoch 4850: Training Loss: 0.20881483455499014 Validation Loss: 0.7347634434700012\n",
      "Epoch 4851: Training Loss: 0.20921994745731354 Validation Loss: 0.7345952987670898\n",
      "Epoch 4852: Training Loss: 0.2083237667878469 Validation Loss: 0.7344170212745667\n",
      "Epoch 4853: Training Loss: 0.2084470490614573 Validation Loss: 0.7342923879623413\n",
      "Epoch 4854: Training Loss: 0.2084301213423411 Validation Loss: 0.7342785596847534\n",
      "Epoch 4855: Training Loss: 0.20849197109540304 Validation Loss: 0.7344228625297546\n",
      "Epoch 4856: Training Loss: 0.20816963911056519 Validation Loss: 0.734991729259491\n",
      "Epoch 4857: Training Loss: 0.20829128722349802 Validation Loss: 0.735164225101471\n",
      "Epoch 4858: Training Loss: 0.20857075850168863 Validation Loss: 0.7351015210151672\n",
      "Epoch 4859: Training Loss: 0.20785960058371225 Validation Loss: 0.7348544597625732\n",
      "Epoch 4860: Training Loss: 0.2082925339539846 Validation Loss: 0.7347410321235657\n",
      "Epoch 4861: Training Loss: 0.2077773759762446 Validation Loss: 0.73514324426651\n",
      "Epoch 4862: Training Loss: 0.20816282431284586 Validation Loss: 0.7349421977996826\n",
      "Epoch 4863: Training Loss: 0.2081785500049591 Validation Loss: 0.7348137497901917\n",
      "Epoch 4864: Training Loss: 0.2079305797815323 Validation Loss: 0.73414546251297\n",
      "Epoch 4865: Training Loss: 0.2077887306610743 Validation Loss: 0.7343606948852539\n",
      "Epoch 4866: Training Loss: 0.20793328185876211 Validation Loss: 0.7346097826957703\n",
      "Epoch 4867: Training Loss: 0.2084409842888514 Validation Loss: 0.7344648838043213\n",
      "Epoch 4868: Training Loss: 0.20770265658696493 Validation Loss: 0.7344374656677246\n",
      "Epoch 4869: Training Loss: 0.20758662621180216 Validation Loss: 0.7342163920402527\n",
      "Epoch 4870: Training Loss: 0.20817783971627554 Validation Loss: 0.7343470454216003\n",
      "Epoch 4871: Training Loss: 0.20725553234418234 Validation Loss: 0.7343975305557251\n",
      "Epoch 4872: Training Loss: 0.20795785884062448 Validation Loss: 0.7345078587532043\n",
      "Epoch 4873: Training Loss: 0.20756523807843527 Validation Loss: 0.734617292881012\n",
      "Epoch 4874: Training Loss: 0.20758992433547974 Validation Loss: 0.7346060872077942\n",
      "Epoch 4875: Training Loss: 0.20757226645946503 Validation Loss: 0.734719455242157\n",
      "Epoch 4876: Training Loss: 0.20769432683785757 Validation Loss: 0.734429657459259\n",
      "Epoch 4877: Training Loss: 0.2085847407579422 Validation Loss: 0.7345213890075684\n",
      "Epoch 4878: Training Loss: 0.2071227729320526 Validation Loss: 0.7346732020378113\n",
      "Epoch 4879: Training Loss: 0.20761196811993918 Validation Loss: 0.7343932390213013\n",
      "Epoch 4880: Training Loss: 0.20737279951572418 Validation Loss: 0.7342224717140198\n",
      "Epoch 4881: Training Loss: 0.20725171267986298 Validation Loss: 0.7345638871192932\n",
      "Epoch 4882: Training Loss: 0.20773820082346597 Validation Loss: 0.7346882224082947\n",
      "Epoch 4883: Training Loss: 0.2068550338347753 Validation Loss: 0.734868586063385\n",
      "Epoch 4884: Training Loss: 0.20680877069632211 Validation Loss: 0.7349393367767334\n",
      "Epoch 4885: Training Loss: 0.207143172621727 Validation Loss: 0.7348691821098328\n",
      "Epoch 4886: Training Loss: 0.20681004722913107 Validation Loss: 0.7347331643104553\n",
      "Epoch 4887: Training Loss: 0.20722085734208426 Validation Loss: 0.7346343994140625\n",
      "Epoch 4888: Training Loss: 0.20734422405560812 Validation Loss: 0.734514057636261\n",
      "Epoch 4889: Training Loss: 0.20657449960708618 Validation Loss: 0.7344059944152832\n",
      "Epoch 4890: Training Loss: 0.20682344834009805 Validation Loss: 0.7345791459083557\n",
      "Epoch 4891: Training Loss: 0.20692060887813568 Validation Loss: 0.7347029447555542\n",
      "Epoch 4892: Training Loss: 0.20834986368815103 Validation Loss: 0.7347172498703003\n",
      "Epoch 4893: Training Loss: 0.20661155382792154 Validation Loss: 0.7345929741859436\n",
      "Epoch 4894: Training Loss: 0.2065055767695109 Validation Loss: 0.7343876361846924\n",
      "Epoch 4895: Training Loss: 0.20682306587696075 Validation Loss: 0.7346482872962952\n",
      "Epoch 4896: Training Loss: 0.20641910036404928 Validation Loss: 0.7346783876419067\n",
      "Epoch 4897: Training Loss: 0.20620009303092957 Validation Loss: 0.734647810459137\n",
      "Epoch 4898: Training Loss: 0.20652958750724792 Validation Loss: 0.7340324521064758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4899: Training Loss: 0.20723730822404227 Validation Loss: 0.7340200543403625\n",
      "Epoch 4900: Training Loss: 0.20626037816206613 Validation Loss: 0.7341005206108093\n",
      "Epoch 4901: Training Loss: 0.20614604651927948 Validation Loss: 0.7342731356620789\n",
      "Epoch 4902: Training Loss: 0.20600773890813193 Validation Loss: 0.7345629334449768\n",
      "Epoch 4903: Training Loss: 0.2068328559398651 Validation Loss: 0.734283983707428\n",
      "Epoch 4904: Training Loss: 0.20613808929920197 Validation Loss: 0.7343547344207764\n",
      "Epoch 4905: Training Loss: 0.20649747550487518 Validation Loss: 0.7349777817726135\n",
      "Epoch 4906: Training Loss: 0.2058367927869161 Validation Loss: 0.7347825765609741\n",
      "Epoch 4907: Training Loss: 0.206197460492452 Validation Loss: 0.7344875931739807\n",
      "Epoch 4908: Training Loss: 0.20612736543019614 Validation Loss: 0.7338634729385376\n",
      "Epoch 4909: Training Loss: 0.20597238341967264 Validation Loss: 0.7338137626647949\n",
      "Epoch 4910: Training Loss: 0.20583764215310416 Validation Loss: 0.7342959642410278\n",
      "Epoch 4911: Training Loss: 0.20534997681776682 Validation Loss: 0.7343347072601318\n",
      "Epoch 4912: Training Loss: 0.20605261127154031 Validation Loss: 0.7343934774398804\n",
      "Epoch 4913: Training Loss: 0.20601974427700043 Validation Loss: 0.7342185378074646\n",
      "Epoch 4914: Training Loss: 0.20613858103752136 Validation Loss: 0.7338229417800903\n",
      "Epoch 4915: Training Loss: 0.2054767608642578 Validation Loss: 0.7338606715202332\n",
      "Epoch 4916: Training Loss: 0.20649699370066324 Validation Loss: 0.7344068884849548\n",
      "Epoch 4917: Training Loss: 0.20563411712646484 Validation Loss: 0.7343526482582092\n",
      "Epoch 4918: Training Loss: 0.20570151507854462 Validation Loss: 0.7337633967399597\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4919: Training Loss: 0.20566551387310028 Validation Loss: 0.7338153123855591\n",
      "Epoch 4920: Training Loss: 0.20546344916025797 Validation Loss: 0.7337298393249512\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4921: Training Loss: 0.2055684526761373 Validation Loss: 0.7339814305305481\n",
      "Epoch 4922: Training Loss: 0.20726820826530457 Validation Loss: 0.7342044115066528\n",
      "Epoch 4923: Training Loss: 0.20572916666666666 Validation Loss: 0.7343676090240479\n",
      "Epoch 4924: Training Loss: 0.20498823126157126 Validation Loss: 0.7342263460159302\n",
      "Epoch 4925: Training Loss: 0.20570492247740427 Validation Loss: 0.7348018884658813\n",
      "Epoch 4926: Training Loss: 0.20507481694221497 Validation Loss: 0.7345635890960693\n",
      "Epoch 4927: Training Loss: 0.2052036722501119 Validation Loss: 0.7339603304862976\n",
      "Epoch 4928: Training Loss: 0.20560459792613983 Validation Loss: 0.7332334518432617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4929: Training Loss: 0.20506523052851358 Validation Loss: 0.7330312728881836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4930: Training Loss: 0.20575980842113495 Validation Loss: 0.733107328414917\n",
      "Epoch 4931: Training Loss: 0.20502493778864542 Validation Loss: 0.733605682849884\n",
      "Epoch 4932: Training Loss: 0.20498197277386984 Validation Loss: 0.7345097661018372\n",
      "Epoch 4933: Training Loss: 0.2047397345304489 Validation Loss: 0.7347821593284607\n",
      "Epoch 4934: Training Loss: 0.20538902779420218 Validation Loss: 0.7347291707992554\n",
      "Epoch 4935: Training Loss: 0.20486533641815186 Validation Loss: 0.734437882900238\n",
      "Epoch 4936: Training Loss: 0.2062756766875585 Validation Loss: 0.7340332269668579\n",
      "Epoch 4937: Training Loss: 0.20457518100738525 Validation Loss: 0.7335716485977173\n",
      "Epoch 4938: Training Loss: 0.2054425279299418 Validation Loss: 0.7336370944976807\n",
      "Epoch 4939: Training Loss: 0.20454184214274088 Validation Loss: 0.7339230179786682\n",
      "Epoch 4940: Training Loss: 0.20520603160063425 Validation Loss: 0.7340087294578552\n",
      "Epoch 4941: Training Loss: 0.20422249535719553 Validation Loss: 0.7338352203369141\n",
      "Epoch 4942: Training Loss: 0.2043126920859019 Validation Loss: 0.7336801886558533\n",
      "Epoch 4943: Training Loss: 0.2044478952884674 Validation Loss: 0.7343622446060181\n",
      "Epoch 4944: Training Loss: 0.2049734095732371 Validation Loss: 0.7349141836166382\n",
      "Epoch 4945: Training Loss: 0.20462964475154877 Validation Loss: 0.7346975803375244\n",
      "Epoch 4946: Training Loss: 0.20426255961259207 Validation Loss: 0.7345091104507446\n",
      "Epoch 4947: Training Loss: 0.2041905870040258 Validation Loss: 0.7340286374092102\n",
      "Epoch 4948: Training Loss: 0.2042312224706014 Validation Loss: 0.7341100573539734\n",
      "Epoch 4949: Training Loss: 0.2046210616827011 Validation Loss: 0.7338956594467163\n",
      "Epoch 4950: Training Loss: 0.2044345587491989 Validation Loss: 0.7335937023162842\n",
      "Epoch 4951: Training Loss: 0.20404298603534698 Validation Loss: 0.7337693572044373\n",
      "Epoch 4952: Training Loss: 0.20463796456654867 Validation Loss: 0.7335194945335388\n",
      "Epoch 4953: Training Loss: 0.20413618286450705 Validation Loss: 0.7339040637016296\n",
      "Epoch 4954: Training Loss: 0.20346305767695108 Validation Loss: 0.7338789105415344\n",
      "Epoch 4955: Training Loss: 0.2039542148510615 Validation Loss: 0.7340269684791565\n",
      "Epoch 4956: Training Loss: 0.20385444164276123 Validation Loss: 0.7344814538955688\n",
      "Epoch 4957: Training Loss: 0.20370526115099588 Validation Loss: 0.7340204119682312\n",
      "Epoch 4958: Training Loss: 0.20387451847394308 Validation Loss: 0.7340457439422607\n",
      "Epoch 4959: Training Loss: 0.2042246162891388 Validation Loss: 0.7343758940696716\n",
      "Epoch 4960: Training Loss: 0.20382447044054666 Validation Loss: 0.7342361211776733\n",
      "Epoch 4961: Training Loss: 0.20367335776487985 Validation Loss: 0.7342237830162048\n",
      "Epoch 4962: Training Loss: 0.20421904822190604 Validation Loss: 0.7340366840362549\n",
      "Epoch 4963: Training Loss: 0.20367278655370077 Validation Loss: 0.7337068319320679\n",
      "Epoch 4964: Training Loss: 0.2053432067235311 Validation Loss: 0.7339346408843994\n",
      "Epoch 4965: Training Loss: 0.20345454414685568 Validation Loss: 0.7333625555038452\n",
      "Epoch 4966: Training Loss: 0.20346382757027945 Validation Loss: 0.7330811023712158\n",
      "Epoch 4967: Training Loss: 0.2034765879313151 Validation Loss: 0.7331249117851257\n",
      "Epoch 4968: Training Loss: 0.20427575707435608 Validation Loss: 0.7334038019180298\n",
      "Epoch 4969: Training Loss: 0.20370961229006448 Validation Loss: 0.7341986298561096\n",
      "Epoch 4970: Training Loss: 0.20325691998004913 Validation Loss: 0.7342022061347961\n",
      "Epoch 4971: Training Loss: 0.20318961640199026 Validation Loss: 0.7345608472824097\n",
      "Epoch 4972: Training Loss: 0.20363978544871011 Validation Loss: 0.7347745895385742\n",
      "Epoch 4973: Training Loss: 0.20296578109264374 Validation Loss: 0.7344310879707336\n",
      "Epoch 4974: Training Loss: 0.20333737631638846 Validation Loss: 0.7341494560241699\n",
      "Epoch 4975: Training Loss: 0.20301765203475952 Validation Loss: 0.7339048981666565\n",
      "Epoch 4976: Training Loss: 0.20315085351467133 Validation Loss: 0.7333642244338989\n",
      "Epoch 4977: Training Loss: 0.20303290585676828 Validation Loss: 0.7328290939331055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4978: Training Loss: 0.20317921042442322 Validation Loss: 0.7331051826477051\n",
      "Epoch 4979: Training Loss: 0.20309003194173178 Validation Loss: 0.7338123917579651\n",
      "Epoch 4980: Training Loss: 0.20310719311237335 Validation Loss: 0.7340577840805054\n",
      "Epoch 4981: Training Loss: 0.20287281274795532 Validation Loss: 0.7338683605194092\n",
      "Epoch 4982: Training Loss: 0.20257042348384857 Validation Loss: 0.7340607643127441\n",
      "Epoch 4983: Training Loss: 0.20245245099067688 Validation Loss: 0.734177827835083\n",
      "Epoch 4984: Training Loss: 0.20270351072152457 Validation Loss: 0.7341530919075012\n",
      "Epoch 4985: Training Loss: 0.20254571239153543 Validation Loss: 0.7341264486312866\n",
      "Epoch 4986: Training Loss: 0.20277561247348785 Validation Loss: 0.7334954738616943\n",
      "Epoch 4987: Training Loss: 0.20375283559163412 Validation Loss: 0.733821451663971\n",
      "Epoch 4988: Training Loss: 0.20307089885075888 Validation Loss: 0.733834981918335\n",
      "Epoch 4989: Training Loss: 0.20256246129671732 Validation Loss: 0.733425498008728\n",
      "Epoch 4990: Training Loss: 0.20248404145240784 Validation Loss: 0.7336787581443787\n",
      "Epoch 4991: Training Loss: 0.20247124135494232 Validation Loss: 0.733714759349823\n",
      "Epoch 4992: Training Loss: 0.20189701517422995 Validation Loss: 0.7336403727531433\n",
      "Epoch 4993: Training Loss: 0.20262563725312552 Validation Loss: 0.73358553647995\n",
      "Epoch 4994: Training Loss: 0.2025639017422994 Validation Loss: 0.7336001992225647\n",
      "Epoch 4995: Training Loss: 0.20248091220855713 Validation Loss: 0.733720064163208\n",
      "Epoch 4996: Training Loss: 0.20234512289365134 Validation Loss: 0.733903169631958\n",
      "Epoch 4997: Training Loss: 0.20213679472605386 Validation Loss: 0.733678936958313\n",
      "Epoch 4998: Training Loss: 0.20296732087930044 Validation Loss: 0.7338127493858337\n",
      "Epoch 4999: Training Loss: 0.2023476560910543 Validation Loss: 0.7336550354957581\n",
      "Epoch 5000: Training Loss: 0.2022477239370346 Validation Loss: 0.7336658835411072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5001: Training Loss: 0.20217456420262656 Validation Loss: 0.7338185906410217\n",
      "Epoch 5002: Training Loss: 0.2020504673322042 Validation Loss: 0.7337599992752075\n",
      "Epoch 5003: Training Loss: 0.2021490534146627 Validation Loss: 0.7338591814041138\n",
      "Epoch 5004: Training Loss: 0.20236557722091675 Validation Loss: 0.7336084246635437\n",
      "Epoch 5005: Training Loss: 0.20206465820471445 Validation Loss: 0.7336984276771545\n",
      "Epoch 5006: Training Loss: 0.20175667107105255 Validation Loss: 0.7337814569473267\n",
      "Epoch 5007: Training Loss: 0.20170487463474274 Validation Loss: 0.733718991279602\n",
      "Epoch 5008: Training Loss: 0.20195668935775757 Validation Loss: 0.7335852384567261\n",
      "Epoch 5009: Training Loss: 0.20224816600481668 Validation Loss: 0.7333881258964539\n",
      "Epoch 5010: Training Loss: 0.2016325742006302 Validation Loss: 0.7331291437149048\n",
      "Epoch 5011: Training Loss: 0.20175381004810333 Validation Loss: 0.7333524823188782\n",
      "Epoch 5012: Training Loss: 0.20167608062426248 Validation Loss: 0.733370304107666\n",
      "Epoch 5013: Training Loss: 0.2016310691833496 Validation Loss: 0.7331573963165283\n",
      "Epoch 5014: Training Loss: 0.20124402145544687 Validation Loss: 0.7332624197006226\n",
      "Epoch 5015: Training Loss: 0.20204989115397134 Validation Loss: 0.7336888313293457\n",
      "Epoch 5016: Training Loss: 0.20119494199752808 Validation Loss: 0.7334590554237366\n",
      "Epoch 5017: Training Loss: 0.20137645800908408 Validation Loss: 0.7333356142044067\n",
      "Epoch 5018: Training Loss: 0.20147615174452463 Validation Loss: 0.7338299751281738\n",
      "Epoch 5019: Training Loss: 0.20190443098545074 Validation Loss: 0.7333095073699951\n",
      "Epoch 5020: Training Loss: 0.20135578513145447 Validation Loss: 0.7327904105186462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5021: Training Loss: 0.20204642415046692 Validation Loss: 0.7333984375\n",
      "Epoch 5022: Training Loss: 0.20122804244359335 Validation Loss: 0.733900249004364\n",
      "Epoch 5023: Training Loss: 0.2016139974196752 Validation Loss: 0.733664333820343\n",
      "Epoch 5024: Training Loss: 0.20123855272928873 Validation Loss: 0.7333370447158813\n",
      "Epoch 5025: Training Loss: 0.20108057061831155 Validation Loss: 0.7334550619125366\n",
      "Epoch 5026: Training Loss: 0.2009260505437851 Validation Loss: 0.7331414818763733\n",
      "Epoch 5027: Training Loss: 0.20162512362003326 Validation Loss: 0.7331221103668213\n",
      "Epoch 5028: Training Loss: 0.20165461798508963 Validation Loss: 0.7330864667892456\n",
      "Epoch 5029: Training Loss: 0.2018300692240397 Validation Loss: 0.7336251139640808\n",
      "Epoch 5030: Training Loss: 0.20100594063599905 Validation Loss: 0.7337497472763062\n",
      "Epoch 5031: Training Loss: 0.20085002481937408 Validation Loss: 0.733220100402832\n",
      "Epoch 5032: Training Loss: 0.20094401141007742 Validation Loss: 0.7334124445915222\n",
      "Epoch 5033: Training Loss: 0.20095925529797873 Validation Loss: 0.7337090969085693\n",
      "Epoch 5034: Training Loss: 0.2008093148469925 Validation Loss: 0.7334076762199402\n",
      "Epoch 5035: Training Loss: 0.20062079032262167 Validation Loss: 0.7333798408508301\n",
      "Epoch 5036: Training Loss: 0.20167642831802368 Validation Loss: 0.733574390411377\n",
      "Epoch 5037: Training Loss: 0.20111977557341257 Validation Loss: 0.7341783046722412\n",
      "Epoch 5038: Training Loss: 0.20090300341447195 Validation Loss: 0.7341716289520264\n",
      "Epoch 5039: Training Loss: 0.20100288589795431 Validation Loss: 0.7341956496238708\n",
      "Epoch 5040: Training Loss: 0.2004388670126597 Validation Loss: 0.7334896922111511\n",
      "Epoch 5041: Training Loss: 0.20072509348392487 Validation Loss: 0.732605516910553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5042: Training Loss: 0.20054283241430917 Validation Loss: 0.7326321601867676\n",
      "Epoch 5043: Training Loss: 0.2007780820131302 Validation Loss: 0.7328429818153381\n",
      "Epoch 5044: Training Loss: 0.20168323814868927 Validation Loss: 0.733425498008728\n",
      "Epoch 5045: Training Loss: 0.20035530626773834 Validation Loss: 0.733492910861969\n",
      "Epoch 5046: Training Loss: 0.2003957082827886 Validation Loss: 0.7338139414787292\n",
      "Epoch 5047: Training Loss: 0.2001236230134964 Validation Loss: 0.7335633039474487\n",
      "Epoch 5048: Training Loss: 0.20026971896489462 Validation Loss: 0.7334833741188049\n",
      "Epoch 5049: Training Loss: 0.19929065306981406 Validation Loss: 0.7336181402206421\n",
      "Epoch 5050: Training Loss: 0.20008283853530884 Validation Loss: 0.7334239482879639\n",
      "Epoch 5051: Training Loss: 0.1999851862589518 Validation Loss: 0.7333434224128723\n",
      "Epoch 5052: Training Loss: 0.20026095708211264 Validation Loss: 0.7332201600074768\n",
      "Epoch 5053: Training Loss: 0.19987109303474426 Validation Loss: 0.7330971360206604\n",
      "Epoch 5054: Training Loss: 0.19966950515906015 Validation Loss: 0.7330552339553833\n",
      "Epoch 5055: Training Loss: 0.1999606986840566 Validation Loss: 0.7333385348320007\n",
      "Epoch 5056: Training Loss: 0.1998758167028427 Validation Loss: 0.7333661317825317\n",
      "Epoch 5057: Training Loss: 0.19986640413602194 Validation Loss: 0.733415961265564\n",
      "Epoch 5058: Training Loss: 0.19976106782754263 Validation Loss: 0.7332115769386292\n",
      "Epoch 5059: Training Loss: 0.2011929303407669 Validation Loss: 0.7337955832481384\n",
      "Epoch 5060: Training Loss: 0.19963673750559488 Validation Loss: 0.7332333326339722\n",
      "Epoch 5061: Training Loss: 0.19953655203183493 Validation Loss: 0.7329035997390747\n",
      "Epoch 5062: Training Loss: 0.1996395339568456 Validation Loss: 0.7327559590339661\n",
      "Epoch 5063: Training Loss: 0.1998348037401835 Validation Loss: 0.7330858111381531\n",
      "Epoch 5064: Training Loss: 0.19965136051177979 Validation Loss: 0.7333292961120605\n",
      "Epoch 5065: Training Loss: 0.1996375322341919 Validation Loss: 0.7332331538200378\n",
      "Epoch 5066: Training Loss: 0.19892603158950806 Validation Loss: 0.7332190275192261\n",
      "Epoch 5067: Training Loss: 0.19941767553488413 Validation Loss: 0.7337626218795776\n",
      "Epoch 5068: Training Loss: 0.19914218286673227 Validation Loss: 0.734183669090271\n",
      "Epoch 5069: Training Loss: 0.20004881421724954 Validation Loss: 0.7337656617164612\n",
      "Epoch 5070: Training Loss: 0.1994010309378306 Validation Loss: 0.7336183190345764\n",
      "Epoch 5071: Training Loss: 0.19933799902598062 Validation Loss: 0.733443558216095\n",
      "Epoch 5072: Training Loss: 0.19920425613721213 Validation Loss: 0.7332413196563721\n",
      "Epoch 5073: Training Loss: 0.1992348482211431 Validation Loss: 0.733409583568573\n",
      "Epoch 5074: Training Loss: 0.19932441910107931 Validation Loss: 0.7327162623405457\n",
      "Epoch 5075: Training Loss: 0.1991474131743113 Validation Loss: 0.732107400894165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5076: Training Loss: 0.19968601564566293 Validation Loss: 0.7320805788040161\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5077: Training Loss: 0.1993375321229299 Validation Loss: 0.7328929901123047\n",
      "Epoch 5078: Training Loss: 0.19958866139252981 Validation Loss: 0.7332205176353455\n",
      "Epoch 5079: Training Loss: 0.19932028154532114 Validation Loss: 0.7335418462753296\n",
      "Epoch 5080: Training Loss: 0.19911175966262817 Validation Loss: 0.7336680293083191\n",
      "Epoch 5081: Training Loss: 0.19903624057769775 Validation Loss: 0.7336464524269104\n",
      "Epoch 5082: Training Loss: 0.19883714119593301 Validation Loss: 0.7333354353904724\n",
      "Epoch 5083: Training Loss: 0.19990882277488708 Validation Loss: 0.7329912185668945\n",
      "Epoch 5084: Training Loss: 0.19874882201353708 Validation Loss: 0.7326552271842957\n",
      "Epoch 5085: Training Loss: 0.19951060911019644 Validation Loss: 0.7326487302780151\n",
      "Epoch 5086: Training Loss: 0.19850161174933115 Validation Loss: 0.7332336902618408\n",
      "Epoch 5087: Training Loss: 0.19856113195419312 Validation Loss: 0.7335813641548157\n",
      "Epoch 5088: Training Loss: 0.19859951734542847 Validation Loss: 0.7337665557861328\n",
      "Epoch 5089: Training Loss: 0.19851727783679962 Validation Loss: 0.7334058284759521\n",
      "Epoch 5090: Training Loss: 0.1991735895474752 Validation Loss: 0.7336461544036865\n",
      "Epoch 5091: Training Loss: 0.19912716249624887 Validation Loss: 0.7336081266403198\n",
      "Epoch 5092: Training Loss: 0.19821110864480337 Validation Loss: 0.733628511428833\n",
      "Epoch 5093: Training Loss: 0.1981860746939977 Validation Loss: 0.7334463000297546\n",
      "Epoch 5094: Training Loss: 0.19831285874048868 Validation Loss: 0.7331887483596802\n",
      "Epoch 5095: Training Loss: 0.1986976017554601 Validation Loss: 0.7333914041519165\n",
      "Epoch 5096: Training Loss: 0.19852452476819357 Validation Loss: 0.7334344983100891\n",
      "Epoch 5097: Training Loss: 0.19841900964577994 Validation Loss: 0.7333052158355713\n",
      "Epoch 5098: Training Loss: 0.19939821461836496 Validation Loss: 0.7325413823127747\n",
      "Epoch 5099: Training Loss: 0.1987327833970388 Validation Loss: 0.7326695919036865\n",
      "Epoch 5100: Training Loss: 0.1980906923611959 Validation Loss: 0.7328999638557434\n",
      "Epoch 5101: Training Loss: 0.19892353812853494 Validation Loss: 0.7329843044281006\n",
      "Epoch 5102: Training Loss: 0.19852649172147116 Validation Loss: 0.7331888675689697\n",
      "Epoch 5103: Training Loss: 0.19811982909838358 Validation Loss: 0.7330994606018066\n",
      "Epoch 5104: Training Loss: 0.1983285297950109 Validation Loss: 0.7333166599273682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5105: Training Loss: 0.19774799048900604 Validation Loss: 0.7332203984260559\n",
      "Epoch 5106: Training Loss: 0.1981295794248581 Validation Loss: 0.7332127690315247\n",
      "Epoch 5107: Training Loss: 0.19835393130779266 Validation Loss: 0.7327970266342163\n",
      "Epoch 5108: Training Loss: 0.19817512234052023 Validation Loss: 0.7324353456497192\n",
      "Epoch 5109: Training Loss: 0.19802903632322946 Validation Loss: 0.7330865263938904\n",
      "Epoch 5110: Training Loss: 0.19769401848316193 Validation Loss: 0.7334256768226624\n",
      "Epoch 5111: Training Loss: 0.19773002962271372 Validation Loss: 0.7335444092750549\n",
      "Epoch 5112: Training Loss: 0.19839324057102203 Validation Loss: 0.7337968945503235\n",
      "Epoch 5113: Training Loss: 0.19765080511569977 Validation Loss: 0.7334226369857788\n",
      "Epoch 5114: Training Loss: 0.19765037298202515 Validation Loss: 0.7326564192771912\n",
      "Epoch 5115: Training Loss: 0.1974471112092336 Validation Loss: 0.7332685589790344\n",
      "Epoch 5116: Training Loss: 0.1977791041135788 Validation Loss: 0.7332024574279785\n",
      "Epoch 5117: Training Loss: 0.19755161305268606 Validation Loss: 0.733644962310791\n",
      "Epoch 5118: Training Loss: 0.19751314322153726 Validation Loss: 0.7334480881690979\n",
      "Epoch 5119: Training Loss: 0.19743080933888754 Validation Loss: 0.7332969307899475\n",
      "Epoch 5120: Training Loss: 0.19722324113051096 Validation Loss: 0.7328261137008667\n",
      "Epoch 5121: Training Loss: 0.19771674275398254 Validation Loss: 0.732495903968811\n",
      "Epoch 5122: Training Loss: 0.19734431306521097 Validation Loss: 0.7326535582542419\n",
      "Epoch 5123: Training Loss: 0.19703745345274606 Validation Loss: 0.7325538992881775\n",
      "Epoch 5124: Training Loss: 0.19736711184183756 Validation Loss: 0.7326455116271973\n",
      "Epoch 5125: Training Loss: 0.19661704202493033 Validation Loss: 0.7328628897666931\n",
      "Epoch 5126: Training Loss: 0.19716023902098337 Validation Loss: 0.732995867729187\n",
      "Epoch 5127: Training Loss: 0.19742363691329956 Validation Loss: 0.7333999872207642\n",
      "Epoch 5128: Training Loss: 0.19692709048589072 Validation Loss: 0.7333906292915344\n",
      "Epoch 5129: Training Loss: 0.1971829185883204 Validation Loss: 0.7328341603279114\n",
      "Epoch 5130: Training Loss: 0.1971908857425054 Validation Loss: 0.7326687574386597\n",
      "Epoch 5131: Training Loss: 0.19650587936242422 Validation Loss: 0.7325865626335144\n",
      "Epoch 5132: Training Loss: 0.19641998906930289 Validation Loss: 0.7326300740242004\n",
      "Epoch 5133: Training Loss: 0.19695942600568137 Validation Loss: 0.732610285282135\n",
      "Epoch 5134: Training Loss: 0.1964025100072225 Validation Loss: 0.7330977916717529\n",
      "Epoch 5135: Training Loss: 0.19662206868330637 Validation Loss: 0.7329176664352417\n",
      "Epoch 5136: Training Loss: 0.1972182790438334 Validation Loss: 0.7333170771598816\n",
      "Epoch 5137: Training Loss: 0.1977276454369227 Validation Loss: 0.7331749796867371\n",
      "Epoch 5138: Training Loss: 0.19697827597459158 Validation Loss: 0.7334679365158081\n",
      "Epoch 5139: Training Loss: 0.1968891123930613 Validation Loss: 0.733335554599762\n",
      "Epoch 5140: Training Loss: 0.19645261267820993 Validation Loss: 0.7332649827003479\n",
      "Epoch 5141: Training Loss: 0.19719975690046945 Validation Loss: 0.7335307002067566\n",
      "Epoch 5142: Training Loss: 0.1965391437212626 Validation Loss: 0.7332642674446106\n",
      "Epoch 5143: Training Loss: 0.19690023362636566 Validation Loss: 0.7330109477043152\n",
      "Epoch 5144: Training Loss: 0.19650602837403616 Validation Loss: 0.7329263091087341\n",
      "Epoch 5145: Training Loss: 0.19641155997912088 Validation Loss: 0.7329227924346924\n",
      "Epoch 5146: Training Loss: 0.19731506208578745 Validation Loss: 0.7332374453544617\n",
      "Epoch 5147: Training Loss: 0.1961257259051005 Validation Loss: 0.7329736351966858\n",
      "Epoch 5148: Training Loss: 0.1967198153336843 Validation Loss: 0.733189582824707\n",
      "Epoch 5149: Training Loss: 0.19627522925535837 Validation Loss: 0.7330001592636108\n",
      "Epoch 5150: Training Loss: 0.19625899692376456 Validation Loss: 0.7329996824264526\n",
      "Epoch 5151: Training Loss: 0.1960057665904363 Validation Loss: 0.7328152656555176\n",
      "Epoch 5152: Training Loss: 0.1961218218008677 Validation Loss: 0.7332947850227356\n",
      "Epoch 5153: Training Loss: 0.19630552331606546 Validation Loss: 0.7331762313842773\n",
      "Epoch 5154: Training Loss: 0.196712593237559 Validation Loss: 0.7325214743614197\n",
      "Epoch 5155: Training Loss: 0.1964973509311676 Validation Loss: 0.7328511476516724\n",
      "Epoch 5156: Training Loss: 0.1969916025797526 Validation Loss: 0.7327886819839478\n",
      "Epoch 5157: Training Loss: 0.19642478227615356 Validation Loss: 0.7327572703361511\n",
      "Epoch 5158: Training Loss: 0.19779173533121744 Validation Loss: 0.7328328490257263\n",
      "Epoch 5159: Training Loss: 0.1964842826128006 Validation Loss: 0.7329817414283752\n",
      "Epoch 5160: Training Loss: 0.19573786854743958 Validation Loss: 0.7331992387771606\n",
      "Epoch 5161: Training Loss: 0.19612054030100504 Validation Loss: 0.7341371178627014\n",
      "Epoch 5162: Training Loss: 0.19611606001853943 Validation Loss: 0.7340008616447449\n",
      "Epoch 5163: Training Loss: 0.1957219143708547 Validation Loss: 0.7337745428085327\n",
      "Epoch 5164: Training Loss: 0.19578126072883606 Validation Loss: 0.733456552028656\n",
      "Epoch 5165: Training Loss: 0.19575274487336478 Validation Loss: 0.7330986857414246\n",
      "Epoch 5166: Training Loss: 0.19565633436044058 Validation Loss: 0.7323303818702698\n",
      "Epoch 5167: Training Loss: 0.1957071671883265 Validation Loss: 0.73227858543396\n",
      "Epoch 5168: Training Loss: 0.19536090393861136 Validation Loss: 0.7321999669075012\n",
      "Epoch 5169: Training Loss: 0.19551197191079459 Validation Loss: 0.7325555682182312\n",
      "Epoch 5170: Training Loss: 0.19541417062282562 Validation Loss: 0.7329102754592896\n",
      "Epoch 5171: Training Loss: 0.19511101146539053 Validation Loss: 0.7331984639167786\n",
      "Epoch 5172: Training Loss: 0.19546151161193848 Validation Loss: 0.7327799797058105\n",
      "Epoch 5173: Training Loss: 0.19513742129007974 Validation Loss: 0.7331082224845886\n",
      "Epoch 5174: Training Loss: 0.19515782594680786 Validation Loss: 0.7327411770820618\n",
      "Epoch 5175: Training Loss: 0.1952974945306778 Validation Loss: 0.7328612208366394\n",
      "Epoch 5176: Training Loss: 0.19526451329390207 Validation Loss: 0.7327705025672913\n",
      "Epoch 5177: Training Loss: 0.19617015620072684 Validation Loss: 0.7328968644142151\n",
      "Epoch 5178: Training Loss: 0.19533459842205048 Validation Loss: 0.7328276038169861\n",
      "Epoch 5179: Training Loss: 0.19542134801546732 Validation Loss: 0.7323023676872253\n",
      "Epoch 5180: Training Loss: 0.1949660231669744 Validation Loss: 0.7328197956085205\n",
      "Epoch 5181: Training Loss: 0.1952760765949885 Validation Loss: 0.7328242659568787\n",
      "Epoch 5182: Training Loss: 0.1948313663403193 Validation Loss: 0.7328987717628479\n",
      "Epoch 5183: Training Loss: 0.1950644999742508 Validation Loss: 0.7328134179115295\n",
      "Epoch 5184: Training Loss: 0.194673091173172 Validation Loss: 0.732909083366394\n",
      "Epoch 5185: Training Loss: 0.19530844688415527 Validation Loss: 0.7331106662750244\n",
      "Epoch 5186: Training Loss: 0.1953144073486328 Validation Loss: 0.7326810359954834\n",
      "Epoch 5187: Training Loss: 0.19494305551052094 Validation Loss: 0.7329055666923523\n",
      "Epoch 5188: Training Loss: 0.19393357634544373 Validation Loss: 0.7331169247627258\n",
      "Epoch 5189: Training Loss: 0.19470329582691193 Validation Loss: 0.7329882979393005\n",
      "Epoch 5190: Training Loss: 0.19459262986977896 Validation Loss: 0.7328043580055237\n",
      "Epoch 5191: Training Loss: 0.19481845696767172 Validation Loss: 0.7330362200737\n",
      "Epoch 5192: Training Loss: 0.1945770134528478 Validation Loss: 0.7326863408088684\n",
      "Epoch 5193: Training Loss: 0.19472001492977142 Validation Loss: 0.7323038578033447\n",
      "Epoch 5194: Training Loss: 0.1953194042046865 Validation Loss: 0.732208251953125\n",
      "Epoch 5195: Training Loss: 0.19413899878660837 Validation Loss: 0.7324802875518799\n",
      "Epoch 5196: Training Loss: 0.19446164866288504 Validation Loss: 0.7326667904853821\n",
      "Epoch 5197: Training Loss: 0.19539315501848856 Validation Loss: 0.7323848009109497\n",
      "Epoch 5198: Training Loss: 0.19447156290213266 Validation Loss: 0.7327428460121155\n",
      "Epoch 5199: Training Loss: 0.19431779781977335 Validation Loss: 0.7333995699882507\n",
      "Epoch 5200: Training Loss: 0.1944780796766281 Validation Loss: 0.7331693172454834\n",
      "Epoch 5201: Training Loss: 0.19433215260505676 Validation Loss: 0.7334645390510559\n",
      "Epoch 5202: Training Loss: 0.19437730809052786 Validation Loss: 0.7331665754318237\n",
      "Epoch 5203: Training Loss: 0.19414643943309784 Validation Loss: 0.7330299019813538\n",
      "Epoch 5204: Training Loss: 0.19369036455949148 Validation Loss: 0.7328138947486877\n",
      "Epoch 5205: Training Loss: 0.19394399722417197 Validation Loss: 0.7328786849975586\n",
      "Epoch 5206: Training Loss: 0.19428887963294983 Validation Loss: 0.7327306270599365\n",
      "Epoch 5207: Training Loss: 0.19414612154165903 Validation Loss: 0.7325395345687866\n",
      "Epoch 5208: Training Loss: 0.19305797417958578 Validation Loss: 0.7327885031700134\n",
      "Epoch 5209: Training Loss: 0.19405749440193176 Validation Loss: 0.7326298952102661\n",
      "Epoch 5210: Training Loss: 0.1945913682381312 Validation Loss: 0.7320839762687683\n",
      "Epoch 5211: Training Loss: 0.19408694406350455 Validation Loss: 0.7319507002830505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5212: Training Loss: 0.1933949887752533 Validation Loss: 0.7319879531860352\n",
      "Epoch 5213: Training Loss: 0.19384733339150748 Validation Loss: 0.732490599155426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5214: Training Loss: 0.19387611746788025 Validation Loss: 0.7332048416137695\n",
      "Epoch 5215: Training Loss: 0.19432474672794342 Validation Loss: 0.7332666516304016\n",
      "Epoch 5216: Training Loss: 0.19364071389039358 Validation Loss: 0.733172595500946\n",
      "Epoch 5217: Training Loss: 0.19407343864440918 Validation Loss: 0.7329764366149902\n",
      "Epoch 5218: Training Loss: 0.1931186964114507 Validation Loss: 0.7325813174247742\n",
      "Epoch 5219: Training Loss: 0.1936850150426229 Validation Loss: 0.7322869300842285\n",
      "Epoch 5220: Training Loss: 0.19338723520437875 Validation Loss: 0.7318609356880188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5221: Training Loss: 0.1933789700269699 Validation Loss: 0.7320235371589661\n",
      "Epoch 5222: Training Loss: 0.19447059432665506 Validation Loss: 0.7322109341621399\n",
      "Epoch 5223: Training Loss: 0.193851500749588 Validation Loss: 0.7330171465873718\n",
      "Epoch 5224: Training Loss: 0.19356538355350494 Validation Loss: 0.7327587604522705\n",
      "Epoch 5225: Training Loss: 0.19323376814524332 Validation Loss: 0.7331364154815674\n",
      "Epoch 5226: Training Loss: 0.19424645602703094 Validation Loss: 0.733319878578186\n",
      "Epoch 5227: Training Loss: 0.19380954404671988 Validation Loss: 0.7334957122802734\n",
      "Epoch 5228: Training Loss: 0.19333850840727487 Validation Loss: 0.7330097556114197\n",
      "Epoch 5229: Training Loss: 0.19375433524449667 Validation Loss: 0.7327952980995178\n",
      "Epoch 5230: Training Loss: 0.193108469247818 Validation Loss: 0.7325720191001892\n",
      "Epoch 5231: Training Loss: 0.1933804154396057 Validation Loss: 0.7325313687324524\n",
      "Epoch 5232: Training Loss: 0.19255431989828745 Validation Loss: 0.7326816320419312\n",
      "Epoch 5233: Training Loss: 0.19417385260264078 Validation Loss: 0.7327070236206055\n",
      "Epoch 5234: Training Loss: 0.19365127881368002 Validation Loss: 0.7327845096588135\n",
      "Epoch 5235: Training Loss: 0.19348618388175964 Validation Loss: 0.7324578166007996\n",
      "Epoch 5236: Training Loss: 0.19358224173386893 Validation Loss: 0.7323610186576843\n",
      "Epoch 5237: Training Loss: 0.19254982471466064 Validation Loss: 0.7326776385307312\n",
      "Epoch 5238: Training Loss: 0.19265984992186228 Validation Loss: 0.7327046990394592\n",
      "Epoch 5239: Training Loss: 0.19315828382968903 Validation Loss: 0.7325004935264587\n",
      "Epoch 5240: Training Loss: 0.1928764283657074 Validation Loss: 0.7329527735710144\n",
      "Epoch 5241: Training Loss: 0.1927543431520462 Validation Loss: 0.732667863368988\n",
      "Epoch 5242: Training Loss: 0.19259429474671683 Validation Loss: 0.7327797412872314\n",
      "Epoch 5243: Training Loss: 0.1945230613152186 Validation Loss: 0.7325486540794373\n",
      "Epoch 5244: Training Loss: 0.1934106300274531 Validation Loss: 0.7328416705131531\n",
      "Epoch 5245: Training Loss: 0.19265680015087128 Validation Loss: 0.7326018214225769\n",
      "Epoch 5246: Training Loss: 0.1941298544406891 Validation Loss: 0.7324034571647644\n",
      "Epoch 5247: Training Loss: 0.19276919960975647 Validation Loss: 0.7323456406593323\n",
      "Epoch 5248: Training Loss: 0.19264939924081168 Validation Loss: 0.7324581146240234\n",
      "Epoch 5249: Training Loss: 0.19353316724300385 Validation Loss: 0.7326689958572388\n",
      "Epoch 5250: Training Loss: 0.19361025591691336 Validation Loss: 0.7326383590698242\n",
      "Epoch 5251: Training Loss: 0.19243828455607095 Validation Loss: 0.7332106828689575\n",
      "Epoch 5252: Training Loss: 0.19314652184645334 Validation Loss: 0.7334920167922974\n",
      "Epoch 5253: Training Loss: 0.19244123001893362 Validation Loss: 0.7329787611961365\n",
      "Epoch 5254: Training Loss: 0.19250312447547913 Validation Loss: 0.7333536148071289\n",
      "Epoch 5255: Training Loss: 0.19232570131619772 Validation Loss: 0.7327418327331543\n",
      "Epoch 5256: Training Loss: 0.192752867937088 Validation Loss: 0.7324864864349365\n",
      "Epoch 5257: Training Loss: 0.19262221455574036 Validation Loss: 0.7321072816848755\n",
      "Epoch 5258: Training Loss: 0.192343071103096 Validation Loss: 0.7312408685684204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5259: Training Loss: 0.19209632774194083 Validation Loss: 0.731385350227356\n",
      "Epoch 5260: Training Loss: 0.19228796660900116 Validation Loss: 0.7316315174102783\n",
      "Epoch 5261: Training Loss: 0.19243291517098746 Validation Loss: 0.7327964901924133\n",
      "Epoch 5262: Training Loss: 0.1923880328734716 Validation Loss: 0.7330674529075623\n",
      "Epoch 5263: Training Loss: 0.192404106259346 Validation Loss: 0.7333046793937683\n",
      "Epoch 5264: Training Loss: 0.19251969953378043 Validation Loss: 0.7324872612953186\n",
      "Epoch 5265: Training Loss: 0.19201147556304932 Validation Loss: 0.7327251434326172\n",
      "Epoch 5266: Training Loss: 0.1919703334569931 Validation Loss: 0.732766330242157\n",
      "Epoch 5267: Training Loss: 0.19177289803822836 Validation Loss: 0.7328466773033142\n",
      "Epoch 5268: Training Loss: 0.19243320326010385 Validation Loss: 0.7330228090286255\n",
      "Epoch 5269: Training Loss: 0.19253984093666077 Validation Loss: 0.7329386472702026\n",
      "Epoch 5270: Training Loss: 0.19165247678756714 Validation Loss: 0.7325412631034851\n",
      "Epoch 5271: Training Loss: 0.19162639478842416 Validation Loss: 0.7324060797691345\n",
      "Epoch 5272: Training Loss: 0.19170181453227997 Validation Loss: 0.7321336269378662\n",
      "Epoch 5273: Training Loss: 0.19222254554430643 Validation Loss: 0.7325984239578247\n",
      "Epoch 5274: Training Loss: 0.19191303352514902 Validation Loss: 0.7330681085586548\n",
      "Epoch 5275: Training Loss: 0.191598708430926 Validation Loss: 0.7325249910354614\n",
      "Epoch 5276: Training Loss: 0.1915799876054128 Validation Loss: 0.7330289483070374\n",
      "Epoch 5277: Training Loss: 0.1916996737321218 Validation Loss: 0.7325236797332764\n",
      "Epoch 5278: Training Loss: 0.19133733212947845 Validation Loss: 0.7323393225669861\n",
      "Epoch 5279: Training Loss: 0.19144431253274283 Validation Loss: 0.7329081296920776\n",
      "Epoch 5280: Training Loss: 0.19131987790266672 Validation Loss: 0.7327597737312317\n",
      "Epoch 5281: Training Loss: 0.19228733579317728 Validation Loss: 0.7323709726333618\n",
      "Epoch 5282: Training Loss: 0.19126117726167044 Validation Loss: 0.7325179576873779\n",
      "Epoch 5283: Training Loss: 0.1914954880873362 Validation Loss: 0.73225998878479\n",
      "Epoch 5284: Training Loss: 0.19136342406272888 Validation Loss: 0.732566773891449\n",
      "Epoch 5285: Training Loss: 0.1915126790603002 Validation Loss: 0.7325839400291443\n",
      "Epoch 5286: Training Loss: 0.19192244112491608 Validation Loss: 0.732673168182373\n",
      "Epoch 5287: Training Loss: 0.1911457727352778 Validation Loss: 0.732780933380127\n",
      "Epoch 5288: Training Loss: 0.1910924712816874 Validation Loss: 0.7322400212287903\n",
      "Epoch 5289: Training Loss: 0.1912235071261724 Validation Loss: 0.7324779629707336\n",
      "Epoch 5290: Training Loss: 0.1909018705288569 Validation Loss: 0.732504665851593\n",
      "Epoch 5291: Training Loss: 0.19152856866518655 Validation Loss: 0.7325009703636169\n",
      "Epoch 5292: Training Loss: 0.19017850855986276 Validation Loss: 0.7323728203773499\n",
      "Epoch 5293: Training Loss: 0.19120033085346222 Validation Loss: 0.7328716516494751\n",
      "Epoch 5294: Training Loss: 0.1910236875216166 Validation Loss: 0.7328770160675049\n",
      "Epoch 5295: Training Loss: 0.19039038817087808 Validation Loss: 0.7327091097831726\n",
      "Epoch 5296: Training Loss: 0.19042923549811044 Validation Loss: 0.7329963445663452\n",
      "Epoch 5297: Training Loss: 0.1906383434931437 Validation Loss: 0.7327193021774292\n",
      "Epoch 5298: Training Loss: 0.19091666241486868 Validation Loss: 0.732282817363739\n",
      "Epoch 5299: Training Loss: 0.19136853019396463 Validation Loss: 0.7324976921081543\n",
      "Epoch 5300: Training Loss: 0.190503790974617 Validation Loss: 0.7319787740707397\n",
      "Epoch 5301: Training Loss: 0.19045769174893698 Validation Loss: 0.7320924997329712\n",
      "Epoch 5302: Training Loss: 0.19055895507335663 Validation Loss: 0.732290506362915\n",
      "Epoch 5303: Training Loss: 0.1903382589419683 Validation Loss: 0.7322263717651367\n",
      "Epoch 5304: Training Loss: 0.19098716974258423 Validation Loss: 0.732440710067749\n",
      "Epoch 5305: Training Loss: 0.19092736641565958 Validation Loss: 0.7320848107337952\n",
      "Epoch 5306: Training Loss: 0.1903584748506546 Validation Loss: 0.732325553894043\n",
      "Epoch 5307: Training Loss: 0.1904575377702713 Validation Loss: 0.7329210042953491\n",
      "Epoch 5308: Training Loss: 0.19074034690856934 Validation Loss: 0.732753574848175\n",
      "Epoch 5309: Training Loss: 0.19063948094844818 Validation Loss: 0.7329214811325073\n",
      "Epoch 5310: Training Loss: 0.1906797488530477 Validation Loss: 0.7329855561256409\n",
      "Epoch 5311: Training Loss: 0.19036799669265747 Validation Loss: 0.7325904369354248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5312: Training Loss: 0.19015399614969888 Validation Loss: 0.7325417399406433\n",
      "Epoch 5313: Training Loss: 0.19059382875760397 Validation Loss: 0.7326560616493225\n",
      "Epoch 5314: Training Loss: 0.19096670051415762 Validation Loss: 0.7326552867889404\n",
      "Epoch 5315: Training Loss: 0.19029215474923453 Validation Loss: 0.7333657741546631\n",
      "Epoch 5316: Training Loss: 0.18971417844295502 Validation Loss: 0.7333248853683472\n",
      "Epoch 5317: Training Loss: 0.1900334507226944 Validation Loss: 0.7326666712760925\n",
      "Epoch 5318: Training Loss: 0.19095027446746826 Validation Loss: 0.7319888472557068\n",
      "Epoch 5319: Training Loss: 0.189661905169487 Validation Loss: 0.7319042682647705\n",
      "Epoch 5320: Training Loss: 0.19004235168298086 Validation Loss: 0.7317266464233398\n",
      "Epoch 5321: Training Loss: 0.190105140209198 Validation Loss: 0.7318470478057861\n",
      "Epoch 5322: Training Loss: 0.19003972907861075 Validation Loss: 0.7324987649917603\n",
      "Epoch 5323: Training Loss: 0.18983401358127594 Validation Loss: 0.7320837378501892\n",
      "Epoch 5324: Training Loss: 0.18989034493764242 Validation Loss: 0.7323340177536011\n",
      "Epoch 5325: Training Loss: 0.19022364417711893 Validation Loss: 0.7323168516159058\n",
      "Epoch 5326: Training Loss: 0.1898368944724401 Validation Loss: 0.7321224212646484\n",
      "Epoch 5327: Training Loss: 0.19018860161304474 Validation Loss: 0.732589840888977\n",
      "Epoch 5328: Training Loss: 0.1894762764374415 Validation Loss: 0.7332295179367065\n",
      "Epoch 5329: Training Loss: 0.18945889174938202 Validation Loss: 0.7331273555755615\n",
      "Epoch 5330: Training Loss: 0.1899008403221766 Validation Loss: 0.7327308654785156\n",
      "Epoch 5331: Training Loss: 0.18969747920831045 Validation Loss: 0.7329427003860474\n",
      "Epoch 5332: Training Loss: 0.19005613029003143 Validation Loss: 0.7320877909660339\n",
      "Epoch 5333: Training Loss: 0.18952889740467072 Validation Loss: 0.7317359447479248\n",
      "Epoch 5334: Training Loss: 0.18975310027599335 Validation Loss: 0.7314119338989258\n",
      "Epoch 5335: Training Loss: 0.1897254834572474 Validation Loss: 0.7319289445877075\n",
      "Epoch 5336: Training Loss: 0.18966932594776154 Validation Loss: 0.7320921421051025\n",
      "Epoch 5337: Training Loss: 0.18950050075848898 Validation Loss: 0.7325082421302795\n",
      "Epoch 5338: Training Loss: 0.1896528402964274 Validation Loss: 0.7329420447349548\n",
      "Epoch 5339: Training Loss: 0.18923334777355194 Validation Loss: 0.7328252196311951\n",
      "Epoch 5340: Training Loss: 0.18929433325926462 Validation Loss: 0.7327406406402588\n",
      "Epoch 5341: Training Loss: 0.18904444575309753 Validation Loss: 0.7323637008666992\n",
      "Epoch 5342: Training Loss: 0.18981224298477173 Validation Loss: 0.7320897579193115\n",
      "Epoch 5343: Training Loss: 0.18928786118825278 Validation Loss: 0.7321923971176147\n",
      "Epoch 5344: Training Loss: 0.18923797210057577 Validation Loss: 0.7325109839439392\n",
      "Epoch 5345: Training Loss: 0.18877895176410675 Validation Loss: 0.732801079750061\n",
      "Epoch 5346: Training Loss: 0.18895756701628366 Validation Loss: 0.7329351902008057\n",
      "Epoch 5347: Training Loss: 0.18903686106204987 Validation Loss: 0.7326375246047974\n",
      "Epoch 5348: Training Loss: 0.18887275954087576 Validation Loss: 0.7321010828018188\n",
      "Epoch 5349: Training Loss: 0.18941201269626617 Validation Loss: 0.7323125004768372\n",
      "Epoch 5350: Training Loss: 0.18924658000469208 Validation Loss: 0.7329252362251282\n",
      "Epoch 5351: Training Loss: 0.1887242595354716 Validation Loss: 0.7326772809028625\n",
      "Epoch 5352: Training Loss: 0.18895105520884195 Validation Loss: 0.7322936654090881\n",
      "Epoch 5353: Training Loss: 0.18884364267190298 Validation Loss: 0.7321333885192871\n",
      "Epoch 5354: Training Loss: 0.1885828971862793 Validation Loss: 0.7327834963798523\n",
      "Epoch 5355: Training Loss: 0.18965783218542734 Validation Loss: 0.7324854135513306\n",
      "Epoch 5356: Training Loss: 0.18897453943888345 Validation Loss: 0.732433021068573\n",
      "Epoch 5357: Training Loss: 0.1882478396097819 Validation Loss: 0.732349157333374\n",
      "Epoch 5358: Training Loss: 0.18889435132344565 Validation Loss: 0.732451856136322\n",
      "Epoch 5359: Training Loss: 0.1889314353466034 Validation Loss: 0.732621431350708\n",
      "Epoch 5360: Training Loss: 0.18862389028072357 Validation Loss: 0.7325000762939453\n",
      "Epoch 5361: Training Loss: 0.18836191793282828 Validation Loss: 0.7321729063987732\n",
      "Epoch 5362: Training Loss: 0.19021750489870706 Validation Loss: 0.7322396039962769\n",
      "Epoch 5363: Training Loss: 0.1884799748659134 Validation Loss: 0.7326652407646179\n",
      "Epoch 5364: Training Loss: 0.1876582702000936 Validation Loss: 0.7327516674995422\n",
      "Epoch 5365: Training Loss: 0.1884749879439672 Validation Loss: 0.7328613996505737\n",
      "Epoch 5366: Training Loss: 0.18889622390270233 Validation Loss: 0.7332525849342346\n",
      "Epoch 5367: Training Loss: 0.18801048398017883 Validation Loss: 0.7331409454345703\n",
      "Epoch 5368: Training Loss: 0.18842660884062448 Validation Loss: 0.7330256700515747\n",
      "Epoch 5369: Training Loss: 0.18831508855024973 Validation Loss: 0.7327511310577393\n",
      "Epoch 5370: Training Loss: 0.18784067531426749 Validation Loss: 0.7324241399765015\n",
      "Epoch 5371: Training Loss: 0.18947938084602356 Validation Loss: 0.7326356172561646\n",
      "Epoch 5372: Training Loss: 0.18811792135238647 Validation Loss: 0.731961190700531\n",
      "Epoch 5373: Training Loss: 0.18834631145000458 Validation Loss: 0.7317788600921631\n",
      "Epoch 5374: Training Loss: 0.18782674769560495 Validation Loss: 0.7318969964981079\n",
      "Epoch 5375: Training Loss: 0.18976947665214539 Validation Loss: 0.7322402596473694\n",
      "Epoch 5376: Training Loss: 0.18842157224814096 Validation Loss: 0.7328081727027893\n",
      "Epoch 5377: Training Loss: 0.18802891671657562 Validation Loss: 0.7326905131340027\n",
      "Epoch 5378: Training Loss: 0.18783372143904367 Validation Loss: 0.7326818704605103\n",
      "Epoch 5379: Training Loss: 0.18840525050957999 Validation Loss: 0.7329159379005432\n",
      "Epoch 5380: Training Loss: 0.18786194423834482 Validation Loss: 0.7329456806182861\n",
      "Epoch 5381: Training Loss: 0.1879807859659195 Validation Loss: 0.7324970960617065\n",
      "Epoch 5382: Training Loss: 0.1876334249973297 Validation Loss: 0.7322090268135071\n",
      "Epoch 5383: Training Loss: 0.18802826603253683 Validation Loss: 0.7326575517654419\n",
      "Epoch 5384: Training Loss: 0.18778646488984427 Validation Loss: 0.7327757477760315\n",
      "Epoch 5385: Training Loss: 0.18773596485455832 Validation Loss: 0.7326237559318542\n",
      "Epoch 5386: Training Loss: 0.18791564802328745 Validation Loss: 0.7320212721824646\n",
      "Epoch 5387: Training Loss: 0.1877781550089518 Validation Loss: 0.7322509288787842\n",
      "Epoch 5388: Training Loss: 0.1879038761059443 Validation Loss: 0.7324318289756775\n",
      "Epoch 5389: Training Loss: 0.18865956862767538 Validation Loss: 0.7321648001670837\n",
      "Epoch 5390: Training Loss: 0.18824331959088644 Validation Loss: 0.7321599125862122\n",
      "Epoch 5391: Training Loss: 0.18771876891454062 Validation Loss: 0.732367217540741\n",
      "Epoch 5392: Training Loss: 0.18855647246042886 Validation Loss: 0.7324797511100769\n",
      "Epoch 5393: Training Loss: 0.18781759838263193 Validation Loss: 0.732521653175354\n",
      "Epoch 5394: Training Loss: 0.18754502634207407 Validation Loss: 0.7325961589813232\n",
      "Epoch 5395: Training Loss: 0.18747377395629883 Validation Loss: 0.7322370409965515\n",
      "Epoch 5396: Training Loss: 0.18678141136964163 Validation Loss: 0.7319841384887695\n",
      "Epoch 5397: Training Loss: 0.18741128842035928 Validation Loss: 0.7323453426361084\n",
      "Epoch 5398: Training Loss: 0.18741141756375632 Validation Loss: 0.7327189445495605\n",
      "Epoch 5399: Training Loss: 0.18744440376758575 Validation Loss: 0.7328360676765442\n",
      "Epoch 5400: Training Loss: 0.187111958861351 Validation Loss: 0.7324098944664001\n",
      "Epoch 5401: Training Loss: 0.18731881181399027 Validation Loss: 0.7321839928627014\n",
      "Epoch 5402: Training Loss: 0.18693358699480692 Validation Loss: 0.7320619821548462\n",
      "Epoch 5403: Training Loss: 0.18716644247372946 Validation Loss: 0.7322714328765869\n",
      "Epoch 5404: Training Loss: 0.18772190809249878 Validation Loss: 0.7324647903442383\n",
      "Epoch 5405: Training Loss: 0.18735848367214203 Validation Loss: 0.7326664328575134\n",
      "Epoch 5406: Training Loss: 0.18702559173107147 Validation Loss: 0.7327516078948975\n",
      "Epoch 5407: Training Loss: 0.1873860905567805 Validation Loss: 0.7324750423431396\n",
      "Epoch 5408: Training Loss: 0.1871992548306783 Validation Loss: 0.732547402381897\n",
      "Epoch 5409: Training Loss: 0.1869958887497584 Validation Loss: 0.732587993144989\n",
      "Epoch 5410: Training Loss: 0.1868215799331665 Validation Loss: 0.7321577072143555\n",
      "Epoch 5411: Training Loss: 0.18725545704364777 Validation Loss: 0.7321583032608032\n",
      "Epoch 5412: Training Loss: 0.18680286407470703 Validation Loss: 0.7324599623680115\n",
      "Epoch 5413: Training Loss: 0.1875136842330297 Validation Loss: 0.7330020070075989\n",
      "Epoch 5414: Training Loss: 0.1867462396621704 Validation Loss: 0.7330803871154785\n",
      "Epoch 5415: Training Loss: 0.1869280238946279 Validation Loss: 0.7327792048454285\n",
      "Epoch 5416: Training Loss: 0.18631968398888907 Validation Loss: 0.7321302890777588\n",
      "Epoch 5417: Training Loss: 0.1868716279665629 Validation Loss: 0.7320618033409119\n",
      "Epoch 5418: Training Loss: 0.18630914390087128 Validation Loss: 0.7319902181625366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5419: Training Loss: 0.186468372742335 Validation Loss: 0.7323896288871765\n",
      "Epoch 5420: Training Loss: 0.1864971766869227 Validation Loss: 0.7323454022407532\n",
      "Epoch 5421: Training Loss: 0.18625149130821228 Validation Loss: 0.7320147752761841\n",
      "Epoch 5422: Training Loss: 0.1870130697886149 Validation Loss: 0.7324493527412415\n",
      "Epoch 5423: Training Loss: 0.18647762636343637 Validation Loss: 0.7318901419639587\n",
      "Epoch 5424: Training Loss: 0.18651219209035239 Validation Loss: 0.7318551540374756\n",
      "Epoch 5425: Training Loss: 0.18653384844462076 Validation Loss: 0.7322052121162415\n",
      "Epoch 5426: Training Loss: 0.18698045114676157 Validation Loss: 0.7325190901756287\n",
      "Epoch 5427: Training Loss: 0.18618528048197427 Validation Loss: 0.7326945066452026\n",
      "Epoch 5428: Training Loss: 0.18645566205183664 Validation Loss: 0.7326797246932983\n",
      "Epoch 5429: Training Loss: 0.18635699152946472 Validation Loss: 0.7325114011764526\n",
      "Epoch 5430: Training Loss: 0.18618165453275046 Validation Loss: 0.7316060662269592\n",
      "Epoch 5431: Training Loss: 0.1859331727027893 Validation Loss: 0.7320640683174133\n",
      "Epoch 5432: Training Loss: 0.18603710333506265 Validation Loss: 0.7319292426109314\n",
      "Epoch 5433: Training Loss: 0.18613318602244058 Validation Loss: 0.731907069683075\n",
      "Epoch 5434: Training Loss: 0.18618138134479523 Validation Loss: 0.7323386669158936\n",
      "Epoch 5435: Training Loss: 0.18635316689809164 Validation Loss: 0.7322263121604919\n",
      "Epoch 5436: Training Loss: 0.18660813570022583 Validation Loss: 0.7319347858428955\n",
      "Epoch 5437: Training Loss: 0.18594234685103098 Validation Loss: 0.73191237449646\n",
      "Epoch 5438: Training Loss: 0.18562854329744974 Validation Loss: 0.7319825291633606\n",
      "Epoch 5439: Training Loss: 0.18590463201204935 Validation Loss: 0.7322803139686584\n",
      "Epoch 5440: Training Loss: 0.1857317934433619 Validation Loss: 0.7321849465370178\n",
      "Epoch 5441: Training Loss: 0.1856199304262797 Validation Loss: 0.7331473231315613\n",
      "Epoch 5442: Training Loss: 0.18572702010472616 Validation Loss: 0.7330051064491272\n",
      "Epoch 5443: Training Loss: 0.18578249712785086 Validation Loss: 0.7325071096420288\n",
      "Epoch 5444: Training Loss: 0.18625937898953757 Validation Loss: 0.7317412495613098\n",
      "Epoch 5445: Training Loss: 0.1856611967086792 Validation Loss: 0.7317829132080078\n",
      "Epoch 5446: Training Loss: 0.18577121694882712 Validation Loss: 0.7320012450218201\n",
      "Epoch 5447: Training Loss: 0.1856792817513148 Validation Loss: 0.7326074242591858\n",
      "Epoch 5448: Training Loss: 0.18681108951568604 Validation Loss: 0.7333369851112366\n",
      "Epoch 5449: Training Loss: 0.1855464925368627 Validation Loss: 0.7333095073699951\n",
      "Epoch 5450: Training Loss: 0.18603888154029846 Validation Loss: 0.7325786352157593\n",
      "Epoch 5451: Training Loss: 0.18541277945041656 Validation Loss: 0.7322079539299011\n",
      "Epoch 5452: Training Loss: 0.18561105926831564 Validation Loss: 0.7322234511375427\n",
      "Epoch 5453: Training Loss: 0.18598556021849313 Validation Loss: 0.7315925359725952\n",
      "Epoch 5454: Training Loss: 0.18498112261295319 Validation Loss: 0.7316513657569885\n",
      "Epoch 5455: Training Loss: 0.18553946912288666 Validation Loss: 0.7314978837966919\n",
      "Epoch 5456: Training Loss: 0.18553411960601807 Validation Loss: 0.7319334745407104\n",
      "Epoch 5457: Training Loss: 0.18527529140313467 Validation Loss: 0.7323057055473328\n",
      "Epoch 5458: Training Loss: 0.18498868743578592 Validation Loss: 0.7325775623321533\n",
      "Epoch 5459: Training Loss: 0.18527686099211374 Validation Loss: 0.73296719789505\n",
      "Epoch 5460: Training Loss: 0.18567268053690592 Validation Loss: 0.7328420281410217\n",
      "Epoch 5461: Training Loss: 0.1852317601442337 Validation Loss: 0.7329373955726624\n",
      "Epoch 5462: Training Loss: 0.1851294537385305 Validation Loss: 0.7324958443641663\n",
      "Epoch 5463: Training Loss: 0.18476237853368124 Validation Loss: 0.7324799299240112\n",
      "Epoch 5464: Training Loss: 0.1851622313261032 Validation Loss: 0.7321524024009705\n",
      "Epoch 5465: Training Loss: 0.18519375224908194 Validation Loss: 0.7320466637611389\n",
      "Epoch 5466: Training Loss: 0.1850387454032898 Validation Loss: 0.7324938774108887\n",
      "Epoch 5467: Training Loss: 0.1845679779847463 Validation Loss: 0.7324145436286926\n",
      "Epoch 5468: Training Loss: 0.18561087052027384 Validation Loss: 0.7322708368301392\n",
      "Epoch 5469: Training Loss: 0.18494302034378052 Validation Loss: 0.7321488261222839\n",
      "Epoch 5470: Training Loss: 0.18488978842894235 Validation Loss: 0.7319657206535339\n",
      "Epoch 5471: Training Loss: 0.1850742350021998 Validation Loss: 0.7322053909301758\n",
      "Epoch 5472: Training Loss: 0.18537545204162598 Validation Loss: 0.7327526211738586\n",
      "Epoch 5473: Training Loss: 0.18529853721459708 Validation Loss: 0.732400119304657\n",
      "Epoch 5474: Training Loss: 0.18479950726032257 Validation Loss: 0.7328788042068481\n",
      "Epoch 5475: Training Loss: 0.18446069955825806 Validation Loss: 0.732695996761322\n",
      "Epoch 5476: Training Loss: 0.1850050538778305 Validation Loss: 0.7329323291778564\n",
      "Epoch 5477: Training Loss: 0.18448840578397116 Validation Loss: 0.7326930165290833\n",
      "Epoch 5478: Training Loss: 0.18547320862611136 Validation Loss: 0.7325702905654907\n",
      "Epoch 5479: Training Loss: 0.18451551596323648 Validation Loss: 0.7322320342063904\n",
      "Epoch 5480: Training Loss: 0.18447264532248178 Validation Loss: 0.7321221828460693\n",
      "Epoch 5481: Training Loss: 0.1845863809188207 Validation Loss: 0.7323546409606934\n",
      "Epoch 5482: Training Loss: 0.18440233170986176 Validation Loss: 0.7325050830841064\n",
      "Epoch 5483: Training Loss: 0.18491279582182565 Validation Loss: 0.7326621413230896\n",
      "Epoch 5484: Training Loss: 0.18410869936148325 Validation Loss: 0.7323970794677734\n",
      "Epoch 5485: Training Loss: 0.18432877461115518 Validation Loss: 0.7321573495864868\n",
      "Epoch 5486: Training Loss: 0.18462631603082022 Validation Loss: 0.7326095700263977\n",
      "Epoch 5487: Training Loss: 0.1847099761168162 Validation Loss: 0.732013463973999\n",
      "Epoch 5488: Training Loss: 0.18414398034413657 Validation Loss: 0.7319222092628479\n",
      "Epoch 5489: Training Loss: 0.18415615459283194 Validation Loss: 0.7323339581489563\n",
      "Epoch 5490: Training Loss: 0.18427807092666626 Validation Loss: 0.7324015498161316\n",
      "Epoch 5491: Training Loss: 0.18411166469256082 Validation Loss: 0.7327620387077332\n",
      "Epoch 5492: Training Loss: 0.18519223729769388 Validation Loss: 0.7319875359535217\n",
      "Epoch 5493: Training Loss: 0.18460769951343536 Validation Loss: 0.7320541739463806\n",
      "Epoch 5494: Training Loss: 0.1842132955789566 Validation Loss: 0.7319880723953247\n",
      "Epoch 5495: Training Loss: 0.1835091511408488 Validation Loss: 0.7321466207504272\n",
      "Epoch 5496: Training Loss: 0.1847501496473948 Validation Loss: 0.7318166494369507\n",
      "Epoch 5497: Training Loss: 0.18385076026121774 Validation Loss: 0.7318324446678162\n",
      "Epoch 5498: Training Loss: 0.18490242958068848 Validation Loss: 0.7321942448616028\n",
      "Epoch 5499: Training Loss: 0.18414142231146494 Validation Loss: 0.7326322197914124\n",
      "Epoch 5500: Training Loss: 0.18457707266012827 Validation Loss: 0.7324132919311523\n",
      "Epoch 5501: Training Loss: 0.1842257877190908 Validation Loss: 0.7324248552322388\n",
      "Epoch 5502: Training Loss: 0.18430466949939728 Validation Loss: 0.732164204120636\n",
      "Epoch 5503: Training Loss: 0.18369369705518088 Validation Loss: 0.7323305010795593\n",
      "Epoch 5504: Training Loss: 0.18388465543588003 Validation Loss: 0.7324696779251099\n",
      "Epoch 5505: Training Loss: 0.1837671995162964 Validation Loss: 0.7325050234794617\n",
      "Epoch 5506: Training Loss: 0.18368397653102875 Validation Loss: 0.7323135137557983\n",
      "Epoch 5507: Training Loss: 0.1836747725804647 Validation Loss: 0.732723593711853\n",
      "Epoch 5508: Training Loss: 0.18416168789068857 Validation Loss: 0.7327706217765808\n",
      "Epoch 5509: Training Loss: 0.18409660955270132 Validation Loss: 0.7328018546104431\n",
      "Epoch 5510: Training Loss: 0.18320404489835104 Validation Loss: 0.7323328256607056\n",
      "Epoch 5511: Training Loss: 0.18348214030265808 Validation Loss: 0.731673538684845\n",
      "Epoch 5512: Training Loss: 0.18364966412385306 Validation Loss: 0.7316408157348633\n",
      "Epoch 5513: Training Loss: 0.1840809832016627 Validation Loss: 0.7318514585494995\n",
      "Epoch 5514: Training Loss: 0.18341497083504996 Validation Loss: 0.7325666546821594\n",
      "Epoch 5515: Training Loss: 0.184642493724823 Validation Loss: 0.7331600785255432\n",
      "Epoch 5516: Training Loss: 0.1834530532360077 Validation Loss: 0.7330355048179626\n",
      "Epoch 5517: Training Loss: 0.183214470744133 Validation Loss: 0.7329944372177124\n",
      "Epoch 5518: Training Loss: 0.18368737399578094 Validation Loss: 0.7325524091720581\n",
      "Epoch 5519: Training Loss: 0.18325824042161307 Validation Loss: 0.7317964434623718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5520: Training Loss: 0.18407473961512247 Validation Loss: 0.7320055365562439\n",
      "Epoch 5521: Training Loss: 0.18325814108053842 Validation Loss: 0.732632577419281\n",
      "Epoch 5522: Training Loss: 0.18339663743972778 Validation Loss: 0.7329339981079102\n",
      "Epoch 5523: Training Loss: 0.18333353102207184 Validation Loss: 0.7323101758956909\n",
      "Epoch 5524: Training Loss: 0.18320485452810922 Validation Loss: 0.7321276664733887\n",
      "Epoch 5525: Training Loss: 0.18410291274388632 Validation Loss: 0.7325986623764038\n",
      "Epoch 5526: Training Loss: 0.18377295633157095 Validation Loss: 0.7321996688842773\n",
      "Epoch 5527: Training Loss: 0.1829860806465149 Validation Loss: 0.7329128384590149\n",
      "Epoch 5528: Training Loss: 0.18329580624898276 Validation Loss: 0.7335148453712463\n",
      "Epoch 5529: Training Loss: 0.1828181246916453 Validation Loss: 0.7332767844200134\n",
      "Epoch 5530: Training Loss: 0.18339539567629495 Validation Loss: 0.7329835295677185\n",
      "Epoch 5531: Training Loss: 0.18260245521863303 Validation Loss: 0.7322195172309875\n",
      "Epoch 5532: Training Loss: 0.18310676515102386 Validation Loss: 0.7320722937583923\n",
      "Epoch 5533: Training Loss: 0.18273263176282248 Validation Loss: 0.7323802709579468\n",
      "Epoch 5534: Training Loss: 0.18279958268006644 Validation Loss: 0.7325037121772766\n",
      "Epoch 5535: Training Loss: 0.18270188570022583 Validation Loss: 0.7326065897941589\n",
      "Epoch 5536: Training Loss: 0.18279410898685455 Validation Loss: 0.732691764831543\n",
      "Epoch 5537: Training Loss: 0.18321536481380463 Validation Loss: 0.7327020764350891\n",
      "Epoch 5538: Training Loss: 0.1835037519534429 Validation Loss: 0.7332512736320496\n",
      "Epoch 5539: Training Loss: 0.18234940866629282 Validation Loss: 0.7328757047653198\n",
      "Epoch 5540: Training Loss: 0.18290738264719644 Validation Loss: 0.7322327494621277\n",
      "Epoch 5541: Training Loss: 0.1823070595661799 Validation Loss: 0.7321739792823792\n",
      "Epoch 5542: Training Loss: 0.18253909548123678 Validation Loss: 0.7319867610931396\n",
      "Epoch 5543: Training Loss: 0.18230881790320078 Validation Loss: 0.7322818636894226\n",
      "Epoch 5544: Training Loss: 0.1821703960498174 Validation Loss: 0.7323289513587952\n",
      "Epoch 5545: Training Loss: 0.1823940227429072 Validation Loss: 0.7328090667724609\n",
      "Epoch 5546: Training Loss: 0.18203737338383993 Validation Loss: 0.7324253916740417\n",
      "Epoch 5547: Training Loss: 0.18225174148877463 Validation Loss: 0.7326780557632446\n",
      "Epoch 5548: Training Loss: 0.18245320518811545 Validation Loss: 0.7330204844474792\n",
      "Epoch 5549: Training Loss: 0.1834028015534083 Validation Loss: 0.732297956943512\n",
      "Epoch 5550: Training Loss: 0.18232111632823944 Validation Loss: 0.7328515648841858\n",
      "Epoch 5551: Training Loss: 0.1829148530960083 Validation Loss: 0.7325606942176819\n",
      "Epoch 5552: Training Loss: 0.18244953950246176 Validation Loss: 0.7327782511711121\n",
      "Epoch 5553: Training Loss: 0.1822537531455358 Validation Loss: 0.7325406074523926\n",
      "Epoch 5554: Training Loss: 0.18204957246780396 Validation Loss: 0.7319595813751221\n",
      "Epoch 5555: Training Loss: 0.18219841519991556 Validation Loss: 0.732309103012085\n",
      "Epoch 5556: Training Loss: 0.18233756721019745 Validation Loss: 0.7321780920028687\n",
      "Epoch 5557: Training Loss: 0.18200509746869406 Validation Loss: 0.7329388856887817\n",
      "Epoch 5558: Training Loss: 0.18187330663204193 Validation Loss: 0.7330919504165649\n",
      "Epoch 5559: Training Loss: 0.1822386085987091 Validation Loss: 0.7330007553100586\n",
      "Epoch 5560: Training Loss: 0.18193556368350983 Validation Loss: 0.7326070070266724\n",
      "Epoch 5561: Training Loss: 0.1818542778491974 Validation Loss: 0.7322425842285156\n",
      "Epoch 5562: Training Loss: 0.1819837143023809 Validation Loss: 0.7322102785110474\n",
      "Epoch 5563: Training Loss: 0.18167507151762644 Validation Loss: 0.7324617505073547\n",
      "Epoch 5564: Training Loss: 0.18159147600332895 Validation Loss: 0.732591450214386\n",
      "Epoch 5565: Training Loss: 0.18207893272240958 Validation Loss: 0.7324101328849792\n",
      "Epoch 5566: Training Loss: 0.1814934810002645 Validation Loss: 0.7319352626800537\n",
      "Epoch 5567: Training Loss: 0.181543231010437 Validation Loss: 0.7321398258209229\n",
      "Epoch 5568: Training Loss: 0.18174212674299875 Validation Loss: 0.7322579622268677\n",
      "Epoch 5569: Training Loss: 0.18163719276587167 Validation Loss: 0.7319282293319702\n",
      "Epoch 5570: Training Loss: 0.1810705711444219 Validation Loss: 0.7322292923927307\n",
      "Epoch 5571: Training Loss: 0.1814519315958023 Validation Loss: 0.732416033744812\n",
      "Epoch 5572: Training Loss: 0.18145852287610373 Validation Loss: 0.7320594787597656\n",
      "Epoch 5573: Training Loss: 0.18162192900975546 Validation Loss: 0.7320935726165771\n",
      "Epoch 5574: Training Loss: 0.18180419007937113 Validation Loss: 0.7323011159896851\n",
      "Epoch 5575: Training Loss: 0.18138772745927176 Validation Loss: 0.7319312691688538\n",
      "Epoch 5576: Training Loss: 0.18277267615000406 Validation Loss: 0.7320292592048645\n",
      "Epoch 5577: Training Loss: 0.18137904504934946 Validation Loss: 0.7319486141204834\n",
      "Epoch 5578: Training Loss: 0.18158101538817087 Validation Loss: 0.7321344614028931\n",
      "Epoch 5579: Training Loss: 0.1820900688568751 Validation Loss: 0.7321375608444214\n",
      "Epoch 5580: Training Loss: 0.18134101231892905 Validation Loss: 0.7325244545936584\n",
      "Epoch 5581: Training Loss: 0.18102267384529114 Validation Loss: 0.7327853441238403\n",
      "Epoch 5582: Training Loss: 0.18113707502683005 Validation Loss: 0.7331516742706299\n",
      "Epoch 5583: Training Loss: 0.18137418727080026 Validation Loss: 0.7330513000488281\n",
      "Epoch 5584: Training Loss: 0.18182955185572305 Validation Loss: 0.7330409288406372\n",
      "Epoch 5585: Training Loss: 0.18281449874242148 Validation Loss: 0.7329683899879456\n",
      "Epoch 5586: Training Loss: 0.18132521212100983 Validation Loss: 0.732284665107727\n",
      "Epoch 5587: Training Loss: 0.18092255791028342 Validation Loss: 0.73224937915802\n",
      "Epoch 5588: Training Loss: 0.18142309288183847 Validation Loss: 0.7319801449775696\n",
      "Epoch 5589: Training Loss: 0.1806871642669042 Validation Loss: 0.7321703433990479\n",
      "Epoch 5590: Training Loss: 0.1816779524087906 Validation Loss: 0.732296347618103\n",
      "Epoch 5591: Training Loss: 0.1811447044213613 Validation Loss: 0.7319799065589905\n",
      "Epoch 5592: Training Loss: 0.1811134566863378 Validation Loss: 0.7320314645767212\n",
      "Epoch 5593: Training Loss: 0.18008626500765482 Validation Loss: 0.7324143052101135\n",
      "Epoch 5594: Training Loss: 0.18079487482706705 Validation Loss: 0.7322721481323242\n",
      "Epoch 5595: Training Loss: 0.18097750842571259 Validation Loss: 0.7326323986053467\n",
      "Epoch 5596: Training Loss: 0.18090191980202994 Validation Loss: 0.7328916788101196\n",
      "Epoch 5597: Training Loss: 0.18116173644860586 Validation Loss: 0.7332276701927185\n",
      "Epoch 5598: Training Loss: 0.18135998646418253 Validation Loss: 0.7329743504524231\n",
      "Epoch 5599: Training Loss: 0.18071861068407694 Validation Loss: 0.7328020334243774\n",
      "Epoch 5600: Training Loss: 0.18074139455954233 Validation Loss: 0.7324161529541016\n",
      "Epoch 5601: Training Loss: 0.1810652216275533 Validation Loss: 0.7326026558876038\n",
      "Epoch 5602: Training Loss: 0.1812178740898768 Validation Loss: 0.7326076030731201\n",
      "Epoch 5603: Training Loss: 0.1805251936117808 Validation Loss: 0.7323615550994873\n",
      "Epoch 5604: Training Loss: 0.18050625920295715 Validation Loss: 0.7324414253234863\n",
      "Epoch 5605: Training Loss: 0.18069238464037576 Validation Loss: 0.7324188351631165\n",
      "Epoch 5606: Training Loss: 0.18002918362617493 Validation Loss: 0.7325406074523926\n",
      "Epoch 5607: Training Loss: 0.18036630749702454 Validation Loss: 0.7330772280693054\n",
      "Epoch 5608: Training Loss: 0.18051764369010925 Validation Loss: 0.733395516872406\n",
      "Epoch 5609: Training Loss: 0.18041201929251352 Validation Loss: 0.733245849609375\n",
      "Epoch 5610: Training Loss: 0.18079178035259247 Validation Loss: 0.7327296137809753\n",
      "Epoch 5611: Training Loss: 0.18022784094015756 Validation Loss: 0.7324685454368591\n",
      "Epoch 5612: Training Loss: 0.17986897627512613 Validation Loss: 0.732235848903656\n",
      "Epoch 5613: Training Loss: 0.18024921913941702 Validation Loss: 0.7321403622627258\n",
      "Epoch 5614: Training Loss: 0.18031766513983408 Validation Loss: 0.732158899307251\n",
      "Epoch 5615: Training Loss: 0.18071307241916656 Validation Loss: 0.7320884466171265\n",
      "Epoch 5616: Training Loss: 0.18013806641101837 Validation Loss: 0.7321158647537231\n",
      "Epoch 5617: Training Loss: 0.1807290663321813 Validation Loss: 0.7319365739822388\n",
      "Epoch 5618: Training Loss: 0.1800089826186498 Validation Loss: 0.7319738268852234\n",
      "Epoch 5619: Training Loss: 0.18009772400061289 Validation Loss: 0.7322909832000732\n",
      "Epoch 5620: Training Loss: 0.1799783855676651 Validation Loss: 0.7323188185691833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5621: Training Loss: 0.18089736998081207 Validation Loss: 0.7325896620750427\n",
      "Epoch 5622: Training Loss: 0.17996793488661447 Validation Loss: 0.7329910397529602\n",
      "Epoch 5623: Training Loss: 0.1796237180630366 Validation Loss: 0.7327990531921387\n",
      "Epoch 5624: Training Loss: 0.1795965482791265 Validation Loss: 0.7330454587936401\n",
      "Epoch 5625: Training Loss: 0.1799811671177546 Validation Loss: 0.7326267957687378\n",
      "Epoch 5626: Training Loss: 0.17995051542917886 Validation Loss: 0.7327451109886169\n",
      "Epoch 5627: Training Loss: 0.17980040609836578 Validation Loss: 0.7323278784751892\n",
      "Epoch 5628: Training Loss: 0.18014225860436758 Validation Loss: 0.731765866279602\n",
      "Epoch 5629: Training Loss: 0.1799479971329371 Validation Loss: 0.7325491309165955\n",
      "Epoch 5630: Training Loss: 0.17967112362384796 Validation Loss: 0.7326533794403076\n",
      "Epoch 5631: Training Loss: 0.17963818709055582 Validation Loss: 0.7328283786773682\n",
      "Epoch 5632: Training Loss: 0.18057241042455038 Validation Loss: 0.7331477403640747\n",
      "Epoch 5633: Training Loss: 0.17993735273679098 Validation Loss: 0.7331929802894592\n",
      "Epoch 5634: Training Loss: 0.18044396241505942 Validation Loss: 0.7331199645996094\n",
      "Epoch 5635: Training Loss: 0.17993804812431335 Validation Loss: 0.7328799962997437\n",
      "Epoch 5636: Training Loss: 0.1796115388472875 Validation Loss: 0.7323839664459229\n",
      "Epoch 5637: Training Loss: 0.1795602192481359 Validation Loss: 0.7324433326721191\n",
      "Epoch 5638: Training Loss: 0.179709126551946 Validation Loss: 0.7324441075325012\n",
      "Epoch 5639: Training Loss: 0.17944000164667764 Validation Loss: 0.733090341091156\n",
      "Epoch 5640: Training Loss: 0.17943429946899414 Validation Loss: 0.7332510352134705\n",
      "Epoch 5641: Training Loss: 0.17907273769378662 Validation Loss: 0.7330310344696045\n",
      "Epoch 5642: Training Loss: 0.17941736181577048 Validation Loss: 0.732571005821228\n",
      "Epoch 5643: Training Loss: 0.1800766090552012 Validation Loss: 0.7324470281600952\n",
      "Epoch 5644: Training Loss: 0.1793541411558787 Validation Loss: 0.732500433921814\n",
      "Epoch 5645: Training Loss: 0.1797866920630137 Validation Loss: 0.7324627637863159\n",
      "Epoch 5646: Training Loss: 0.1788975844780604 Validation Loss: 0.733144998550415\n",
      "Epoch 5647: Training Loss: 0.1791115552186966 Validation Loss: 0.7323984503746033\n",
      "Epoch 5648: Training Loss: 0.1791202425956726 Validation Loss: 0.7318608164787292\n",
      "Epoch 5649: Training Loss: 0.17934810618559519 Validation Loss: 0.7321636080741882\n",
      "Epoch 5650: Training Loss: 0.1790610651175181 Validation Loss: 0.7326168417930603\n",
      "Epoch 5651: Training Loss: 0.17911561826864877 Validation Loss: 0.7327654957771301\n",
      "Epoch 5652: Training Loss: 0.17968509594599405 Validation Loss: 0.7325153350830078\n",
      "Epoch 5653: Training Loss: 0.17898442844549814 Validation Loss: 0.7325937747955322\n",
      "Epoch 5654: Training Loss: 0.17904406785964966 Validation Loss: 0.732578694820404\n",
      "Epoch 5655: Training Loss: 0.17895174026489258 Validation Loss: 0.7323045134544373\n",
      "Epoch 5656: Training Loss: 0.17887591322263083 Validation Loss: 0.7323410511016846\n",
      "Epoch 5657: Training Loss: 0.1791076958179474 Validation Loss: 0.7324538826942444\n",
      "Epoch 5658: Training Loss: 0.17892137666543326 Validation Loss: 0.7324066758155823\n",
      "Epoch 5659: Training Loss: 0.17902605732282004 Validation Loss: 0.7329285144805908\n",
      "Epoch 5660: Training Loss: 0.17969427009423575 Validation Loss: 0.7329367995262146\n",
      "Epoch 5661: Training Loss: 0.17908682922522226 Validation Loss: 0.7324844598770142\n",
      "Epoch 5662: Training Loss: 0.17873031397660574 Validation Loss: 0.7325299978256226\n",
      "Epoch 5663: Training Loss: 0.17886115610599518 Validation Loss: 0.7325735092163086\n",
      "Epoch 5664: Training Loss: 0.1787593960762024 Validation Loss: 0.7328179478645325\n",
      "Epoch 5665: Training Loss: 0.17871862153212228 Validation Loss: 0.7330099940299988\n",
      "Epoch 5666: Training Loss: 0.17893412709236145 Validation Loss: 0.732071042060852\n",
      "Epoch 5667: Training Loss: 0.17861582338809967 Validation Loss: 0.731829047203064\n",
      "Epoch 5668: Training Loss: 0.1788374731938044 Validation Loss: 0.7320041060447693\n",
      "Epoch 5669: Training Loss: 0.17941482365131378 Validation Loss: 0.7320087552070618\n",
      "Epoch 5670: Training Loss: 0.17883714536825815 Validation Loss: 0.7322126626968384\n",
      "Epoch 5671: Training Loss: 0.17873990535736084 Validation Loss: 0.7324689626693726\n",
      "Epoch 5672: Training Loss: 0.17846504350503287 Validation Loss: 0.7328007221221924\n",
      "Epoch 5673: Training Loss: 0.17921769122282663 Validation Loss: 0.7331056594848633\n",
      "Epoch 5674: Training Loss: 0.17885575691858926 Validation Loss: 0.7335276007652283\n",
      "Epoch 5675: Training Loss: 0.17928738395373026 Validation Loss: 0.7332888841629028\n",
      "Epoch 5676: Training Loss: 0.17837866644064584 Validation Loss: 0.733167827129364\n",
      "Epoch 5677: Training Loss: 0.17874081432819366 Validation Loss: 0.7330611348152161\n",
      "Epoch 5678: Training Loss: 0.17839261889457703 Validation Loss: 0.7327898740768433\n",
      "Epoch 5679: Training Loss: 0.17795739074548086 Validation Loss: 0.7326331734657288\n",
      "Epoch 5680: Training Loss: 0.17827831208705902 Validation Loss: 0.7323696613311768\n",
      "Epoch 5681: Training Loss: 0.1782321979602178 Validation Loss: 0.7325470447540283\n",
      "Epoch 5682: Training Loss: 0.1782665898402532 Validation Loss: 0.7326386570930481\n",
      "Epoch 5683: Training Loss: 0.17818809549013773 Validation Loss: 0.7324706315994263\n",
      "Epoch 5684: Training Loss: 0.17795743544896445 Validation Loss: 0.7323145270347595\n",
      "Epoch 5685: Training Loss: 0.17821252346038818 Validation Loss: 0.7330858111381531\n",
      "Epoch 5686: Training Loss: 0.17965300381183624 Validation Loss: 0.7329168915748596\n",
      "Epoch 5687: Training Loss: 0.1778480460246404 Validation Loss: 0.7329052686691284\n",
      "Epoch 5688: Training Loss: 0.1779324064652125 Validation Loss: 0.7325457334518433\n",
      "Epoch 5689: Training Loss: 0.17762638628482819 Validation Loss: 0.7326611876487732\n",
      "Epoch 5690: Training Loss: 0.17842777570088705 Validation Loss: 0.7327547669410706\n",
      "Epoch 5691: Training Loss: 0.1778425325949987 Validation Loss: 0.7332301139831543\n",
      "Epoch 5692: Training Loss: 0.17801900207996368 Validation Loss: 0.7330577969551086\n",
      "Epoch 5693: Training Loss: 0.1775063673655192 Validation Loss: 0.7327508926391602\n",
      "Epoch 5694: Training Loss: 0.17796879013379416 Validation Loss: 0.7328320741653442\n",
      "Epoch 5695: Training Loss: 0.17757613956928253 Validation Loss: 0.7328574657440186\n",
      "Epoch 5696: Training Loss: 0.17808676759401956 Validation Loss: 0.7325437664985657\n",
      "Epoch 5697: Training Loss: 0.17768718302249908 Validation Loss: 0.7324927449226379\n",
      "Epoch 5698: Training Loss: 0.17859283089637756 Validation Loss: 0.7324799299240112\n",
      "Epoch 5699: Training Loss: 0.17756353815396628 Validation Loss: 0.7321130037307739\n",
      "Epoch 5700: Training Loss: 0.17798430720965067 Validation Loss: 0.7318953275680542\n",
      "Epoch 5701: Training Loss: 0.17788946628570557 Validation Loss: 0.7325371503829956\n",
      "Epoch 5702: Training Loss: 0.1774489482243856 Validation Loss: 0.7323166131973267\n",
      "Epoch 5703: Training Loss: 0.1771644651889801 Validation Loss: 0.7328062653541565\n",
      "Epoch 5704: Training Loss: 0.17754699289798737 Validation Loss: 0.7323842644691467\n",
      "Epoch 5705: Training Loss: 0.17845656971136728 Validation Loss: 0.7323464155197144\n",
      "Epoch 5706: Training Loss: 0.17741029957930246 Validation Loss: 0.7324299216270447\n",
      "Epoch 5707: Training Loss: 0.1772640993197759 Validation Loss: 0.7326940894126892\n",
      "Epoch 5708: Training Loss: 0.1777320553859075 Validation Loss: 0.7332190275192261\n",
      "Epoch 5709: Training Loss: 0.1775796115398407 Validation Loss: 0.7337627410888672\n",
      "Epoch 5710: Training Loss: 0.17674734691778818 Validation Loss: 0.7329263091087341\n",
      "Epoch 5711: Training Loss: 0.1772450009981791 Validation Loss: 0.7327948212623596\n",
      "Epoch 5712: Training Loss: 0.17726166049639383 Validation Loss: 0.7327463030815125\n",
      "Epoch 5713: Training Loss: 0.17735985418160757 Validation Loss: 0.7325272560119629\n",
      "Epoch 5714: Training Loss: 0.17707758148511252 Validation Loss: 0.7325445413589478\n",
      "Epoch 5715: Training Loss: 0.1778006305297216 Validation Loss: 0.7330425977706909\n",
      "Epoch 5716: Training Loss: 0.17708349724610647 Validation Loss: 0.7329141497612\n",
      "Epoch 5717: Training Loss: 0.17718119422594705 Validation Loss: 0.732663631439209\n",
      "Epoch 5718: Training Loss: 0.1772714157899221 Validation Loss: 0.7323720455169678\n",
      "Epoch 5719: Training Loss: 0.17702469726403555 Validation Loss: 0.732770562171936\n",
      "Epoch 5720: Training Loss: 0.17726828654607138 Validation Loss: 0.7324621677398682\n",
      "Epoch 5721: Training Loss: 0.17696835597356161 Validation Loss: 0.7325007915496826\n",
      "Epoch 5722: Training Loss: 0.17686590552330017 Validation Loss: 0.7321817278862\n",
      "Epoch 5723: Training Loss: 0.17684535185496011 Validation Loss: 0.7326558232307434\n",
      "Epoch 5724: Training Loss: 0.177580455938975 Validation Loss: 0.732211709022522\n",
      "Epoch 5725: Training Loss: 0.17726798355579376 Validation Loss: 0.7325888872146606\n",
      "Epoch 5726: Training Loss: 0.17679857710997263 Validation Loss: 0.7330820560455322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5727: Training Loss: 0.17729362845420837 Validation Loss: 0.7331226468086243\n",
      "Epoch 5728: Training Loss: 0.17721475660800934 Validation Loss: 0.7326958179473877\n",
      "Epoch 5729: Training Loss: 0.17706630130608877 Validation Loss: 0.7324676513671875\n",
      "Epoch 5730: Training Loss: 0.17719674607117972 Validation Loss: 0.732109010219574\n",
      "Epoch 5731: Training Loss: 0.17661996682484946 Validation Loss: 0.7320854663848877\n",
      "Epoch 5732: Training Loss: 0.17684831221898398 Validation Loss: 0.7322989702224731\n",
      "Epoch 5733: Training Loss: 0.17649749418099722 Validation Loss: 0.7330590486526489\n",
      "Epoch 5734: Training Loss: 0.17656944195429483 Validation Loss: 0.7334734201431274\n",
      "Epoch 5735: Training Loss: 0.1780238002538681 Validation Loss: 0.7336187958717346\n",
      "Epoch 5736: Training Loss: 0.1765639136234919 Validation Loss: 0.7333036065101624\n",
      "Epoch 5737: Training Loss: 0.17657587925593057 Validation Loss: 0.7330624461174011\n",
      "Epoch 5738: Training Loss: 0.17666696508725485 Validation Loss: 0.733252763748169\n",
      "Epoch 5739: Training Loss: 0.1764214684565862 Validation Loss: 0.7330458164215088\n",
      "Epoch 5740: Training Loss: 0.1766111652056376 Validation Loss: 0.732448160648346\n",
      "Epoch 5741: Training Loss: 0.17638239761193594 Validation Loss: 0.7329124212265015\n",
      "Epoch 5742: Training Loss: 0.17646955450375876 Validation Loss: 0.7323880791664124\n",
      "Epoch 5743: Training Loss: 0.1765502393245697 Validation Loss: 0.732399046421051\n",
      "Epoch 5744: Training Loss: 0.1762197514375051 Validation Loss: 0.7324907183647156\n",
      "Epoch 5745: Training Loss: 0.1765134185552597 Validation Loss: 0.7320964336395264\n",
      "Epoch 5746: Training Loss: 0.17625954250494638 Validation Loss: 0.7329698204994202\n",
      "Epoch 5747: Training Loss: 0.17662466069062552 Validation Loss: 0.7331644296646118\n",
      "Epoch 5748: Training Loss: 0.17614260812600455 Validation Loss: 0.7332221269607544\n",
      "Epoch 5749: Training Loss: 0.1760898381471634 Validation Loss: 0.7330193519592285\n",
      "Epoch 5750: Training Loss: 0.17604103684425354 Validation Loss: 0.7326061129570007\n",
      "Epoch 5751: Training Loss: 0.17647111415863037 Validation Loss: 0.7328537702560425\n",
      "Epoch 5752: Training Loss: 0.17609853545824686 Validation Loss: 0.7329902052879333\n",
      "Epoch 5753: Training Loss: 0.17587235073248544 Validation Loss: 0.7326847314834595\n",
      "Epoch 5754: Training Loss: 0.1760583370923996 Validation Loss: 0.732793927192688\n",
      "Epoch 5755: Training Loss: 0.17655258377393088 Validation Loss: 0.7323356866836548\n",
      "Epoch 5756: Training Loss: 0.1760403166214625 Validation Loss: 0.7323794960975647\n",
      "Epoch 5757: Training Loss: 0.176017959912618 Validation Loss: 0.7330726981163025\n",
      "Epoch 5758: Training Loss: 0.17564910650253296 Validation Loss: 0.7330034971237183\n",
      "Epoch 5759: Training Loss: 0.17602324982484183 Validation Loss: 0.7330528497695923\n",
      "Epoch 5760: Training Loss: 0.17575382689634958 Validation Loss: 0.7331123948097229\n",
      "Epoch 5761: Training Loss: 0.17614753047625223 Validation Loss: 0.7329269647598267\n",
      "Epoch 5762: Training Loss: 0.1756777216990789 Validation Loss: 0.7332670092582703\n",
      "Epoch 5763: Training Loss: 0.1758890599012375 Validation Loss: 0.7330387830734253\n",
      "Epoch 5764: Training Loss: 0.1756956030925115 Validation Loss: 0.7329221963882446\n",
      "Epoch 5765: Training Loss: 0.17573032776514688 Validation Loss: 0.7323660254478455\n",
      "Epoch 5766: Training Loss: 0.17578061918417612 Validation Loss: 0.732300877571106\n",
      "Epoch 5767: Training Loss: 0.17581679920355478 Validation Loss: 0.7329480051994324\n",
      "Epoch 5768: Training Loss: 0.17559567093849182 Validation Loss: 0.7329761385917664\n",
      "Epoch 5769: Training Loss: 0.17550861338774362 Validation Loss: 0.7329509258270264\n",
      "Epoch 5770: Training Loss: 0.17675120135148367 Validation Loss: 0.7327768802642822\n",
      "Epoch 5771: Training Loss: 0.17545981208483377 Validation Loss: 0.7328321933746338\n",
      "Epoch 5772: Training Loss: 0.17533956468105316 Validation Loss: 0.732548713684082\n",
      "Epoch 5773: Training Loss: 0.17566779752572378 Validation Loss: 0.7330772280693054\n",
      "Epoch 5774: Training Loss: 0.1751161515712738 Validation Loss: 0.7334021329879761\n",
      "Epoch 5775: Training Loss: 0.1750824898481369 Validation Loss: 0.7331444621086121\n",
      "Epoch 5776: Training Loss: 0.17537339528401694 Validation Loss: 0.7333629131317139\n",
      "Epoch 5777: Training Loss: 0.17526173094908395 Validation Loss: 0.7333979606628418\n",
      "Epoch 5778: Training Loss: 0.17684194445610046 Validation Loss: 0.7331047058105469\n",
      "Epoch 5779: Training Loss: 0.17561155557632446 Validation Loss: 0.7326160669326782\n",
      "Epoch 5780: Training Loss: 0.17508972187836966 Validation Loss: 0.7326936721801758\n",
      "Epoch 5781: Training Loss: 0.17522872487703958 Validation Loss: 0.7327697277069092\n",
      "Epoch 5782: Training Loss: 0.1751340130964915 Validation Loss: 0.7323804497718811\n",
      "Epoch 5783: Training Loss: 0.17524259785811105 Validation Loss: 0.732498288154602\n",
      "Epoch 5784: Training Loss: 0.1750380595525106 Validation Loss: 0.7328948378562927\n",
      "Epoch 5785: Training Loss: 0.17555063466231027 Validation Loss: 0.7328433990478516\n",
      "Epoch 5786: Training Loss: 0.17511569956938425 Validation Loss: 0.7328472137451172\n",
      "Epoch 5787: Training Loss: 0.1749231517314911 Validation Loss: 0.7331085205078125\n",
      "Epoch 5788: Training Loss: 0.1749995251496633 Validation Loss: 0.7334384918212891\n",
      "Epoch 5789: Training Loss: 0.17486026883125305 Validation Loss: 0.7334542274475098\n",
      "Epoch 5790: Training Loss: 0.17570067942142487 Validation Loss: 0.7335420250892639\n",
      "Epoch 5791: Training Loss: 0.1751439869403839 Validation Loss: 0.7332184910774231\n",
      "Epoch 5792: Training Loss: 0.17478516201178232 Validation Loss: 0.7332848906517029\n",
      "Epoch 5793: Training Loss: 0.1748667707045873 Validation Loss: 0.7330629825592041\n",
      "Epoch 5794: Training Loss: 0.17455867429574332 Validation Loss: 0.7331818342208862\n",
      "Epoch 5795: Training Loss: 0.17487331728140512 Validation Loss: 0.7331083416938782\n",
      "Epoch 5796: Training Loss: 0.17480766773223877 Validation Loss: 0.7328867316246033\n",
      "Epoch 5797: Training Loss: 0.1750728835662206 Validation Loss: 0.7328740358352661\n",
      "Epoch 5798: Training Loss: 0.17488956451416016 Validation Loss: 0.7330890893936157\n",
      "Epoch 5799: Training Loss: 0.17574473222096762 Validation Loss: 0.732814371585846\n",
      "Epoch 5800: Training Loss: 0.17459197839101157 Validation Loss: 0.7328894734382629\n",
      "Epoch 5801: Training Loss: 0.17481213808059692 Validation Loss: 0.7328811287879944\n",
      "Epoch 5802: Training Loss: 0.17463531096776327 Validation Loss: 0.7319912314414978\n",
      "Epoch 5803: Training Loss: 0.17519220213095346 Validation Loss: 0.7323915362358093\n",
      "Epoch 5804: Training Loss: 0.17473673820495605 Validation Loss: 0.7326392531394958\n",
      "Epoch 5805: Training Loss: 0.17483300964037576 Validation Loss: 0.7325968146324158\n",
      "Epoch 5806: Training Loss: 0.17459740738073984 Validation Loss: 0.731935977935791\n",
      "Epoch 5807: Training Loss: 0.17435950537522635 Validation Loss: 0.7322404384613037\n",
      "Epoch 5808: Training Loss: 0.17539425194263458 Validation Loss: 0.7330424785614014\n",
      "Epoch 5809: Training Loss: 0.17495139439900717 Validation Loss: 0.7335860729217529\n",
      "Epoch 5810: Training Loss: 0.17440817753473917 Validation Loss: 0.7340846657752991\n",
      "Epoch 5811: Training Loss: 0.17436903218428293 Validation Loss: 0.7333179712295532\n",
      "Epoch 5812: Training Loss: 0.17424528300762177 Validation Loss: 0.7329916954040527\n",
      "Epoch 5813: Training Loss: 0.17509310940901437 Validation Loss: 0.732909083366394\n",
      "Epoch 5814: Training Loss: 0.17468215028444925 Validation Loss: 0.732749879360199\n",
      "Epoch 5815: Training Loss: 0.17437157034873962 Validation Loss: 0.7326966524124146\n",
      "Epoch 5816: Training Loss: 0.17396149535973868 Validation Loss: 0.7330178022384644\n",
      "Epoch 5817: Training Loss: 0.17407900094985962 Validation Loss: 0.7334394454956055\n",
      "Epoch 5818: Training Loss: 0.17421136299769083 Validation Loss: 0.733410656452179\n",
      "Epoch 5819: Training Loss: 0.17405961950620016 Validation Loss: 0.7334248423576355\n",
      "Epoch 5820: Training Loss: 0.17384342352549234 Validation Loss: 0.7328618168830872\n",
      "Epoch 5821: Training Loss: 0.17447478075822195 Validation Loss: 0.7326595187187195\n",
      "Epoch 5822: Training Loss: 0.17535220583279928 Validation Loss: 0.7324815988540649\n",
      "Epoch 5823: Training Loss: 0.17351282139619192 Validation Loss: 0.7332101464271545\n",
      "Epoch 5824: Training Loss: 0.17463973661263785 Validation Loss: 0.7333569526672363\n",
      "Epoch 5825: Training Loss: 0.17387505372365317 Validation Loss: 0.7333448529243469\n",
      "Epoch 5826: Training Loss: 0.17376949389775595 Validation Loss: 0.7335847020149231\n",
      "Epoch 5827: Training Loss: 0.17396549880504608 Validation Loss: 0.7334750294685364\n",
      "Epoch 5828: Training Loss: 0.17394015192985535 Validation Loss: 0.7331030964851379\n",
      "Epoch 5829: Training Loss: 0.17364738881587982 Validation Loss: 0.7330150008201599\n",
      "Epoch 5830: Training Loss: 0.17401021222273508 Validation Loss: 0.7334318161010742\n",
      "Epoch 5831: Training Loss: 0.17405652006467184 Validation Loss: 0.7337113618850708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5832: Training Loss: 0.17389002442359924 Validation Loss: 0.7331962585449219\n",
      "Epoch 5833: Training Loss: 0.17360243201255798 Validation Loss: 0.7332305312156677\n",
      "Epoch 5834: Training Loss: 0.1738042285044988 Validation Loss: 0.7328693866729736\n",
      "Epoch 5835: Training Loss: 0.17360788583755493 Validation Loss: 0.7332261204719543\n",
      "Epoch 5836: Training Loss: 0.17372978230317435 Validation Loss: 0.732833981513977\n",
      "Epoch 5837: Training Loss: 0.17391290267308554 Validation Loss: 0.7326300740242004\n",
      "Epoch 5838: Training Loss: 0.17367875079313913 Validation Loss: 0.7333388924598694\n",
      "Epoch 5839: Training Loss: 0.1734884331623713 Validation Loss: 0.7337855696678162\n",
      "Epoch 5840: Training Loss: 0.17412093778451285 Validation Loss: 0.7335529327392578\n",
      "Epoch 5841: Training Loss: 0.17342381179332733 Validation Loss: 0.7330355644226074\n",
      "Epoch 5842: Training Loss: 0.1734848916530609 Validation Loss: 0.7325027585029602\n",
      "Epoch 5843: Training Loss: 0.1734125018119812 Validation Loss: 0.7321255803108215\n",
      "Epoch 5844: Training Loss: 0.1732780933380127 Validation Loss: 0.7328779101371765\n",
      "Epoch 5845: Training Loss: 0.1733374446630478 Validation Loss: 0.7334787249565125\n",
      "Epoch 5846: Training Loss: 0.17341957489649454 Validation Loss: 0.733415961265564\n",
      "Epoch 5847: Training Loss: 0.17325558761755624 Validation Loss: 0.7329095005989075\n",
      "Epoch 5848: Training Loss: 0.17381378511587778 Validation Loss: 0.7333252429962158\n",
      "Epoch 5849: Training Loss: 0.17384865880012512 Validation Loss: 0.7330406904220581\n",
      "Epoch 5850: Training Loss: 0.1730376531680425 Validation Loss: 0.7329854369163513\n",
      "Epoch 5851: Training Loss: 0.17320978144804636 Validation Loss: 0.7330808043479919\n",
      "Epoch 5852: Training Loss: 0.17293247083822885 Validation Loss: 0.7329099774360657\n",
      "Epoch 5853: Training Loss: 0.17339768509070078 Validation Loss: 0.7326610088348389\n",
      "Epoch 5854: Training Loss: 0.17310655613740286 Validation Loss: 0.7337592840194702\n",
      "Epoch 5855: Training Loss: 0.17388112843036652 Validation Loss: 0.7341662049293518\n",
      "Epoch 5856: Training Loss: 0.17378869652748108 Validation Loss: 0.7338768243789673\n",
      "Epoch 5857: Training Loss: 0.17293577392896017 Validation Loss: 0.7333228588104248\n",
      "Epoch 5858: Training Loss: 0.17306924859682718 Validation Loss: 0.7325918674468994\n",
      "Epoch 5859: Training Loss: 0.17334500948588052 Validation Loss: 0.732579231262207\n",
      "Epoch 5860: Training Loss: 0.17260529100894928 Validation Loss: 0.7321319580078125\n",
      "Epoch 5861: Training Loss: 0.17354019979635874 Validation Loss: 0.7325423359870911\n",
      "Epoch 5862: Training Loss: 0.17389986415704092 Validation Loss: 0.7328494191169739\n",
      "Epoch 5863: Training Loss: 0.17300543189048767 Validation Loss: 0.7328786253929138\n",
      "Epoch 5864: Training Loss: 0.17316915094852448 Validation Loss: 0.7331868410110474\n",
      "Epoch 5865: Training Loss: 0.17232968409856161 Validation Loss: 0.7336328029632568\n",
      "Epoch 5866: Training Loss: 0.172890638311704 Validation Loss: 0.7334490418434143\n",
      "Epoch 5867: Training Loss: 0.17326596875985464 Validation Loss: 0.7336960434913635\n",
      "Epoch 5868: Training Loss: 0.1731200615564982 Validation Loss: 0.7335530519485474\n",
      "Epoch 5869: Training Loss: 0.17270801961421967 Validation Loss: 0.7334166169166565\n",
      "Epoch 5870: Training Loss: 0.17285209397474924 Validation Loss: 0.7332314252853394\n",
      "Epoch 5871: Training Loss: 0.17283226052920023 Validation Loss: 0.7325324416160583\n",
      "Epoch 5872: Training Loss: 0.17255265017350516 Validation Loss: 0.733016312122345\n",
      "Epoch 5873: Training Loss: 0.17227570712566376 Validation Loss: 0.7337169647216797\n",
      "Epoch 5874: Training Loss: 0.17205195625623068 Validation Loss: 0.7339866161346436\n",
      "Epoch 5875: Training Loss: 0.17249216636021933 Validation Loss: 0.7341301441192627\n",
      "Epoch 5876: Training Loss: 0.17261366546154022 Validation Loss: 0.7337083220481873\n",
      "Epoch 5877: Training Loss: 0.17260804772377014 Validation Loss: 0.7335033416748047\n",
      "Epoch 5878: Training Loss: 0.1724105030298233 Validation Loss: 0.7337213754653931\n",
      "Epoch 5879: Training Loss: 0.17317852874596915 Validation Loss: 0.7332072854042053\n",
      "Epoch 5880: Training Loss: 0.17324987053871155 Validation Loss: 0.7334995865821838\n",
      "Epoch 5881: Training Loss: 0.17236418525377908 Validation Loss: 0.7332524061203003\n",
      "Epoch 5882: Training Loss: 0.1723413666089376 Validation Loss: 0.7333637475967407\n",
      "Epoch 5883: Training Loss: 0.1723635494709015 Validation Loss: 0.7330341935157776\n",
      "Epoch 5884: Training Loss: 0.17261398335297903 Validation Loss: 0.7325085401535034\n",
      "Epoch 5885: Training Loss: 0.17305880784988403 Validation Loss: 0.7325040698051453\n",
      "Epoch 5886: Training Loss: 0.17245624959468842 Validation Loss: 0.7332074046134949\n",
      "Epoch 5887: Training Loss: 0.17211827139059702 Validation Loss: 0.7331483364105225\n",
      "Epoch 5888: Training Loss: 0.17241043349107107 Validation Loss: 0.7334091663360596\n",
      "Epoch 5889: Training Loss: 0.1728507528702418 Validation Loss: 0.7329955101013184\n",
      "Epoch 5890: Training Loss: 0.17205307881037393 Validation Loss: 0.7330761551856995\n",
      "Epoch 5891: Training Loss: 0.17249134679635367 Validation Loss: 0.733539342880249\n",
      "Epoch 5892: Training Loss: 0.17190886040528616 Validation Loss: 0.7335549592971802\n",
      "Epoch 5893: Training Loss: 0.17196470002333322 Validation Loss: 0.7327622771263123\n",
      "Epoch 5894: Training Loss: 0.17209229369958243 Validation Loss: 0.7334176301956177\n",
      "Epoch 5895: Training Loss: 0.17243064939975739 Validation Loss: 0.7336410880088806\n",
      "Epoch 5896: Training Loss: 0.1720917522907257 Validation Loss: 0.733272135257721\n",
      "Epoch 5897: Training Loss: 0.17140223582585654 Validation Loss: 0.7333506941795349\n",
      "Epoch 5898: Training Loss: 0.17196265359719595 Validation Loss: 0.7338821291923523\n",
      "Epoch 5899: Training Loss: 0.1715818097194036 Validation Loss: 0.7335184216499329\n",
      "Epoch 5900: Training Loss: 0.17185804744561514 Validation Loss: 0.7338783144950867\n",
      "Epoch 5901: Training Loss: 0.17225666344165802 Validation Loss: 0.7334516644477844\n",
      "Epoch 5902: Training Loss: 0.17176573475201926 Validation Loss: 0.7331143617630005\n",
      "Epoch 5903: Training Loss: 0.17177586257457733 Validation Loss: 0.7328599691390991\n",
      "Epoch 5904: Training Loss: 0.17222005128860474 Validation Loss: 0.7327315807342529\n",
      "Epoch 5905: Training Loss: 0.17141531904538473 Validation Loss: 0.7332913279533386\n",
      "Epoch 5906: Training Loss: 0.17198837796847025 Validation Loss: 0.7338318228721619\n",
      "Epoch 5907: Training Loss: 0.17172268529733023 Validation Loss: 0.734195351600647\n",
      "Epoch 5908: Training Loss: 0.17128677169481912 Validation Loss: 0.7341952919960022\n",
      "Epoch 5909: Training Loss: 0.17227527995904288 Validation Loss: 0.7341449856758118\n",
      "Epoch 5910: Training Loss: 0.1716895798842112 Validation Loss: 0.7336544394493103\n",
      "Epoch 5911: Training Loss: 0.17148414750893912 Validation Loss: 0.7340079545974731\n",
      "Epoch 5912: Training Loss: 0.17192800343036652 Validation Loss: 0.7337816953659058\n",
      "Epoch 5913: Training Loss: 0.17172016700108847 Validation Loss: 0.7343252897262573\n",
      "Epoch 5914: Training Loss: 0.17246720691521963 Validation Loss: 0.7341794371604919\n",
      "Epoch 5915: Training Loss: 0.17140128711859384 Validation Loss: 0.7333759665489197\n",
      "Epoch 5916: Training Loss: 0.17181515196959177 Validation Loss: 0.7333329916000366\n",
      "Epoch 5917: Training Loss: 0.17140008012453714 Validation Loss: 0.7331769466400146\n",
      "Epoch 5918: Training Loss: 0.171660378575325 Validation Loss: 0.7333882451057434\n",
      "Epoch 5919: Training Loss: 0.17119043072064719 Validation Loss: 0.733048677444458\n",
      "Epoch 5920: Training Loss: 0.1715101053317388 Validation Loss: 0.7328612804412842\n",
      "Epoch 5921: Training Loss: 0.1719900369644165 Validation Loss: 0.7332587838172913\n",
      "Epoch 5922: Training Loss: 0.172663946946462 Validation Loss: 0.733726441860199\n",
      "Epoch 5923: Training Loss: 0.17064273854096731 Validation Loss: 0.7335518002510071\n",
      "Epoch 5924: Training Loss: 0.17103034257888794 Validation Loss: 0.7330307364463806\n",
      "Epoch 5925: Training Loss: 0.17113934457302094 Validation Loss: 0.7329974174499512\n",
      "Epoch 5926: Training Loss: 0.17127448817094168 Validation Loss: 0.7333674430847168\n",
      "Epoch 5927: Training Loss: 0.17080138127009073 Validation Loss: 0.7341098785400391\n",
      "Epoch 5928: Training Loss: 0.17162257432937622 Validation Loss: 0.733860194683075\n",
      "Epoch 5929: Training Loss: 0.1710601548353831 Validation Loss: 0.7334415316581726\n",
      "Epoch 5930: Training Loss: 0.17207370201746622 Validation Loss: 0.7328214645385742\n",
      "Epoch 5931: Training Loss: 0.17105943461259207 Validation Loss: 0.7324523329734802\n",
      "Epoch 5932: Training Loss: 0.17133696873982748 Validation Loss: 0.7327144145965576\n",
      "Epoch 5933: Training Loss: 0.1707178751627604 Validation Loss: 0.733167827129364\n",
      "Epoch 5934: Training Loss: 0.17099978029727936 Validation Loss: 0.7334662079811096\n",
      "Epoch 5935: Training Loss: 0.17129139602184296 Validation Loss: 0.7331019043922424\n",
      "Epoch 5936: Training Loss: 0.17096429069836935 Validation Loss: 0.7336097955703735\n",
      "Epoch 5937: Training Loss: 0.1705257991949717 Validation Loss: 0.733792245388031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5938: Training Loss: 0.17128636439641318 Validation Loss: 0.7335553765296936\n",
      "Epoch 5939: Training Loss: 0.17096108694871268 Validation Loss: 0.7331194281578064\n",
      "Epoch 5940: Training Loss: 0.17089328169822693 Validation Loss: 0.7335659861564636\n",
      "Epoch 5941: Training Loss: 0.1707448959350586 Validation Loss: 0.7335141897201538\n",
      "Epoch 5942: Training Loss: 0.17088985443115234 Validation Loss: 0.7338396310806274\n",
      "Epoch 5943: Training Loss: 0.1709174265464147 Validation Loss: 0.7343046069145203\n",
      "Epoch 5944: Training Loss: 0.17074047525723776 Validation Loss: 0.7345922589302063\n",
      "Epoch 5945: Training Loss: 0.1711741437514623 Validation Loss: 0.7340092062950134\n",
      "Epoch 5946: Training Loss: 0.17009267210960388 Validation Loss: 0.7339376211166382\n",
      "Epoch 5947: Training Loss: 0.17038317521413168 Validation Loss: 0.7334703207015991\n",
      "Epoch 5948: Training Loss: 0.17056060334046683 Validation Loss: 0.7328875064849854\n",
      "Epoch 5949: Training Loss: 0.1707201898097992 Validation Loss: 0.7334898710250854\n",
      "Epoch 5950: Training Loss: 0.17039911448955536 Validation Loss: 0.7332490086555481\n",
      "Epoch 5951: Training Loss: 0.17046953241030374 Validation Loss: 0.7334603667259216\n",
      "Epoch 5952: Training Loss: 0.1707685043414434 Validation Loss: 0.733340859413147\n",
      "Epoch 5953: Training Loss: 0.17092000444730124 Validation Loss: 0.7333325147628784\n",
      "Epoch 5954: Training Loss: 0.17043523490428925 Validation Loss: 0.7334094047546387\n",
      "Epoch 5955: Training Loss: 0.17074749370416006 Validation Loss: 0.7341601848602295\n",
      "Epoch 5956: Training Loss: 0.1705400695403417 Validation Loss: 0.7345836758613586\n",
      "Epoch 5957: Training Loss: 0.17096654574076334 Validation Loss: 0.7348562479019165\n",
      "Epoch 5958: Training Loss: 0.17018591860930124 Validation Loss: 0.7343494296073914\n",
      "Epoch 5959: Training Loss: 0.17007293303807577 Validation Loss: 0.7336204051971436\n",
      "Epoch 5960: Training Loss: 0.1698051393032074 Validation Loss: 0.7335408926010132\n",
      "Epoch 5961: Training Loss: 0.17044119040171304 Validation Loss: 0.7331064343452454\n",
      "Epoch 5962: Training Loss: 0.17114178836345673 Validation Loss: 0.733439564704895\n",
      "Epoch 5963: Training Loss: 0.17038882275422415 Validation Loss: 0.7333785891532898\n",
      "Epoch 5964: Training Loss: 0.17012262841065726 Validation Loss: 0.7343319058418274\n",
      "Epoch 5965: Training Loss: 0.17073685924212137 Validation Loss: 0.7341095209121704\n",
      "Epoch 5966: Training Loss: 0.1699251929918925 Validation Loss: 0.7338916063308716\n",
      "Epoch 5967: Training Loss: 0.1712073584397634 Validation Loss: 0.7330619096755981\n",
      "Epoch 5968: Training Loss: 0.17006869117418924 Validation Loss: 0.733158528804779\n",
      "Epoch 5969: Training Loss: 0.1700353721777598 Validation Loss: 0.733410120010376\n",
      "Epoch 5970: Training Loss: 0.16978350281715393 Validation Loss: 0.7340812087059021\n",
      "Epoch 5971: Training Loss: 0.17007102568944296 Validation Loss: 0.734441876411438\n",
      "Epoch 5972: Training Loss: 0.16990795731544495 Validation Loss: 0.7347002625465393\n",
      "Epoch 5973: Training Loss: 0.16980335116386414 Validation Loss: 0.7337902188301086\n",
      "Epoch 5974: Training Loss: 0.1707193354765574 Validation Loss: 0.7338236570358276\n",
      "Epoch 5975: Training Loss: 0.16976902882258096 Validation Loss: 0.7335190176963806\n",
      "Epoch 5976: Training Loss: 0.16966234147548676 Validation Loss: 0.7329574823379517\n",
      "Epoch 5977: Training Loss: 0.16977002223332724 Validation Loss: 0.7330960035324097\n",
      "Epoch 5978: Training Loss: 0.17045005162556967 Validation Loss: 0.7332789301872253\n",
      "Epoch 5979: Training Loss: 0.17164524892965952 Validation Loss: 0.7336721420288086\n",
      "Epoch 5980: Training Loss: 0.16956785321235657 Validation Loss: 0.7344182133674622\n",
      "Epoch 5981: Training Loss: 0.16992316643397012 Validation Loss: 0.7345579266548157\n",
      "Epoch 5982: Training Loss: 0.1695231298605601 Validation Loss: 0.7341967821121216\n",
      "Epoch 5983: Training Loss: 0.17012891173362732 Validation Loss: 0.7343381643295288\n",
      "Epoch 5984: Training Loss: 0.16967944304148355 Validation Loss: 0.7336956858634949\n",
      "Epoch 5985: Training Loss: 0.16987737516562143 Validation Loss: 0.7336350083351135\n",
      "Epoch 5986: Training Loss: 0.1696175535519918 Validation Loss: 0.7334429621696472\n",
      "Epoch 5987: Training Loss: 0.1697006622950236 Validation Loss: 0.7335038781166077\n",
      "Epoch 5988: Training Loss: 0.16908366978168488 Validation Loss: 0.7335058450698853\n",
      "Epoch 5989: Training Loss: 0.1695287525653839 Validation Loss: 0.7339920997619629\n",
      "Epoch 5990: Training Loss: 0.17003695666790009 Validation Loss: 0.733374297618866\n",
      "Epoch 5991: Training Loss: 0.16986398895581564 Validation Loss: 0.7338721752166748\n",
      "Epoch 5992: Training Loss: 0.1693412959575653 Validation Loss: 0.7340580821037292\n",
      "Epoch 5993: Training Loss: 0.16962511340777078 Validation Loss: 0.733396589756012\n",
      "Epoch 5994: Training Loss: 0.16920973857243857 Validation Loss: 0.7335249781608582\n",
      "Epoch 5995: Training Loss: 0.16922524571418762 Validation Loss: 0.733676016330719\n",
      "Epoch 5996: Training Loss: 0.1692830870548884 Validation Loss: 0.7337395548820496\n",
      "Epoch 5997: Training Loss: 0.16932919124762216 Validation Loss: 0.7337159514427185\n",
      "Epoch 5998: Training Loss: 0.1691843867301941 Validation Loss: 0.7339074611663818\n",
      "Epoch 5999: Training Loss: 0.16888932883739471 Validation Loss: 0.7342685461044312\n",
      "Epoch 6000: Training Loss: 0.16926236947377524 Validation Loss: 0.733944833278656\n",
      "Epoch 6001: Training Loss: 0.16959795355796814 Validation Loss: 0.7342286109924316\n",
      "Epoch 6002: Training Loss: 0.168874720732371 Validation Loss: 0.7338371276855469\n",
      "Epoch 6003: Training Loss: 0.16903901100158691 Validation Loss: 0.7338530421257019\n",
      "Epoch 6004: Training Loss: 0.16940589745839438 Validation Loss: 0.7332878708839417\n",
      "Epoch 6005: Training Loss: 0.1689437280098597 Validation Loss: 0.7331874966621399\n",
      "Epoch 6006: Training Loss: 0.1690941552321116 Validation Loss: 0.7341758012771606\n",
      "Epoch 6007: Training Loss: 0.1690481255451838 Validation Loss: 0.7341582179069519\n",
      "Epoch 6008: Training Loss: 0.16872263451417288 Validation Loss: 0.7338448166847229\n",
      "Epoch 6009: Training Loss: 0.1687526454528173 Validation Loss: 0.7337998151779175\n",
      "Epoch 6010: Training Loss: 0.16871785620848337 Validation Loss: 0.7339746952056885\n",
      "Epoch 6011: Training Loss: 0.1694657305876414 Validation Loss: 0.7335931658744812\n",
      "Epoch 6012: Training Loss: 0.1685811181863149 Validation Loss: 0.7337643504142761\n",
      "Epoch 6013: Training Loss: 0.16844859719276428 Validation Loss: 0.733607828617096\n",
      "Epoch 6014: Training Loss: 0.16855678955713907 Validation Loss: 0.7338536977767944\n",
      "Epoch 6015: Training Loss: 0.16871069371700287 Validation Loss: 0.7339001297950745\n",
      "Epoch 6016: Training Loss: 0.1685172567764918 Validation Loss: 0.733894944190979\n",
      "Epoch 6017: Training Loss: 0.16863513986269632 Validation Loss: 0.7338111400604248\n",
      "Epoch 6018: Training Loss: 0.1687436451514562 Validation Loss: 0.7336259484291077\n",
      "Epoch 6019: Training Loss: 0.16872782508532205 Validation Loss: 0.7336761951446533\n",
      "Epoch 6020: Training Loss: 0.16857145229975382 Validation Loss: 0.7334088683128357\n",
      "Epoch 6021: Training Loss: 0.16848361988862356 Validation Loss: 0.7333555817604065\n",
      "Epoch 6022: Training Loss: 0.1689351201057434 Validation Loss: 0.7339085936546326\n",
      "Epoch 6023: Training Loss: 0.16907881697018942 Validation Loss: 0.7340137362480164\n",
      "Epoch 6024: Training Loss: 0.1683341364065806 Validation Loss: 0.7337579727172852\n",
      "Epoch 6025: Training Loss: 0.1692990263303121 Validation Loss: 0.7334122657775879\n",
      "Epoch 6026: Training Loss: 0.16838722924391428 Validation Loss: 0.733638346195221\n",
      "Epoch 6027: Training Loss: 0.16861654818058014 Validation Loss: 0.734213650226593\n",
      "Epoch 6028: Training Loss: 0.16854025423526764 Validation Loss: 0.7335978150367737\n",
      "Epoch 6029: Training Loss: 0.1686436931292216 Validation Loss: 0.7342807054519653\n",
      "Epoch 6030: Training Loss: 0.16866160929203033 Validation Loss: 0.7342561483383179\n",
      "Epoch 6031: Training Loss: 0.16899987558523813 Validation Loss: 0.7341647744178772\n",
      "Epoch 6032: Training Loss: 0.16859017809232077 Validation Loss: 0.7344115376472473\n",
      "Epoch 6033: Training Loss: 0.1684278150399526 Validation Loss: 0.7337296605110168\n",
      "Epoch 6034: Training Loss: 0.16825181742509207 Validation Loss: 0.7340810894966125\n",
      "Epoch 6035: Training Loss: 0.16814981897672018 Validation Loss: 0.7339632511138916\n",
      "Epoch 6036: Training Loss: 0.16826953987280527 Validation Loss: 0.7336516976356506\n",
      "Epoch 6037: Training Loss: 0.16774788002173105 Validation Loss: 0.7334643006324768\n",
      "Epoch 6038: Training Loss: 0.16803350547949472 Validation Loss: 0.7332825660705566\n",
      "Epoch 6039: Training Loss: 0.1677106867233912 Validation Loss: 0.7338044047355652\n",
      "Epoch 6040: Training Loss: 0.16827146212259927 Validation Loss: 0.7338017821311951\n",
      "Epoch 6041: Training Loss: 0.16951046884059906 Validation Loss: 0.7339807152748108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6042: Training Loss: 0.16860741873582205 Validation Loss: 0.7337554693222046\n",
      "Epoch 6043: Training Loss: 0.16844371457894644 Validation Loss: 0.7342327237129211\n",
      "Epoch 6044: Training Loss: 0.167711873849233 Validation Loss: 0.7345694899559021\n",
      "Epoch 6045: Training Loss: 0.16814896961053213 Validation Loss: 0.7348760366439819\n",
      "Epoch 6046: Training Loss: 0.1679121752580007 Validation Loss: 0.734517514705658\n",
      "Epoch 6047: Training Loss: 0.16792052487532297 Validation Loss: 0.7346292734146118\n",
      "Epoch 6048: Training Loss: 0.16860905786355337 Validation Loss: 0.734463095664978\n",
      "Epoch 6049: Training Loss: 0.16797293722629547 Validation Loss: 0.7339931130409241\n",
      "Epoch 6050: Training Loss: 0.16787348687648773 Validation Loss: 0.733703076839447\n",
      "Epoch 6051: Training Loss: 0.16885976493358612 Validation Loss: 0.7335529923439026\n",
      "Epoch 6052: Training Loss: 0.1678197681903839 Validation Loss: 0.7336939573287964\n",
      "Epoch 6053: Training Loss: 0.16829311350981394 Validation Loss: 0.7341851592063904\n",
      "Epoch 6054: Training Loss: 0.16725625594456991 Validation Loss: 0.733963131904602\n",
      "Epoch 6055: Training Loss: 0.16758789122104645 Validation Loss: 0.7337899804115295\n",
      "Epoch 6056: Training Loss: 0.16759290794531503 Validation Loss: 0.7334533333778381\n",
      "Epoch 6057: Training Loss: 0.16869870324929556 Validation Loss: 0.734083890914917\n",
      "Epoch 6058: Training Loss: 0.1677060623963674 Validation Loss: 0.7342849969863892\n",
      "Epoch 6059: Training Loss: 0.16752740244070688 Validation Loss: 0.734083354473114\n",
      "Epoch 6060: Training Loss: 0.16776336232821146 Validation Loss: 0.7338301539421082\n",
      "Epoch 6061: Training Loss: 0.16757884124914804 Validation Loss: 0.7336755394935608\n",
      "Epoch 6062: Training Loss: 0.16738104820251465 Validation Loss: 0.7338405251502991\n",
      "Epoch 6063: Training Loss: 0.16751422981421152 Validation Loss: 0.7333511114120483\n",
      "Epoch 6064: Training Loss: 0.16783597568670908 Validation Loss: 0.7341455817222595\n",
      "Epoch 6065: Training Loss: 0.1674260546763738 Validation Loss: 0.7343058586120605\n",
      "Epoch 6066: Training Loss: 0.16787261267503104 Validation Loss: 0.7341471314430237\n",
      "Epoch 6067: Training Loss: 0.16742856800556183 Validation Loss: 0.733890950679779\n",
      "Epoch 6068: Training Loss: 0.16731101274490356 Validation Loss: 0.7342636585235596\n",
      "Epoch 6069: Training Loss: 0.16741910080115 Validation Loss: 0.7336755394935608\n",
      "Epoch 6070: Training Loss: 0.16732807457447052 Validation Loss: 0.7335678339004517\n",
      "Epoch 6071: Training Loss: 0.16716026266415915 Validation Loss: 0.7337851524353027\n",
      "Epoch 6072: Training Loss: 0.16769403219223022 Validation Loss: 0.7337522506713867\n",
      "Epoch 6073: Training Loss: 0.16727740069230398 Validation Loss: 0.7337356209754944\n",
      "Epoch 6074: Training Loss: 0.1677141636610031 Validation Loss: 0.733862042427063\n",
      "Epoch 6075: Training Loss: 0.1667321672042211 Validation Loss: 0.7341642379760742\n",
      "Epoch 6076: Training Loss: 0.1668456494808197 Validation Loss: 0.7341855764389038\n",
      "Epoch 6077: Training Loss: 0.16672841211160025 Validation Loss: 0.7348157167434692\n",
      "Epoch 6078: Training Loss: 0.1670283724864324 Validation Loss: 0.7348698377609253\n",
      "Epoch 6079: Training Loss: 0.16771793365478516 Validation Loss: 0.7347202301025391\n",
      "Epoch 6080: Training Loss: 0.16683358947436014 Validation Loss: 0.734737753868103\n",
      "Epoch 6081: Training Loss: 0.1671291391054789 Validation Loss: 0.7342755794525146\n",
      "Epoch 6082: Training Loss: 0.16706903278827667 Validation Loss: 0.7338348031044006\n",
      "Epoch 6083: Training Loss: 0.16685684025287628 Validation Loss: 0.7333964705467224\n",
      "Epoch 6084: Training Loss: 0.16690686841805777 Validation Loss: 0.7333897352218628\n",
      "Epoch 6085: Training Loss: 0.1670947472254435 Validation Loss: 0.7345019578933716\n",
      "Epoch 6086: Training Loss: 0.16724120577176413 Validation Loss: 0.7347092628479004\n",
      "Epoch 6087: Training Loss: 0.1655754695336024 Validation Loss: 0.7348574995994568\n",
      "Epoch 6088: Training Loss: 0.16698783139387766 Validation Loss: 0.7350729703903198\n",
      "Epoch 6089: Training Loss: 0.16664164264996847 Validation Loss: 0.7348633408546448\n",
      "Epoch 6090: Training Loss: 0.1674787700176239 Validation Loss: 0.7345500588417053\n",
      "Epoch 6091: Training Loss: 0.1668499857187271 Validation Loss: 0.7337958812713623\n",
      "Epoch 6092: Training Loss: 0.1671083668867747 Validation Loss: 0.7330856919288635\n",
      "Epoch 6093: Training Loss: 0.1666181484858195 Validation Loss: 0.733346700668335\n",
      "Epoch 6094: Training Loss: 0.16658014059066772 Validation Loss: 0.7336609959602356\n",
      "Epoch 6095: Training Loss: 0.16654308140277863 Validation Loss: 0.7342812418937683\n",
      "Epoch 6096: Training Loss: 0.16669905185699463 Validation Loss: 0.7340723276138306\n",
      "Epoch 6097: Training Loss: 0.16629755993684134 Validation Loss: 0.7338758111000061\n",
      "Epoch 6098: Training Loss: 0.1668549875418345 Validation Loss: 0.7331000566482544\n",
      "Epoch 6099: Training Loss: 0.167080228527387 Validation Loss: 0.7334562540054321\n",
      "Epoch 6100: Training Loss: 0.16645600895086923 Validation Loss: 0.7342367172241211\n",
      "Epoch 6101: Training Loss: 0.16687204440434775 Validation Loss: 0.7344164848327637\n",
      "Epoch 6102: Training Loss: 0.1666443943977356 Validation Loss: 0.7349171042442322\n",
      "Epoch 6103: Training Loss: 0.16728492081165314 Validation Loss: 0.7349797487258911\n",
      "Epoch 6104: Training Loss: 0.16627945999304453 Validation Loss: 0.7348924279212952\n",
      "Epoch 6105: Training Loss: 0.16648287077744803 Validation Loss: 0.7349424958229065\n",
      "Epoch 6106: Training Loss: 0.16606546938419342 Validation Loss: 0.7343884110450745\n",
      "Epoch 6107: Training Loss: 0.1663153717915217 Validation Loss: 0.7341418862342834\n",
      "Epoch 6108: Training Loss: 0.16739758849143982 Validation Loss: 0.7339047193527222\n",
      "Epoch 6109: Training Loss: 0.1666646252075831 Validation Loss: 0.7340360283851624\n",
      "Epoch 6110: Training Loss: 0.1661879817644755 Validation Loss: 0.7342709898948669\n",
      "Epoch 6111: Training Loss: 0.1661243885755539 Validation Loss: 0.7341874241828918\n",
      "Epoch 6112: Training Loss: 0.1664697825908661 Validation Loss: 0.7344225645065308\n",
      "Epoch 6113: Training Loss: 0.16656253238519034 Validation Loss: 0.7339270114898682\n",
      "Epoch 6114: Training Loss: 0.16684572398662567 Validation Loss: 0.7346082329750061\n",
      "Epoch 6115: Training Loss: 0.16608383258183798 Validation Loss: 0.7341809868812561\n",
      "Epoch 6116: Training Loss: 0.16633879144986471 Validation Loss: 0.7341188192367554\n",
      "Epoch 6117: Training Loss: 0.16596649587154388 Validation Loss: 0.7340030670166016\n",
      "Epoch 6118: Training Loss: 0.16565343737602234 Validation Loss: 0.7340161800384521\n",
      "Epoch 6119: Training Loss: 0.16568703949451447 Validation Loss: 0.7343057990074158\n",
      "Epoch 6120: Training Loss: 0.1660096397002538 Validation Loss: 0.7342837452888489\n",
      "Epoch 6121: Training Loss: 0.1663864403963089 Validation Loss: 0.7345263957977295\n",
      "Epoch 6122: Training Loss: 0.1657936026652654 Validation Loss: 0.7342162132263184\n",
      "Epoch 6123: Training Loss: 0.16688973704973856 Validation Loss: 0.7344238758087158\n",
      "Epoch 6124: Training Loss: 0.1663738191127777 Validation Loss: 0.7346522808074951\n",
      "Epoch 6125: Training Loss: 0.16592872142791748 Validation Loss: 0.7348989844322205\n",
      "Epoch 6126: Training Loss: 0.1658691664536794 Validation Loss: 0.7343624234199524\n",
      "Epoch 6127: Training Loss: 0.1653945247332255 Validation Loss: 0.7342908382415771\n",
      "Epoch 6128: Training Loss: 0.16575794418652853 Validation Loss: 0.7339487075805664\n",
      "Epoch 6129: Training Loss: 0.16595802207787833 Validation Loss: 0.7343866229057312\n",
      "Epoch 6130: Training Loss: 0.1659763107697169 Validation Loss: 0.7343825101852417\n",
      "Epoch 6131: Training Loss: 0.1652781864007314 Validation Loss: 0.734662652015686\n",
      "Epoch 6132: Training Loss: 0.1658092588186264 Validation Loss: 0.7344729900360107\n",
      "Epoch 6133: Training Loss: 0.16565604507923126 Validation Loss: 0.7345640063285828\n",
      "Epoch 6134: Training Loss: 0.1657958428064982 Validation Loss: 0.7343188524246216\n",
      "Epoch 6135: Training Loss: 0.16563624640305838 Validation Loss: 0.7338618636131287\n",
      "Epoch 6136: Training Loss: 0.1658683717250824 Validation Loss: 0.7340912222862244\n",
      "Epoch 6137: Training Loss: 0.16558456917603812 Validation Loss: 0.7340437173843384\n",
      "Epoch 6138: Training Loss: 0.1663536330064138 Validation Loss: 0.7345879077911377\n",
      "Epoch 6139: Training Loss: 0.16544380287329355 Validation Loss: 0.7352379560470581\n",
      "Epoch 6140: Training Loss: 0.16555619736512503 Validation Loss: 0.7353441119194031\n",
      "Epoch 6141: Training Loss: 0.16538692017396292 Validation Loss: 0.735051691532135\n",
      "Epoch 6142: Training Loss: 0.16539247334003448 Validation Loss: 0.7346787452697754\n",
      "Epoch 6143: Training Loss: 0.16556396087010702 Validation Loss: 0.7344608902931213\n",
      "Epoch 6144: Training Loss: 0.16554626325766245 Validation Loss: 0.7339811325073242\n",
      "Epoch 6145: Training Loss: 0.1659486691157023 Validation Loss: 0.7339995503425598\n",
      "Epoch 6146: Training Loss: 0.16543170313040415 Validation Loss: 0.7345128655433655\n",
      "Epoch 6147: Training Loss: 0.16606206695238748 Validation Loss: 0.7349228262901306\n",
      "Epoch 6148: Training Loss: 0.1652261565128962 Validation Loss: 0.7348719835281372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6149: Training Loss: 0.16494794189929962 Validation Loss: 0.7345036268234253\n",
      "Epoch 6150: Training Loss: 0.165082057317098 Validation Loss: 0.7344096302986145\n",
      "Epoch 6151: Training Loss: 0.16551578044891357 Validation Loss: 0.7343900203704834\n",
      "Epoch 6152: Training Loss: 0.16529339055220285 Validation Loss: 0.7343236207962036\n",
      "Epoch 6153: Training Loss: 0.16625804950793585 Validation Loss: 0.7345640063285828\n",
      "Epoch 6154: Training Loss: 0.16523146629333496 Validation Loss: 0.7347513437271118\n",
      "Epoch 6155: Training Loss: 0.16508008042971292 Validation Loss: 0.7349355220794678\n",
      "Epoch 6156: Training Loss: 0.16511468092600504 Validation Loss: 0.7342416048049927\n",
      "Epoch 6157: Training Loss: 0.16500764091809592 Validation Loss: 0.7347962856292725\n",
      "Epoch 6158: Training Loss: 0.16554471850395203 Validation Loss: 0.7344750165939331\n",
      "Epoch 6159: Training Loss: 0.1656950314839681 Validation Loss: 0.7346113920211792\n",
      "Epoch 6160: Training Loss: 0.16493745148181915 Validation Loss: 0.7350415587425232\n",
      "Epoch 6161: Training Loss: 0.16509497165679932 Validation Loss: 0.7350189089775085\n",
      "Epoch 6162: Training Loss: 0.1649319330851237 Validation Loss: 0.7346689701080322\n",
      "Epoch 6163: Training Loss: 0.16509721676508585 Validation Loss: 0.7343478798866272\n",
      "Epoch 6164: Training Loss: 0.16595041751861572 Validation Loss: 0.7343518137931824\n",
      "Epoch 6165: Training Loss: 0.16445540388425192 Validation Loss: 0.7347509860992432\n",
      "Epoch 6166: Training Loss: 0.16480936110019684 Validation Loss: 0.7349205613136292\n",
      "Epoch 6167: Training Loss: 0.16474372645219168 Validation Loss: 0.7348933219909668\n",
      "Epoch 6168: Training Loss: 0.16490564743677774 Validation Loss: 0.7350013256072998\n",
      "Epoch 6169: Training Loss: 0.1644808699687322 Validation Loss: 0.7352643609046936\n",
      "Epoch 6170: Training Loss: 0.16523414850234985 Validation Loss: 0.7353274822235107\n",
      "Epoch 6171: Training Loss: 0.16475761930147806 Validation Loss: 0.7345811128616333\n",
      "Epoch 6172: Training Loss: 0.16484939058621725 Validation Loss: 0.7341101169586182\n",
      "Epoch 6173: Training Loss: 0.16451892256736755 Validation Loss: 0.7342231869697571\n",
      "Epoch 6174: Training Loss: 0.16491894920667013 Validation Loss: 0.7344064712524414\n",
      "Epoch 6175: Training Loss: 0.16468713184197745 Validation Loss: 0.7353152632713318\n",
      "Epoch 6176: Training Loss: 0.16573679447174072 Validation Loss: 0.735225260257721\n",
      "Epoch 6177: Training Loss: 0.1643351415793101 Validation Loss: 0.7349650263786316\n",
      "Epoch 6178: Training Loss: 0.16518534223238626 Validation Loss: 0.7351443767547607\n",
      "Epoch 6179: Training Loss: 0.16522647440433502 Validation Loss: 0.7353875041007996\n",
      "Epoch 6180: Training Loss: 0.16468042135238647 Validation Loss: 0.7351213097572327\n",
      "Epoch 6181: Training Loss: 0.1647562434275945 Validation Loss: 0.7344539761543274\n",
      "Epoch 6182: Training Loss: 0.16456141571203867 Validation Loss: 0.7340570092201233\n",
      "Epoch 6183: Training Loss: 0.16451483964920044 Validation Loss: 0.7339853644371033\n",
      "Epoch 6184: Training Loss: 0.16401901841163635 Validation Loss: 0.734688937664032\n",
      "Epoch 6185: Training Loss: 0.16426810125509897 Validation Loss: 0.7349506616592407\n",
      "Epoch 6186: Training Loss: 0.1645412047704061 Validation Loss: 0.7345260381698608\n",
      "Epoch 6187: Training Loss: 0.16467837989330292 Validation Loss: 0.7347565293312073\n",
      "Epoch 6188: Training Loss: 0.16439462701479593 Validation Loss: 0.7348092198371887\n",
      "Epoch 6189: Training Loss: 0.1642602781454722 Validation Loss: 0.7344892621040344\n",
      "Epoch 6190: Training Loss: 0.16435163716475168 Validation Loss: 0.7344979643821716\n",
      "Epoch 6191: Training Loss: 0.16434181729952493 Validation Loss: 0.7344996929168701\n",
      "Epoch 6192: Training Loss: 0.16411926845709482 Validation Loss: 0.7347235679626465\n",
      "Epoch 6193: Training Loss: 0.16445017357667288 Validation Loss: 0.7353727221488953\n",
      "Epoch 6194: Training Loss: 0.16404947638511658 Validation Loss: 0.7355631589889526\n",
      "Epoch 6195: Training Loss: 0.16418769458929697 Validation Loss: 0.7353922724723816\n",
      "Epoch 6196: Training Loss: 0.16437479356924692 Validation Loss: 0.7354512810707092\n",
      "Epoch 6197: Training Loss: 0.16427373886108398 Validation Loss: 0.7349218130111694\n",
      "Epoch 6198: Training Loss: 0.1644976188739141 Validation Loss: 0.7346385717391968\n",
      "Epoch 6199: Training Loss: 0.16397539774576822 Validation Loss: 0.734621524810791\n",
      "Epoch 6200: Training Loss: 0.16390774647394815 Validation Loss: 0.7345680594444275\n",
      "Epoch 6201: Training Loss: 0.1639475623766581 Validation Loss: 0.7350757718086243\n",
      "Epoch 6202: Training Loss: 0.16392775376637778 Validation Loss: 0.7354257702827454\n",
      "Epoch 6203: Training Loss: 0.1641885886589686 Validation Loss: 0.7346948981285095\n",
      "Epoch 6204: Training Loss: 0.16348833839098612 Validation Loss: 0.734724760055542\n",
      "Epoch 6205: Training Loss: 0.16424071788787842 Validation Loss: 0.7351217269897461\n",
      "Epoch 6206: Training Loss: 0.16404980421066284 Validation Loss: 0.7350572347640991\n",
      "Epoch 6207: Training Loss: 0.16393355031808218 Validation Loss: 0.73492830991745\n",
      "Epoch 6208: Training Loss: 0.1638966848452886 Validation Loss: 0.7349140644073486\n",
      "Epoch 6209: Training Loss: 0.16387238105138144 Validation Loss: 0.7351953387260437\n",
      "Epoch 6210: Training Loss: 0.16407741606235504 Validation Loss: 0.7354537844657898\n",
      "Epoch 6211: Training Loss: 0.1641508787870407 Validation Loss: 0.7355737686157227\n",
      "Epoch 6212: Training Loss: 0.16451147695382437 Validation Loss: 0.7344787120819092\n",
      "Epoch 6213: Training Loss: 0.16443322598934174 Validation Loss: 0.7346858978271484\n",
      "Epoch 6214: Training Loss: 0.1641128808259964 Validation Loss: 0.734597384929657\n",
      "Epoch 6215: Training Loss: 0.16364243626594543 Validation Loss: 0.7349199056625366\n",
      "Epoch 6216: Training Loss: 0.16331137220064798 Validation Loss: 0.734741747379303\n",
      "Epoch 6217: Training Loss: 0.16355648140112558 Validation Loss: 0.7346771359443665\n",
      "Epoch 6218: Training Loss: 0.16345017155011496 Validation Loss: 0.7350007891654968\n",
      "Epoch 6219: Training Loss: 0.16364479064941406 Validation Loss: 0.7350180745124817\n",
      "Epoch 6220: Training Loss: 0.16347457468509674 Validation Loss: 0.7348305583000183\n",
      "Epoch 6221: Training Loss: 0.16365795830885568 Validation Loss: 0.7349084615707397\n",
      "Epoch 6222: Training Loss: 0.1653898060321808 Validation Loss: 0.7349312901496887\n",
      "Epoch 6223: Training Loss: 0.16367616256078085 Validation Loss: 0.7347278594970703\n",
      "Epoch 6224: Training Loss: 0.16398409505685171 Validation Loss: 0.7350330948829651\n",
      "Epoch 6225: Training Loss: 0.16316310067971548 Validation Loss: 0.7349762916564941\n",
      "Epoch 6226: Training Loss: 0.1639330337444941 Validation Loss: 0.7353864312171936\n",
      "Epoch 6227: Training Loss: 0.16319206357002258 Validation Loss: 0.7353101968765259\n",
      "Epoch 6228: Training Loss: 0.16367749373118082 Validation Loss: 0.7352884411811829\n",
      "Epoch 6229: Training Loss: 0.16347338259220123 Validation Loss: 0.7352109551429749\n",
      "Epoch 6230: Training Loss: 0.16398738324642181 Validation Loss: 0.7348864078521729\n",
      "Epoch 6231: Training Loss: 0.16366425156593323 Validation Loss: 0.734599769115448\n",
      "Epoch 6232: Training Loss: 0.16330804924170175 Validation Loss: 0.7346919178962708\n",
      "Epoch 6233: Training Loss: 0.16311989724636078 Validation Loss: 0.7349497675895691\n",
      "Epoch 6234: Training Loss: 0.16278492907683054 Validation Loss: 0.7350003719329834\n",
      "Epoch 6235: Training Loss: 0.16272459427515665 Validation Loss: 0.7348445653915405\n",
      "Epoch 6236: Training Loss: 0.16340946157773337 Validation Loss: 0.7345553636550903\n",
      "Epoch 6237: Training Loss: 0.16341102619965872 Validation Loss: 0.735038161277771\n",
      "Epoch 6238: Training Loss: 0.16349802911281586 Validation Loss: 0.7356085181236267\n",
      "Epoch 6239: Training Loss: 0.16324855387210846 Validation Loss: 0.7353107333183289\n",
      "Epoch 6240: Training Loss: 0.16367238759994507 Validation Loss: 0.735260546207428\n",
      "Epoch 6241: Training Loss: 0.1631192813316981 Validation Loss: 0.7358761429786682\n",
      "Epoch 6242: Training Loss: 0.16284934679667154 Validation Loss: 0.7352458834648132\n",
      "Epoch 6243: Training Loss: 0.16310847302277884 Validation Loss: 0.734442949295044\n",
      "Epoch 6244: Training Loss: 0.16240748763084412 Validation Loss: 0.7347393035888672\n",
      "Epoch 6245: Training Loss: 0.16390750308831534 Validation Loss: 0.7347972989082336\n",
      "Epoch 6246: Training Loss: 0.16283584634462991 Validation Loss: 0.7347769737243652\n",
      "Epoch 6247: Training Loss: 0.1635657101869583 Validation Loss: 0.7345750331878662\n",
      "Epoch 6248: Training Loss: 0.1633560061454773 Validation Loss: 0.7349646091461182\n",
      "Epoch 6249: Training Loss: 0.16217119991779327 Validation Loss: 0.7352322936058044\n",
      "Epoch 6250: Training Loss: 0.16328804194927216 Validation Loss: 0.7353484630584717\n",
      "Epoch 6251: Training Loss: 0.16241023937861124 Validation Loss: 0.7353819608688354\n",
      "Epoch 6252: Training Loss: 0.16261319319407144 Validation Loss: 0.7350485324859619\n",
      "Epoch 6253: Training Loss: 0.16254120568434396 Validation Loss: 0.7349876761436462\n",
      "Epoch 6254: Training Loss: 0.16271435221036276 Validation Loss: 0.7349021434783936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6255: Training Loss: 0.16273411611715952 Validation Loss: 0.7349662184715271\n",
      "Epoch 6256: Training Loss: 0.16293534636497498 Validation Loss: 0.7348361611366272\n",
      "Epoch 6257: Training Loss: 0.1632912258307139 Validation Loss: 0.7355190515518188\n",
      "Epoch 6258: Training Loss: 0.1629402736822764 Validation Loss: 0.7353666424751282\n",
      "Epoch 6259: Training Loss: 0.1625900318225225 Validation Loss: 0.7354752421379089\n",
      "Epoch 6260: Training Loss: 0.16248221695423126 Validation Loss: 0.73570317029953\n",
      "Epoch 6261: Training Loss: 0.16238920390605927 Validation Loss: 0.7359494566917419\n",
      "Epoch 6262: Training Loss: 0.1630640576283137 Validation Loss: 0.7359034419059753\n",
      "Epoch 6263: Training Loss: 0.16335575779279074 Validation Loss: 0.7357489466667175\n",
      "Epoch 6264: Training Loss: 0.16223003466924033 Validation Loss: 0.735564649105072\n",
      "Epoch 6265: Training Loss: 0.16320890188217163 Validation Loss: 0.7352254986763\n",
      "Epoch 6266: Training Loss: 0.16254591941833496 Validation Loss: 0.7354874014854431\n",
      "Epoch 6267: Training Loss: 0.16262189050515494 Validation Loss: 0.7357162237167358\n",
      "Epoch 6268: Training Loss: 0.16270451247692108 Validation Loss: 0.7351418733596802\n",
      "Epoch 6269: Training Loss: 0.16247196992238364 Validation Loss: 0.7348800897598267\n",
      "Epoch 6270: Training Loss: 0.16230603555838266 Validation Loss: 0.7355769276618958\n",
      "Epoch 6271: Training Loss: 0.16279196242491403 Validation Loss: 0.7350172996520996\n",
      "Epoch 6272: Training Loss: 0.16246749957402548 Validation Loss: 0.7350271940231323\n",
      "Epoch 6273: Training Loss: 0.16261045138041177 Validation Loss: 0.734749436378479\n",
      "Epoch 6274: Training Loss: 0.1621930201848348 Validation Loss: 0.7350229620933533\n",
      "Epoch 6275: Training Loss: 0.16211230556170145 Validation Loss: 0.735342800617218\n",
      "Epoch 6276: Training Loss: 0.1622106283903122 Validation Loss: 0.7353923320770264\n",
      "Epoch 6277: Training Loss: 0.1633038173119227 Validation Loss: 0.7354646921157837\n",
      "Epoch 6278: Training Loss: 0.16229137281576791 Validation Loss: 0.7347453236579895\n",
      "Epoch 6279: Training Loss: 0.16209147373835245 Validation Loss: 0.7349584698677063\n",
      "Epoch 6280: Training Loss: 0.16240625580151877 Validation Loss: 0.7351003289222717\n",
      "Epoch 6281: Training Loss: 0.16248453160127005 Validation Loss: 0.7350171804428101\n",
      "Epoch 6282: Training Loss: 0.16182799637317657 Validation Loss: 0.7356030941009521\n",
      "Epoch 6283: Training Loss: 0.16228843231995901 Validation Loss: 0.7354239821434021\n",
      "Epoch 6284: Training Loss: 0.16235915819803873 Validation Loss: 0.7354063987731934\n",
      "Epoch 6285: Training Loss: 0.16214482486248016 Validation Loss: 0.7359676957130432\n",
      "Epoch 6286: Training Loss: 0.16198599338531494 Validation Loss: 0.7355560064315796\n",
      "Epoch 6287: Training Loss: 0.16269440948963165 Validation Loss: 0.7355920672416687\n",
      "Epoch 6288: Training Loss: 0.1621898611386617 Validation Loss: 0.7353702783584595\n",
      "Epoch 6289: Training Loss: 0.16181929409503937 Validation Loss: 0.7353659272193909\n",
      "Epoch 6290: Training Loss: 0.1618764599164327 Validation Loss: 0.735622763633728\n",
      "Epoch 6291: Training Loss: 0.16227000951766968 Validation Loss: 0.7354596853256226\n",
      "Epoch 6292: Training Loss: 0.1617924322684606 Validation Loss: 0.7354877591133118\n",
      "Epoch 6293: Training Loss: 0.16221994161605835 Validation Loss: 0.7360169887542725\n",
      "Epoch 6294: Training Loss: 0.16176269948482513 Validation Loss: 0.7355551719665527\n",
      "Epoch 6295: Training Loss: 0.16203191876411438 Validation Loss: 0.7353071570396423\n",
      "Epoch 6296: Training Loss: 0.16224811474482217 Validation Loss: 0.7350667715072632\n",
      "Epoch 6297: Training Loss: 0.16192758083343506 Validation Loss: 0.7355597019195557\n",
      "Epoch 6298: Training Loss: 0.16192552944024405 Validation Loss: 0.7359281778335571\n",
      "Epoch 6299: Training Loss: 0.1618415762980779 Validation Loss: 0.7354872822761536\n",
      "Epoch 6300: Training Loss: 0.1619430681069692 Validation Loss: 0.7356640100479126\n",
      "Epoch 6301: Training Loss: 0.16146400570869446 Validation Loss: 0.7357380986213684\n",
      "Epoch 6302: Training Loss: 0.16201022267341614 Validation Loss: 0.7358372807502747\n",
      "Epoch 6303: Training Loss: 0.16215644280115762 Validation Loss: 0.7359405159950256\n",
      "Epoch 6304: Training Loss: 0.16162307560443878 Validation Loss: 0.7353116869926453\n",
      "Epoch 6305: Training Loss: 0.16158134738604227 Validation Loss: 0.7350189089775085\n",
      "Epoch 6306: Training Loss: 0.1613994042078654 Validation Loss: 0.7352890372276306\n",
      "Epoch 6307: Training Loss: 0.16134935120741525 Validation Loss: 0.7351545691490173\n",
      "Epoch 6308: Training Loss: 0.1613772710164388 Validation Loss: 0.735469400882721\n",
      "Epoch 6309: Training Loss: 0.16171629230181375 Validation Loss: 0.7357238531112671\n",
      "Epoch 6310: Training Loss: 0.16130907833576202 Validation Loss: 0.7354950904846191\n",
      "Epoch 6311: Training Loss: 0.162240336338679 Validation Loss: 0.7353694438934326\n",
      "Epoch 6312: Training Loss: 0.1616475929816564 Validation Loss: 0.7349143624305725\n",
      "Epoch 6313: Training Loss: 0.16098548471927643 Validation Loss: 0.7349534630775452\n",
      "Epoch 6314: Training Loss: 0.16156723101933798 Validation Loss: 0.736031174659729\n",
      "Epoch 6315: Training Loss: 0.1613332231839498 Validation Loss: 0.7359633445739746\n",
      "Epoch 6316: Training Loss: 0.16157842179139456 Validation Loss: 0.7363121509552002\n",
      "Epoch 6317: Training Loss: 0.161008154352506 Validation Loss: 0.7365847229957581\n",
      "Epoch 6318: Training Loss: 0.16152799129486084 Validation Loss: 0.7359679341316223\n",
      "Epoch 6319: Training Loss: 0.1610220968723297 Validation Loss: 0.7350172400474548\n",
      "Epoch 6320: Training Loss: 0.16104660431543985 Validation Loss: 0.7359670400619507\n",
      "Epoch 6321: Training Loss: 0.16163401305675507 Validation Loss: 0.7357314229011536\n",
      "Epoch 6322: Training Loss: 0.1610876719156901 Validation Loss: 0.736253559589386\n",
      "Epoch 6323: Training Loss: 0.1610269546508789 Validation Loss: 0.7363645434379578\n",
      "Epoch 6324: Training Loss: 0.16161948442459106 Validation Loss: 0.7358953952789307\n",
      "Epoch 6325: Training Loss: 0.16099148988723755 Validation Loss: 0.7351790070533752\n",
      "Epoch 6326: Training Loss: 0.16091428200403848 Validation Loss: 0.7353844046592712\n",
      "Epoch 6327: Training Loss: 0.16089199980099997 Validation Loss: 0.7350831627845764\n",
      "Epoch 6328: Training Loss: 0.16086197396119437 Validation Loss: 0.7350648641586304\n",
      "Epoch 6329: Training Loss: 0.16095012923081717 Validation Loss: 0.7352761626243591\n",
      "Epoch 6330: Training Loss: 0.1607386122147242 Validation Loss: 0.7352321743965149\n",
      "Epoch 6331: Training Loss: 0.16074234247207642 Validation Loss: 0.7357653379440308\n",
      "Epoch 6332: Training Loss: 0.1611399898926417 Validation Loss: 0.7359258532524109\n",
      "Epoch 6333: Training Loss: 0.16081771751244864 Validation Loss: 0.7357469201087952\n",
      "Epoch 6334: Training Loss: 0.16070741911729178 Validation Loss: 0.7357142567634583\n",
      "Epoch 6335: Training Loss: 0.1607414335012436 Validation Loss: 0.7350529432296753\n",
      "Epoch 6336: Training Loss: 0.16055897871653238 Validation Loss: 0.7352709174156189\n",
      "Epoch 6337: Training Loss: 0.16089090208212534 Validation Loss: 0.7358571887016296\n",
      "Epoch 6338: Training Loss: 0.16059603293736777 Validation Loss: 0.7360566258430481\n",
      "Epoch 6339: Training Loss: 0.16097780068715414 Validation Loss: 0.7359310984611511\n",
      "Epoch 6340: Training Loss: 0.1610682805379232 Validation Loss: 0.7351654767990112\n",
      "Epoch 6341: Training Loss: 0.1619019110997518 Validation Loss: 0.7351142168045044\n",
      "Epoch 6342: Training Loss: 0.1604364961385727 Validation Loss: 0.7356740236282349\n",
      "Epoch 6343: Training Loss: 0.16063523292541504 Validation Loss: 0.7351765632629395\n",
      "Epoch 6344: Training Loss: 0.16071085631847382 Validation Loss: 0.7349128127098083\n",
      "Epoch 6345: Training Loss: 0.1605025827884674 Validation Loss: 0.7351939082145691\n",
      "Epoch 6346: Training Loss: 0.16083330909411112 Validation Loss: 0.7355908751487732\n",
      "Epoch 6347: Training Loss: 0.160539448261261 Validation Loss: 0.7363216876983643\n",
      "Epoch 6348: Training Loss: 0.16051716605822244 Validation Loss: 0.7370918989181519\n",
      "Epoch 6349: Training Loss: 0.16058326760927835 Validation Loss: 0.7365612387657166\n",
      "Epoch 6350: Training Loss: 0.1602379779020945 Validation Loss: 0.7359684109687805\n",
      "Epoch 6351: Training Loss: 0.1602940857410431 Validation Loss: 0.7357951402664185\n",
      "Epoch 6352: Training Loss: 0.16061438620090485 Validation Loss: 0.7356998920440674\n",
      "Epoch 6353: Training Loss: 0.1606205552816391 Validation Loss: 0.7355982661247253\n",
      "Epoch 6354: Training Loss: 0.16026542087395987 Validation Loss: 0.7355651259422302\n",
      "Epoch 6355: Training Loss: 0.16072024405002594 Validation Loss: 0.735543966293335\n",
      "Epoch 6356: Training Loss: 0.16031959156195322 Validation Loss: 0.7358460426330566\n",
      "Epoch 6357: Training Loss: 0.1600861152013143 Validation Loss: 0.7354353666305542\n",
      "Epoch 6358: Training Loss: 0.1601050396760305 Validation Loss: 0.7354616522789001\n",
      "Epoch 6359: Training Loss: 0.1604055811961492 Validation Loss: 0.7354558110237122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6360: Training Loss: 0.1602830390135447 Validation Loss: 0.7357948422431946\n",
      "Epoch 6361: Training Loss: 0.16096187134583792 Validation Loss: 0.7357189655303955\n",
      "Epoch 6362: Training Loss: 0.1603080282608668 Validation Loss: 0.7360756397247314\n",
      "Epoch 6363: Training Loss: 0.16023174424966177 Validation Loss: 0.7365882992744446\n",
      "Epoch 6364: Training Loss: 0.16030930976072946 Validation Loss: 0.7361043691635132\n",
      "Epoch 6365: Training Loss: 0.1604566921790441 Validation Loss: 0.7356041073799133\n",
      "Epoch 6366: Training Loss: 0.16002225379149118 Validation Loss: 0.7359213829040527\n",
      "Epoch 6367: Training Loss: 0.16053285698095957 Validation Loss: 0.7364453673362732\n",
      "Epoch 6368: Training Loss: 0.1605302890141805 Validation Loss: 0.736054003238678\n",
      "Epoch 6369: Training Loss: 0.15991092224915823 Validation Loss: 0.7353553771972656\n",
      "Epoch 6370: Training Loss: 0.1598799874385198 Validation Loss: 0.7359105944633484\n",
      "Epoch 6371: Training Loss: 0.15995149314403534 Validation Loss: 0.7364850640296936\n",
      "Epoch 6372: Training Loss: 0.16020938754081726 Validation Loss: 0.7370170950889587\n",
      "Epoch 6373: Training Loss: 0.1596775452295939 Validation Loss: 0.7361542582511902\n",
      "Epoch 6374: Training Loss: 0.16001302500565848 Validation Loss: 0.7355095148086548\n",
      "Epoch 6375: Training Loss: 0.159640833735466 Validation Loss: 0.7353028059005737\n",
      "Epoch 6376: Training Loss: 0.15983092288176218 Validation Loss: 0.7353127598762512\n",
      "Epoch 6377: Training Loss: 0.15999525288740793 Validation Loss: 0.7358842492103577\n",
      "Epoch 6378: Training Loss: 0.160017396012942 Validation Loss: 0.736039936542511\n",
      "Epoch 6379: Training Loss: 0.1607512633005778 Validation Loss: 0.7367005944252014\n",
      "Epoch 6380: Training Loss: 0.1593671292066574 Validation Loss: 0.7369158864021301\n",
      "Epoch 6381: Training Loss: 0.16015349825223288 Validation Loss: 0.736846387386322\n",
      "Epoch 6382: Training Loss: 0.15971906979878744 Validation Loss: 0.7360740900039673\n",
      "Epoch 6383: Training Loss: 0.16021764278411865 Validation Loss: 0.7356710433959961\n",
      "Epoch 6384: Training Loss: 0.1596173495054245 Validation Loss: 0.7356503009796143\n",
      "Epoch 6385: Training Loss: 0.15980968872706094 Validation Loss: 0.7365425825119019\n",
      "Epoch 6386: Training Loss: 0.16013922293980917 Validation Loss: 0.7362132668495178\n",
      "Epoch 6387: Training Loss: 0.1594106505314509 Validation Loss: 0.7356764078140259\n",
      "Epoch 6388: Training Loss: 0.15972637136777243 Validation Loss: 0.7362815141677856\n",
      "Epoch 6389: Training Loss: 0.16023139158884683 Validation Loss: 0.736076831817627\n",
      "Epoch 6390: Training Loss: 0.15991926193237305 Validation Loss: 0.7360600233078003\n",
      "Epoch 6391: Training Loss: 0.15993757049242655 Validation Loss: 0.7361926436424255\n",
      "Epoch 6392: Training Loss: 0.16001581648985544 Validation Loss: 0.7363371849060059\n",
      "Epoch 6393: Training Loss: 0.15945414702097574 Validation Loss: 0.7360586524009705\n",
      "Epoch 6394: Training Loss: 0.15933193266391754 Validation Loss: 0.7363042235374451\n",
      "Epoch 6395: Training Loss: 0.15944467981656393 Validation Loss: 0.7360677123069763\n",
      "Epoch 6396: Training Loss: 0.1597196708122889 Validation Loss: 0.735557496547699\n",
      "Epoch 6397: Training Loss: 0.15958388149738312 Validation Loss: 0.7353292107582092\n",
      "Epoch 6398: Training Loss: 0.15914307037989298 Validation Loss: 0.7355269193649292\n",
      "Epoch 6399: Training Loss: 0.1609162613749504 Validation Loss: 0.736101508140564\n",
      "Epoch 6400: Training Loss: 0.1591762900352478 Validation Loss: 0.7364660501480103\n",
      "Epoch 6401: Training Loss: 0.15915322800477347 Validation Loss: 0.7363156676292419\n",
      "Epoch 6402: Training Loss: 0.1591885487238566 Validation Loss: 0.7365684509277344\n",
      "Epoch 6403: Training Loss: 0.15928701062997183 Validation Loss: 0.7361485958099365\n",
      "Epoch 6404: Training Loss: 0.15932377179463705 Validation Loss: 0.736415684223175\n",
      "Epoch 6405: Training Loss: 0.1592058539390564 Validation Loss: 0.7362666726112366\n",
      "Epoch 6406: Training Loss: 0.15901991228262582 Validation Loss: 0.7360880374908447\n",
      "Epoch 6407: Training Loss: 0.15906476974487305 Validation Loss: 0.7361391186714172\n",
      "Epoch 6408: Training Loss: 0.15901904304822287 Validation Loss: 0.7362174391746521\n",
      "Epoch 6409: Training Loss: 0.1592536966005961 Validation Loss: 0.7369564175605774\n",
      "Epoch 6410: Training Loss: 0.15937373538812002 Validation Loss: 0.7366790771484375\n",
      "Epoch 6411: Training Loss: 0.1597265899181366 Validation Loss: 0.7366070747375488\n",
      "Epoch 6412: Training Loss: 0.1589743991692861 Validation Loss: 0.7369338274002075\n",
      "Epoch 6413: Training Loss: 0.15913923581441244 Validation Loss: 0.7363265156745911\n",
      "Epoch 6414: Training Loss: 0.1594545841217041 Validation Loss: 0.7359469532966614\n",
      "Epoch 6415: Training Loss: 0.1588700513044993 Validation Loss: 0.7359034419059753\n",
      "Epoch 6416: Training Loss: 0.15940583745638529 Validation Loss: 0.7360907793045044\n",
      "Epoch 6417: Training Loss: 0.15936399002869925 Validation Loss: 0.7355361580848694\n",
      "Epoch 6418: Training Loss: 0.15892791748046875 Validation Loss: 0.7359965443611145\n",
      "Epoch 6419: Training Loss: 0.15869209667046866 Validation Loss: 0.7363669872283936\n",
      "Epoch 6420: Training Loss: 0.1590080459912618 Validation Loss: 0.7364091873168945\n",
      "Epoch 6421: Training Loss: 0.15924397110939026 Validation Loss: 0.7365610599517822\n",
      "Epoch 6422: Training Loss: 0.15913166602452597 Validation Loss: 0.736619770526886\n",
      "Epoch 6423: Training Loss: 0.15899525086085 Validation Loss: 0.7367798089981079\n",
      "Epoch 6424: Training Loss: 0.1587613970041275 Validation Loss: 0.7366816401481628\n",
      "Epoch 6425: Training Loss: 0.15869072079658508 Validation Loss: 0.7370336651802063\n",
      "Epoch 6426: Training Loss: 0.1587514877319336 Validation Loss: 0.7368326783180237\n",
      "Epoch 6427: Training Loss: 0.15880170464515686 Validation Loss: 0.7366246581077576\n",
      "Epoch 6428: Training Loss: 0.15829864144325256 Validation Loss: 0.7358043789863586\n",
      "Epoch 6429: Training Loss: 0.1590582380692164 Validation Loss: 0.735117495059967\n",
      "Epoch 6430: Training Loss: 0.15866812070210776 Validation Loss: 0.7354208827018738\n",
      "Epoch 6431: Training Loss: 0.15981886287530264 Validation Loss: 0.7361890077590942\n",
      "Epoch 6432: Training Loss: 0.1588287651538849 Validation Loss: 0.7361577153205872\n",
      "Epoch 6433: Training Loss: 0.1586592048406601 Validation Loss: 0.7364258170127869\n",
      "Epoch 6434: Training Loss: 0.1585658093293508 Validation Loss: 0.7360617518424988\n",
      "Epoch 6435: Training Loss: 0.15897120038668314 Validation Loss: 0.7361047267913818\n",
      "Epoch 6436: Training Loss: 0.15891416370868683 Validation Loss: 0.7358320951461792\n",
      "Epoch 6437: Training Loss: 0.1584389607111613 Validation Loss: 0.7362517714500427\n",
      "Epoch 6438: Training Loss: 0.15866305430730185 Validation Loss: 0.7370679378509521\n",
      "Epoch 6439: Training Loss: 0.15828344225883484 Validation Loss: 0.73726886510849\n",
      "Epoch 6440: Training Loss: 0.1578706552584966 Validation Loss: 0.7378318309783936\n",
      "Epoch 6441: Training Loss: 0.15881655613581339 Validation Loss: 0.7373144030570984\n",
      "Epoch 6442: Training Loss: 0.15817943215370178 Validation Loss: 0.7373226881027222\n",
      "Epoch 6443: Training Loss: 0.1588968833287557 Validation Loss: 0.7368404865264893\n",
      "Epoch 6444: Training Loss: 0.15817093352476755 Validation Loss: 0.7360807657241821\n",
      "Epoch 6445: Training Loss: 0.15789255499839783 Validation Loss: 0.7363576292991638\n",
      "Epoch 6446: Training Loss: 0.15825592478116354 Validation Loss: 0.735891580581665\n",
      "Epoch 6447: Training Loss: 0.15812695026397705 Validation Loss: 0.7362408638000488\n",
      "Epoch 6448: Training Loss: 0.15778838098049164 Validation Loss: 0.7362790703773499\n",
      "Epoch 6449: Training Loss: 0.15807409087816873 Validation Loss: 0.7364514470100403\n",
      "Epoch 6450: Training Loss: 0.15829079349835715 Validation Loss: 0.736868143081665\n",
      "Epoch 6451: Training Loss: 0.15795256197452545 Validation Loss: 0.7371217012405396\n",
      "Epoch 6452: Training Loss: 0.15841497480869293 Validation Loss: 0.7371222376823425\n",
      "Epoch 6453: Training Loss: 0.1581072856982549 Validation Loss: 0.736857533454895\n",
      "Epoch 6454: Training Loss: 0.1586141288280487 Validation Loss: 0.7358787655830383\n",
      "Epoch 6455: Training Loss: 0.15825953086217245 Validation Loss: 0.7354075312614441\n",
      "Epoch 6456: Training Loss: 0.1581661601861318 Validation Loss: 0.736196756362915\n",
      "Epoch 6457: Training Loss: 0.15875699122746786 Validation Loss: 0.7365221381187439\n",
      "Epoch 6458: Training Loss: 0.15812835097312927 Validation Loss: 0.7362457513809204\n",
      "Epoch 6459: Training Loss: 0.1582958996295929 Validation Loss: 0.7366294860839844\n",
      "Epoch 6460: Training Loss: 0.1586339920759201 Validation Loss: 0.7369023561477661\n",
      "Epoch 6461: Training Loss: 0.15811550120512644 Validation Loss: 0.737213671207428\n",
      "Epoch 6462: Training Loss: 0.15795580546061197 Validation Loss: 0.736953616142273\n",
      "Epoch 6463: Training Loss: 0.15819760163625082 Validation Loss: 0.7365657091140747\n",
      "Epoch 6464: Training Loss: 0.15839767952760062 Validation Loss: 0.7363967895507812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6465: Training Loss: 0.15776597956816354 Validation Loss: 0.7362220883369446\n",
      "Epoch 6466: Training Loss: 0.1583115061124166 Validation Loss: 0.7360897064208984\n",
      "Epoch 6467: Training Loss: 0.15800378223260245 Validation Loss: 0.7364252805709839\n",
      "Epoch 6468: Training Loss: 0.15790788332621256 Validation Loss: 0.7371315360069275\n",
      "Epoch 6469: Training Loss: 0.15797402461369833 Validation Loss: 0.7369809746742249\n",
      "Epoch 6470: Training Loss: 0.1575429638226827 Validation Loss: 0.7372429966926575\n",
      "Epoch 6471: Training Loss: 0.1577535718679428 Validation Loss: 0.7370524406433105\n",
      "Epoch 6472: Training Loss: 0.15779092411200205 Validation Loss: 0.7363251447677612\n",
      "Epoch 6473: Training Loss: 0.15764228502909342 Validation Loss: 0.7360433340072632\n",
      "Epoch 6474: Training Loss: 0.15756220122178397 Validation Loss: 0.7364388704299927\n",
      "Epoch 6475: Training Loss: 0.157303253809611 Validation Loss: 0.7368350028991699\n",
      "Epoch 6476: Training Loss: 0.15797128776709238 Validation Loss: 0.7366107106208801\n",
      "Epoch 6477: Training Loss: 0.1576792299747467 Validation Loss: 0.7367239594459534\n",
      "Epoch 6478: Training Loss: 0.1573317696650823 Validation Loss: 0.7374985814094543\n",
      "Epoch 6479: Training Loss: 0.15756762027740479 Validation Loss: 0.7372305393218994\n",
      "Epoch 6480: Training Loss: 0.15793095529079437 Validation Loss: 0.737243115901947\n",
      "Epoch 6481: Training Loss: 0.1573374072710673 Validation Loss: 0.7370452880859375\n",
      "Epoch 6482: Training Loss: 0.15741094946861267 Validation Loss: 0.7369474172592163\n",
      "Epoch 6483: Training Loss: 0.15858217080434164 Validation Loss: 0.7367150187492371\n",
      "Epoch 6484: Training Loss: 0.15742117166519165 Validation Loss: 0.7363616824150085\n",
      "Epoch 6485: Training Loss: 0.1573472668727239 Validation Loss: 0.7364925742149353\n",
      "Epoch 6486: Training Loss: 0.15725589295228323 Validation Loss: 0.7363975048065186\n",
      "Epoch 6487: Training Loss: 0.1570828507343928 Validation Loss: 0.7365082502365112\n",
      "Epoch 6488: Training Loss: 0.15723240872224173 Validation Loss: 0.7370836734771729\n",
      "Epoch 6489: Training Loss: 0.15736623108386993 Validation Loss: 0.736838698387146\n",
      "Epoch 6490: Training Loss: 0.15712906420230865 Validation Loss: 0.7366214990615845\n",
      "Epoch 6491: Training Loss: 0.1572957287232081 Validation Loss: 0.7368170022964478\n",
      "Epoch 6492: Training Loss: 0.1571569194396337 Validation Loss: 0.7368009686470032\n",
      "Epoch 6493: Training Loss: 0.15710489451885223 Validation Loss: 0.7368237972259521\n",
      "Epoch 6494: Training Loss: 0.15699584782123566 Validation Loss: 0.7361762523651123\n",
      "Epoch 6495: Training Loss: 0.15686034659544626 Validation Loss: 0.7365482449531555\n",
      "Epoch 6496: Training Loss: 0.15715246399243674 Validation Loss: 0.736792266368866\n",
      "Epoch 6497: Training Loss: 0.1576160341501236 Validation Loss: 0.7374244332313538\n",
      "Epoch 6498: Training Loss: 0.1581224004427592 Validation Loss: 0.7371943593025208\n",
      "Epoch 6499: Training Loss: 0.15669823189576468 Validation Loss: 0.7371119856834412\n",
      "Epoch 6500: Training Loss: 0.1570215423901876 Validation Loss: 0.7365094423294067\n",
      "Epoch 6501: Training Loss: 0.15726148088773093 Validation Loss: 0.7368342280387878\n",
      "Epoch 6502: Training Loss: 0.1570169379313787 Validation Loss: 0.7367958426475525\n",
      "Epoch 6503: Training Loss: 0.15712842841943106 Validation Loss: 0.7375314235687256\n",
      "Epoch 6504: Training Loss: 0.15693857272466025 Validation Loss: 0.7372138500213623\n",
      "Epoch 6505: Training Loss: 0.15710735817750296 Validation Loss: 0.7362702488899231\n",
      "Epoch 6506: Training Loss: 0.15688245495160422 Validation Loss: 0.7362949848175049\n",
      "Epoch 6507: Training Loss: 0.15700866281986237 Validation Loss: 0.7366964817047119\n",
      "Epoch 6508: Training Loss: 0.15693451464176178 Validation Loss: 0.7368141412734985\n",
      "Epoch 6509: Training Loss: 0.1570169379313787 Validation Loss: 0.7366678714752197\n",
      "Epoch 6510: Training Loss: 0.15682033201058707 Validation Loss: 0.7374153137207031\n",
      "Epoch 6511: Training Loss: 0.1568449834982554 Validation Loss: 0.7372553944587708\n",
      "Epoch 6512: Training Loss: 0.15702000757058462 Validation Loss: 0.7365893125534058\n",
      "Epoch 6513: Training Loss: 0.15645936131477356 Validation Loss: 0.7366657853126526\n",
      "Epoch 6514: Training Loss: 0.1571701616048813 Validation Loss: 0.7369508147239685\n",
      "Epoch 6515: Training Loss: 0.1572821189959844 Validation Loss: 0.7367017865180969\n",
      "Epoch 6516: Training Loss: 0.1567568232615789 Validation Loss: 0.7371222376823425\n",
      "Epoch 6517: Training Loss: 0.15660970906416574 Validation Loss: 0.736751139163971\n",
      "Epoch 6518: Training Loss: 0.15702906250953674 Validation Loss: 0.7368543744087219\n",
      "Epoch 6519: Training Loss: 0.15680451691150665 Validation Loss: 0.7376176118850708\n",
      "Epoch 6520: Training Loss: 0.15657417476177216 Validation Loss: 0.7373225688934326\n",
      "Epoch 6521: Training Loss: 0.1564637025197347 Validation Loss: 0.7370714545249939\n",
      "Epoch 6522: Training Loss: 0.15671699742476145 Validation Loss: 0.7369465231895447\n",
      "Epoch 6523: Training Loss: 0.1567292461792628 Validation Loss: 0.7370939254760742\n",
      "Epoch 6524: Training Loss: 0.1568871388832728 Validation Loss: 0.7378731369972229\n",
      "Epoch 6525: Training Loss: 0.1564312775929769 Validation Loss: 0.7380678057670593\n",
      "Epoch 6526: Training Loss: 0.15634466211001077 Validation Loss: 0.7379149198532104\n",
      "Epoch 6527: Training Loss: 0.15650282303492227 Validation Loss: 0.7372059226036072\n",
      "Epoch 6528: Training Loss: 0.15672804415225983 Validation Loss: 0.7373005747795105\n",
      "Epoch 6529: Training Loss: 0.15649762252966562 Validation Loss: 0.7372166514396667\n",
      "Epoch 6530: Training Loss: 0.15621517598628998 Validation Loss: 0.7367361783981323\n",
      "Epoch 6531: Training Loss: 0.15659166375796 Validation Loss: 0.736748993396759\n",
      "Epoch 6532: Training Loss: 0.1565349300702413 Validation Loss: 0.7373207211494446\n",
      "Epoch 6533: Training Loss: 0.15707186361153921 Validation Loss: 0.7371965646743774\n",
      "Epoch 6534: Training Loss: 0.15770559509595236 Validation Loss: 0.7369234561920166\n",
      "Epoch 6535: Training Loss: 0.15629779795805612 Validation Loss: 0.7372294664382935\n",
      "Epoch 6536: Training Loss: 0.15616421401500702 Validation Loss: 0.7376028895378113\n",
      "Epoch 6537: Training Loss: 0.15608038504918417 Validation Loss: 0.7372139692306519\n",
      "Epoch 6538: Training Loss: 0.15625696380933127 Validation Loss: 0.7365930676460266\n",
      "Epoch 6539: Training Loss: 0.1570433477560679 Validation Loss: 0.7368384003639221\n",
      "Epoch 6540: Training Loss: 0.1565796434879303 Validation Loss: 0.7369617819786072\n",
      "Epoch 6541: Training Loss: 0.15634040534496307 Validation Loss: 0.7375902533531189\n",
      "Epoch 6542: Training Loss: 0.15576912959416708 Validation Loss: 0.7372164726257324\n",
      "Epoch 6543: Training Loss: 0.15595819056034088 Validation Loss: 0.7370477914810181\n",
      "Epoch 6544: Training Loss: 0.15571904182434082 Validation Loss: 0.7368894219398499\n",
      "Epoch 6545: Training Loss: 0.15658521155516306 Validation Loss: 0.7375319004058838\n",
      "Epoch 6546: Training Loss: 0.1560985048611959 Validation Loss: 0.7375802993774414\n",
      "Epoch 6547: Training Loss: 0.1558874398469925 Validation Loss: 0.7377682328224182\n",
      "Epoch 6548: Training Loss: 0.156523659825325 Validation Loss: 0.7366750836372375\n",
      "Epoch 6549: Training Loss: 0.15589059392611185 Validation Loss: 0.7375739812850952\n",
      "Epoch 6550: Training Loss: 0.1555892527103424 Validation Loss: 0.7379055619239807\n",
      "Epoch 6551: Training Loss: 0.15655745814243952 Validation Loss: 0.7377035021781921\n",
      "Epoch 6552: Training Loss: 0.15610923866430917 Validation Loss: 0.7377179861068726\n",
      "Epoch 6553: Training Loss: 0.15571068227291107 Validation Loss: 0.7375453114509583\n",
      "Epoch 6554: Training Loss: 0.15612972776095072 Validation Loss: 0.7378451228141785\n",
      "Epoch 6555: Training Loss: 0.15682839850584665 Validation Loss: 0.7371050119400024\n",
      "Epoch 6556: Training Loss: 0.156174307068189 Validation Loss: 0.7368301153182983\n",
      "Epoch 6557: Training Loss: 0.15584414700667062 Validation Loss: 0.7374043464660645\n",
      "Epoch 6558: Training Loss: 0.15618064006169638 Validation Loss: 0.7377992868423462\n",
      "Epoch 6559: Training Loss: 0.15626210967699686 Validation Loss: 0.7378113269805908\n",
      "Epoch 6560: Training Loss: 0.15553683042526245 Validation Loss: 0.7376319766044617\n",
      "Epoch 6561: Training Loss: 0.15669668714205423 Validation Loss: 0.7367949485778809\n",
      "Epoch 6562: Training Loss: 0.15543817480405173 Validation Loss: 0.7370836734771729\n",
      "Epoch 6563: Training Loss: 0.1560147205988566 Validation Loss: 0.7370847463607788\n",
      "Epoch 6564: Training Loss: 0.15563001235326132 Validation Loss: 0.7375947833061218\n",
      "Epoch 6565: Training Loss: 0.15583296616872153 Validation Loss: 0.7378296852111816\n",
      "Epoch 6566: Training Loss: 0.15617271761099497 Validation Loss: 0.7380935549736023\n",
      "Epoch 6567: Training Loss: 0.15611901382605234 Validation Loss: 0.7379822134971619\n",
      "Epoch 6568: Training Loss: 0.15554922819137573 Validation Loss: 0.7374837398529053\n",
      "Epoch 6569: Training Loss: 0.15598590672016144 Validation Loss: 0.7367000579833984\n",
      "Epoch 6570: Training Loss: 0.15563233693440756 Validation Loss: 0.736739993095398\n",
      "Epoch 6571: Training Loss: 0.15535456935564676 Validation Loss: 0.7372608184814453\n",
      "Epoch 6572: Training Loss: 0.15607700745264688 Validation Loss: 0.7375754714012146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6573: Training Loss: 0.15563431878884634 Validation Loss: 0.7368430495262146\n",
      "Epoch 6574: Training Loss: 0.1553255965312322 Validation Loss: 0.7373114228248596\n",
      "Epoch 6575: Training Loss: 0.15517030159632364 Validation Loss: 0.7377500534057617\n",
      "Epoch 6576: Training Loss: 0.1552730848391851 Validation Loss: 0.7379392981529236\n",
      "Epoch 6577: Training Loss: 0.1554218033949534 Validation Loss: 0.7376523613929749\n",
      "Epoch 6578: Training Loss: 0.15569616357485452 Validation Loss: 0.737705409526825\n",
      "Epoch 6579: Training Loss: 0.15518361826737723 Validation Loss: 0.7375430464744568\n",
      "Epoch 6580: Training Loss: 0.15524545311927795 Validation Loss: 0.7370665073394775\n",
      "Epoch 6581: Training Loss: 0.15580540398756662 Validation Loss: 0.7370520830154419\n",
      "Epoch 6582: Training Loss: 0.15534425775210062 Validation Loss: 0.73716139793396\n",
      "Epoch 6583: Training Loss: 0.1551909645398458 Validation Loss: 0.7375892400741577\n",
      "Epoch 6584: Training Loss: 0.15527223547299704 Validation Loss: 0.7379967570304871\n",
      "Epoch 6585: Training Loss: 0.15509762366612753 Validation Loss: 0.738139808177948\n",
      "Epoch 6586: Training Loss: 0.1557498723268509 Validation Loss: 0.7380151748657227\n",
      "Epoch 6587: Training Loss: 0.15473991632461548 Validation Loss: 0.737731397151947\n",
      "Epoch 6588: Training Loss: 0.15491126974423727 Validation Loss: 0.7374654412269592\n",
      "Epoch 6589: Training Loss: 0.15456520517667136 Validation Loss: 0.7371847629547119\n",
      "Epoch 6590: Training Loss: 0.15536858638127646 Validation Loss: 0.7373363971710205\n",
      "Epoch 6591: Training Loss: 0.15522976716359457 Validation Loss: 0.7380163669586182\n",
      "Epoch 6592: Training Loss: 0.1548779010772705 Validation Loss: 0.7381380796432495\n",
      "Epoch 6593: Training Loss: 0.1552384446064631 Validation Loss: 0.7378019094467163\n",
      "Epoch 6594: Training Loss: 0.15513198574384054 Validation Loss: 0.7376212477684021\n",
      "Epoch 6595: Training Loss: 0.15516887108484903 Validation Loss: 0.7374061942100525\n",
      "Epoch 6596: Training Loss: 0.15515291690826416 Validation Loss: 0.7380293607711792\n",
      "Epoch 6597: Training Loss: 0.15488484501838684 Validation Loss: 0.7378485798835754\n",
      "Epoch 6598: Training Loss: 0.1552953670422236 Validation Loss: 0.7379595041275024\n",
      "Epoch 6599: Training Loss: 0.15478986501693726 Validation Loss: 0.7381321787834167\n",
      "Epoch 6600: Training Loss: 0.15550030767917633 Validation Loss: 0.7383484840393066\n",
      "Epoch 6601: Training Loss: 0.15550610423088074 Validation Loss: 0.7374239563941956\n",
      "Epoch 6602: Training Loss: 0.15483225385348 Validation Loss: 0.7366206049919128\n",
      "Epoch 6603: Training Loss: 0.15486940244833627 Validation Loss: 0.7371089458465576\n",
      "Epoch 6604: Training Loss: 0.15458466609319052 Validation Loss: 0.7375456690788269\n",
      "Epoch 6605: Training Loss: 0.15557793527841568 Validation Loss: 0.7370523810386658\n",
      "Epoch 6606: Training Loss: 0.15446404616038004 Validation Loss: 0.7377321124076843\n",
      "Epoch 6607: Training Loss: 0.15460091829299927 Validation Loss: 0.7383463382720947\n",
      "Epoch 6608: Training Loss: 0.15454336007436117 Validation Loss: 0.7385829091072083\n",
      "Epoch 6609: Training Loss: 0.15439819792906442 Validation Loss: 0.7383215427398682\n",
      "Epoch 6610: Training Loss: 0.15463762482007345 Validation Loss: 0.737882137298584\n",
      "Epoch 6611: Training Loss: 0.15461845199267069 Validation Loss: 0.7373660802841187\n",
      "Epoch 6612: Training Loss: 0.15601787467797598 Validation Loss: 0.7373810410499573\n",
      "Epoch 6613: Training Loss: 0.15451114376386008 Validation Loss: 0.7372153401374817\n",
      "Epoch 6614: Training Loss: 0.1544576237599055 Validation Loss: 0.737751305103302\n",
      "Epoch 6615: Training Loss: 0.15450040996074677 Validation Loss: 0.7385512590408325\n",
      "Epoch 6616: Training Loss: 0.15446686744689941 Validation Loss: 0.7382370829582214\n",
      "Epoch 6617: Training Loss: 0.15445501108964285 Validation Loss: 0.7377195358276367\n",
      "Epoch 6618: Training Loss: 0.15442730486392975 Validation Loss: 0.7371581196784973\n",
      "Epoch 6619: Training Loss: 0.1547426680723826 Validation Loss: 0.7369527816772461\n",
      "Epoch 6620: Training Loss: 0.15405244131882986 Validation Loss: 0.7372292876243591\n",
      "Epoch 6621: Training Loss: 0.15630691250165304 Validation Loss: 0.7379235029220581\n",
      "Epoch 6622: Training Loss: 0.15498278041680655 Validation Loss: 0.7374745011329651\n",
      "Epoch 6623: Training Loss: 0.15486069520314535 Validation Loss: 0.7373111844062805\n",
      "Epoch 6624: Training Loss: 0.15440942843755087 Validation Loss: 0.7370772957801819\n",
      "Epoch 6625: Training Loss: 0.154545396566391 Validation Loss: 0.7374987602233887\n",
      "Epoch 6626: Training Loss: 0.15477773050467172 Validation Loss: 0.7387592196464539\n",
      "Epoch 6627: Training Loss: 0.15446784098943075 Validation Loss: 0.7388932704925537\n",
      "Epoch 6628: Training Loss: 0.15458124379316965 Validation Loss: 0.7379674315452576\n",
      "Epoch 6629: Training Loss: 0.15440361698468527 Validation Loss: 0.7375017404556274\n",
      "Epoch 6630: Training Loss: 0.15427400171756744 Validation Loss: 0.7375366687774658\n",
      "Epoch 6631: Training Loss: 0.1542392075061798 Validation Loss: 0.7380616664886475\n",
      "Epoch 6632: Training Loss: 0.15430128077665964 Validation Loss: 0.7385625243186951\n",
      "Epoch 6633: Training Loss: 0.15422392388184866 Validation Loss: 0.7385160326957703\n",
      "Epoch 6634: Training Loss: 0.15458117425441742 Validation Loss: 0.737448513507843\n",
      "Epoch 6635: Training Loss: 0.1542461613814036 Validation Loss: 0.738115668296814\n",
      "Epoch 6636: Training Loss: 0.15432997544606528 Validation Loss: 0.737673282623291\n",
      "Epoch 6637: Training Loss: 0.153949573636055 Validation Loss: 0.7378697395324707\n",
      "Epoch 6638: Training Loss: 0.15411919355392456 Validation Loss: 0.7385106086730957\n",
      "Epoch 6639: Training Loss: 0.1544379492600759 Validation Loss: 0.7383849620819092\n",
      "Epoch 6640: Training Loss: 0.15401014188925424 Validation Loss: 0.7381854057312012\n",
      "Epoch 6641: Training Loss: 0.15397020677725473 Validation Loss: 0.7386277318000793\n",
      "Epoch 6642: Training Loss: 0.1535546282927195 Validation Loss: 0.7384529709815979\n",
      "Epoch 6643: Training Loss: 0.1541947921117147 Validation Loss: 0.7379005551338196\n",
      "Epoch 6644: Training Loss: 0.15467338263988495 Validation Loss: 0.7377427220344543\n",
      "Epoch 6645: Training Loss: 0.15550817797581354 Validation Loss: 0.7380145192146301\n",
      "Epoch 6646: Training Loss: 0.15353182454903921 Validation Loss: 0.7387240529060364\n",
      "Epoch 6647: Training Loss: 0.1545756459236145 Validation Loss: 0.7388639450073242\n",
      "Epoch 6648: Training Loss: 0.1538793444633484 Validation Loss: 0.739096462726593\n",
      "Epoch 6649: Training Loss: 0.1541099101305008 Validation Loss: 0.7377923130989075\n",
      "Epoch 6650: Training Loss: 0.15450550119082132 Validation Loss: 0.7379686832427979\n",
      "Epoch 6651: Training Loss: 0.15365427235762277 Validation Loss: 0.737411379814148\n",
      "Epoch 6652: Training Loss: 0.15454531709353128 Validation Loss: 0.7378029227256775\n",
      "Epoch 6653: Training Loss: 0.15372813741366068 Validation Loss: 0.7382091879844666\n",
      "Epoch 6654: Training Loss: 0.15386560062567392 Validation Loss: 0.7383199334144592\n",
      "Epoch 6655: Training Loss: 0.15363792578379312 Validation Loss: 0.7380028963088989\n",
      "Epoch 6656: Training Loss: 0.15452609459559122 Validation Loss: 0.7386242151260376\n",
      "Epoch 6657: Training Loss: 0.15383254985014597 Validation Loss: 0.7385460734367371\n",
      "Epoch 6658: Training Loss: 0.15355301400025687 Validation Loss: 0.7376542091369629\n",
      "Epoch 6659: Training Loss: 0.15406467020511627 Validation Loss: 0.7371737360954285\n",
      "Epoch 6660: Training Loss: 0.1539360781510671 Validation Loss: 0.7373883724212646\n",
      "Epoch 6661: Training Loss: 0.1533928910891215 Validation Loss: 0.7376392483711243\n",
      "Epoch 6662: Training Loss: 0.15371629099051157 Validation Loss: 0.7378621101379395\n",
      "Epoch 6663: Training Loss: 0.15362841387589773 Validation Loss: 0.7390897274017334\n",
      "Epoch 6664: Training Loss: 0.1546422392129898 Validation Loss: 0.7388161420822144\n",
      "Epoch 6665: Training Loss: 0.1537770926952362 Validation Loss: 0.7386050224304199\n",
      "Epoch 6666: Training Loss: 0.15330402553081512 Validation Loss: 0.7377121448516846\n",
      "Epoch 6667: Training Loss: 0.15368124842643738 Validation Loss: 0.7379578351974487\n",
      "Epoch 6668: Training Loss: 0.15370780229568481 Validation Loss: 0.737873375415802\n",
      "Epoch 6669: Training Loss: 0.15351047615210214 Validation Loss: 0.7379598617553711\n",
      "Epoch 6670: Training Loss: 0.1532613088687261 Validation Loss: 0.7384551167488098\n",
      "Epoch 6671: Training Loss: 0.15354092915852866 Validation Loss: 0.7385089993476868\n",
      "Epoch 6672: Training Loss: 0.15328757464885712 Validation Loss: 0.738806962966919\n",
      "Epoch 6673: Training Loss: 0.15338166058063507 Validation Loss: 0.7390056252479553\n",
      "Epoch 6674: Training Loss: 0.15336275100708008 Validation Loss: 0.7379740476608276\n",
      "Epoch 6675: Training Loss: 0.1532728523015976 Validation Loss: 0.737912118434906\n",
      "Epoch 6676: Training Loss: 0.15310417115688324 Validation Loss: 0.737369179725647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6677: Training Loss: 0.15355800092220306 Validation Loss: 0.738384485244751\n",
      "Epoch 6678: Training Loss: 0.15332515041033426 Validation Loss: 0.7384041547775269\n",
      "Epoch 6679: Training Loss: 0.1533153106768926 Validation Loss: 0.7382366061210632\n",
      "Epoch 6680: Training Loss: 0.1541754404703776 Validation Loss: 0.7389461994171143\n",
      "Epoch 6681: Training Loss: 0.15299423038959503 Validation Loss: 0.7384275197982788\n",
      "Epoch 6682: Training Loss: 0.15356196463108063 Validation Loss: 0.7386741042137146\n",
      "Epoch 6683: Training Loss: 0.15323498845100403 Validation Loss: 0.7384210228919983\n",
      "Epoch 6684: Training Loss: 0.15325827399889627 Validation Loss: 0.7380273342132568\n",
      "Epoch 6685: Training Loss: 0.1531983365615209 Validation Loss: 0.7378261089324951\n",
      "Epoch 6686: Training Loss: 0.15320985515912375 Validation Loss: 0.7378670573234558\n",
      "Epoch 6687: Training Loss: 0.15345928072929382 Validation Loss: 0.7380466461181641\n",
      "Epoch 6688: Training Loss: 0.15303386251131693 Validation Loss: 0.7382656931877136\n",
      "Epoch 6689: Training Loss: 0.15330741802851358 Validation Loss: 0.73881995677948\n",
      "Epoch 6690: Training Loss: 0.15320946276187897 Validation Loss: 0.7395702600479126\n",
      "Epoch 6691: Training Loss: 0.1529233753681183 Validation Loss: 0.7395936846733093\n",
      "Epoch 6692: Training Loss: 0.15307530264059702 Validation Loss: 0.7389953136444092\n",
      "Epoch 6693: Training Loss: 0.1526108185450236 Validation Loss: 0.7385854721069336\n",
      "Epoch 6694: Training Loss: 0.1526644378900528 Validation Loss: 0.7378073334693909\n",
      "Epoch 6695: Training Loss: 0.15288050969441733 Validation Loss: 0.737147331237793\n",
      "Epoch 6696: Training Loss: 0.1528137524922689 Validation Loss: 0.7376204133033752\n",
      "Epoch 6697: Training Loss: 0.1527453511953354 Validation Loss: 0.7380867600440979\n",
      "Epoch 6698: Training Loss: 0.15280462801456451 Validation Loss: 0.7383880019187927\n",
      "Epoch 6699: Training Loss: 0.15282641847928366 Validation Loss: 0.738088846206665\n",
      "Epoch 6700: Training Loss: 0.15283233424027762 Validation Loss: 0.7381023168563843\n",
      "Epoch 6701: Training Loss: 0.15259412924448648 Validation Loss: 0.737527072429657\n",
      "Epoch 6702: Training Loss: 0.1530822366476059 Validation Loss: 0.7375178933143616\n",
      "Epoch 6703: Training Loss: 0.1530018945535024 Validation Loss: 0.7382552623748779\n",
      "Epoch 6704: Training Loss: 0.15299352506796518 Validation Loss: 0.7392272353172302\n",
      "Epoch 6705: Training Loss: 0.1525373508532842 Validation Loss: 0.739189863204956\n",
      "Epoch 6706: Training Loss: 0.1533927619457245 Validation Loss: 0.7389410138130188\n",
      "Epoch 6707: Training Loss: 0.15279378990332285 Validation Loss: 0.7394832372665405\n",
      "Epoch 6708: Training Loss: 0.15260830521583557 Validation Loss: 0.7393757700920105\n",
      "Epoch 6709: Training Loss: 0.15249445537726083 Validation Loss: 0.7390270829200745\n",
      "Epoch 6710: Training Loss: 0.15267103910446167 Validation Loss: 0.7385419011116028\n",
      "Epoch 6711: Training Loss: 0.15255264937877655 Validation Loss: 0.7383203506469727\n",
      "Epoch 6712: Training Loss: 0.15234003961086273 Validation Loss: 0.7387363910675049\n",
      "Epoch 6713: Training Loss: 0.15267588198184967 Validation Loss: 0.7394378185272217\n",
      "Epoch 6714: Training Loss: 0.15305809179941812 Validation Loss: 0.7395910024642944\n",
      "Epoch 6715: Training Loss: 0.15273653467496237 Validation Loss: 0.7391188740730286\n",
      "Epoch 6716: Training Loss: 0.15226716796557108 Validation Loss: 0.7390532493591309\n",
      "Epoch 6717: Training Loss: 0.1525536080201467 Validation Loss: 0.7384627461433411\n",
      "Epoch 6718: Training Loss: 0.15244113902250925 Validation Loss: 0.7381747364997864\n",
      "Epoch 6719: Training Loss: 0.15252573788166046 Validation Loss: 0.7388085722923279\n",
      "Epoch 6720: Training Loss: 0.15220322211583456 Validation Loss: 0.7385458946228027\n",
      "Epoch 6721: Training Loss: 0.15233697493871054 Validation Loss: 0.7384932041168213\n",
      "Epoch 6722: Training Loss: 0.15290533999602 Validation Loss: 0.7382726073265076\n",
      "Epoch 6723: Training Loss: 0.1522149940331777 Validation Loss: 0.737981379032135\n",
      "Epoch 6724: Training Loss: 0.15256723761558533 Validation Loss: 0.7384376525878906\n",
      "Epoch 6725: Training Loss: 0.15232001741727194 Validation Loss: 0.7383545637130737\n",
      "Epoch 6726: Training Loss: 0.1531179795662562 Validation Loss: 0.7383565902709961\n",
      "Epoch 6727: Training Loss: 0.1524910181760788 Validation Loss: 0.7387749552726746\n",
      "Epoch 6728: Training Loss: 0.15232170124848685 Validation Loss: 0.7386206388473511\n",
      "Epoch 6729: Training Loss: 0.15248067180315653 Validation Loss: 0.7382774353027344\n",
      "Epoch 6730: Training Loss: 0.15198091169198355 Validation Loss: 0.7380517721176147\n",
      "Epoch 6731: Training Loss: 0.15175304810206094 Validation Loss: 0.7384229898452759\n",
      "Epoch 6732: Training Loss: 0.15208967526753744 Validation Loss: 0.7388149499893188\n",
      "Epoch 6733: Training Loss: 0.1521893839041392 Validation Loss: 0.7386327981948853\n",
      "Epoch 6734: Training Loss: 0.15223428110281625 Validation Loss: 0.73856520652771\n",
      "Epoch 6735: Training Loss: 0.15207664171854654 Validation Loss: 0.7386407852172852\n",
      "Epoch 6736: Training Loss: 0.15249777336915335 Validation Loss: 0.7390182614326477\n",
      "Epoch 6737: Training Loss: 0.1524686465660731 Validation Loss: 0.7389422655105591\n",
      "Epoch 6738: Training Loss: 0.1525141050418218 Validation Loss: 0.7389163374900818\n",
      "Epoch 6739: Training Loss: 0.1523402233918508 Validation Loss: 0.7397559285163879\n",
      "Epoch 6740: Training Loss: 0.15211635331312814 Validation Loss: 0.7390671372413635\n",
      "Epoch 6741: Training Loss: 0.15203693012396494 Validation Loss: 0.738642156124115\n",
      "Epoch 6742: Training Loss: 0.15188456575075784 Validation Loss: 0.7387664914131165\n",
      "Epoch 6743: Training Loss: 0.1518708219130834 Validation Loss: 0.7392375469207764\n",
      "Epoch 6744: Training Loss: 0.1517501324415207 Validation Loss: 0.739469051361084\n",
      "Epoch 6745: Training Loss: 0.15245890617370605 Validation Loss: 0.7401179075241089\n",
      "Epoch 6746: Training Loss: 0.15208429594834647 Validation Loss: 0.740073561668396\n",
      "Epoch 6747: Training Loss: 0.15246413896481195 Validation Loss: 0.7388966679573059\n",
      "Epoch 6748: Training Loss: 0.15191312630971274 Validation Loss: 0.7384772896766663\n",
      "Epoch 6749: Training Loss: 0.15172321597735086 Validation Loss: 0.7387903332710266\n",
      "Epoch 6750: Training Loss: 0.15189926822980246 Validation Loss: 0.7388829588890076\n",
      "Epoch 6751: Training Loss: 0.1524837464094162 Validation Loss: 0.7385427355766296\n",
      "Epoch 6752: Training Loss: 0.15181458493073782 Validation Loss: 0.7389229536056519\n",
      "Epoch 6753: Training Loss: 0.15169966220855713 Validation Loss: 0.7389465570449829\n",
      "Epoch 6754: Training Loss: 0.1521486441294352 Validation Loss: 0.7390624284744263\n",
      "Epoch 6755: Training Loss: 0.15211205184459686 Validation Loss: 0.7392145395278931\n",
      "Epoch 6756: Training Loss: 0.15162901083628336 Validation Loss: 0.7384147047996521\n",
      "Epoch 6757: Training Loss: 0.15166464944680533 Validation Loss: 0.7382429838180542\n",
      "Epoch 6758: Training Loss: 0.15143674612045288 Validation Loss: 0.7382069230079651\n",
      "Epoch 6759: Training Loss: 0.15179891884326935 Validation Loss: 0.7386575937271118\n",
      "Epoch 6760: Training Loss: 0.15192248423894247 Validation Loss: 0.7386284470558167\n",
      "Epoch 6761: Training Loss: 0.1517771234114965 Validation Loss: 0.738818347454071\n",
      "Epoch 6762: Training Loss: 0.1517955164114634 Validation Loss: 0.7391520142555237\n",
      "Epoch 6763: Training Loss: 0.15234571198622385 Validation Loss: 0.7396164536476135\n",
      "Epoch 6764: Training Loss: 0.1515090117851893 Validation Loss: 0.7394188046455383\n",
      "Epoch 6765: Training Loss: 0.15172955890496573 Validation Loss: 0.739841639995575\n",
      "Epoch 6766: Training Loss: 0.15171287457148233 Validation Loss: 0.7394254803657532\n",
      "Epoch 6767: Training Loss: 0.15184665222962698 Validation Loss: 0.7393896579742432\n",
      "Epoch 6768: Training Loss: 0.15211939811706543 Validation Loss: 0.7390305995941162\n",
      "Epoch 6769: Training Loss: 0.15221825242042542 Validation Loss: 0.7395217418670654\n",
      "Epoch 6770: Training Loss: 0.15151231487592062 Validation Loss: 0.7394768595695496\n",
      "Epoch 6771: Training Loss: 0.15283105274041495 Validation Loss: 0.7385713458061218\n",
      "Epoch 6772: Training Loss: 0.15158611536026 Validation Loss: 0.7389522194862366\n",
      "Epoch 6773: Training Loss: 0.15173564354578653 Validation Loss: 0.7391490340232849\n",
      "Epoch 6774: Training Loss: 0.1511224905649821 Validation Loss: 0.7394023537635803\n",
      "Epoch 6775: Training Loss: 0.15109616021315256 Validation Loss: 0.7393258810043335\n",
      "Epoch 6776: Training Loss: 0.15182592968146005 Validation Loss: 0.7389861941337585\n",
      "Epoch 6777: Training Loss: 0.15098159511884054 Validation Loss: 0.7391093373298645\n",
      "Epoch 6778: Training Loss: 0.1512498160203298 Validation Loss: 0.7389339208602905\n",
      "Epoch 6779: Training Loss: 0.15114358564217886 Validation Loss: 0.7389156222343445\n",
      "Epoch 6780: Training Loss: 0.15084861715634665 Validation Loss: 0.7383004426956177\n",
      "Epoch 6781: Training Loss: 0.15126530329386392 Validation Loss: 0.7381585240364075\n",
      "Epoch 6782: Training Loss: 0.15133076906204224 Validation Loss: 0.7388394474983215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6783: Training Loss: 0.15137108663717905 Validation Loss: 0.7395769357681274\n",
      "Epoch 6784: Training Loss: 0.1515560249487559 Validation Loss: 0.7406172752380371\n",
      "Epoch 6785: Training Loss: 0.15108896791934967 Validation Loss: 0.7401580810546875\n",
      "Epoch 6786: Training Loss: 0.15149447321891785 Validation Loss: 0.7391775250434875\n",
      "Epoch 6787: Training Loss: 0.15106773873170218 Validation Loss: 0.7388628721237183\n",
      "Epoch 6788: Training Loss: 0.15072100361188254 Validation Loss: 0.7385081052780151\n",
      "Epoch 6789: Training Loss: 0.1511357476313909 Validation Loss: 0.7388460040092468\n",
      "Epoch 6790: Training Loss: 0.15098053216934204 Validation Loss: 0.7396854758262634\n",
      "Epoch 6791: Training Loss: 0.15112572411696115 Validation Loss: 0.7399376630783081\n",
      "Epoch 6792: Training Loss: 0.15140476822853088 Validation Loss: 0.7399716377258301\n",
      "Epoch 6793: Training Loss: 0.1511340538660685 Validation Loss: 0.7398694157600403\n",
      "Epoch 6794: Training Loss: 0.1510466585556666 Validation Loss: 0.738804817199707\n",
      "Epoch 6795: Training Loss: 0.1509951949119568 Validation Loss: 0.7388262748718262\n",
      "Epoch 6796: Training Loss: 0.15103701253732046 Validation Loss: 0.7389840483665466\n",
      "Epoch 6797: Training Loss: 0.1506468007961909 Validation Loss: 0.7390967011451721\n",
      "Epoch 6798: Training Loss: 0.15050180753072104 Validation Loss: 0.7390421628952026\n",
      "Epoch 6799: Training Loss: 0.1511588195959727 Validation Loss: 0.7394262552261353\n",
      "Epoch 6800: Training Loss: 0.15091457962989807 Validation Loss: 0.7393421530723572\n",
      "Epoch 6801: Training Loss: 0.15104742844899496 Validation Loss: 0.7391185760498047\n",
      "Epoch 6802: Training Loss: 0.1507329841454824 Validation Loss: 0.7390197515487671\n",
      "Epoch 6803: Training Loss: 0.1510325918594996 Validation Loss: 0.739285945892334\n",
      "Epoch 6804: Training Loss: 0.15071465075016022 Validation Loss: 0.7392650246620178\n",
      "Epoch 6805: Training Loss: 0.15049259861310324 Validation Loss: 0.7393826246261597\n",
      "Epoch 6806: Training Loss: 0.15039643148581186 Validation Loss: 0.7395119071006775\n",
      "Epoch 6807: Training Loss: 0.15073815484841666 Validation Loss: 0.7397210001945496\n",
      "Epoch 6808: Training Loss: 0.1513358255227407 Validation Loss: 0.739847719669342\n",
      "Epoch 6809: Training Loss: 0.15044262011845908 Validation Loss: 0.7396209239959717\n",
      "Epoch 6810: Training Loss: 0.15080268681049347 Validation Loss: 0.739861249923706\n",
      "Epoch 6811: Training Loss: 0.15053780376911163 Validation Loss: 0.7396559119224548\n",
      "Epoch 6812: Training Loss: 0.15095615883668265 Validation Loss: 0.7387690544128418\n",
      "Epoch 6813: Training Loss: 0.15050719678401947 Validation Loss: 0.7390184998512268\n",
      "Epoch 6814: Training Loss: 0.15049274265766144 Validation Loss: 0.7390400171279907\n",
      "Epoch 6815: Training Loss: 0.15062356491883597 Validation Loss: 0.7394653558731079\n",
      "Epoch 6816: Training Loss: 0.15019787351290384 Validation Loss: 0.7392748594284058\n",
      "Epoch 6817: Training Loss: 0.15029961367448172 Validation Loss: 0.7402507662773132\n",
      "Epoch 6818: Training Loss: 0.15061898032824197 Validation Loss: 0.7403483986854553\n",
      "Epoch 6819: Training Loss: 0.15031695365905762 Validation Loss: 0.7409283518791199\n",
      "Epoch 6820: Training Loss: 0.15049706399440765 Validation Loss: 0.7404087781906128\n",
      "Epoch 6821: Training Loss: 0.15016524493694305 Validation Loss: 0.7395251989364624\n",
      "Epoch 6822: Training Loss: 0.15041673183441162 Validation Loss: 0.73876953125\n",
      "Epoch 6823: Training Loss: 0.15026341378688812 Validation Loss: 0.7390230894088745\n",
      "Epoch 6824: Training Loss: 0.1502140462398529 Validation Loss: 0.739598274230957\n",
      "Epoch 6825: Training Loss: 0.15026195844014487 Validation Loss: 0.7395586371421814\n",
      "Epoch 6826: Training Loss: 0.14983424047629038 Validation Loss: 0.7394359707832336\n",
      "Epoch 6827: Training Loss: 0.1507054070631663 Validation Loss: 0.739732563495636\n",
      "Epoch 6828: Training Loss: 0.15010398626327515 Validation Loss: 0.7398316264152527\n",
      "Epoch 6829: Training Loss: 0.15036475161711374 Validation Loss: 0.7398577332496643\n",
      "Epoch 6830: Training Loss: 0.15051953991254172 Validation Loss: 0.7400495409965515\n",
      "Epoch 6831: Training Loss: 0.15040865043799082 Validation Loss: 0.7399528622627258\n",
      "Epoch 6832: Training Loss: 0.150685116648674 Validation Loss: 0.7398343086242676\n",
      "Epoch 6833: Training Loss: 0.15011781950791678 Validation Loss: 0.7398914098739624\n",
      "Epoch 6834: Training Loss: 0.15044287840525308 Validation Loss: 0.7399411201477051\n",
      "Epoch 6835: Training Loss: 0.15016829470793405 Validation Loss: 0.7400572896003723\n",
      "Epoch 6836: Training Loss: 0.1504984349012375 Validation Loss: 0.7400239706039429\n",
      "Epoch 6837: Training Loss: 0.15019609034061432 Validation Loss: 0.7391267418861389\n",
      "Epoch 6838: Training Loss: 0.15032078822453818 Validation Loss: 0.7391501069068909\n",
      "Epoch 6839: Training Loss: 0.14963915447394052 Validation Loss: 0.7393365502357483\n",
      "Epoch 6840: Training Loss: 0.15071960290273032 Validation Loss: 0.7398589253425598\n",
      "Epoch 6841: Training Loss: 0.14986143012841543 Validation Loss: 0.739802896976471\n",
      "Epoch 6842: Training Loss: 0.15009639412164688 Validation Loss: 0.7400994896888733\n",
      "Epoch 6843: Training Loss: 0.14995785057544708 Validation Loss: 0.7400789856910706\n",
      "Epoch 6844: Training Loss: 0.1499768247207006 Validation Loss: 0.7400559782981873\n",
      "Epoch 6845: Training Loss: 0.14984366794427237 Validation Loss: 0.7397805452346802\n",
      "Epoch 6846: Training Loss: 0.15092653532822928 Validation Loss: 0.7396692037582397\n",
      "Epoch 6847: Training Loss: 0.15022104481856027 Validation Loss: 0.7403720617294312\n",
      "Epoch 6848: Training Loss: 0.14982594549655914 Validation Loss: 0.7399028539657593\n",
      "Epoch 6849: Training Loss: 0.1496749222278595 Validation Loss: 0.7394914031028748\n",
      "Epoch 6850: Training Loss: 0.14992224673430124 Validation Loss: 0.7390335202217102\n",
      "Epoch 6851: Training Loss: 0.1499429096778234 Validation Loss: 0.7392302751541138\n",
      "Epoch 6852: Training Loss: 0.1499248594045639 Validation Loss: 0.7393322587013245\n",
      "Epoch 6853: Training Loss: 0.1498270481824875 Validation Loss: 0.7403634190559387\n",
      "Epoch 6854: Training Loss: 0.14988634486993155 Validation Loss: 0.7403802871704102\n",
      "Epoch 6855: Training Loss: 0.14975841343402863 Validation Loss: 0.7401204705238342\n",
      "Epoch 6856: Training Loss: 0.15097299218177795 Validation Loss: 0.7395097613334656\n",
      "Epoch 6857: Training Loss: 0.14966630935668945 Validation Loss: 0.7390422821044922\n",
      "Epoch 6858: Training Loss: 0.14962798357009888 Validation Loss: 0.7390098571777344\n",
      "Epoch 6859: Training Loss: 0.14911426107088724 Validation Loss: 0.7395163774490356\n",
      "Epoch 6860: Training Loss: 0.14938423534234366 Validation Loss: 0.7397188544273376\n",
      "Epoch 6861: Training Loss: 0.14999072750409445 Validation Loss: 0.7401769161224365\n",
      "Epoch 6862: Training Loss: 0.1497483750184377 Validation Loss: 0.7402762174606323\n",
      "Epoch 6863: Training Loss: 0.14967566231886545 Validation Loss: 0.7406563758850098\n",
      "Epoch 6864: Training Loss: 0.14955332378546396 Validation Loss: 0.7405449151992798\n",
      "Epoch 6865: Training Loss: 0.15010633071263632 Validation Loss: 0.7400451302528381\n",
      "Epoch 6866: Training Loss: 0.1496934046347936 Validation Loss: 0.7402564883232117\n",
      "Epoch 6867: Training Loss: 0.14960908889770508 Validation Loss: 0.7400341629981995\n",
      "Epoch 6868: Training Loss: 0.14947684109210968 Validation Loss: 0.7403820753097534\n",
      "Epoch 6869: Training Loss: 0.14979534844557443 Validation Loss: 0.7403562664985657\n",
      "Epoch 6870: Training Loss: 0.14931075274944305 Validation Loss: 0.739971399307251\n",
      "Epoch 6871: Training Loss: 0.1493071218331655 Validation Loss: 0.7395945191383362\n",
      "Epoch 6872: Training Loss: 0.14955412348111471 Validation Loss: 0.7393543124198914\n",
      "Epoch 6873: Training Loss: 0.14959339300791422 Validation Loss: 0.7391455173492432\n",
      "Epoch 6874: Training Loss: 0.14932577311992645 Validation Loss: 0.7399630546569824\n",
      "Epoch 6875: Training Loss: 0.1496166189511617 Validation Loss: 0.7404415607452393\n",
      "Epoch 6876: Training Loss: 0.15005875378847122 Validation Loss: 0.7401857376098633\n",
      "Epoch 6877: Training Loss: 0.14864802360534668 Validation Loss: 0.7397151589393616\n",
      "Epoch 6878: Training Loss: 0.14937139054139456 Validation Loss: 0.7397858500480652\n",
      "Epoch 6879: Training Loss: 0.14931949973106384 Validation Loss: 0.7397552728652954\n",
      "Epoch 6880: Training Loss: 0.14965803921222687 Validation Loss: 0.7397645115852356\n",
      "Epoch 6881: Training Loss: 0.14998850226402283 Validation Loss: 0.7404114603996277\n",
      "Epoch 6882: Training Loss: 0.15015496810277304 Validation Loss: 0.7407559752464294\n",
      "Epoch 6883: Training Loss: 0.14934998253981271 Validation Loss: 0.74072265625\n",
      "Epoch 6884: Training Loss: 0.15000182390213013 Validation Loss: 0.74041348695755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6885: Training Loss: 0.14932026465733847 Validation Loss: 0.7403252720832825\n",
      "Epoch 6886: Training Loss: 0.14893676340579987 Validation Loss: 0.7404259443283081\n",
      "Epoch 6887: Training Loss: 0.14958720405896506 Validation Loss: 0.7404120564460754\n",
      "Epoch 6888: Training Loss: 0.1491547872622808 Validation Loss: 0.740276575088501\n",
      "Epoch 6889: Training Loss: 0.14925605555375418 Validation Loss: 0.7396527528762817\n",
      "Epoch 6890: Training Loss: 0.1491680790980657 Validation Loss: 0.7401921153068542\n",
      "Epoch 6891: Training Loss: 0.14930051068464914 Validation Loss: 0.7408828139305115\n",
      "Epoch 6892: Training Loss: 0.14891975621382395 Validation Loss: 0.7404459714889526\n",
      "Epoch 6893: Training Loss: 0.14895108342170715 Validation Loss: 0.74003666639328\n",
      "Epoch 6894: Training Loss: 0.14912395675977072 Validation Loss: 0.7393790483474731\n",
      "Epoch 6895: Training Loss: 0.14918217062950134 Validation Loss: 0.7398102283477783\n",
      "Epoch 6896: Training Loss: 0.14917844533920288 Validation Loss: 0.7401940822601318\n",
      "Epoch 6897: Training Loss: 0.14906762540340424 Validation Loss: 0.740265965461731\n",
      "Epoch 6898: Training Loss: 0.14886756241321564 Validation Loss: 0.7408247590065002\n",
      "Epoch 6899: Training Loss: 0.149103045463562 Validation Loss: 0.7412397265434265\n",
      "Epoch 6900: Training Loss: 0.14944420754909515 Validation Loss: 0.7411887049674988\n",
      "Epoch 6901: Training Loss: 0.14883786936601004 Validation Loss: 0.7407987117767334\n",
      "Epoch 6902: Training Loss: 0.1486296753088633 Validation Loss: 0.7404183149337769\n",
      "Epoch 6903: Training Loss: 0.14914990961551666 Validation Loss: 0.7405216693878174\n",
      "Epoch 6904: Training Loss: 0.14945210268100104 Validation Loss: 0.7392322421073914\n",
      "Epoch 6905: Training Loss: 0.14871319135030112 Validation Loss: 0.7393949031829834\n",
      "Epoch 6906: Training Loss: 0.1484332432349523 Validation Loss: 0.7395697236061096\n",
      "Epoch 6907: Training Loss: 0.14934967458248138 Validation Loss: 0.740678071975708\n",
      "Epoch 6908: Training Loss: 0.14912852148214975 Validation Loss: 0.741079568862915\n",
      "Epoch 6909: Training Loss: 0.1494370698928833 Validation Loss: 0.741992175579071\n",
      "Epoch 6910: Training Loss: 0.14870595435301462 Validation Loss: 0.7410223484039307\n",
      "Epoch 6911: Training Loss: 0.14907083908716837 Validation Loss: 0.740027666091919\n",
      "Epoch 6912: Training Loss: 0.1494368463754654 Validation Loss: 0.7405598163604736\n",
      "Epoch 6913: Training Loss: 0.14828617870807648 Validation Loss: 0.7402457594871521\n",
      "Epoch 6914: Training Loss: 0.1490407039721807 Validation Loss: 0.7405662536621094\n",
      "Epoch 6915: Training Loss: 0.14865968624750772 Validation Loss: 0.74040287733078\n",
      "Epoch 6916: Training Loss: 0.14885463813940683 Validation Loss: 0.7407839298248291\n",
      "Epoch 6917: Training Loss: 0.14890768627325693 Validation Loss: 0.7401867508888245\n",
      "Epoch 6918: Training Loss: 0.14885571599006653 Validation Loss: 0.7410175204277039\n",
      "Epoch 6919: Training Loss: 0.1490062971909841 Validation Loss: 0.7412598133087158\n",
      "Epoch 6920: Training Loss: 0.14853251973787943 Validation Loss: 0.7403222918510437\n",
      "Epoch 6921: Training Loss: 0.14841262499491373 Validation Loss: 0.7399553060531616\n",
      "Epoch 6922: Training Loss: 0.1488043119510015 Validation Loss: 0.7398774027824402\n",
      "Epoch 6923: Training Loss: 0.14848868052164713 Validation Loss: 0.7399753332138062\n",
      "Epoch 6924: Training Loss: 0.1483920713265737 Validation Loss: 0.739839494228363\n",
      "Epoch 6925: Training Loss: 0.14996241529782614 Validation Loss: 0.7394167184829712\n",
      "Epoch 6926: Training Loss: 0.1483150174220403 Validation Loss: 0.7404630780220032\n",
      "Epoch 6927: Training Loss: 0.14834350844224295 Validation Loss: 0.7411518096923828\n",
      "Epoch 6928: Training Loss: 0.14816136161486307 Validation Loss: 0.741409957408905\n",
      "Epoch 6929: Training Loss: 0.1485846017797788 Validation Loss: 0.7412421703338623\n",
      "Epoch 6930: Training Loss: 0.1481861819823583 Validation Loss: 0.7407984137535095\n",
      "Epoch 6931: Training Loss: 0.14841649929682413 Validation Loss: 0.7406827211380005\n",
      "Epoch 6932: Training Loss: 0.14846767981847128 Validation Loss: 0.7406535148620605\n",
      "Epoch 6933: Training Loss: 0.14851335684458414 Validation Loss: 0.7405720949172974\n",
      "Epoch 6934: Training Loss: 0.14849505325158438 Validation Loss: 0.7406911849975586\n",
      "Epoch 6935: Training Loss: 0.14818347493807474 Validation Loss: 0.7411051392555237\n",
      "Epoch 6936: Training Loss: 0.14791560173034668 Validation Loss: 0.7405440211296082\n",
      "Epoch 6937: Training Loss: 0.14837228755156198 Validation Loss: 0.7400484681129456\n",
      "Epoch 6938: Training Loss: 0.14840948084990183 Validation Loss: 0.7396564483642578\n",
      "Epoch 6939: Training Loss: 0.14816064635912576 Validation Loss: 0.740392804145813\n",
      "Epoch 6940: Training Loss: 0.14808114369710287 Validation Loss: 0.7403658628463745\n",
      "Epoch 6941: Training Loss: 0.14793147146701813 Validation Loss: 0.7410239577293396\n",
      "Epoch 6942: Training Loss: 0.14830734332402548 Validation Loss: 0.7417883276939392\n",
      "Epoch 6943: Training Loss: 0.148047665754954 Validation Loss: 0.741575300693512\n",
      "Epoch 6944: Training Loss: 0.14810306827227274 Validation Loss: 0.7406929135322571\n",
      "Epoch 6945: Training Loss: 0.14849461118380228 Validation Loss: 0.7406151294708252\n",
      "Epoch 6946: Training Loss: 0.1480899304151535 Validation Loss: 0.7410274744033813\n",
      "Epoch 6947: Training Loss: 0.14791289965311685 Validation Loss: 0.7409663200378418\n",
      "Epoch 6948: Training Loss: 0.1484972983598709 Validation Loss: 0.7404130697250366\n",
      "Epoch 6949: Training Loss: 0.1490210642417272 Validation Loss: 0.7409300208091736\n",
      "Epoch 6950: Training Loss: 0.14804606636365256 Validation Loss: 0.7409496307373047\n",
      "Epoch 6951: Training Loss: 0.147954061627388 Validation Loss: 0.7409699559211731\n",
      "Epoch 6952: Training Loss: 0.14882320910692215 Validation Loss: 0.7408202290534973\n",
      "Epoch 6953: Training Loss: 0.14775382975737253 Validation Loss: 0.740158200263977\n",
      "Epoch 6954: Training Loss: 0.14836695293585458 Validation Loss: 0.7399918437004089\n",
      "Epoch 6955: Training Loss: 0.14809941252072653 Validation Loss: 0.740218997001648\n",
      "Epoch 6956: Training Loss: 0.1479185869296392 Validation Loss: 0.7408034801483154\n",
      "Epoch 6957: Training Loss: 0.1485606829325358 Validation Loss: 0.7414793968200684\n",
      "Epoch 6958: Training Loss: 0.14814746379852295 Validation Loss: 0.7411772012710571\n",
      "Epoch 6959: Training Loss: 0.14756039281686148 Validation Loss: 0.741254448890686\n",
      "Epoch 6960: Training Loss: 0.14780603349208832 Validation Loss: 0.7416078448295593\n",
      "Epoch 6961: Training Loss: 0.14798287798961005 Validation Loss: 0.7413190603256226\n",
      "Epoch 6962: Training Loss: 0.14826702078183493 Validation Loss: 0.7411061525344849\n",
      "Epoch 6963: Training Loss: 0.14767278730869293 Validation Loss: 0.7409493327140808\n",
      "Epoch 6964: Training Loss: 0.14762173096338907 Validation Loss: 0.740755021572113\n",
      "Epoch 6965: Training Loss: 0.14776854713757834 Validation Loss: 0.7404683828353882\n",
      "Epoch 6966: Training Loss: 0.14818683763345084 Validation Loss: 0.7405807375907898\n",
      "Epoch 6967: Training Loss: 0.147581547498703 Validation Loss: 0.7403650879859924\n",
      "Epoch 6968: Training Loss: 0.14772944152355194 Validation Loss: 0.7401741743087769\n",
      "Epoch 6969: Training Loss: 0.14783428609371185 Validation Loss: 0.7402181029319763\n",
      "Epoch 6970: Training Loss: 0.1479726036389669 Validation Loss: 0.7409366965293884\n",
      "Epoch 6971: Training Loss: 0.1473226398229599 Validation Loss: 0.7412660121917725\n",
      "Epoch 6972: Training Loss: 0.1475364714860916 Validation Loss: 0.7410204410552979\n",
      "Epoch 6973: Training Loss: 0.14741367598374686 Validation Loss: 0.7414907217025757\n",
      "Epoch 6974: Training Loss: 0.14710242549578348 Validation Loss: 0.741675078868866\n",
      "Epoch 6975: Training Loss: 0.1475727210442225 Validation Loss: 0.741072416305542\n",
      "Epoch 6976: Training Loss: 0.14735154310862222 Validation Loss: 0.7408120036125183\n",
      "Epoch 6977: Training Loss: 0.1479019820690155 Validation Loss: 0.7406240105628967\n",
      "Epoch 6978: Training Loss: 0.14760160942872366 Validation Loss: 0.7402185201644897\n",
      "Epoch 6979: Training Loss: 0.14727911353111267 Validation Loss: 0.7407805323600769\n",
      "Epoch 6980: Training Loss: 0.14757726093133292 Validation Loss: 0.7411155104637146\n",
      "Epoch 6981: Training Loss: 0.14848355452219644 Validation Loss: 0.7420207262039185\n",
      "Epoch 6982: Training Loss: 0.14722243944803873 Validation Loss: 0.7421376705169678\n",
      "Epoch 6983: Training Loss: 0.14747036000092825 Validation Loss: 0.7413267493247986\n",
      "Epoch 6984: Training Loss: 0.1473488708337148 Validation Loss: 0.740858256816864\n",
      "Epoch 6985: Training Loss: 0.14771987994511923 Validation Loss: 0.7405019998550415\n",
      "Epoch 6986: Training Loss: 0.1477966457605362 Validation Loss: 0.7402431964874268\n",
      "Epoch 6987: Training Loss: 0.14726593593756357 Validation Loss: 0.7406644821166992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6988: Training Loss: 0.14743241667747498 Validation Loss: 0.741309642791748\n",
      "Epoch 6989: Training Loss: 0.14725281794865927 Validation Loss: 0.7419174909591675\n",
      "Epoch 6990: Training Loss: 0.1471437414487203 Validation Loss: 0.7410772442817688\n",
      "Epoch 6991: Training Loss: 0.1474288503328959 Validation Loss: 0.7407844066619873\n",
      "Epoch 6992: Training Loss: 0.14830058813095093 Validation Loss: 0.7404590845108032\n",
      "Epoch 6993: Training Loss: 0.14706066250801086 Validation Loss: 0.7402546405792236\n",
      "Epoch 6994: Training Loss: 0.14696832497914633 Validation Loss: 0.7407769560813904\n",
      "Epoch 6995: Training Loss: 0.14736467599868774 Validation Loss: 0.7411606311798096\n",
      "Epoch 6996: Training Loss: 0.14698764185110727 Validation Loss: 0.7415540218353271\n",
      "Epoch 6997: Training Loss: 0.14794854323069254 Validation Loss: 0.7417875528335571\n",
      "Epoch 6998: Training Loss: 0.14693323274453482 Validation Loss: 0.7418702244758606\n",
      "Epoch 6999: Training Loss: 0.1471536805232366 Validation Loss: 0.7409307956695557\n",
      "Epoch 7000: Training Loss: 0.14697836339473724 Validation Loss: 0.7409287691116333\n",
      "Epoch 7001: Training Loss: 0.14671114583810171 Validation Loss: 0.7410550713539124\n",
      "Epoch 7002: Training Loss: 0.14699659744898477 Validation Loss: 0.7413130402565002\n",
      "Epoch 7003: Training Loss: 0.14731555680433908 Validation Loss: 0.7404341697692871\n",
      "Epoch 7004: Training Loss: 0.14698785046736398 Validation Loss: 0.7406022548675537\n",
      "Epoch 7005: Training Loss: 0.14732633034388223 Validation Loss: 0.7414342761039734\n",
      "Epoch 7006: Training Loss: 0.14692067603270212 Validation Loss: 0.7414328455924988\n",
      "Epoch 7007: Training Loss: 0.1469655136267344 Validation Loss: 0.7420464754104614\n",
      "Epoch 7008: Training Loss: 0.14665259420871735 Validation Loss: 0.7425363063812256\n",
      "Epoch 7009: Training Loss: 0.14659900466601053 Validation Loss: 0.7423149943351746\n",
      "Epoch 7010: Training Loss: 0.14835664133230844 Validation Loss: 0.7415298223495483\n",
      "Epoch 7011: Training Loss: 0.14732633282740912 Validation Loss: 0.7414392232894897\n",
      "Epoch 7012: Training Loss: 0.1466917097568512 Validation Loss: 0.7412710189819336\n",
      "Epoch 7013: Training Loss: 0.1467691014210383 Validation Loss: 0.7424064874649048\n",
      "Epoch 7014: Training Loss: 0.14715367058912912 Validation Loss: 0.7416231036186218\n",
      "Epoch 7015: Training Loss: 0.1467413455247879 Validation Loss: 0.7411285638809204\n",
      "Epoch 7016: Training Loss: 0.1467407743136088 Validation Loss: 0.7411189675331116\n",
      "Epoch 7017: Training Loss: 0.14703437189261118 Validation Loss: 0.7417370080947876\n",
      "Epoch 7018: Training Loss: 0.1468608876069387 Validation Loss: 0.7420075535774231\n",
      "Epoch 7019: Training Loss: 0.14660357435544333 Validation Loss: 0.7420620322227478\n",
      "Epoch 7020: Training Loss: 0.14681314677000046 Validation Loss: 0.7418281435966492\n",
      "Epoch 7021: Training Loss: 0.14655464887619019 Validation Loss: 0.7410383224487305\n",
      "Epoch 7022: Training Loss: 0.14652409652868906 Validation Loss: 0.7406688928604126\n",
      "Epoch 7023: Training Loss: 0.14731998989979425 Validation Loss: 0.740871787071228\n",
      "Epoch 7024: Training Loss: 0.14609161019325256 Validation Loss: 0.7409633994102478\n",
      "Epoch 7025: Training Loss: 0.1465853601694107 Validation Loss: 0.7411747574806213\n",
      "Epoch 7026: Training Loss: 0.14667552709579468 Validation Loss: 0.7415080666542053\n",
      "Epoch 7027: Training Loss: 0.14664173871278763 Validation Loss: 0.741989016532898\n",
      "Epoch 7028: Training Loss: 0.14700830231110254 Validation Loss: 0.7420421838760376\n",
      "Epoch 7029: Training Loss: 0.14648490895827612 Validation Loss: 0.740717887878418\n",
      "Epoch 7030: Training Loss: 0.14627974728743234 Validation Loss: 0.7408086061477661\n",
      "Epoch 7031: Training Loss: 0.14629505574703217 Validation Loss: 0.7406470775604248\n",
      "Epoch 7032: Training Loss: 0.14651291569073996 Validation Loss: 0.7414314150810242\n",
      "Epoch 7033: Training Loss: 0.14562482635180155 Validation Loss: 0.7424071431159973\n",
      "Epoch 7034: Training Loss: 0.14621666073799133 Validation Loss: 0.7424185872077942\n",
      "Epoch 7035: Training Loss: 0.14600750307242075 Validation Loss: 0.7422218322753906\n",
      "Epoch 7036: Training Loss: 0.14641577998797098 Validation Loss: 0.7418461441993713\n",
      "Epoch 7037: Training Loss: 0.14651275674502054 Validation Loss: 0.7412852048873901\n",
      "Epoch 7038: Training Loss: 0.1464666227499644 Validation Loss: 0.7407065033912659\n",
      "Epoch 7039: Training Loss: 0.14623003701368967 Validation Loss: 0.7410600781440735\n",
      "Epoch 7040: Training Loss: 0.1467114413777987 Validation Loss: 0.7413125038146973\n",
      "Epoch 7041: Training Loss: 0.14667905618747076 Validation Loss: 0.740792453289032\n",
      "Epoch 7042: Training Loss: 0.1461836944023768 Validation Loss: 0.7416755557060242\n",
      "Epoch 7043: Training Loss: 0.1461183081070582 Validation Loss: 0.7418427467346191\n",
      "Epoch 7044: Training Loss: 0.14628629883130392 Validation Loss: 0.7417504787445068\n",
      "Epoch 7045: Training Loss: 0.14613568782806396 Validation Loss: 0.7422253489494324\n",
      "Epoch 7046: Training Loss: 0.14648146679004034 Validation Loss: 0.7420064210891724\n",
      "Epoch 7047: Training Loss: 0.14614015817642212 Validation Loss: 0.7417615056037903\n",
      "Epoch 7048: Training Loss: 0.1468071142832438 Validation Loss: 0.7419571876525879\n",
      "Epoch 7049: Training Loss: 0.14673956235249838 Validation Loss: 0.7412764430046082\n",
      "Epoch 7050: Training Loss: 0.14617309470971426 Validation Loss: 0.7411220073699951\n",
      "Epoch 7051: Training Loss: 0.14613946775595346 Validation Loss: 0.7414110898971558\n",
      "Epoch 7052: Training Loss: 0.14660759270191193 Validation Loss: 0.7416150569915771\n",
      "Epoch 7053: Training Loss: 0.1471285050113996 Validation Loss: 0.74248868227005\n",
      "Epoch 7054: Training Loss: 0.1462074120839437 Validation Loss: 0.7421836853027344\n",
      "Epoch 7055: Training Loss: 0.1458311453461647 Validation Loss: 0.7414610981941223\n",
      "Epoch 7056: Training Loss: 0.14623108009497324 Validation Loss: 0.7413516044616699\n",
      "Epoch 7057: Training Loss: 0.14597419401009878 Validation Loss: 0.7413334846496582\n",
      "Epoch 7058: Training Loss: 0.14599761366844177 Validation Loss: 0.7416263818740845\n",
      "Epoch 7059: Training Loss: 0.14598865310351053 Validation Loss: 0.7416938543319702\n",
      "Epoch 7060: Training Loss: 0.1459167351325353 Validation Loss: 0.7424377799034119\n",
      "Epoch 7061: Training Loss: 0.14584510028362274 Validation Loss: 0.7426471710205078\n",
      "Epoch 7062: Training Loss: 0.1460754523674647 Validation Loss: 0.74247145652771\n",
      "Epoch 7063: Training Loss: 0.14608416457970938 Validation Loss: 0.7421250343322754\n",
      "Epoch 7064: Training Loss: 0.14605755607287088 Validation Loss: 0.7417581081390381\n",
      "Epoch 7065: Training Loss: 0.1467934176325798 Validation Loss: 0.7415188550949097\n",
      "Epoch 7066: Training Loss: 0.14577617247899374 Validation Loss: 0.7416836619377136\n",
      "Epoch 7067: Training Loss: 0.14656045039494833 Validation Loss: 0.7424578070640564\n",
      "Epoch 7068: Training Loss: 0.14586183428764343 Validation Loss: 0.7425748109817505\n",
      "Epoch 7069: Training Loss: 0.14596018691857657 Validation Loss: 0.7420375347137451\n",
      "Epoch 7070: Training Loss: 0.145618865887324 Validation Loss: 0.741397500038147\n",
      "Epoch 7071: Training Loss: 0.145897443095843 Validation Loss: 0.7413426637649536\n",
      "Epoch 7072: Training Loss: 0.14573376377423605 Validation Loss: 0.7410949468612671\n",
      "Epoch 7073: Training Loss: 0.1456709603468577 Validation Loss: 0.7406756281852722\n",
      "Epoch 7074: Training Loss: 0.14544995625813803 Validation Loss: 0.741403341293335\n",
      "Epoch 7075: Training Loss: 0.1455558786789576 Validation Loss: 0.7415131330490112\n",
      "Epoch 7076: Training Loss: 0.14555500447750092 Validation Loss: 0.7420706152915955\n",
      "Epoch 7077: Training Loss: 0.14574351410071054 Validation Loss: 0.7423431277275085\n",
      "Epoch 7078: Training Loss: 0.14573562145233154 Validation Loss: 0.74257493019104\n",
      "Epoch 7079: Training Loss: 0.14573830862840018 Validation Loss: 0.7426128387451172\n",
      "Epoch 7080: Training Loss: 0.14572612444559732 Validation Loss: 0.7427898645401001\n",
      "Epoch 7081: Training Loss: 0.14547020693620047 Validation Loss: 0.7421812415122986\n",
      "Epoch 7082: Training Loss: 0.14551218350728354 Validation Loss: 0.7415654063224792\n",
      "Epoch 7083: Training Loss: 0.14590780436992645 Validation Loss: 0.7413298487663269\n",
      "Epoch 7084: Training Loss: 0.145907923579216 Validation Loss: 0.7411551475524902\n",
      "Epoch 7085: Training Loss: 0.14538543423016867 Validation Loss: 0.7414769530296326\n",
      "Epoch 7086: Training Loss: 0.14584889014561972 Validation Loss: 0.7414875626564026\n",
      "Epoch 7087: Training Loss: 0.1455640196800232 Validation Loss: 0.7415592074394226\n",
      "Epoch 7088: Training Loss: 0.14549415310223898 Validation Loss: 0.7415444254875183\n",
      "Epoch 7089: Training Loss: 0.14573467274506888 Validation Loss: 0.7417072653770447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7090: Training Loss: 0.14601378639539084 Validation Loss: 0.74200439453125\n",
      "Epoch 7091: Training Loss: 0.1451636403799057 Validation Loss: 0.7426761984825134\n",
      "Epoch 7092: Training Loss: 0.14540300766626993 Validation Loss: 0.7426324486732483\n",
      "Epoch 7093: Training Loss: 0.14582487444082895 Validation Loss: 0.7419794201850891\n",
      "Epoch 7094: Training Loss: 0.14516910910606384 Validation Loss: 0.7421441078186035\n",
      "Epoch 7095: Training Loss: 0.14530531068642935 Validation Loss: 0.7416731715202332\n",
      "Epoch 7096: Training Loss: 0.145363911986351 Validation Loss: 0.7417357563972473\n",
      "Epoch 7097: Training Loss: 0.14583957691987356 Validation Loss: 0.7425046563148499\n",
      "Epoch 7098: Training Loss: 0.14589358866214752 Validation Loss: 0.7422638535499573\n",
      "Epoch 7099: Training Loss: 0.14579127232233682 Validation Loss: 0.7419694066047668\n",
      "Epoch 7100: Training Loss: 0.14517398178577423 Validation Loss: 0.7423015236854553\n",
      "Epoch 7101: Training Loss: 0.14515924453735352 Validation Loss: 0.7424352765083313\n",
      "Epoch 7102: Training Loss: 0.1455202450354894 Validation Loss: 0.7420693635940552\n",
      "Epoch 7103: Training Loss: 0.14695768555005392 Validation Loss: 0.7421413064002991\n",
      "Epoch 7104: Training Loss: 0.1449775000413259 Validation Loss: 0.7422283887863159\n",
      "Epoch 7105: Training Loss: 0.14526931444803873 Validation Loss: 0.7417524456977844\n",
      "Epoch 7106: Training Loss: 0.145156259338061 Validation Loss: 0.7415310144424438\n",
      "Epoch 7107: Training Loss: 0.14533627529939017 Validation Loss: 0.741926372051239\n",
      "Epoch 7108: Training Loss: 0.14493522544701895 Validation Loss: 0.7431066036224365\n",
      "Epoch 7109: Training Loss: 0.14541561404863992 Validation Loss: 0.7427544593811035\n",
      "Epoch 7110: Training Loss: 0.1449038883050283 Validation Loss: 0.7426519393920898\n",
      "Epoch 7111: Training Loss: 0.1453669418891271 Validation Loss: 0.742581307888031\n",
      "Epoch 7112: Training Loss: 0.14478294054667154 Validation Loss: 0.7430274486541748\n",
      "Epoch 7113: Training Loss: 0.14547599852085114 Validation Loss: 0.7431526780128479\n",
      "Epoch 7114: Training Loss: 0.14501109222571054 Validation Loss: 0.7429524064064026\n",
      "Epoch 7115: Training Loss: 0.14579003055890402 Validation Loss: 0.7424654364585876\n",
      "Epoch 7116: Training Loss: 0.1448820581038793 Validation Loss: 0.7429004907608032\n",
      "Epoch 7117: Training Loss: 0.1449302931626638 Validation Loss: 0.7423608303070068\n",
      "Epoch 7118: Training Loss: 0.14508169889450073 Validation Loss: 0.7427476048469543\n",
      "Epoch 7119: Training Loss: 0.14490900933742523 Validation Loss: 0.7428275942802429\n",
      "Epoch 7120: Training Loss: 0.14502602318922678 Validation Loss: 0.7421547770500183\n",
      "Epoch 7121: Training Loss: 0.14477338393529257 Validation Loss: 0.7421712279319763\n",
      "Epoch 7122: Training Loss: 0.14468909800052643 Validation Loss: 0.7416865229606628\n",
      "Epoch 7123: Training Loss: 0.14501753449440002 Validation Loss: 0.7416219115257263\n",
      "Epoch 7124: Training Loss: 0.14462252457936606 Validation Loss: 0.7419658303260803\n",
      "Epoch 7125: Training Loss: 0.14519351720809937 Validation Loss: 0.7419672012329102\n",
      "Epoch 7126: Training Loss: 0.14485975603262582 Validation Loss: 0.7425983548164368\n",
      "Epoch 7127: Training Loss: 0.14459896584351858 Validation Loss: 0.7428252696990967\n",
      "Epoch 7128: Training Loss: 0.14477360745271048 Validation Loss: 0.7424684166908264\n",
      "Epoch 7129: Training Loss: 0.14510752260684967 Validation Loss: 0.7426139116287231\n",
      "Epoch 7130: Training Loss: 0.14501325289408365 Validation Loss: 0.7424156069755554\n",
      "Epoch 7131: Training Loss: 0.1444091498851776 Validation Loss: 0.7426071763038635\n",
      "Epoch 7132: Training Loss: 0.1451978882153829 Validation Loss: 0.7428903579711914\n",
      "Epoch 7133: Training Loss: 0.14472962419191995 Validation Loss: 0.7430272102355957\n",
      "Epoch 7134: Training Loss: 0.14455742637316385 Validation Loss: 0.7430912256240845\n",
      "Epoch 7135: Training Loss: 0.14483759800593057 Validation Loss: 0.7426115274429321\n",
      "Epoch 7136: Training Loss: 0.14466333389282227 Validation Loss: 0.7428469657897949\n",
      "Epoch 7137: Training Loss: 0.14493748545646667 Validation Loss: 0.7421566247940063\n",
      "Epoch 7138: Training Loss: 0.14443034927050272 Validation Loss: 0.7420349717140198\n",
      "Epoch 7139: Training Loss: 0.14482500652472177 Validation Loss: 0.7422215342521667\n",
      "Epoch 7140: Training Loss: 0.14448639750480652 Validation Loss: 0.7422508597373962\n",
      "Epoch 7141: Training Loss: 0.14469936986764273 Validation Loss: 0.7424193024635315\n",
      "Epoch 7142: Training Loss: 0.1446127394835154 Validation Loss: 0.7420406341552734\n",
      "Epoch 7143: Training Loss: 0.14442917704582214 Validation Loss: 0.742594301700592\n",
      "Epoch 7144: Training Loss: 0.14410704374313354 Validation Loss: 0.7429640889167786\n",
      "Epoch 7145: Training Loss: 0.1445579876502355 Validation Loss: 0.7431507706642151\n",
      "Epoch 7146: Training Loss: 0.14413142700990042 Validation Loss: 0.7425905466079712\n",
      "Epoch 7147: Training Loss: 0.14476211865743002 Validation Loss: 0.7425090670585632\n",
      "Epoch 7148: Training Loss: 0.14423178633054098 Validation Loss: 0.7425616383552551\n",
      "Epoch 7149: Training Loss: 0.1449645459651947 Validation Loss: 0.7424026727676392\n",
      "Epoch 7150: Training Loss: 0.14504249890645346 Validation Loss: 0.7421867847442627\n",
      "Epoch 7151: Training Loss: 0.14438368628422418 Validation Loss: 0.741873025894165\n",
      "Epoch 7152: Training Loss: 0.14399849871794382 Validation Loss: 0.7419909834861755\n",
      "Epoch 7153: Training Loss: 0.14474880695343018 Validation Loss: 0.7425600290298462\n",
      "Epoch 7154: Training Loss: 0.14457014203071594 Validation Loss: 0.7429674863815308\n",
      "Epoch 7155: Training Loss: 0.14421594639619192 Validation Loss: 0.7430060505867004\n",
      "Epoch 7156: Training Loss: 0.14429027338822684 Validation Loss: 0.7433794736862183\n",
      "Epoch 7157: Training Loss: 0.14437720427910486 Validation Loss: 0.7432238459587097\n",
      "Epoch 7158: Training Loss: 0.1443395415941874 Validation Loss: 0.7425652742385864\n",
      "Epoch 7159: Training Loss: 0.14410837491353354 Validation Loss: 0.7419281601905823\n",
      "Epoch 7160: Training Loss: 0.14441240827242532 Validation Loss: 0.7423324584960938\n",
      "Epoch 7161: Training Loss: 0.14391262829303741 Validation Loss: 0.7419984936714172\n",
      "Epoch 7162: Training Loss: 0.14351731538772583 Validation Loss: 0.7422037124633789\n",
      "Epoch 7163: Training Loss: 0.1440714697043101 Validation Loss: 0.7431846857070923\n",
      "Epoch 7164: Training Loss: 0.1439276784658432 Validation Loss: 0.7436234951019287\n",
      "Epoch 7165: Training Loss: 0.144543985525767 Validation Loss: 0.7434994578361511\n",
      "Epoch 7166: Training Loss: 0.14415344099203745 Validation Loss: 0.7431819438934326\n",
      "Epoch 7167: Training Loss: 0.1440699448188146 Validation Loss: 0.742588222026825\n",
      "Epoch 7168: Training Loss: 0.14399453500906625 Validation Loss: 0.7420139908790588\n",
      "Epoch 7169: Training Loss: 0.1439696103334427 Validation Loss: 0.7426820397377014\n",
      "Epoch 7170: Training Loss: 0.1442283888657888 Validation Loss: 0.7426605820655823\n",
      "Epoch 7171: Training Loss: 0.14381191631158194 Validation Loss: 0.7434331774711609\n",
      "Epoch 7172: Training Loss: 0.14403188228607178 Validation Loss: 0.7444434762001038\n",
      "Epoch 7173: Training Loss: 0.14528573552767435 Validation Loss: 0.7442765235900879\n",
      "Epoch 7174: Training Loss: 0.14387927452723184 Validation Loss: 0.7428881525993347\n",
      "Epoch 7175: Training Loss: 0.14390610655148825 Validation Loss: 0.7429614663124084\n",
      "Epoch 7176: Training Loss: 0.14387863874435425 Validation Loss: 0.7427627444267273\n",
      "Epoch 7177: Training Loss: 0.14388959109783173 Validation Loss: 0.7423929572105408\n",
      "Epoch 7178: Training Loss: 0.1440466344356537 Validation Loss: 0.7425785064697266\n",
      "Epoch 7179: Training Loss: 0.14433236420154572 Validation Loss: 0.7430882453918457\n",
      "Epoch 7180: Training Loss: 0.14442371328671774 Validation Loss: 0.7433452606201172\n",
      "Epoch 7181: Training Loss: 0.14375697076320648 Validation Loss: 0.7428073883056641\n",
      "Epoch 7182: Training Loss: 0.1441937784353892 Validation Loss: 0.7426776885986328\n",
      "Epoch 7183: Training Loss: 0.14372883240381876 Validation Loss: 0.7426570653915405\n",
      "Epoch 7184: Training Loss: 0.14379942417144775 Validation Loss: 0.7425737380981445\n",
      "Epoch 7185: Training Loss: 0.14388668537139893 Validation Loss: 0.7433913350105286\n",
      "Epoch 7186: Training Loss: 0.14372635881106058 Validation Loss: 0.7437136173248291\n",
      "Epoch 7187: Training Loss: 0.14367354412873587 Validation Loss: 0.7434735298156738\n",
      "Epoch 7188: Training Loss: 0.14374369382858276 Validation Loss: 0.7427520155906677\n",
      "Epoch 7189: Training Loss: 0.1442602276802063 Validation Loss: 0.7423833608627319\n",
      "Epoch 7190: Training Loss: 0.14467688153187433 Validation Loss: 0.7423884868621826\n",
      "Epoch 7191: Training Loss: 0.14353657265504202 Validation Loss: 0.742652952671051\n",
      "Epoch 7192: Training Loss: 0.14350207646687826 Validation Loss: 0.7428153157234192\n",
      "Epoch 7193: Training Loss: 0.14344986279805502 Validation Loss: 0.7430636882781982\n",
      "Epoch 7194: Training Loss: 0.1436046908299128 Validation Loss: 0.7427743077278137\n",
      "Epoch 7195: Training Loss: 0.1434165338675181 Validation Loss: 0.742840588092804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7196: Training Loss: 0.14414425194263458 Validation Loss: 0.7434568405151367\n",
      "Epoch 7197: Training Loss: 0.14368578294912973 Validation Loss: 0.743809700012207\n",
      "Epoch 7198: Training Loss: 0.14365180830160776 Validation Loss: 0.7445322871208191\n",
      "Epoch 7199: Training Loss: 0.14350221554438272 Validation Loss: 0.7445427775382996\n",
      "Epoch 7200: Training Loss: 0.1436042090257009 Validation Loss: 0.7438535094261169\n",
      "Epoch 7201: Training Loss: 0.14408233761787415 Validation Loss: 0.7433210611343384\n",
      "Epoch 7202: Training Loss: 0.14348061879475912 Validation Loss: 0.7429161667823792\n",
      "Epoch 7203: Training Loss: 0.14330515762170157 Validation Loss: 0.7427656650543213\n",
      "Epoch 7204: Training Loss: 0.14351926743984222 Validation Loss: 0.7431644201278687\n",
      "Epoch 7205: Training Loss: 0.1437308837970098 Validation Loss: 0.743028461933136\n",
      "Epoch 7206: Training Loss: 0.14347956577936807 Validation Loss: 0.7433665990829468\n",
      "Epoch 7207: Training Loss: 0.1438460797071457 Validation Loss: 0.7432517409324646\n",
      "Epoch 7208: Training Loss: 0.1432961622873942 Validation Loss: 0.7430736422538757\n",
      "Epoch 7209: Training Loss: 0.1444021537899971 Validation Loss: 0.7431442141532898\n",
      "Epoch 7210: Training Loss: 0.14328436056772867 Validation Loss: 0.7431163191795349\n",
      "Epoch 7211: Training Loss: 0.14312867323557535 Validation Loss: 0.7428818941116333\n",
      "Epoch 7212: Training Loss: 0.14347594479719797 Validation Loss: 0.7427273988723755\n",
      "Epoch 7213: Training Loss: 0.1432413657506307 Validation Loss: 0.7427603602409363\n",
      "Epoch 7214: Training Loss: 0.14313741028308868 Validation Loss: 0.742933452129364\n",
      "Epoch 7215: Training Loss: 0.1430190900961558 Validation Loss: 0.7432368397712708\n",
      "Epoch 7216: Training Loss: 0.14325877030690512 Validation Loss: 0.7432029247283936\n",
      "Epoch 7217: Training Loss: 0.14336257427930832 Validation Loss: 0.7431946992874146\n",
      "Epoch 7218: Training Loss: 0.14355404675006866 Validation Loss: 0.7429659962654114\n",
      "Epoch 7219: Training Loss: 0.1446459243694941 Validation Loss: 0.7432869076728821\n",
      "Epoch 7220: Training Loss: 0.14309687912464142 Validation Loss: 0.7434232234954834\n",
      "Epoch 7221: Training Loss: 0.14325727025667825 Validation Loss: 0.7431941032409668\n",
      "Epoch 7222: Training Loss: 0.1433166762193044 Validation Loss: 0.7433722615242004\n",
      "Epoch 7223: Training Loss: 0.14321056505044302 Validation Loss: 0.7435404658317566\n",
      "Epoch 7224: Training Loss: 0.14266497393449148 Validation Loss: 0.7435526847839355\n",
      "Epoch 7225: Training Loss: 0.14294720192750296 Validation Loss: 0.7440552115440369\n",
      "Epoch 7226: Training Loss: 0.14314135909080505 Validation Loss: 0.7443656325340271\n",
      "Epoch 7227: Training Loss: 0.14308570325374603 Validation Loss: 0.7439574003219604\n",
      "Epoch 7228: Training Loss: 0.14299996693929037 Validation Loss: 0.743634045124054\n",
      "Epoch 7229: Training Loss: 0.14291653533776602 Validation Loss: 0.7427628040313721\n",
      "Epoch 7230: Training Loss: 0.14389067391554514 Validation Loss: 0.7428228855133057\n",
      "Epoch 7231: Training Loss: 0.14288764695326486 Validation Loss: 0.7425026297569275\n",
      "Epoch 7232: Training Loss: 0.142398864030838 Validation Loss: 0.7432129383087158\n",
      "Epoch 7233: Training Loss: 0.14254499226808548 Validation Loss: 0.7437440156936646\n",
      "Epoch 7234: Training Loss: 0.1430541475613912 Validation Loss: 0.7435541152954102\n",
      "Epoch 7235: Training Loss: 0.1426448474327723 Validation Loss: 0.7431989312171936\n",
      "Epoch 7236: Training Loss: 0.14217905203501383 Validation Loss: 0.7427175045013428\n",
      "Epoch 7237: Training Loss: 0.142804354429245 Validation Loss: 0.7431151866912842\n",
      "Epoch 7238: Training Loss: 0.14279708514610925 Validation Loss: 0.743505597114563\n",
      "Epoch 7239: Training Loss: 0.14302664250135422 Validation Loss: 0.7441452145576477\n",
      "Epoch 7240: Training Loss: 0.14399741093317667 Validation Loss: 0.7436810731887817\n",
      "Epoch 7241: Training Loss: 0.14271950225035349 Validation Loss: 0.7436853051185608\n",
      "Epoch 7242: Training Loss: 0.14268656075000763 Validation Loss: 0.7441469430923462\n",
      "Epoch 7243: Training Loss: 0.14304327964782715 Validation Loss: 0.7432998418807983\n",
      "Epoch 7244: Training Loss: 0.14284785588582358 Validation Loss: 0.7431210279464722\n",
      "Epoch 7245: Training Loss: 0.1427468260129293 Validation Loss: 0.7438521981239319\n",
      "Epoch 7246: Training Loss: 0.14302198588848114 Validation Loss: 0.7437746524810791\n",
      "Epoch 7247: Training Loss: 0.14336258172988892 Validation Loss: 0.7437297701835632\n",
      "Epoch 7248: Training Loss: 0.14275074501832327 Validation Loss: 0.7435430288314819\n",
      "Epoch 7249: Training Loss: 0.14210343857606253 Validation Loss: 0.7443037033081055\n",
      "Epoch 7250: Training Loss: 0.14275719225406647 Validation Loss: 0.7435117959976196\n",
      "Epoch 7251: Training Loss: 0.14288018147150675 Validation Loss: 0.7435340881347656\n",
      "Epoch 7252: Training Loss: 0.1427103877067566 Validation Loss: 0.743028998374939\n",
      "Epoch 7253: Training Loss: 0.14292349169651666 Validation Loss: 0.7430508732795715\n",
      "Epoch 7254: Training Loss: 0.14266914129257202 Validation Loss: 0.7437323927879333\n",
      "Epoch 7255: Training Loss: 0.14291044821341833 Validation Loss: 0.7440670728683472\n",
      "Epoch 7256: Training Loss: 0.14261173208554587 Validation Loss: 0.7439029812812805\n",
      "Epoch 7257: Training Loss: 0.1424665649731954 Validation Loss: 0.7437273263931274\n",
      "Epoch 7258: Training Loss: 0.14258214831352234 Validation Loss: 0.7437773942947388\n",
      "Epoch 7259: Training Loss: 0.14238475759824118 Validation Loss: 0.7431011199951172\n",
      "Epoch 7260: Training Loss: 0.14244491855303446 Validation Loss: 0.7426558136940002\n",
      "Epoch 7261: Training Loss: 0.14255206286907196 Validation Loss: 0.7426816821098328\n",
      "Epoch 7262: Training Loss: 0.14250698188940683 Validation Loss: 0.7433441877365112\n",
      "Epoch 7263: Training Loss: 0.14289292693138123 Validation Loss: 0.7446250915527344\n",
      "Epoch 7264: Training Loss: 0.14216068387031555 Validation Loss: 0.7446846961975098\n",
      "Epoch 7265: Training Loss: 0.14231514434019724 Validation Loss: 0.7445589303970337\n",
      "Epoch 7266: Training Loss: 0.14284691711266836 Validation Loss: 0.7442055940628052\n",
      "Epoch 7267: Training Loss: 0.1426489551862081 Validation Loss: 0.7432917356491089\n",
      "Epoch 7268: Training Loss: 0.14218170940876007 Validation Loss: 0.743187427520752\n",
      "Epoch 7269: Training Loss: 0.1420557051897049 Validation Loss: 0.7434197068214417\n",
      "Epoch 7270: Training Loss: 0.1422717422246933 Validation Loss: 0.7434871792793274\n",
      "Epoch 7271: Training Loss: 0.14315912127494812 Validation Loss: 0.7439094185829163\n",
      "Epoch 7272: Training Loss: 0.14251463611920676 Validation Loss: 0.7439514994621277\n",
      "Epoch 7273: Training Loss: 0.14248250424861908 Validation Loss: 0.7438027858734131\n",
      "Epoch 7274: Training Loss: 0.14295221865177155 Validation Loss: 0.7439924478530884\n",
      "Epoch 7275: Training Loss: 0.14238855242729187 Validation Loss: 0.7438772916793823\n",
      "Epoch 7276: Training Loss: 0.14273575445016226 Validation Loss: 0.7437680959701538\n",
      "Epoch 7277: Training Loss: 0.1426533410946528 Validation Loss: 0.7438791990280151\n",
      "Epoch 7278: Training Loss: 0.14314984530210495 Validation Loss: 0.7436715960502625\n",
      "Epoch 7279: Training Loss: 0.14211699863274893 Validation Loss: 0.7437076568603516\n",
      "Epoch 7280: Training Loss: 0.14179733395576477 Validation Loss: 0.7437078356742859\n",
      "Epoch 7281: Training Loss: 0.1419974441329638 Validation Loss: 0.7434023022651672\n",
      "Epoch 7282: Training Loss: 0.14235396683216095 Validation Loss: 0.7434231638908386\n",
      "Epoch 7283: Training Loss: 0.14191358288129172 Validation Loss: 0.7442647218704224\n",
      "Epoch 7284: Training Loss: 0.14222370584805807 Validation Loss: 0.7440801858901978\n",
      "Epoch 7285: Training Loss: 0.14197845260302225 Validation Loss: 0.7442599534988403\n",
      "Epoch 7286: Training Loss: 0.14197731018066406 Validation Loss: 0.7445142865180969\n",
      "Epoch 7287: Training Loss: 0.14264407008886337 Validation Loss: 0.7446014285087585\n",
      "Epoch 7288: Training Loss: 0.14198321104049683 Validation Loss: 0.7440142631530762\n",
      "Epoch 7289: Training Loss: 0.14261620740095773 Validation Loss: 0.743457555770874\n",
      "Epoch 7290: Training Loss: 0.14178709189097086 Validation Loss: 0.7438173294067383\n",
      "Epoch 7291: Training Loss: 0.14186984797318777 Validation Loss: 0.7441843748092651\n",
      "Epoch 7292: Training Loss: 0.14209594329198202 Validation Loss: 0.7437061071395874\n",
      "Epoch 7293: Training Loss: 0.14197182655334473 Validation Loss: 0.743685245513916\n",
      "Epoch 7294: Training Loss: 0.1421279013156891 Validation Loss: 0.7442992329597473\n",
      "Epoch 7295: Training Loss: 0.14228216807047525 Validation Loss: 0.744230329990387\n",
      "Epoch 7296: Training Loss: 0.14225802818934122 Validation Loss: 0.7443894147872925\n",
      "Epoch 7297: Training Loss: 0.14186313251654306 Validation Loss: 0.7437303066253662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7298: Training Loss: 0.14191908140977225 Validation Loss: 0.7439258694648743\n",
      "Epoch 7299: Training Loss: 0.14184094468752542 Validation Loss: 0.743958055973053\n",
      "Epoch 7300: Training Loss: 0.1417226791381836 Validation Loss: 0.7442392706871033\n",
      "Epoch 7301: Training Loss: 0.1415864179531733 Validation Loss: 0.7444765567779541\n",
      "Epoch 7302: Training Loss: 0.14125636716683707 Validation Loss: 0.7445724010467529\n",
      "Epoch 7303: Training Loss: 0.14129621287186941 Validation Loss: 0.7441792488098145\n",
      "Epoch 7304: Training Loss: 0.14239533493916193 Validation Loss: 0.7444028854370117\n",
      "Epoch 7305: Training Loss: 0.14183516800403595 Validation Loss: 0.743926465511322\n",
      "Epoch 7306: Training Loss: 0.14209139347076416 Validation Loss: 0.7440035343170166\n",
      "Epoch 7307: Training Loss: 0.14156712591648102 Validation Loss: 0.7439672350883484\n",
      "Epoch 7308: Training Loss: 0.1416973372300466 Validation Loss: 0.7438198924064636\n",
      "Epoch 7309: Training Loss: 0.141818235317866 Validation Loss: 0.7440018057823181\n",
      "Epoch 7310: Training Loss: 0.14159508049488068 Validation Loss: 0.7442111968994141\n",
      "Epoch 7311: Training Loss: 0.14138912657896677 Validation Loss: 0.7441043257713318\n",
      "Epoch 7312: Training Loss: 0.141536017258962 Validation Loss: 0.7441319227218628\n",
      "Epoch 7313: Training Loss: 0.14224925140539804 Validation Loss: 0.7443578839302063\n",
      "Epoch 7314: Training Loss: 0.14166802167892456 Validation Loss: 0.7442914843559265\n",
      "Epoch 7315: Training Loss: 0.14154710868994394 Validation Loss: 0.7449620962142944\n",
      "Epoch 7316: Training Loss: 0.14173526813586554 Validation Loss: 0.7451876401901245\n",
      "Epoch 7317: Training Loss: 0.14204548299312592 Validation Loss: 0.7444556355476379\n",
      "Epoch 7318: Training Loss: 0.1413745085398356 Validation Loss: 0.7449010014533997\n",
      "Epoch 7319: Training Loss: 0.14155802627404532 Validation Loss: 0.744506299495697\n",
      "Epoch 7320: Training Loss: 0.14223438998063406 Validation Loss: 0.7447676658630371\n",
      "Epoch 7321: Training Loss: 0.14130755265553793 Validation Loss: 0.7446563839912415\n",
      "Epoch 7322: Training Loss: 0.14125447968641916 Validation Loss: 0.7438656091690063\n",
      "Epoch 7323: Training Loss: 0.1416530360778173 Validation Loss: 0.7434014678001404\n",
      "Epoch 7324: Training Loss: 0.14157951871554056 Validation Loss: 0.7433987259864807\n",
      "Epoch 7325: Training Loss: 0.14135188857714334 Validation Loss: 0.7439454793930054\n",
      "Epoch 7326: Training Loss: 0.14248421539862952 Validation Loss: 0.7444280982017517\n",
      "Epoch 7327: Training Loss: 0.14173429707686105 Validation Loss: 0.7447932362556458\n",
      "Epoch 7328: Training Loss: 0.1414353847503662 Validation Loss: 0.7454186677932739\n",
      "Epoch 7329: Training Loss: 0.14122970402240753 Validation Loss: 0.7448862791061401\n",
      "Epoch 7330: Training Loss: 0.1422098701198896 Validation Loss: 0.7449024319648743\n",
      "Epoch 7331: Training Loss: 0.14135236541430155 Validation Loss: 0.7441360950469971\n",
      "Epoch 7332: Training Loss: 0.14127855002880096 Validation Loss: 0.7440515160560608\n",
      "Epoch 7333: Training Loss: 0.14162880927324295 Validation Loss: 0.7442210912704468\n",
      "Epoch 7334: Training Loss: 0.14200423657894135 Validation Loss: 0.7449619770050049\n",
      "Epoch 7335: Training Loss: 0.14125037690003714 Validation Loss: 0.7449422478675842\n",
      "Epoch 7336: Training Loss: 0.14146950840950012 Validation Loss: 0.744094729423523\n",
      "Epoch 7337: Training Loss: 0.14195440709590912 Validation Loss: 0.7443229556083679\n",
      "Epoch 7338: Training Loss: 0.14120193322499594 Validation Loss: 0.7452117800712585\n",
      "Epoch 7339: Training Loss: 0.14113334814707437 Validation Loss: 0.7449522614479065\n",
      "Epoch 7340: Training Loss: 0.14080623288949332 Validation Loss: 0.7443854212760925\n",
      "Epoch 7341: Training Loss: 0.14120261371135712 Validation Loss: 0.7441211938858032\n",
      "Epoch 7342: Training Loss: 0.14103714376688004 Validation Loss: 0.7445575594902039\n",
      "Epoch 7343: Training Loss: 0.1411194105943044 Validation Loss: 0.7452443838119507\n",
      "Epoch 7344: Training Loss: 0.14092602084080377 Validation Loss: 0.7450984120368958\n",
      "Epoch 7345: Training Loss: 0.14124321937561035 Validation Loss: 0.7449986934661865\n",
      "Epoch 7346: Training Loss: 0.14105250189701715 Validation Loss: 0.7450369000434875\n",
      "Epoch 7347: Training Loss: 0.14112462600072226 Validation Loss: 0.7444185614585876\n",
      "Epoch 7348: Training Loss: 0.1412431299686432 Validation Loss: 0.7446107268333435\n",
      "Epoch 7349: Training Loss: 0.1407938301563263 Validation Loss: 0.7445271015167236\n",
      "Epoch 7350: Training Loss: 0.14093435058991113 Validation Loss: 0.7442466616630554\n",
      "Epoch 7351: Training Loss: 0.14108747988939285 Validation Loss: 0.7437803149223328\n",
      "Epoch 7352: Training Loss: 0.14034437139829 Validation Loss: 0.7440707087516785\n",
      "Epoch 7353: Training Loss: 0.14098030577103296 Validation Loss: 0.7444842457771301\n",
      "Epoch 7354: Training Loss: 0.14097806314627329 Validation Loss: 0.7450578808784485\n",
      "Epoch 7355: Training Loss: 0.14088244239489237 Validation Loss: 0.7449395656585693\n",
      "Epoch 7356: Training Loss: 0.14139609783887863 Validation Loss: 0.7449706196784973\n",
      "Epoch 7357: Training Loss: 0.14066263536612192 Validation Loss: 0.7449463605880737\n",
      "Epoch 7358: Training Loss: 0.14051595826943716 Validation Loss: 0.74458247423172\n",
      "Epoch 7359: Training Loss: 0.1409399708112081 Validation Loss: 0.744724690914154\n",
      "Epoch 7360: Training Loss: 0.14091382920742035 Validation Loss: 0.7455023527145386\n",
      "Epoch 7361: Training Loss: 0.14075113832950592 Validation Loss: 0.7456381916999817\n",
      "Epoch 7362: Training Loss: 0.14062231530745825 Validation Loss: 0.744642436504364\n",
      "Epoch 7363: Training Loss: 0.1405006299416224 Validation Loss: 0.7442588806152344\n",
      "Epoch 7364: Training Loss: 0.14066853870948157 Validation Loss: 0.7447190284729004\n",
      "Epoch 7365: Training Loss: 0.14087340732415518 Validation Loss: 0.7448956370353699\n",
      "Epoch 7366: Training Loss: 0.1410807967185974 Validation Loss: 0.7444738745689392\n",
      "Epoch 7367: Training Loss: 0.14067373673121134 Validation Loss: 0.7442213296890259\n",
      "Epoch 7368: Training Loss: 0.14078789204359055 Validation Loss: 0.7444815039634705\n",
      "Epoch 7369: Training Loss: 0.14243907978137335 Validation Loss: 0.7445160150527954\n",
      "Epoch 7370: Training Loss: 0.14078689614931741 Validation Loss: 0.7452682852745056\n",
      "Epoch 7371: Training Loss: 0.14060319463411966 Validation Loss: 0.7453306317329407\n",
      "Epoch 7372: Training Loss: 0.14033309370279312 Validation Loss: 0.7451185584068298\n",
      "Epoch 7373: Training Loss: 0.14070557057857513 Validation Loss: 0.7453283667564392\n",
      "Epoch 7374: Training Loss: 0.1402902031938235 Validation Loss: 0.7449678778648376\n",
      "Epoch 7375: Training Loss: 0.14037823428710303 Validation Loss: 0.7451928853988647\n",
      "Epoch 7376: Training Loss: 0.140842134753863 Validation Loss: 0.7450721263885498\n",
      "Epoch 7377: Training Loss: 0.14006549616654715 Validation Loss: 0.7449753284454346\n",
      "Epoch 7378: Training Loss: 0.14068645735581717 Validation Loss: 0.744866669178009\n",
      "Epoch 7379: Training Loss: 0.14015152057011923 Validation Loss: 0.7452920079231262\n",
      "Epoch 7380: Training Loss: 0.14056410640478134 Validation Loss: 0.7445700764656067\n",
      "Epoch 7381: Training Loss: 0.1403575837612152 Validation Loss: 0.7445372343063354\n",
      "Epoch 7382: Training Loss: 0.14036202430725098 Validation Loss: 0.7452479004859924\n",
      "Epoch 7383: Training Loss: 0.14052371184031168 Validation Loss: 0.7452380061149597\n",
      "Epoch 7384: Training Loss: 0.14108522484699884 Validation Loss: 0.7441278100013733\n",
      "Epoch 7385: Training Loss: 0.14067342380682626 Validation Loss: 0.7437999844551086\n",
      "Epoch 7386: Training Loss: 0.14048247039318085 Validation Loss: 0.7441775798797607\n",
      "Epoch 7387: Training Loss: 0.1406364987293879 Validation Loss: 0.7447171807289124\n",
      "Epoch 7388: Training Loss: 0.14034858345985413 Validation Loss: 0.7447465658187866\n",
      "Epoch 7389: Training Loss: 0.14061732838551202 Validation Loss: 0.7448639869689941\n",
      "Epoch 7390: Training Loss: 0.1402867784102758 Validation Loss: 0.7447261810302734\n",
      "Epoch 7391: Training Loss: 0.1407566418250402 Validation Loss: 0.7447574138641357\n",
      "Epoch 7392: Training Loss: 0.1403905302286148 Validation Loss: 0.7452508807182312\n",
      "Epoch 7393: Training Loss: 0.14023330310980478 Validation Loss: 0.7451784610748291\n",
      "Epoch 7394: Training Loss: 0.14049786577622095 Validation Loss: 0.7448211908340454\n",
      "Epoch 7395: Training Loss: 0.14073594907919565 Validation Loss: 0.7452880144119263\n",
      "Epoch 7396: Training Loss: 0.14021838704744974 Validation Loss: 0.7449607253074646\n",
      "Epoch 7397: Training Loss: 0.14038319885730743 Validation Loss: 0.7449266910552979\n",
      "Epoch 7398: Training Loss: 0.13974613944689432 Validation Loss: 0.7451090216636658\n",
      "Epoch 7399: Training Loss: 0.14056630929311117 Validation Loss: 0.7454914450645447\n",
      "Epoch 7400: Training Loss: 0.14015268286069235 Validation Loss: 0.7460771203041077\n",
      "Epoch 7401: Training Loss: 0.14022226631641388 Validation Loss: 0.7462031841278076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7402: Training Loss: 0.14123634000619253 Validation Loss: 0.7460386157035828\n",
      "Epoch 7403: Training Loss: 0.1400770296653112 Validation Loss: 0.7459903359413147\n",
      "Epoch 7404: Training Loss: 0.14011559883753458 Validation Loss: 0.7454708814620972\n",
      "Epoch 7405: Training Loss: 0.14046298960844675 Validation Loss: 0.7446383833885193\n",
      "Epoch 7406: Training Loss: 0.14000126222769418 Validation Loss: 0.744990885257721\n",
      "Epoch 7407: Training Loss: 0.14009235302607217 Validation Loss: 0.7451560497283936\n",
      "Epoch 7408: Training Loss: 0.140074556072553 Validation Loss: 0.7449321746826172\n",
      "Epoch 7409: Training Loss: 0.1404756853977839 Validation Loss: 0.7454774975776672\n",
      "Epoch 7410: Training Loss: 0.13996616502602896 Validation Loss: 0.7451143264770508\n",
      "Epoch 7411: Training Loss: 0.1399815777937571 Validation Loss: 0.7446991801261902\n",
      "Epoch 7412: Training Loss: 0.14034402867158255 Validation Loss: 0.7444515228271484\n",
      "Epoch 7413: Training Loss: 0.140314648548762 Validation Loss: 0.7452698945999146\n",
      "Epoch 7414: Training Loss: 0.1400721420844396 Validation Loss: 0.7452457547187805\n",
      "Epoch 7415: Training Loss: 0.1403656154870987 Validation Loss: 0.7449159026145935\n",
      "Epoch 7416: Training Loss: 0.13975301881631216 Validation Loss: 0.7449841499328613\n",
      "Epoch 7417: Training Loss: 0.14049430191516876 Validation Loss: 0.7450141310691833\n",
      "Epoch 7418: Training Loss: 0.14014368752638498 Validation Loss: 0.745854914188385\n",
      "Epoch 7419: Training Loss: 0.13962335884571075 Validation Loss: 0.7460411787033081\n",
      "Epoch 7420: Training Loss: 0.13986864686012268 Validation Loss: 0.7464513182640076\n",
      "Epoch 7421: Training Loss: 0.14000777155160904 Validation Loss: 0.7459735870361328\n",
      "Epoch 7422: Training Loss: 0.1401986206571261 Validation Loss: 0.7448830008506775\n",
      "Epoch 7423: Training Loss: 0.13998514165480933 Validation Loss: 0.7448045611381531\n",
      "Epoch 7424: Training Loss: 0.13992867867151895 Validation Loss: 0.7450145483016968\n",
      "Epoch 7425: Training Loss: 0.1406235545873642 Validation Loss: 0.7460202574729919\n",
      "Epoch 7426: Training Loss: 0.1397799700498581 Validation Loss: 0.7459964156150818\n",
      "Epoch 7427: Training Loss: 0.14000017444292703 Validation Loss: 0.7464891076087952\n",
      "Epoch 7428: Training Loss: 0.13974768916765848 Validation Loss: 0.7462570667266846\n",
      "Epoch 7429: Training Loss: 0.13973229875167212 Validation Loss: 0.7460318207740784\n",
      "Epoch 7430: Training Loss: 0.13973377148310342 Validation Loss: 0.7453593611717224\n",
      "Epoch 7431: Training Loss: 0.1396216774980227 Validation Loss: 0.7449038624763489\n",
      "Epoch 7432: Training Loss: 0.13876556356747946 Validation Loss: 0.7450142502784729\n",
      "Epoch 7433: Training Loss: 0.13967632253964743 Validation Loss: 0.7456392645835876\n",
      "Epoch 7434: Training Loss: 0.13922054568926492 Validation Loss: 0.7455326318740845\n",
      "Epoch 7435: Training Loss: 0.1397564858198166 Validation Loss: 0.7458166480064392\n",
      "Epoch 7436: Training Loss: 0.14010846614837646 Validation Loss: 0.7450960278511047\n",
      "Epoch 7437: Training Loss: 0.1397883047660192 Validation Loss: 0.7450663447380066\n",
      "Epoch 7438: Training Loss: 0.1391752635439237 Validation Loss: 0.7454602718353271\n",
      "Epoch 7439: Training Loss: 0.13939416905244192 Validation Loss: 0.7456124424934387\n",
      "Epoch 7440: Training Loss: 0.13939249018828073 Validation Loss: 0.7460263967514038\n",
      "Epoch 7441: Training Loss: 0.13961913188298544 Validation Loss: 0.7455310225486755\n",
      "Epoch 7442: Training Loss: 0.14051537215709686 Validation Loss: 0.7459439635276794\n",
      "Epoch 7443: Training Loss: 0.1396042083700498 Validation Loss: 0.7456769347190857\n",
      "Epoch 7444: Training Loss: 0.1398062805334727 Validation Loss: 0.7452325820922852\n",
      "Epoch 7445: Training Loss: 0.13996648788452148 Validation Loss: 0.7450314164161682\n",
      "Epoch 7446: Training Loss: 0.14038495471080145 Validation Loss: 0.7452986240386963\n",
      "Epoch 7447: Training Loss: 0.13949531316757202 Validation Loss: 0.7459968328475952\n",
      "Epoch 7448: Training Loss: 0.13966615498065948 Validation Loss: 0.745833694934845\n",
      "Epoch 7449: Training Loss: 0.14018403987089792 Validation Loss: 0.7462271451950073\n",
      "Epoch 7450: Training Loss: 0.13914595047632852 Validation Loss: 0.7464864253997803\n",
      "Epoch 7451: Training Loss: 0.13931103547414145 Validation Loss: 0.7463411092758179\n",
      "Epoch 7452: Training Loss: 0.1395402600367864 Validation Loss: 0.7459777593612671\n",
      "Epoch 7453: Training Loss: 0.13931499669949213 Validation Loss: 0.7459282875061035\n",
      "Epoch 7454: Training Loss: 0.14035229136546454 Validation Loss: 0.7457404136657715\n",
      "Epoch 7455: Training Loss: 0.1392961194117864 Validation Loss: 0.7454990744590759\n",
      "Epoch 7456: Training Loss: 0.13918831944465637 Validation Loss: 0.7456679940223694\n",
      "Epoch 7457: Training Loss: 0.1392632375160853 Validation Loss: 0.7449427843093872\n",
      "Epoch 7458: Training Loss: 0.1392119973897934 Validation Loss: 0.7451321482658386\n",
      "Epoch 7459: Training Loss: 0.13934568067391714 Validation Loss: 0.7460740804672241\n",
      "Epoch 7460: Training Loss: 0.1400440533955892 Validation Loss: 0.7462905645370483\n",
      "Epoch 7461: Training Loss: 0.1392749473452568 Validation Loss: 0.7465124130249023\n",
      "Epoch 7462: Training Loss: 0.13915499796470007 Validation Loss: 0.7463216185569763\n",
      "Epoch 7463: Training Loss: 0.13921483854452768 Validation Loss: 0.7453277707099915\n",
      "Epoch 7464: Training Loss: 0.13955345998207727 Validation Loss: 0.7449961304664612\n",
      "Epoch 7465: Training Loss: 0.13921334346135458 Validation Loss: 0.7458743453025818\n",
      "Epoch 7466: Training Loss: 0.1392883906761805 Validation Loss: 0.7456650137901306\n",
      "Epoch 7467: Training Loss: 0.13887424767017365 Validation Loss: 0.7453685402870178\n",
      "Epoch 7468: Training Loss: 0.13918804625670114 Validation Loss: 0.745911180973053\n",
      "Epoch 7469: Training Loss: 0.13906807700792947 Validation Loss: 0.7457393407821655\n",
      "Epoch 7470: Training Loss: 0.13897222777207693 Validation Loss: 0.7462531924247742\n",
      "Epoch 7471: Training Loss: 0.13885992268721262 Validation Loss: 0.7455216646194458\n",
      "Epoch 7472: Training Loss: 0.13900408645470938 Validation Loss: 0.7455522418022156\n",
      "Epoch 7473: Training Loss: 0.13906894127527872 Validation Loss: 0.7451379895210266\n",
      "Epoch 7474: Training Loss: 0.13895036528507868 Validation Loss: 0.7454583644866943\n",
      "Epoch 7475: Training Loss: 0.13892019291718802 Validation Loss: 0.7458920478820801\n",
      "Epoch 7476: Training Loss: 0.13890041907628378 Validation Loss: 0.7459481358528137\n",
      "Epoch 7477: Training Loss: 0.13949925949176153 Validation Loss: 0.7451053261756897\n",
      "Epoch 7478: Training Loss: 0.13932393987973532 Validation Loss: 0.7457118630409241\n",
      "Epoch 7479: Training Loss: 0.13914133359988531 Validation Loss: 0.7460017204284668\n",
      "Epoch 7480: Training Loss: 0.13918602218230566 Validation Loss: 0.747103214263916\n",
      "Epoch 7481: Training Loss: 0.13890860974788666 Validation Loss: 0.7469581365585327\n",
      "Epoch 7482: Training Loss: 0.13888487219810486 Validation Loss: 0.7470329403877258\n",
      "Epoch 7483: Training Loss: 0.13924731810887656 Validation Loss: 0.7461494207382202\n",
      "Epoch 7484: Training Loss: 0.13886716961860657 Validation Loss: 0.74578458070755\n",
      "Epoch 7485: Training Loss: 0.13888880610466003 Validation Loss: 0.7454909682273865\n",
      "Epoch 7486: Training Loss: 0.13881469269593558 Validation Loss: 0.7456610202789307\n",
      "Epoch 7487: Training Loss: 0.1382834737499555 Validation Loss: 0.7465047240257263\n",
      "Epoch 7488: Training Loss: 0.13893121480941772 Validation Loss: 0.7463134527206421\n",
      "Epoch 7489: Training Loss: 0.13861359655857086 Validation Loss: 0.7467610836029053\n",
      "Epoch 7490: Training Loss: 0.13956864178180695 Validation Loss: 0.746779203414917\n",
      "Epoch 7491: Training Loss: 0.13864419857660928 Validation Loss: 0.7465412020683289\n",
      "Epoch 7492: Training Loss: 0.1387244015932083 Validation Loss: 0.7463788390159607\n",
      "Epoch 7493: Training Loss: 0.13944307963053384 Validation Loss: 0.7459638714790344\n",
      "Epoch 7494: Training Loss: 0.13855955501397452 Validation Loss: 0.7469670176506042\n",
      "Epoch 7495: Training Loss: 0.13909450670083365 Validation Loss: 0.7470923066139221\n",
      "Epoch 7496: Training Loss: 0.13875173777341843 Validation Loss: 0.7464295625686646\n",
      "Epoch 7497: Training Loss: 0.1387729768951734 Validation Loss: 0.7460517287254333\n",
      "Epoch 7498: Training Loss: 0.13862986862659454 Validation Loss: 0.7463273406028748\n",
      "Epoch 7499: Training Loss: 0.13894832134246826 Validation Loss: 0.7461255788803101\n",
      "Epoch 7500: Training Loss: 0.13834108412265778 Validation Loss: 0.7459927201271057\n",
      "Epoch 7501: Training Loss: 0.1385327676932017 Validation Loss: 0.7460342645645142\n",
      "Epoch 7502: Training Loss: 0.138601948817571 Validation Loss: 0.7457347512245178\n",
      "Epoch 7503: Training Loss: 0.13860388845205307 Validation Loss: 0.7460572719573975\n",
      "Epoch 7504: Training Loss: 0.1389445662498474 Validation Loss: 0.7457612752914429\n",
      "Epoch 7505: Training Loss: 0.13849524160226187 Validation Loss: 0.7463265061378479\n",
      "Epoch 7506: Training Loss: 0.13835876186688742 Validation Loss: 0.7464478015899658\n",
      "Epoch 7507: Training Loss: 0.13848593334356943 Validation Loss: 0.746954619884491\n",
      "Epoch 7508: Training Loss: 0.13856003185113272 Validation Loss: 0.7462434768676758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7509: Training Loss: 0.1385473906993866 Validation Loss: 0.7453243732452393\n",
      "Epoch 7510: Training Loss: 0.13865618407726288 Validation Loss: 0.7460464239120483\n",
      "Epoch 7511: Training Loss: 0.13882288336753845 Validation Loss: 0.7467038631439209\n",
      "Epoch 7512: Training Loss: 0.13899396111567816 Validation Loss: 0.7469900846481323\n",
      "Epoch 7513: Training Loss: 0.13893729448318481 Validation Loss: 0.7466650009155273\n",
      "Epoch 7514: Training Loss: 0.13814527789751688 Validation Loss: 0.7466020584106445\n",
      "Epoch 7515: Training Loss: 0.1383075217405955 Validation Loss: 0.7464008927345276\n",
      "Epoch 7516: Training Loss: 0.13831523060798645 Validation Loss: 0.7460857629776001\n",
      "Epoch 7517: Training Loss: 0.13848785559336343 Validation Loss: 0.7456361055374146\n",
      "Epoch 7518: Training Loss: 0.13833987216154733 Validation Loss: 0.7454323172569275\n",
      "Epoch 7519: Training Loss: 0.1383184939622879 Validation Loss: 0.7459436058998108\n",
      "Epoch 7520: Training Loss: 0.13834849993387857 Validation Loss: 0.7459901571273804\n",
      "Epoch 7521: Training Loss: 0.1384891470273336 Validation Loss: 0.7462961077690125\n",
      "Epoch 7522: Training Loss: 0.13867340981960297 Validation Loss: 0.7466309070587158\n",
      "Epoch 7523: Training Loss: 0.13864470769961676 Validation Loss: 0.7463439106941223\n",
      "Epoch 7524: Training Loss: 0.13811354339122772 Validation Loss: 0.7459719181060791\n",
      "Epoch 7525: Training Loss: 0.13829868038495383 Validation Loss: 0.7454965114593506\n",
      "Epoch 7526: Training Loss: 0.13808055222034454 Validation Loss: 0.7465333938598633\n",
      "Epoch 7527: Training Loss: 0.13940454771121344 Validation Loss: 0.7473248243331909\n",
      "Epoch 7528: Training Loss: 0.13834801067908606 Validation Loss: 0.7477849125862122\n",
      "Epoch 7529: Training Loss: 0.13837172836065292 Validation Loss: 0.7470539808273315\n",
      "Epoch 7530: Training Loss: 0.13816066086292267 Validation Loss: 0.7467816472053528\n",
      "Epoch 7531: Training Loss: 0.1382534603277842 Validation Loss: 0.7459522485733032\n",
      "Epoch 7532: Training Loss: 0.13839496672153473 Validation Loss: 0.7463913559913635\n",
      "Epoch 7533: Training Loss: 0.13806247462828955 Validation Loss: 0.7464095950126648\n",
      "Epoch 7534: Training Loss: 0.13804054260253906 Validation Loss: 0.7465339303016663\n",
      "Epoch 7535: Training Loss: 0.13798328240712485 Validation Loss: 0.7470783591270447\n",
      "Epoch 7536: Training Loss: 0.13804891208807626 Validation Loss: 0.746721625328064\n",
      "Epoch 7537: Training Loss: 0.13789892196655273 Validation Loss: 0.7464672327041626\n",
      "Epoch 7538: Training Loss: 0.13824552297592163 Validation Loss: 0.7465125322341919\n",
      "Epoch 7539: Training Loss: 0.1378584106763204 Validation Loss: 0.7463388442993164\n",
      "Epoch 7540: Training Loss: 0.13840556144714355 Validation Loss: 0.7460684180259705\n",
      "Epoch 7541: Training Loss: 0.13823135197162628 Validation Loss: 0.7456655502319336\n",
      "Epoch 7542: Training Loss: 0.1380140334367752 Validation Loss: 0.7453418374061584\n",
      "Epoch 7543: Training Loss: 0.138041819135348 Validation Loss: 0.7460010647773743\n",
      "Epoch 7544: Training Loss: 0.1378046621878942 Validation Loss: 0.7470811605453491\n",
      "Epoch 7545: Training Loss: 0.1377610688408216 Validation Loss: 0.7472606301307678\n",
      "Epoch 7546: Training Loss: 0.13826118657986322 Validation Loss: 0.7481188774108887\n",
      "Epoch 7547: Training Loss: 0.1382239113251368 Validation Loss: 0.747063398361206\n",
      "Epoch 7548: Training Loss: 0.1375905101497968 Validation Loss: 0.7471046447753906\n",
      "Epoch 7549: Training Loss: 0.13731827586889267 Validation Loss: 0.7466486096382141\n",
      "Epoch 7550: Training Loss: 0.13781744738419852 Validation Loss: 0.7470899224281311\n",
      "Epoch 7551: Training Loss: 0.13770304123560587 Validation Loss: 0.7474015355110168\n",
      "Epoch 7552: Training Loss: 0.13776171952486038 Validation Loss: 0.747803270816803\n",
      "Epoch 7553: Training Loss: 0.13799388209978738 Validation Loss: 0.7474655508995056\n",
      "Epoch 7554: Training Loss: 0.13788286099831262 Validation Loss: 0.7470299601554871\n",
      "Epoch 7555: Training Loss: 0.13761635373036066 Validation Loss: 0.7466151714324951\n",
      "Epoch 7556: Training Loss: 0.13772548735141754 Validation Loss: 0.7462900280952454\n",
      "Epoch 7557: Training Loss: 0.1385507583618164 Validation Loss: 0.7464033365249634\n",
      "Epoch 7558: Training Loss: 0.1383154739936193 Validation Loss: 0.7470192313194275\n",
      "Epoch 7559: Training Loss: 0.13764938215414682 Validation Loss: 0.7471518516540527\n",
      "Epoch 7560: Training Loss: 0.13735665380954742 Validation Loss: 0.7471961379051208\n",
      "Epoch 7561: Training Loss: 0.13759179413318634 Validation Loss: 0.7463933229446411\n",
      "Epoch 7562: Training Loss: 0.13786029815673828 Validation Loss: 0.7464694976806641\n",
      "Epoch 7563: Training Loss: 0.13761244217554727 Validation Loss: 0.7470531463623047\n",
      "Epoch 7564: Training Loss: 0.13755507518847784 Validation Loss: 0.7471404075622559\n",
      "Epoch 7565: Training Loss: 0.13761001080274582 Validation Loss: 0.7468498349189758\n",
      "Epoch 7566: Training Loss: 0.1377108097076416 Validation Loss: 0.7464882731437683\n",
      "Epoch 7567: Training Loss: 0.13771648705005646 Validation Loss: 0.7474612593650818\n",
      "Epoch 7568: Training Loss: 0.13778736193974814 Validation Loss: 0.7465628385543823\n",
      "Epoch 7569: Training Loss: 0.13746607303619385 Validation Loss: 0.7461873292922974\n",
      "Epoch 7570: Training Loss: 0.1374055122335752 Validation Loss: 0.746477484703064\n",
      "Epoch 7571: Training Loss: 0.13721947123607 Validation Loss: 0.7465165853500366\n",
      "Epoch 7572: Training Loss: 0.13773192216952643 Validation Loss: 0.7466373443603516\n",
      "Epoch 7573: Training Loss: 0.1375539799531301 Validation Loss: 0.7466155886650085\n",
      "Epoch 7574: Training Loss: 0.13737613956133524 Validation Loss: 0.7466380000114441\n",
      "Epoch 7575: Training Loss: 0.13809179762999216 Validation Loss: 0.746525764465332\n",
      "Epoch 7576: Training Loss: 0.13797356933355331 Validation Loss: 0.7461864352226257\n",
      "Epoch 7577: Training Loss: 0.1379188895225525 Validation Loss: 0.7466219067573547\n",
      "Epoch 7578: Training Loss: 0.13762551049391428 Validation Loss: 0.7471620440483093\n",
      "Epoch 7579: Training Loss: 0.1379861906170845 Validation Loss: 0.747734546661377\n",
      "Epoch 7580: Training Loss: 0.13748637586832047 Validation Loss: 0.7476344108581543\n",
      "Epoch 7581: Training Loss: 0.1376329908768336 Validation Loss: 0.7478725910186768\n",
      "Epoch 7582: Training Loss: 0.1377567599217097 Validation Loss: 0.7468416094779968\n",
      "Epoch 7583: Training Loss: 0.13740205268065134 Validation Loss: 0.7474361062049866\n",
      "Epoch 7584: Training Loss: 0.13737946252028146 Validation Loss: 0.7467668056488037\n",
      "Epoch 7585: Training Loss: 0.13733117779095969 Validation Loss: 0.7461156249046326\n",
      "Epoch 7586: Training Loss: 0.13734843830267587 Validation Loss: 0.746537446975708\n",
      "Epoch 7587: Training Loss: 0.13733058174451193 Validation Loss: 0.7468279004096985\n",
      "Epoch 7588: Training Loss: 0.13719531893730164 Validation Loss: 0.7475518584251404\n",
      "Epoch 7589: Training Loss: 0.13750766714413962 Validation Loss: 0.7482151389122009\n",
      "Epoch 7590: Training Loss: 0.13726345946391424 Validation Loss: 0.7478320598602295\n",
      "Epoch 7591: Training Loss: 0.1367968867222468 Validation Loss: 0.7482194304466248\n",
      "Epoch 7592: Training Loss: 0.13731896380583444 Validation Loss: 0.7481980919837952\n",
      "Epoch 7593: Training Loss: 0.13710850973924002 Validation Loss: 0.7470483779907227\n",
      "Epoch 7594: Training Loss: 0.1371885985136032 Validation Loss: 0.7471856474876404\n",
      "Epoch 7595: Training Loss: 0.13756620387236276 Validation Loss: 0.7467812299728394\n",
      "Epoch 7596: Training Loss: 0.13778185099363327 Validation Loss: 0.7467496991157532\n",
      "Epoch 7597: Training Loss: 0.13714325428009033 Validation Loss: 0.7468641400337219\n",
      "Epoch 7598: Training Loss: 0.1368025466799736 Validation Loss: 0.747174859046936\n",
      "Epoch 7599: Training Loss: 0.13742671410242716 Validation Loss: 0.7478074431419373\n",
      "Epoch 7600: Training Loss: 0.13708711663881937 Validation Loss: 0.7471296787261963\n",
      "Epoch 7601: Training Loss: 0.13715294500192007 Validation Loss: 0.7474324703216553\n",
      "Epoch 7602: Training Loss: 0.13767790546019873 Validation Loss: 0.7470629215240479\n",
      "Epoch 7603: Training Loss: 0.13674738506476083 Validation Loss: 0.7473853230476379\n",
      "Epoch 7604: Training Loss: 0.13694984714190164 Validation Loss: 0.7475866079330444\n",
      "Epoch 7605: Training Loss: 0.13692782819271088 Validation Loss: 0.7481462955474854\n",
      "Epoch 7606: Training Loss: 0.13698366781075796 Validation Loss: 0.7473735213279724\n",
      "Epoch 7607: Training Loss: 0.1366480141878128 Validation Loss: 0.7479920983314514\n",
      "Epoch 7608: Training Loss: 0.13679519295692444 Validation Loss: 0.747962474822998\n",
      "Epoch 7609: Training Loss: 0.13702386120955148 Validation Loss: 0.7479967474937439\n",
      "Epoch 7610: Training Loss: 0.13682104150454202 Validation Loss: 0.7475717067718506\n",
      "Epoch 7611: Training Loss: 0.13725793361663818 Validation Loss: 0.7473101019859314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7612: Training Loss: 0.13684349755446115 Validation Loss: 0.7472649812698364\n",
      "Epoch 7613: Training Loss: 0.13710320740938187 Validation Loss: 0.7467267513275146\n",
      "Epoch 7614: Training Loss: 0.13710190107425055 Validation Loss: 0.7464200854301453\n",
      "Epoch 7615: Training Loss: 0.13688958684603372 Validation Loss: 0.7466907501220703\n",
      "Epoch 7616: Training Loss: 0.1368286112944285 Validation Loss: 0.7473322749137878\n",
      "Epoch 7617: Training Loss: 0.13680634647607803 Validation Loss: 0.747388482093811\n",
      "Epoch 7618: Training Loss: 0.1385782609383265 Validation Loss: 0.7475634813308716\n",
      "Epoch 7619: Training Loss: 0.13685475786527 Validation Loss: 0.7480176687240601\n",
      "Epoch 7620: Training Loss: 0.1371510997414589 Validation Loss: 0.7473377585411072\n",
      "Epoch 7621: Training Loss: 0.1381512259443601 Validation Loss: 0.747124969959259\n",
      "Epoch 7622: Training Loss: 0.13680459062258402 Validation Loss: 0.7466728687286377\n",
      "Epoch 7623: Training Loss: 0.13684636851151785 Validation Loss: 0.7470253109931946\n",
      "Epoch 7624: Training Loss: 0.13657347857952118 Validation Loss: 0.7483243346214294\n",
      "Epoch 7625: Training Loss: 0.1383209154009819 Validation Loss: 0.7493285536766052\n",
      "Epoch 7626: Training Loss: 0.13680554429690042 Validation Loss: 0.7485635876655579\n",
      "Epoch 7627: Training Loss: 0.13692439099152884 Validation Loss: 0.7481696605682373\n",
      "Epoch 7628: Training Loss: 0.13682148853937784 Validation Loss: 0.747734546661377\n",
      "Epoch 7629: Training Loss: 0.13700723151365915 Validation Loss: 0.7472387552261353\n",
      "Epoch 7630: Training Loss: 0.1364916811386744 Validation Loss: 0.7471383213996887\n",
      "Epoch 7631: Training Loss: 0.13670585056145987 Validation Loss: 0.7470675110816956\n",
      "Epoch 7632: Training Loss: 0.1367223933339119 Validation Loss: 0.7482578158378601\n",
      "Epoch 7633: Training Loss: 0.13657471785942712 Validation Loss: 0.7478576302528381\n",
      "Epoch 7634: Training Loss: 0.1367135097583135 Validation Loss: 0.747905433177948\n",
      "Epoch 7635: Training Loss: 0.13608911633491516 Validation Loss: 0.7477867603302002\n",
      "Epoch 7636: Training Loss: 0.13673505932092667 Validation Loss: 0.746975302696228\n",
      "Epoch 7637: Training Loss: 0.13672897716363272 Validation Loss: 0.7467203736305237\n",
      "Epoch 7638: Training Loss: 0.13678894937038422 Validation Loss: 0.7469110488891602\n",
      "Epoch 7639: Training Loss: 0.13632937769095102 Validation Loss: 0.7474257946014404\n",
      "Epoch 7640: Training Loss: 0.1363745629787445 Validation Loss: 0.7476251125335693\n",
      "Epoch 7641: Training Loss: 0.13646191855271658 Validation Loss: 0.747623860836029\n",
      "Epoch 7642: Training Loss: 0.1363687813282013 Validation Loss: 0.74748694896698\n",
      "Epoch 7643: Training Loss: 0.13625953594843546 Validation Loss: 0.7475048899650574\n",
      "Epoch 7644: Training Loss: 0.13629349072774252 Validation Loss: 0.7471869587898254\n",
      "Epoch 7645: Training Loss: 0.1363372951745987 Validation Loss: 0.7477853894233704\n",
      "Epoch 7646: Training Loss: 0.13634876410166422 Validation Loss: 0.748168408870697\n",
      "Epoch 7647: Training Loss: 0.13659163316090903 Validation Loss: 0.7483379244804382\n",
      "Epoch 7648: Training Loss: 0.13637982308864594 Validation Loss: 0.7481181025505066\n",
      "Epoch 7649: Training Loss: 0.13680260380109152 Validation Loss: 0.7478275895118713\n",
      "Epoch 7650: Training Loss: 0.1364419162273407 Validation Loss: 0.7470718026161194\n",
      "Epoch 7651: Training Loss: 0.1364759107430776 Validation Loss: 0.7468299865722656\n",
      "Epoch 7652: Training Loss: 0.1360918382803599 Validation Loss: 0.7477939128875732\n",
      "Epoch 7653: Training Loss: 0.13628943264484406 Validation Loss: 0.7473398447036743\n",
      "Epoch 7654: Training Loss: 0.1361298312743505 Validation Loss: 0.7476879954338074\n",
      "Epoch 7655: Training Loss: 0.13620459288358688 Validation Loss: 0.7479349374771118\n",
      "Epoch 7656: Training Loss: 0.13625071942806244 Validation Loss: 0.7472466230392456\n",
      "Epoch 7657: Training Loss: 0.13630878428618112 Validation Loss: 0.7473421096801758\n",
      "Epoch 7658: Training Loss: 0.13604658842086792 Validation Loss: 0.7474157810211182\n",
      "Epoch 7659: Training Loss: 0.1358890781799952 Validation Loss: 0.7472458481788635\n",
      "Epoch 7660: Training Loss: 0.1368086909254392 Validation Loss: 0.7477673292160034\n",
      "Epoch 7661: Training Loss: 0.13629871606826782 Validation Loss: 0.7475613355636597\n",
      "Epoch 7662: Training Loss: 0.1366446390748024 Validation Loss: 0.7479152083396912\n",
      "Epoch 7663: Training Loss: 0.13608871400356293 Validation Loss: 0.7476653456687927\n",
      "Epoch 7664: Training Loss: 0.135590061545372 Validation Loss: 0.7473585605621338\n",
      "Epoch 7665: Training Loss: 0.13704097519318262 Validation Loss: 0.7482854127883911\n",
      "Epoch 7666: Training Loss: 0.13540255526701608 Validation Loss: 0.7485713958740234\n",
      "Epoch 7667: Training Loss: 0.13612946371237436 Validation Loss: 0.7480152249336243\n",
      "Epoch 7668: Training Loss: 0.13604910671710968 Validation Loss: 0.74855637550354\n",
      "Epoch 7669: Training Loss: 0.13626611977815628 Validation Loss: 0.7479043006896973\n",
      "Epoch 7670: Training Loss: 0.1358755057056745 Validation Loss: 0.7476592659950256\n",
      "Epoch 7671: Training Loss: 0.13596767683823904 Validation Loss: 0.7480308413505554\n",
      "Epoch 7672: Training Loss: 0.13593573371569315 Validation Loss: 0.7481687068939209\n",
      "Epoch 7673: Training Loss: 0.13610516488552094 Validation Loss: 0.7485184669494629\n",
      "Epoch 7674: Training Loss: 0.13648844758669534 Validation Loss: 0.7483924031257629\n",
      "Epoch 7675: Training Loss: 0.13649415721495947 Validation Loss: 0.7479844689369202\n",
      "Epoch 7676: Training Loss: 0.13607483605543771 Validation Loss: 0.7482198476791382\n",
      "Epoch 7677: Training Loss: 0.13598917921384177 Validation Loss: 0.7483383417129517\n",
      "Epoch 7678: Training Loss: 0.13603675862153372 Validation Loss: 0.748680055141449\n",
      "Epoch 7679: Training Loss: 0.13585378229618073 Validation Loss: 0.748236894607544\n",
      "Epoch 7680: Training Loss: 0.13593951364358267 Validation Loss: 0.7488082647323608\n",
      "Epoch 7681: Training Loss: 0.13609694441159567 Validation Loss: 0.7493411898612976\n",
      "Epoch 7682: Training Loss: 0.1356183315316836 Validation Loss: 0.7489520311355591\n",
      "Epoch 7683: Training Loss: 0.13665583233038583 Validation Loss: 0.7478861212730408\n",
      "Epoch 7684: Training Loss: 0.1357752208908399 Validation Loss: 0.7479670643806458\n",
      "Epoch 7685: Training Loss: 0.13589646915594736 Validation Loss: 0.7484810948371887\n",
      "Epoch 7686: Training Loss: 0.13562979300816855 Validation Loss: 0.7489266991615295\n",
      "Epoch 7687: Training Loss: 0.13635190079609552 Validation Loss: 0.7489455938339233\n",
      "Epoch 7688: Training Loss: 0.13582847764094672 Validation Loss: 0.7484203577041626\n",
      "Epoch 7689: Training Loss: 0.1356715907653173 Validation Loss: 0.7479240298271179\n",
      "Epoch 7690: Training Loss: 0.13580626994371414 Validation Loss: 0.7483533620834351\n",
      "Epoch 7691: Training Loss: 0.13567019502321878 Validation Loss: 0.7481902837753296\n",
      "Epoch 7692: Training Loss: 0.13612031936645508 Validation Loss: 0.7474954724311829\n",
      "Epoch 7693: Training Loss: 0.13575559357802072 Validation Loss: 0.747991681098938\n",
      "Epoch 7694: Training Loss: 0.13555167615413666 Validation Loss: 0.7483664155006409\n",
      "Epoch 7695: Training Loss: 0.13594458003838858 Validation Loss: 0.7480626702308655\n",
      "Epoch 7696: Training Loss: 0.1360312427083651 Validation Loss: 0.7475869059562683\n",
      "Epoch 7697: Training Loss: 0.13570371766885123 Validation Loss: 0.7480188012123108\n",
      "Epoch 7698: Training Loss: 0.1355836515625318 Validation Loss: 0.7477983236312866\n",
      "Epoch 7699: Training Loss: 0.1357236827413241 Validation Loss: 0.7475577592849731\n",
      "Epoch 7700: Training Loss: 0.13566203912099203 Validation Loss: 0.7480934858322144\n",
      "Epoch 7701: Training Loss: 0.13600993901491165 Validation Loss: 0.7482678890228271\n",
      "Epoch 7702: Training Loss: 0.13589045405387878 Validation Loss: 0.7480934262275696\n",
      "Epoch 7703: Training Loss: 0.13573223849137625 Validation Loss: 0.7479984164237976\n",
      "Epoch 7704: Training Loss: 0.13572193185488382 Validation Loss: 0.7493566870689392\n",
      "Epoch 7705: Training Loss: 0.13537351787090302 Validation Loss: 0.7489903569221497\n",
      "Epoch 7706: Training Loss: 0.13534442087014517 Validation Loss: 0.7484300136566162\n",
      "Epoch 7707: Training Loss: 0.13607039799292883 Validation Loss: 0.7483766674995422\n",
      "Epoch 7708: Training Loss: 0.13577914983034134 Validation Loss: 0.7476293444633484\n",
      "Epoch 7709: Training Loss: 0.1354876011610031 Validation Loss: 0.7486073970794678\n",
      "Epoch 7710: Training Loss: 0.13608220467964807 Validation Loss: 0.7485565543174744\n",
      "Epoch 7711: Training Loss: 0.1354236751794815 Validation Loss: 0.7484255433082581\n",
      "Epoch 7712: Training Loss: 0.13536112755537033 Validation Loss: 0.7485508322715759\n",
      "Epoch 7713: Training Loss: 0.13533969223499298 Validation Loss: 0.7486248016357422\n",
      "Epoch 7714: Training Loss: 0.13546463350454965 Validation Loss: 0.7488849759101868\n",
      "Epoch 7715: Training Loss: 0.13581096132596335 Validation Loss: 0.7486901879310608\n",
      "Epoch 7716: Training Loss: 0.13543689250946045 Validation Loss: 0.7479370832443237\n",
      "Epoch 7717: Training Loss: 0.13574637472629547 Validation Loss: 0.7482380867004395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7718: Training Loss: 0.1352949490149816 Validation Loss: 0.7481047511100769\n",
      "Epoch 7719: Training Loss: 0.135333185394605 Validation Loss: 0.7485774159431458\n",
      "Epoch 7720: Training Loss: 0.13615265736977258 Validation Loss: 0.7482470870018005\n",
      "Epoch 7721: Training Loss: 0.13521850109100342 Validation Loss: 0.7487780451774597\n",
      "Epoch 7722: Training Loss: 0.1349691723783811 Validation Loss: 0.7491782903671265\n",
      "Epoch 7723: Training Loss: 0.13534864286581674 Validation Loss: 0.7488128542900085\n",
      "Epoch 7724: Training Loss: 0.1354383478562037 Validation Loss: 0.7484057545661926\n",
      "Epoch 7725: Training Loss: 0.1349710077047348 Validation Loss: 0.7482719421386719\n",
      "Epoch 7726: Training Loss: 0.13553465406099954 Validation Loss: 0.7485559582710266\n",
      "Epoch 7727: Training Loss: 0.13530957698822021 Validation Loss: 0.7494168877601624\n",
      "Epoch 7728: Training Loss: 0.13538125902414322 Validation Loss: 0.7498679757118225\n",
      "Epoch 7729: Training Loss: 0.13494267563025156 Validation Loss: 0.7494989633560181\n",
      "Epoch 7730: Training Loss: 0.1351385861635208 Validation Loss: 0.7496504187583923\n",
      "Epoch 7731: Training Loss: 0.13513686756292978 Validation Loss: 0.7488741278648376\n",
      "Epoch 7732: Training Loss: 0.13515762984752655 Validation Loss: 0.7484700083732605\n",
      "Epoch 7733: Training Loss: 0.13494448363780975 Validation Loss: 0.748089075088501\n",
      "Epoch 7734: Training Loss: 0.13498048235972723 Validation Loss: 0.7480958700180054\n",
      "Epoch 7735: Training Loss: 0.13621894518534342 Validation Loss: 0.7488743662834167\n",
      "Epoch 7736: Training Loss: 0.13515935838222504 Validation Loss: 0.749677836894989\n",
      "Epoch 7737: Training Loss: 0.13502169152100882 Validation Loss: 0.7496223449707031\n",
      "Epoch 7738: Training Loss: 0.1355284551779429 Validation Loss: 0.7491791844367981\n",
      "Epoch 7739: Training Loss: 0.134968231121699 Validation Loss: 0.7486717104911804\n",
      "Epoch 7740: Training Loss: 0.1351676732301712 Validation Loss: 0.7477874159812927\n",
      "Epoch 7741: Training Loss: 0.1360561673839887 Validation Loss: 0.7485339045524597\n",
      "Epoch 7742: Training Loss: 0.13550369441509247 Validation Loss: 0.7491239905357361\n",
      "Epoch 7743: Training Loss: 0.13501779238382974 Validation Loss: 0.7491127252578735\n",
      "Epoch 7744: Training Loss: 0.13482006887594858 Validation Loss: 0.749034583568573\n",
      "Epoch 7745: Training Loss: 0.13527185718218485 Validation Loss: 0.7496024966239929\n",
      "Epoch 7746: Training Loss: 0.13511350254217783 Validation Loss: 0.748888373374939\n",
      "Epoch 7747: Training Loss: 0.13515367607275644 Validation Loss: 0.7493054866790771\n",
      "Epoch 7748: Training Loss: 0.13484243055184683 Validation Loss: 0.749834418296814\n",
      "Epoch 7749: Training Loss: 0.134980209171772 Validation Loss: 0.7494825720787048\n",
      "Epoch 7750: Training Loss: 0.1344628930091858 Validation Loss: 0.7495397925376892\n",
      "Epoch 7751: Training Loss: 0.1350408891836802 Validation Loss: 0.7492232322692871\n",
      "Epoch 7752: Training Loss: 0.13481338322162628 Validation Loss: 0.7495421171188354\n",
      "Epoch 7753: Training Loss: 0.13479929169019064 Validation Loss: 0.7489458322525024\n",
      "Epoch 7754: Training Loss: 0.13462913036346436 Validation Loss: 0.7491247057914734\n",
      "Epoch 7755: Training Loss: 0.13501346608002981 Validation Loss: 0.7490620613098145\n",
      "Epoch 7756: Training Loss: 0.1352190151810646 Validation Loss: 0.7487383484840393\n",
      "Epoch 7757: Training Loss: 0.13473779211441675 Validation Loss: 0.7487584352493286\n",
      "Epoch 7758: Training Loss: 0.134441909690698 Validation Loss: 0.7484748363494873\n",
      "Epoch 7759: Training Loss: 0.13488943378130594 Validation Loss: 0.7484184503555298\n",
      "Epoch 7760: Training Loss: 0.135809488594532 Validation Loss: 0.7493492960929871\n",
      "Epoch 7761: Training Loss: 0.13464980820814768 Validation Loss: 0.7498124241828918\n",
      "Epoch 7762: Training Loss: 0.134579469760259 Validation Loss: 0.7488167881965637\n",
      "Epoch 7763: Training Loss: 0.13485126197338104 Validation Loss: 0.7481880187988281\n",
      "Epoch 7764: Training Loss: 0.1348707228899002 Validation Loss: 0.7482994794845581\n",
      "Epoch 7765: Training Loss: 0.13458270082871118 Validation Loss: 0.7484822869300842\n",
      "Epoch 7766: Training Loss: 0.134732315937678 Validation Loss: 0.7490260601043701\n",
      "Epoch 7767: Training Loss: 0.13488899419705072 Validation Loss: 0.7491692304611206\n",
      "Epoch 7768: Training Loss: 0.1343891272942225 Validation Loss: 0.7496162056922913\n",
      "Epoch 7769: Training Loss: 0.13471206029256186 Validation Loss: 0.749693751335144\n",
      "Epoch 7770: Training Loss: 0.1348562240600586 Validation Loss: 0.7501637935638428\n",
      "Epoch 7771: Training Loss: 0.1344150354464849 Validation Loss: 0.7497529983520508\n",
      "Epoch 7772: Training Loss: 0.1346809839208921 Validation Loss: 0.749348521232605\n",
      "Epoch 7773: Training Loss: 0.13588225096464157 Validation Loss: 0.7490119338035583\n",
      "Epoch 7774: Training Loss: 0.13484233369429907 Validation Loss: 0.749068558216095\n",
      "Epoch 7775: Training Loss: 0.1345688154300054 Validation Loss: 0.748767077922821\n",
      "Epoch 7776: Training Loss: 0.1345501889785131 Validation Loss: 0.750026285648346\n",
      "Epoch 7777: Training Loss: 0.13446369767189026 Validation Loss: 0.7502756118774414\n",
      "Epoch 7778: Training Loss: 0.13450250029563904 Validation Loss: 0.7497490048408508\n",
      "Epoch 7779: Training Loss: 0.13483873258034387 Validation Loss: 0.7492926716804504\n",
      "Epoch 7780: Training Loss: 0.13489626596371332 Validation Loss: 0.749098002910614\n",
      "Epoch 7781: Training Loss: 0.13441879550615946 Validation Loss: 0.7483841776847839\n",
      "Epoch 7782: Training Loss: 0.13426121572653452 Validation Loss: 0.7482216954231262\n",
      "Epoch 7783: Training Loss: 0.13431610415379205 Validation Loss: 0.7484787702560425\n",
      "Epoch 7784: Training Loss: 0.13408300280570984 Validation Loss: 0.7490792274475098\n",
      "Epoch 7785: Training Loss: 0.13433142006397247 Validation Loss: 0.7489968538284302\n",
      "Epoch 7786: Training Loss: 0.13514121621847153 Validation Loss: 0.7494552135467529\n",
      "Epoch 7787: Training Loss: 0.1344566891590754 Validation Loss: 0.7497586011886597\n",
      "Epoch 7788: Training Loss: 0.1340294157465299 Validation Loss: 0.7495463490486145\n",
      "Epoch 7789: Training Loss: 0.1341503361860911 Validation Loss: 0.7497104406356812\n",
      "Epoch 7790: Training Loss: 0.13419794042905173 Validation Loss: 0.7497134208679199\n",
      "Epoch 7791: Training Loss: 0.1348259076476097 Validation Loss: 0.749409019947052\n",
      "Epoch 7792: Training Loss: 0.13416080673535666 Validation Loss: 0.7493752241134644\n",
      "Epoch 7793: Training Loss: 0.13435934980710348 Validation Loss: 0.7500189542770386\n",
      "Epoch 7794: Training Loss: 0.1338874250650406 Validation Loss: 0.7498616576194763\n",
      "Epoch 7795: Training Loss: 0.13409439474344254 Validation Loss: 0.7501739263534546\n",
      "Epoch 7796: Training Loss: 0.1339697539806366 Validation Loss: 0.7494557499885559\n",
      "Epoch 7797: Training Loss: 0.13421926399072012 Validation Loss: 0.7499979138374329\n",
      "Epoch 7798: Training Loss: 0.1341562569141388 Validation Loss: 0.7493189573287964\n",
      "Epoch 7799: Training Loss: 0.13410616417725882 Validation Loss: 0.7495643496513367\n",
      "Epoch 7800: Training Loss: 0.13410313924153647 Validation Loss: 0.749466598033905\n",
      "Epoch 7801: Training Loss: 0.13403284549713135 Validation Loss: 0.7490394115447998\n",
      "Epoch 7802: Training Loss: 0.13395128150780997 Validation Loss: 0.7491346597671509\n",
      "Epoch 7803: Training Loss: 0.13401838888724646 Validation Loss: 0.7490765452384949\n",
      "Epoch 7804: Training Loss: 0.13402659694353738 Validation Loss: 0.7491416335105896\n",
      "Epoch 7805: Training Loss: 0.13410570720831552 Validation Loss: 0.7488766312599182\n",
      "Epoch 7806: Training Loss: 0.13515359411636987 Validation Loss: 0.7490838170051575\n",
      "Epoch 7807: Training Loss: 0.13400491327047348 Validation Loss: 0.7493913769721985\n",
      "Epoch 7808: Training Loss: 0.13405106713374457 Validation Loss: 0.7494995594024658\n",
      "Epoch 7809: Training Loss: 0.1347756658991178 Validation Loss: 0.7495731115341187\n",
      "Epoch 7810: Training Loss: 0.1338161677122116 Validation Loss: 0.7494837641716003\n",
      "Epoch 7811: Training Loss: 0.13384523491064707 Validation Loss: 0.749369204044342\n",
      "Epoch 7812: Training Loss: 0.13417162746191025 Validation Loss: 0.7489274144172668\n",
      "Epoch 7813: Training Loss: 0.13447578251361847 Validation Loss: 0.7493708729743958\n",
      "Epoch 7814: Training Loss: 0.13383494317531586 Validation Loss: 0.7499913573265076\n",
      "Epoch 7815: Training Loss: 0.13410032788912454 Validation Loss: 0.7504285573959351\n",
      "Epoch 7816: Training Loss: 0.1336473971605301 Validation Loss: 0.750763475894928\n",
      "Epoch 7817: Training Loss: 0.13479048758745193 Validation Loss: 0.7498160600662231\n",
      "Epoch 7818: Training Loss: 0.13386854032675424 Validation Loss: 0.7503980398178101\n",
      "Epoch 7819: Training Loss: 0.13481623927752176 Validation Loss: 0.7500814199447632\n",
      "Epoch 7820: Training Loss: 0.13400670140981674 Validation Loss: 0.7499515414237976\n",
      "Epoch 7821: Training Loss: 0.13412722696860632 Validation Loss: 0.7496216893196106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7822: Training Loss: 0.1345230663816134 Validation Loss: 0.750474214553833\n",
      "Epoch 7823: Training Loss: 0.13380752503871918 Validation Loss: 0.7500454187393188\n",
      "Epoch 7824: Training Loss: 0.133792611459891 Validation Loss: 0.7489845156669617\n",
      "Epoch 7825: Training Loss: 0.13392933706442514 Validation Loss: 0.7487303614616394\n",
      "Epoch 7826: Training Loss: 0.13390465080738068 Validation Loss: 0.7491480708122253\n",
      "Epoch 7827: Training Loss: 0.1338613157471021 Validation Loss: 0.7498232126235962\n",
      "Epoch 7828: Training Loss: 0.1337700436512629 Validation Loss: 0.7497395277023315\n",
      "Epoch 7829: Training Loss: 0.13355987270673117 Validation Loss: 0.7502601742744446\n",
      "Epoch 7830: Training Loss: 0.13398887713750204 Validation Loss: 0.750020444393158\n",
      "Epoch 7831: Training Loss: 0.13374979545672736 Validation Loss: 0.7498349547386169\n",
      "Epoch 7832: Training Loss: 0.13361618667840958 Validation Loss: 0.7497100234031677\n",
      "Epoch 7833: Training Loss: 0.133856271704038 Validation Loss: 0.7500139474868774\n",
      "Epoch 7834: Training Loss: 0.13351422796646753 Validation Loss: 0.7491896152496338\n",
      "Epoch 7835: Training Loss: 0.13408300032218298 Validation Loss: 0.7490454316139221\n",
      "Epoch 7836: Training Loss: 0.13390787690877914 Validation Loss: 0.7495709657669067\n",
      "Epoch 7837: Training Loss: 0.13373159368832907 Validation Loss: 0.7500908374786377\n",
      "Epoch 7838: Training Loss: 0.13382766644159952 Validation Loss: 0.7501975297927856\n",
      "Epoch 7839: Training Loss: 0.13349618514378866 Validation Loss: 0.7504562735557556\n",
      "Epoch 7840: Training Loss: 0.1335421403249105 Validation Loss: 0.7503254413604736\n",
      "Epoch 7841: Training Loss: 0.133368710676829 Validation Loss: 0.7500078082084656\n",
      "Epoch 7842: Training Loss: 0.1335348760088285 Validation Loss: 0.7490845322608948\n",
      "Epoch 7843: Training Loss: 0.1334951420625051 Validation Loss: 0.7494744062423706\n",
      "Epoch 7844: Training Loss: 0.1334324131409327 Validation Loss: 0.7490635514259338\n",
      "Epoch 7845: Training Loss: 0.13366753607988358 Validation Loss: 0.7492620944976807\n",
      "Epoch 7846: Training Loss: 0.134330282608668 Validation Loss: 0.7493550181388855\n",
      "Epoch 7847: Training Loss: 0.13379461069901785 Validation Loss: 0.7505089044570923\n",
      "Epoch 7848: Training Loss: 0.1336874340971311 Validation Loss: 0.7498070597648621\n",
      "Epoch 7849: Training Loss: 0.13351399699846903 Validation Loss: 0.7495561838150024\n",
      "Epoch 7850: Training Loss: 0.13348824779192606 Validation Loss: 0.7500000596046448\n",
      "Epoch 7851: Training Loss: 0.13428671409686407 Validation Loss: 0.7499594688415527\n",
      "Epoch 7852: Training Loss: 0.13305320839087167 Validation Loss: 0.7497661113739014\n",
      "Epoch 7853: Training Loss: 0.13345319529374441 Validation Loss: 0.7501242160797119\n",
      "Epoch 7854: Training Loss: 0.13343685865402222 Validation Loss: 0.7499542236328125\n",
      "Epoch 7855: Training Loss: 0.13365486512581506 Validation Loss: 0.7503216862678528\n",
      "Epoch 7856: Training Loss: 0.1332270155350367 Validation Loss: 0.7505283951759338\n",
      "Epoch 7857: Training Loss: 0.1332626243432363 Validation Loss: 0.7499486804008484\n",
      "Epoch 7858: Training Loss: 0.13351954768101373 Validation Loss: 0.7495126724243164\n",
      "Epoch 7859: Training Loss: 0.13340216875076294 Validation Loss: 0.7500479221343994\n",
      "Epoch 7860: Training Loss: 0.13329672316710153 Validation Loss: 0.750207781791687\n",
      "Epoch 7861: Training Loss: 0.13318022092183432 Validation Loss: 0.7503823637962341\n",
      "Epoch 7862: Training Loss: 0.13322992622852325 Validation Loss: 0.7507294416427612\n",
      "Epoch 7863: Training Loss: 0.13315076132615408 Validation Loss: 0.750436544418335\n",
      "Epoch 7864: Training Loss: 0.13314068069060644 Validation Loss: 0.749656617641449\n",
      "Epoch 7865: Training Loss: 0.13333953668673834 Validation Loss: 0.7491441965103149\n",
      "Epoch 7866: Training Loss: 0.13360178470611572 Validation Loss: 0.7492761015892029\n",
      "Epoch 7867: Training Loss: 0.13324196139971414 Validation Loss: 0.749528706073761\n",
      "Epoch 7868: Training Loss: 0.1333601971467336 Validation Loss: 0.7500076293945312\n",
      "Epoch 7869: Training Loss: 0.13315807282924652 Validation Loss: 0.7498322129249573\n",
      "Epoch 7870: Training Loss: 0.13302742938200632 Validation Loss: 0.7498942613601685\n",
      "Epoch 7871: Training Loss: 0.13293913503487906 Validation Loss: 0.7505898475646973\n",
      "Epoch 7872: Training Loss: 0.13325156768163046 Validation Loss: 0.7508024573326111\n",
      "Epoch 7873: Training Loss: 0.13343855986992517 Validation Loss: 0.7515546679496765\n",
      "Epoch 7874: Training Loss: 0.13306150337060293 Validation Loss: 0.7515659928321838\n",
      "Epoch 7875: Training Loss: 0.13348101824522018 Validation Loss: 0.7515307068824768\n",
      "Epoch 7876: Training Loss: 0.1331662858525912 Validation Loss: 0.7513421177864075\n",
      "Epoch 7877: Training Loss: 0.133156880736351 Validation Loss: 0.750155508518219\n",
      "Epoch 7878: Training Loss: 0.13313361008961996 Validation Loss: 0.74952232837677\n",
      "Epoch 7879: Training Loss: 0.1341133862733841 Validation Loss: 0.7491980195045471\n",
      "Epoch 7880: Training Loss: 0.1336080034573873 Validation Loss: 0.7496384978294373\n",
      "Epoch 7881: Training Loss: 0.13320118933916092 Validation Loss: 0.7498133182525635\n",
      "Epoch 7882: Training Loss: 0.13327348232269287 Validation Loss: 0.7497115135192871\n",
      "Epoch 7883: Training Loss: 0.1330027381579081 Validation Loss: 0.7504069805145264\n",
      "Epoch 7884: Training Loss: 0.13292123874028525 Validation Loss: 0.7505736351013184\n",
      "Epoch 7885: Training Loss: 0.13298720121383667 Validation Loss: 0.7507478594779968\n",
      "Epoch 7886: Training Loss: 0.13319618006547293 Validation Loss: 0.7505022883415222\n",
      "Epoch 7887: Training Loss: 0.13292023042837778 Validation Loss: 0.7505596280097961\n",
      "Epoch 7888: Training Loss: 0.13287715117136636 Validation Loss: 0.7501434683799744\n",
      "Epoch 7889: Training Loss: 0.1328906441728274 Validation Loss: 0.7501280903816223\n",
      "Epoch 7890: Training Loss: 0.13321958730618158 Validation Loss: 0.7505484819412231\n",
      "Epoch 7891: Training Loss: 0.1331799179315567 Validation Loss: 0.750428318977356\n",
      "Epoch 7892: Training Loss: 0.13362026462952295 Validation Loss: 0.7507321834564209\n",
      "Epoch 7893: Training Loss: 0.13274523615837097 Validation Loss: 0.750340461730957\n",
      "Epoch 7894: Training Loss: 0.13282391925652823 Validation Loss: 0.7500988841056824\n",
      "Epoch 7895: Training Loss: 0.1328882152835528 Validation Loss: 0.7500123381614685\n",
      "Epoch 7896: Training Loss: 0.13301203151543936 Validation Loss: 0.7505524158477783\n",
      "Epoch 7897: Training Loss: 0.13367057343324026 Validation Loss: 0.7506844997406006\n",
      "Epoch 7898: Training Loss: 0.13303988675276437 Validation Loss: 0.7508769035339355\n",
      "Epoch 7899: Training Loss: 0.13276274005572 Validation Loss: 0.7507622241973877\n",
      "Epoch 7900: Training Loss: 0.13281197597583136 Validation Loss: 0.7504358887672424\n",
      "Epoch 7901: Training Loss: 0.13308213899532953 Validation Loss: 0.7503002882003784\n",
      "Epoch 7902: Training Loss: 0.1329269881049792 Validation Loss: 0.7508912682533264\n",
      "Epoch 7903: Training Loss: 0.13312633335590363 Validation Loss: 0.750457227230072\n",
      "Epoch 7904: Training Loss: 0.1328726808230082 Validation Loss: 0.750698447227478\n",
      "Epoch 7905: Training Loss: 0.1325931871930758 Validation Loss: 0.7509821653366089\n",
      "Epoch 7906: Training Loss: 0.1331497480471929 Validation Loss: 0.7511575818061829\n",
      "Epoch 7907: Training Loss: 0.1329715425769488 Validation Loss: 0.7511696815490723\n",
      "Epoch 7908: Training Loss: 0.13258148481448492 Validation Loss: 0.7508842349052429\n",
      "Epoch 7909: Training Loss: 0.13273325810829797 Validation Loss: 0.7511464357376099\n",
      "Epoch 7910: Training Loss: 0.13257922232151031 Validation Loss: 0.7502717971801758\n",
      "Epoch 7911: Training Loss: 0.13303646941979727 Validation Loss: 0.7505544424057007\n",
      "Epoch 7912: Training Loss: 0.13286296278238297 Validation Loss: 0.7503212094306946\n",
      "Epoch 7913: Training Loss: 0.1326571082075437 Validation Loss: 0.7514814734458923\n",
      "Epoch 7914: Training Loss: 0.1327608972787857 Validation Loss: 0.7514829039573669\n",
      "Epoch 7915: Training Loss: 0.13266473511854807 Validation Loss: 0.7515559196472168\n",
      "Epoch 7916: Training Loss: 0.13235238194465637 Validation Loss: 0.7514856457710266\n",
      "Epoch 7917: Training Loss: 0.1326659470796585 Validation Loss: 0.7505440711975098\n",
      "Epoch 7918: Training Loss: 0.13277989129225412 Validation Loss: 0.7502439022064209\n",
      "Epoch 7919: Training Loss: 0.13248962412277857 Validation Loss: 0.7497243285179138\n",
      "Epoch 7920: Training Loss: 0.13267351686954498 Validation Loss: 0.7499393820762634\n",
      "Epoch 7921: Training Loss: 0.1323212037483851 Validation Loss: 0.7507576942443848\n",
      "Epoch 7922: Training Loss: 0.13246088474988937 Validation Loss: 0.7515051364898682\n",
      "Epoch 7923: Training Loss: 0.13269457469383875 Validation Loss: 0.7518680095672607\n",
      "Epoch 7924: Training Loss: 0.13237247367699942 Validation Loss: 0.7516586780548096\n",
      "Epoch 7925: Training Loss: 0.13221822927395502 Validation Loss: 0.7511205673217773\n",
      "Epoch 7926: Training Loss: 0.13232812782128653 Validation Loss: 0.7512823343276978\n",
      "Epoch 7927: Training Loss: 0.13227135439713797 Validation Loss: 0.7510967254638672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7928: Training Loss: 0.13282492756843567 Validation Loss: 0.7510342597961426\n",
      "Epoch 7929: Training Loss: 0.1323577736814817 Validation Loss: 0.750734269618988\n",
      "Epoch 7930: Training Loss: 0.13191241025924683 Validation Loss: 0.7507957816123962\n",
      "Epoch 7931: Training Loss: 0.13213435808817545 Validation Loss: 0.7508450150489807\n",
      "Epoch 7932: Training Loss: 0.1325007602572441 Validation Loss: 0.7510136365890503\n",
      "Epoch 7933: Training Loss: 0.13209754725297293 Validation Loss: 0.7507699131965637\n",
      "Epoch 7934: Training Loss: 0.13212341318527857 Validation Loss: 0.7508935928344727\n",
      "Epoch 7935: Training Loss: 0.13208860407272974 Validation Loss: 0.7511729001998901\n",
      "Epoch 7936: Training Loss: 0.13217889269193014 Validation Loss: 0.7510908246040344\n",
      "Epoch 7937: Training Loss: 0.1328348144888878 Validation Loss: 0.7513684034347534\n",
      "Epoch 7938: Training Loss: 0.13248373319705328 Validation Loss: 0.751197874546051\n",
      "Epoch 7939: Training Loss: 0.13208626955747604 Validation Loss: 0.7516430616378784\n",
      "Epoch 7940: Training Loss: 0.13219431042671204 Validation Loss: 0.7513014078140259\n",
      "Epoch 7941: Training Loss: 0.13257655998071036 Validation Loss: 0.7512570023536682\n",
      "Epoch 7942: Training Loss: 0.13229738424221674 Validation Loss: 0.7513157725334167\n",
      "Epoch 7943: Training Loss: 0.13229191054900488 Validation Loss: 0.7514373660087585\n",
      "Epoch 7944: Training Loss: 0.1320343812306722 Validation Loss: 0.7509374618530273\n",
      "Epoch 7945: Training Loss: 0.13198494911193848 Validation Loss: 0.7502578496932983\n",
      "Epoch 7946: Training Loss: 0.13248524814844131 Validation Loss: 0.7496967315673828\n",
      "Epoch 7947: Training Loss: 0.13236302882432938 Validation Loss: 0.75020831823349\n",
      "Epoch 7948: Training Loss: 0.13225761552651724 Validation Loss: 0.7516573667526245\n",
      "Epoch 7949: Training Loss: 0.13227933645248413 Validation Loss: 0.7521910667419434\n",
      "Epoch 7950: Training Loss: 0.13210795323053995 Validation Loss: 0.7523250579833984\n",
      "Epoch 7951: Training Loss: 0.13202412178119025 Validation Loss: 0.7521786689758301\n",
      "Epoch 7952: Training Loss: 0.13326859722534815 Validation Loss: 0.7520366311073303\n",
      "Epoch 7953: Training Loss: 0.13296862691640854 Validation Loss: 0.7523959875106812\n",
      "Epoch 7954: Training Loss: 0.1322081113855044 Validation Loss: 0.7513841390609741\n",
      "Epoch 7955: Training Loss: 0.13203570246696472 Validation Loss: 0.7512741088867188\n",
      "Epoch 7956: Training Loss: 0.1321487749616305 Validation Loss: 0.7510420083999634\n",
      "Epoch 7957: Training Loss: 0.1321641132235527 Validation Loss: 0.7512871026992798\n",
      "Epoch 7958: Training Loss: 0.13199756294488907 Validation Loss: 0.7503994107246399\n",
      "Epoch 7959: Training Loss: 0.13194555044174194 Validation Loss: 0.7509480118751526\n",
      "Epoch 7960: Training Loss: 0.13300858438014984 Validation Loss: 0.7509418725967407\n",
      "Epoch 7961: Training Loss: 0.1321232666571935 Validation Loss: 0.751097559928894\n",
      "Epoch 7962: Training Loss: 0.1317439874013265 Validation Loss: 0.7510276436805725\n",
      "Epoch 7963: Training Loss: 0.13151499877373377 Validation Loss: 0.7511221766471863\n",
      "Epoch 7964: Training Loss: 0.13248349726200104 Validation Loss: 0.7510861158370972\n",
      "Epoch 7965: Training Loss: 0.13178527355194092 Validation Loss: 0.7510159015655518\n",
      "Epoch 7966: Training Loss: 0.13170427083969116 Validation Loss: 0.7510504722595215\n",
      "Epoch 7967: Training Loss: 0.1323478768269221 Validation Loss: 0.7507089972496033\n",
      "Epoch 7968: Training Loss: 0.13193213442961374 Validation Loss: 0.7507174015045166\n",
      "Epoch 7969: Training Loss: 0.1319022849202156 Validation Loss: 0.7512505650520325\n",
      "Epoch 7970: Training Loss: 0.13188935816287994 Validation Loss: 0.751578688621521\n",
      "Epoch 7971: Training Loss: 0.1319455529252688 Validation Loss: 0.752045214176178\n",
      "Epoch 7972: Training Loss: 0.13196353365977606 Validation Loss: 0.7522423267364502\n",
      "Epoch 7973: Training Loss: 0.13179649412631989 Validation Loss: 0.7519192695617676\n",
      "Epoch 7974: Training Loss: 0.13166122138500214 Validation Loss: 0.7518720030784607\n",
      "Epoch 7975: Training Loss: 0.13206875324249268 Validation Loss: 0.7518019676208496\n",
      "Epoch 7976: Training Loss: 0.13190502921740213 Validation Loss: 0.7515527009963989\n",
      "Epoch 7977: Training Loss: 0.13209117203950882 Validation Loss: 0.7523160576820374\n",
      "Epoch 7978: Training Loss: 0.1326488529642423 Validation Loss: 0.7517121434211731\n",
      "Epoch 7979: Training Loss: 0.13166406005620956 Validation Loss: 0.7514675855636597\n",
      "Epoch 7980: Training Loss: 0.13168625036875406 Validation Loss: 0.7510276436805725\n",
      "Epoch 7981: Training Loss: 0.1319655254483223 Validation Loss: 0.7517104148864746\n",
      "Epoch 7982: Training Loss: 0.13176579276720682 Validation Loss: 0.7517651319503784\n",
      "Epoch 7983: Training Loss: 0.132310522099336 Validation Loss: 0.7519145011901855\n",
      "Epoch 7984: Training Loss: 0.13145813842614493 Validation Loss: 0.751590371131897\n",
      "Epoch 7985: Training Loss: 0.13169911007086435 Validation Loss: 0.7515909075737\n",
      "Epoch 7986: Training Loss: 0.13205876449743906 Validation Loss: 0.7519590258598328\n",
      "Epoch 7987: Training Loss: 0.13235721985499063 Validation Loss: 0.7519218325614929\n",
      "Epoch 7988: Training Loss: 0.13168949882189432 Validation Loss: 0.7521452307701111\n",
      "Epoch 7989: Training Loss: 0.1310920591155688 Validation Loss: 0.7521197199821472\n",
      "Epoch 7990: Training Loss: 0.1316458781560262 Validation Loss: 0.7525438070297241\n",
      "Epoch 7991: Training Loss: 0.13137236734231314 Validation Loss: 0.7522989511489868\n",
      "Epoch 7992: Training Loss: 0.13132411489884058 Validation Loss: 0.7512367963790894\n",
      "Epoch 7993: Training Loss: 0.13180658469597498 Validation Loss: 0.7505020499229431\n",
      "Epoch 7994: Training Loss: 0.131490687529246 Validation Loss: 0.7505995631217957\n",
      "Epoch 7995: Training Loss: 0.13193091253439584 Validation Loss: 0.7504922747612\n",
      "Epoch 7996: Training Loss: 0.13251368949810663 Validation Loss: 0.751349925994873\n",
      "Epoch 7997: Training Loss: 0.1312879423300425 Validation Loss: 0.7515622973442078\n",
      "Epoch 7998: Training Loss: 0.13142448167006174 Validation Loss: 0.752350389957428\n",
      "Epoch 7999: Training Loss: 0.13215280075867972 Validation Loss: 0.7523322701454163\n",
      "Epoch 8000: Training Loss: 0.13142591963211694 Validation Loss: 0.7525002956390381\n",
      "Epoch 8001: Training Loss: 0.13190052658319473 Validation Loss: 0.7521693706512451\n",
      "Epoch 8002: Training Loss: 0.13186461726824442 Validation Loss: 0.7517167329788208\n",
      "Epoch 8003: Training Loss: 0.13135752578576407 Validation Loss: 0.7516298890113831\n",
      "Epoch 8004: Training Loss: 0.13184058914581934 Validation Loss: 0.7523508071899414\n",
      "Epoch 8005: Training Loss: 0.13166851550340652 Validation Loss: 0.7524932622909546\n",
      "Epoch 8006: Training Loss: 0.13162017862002054 Validation Loss: 0.7521934509277344\n",
      "Epoch 8007: Training Loss: 0.13141669829686484 Validation Loss: 0.7520933747291565\n",
      "Epoch 8008: Training Loss: 0.13143039246400198 Validation Loss: 0.7518407106399536\n",
      "Epoch 8009: Training Loss: 0.1315094605088234 Validation Loss: 0.7516751885414124\n",
      "Epoch 8010: Training Loss: 0.13118673612674078 Validation Loss: 0.7516325116157532\n",
      "Epoch 8011: Training Loss: 0.13176126778125763 Validation Loss: 0.7520217895507812\n",
      "Epoch 8012: Training Loss: 0.13087382912635803 Validation Loss: 0.7519884705543518\n",
      "Epoch 8013: Training Loss: 0.13140136500199637 Validation Loss: 0.7521540522575378\n",
      "Epoch 8014: Training Loss: 0.13124597569306692 Validation Loss: 0.7508766055107117\n",
      "Epoch 8015: Training Loss: 0.13128975530465445 Validation Loss: 0.750664234161377\n",
      "Epoch 8016: Training Loss: 0.1312157412370046 Validation Loss: 0.7514159083366394\n",
      "Epoch 8017: Training Loss: 0.13128838191429773 Validation Loss: 0.7513604164123535\n",
      "Epoch 8018: Training Loss: 0.1313553179303805 Validation Loss: 0.7514017224311829\n",
      "Epoch 8019: Training Loss: 0.13129589706659317 Validation Loss: 0.7522861957550049\n",
      "Epoch 8020: Training Loss: 0.13119258234898248 Validation Loss: 0.7523347735404968\n",
      "Epoch 8021: Training Loss: 0.13118881483872732 Validation Loss: 0.7520166039466858\n",
      "Epoch 8022: Training Loss: 0.13100466628869376 Validation Loss: 0.7524126768112183\n",
      "Epoch 8023: Training Loss: 0.13112729787826538 Validation Loss: 0.7531383037567139\n",
      "Epoch 8024: Training Loss: 0.13168260703484216 Validation Loss: 0.7535398006439209\n",
      "Epoch 8025: Training Loss: 0.1312499443689982 Validation Loss: 0.7526178956031799\n",
      "Epoch 8026: Training Loss: 0.13111615429321924 Validation Loss: 0.7519155144691467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8027: Training Loss: 0.13156047960122427 Validation Loss: 0.7523253560066223\n",
      "Epoch 8028: Training Loss: 0.1309469739596049 Validation Loss: 0.7516244649887085\n",
      "Epoch 8029: Training Loss: 0.13087432086467743 Validation Loss: 0.7517070770263672\n",
      "Epoch 8030: Training Loss: 0.13147470851739249 Validation Loss: 0.7526215314865112\n",
      "Epoch 8031: Training Loss: 0.13141405085722604 Validation Loss: 0.7527941465377808\n",
      "Epoch 8032: Training Loss: 0.1313462108373642 Validation Loss: 0.7521138191223145\n",
      "Epoch 8033: Training Loss: 0.1313897743821144 Validation Loss: 0.7518521547317505\n",
      "Epoch 8034: Training Loss: 0.1312837153673172 Validation Loss: 0.7514322400093079\n",
      "Epoch 8035: Training Loss: 0.13095788657665253 Validation Loss: 0.7517683506011963\n",
      "Epoch 8036: Training Loss: 0.13121435046195984 Validation Loss: 0.7516369819641113\n",
      "Epoch 8037: Training Loss: 0.1311753715078036 Validation Loss: 0.7512913942337036\n",
      "Epoch 8038: Training Loss: 0.13075411568085352 Validation Loss: 0.7519423365592957\n",
      "Epoch 8039: Training Loss: 0.1308023432890574 Validation Loss: 0.7520215511322021\n",
      "Epoch 8040: Training Loss: 0.13141274948914847 Validation Loss: 0.75221848487854\n",
      "Epoch 8041: Training Loss: 0.13255652785301208 Validation Loss: 0.7524333000183105\n",
      "Epoch 8042: Training Loss: 0.13110026717185974 Validation Loss: 0.7525851130485535\n",
      "Epoch 8043: Training Loss: 0.1306761105855306 Validation Loss: 0.7529364824295044\n",
      "Epoch 8044: Training Loss: 0.13116571307182312 Validation Loss: 0.7533661723136902\n",
      "Epoch 8045: Training Loss: 0.1314325953523318 Validation Loss: 0.7528238892555237\n",
      "Epoch 8046: Training Loss: 0.130928635597229 Validation Loss: 0.7528761625289917\n",
      "Epoch 8047: Training Loss: 0.13086684793233871 Validation Loss: 0.7528440952301025\n",
      "Epoch 8048: Training Loss: 0.1308936576048533 Validation Loss: 0.7522098422050476\n",
      "Epoch 8049: Training Loss: 0.13112914562225342 Validation Loss: 0.7520187497138977\n",
      "Epoch 8050: Training Loss: 0.13085918625195822 Validation Loss: 0.7518495321273804\n",
      "Epoch 8051: Training Loss: 0.13070764392614365 Validation Loss: 0.7519670724868774\n",
      "Epoch 8052: Training Loss: 0.13091726849476495 Validation Loss: 0.7521165609359741\n",
      "Epoch 8053: Training Loss: 0.13138396541277567 Validation Loss: 0.7530961036682129\n",
      "Epoch 8054: Training Loss: 0.13081072519222894 Validation Loss: 0.752573549747467\n",
      "Epoch 8055: Training Loss: 0.1308355008562406 Validation Loss: 0.7524612545967102\n",
      "Epoch 8056: Training Loss: 0.13169900824626288 Validation Loss: 0.7516565918922424\n",
      "Epoch 8057: Training Loss: 0.13137018432219824 Validation Loss: 0.7516084909439087\n",
      "Epoch 8058: Training Loss: 0.1307607740163803 Validation Loss: 0.7514961361885071\n",
      "Epoch 8059: Training Loss: 0.13048150390386581 Validation Loss: 0.7516524791717529\n",
      "Epoch 8060: Training Loss: 0.13099621484676996 Validation Loss: 0.7525805830955505\n",
      "Epoch 8061: Training Loss: 0.1305682212114334 Validation Loss: 0.7522793412208557\n",
      "Epoch 8062: Training Loss: 0.13091609378655752 Validation Loss: 0.7523511648178101\n",
      "Epoch 8063: Training Loss: 0.13077455759048462 Validation Loss: 0.752658486366272\n",
      "Epoch 8064: Training Loss: 0.13074390341838202 Validation Loss: 0.7527241706848145\n",
      "Epoch 8065: Training Loss: 0.13088737179835638 Validation Loss: 0.7523619532585144\n",
      "Epoch 8066: Training Loss: 0.13047452767690024 Validation Loss: 0.7532357573509216\n",
      "Epoch 8067: Training Loss: 0.13103635609149933 Validation Loss: 0.7530420422554016\n",
      "Epoch 8068: Training Loss: 0.13057278593381247 Validation Loss: 0.752938449382782\n",
      "Epoch 8069: Training Loss: 0.13180443892876306 Validation Loss: 0.753229022026062\n",
      "Epoch 8070: Training Loss: 0.13103353480497995 Validation Loss: 0.7536033987998962\n",
      "Epoch 8071: Training Loss: 0.13048236072063446 Validation Loss: 0.7537991404533386\n",
      "Epoch 8072: Training Loss: 0.13106772800286612 Validation Loss: 0.7535644769668579\n",
      "Epoch 8073: Training Loss: 0.13027775784333548 Validation Loss: 0.7533515095710754\n",
      "Epoch 8074: Training Loss: 0.1305088053146998 Validation Loss: 0.7534071803092957\n",
      "Epoch 8075: Training Loss: 0.13047590603431067 Validation Loss: 0.7523099184036255\n",
      "Epoch 8076: Training Loss: 0.13045043498277664 Validation Loss: 0.7521945834159851\n",
      "Epoch 8077: Training Loss: 0.13049032787481943 Validation Loss: 0.7525628209114075\n",
      "Epoch 8078: Training Loss: 0.13058457523584366 Validation Loss: 0.7518085837364197\n",
      "Epoch 8079: Training Loss: 0.13074714442094168 Validation Loss: 0.7521390318870544\n",
      "Epoch 8080: Training Loss: 0.13025865952173868 Validation Loss: 0.7524006366729736\n",
      "Epoch 8081: Training Loss: 0.13111134866873422 Validation Loss: 0.7530776262283325\n",
      "Epoch 8082: Training Loss: 0.1302882879972458 Validation Loss: 0.7531453371047974\n",
      "Epoch 8083: Training Loss: 0.1309907560547193 Validation Loss: 0.7533237934112549\n",
      "Epoch 8084: Training Loss: 0.13044658054908118 Validation Loss: 0.7531487941741943\n",
      "Epoch 8085: Training Loss: 0.13027288764715195 Validation Loss: 0.752597451210022\n",
      "Epoch 8086: Training Loss: 0.13035398721694946 Validation Loss: 0.7523516416549683\n",
      "Epoch 8087: Training Loss: 0.1298315798242887 Validation Loss: 0.7527115345001221\n",
      "Epoch 8088: Training Loss: 0.13042690604925156 Validation Loss: 0.7520737051963806\n",
      "Epoch 8089: Training Loss: 0.13045119494199753 Validation Loss: 0.7520597577095032\n",
      "Epoch 8090: Training Loss: 0.13015015423297882 Validation Loss: 0.7526366710662842\n",
      "Epoch 8091: Training Loss: 0.1303476889928182 Validation Loss: 0.7524434328079224\n",
      "Epoch 8092: Training Loss: 0.13058809439341226 Validation Loss: 0.7519588470458984\n",
      "Epoch 8093: Training Loss: 0.13061476995547613 Validation Loss: 0.7523328065872192\n",
      "Epoch 8094: Training Loss: 0.12965372949838638 Validation Loss: 0.7517809867858887\n",
      "Epoch 8095: Training Loss: 0.13038998345534006 Validation Loss: 0.7523103952407837\n",
      "Epoch 8096: Training Loss: 0.13039691746234894 Validation Loss: 0.753411591053009\n",
      "Epoch 8097: Training Loss: 0.13041628648837408 Validation Loss: 0.7531400918960571\n",
      "Epoch 8098: Training Loss: 0.12985934813817343 Validation Loss: 0.7534752488136292\n",
      "Epoch 8099: Training Loss: 0.13023223479588827 Validation Loss: 0.753195583820343\n",
      "Epoch 8100: Training Loss: 0.13014269868532816 Validation Loss: 0.7533417344093323\n",
      "Epoch 8101: Training Loss: 0.1300899808605512 Validation Loss: 0.753629744052887\n",
      "Epoch 8102: Training Loss: 0.1306332920988401 Validation Loss: 0.7534130215644836\n",
      "Epoch 8103: Training Loss: 0.1304090494910876 Validation Loss: 0.7534583806991577\n",
      "Epoch 8104: Training Loss: 0.13000260293483734 Validation Loss: 0.7531841397285461\n",
      "Epoch 8105: Training Loss: 0.13026065876086554 Validation Loss: 0.7526717782020569\n",
      "Epoch 8106: Training Loss: 0.13007111847400665 Validation Loss: 0.7527726888656616\n",
      "Epoch 8107: Training Loss: 0.13096180806557337 Validation Loss: 0.7534285187721252\n",
      "Epoch 8108: Training Loss: 0.13034484535455704 Validation Loss: 0.7529267072677612\n",
      "Epoch 8109: Training Loss: 0.12960490336020788 Validation Loss: 0.753689706325531\n",
      "Epoch 8110: Training Loss: 0.12978036453326544 Validation Loss: 0.7533815503120422\n",
      "Epoch 8111: Training Loss: 0.13004029045502344 Validation Loss: 0.7534669041633606\n",
      "Epoch 8112: Training Loss: 0.1298841411868731 Validation Loss: 0.7530473470687866\n",
      "Epoch 8113: Training Loss: 0.13001118352015814 Validation Loss: 0.7530732750892639\n",
      "Epoch 8114: Training Loss: 0.13042927285035452 Validation Loss: 0.7525347471237183\n",
      "Epoch 8115: Training Loss: 0.13045555353164673 Validation Loss: 0.752274751663208\n",
      "Epoch 8116: Training Loss: 0.12993638217449188 Validation Loss: 0.7524865865707397\n",
      "Epoch 8117: Training Loss: 0.13022267321745554 Validation Loss: 0.7526656985282898\n",
      "Epoch 8118: Training Loss: 0.1297133614619573 Validation Loss: 0.7525202035903931\n",
      "Epoch 8119: Training Loss: 0.12954498330752054 Validation Loss: 0.7534735202789307\n",
      "Epoch 8120: Training Loss: 0.13139730940262476 Validation Loss: 0.7531531453132629\n",
      "Epoch 8121: Training Loss: 0.12994914998610815 Validation Loss: 0.7538638710975647\n",
      "Epoch 8122: Training Loss: 0.130082073311011 Validation Loss: 0.7543461322784424\n",
      "Epoch 8123: Training Loss: 0.12987808138132095 Validation Loss: 0.7541576623916626\n",
      "Epoch 8124: Training Loss: 0.13001330941915512 Validation Loss: 0.753464937210083\n",
      "Epoch 8125: Training Loss: 0.12976263463497162 Validation Loss: 0.7524443864822388\n",
      "Epoch 8126: Training Loss: 0.12971123059590658 Validation Loss: 0.752124011516571\n",
      "Epoch 8127: Training Loss: 0.1296634996930758 Validation Loss: 0.7521193623542786\n",
      "Epoch 8128: Training Loss: 0.12984151393175125 Validation Loss: 0.7528705596923828\n",
      "Epoch 8129: Training Loss: 0.1298729528983434 Validation Loss: 0.7530900835990906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8130: Training Loss: 0.12975058207909265 Validation Loss: 0.7535651326179504\n",
      "Epoch 8131: Training Loss: 0.129476897418499 Validation Loss: 0.7531632781028748\n",
      "Epoch 8132: Training Loss: 0.12957105785608292 Validation Loss: 0.753174901008606\n",
      "Epoch 8133: Training Loss: 0.12953138103087744 Validation Loss: 0.7534993886947632\n",
      "Epoch 8134: Training Loss: 0.12988358984390894 Validation Loss: 0.7532389163970947\n",
      "Epoch 8135: Training Loss: 0.129876546561718 Validation Loss: 0.7532691955566406\n",
      "Epoch 8136: Training Loss: 0.12962441394726434 Validation Loss: 0.7532275915145874\n",
      "Epoch 8137: Training Loss: 0.12917618453502655 Validation Loss: 0.7536202073097229\n",
      "Epoch 8138: Training Loss: 0.12973400950431824 Validation Loss: 0.7540155649185181\n",
      "Epoch 8139: Training Loss: 0.12946716944376627 Validation Loss: 0.7537007331848145\n",
      "Epoch 8140: Training Loss: 0.12988473226626715 Validation Loss: 0.7540652751922607\n",
      "Epoch 8141: Training Loss: 0.12988906353712082 Validation Loss: 0.7535421848297119\n",
      "Epoch 8142: Training Loss: 0.13017866760492325 Validation Loss: 0.7530924677848816\n",
      "Epoch 8143: Training Loss: 0.1295511027177175 Validation Loss: 0.7530673742294312\n",
      "Epoch 8144: Training Loss: 0.1296523337562879 Validation Loss: 0.7529633641242981\n",
      "Epoch 8145: Training Loss: 0.12947764496008554 Validation Loss: 0.7530110478401184\n",
      "Epoch 8146: Training Loss: 0.12911543995141983 Validation Loss: 0.7531841993331909\n",
      "Epoch 8147: Training Loss: 0.13025827209154764 Validation Loss: 0.7529888153076172\n",
      "Epoch 8148: Training Loss: 0.12955298274755478 Validation Loss: 0.7530834674835205\n",
      "Epoch 8149: Training Loss: 0.12954202045996985 Validation Loss: 0.7529816627502441\n",
      "Epoch 8150: Training Loss: 0.12941025694211325 Validation Loss: 0.7533486485481262\n",
      "Epoch 8151: Training Loss: 0.1294750819603602 Validation Loss: 0.7532840967178345\n",
      "Epoch 8152: Training Loss: 0.1300694594780604 Validation Loss: 0.7537221312522888\n",
      "Epoch 8153: Training Loss: 0.12934687733650208 Validation Loss: 0.7533708214759827\n",
      "Epoch 8154: Training Loss: 0.1292430485288302 Validation Loss: 0.7536207437515259\n",
      "Epoch 8155: Training Loss: 0.12960465997457504 Validation Loss: 0.7539584040641785\n",
      "Epoch 8156: Training Loss: 0.13007255146900812 Validation Loss: 0.7545592784881592\n",
      "Epoch 8157: Training Loss: 0.12965847800175348 Validation Loss: 0.7540443539619446\n",
      "Epoch 8158: Training Loss: 0.12952898691097894 Validation Loss: 0.7532021403312683\n",
      "Epoch 8159: Training Loss: 0.1296393076578776 Validation Loss: 0.7538959980010986\n",
      "Epoch 8160: Training Loss: 0.12923109034697214 Validation Loss: 0.7538859844207764\n",
      "Epoch 8161: Training Loss: 0.1293894648551941 Validation Loss: 0.7536107897758484\n",
      "Epoch 8162: Training Loss: 0.12934186806281409 Validation Loss: 0.7534555792808533\n",
      "Epoch 8163: Training Loss: 0.1293299893538157 Validation Loss: 0.7529966831207275\n",
      "Epoch 8164: Training Loss: 0.12919759253660837 Validation Loss: 0.7536565661430359\n",
      "Epoch 8165: Training Loss: 0.1291335473457972 Validation Loss: 0.7541647553443909\n",
      "Epoch 8166: Training Loss: 0.12931224703788757 Validation Loss: 0.7542747855186462\n",
      "Epoch 8167: Training Loss: 0.12935145696004233 Validation Loss: 0.754314124584198\n",
      "Epoch 8168: Training Loss: 0.12943163514137268 Validation Loss: 0.753426194190979\n",
      "Epoch 8169: Training Loss: 0.12920915832122168 Validation Loss: 0.7528946399688721\n",
      "Epoch 8170: Training Loss: 0.12931563208500543 Validation Loss: 0.7532791495323181\n",
      "Epoch 8171: Training Loss: 0.13018225133419037 Validation Loss: 0.753535270690918\n",
      "Epoch 8172: Training Loss: 0.12930775185426077 Validation Loss: 0.7540647983551025\n",
      "Epoch 8173: Training Loss: 0.12907622754573822 Validation Loss: 0.753600001335144\n",
      "Epoch 8174: Training Loss: 0.12902201215426126 Validation Loss: 0.7535240650177002\n",
      "Epoch 8175: Training Loss: 0.12876088917255402 Validation Loss: 0.753677487373352\n",
      "Epoch 8176: Training Loss: 0.1294813578327497 Validation Loss: 0.753614604473114\n",
      "Epoch 8177: Training Loss: 0.12926921745141348 Validation Loss: 0.753296971321106\n",
      "Epoch 8178: Training Loss: 0.12890549997488657 Validation Loss: 0.7534316182136536\n",
      "Epoch 8179: Training Loss: 0.12923070043325424 Validation Loss: 0.7537869215011597\n",
      "Epoch 8180: Training Loss: 0.1294625848531723 Validation Loss: 0.7538001537322998\n",
      "Epoch 8181: Training Loss: 0.1288923720518748 Validation Loss: 0.7536994218826294\n",
      "Epoch 8182: Training Loss: 0.129103551308314 Validation Loss: 0.7538211345672607\n",
      "Epoch 8183: Training Loss: 0.12864806999762854 Validation Loss: 0.7543448805809021\n",
      "Epoch 8184: Training Loss: 0.1289982795715332 Validation Loss: 0.7540237903594971\n",
      "Epoch 8185: Training Loss: 0.12908159693082175 Validation Loss: 0.753840982913971\n",
      "Epoch 8186: Training Loss: 0.12915335843960443 Validation Loss: 0.753892719745636\n",
      "Epoch 8187: Training Loss: 0.1293288618326187 Validation Loss: 0.7541974782943726\n",
      "Epoch 8188: Training Loss: 0.12894541025161743 Validation Loss: 0.7536344528198242\n",
      "Epoch 8189: Training Loss: 0.12906052420536676 Validation Loss: 0.7540989518165588\n",
      "Epoch 8190: Training Loss: 0.1289100075761477 Validation Loss: 0.7544018626213074\n",
      "Epoch 8191: Training Loss: 0.12863597770531973 Validation Loss: 0.7540466785430908\n",
      "Epoch 8192: Training Loss: 0.13014991333087286 Validation Loss: 0.7540282011032104\n",
      "Epoch 8193: Training Loss: 0.12911042819420496 Validation Loss: 0.7543310523033142\n",
      "Epoch 8194: Training Loss: 0.12896217902501425 Validation Loss: 0.7535770535469055\n",
      "Epoch 8195: Training Loss: 0.1295576790968577 Validation Loss: 0.7536978721618652\n",
      "Epoch 8196: Training Loss: 0.12917364637056986 Validation Loss: 0.7541073560714722\n",
      "Epoch 8197: Training Loss: 0.12848033010959625 Validation Loss: 0.7543186545372009\n",
      "Epoch 8198: Training Loss: 0.128754494090875 Validation Loss: 0.7545551061630249\n",
      "Epoch 8199: Training Loss: 0.12905427813529968 Validation Loss: 0.7548380494117737\n",
      "Epoch 8200: Training Loss: 0.1289946585893631 Validation Loss: 0.753882646560669\n",
      "Epoch 8201: Training Loss: 0.1288906211654345 Validation Loss: 0.7530980110168457\n",
      "Epoch 8202: Training Loss: 0.1286600554982821 Validation Loss: 0.7532275319099426\n",
      "Epoch 8203: Training Loss: 0.12878037989139557 Validation Loss: 0.753538966178894\n",
      "Epoch 8204: Training Loss: 0.12923253079255423 Validation Loss: 0.7538580298423767\n",
      "Epoch 8205: Training Loss: 0.12897316366434097 Validation Loss: 0.7537209391593933\n",
      "Epoch 8206: Training Loss: 0.1287266438206037 Validation Loss: 0.7542920112609863\n",
      "Epoch 8207: Training Loss: 0.12869027505318323 Validation Loss: 0.7551161646842957\n",
      "Epoch 8208: Training Loss: 0.12869546314080557 Validation Loss: 0.7545298933982849\n",
      "Epoch 8209: Training Loss: 0.1287722041209539 Validation Loss: 0.7541528344154358\n",
      "Epoch 8210: Training Loss: 0.12888413667678833 Validation Loss: 0.7547829747200012\n",
      "Epoch 8211: Training Loss: 0.12878219286600748 Validation Loss: 0.7552663087844849\n",
      "Epoch 8212: Training Loss: 0.12938380489746729 Validation Loss: 0.7546863555908203\n",
      "Epoch 8213: Training Loss: 0.12922810514767966 Validation Loss: 0.7544687390327454\n",
      "Epoch 8214: Training Loss: 0.12870115538438162 Validation Loss: 0.7540234327316284\n",
      "Epoch 8215: Training Loss: 0.12841407706340155 Validation Loss: 0.753250777721405\n",
      "Epoch 8216: Training Loss: 0.1293085739016533 Validation Loss: 0.7530969381332397\n",
      "Epoch 8217: Training Loss: 0.1287035048007965 Validation Loss: 0.7537410855293274\n",
      "Epoch 8218: Training Loss: 0.12991628547509512 Validation Loss: 0.7540010809898376\n",
      "Epoch 8219: Training Loss: 0.1293016572793325 Validation Loss: 0.7547342777252197\n",
      "Epoch 8220: Training Loss: 0.1286266694466273 Validation Loss: 0.7547414898872375\n",
      "Epoch 8221: Training Loss: 0.12892517199118933 Validation Loss: 0.7548992037773132\n",
      "Epoch 8222: Training Loss: 0.129254420598348 Validation Loss: 0.7545049786567688\n",
      "Epoch 8223: Training Loss: 0.12884287784496942 Validation Loss: 0.7543216943740845\n",
      "Epoch 8224: Training Loss: 0.128528892993927 Validation Loss: 0.7545562386512756\n",
      "Epoch 8225: Training Loss: 0.12841977179050446 Validation Loss: 0.7543067932128906\n",
      "Epoch 8226: Training Loss: 0.12846759458382925 Validation Loss: 0.7546786069869995\n",
      "Epoch 8227: Training Loss: 0.1287335380911827 Validation Loss: 0.7549970149993896\n",
      "Epoch 8228: Training Loss: 0.12850622336069742 Validation Loss: 0.7554910778999329\n",
      "Epoch 8229: Training Loss: 0.12858266880114874 Validation Loss: 0.7549934983253479\n",
      "Epoch 8230: Training Loss: 0.1285810867945353 Validation Loss: 0.7549898028373718\n",
      "Epoch 8231: Training Loss: 0.12885558108488718 Validation Loss: 0.7540211081504822\n",
      "Epoch 8232: Training Loss: 0.12804920226335526 Validation Loss: 0.7535954713821411\n",
      "Epoch 8233: Training Loss: 0.12846852838993073 Validation Loss: 0.7534508109092712\n",
      "Epoch 8234: Training Loss: 0.12839324275652567 Validation Loss: 0.7532220482826233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8235: Training Loss: 0.12853277971347174 Validation Loss: 0.7539663910865784\n",
      "Epoch 8236: Training Loss: 0.12891927609841028 Validation Loss: 0.7545561194419861\n",
      "Epoch 8237: Training Loss: 0.12953720490137735 Validation Loss: 0.7547576427459717\n",
      "Epoch 8238: Training Loss: 0.1284258415301641 Validation Loss: 0.7552298307418823\n",
      "Epoch 8239: Training Loss: 0.12851342807213464 Validation Loss: 0.7547163963317871\n",
      "Epoch 8240: Training Loss: 0.12842479596535364 Validation Loss: 0.754459023475647\n",
      "Epoch 8241: Training Loss: 0.1285344734787941 Validation Loss: 0.754550039768219\n",
      "Epoch 8242: Training Loss: 0.12865814566612244 Validation Loss: 0.7552608847618103\n",
      "Epoch 8243: Training Loss: 0.1285814419388771 Validation Loss: 0.755589485168457\n",
      "Epoch 8244: Training Loss: 0.12851976603269577 Validation Loss: 0.7551168203353882\n",
      "Epoch 8245: Training Loss: 0.12840202947457632 Validation Loss: 0.7542809247970581\n",
      "Epoch 8246: Training Loss: 0.12810555597146353 Validation Loss: 0.7544202208518982\n",
      "Epoch 8247: Training Loss: 0.12827521314223608 Validation Loss: 0.7550896406173706\n",
      "Epoch 8248: Training Loss: 0.1281372532248497 Validation Loss: 0.7552840113639832\n",
      "Epoch 8249: Training Loss: 0.12802728513876596 Validation Loss: 0.755095899105072\n",
      "Epoch 8250: Training Loss: 0.12908360113700232 Validation Loss: 0.7543651461601257\n",
      "Epoch 8251: Training Loss: 0.12839007874329886 Validation Loss: 0.7550786733627319\n",
      "Epoch 8252: Training Loss: 0.12841531137625375 Validation Loss: 0.7546736598014832\n",
      "Epoch 8253: Training Loss: 0.1285060370961825 Validation Loss: 0.7543628215789795\n",
      "Epoch 8254: Training Loss: 0.1284852772951126 Validation Loss: 0.7554427981376648\n",
      "Epoch 8255: Training Loss: 0.1280925174554189 Validation Loss: 0.7546939849853516\n",
      "Epoch 8256: Training Loss: 0.1281504606207212 Validation Loss: 0.7552870512008667\n",
      "Epoch 8257: Training Loss: 0.12803236891825995 Validation Loss: 0.7555432319641113\n",
      "Epoch 8258: Training Loss: 0.12813081840674082 Validation Loss: 0.7551710605621338\n",
      "Epoch 8259: Training Loss: 0.12867964059114456 Validation Loss: 0.7555835843086243\n",
      "Epoch 8260: Training Loss: 0.12826351076364517 Validation Loss: 0.7555155754089355\n",
      "Epoch 8261: Training Loss: 0.12804552664359412 Validation Loss: 0.7552948594093323\n",
      "Epoch 8262: Training Loss: 0.1281674181421598 Validation Loss: 0.755499541759491\n",
      "Epoch 8263: Training Loss: 0.12809516241153082 Validation Loss: 0.7548671364784241\n",
      "Epoch 8264: Training Loss: 0.1279821073015531 Validation Loss: 0.7543975710868835\n",
      "Epoch 8265: Training Loss: 0.12797660628954569 Validation Loss: 0.754238486289978\n",
      "Epoch 8266: Training Loss: 0.12837309141953787 Validation Loss: 0.7541114091873169\n",
      "Epoch 8267: Training Loss: 0.12763859579960504 Validation Loss: 0.7544193267822266\n",
      "Epoch 8268: Training Loss: 0.12801369527975717 Validation Loss: 0.7548893690109253\n",
      "Epoch 8269: Training Loss: 0.12781903892755508 Validation Loss: 0.7548770308494568\n",
      "Epoch 8270: Training Loss: 0.12872897336880365 Validation Loss: 0.7540424466133118\n",
      "Epoch 8271: Training Loss: 0.12814702341953912 Validation Loss: 0.7533982992172241\n",
      "Epoch 8272: Training Loss: 0.12779858708381653 Validation Loss: 0.7541821002960205\n",
      "Epoch 8273: Training Loss: 0.12796572595834732 Validation Loss: 0.7549300193786621\n",
      "Epoch 8274: Training Loss: 0.12824360529581705 Validation Loss: 0.7557081580162048\n",
      "Epoch 8275: Training Loss: 0.12783139944076538 Validation Loss: 0.7564339637756348\n",
      "Epoch 8276: Training Loss: 0.1283124933640162 Validation Loss: 0.7565326690673828\n",
      "Epoch 8277: Training Loss: 0.12780477851629257 Validation Loss: 0.7558512091636658\n",
      "Epoch 8278: Training Loss: 0.12797003239393234 Validation Loss: 0.755150318145752\n",
      "Epoch 8279: Training Loss: 0.12807844330867132 Validation Loss: 0.7547698020935059\n",
      "Epoch 8280: Training Loss: 0.12775289764006934 Validation Loss: 0.7545709609985352\n",
      "Epoch 8281: Training Loss: 0.12781775991121927 Validation Loss: 0.7550310492515564\n",
      "Epoch 8282: Training Loss: 0.12771374732255936 Validation Loss: 0.755325973033905\n",
      "Epoch 8283: Training Loss: 0.12783531099557877 Validation Loss: 0.7555058598518372\n",
      "Epoch 8284: Training Loss: 0.12831886112689972 Validation Loss: 0.7554486393928528\n",
      "Epoch 8285: Training Loss: 0.1280425488948822 Validation Loss: 0.7536420226097107\n",
      "Epoch 8286: Training Loss: 0.12817827860514322 Validation Loss: 0.753133237361908\n",
      "Epoch 8287: Training Loss: 0.12771759182214737 Validation Loss: 0.7540498971939087\n",
      "Epoch 8288: Training Loss: 0.1278455356756846 Validation Loss: 0.7548578977584839\n",
      "Epoch 8289: Training Loss: 0.1277022659778595 Validation Loss: 0.7553701400756836\n",
      "Epoch 8290: Training Loss: 0.1277828390399615 Validation Loss: 0.7558806538581848\n",
      "Epoch 8291: Training Loss: 0.12759057680765787 Validation Loss: 0.7552105784416199\n",
      "Epoch 8292: Training Loss: 0.12770209461450577 Validation Loss: 0.7545238137245178\n",
      "Epoch 8293: Training Loss: 0.1280146191517512 Validation Loss: 0.7549272775650024\n",
      "Epoch 8294: Training Loss: 0.1280175745487213 Validation Loss: 0.755366861820221\n",
      "Epoch 8295: Training Loss: 0.1275863175590833 Validation Loss: 0.7553990483283997\n",
      "Epoch 8296: Training Loss: 0.12762346863746643 Validation Loss: 0.7560636401176453\n",
      "Epoch 8297: Training Loss: 0.12772220373153687 Validation Loss: 0.7556650638580322\n",
      "Epoch 8298: Training Loss: 0.12809938689072928 Validation Loss: 0.7549664378166199\n",
      "Epoch 8299: Training Loss: 0.1276205057899157 Validation Loss: 0.7550078630447388\n",
      "Epoch 8300: Training Loss: 0.12751264621814093 Validation Loss: 0.7550004124641418\n",
      "Epoch 8301: Training Loss: 0.12767242143551508 Validation Loss: 0.7547653913497925\n",
      "Epoch 8302: Training Loss: 0.12784598767757416 Validation Loss: 0.754839301109314\n",
      "Epoch 8303: Training Loss: 0.12792945156494775 Validation Loss: 0.7546833753585815\n",
      "Epoch 8304: Training Loss: 0.12764033675193787 Validation Loss: 0.7550470232963562\n",
      "Epoch 8305: Training Loss: 0.12763814379771551 Validation Loss: 0.7557738423347473\n",
      "Epoch 8306: Training Loss: 0.1276200662056605 Validation Loss: 0.7557498812675476\n",
      "Epoch 8307: Training Loss: 0.1275730455915133 Validation Loss: 0.756117582321167\n",
      "Epoch 8308: Training Loss: 0.1274133399128914 Validation Loss: 0.7559372782707214\n",
      "Epoch 8309: Training Loss: 0.12761767456928888 Validation Loss: 0.7553225159645081\n",
      "Epoch 8310: Training Loss: 0.12786324818929037 Validation Loss: 0.7547447085380554\n",
      "Epoch 8311: Training Loss: 0.12821430216232935 Validation Loss: 0.754550576210022\n",
      "Epoch 8312: Training Loss: 0.12781914323568344 Validation Loss: 0.7549795508384705\n",
      "Epoch 8313: Training Loss: 0.1272686024506887 Validation Loss: 0.7555816769599915\n",
      "Epoch 8314: Training Loss: 0.127140740553538 Validation Loss: 0.7558883428573608\n",
      "Epoch 8315: Training Loss: 0.1272790531317393 Validation Loss: 0.756205141544342\n",
      "Epoch 8316: Training Loss: 0.1278116206328074 Validation Loss: 0.754828929901123\n",
      "Epoch 8317: Training Loss: 0.1289329951008161 Validation Loss: 0.7546101212501526\n",
      "Epoch 8318: Training Loss: 0.1274337371190389 Validation Loss: 0.7552081942558289\n",
      "Epoch 8319: Training Loss: 0.12744449824094772 Validation Loss: 0.7557337284088135\n",
      "Epoch 8320: Training Loss: 0.12736299137274423 Validation Loss: 0.7555406093597412\n",
      "Epoch 8321: Training Loss: 0.127440445125103 Validation Loss: 0.7557070851325989\n",
      "Epoch 8322: Training Loss: 0.12721751381953558 Validation Loss: 0.7560071349143982\n",
      "Epoch 8323: Training Loss: 0.1275042469302813 Validation Loss: 0.7561168074607849\n",
      "Epoch 8324: Training Loss: 0.12755899876356125 Validation Loss: 0.756390392780304\n",
      "Epoch 8325: Training Loss: 0.1269960030913353 Validation Loss: 0.7562665939331055\n",
      "Epoch 8326: Training Loss: 0.12710932393868765 Validation Loss: 0.756012499332428\n",
      "Epoch 8327: Training Loss: 0.1271184136470159 Validation Loss: 0.7558587193489075\n",
      "Epoch 8328: Training Loss: 0.12732924272616705 Validation Loss: 0.7557811737060547\n",
      "Epoch 8329: Training Loss: 0.12757490078608194 Validation Loss: 0.7559457421302795\n",
      "Epoch 8330: Training Loss: 0.12696848809719086 Validation Loss: 0.7555162906646729\n",
      "Epoch 8331: Training Loss: 0.127353698015213 Validation Loss: 0.754707396030426\n",
      "Epoch 8332: Training Loss: 0.12780641267697015 Validation Loss: 0.7551555633544922\n",
      "Epoch 8333: Training Loss: 0.12733633319536844 Validation Loss: 0.7560747265815735\n",
      "Epoch 8334: Training Loss: 0.12732605139414468 Validation Loss: 0.7555264830589294\n",
      "Epoch 8335: Training Loss: 0.12709496915340424 Validation Loss: 0.7555803656578064\n",
      "Epoch 8336: Training Loss: 0.1272165055076281 Validation Loss: 0.7553123831748962\n",
      "Epoch 8337: Training Loss: 0.12730498611927032 Validation Loss: 0.7551189661026001\n",
      "Epoch 8338: Training Loss: 0.12714426716168722 Validation Loss: 0.7551164031028748\n",
      "Epoch 8339: Training Loss: 0.12758098791042963 Validation Loss: 0.7546452879905701\n",
      "Epoch 8340: Training Loss: 0.12769496937592825 Validation Loss: 0.7552697062492371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8341: Training Loss: 0.12749872853358588 Validation Loss: 0.7557689547538757\n",
      "Epoch 8342: Training Loss: 0.12725792825222015 Validation Loss: 0.7564849257469177\n",
      "Epoch 8343: Training Loss: 0.1272975131869316 Validation Loss: 0.7557232975959778\n",
      "Epoch 8344: Training Loss: 0.1269485428929329 Validation Loss: 0.7558368444442749\n",
      "Epoch 8345: Training Loss: 0.12670084337393442 Validation Loss: 0.7558250427246094\n",
      "Epoch 8346: Training Loss: 0.12749524414539337 Validation Loss: 0.7554132342338562\n",
      "Epoch 8347: Training Loss: 0.1273635278145472 Validation Loss: 0.7561479806900024\n",
      "Epoch 8348: Training Loss: 0.12706837803125381 Validation Loss: 0.7567005753517151\n",
      "Epoch 8349: Training Loss: 0.12795508156220117 Validation Loss: 0.7564380764961243\n",
      "Epoch 8350: Training Loss: 0.12695676336685816 Validation Loss: 0.7564866542816162\n",
      "Epoch 8351: Training Loss: 0.12687050302823386 Validation Loss: 0.7568092942237854\n",
      "Epoch 8352: Training Loss: 0.12691501528024673 Validation Loss: 0.756596028804779\n",
      "Epoch 8353: Training Loss: 0.12701388945182165 Validation Loss: 0.756752073764801\n",
      "Epoch 8354: Training Loss: 0.1271295870343844 Validation Loss: 0.755970299243927\n",
      "Epoch 8355: Training Loss: 0.12701506167650223 Validation Loss: 0.7556678652763367\n",
      "Epoch 8356: Training Loss: 0.12701938798030218 Validation Loss: 0.7564667463302612\n",
      "Epoch 8357: Training Loss: 0.1268152023355166 Validation Loss: 0.7566735148429871\n",
      "Epoch 8358: Training Loss: 0.1266736164689064 Validation Loss: 0.7555926442146301\n",
      "Epoch 8359: Training Loss: 0.1278779755036036 Validation Loss: 0.7548711895942688\n",
      "Epoch 8360: Training Loss: 0.1267386203010877 Validation Loss: 0.7548305988311768\n",
      "Epoch 8361: Training Loss: 0.1268338362375895 Validation Loss: 0.7553138732910156\n",
      "Epoch 8362: Training Loss: 0.12698985636234283 Validation Loss: 0.7561559677124023\n",
      "Epoch 8363: Training Loss: 0.12673552334308624 Validation Loss: 0.7562941312789917\n",
      "Epoch 8364: Training Loss: 0.1262944464882215 Validation Loss: 0.7563765645027161\n",
      "Epoch 8365: Training Loss: 0.12659929196039835 Validation Loss: 0.7566361427307129\n",
      "Epoch 8366: Training Loss: 0.12705223510662714 Validation Loss: 0.7554971575737\n",
      "Epoch 8367: Training Loss: 0.12767244627078375 Validation Loss: 0.7555636167526245\n",
      "Epoch 8368: Training Loss: 0.1271129474043846 Validation Loss: 0.7555526494979858\n",
      "Epoch 8369: Training Loss: 0.12682322412729263 Validation Loss: 0.7559658885002136\n",
      "Epoch 8370: Training Loss: 0.12669411798318228 Validation Loss: 0.756403923034668\n",
      "Epoch 8371: Training Loss: 0.12713825702667236 Validation Loss: 0.7560245990753174\n",
      "Epoch 8372: Training Loss: 0.1267916038632393 Validation Loss: 0.7558823823928833\n",
      "Epoch 8373: Training Loss: 0.12733548134565353 Validation Loss: 0.7558607459068298\n",
      "Epoch 8374: Training Loss: 0.1266163339217504 Validation Loss: 0.7556343674659729\n",
      "Epoch 8375: Training Loss: 0.1268281191587448 Validation Loss: 0.7557307481765747\n",
      "Epoch 8376: Training Loss: 0.12671038011709848 Validation Loss: 0.7560036778450012\n",
      "Epoch 8377: Training Loss: 0.12661816428105035 Validation Loss: 0.7565162181854248\n",
      "Epoch 8378: Training Loss: 0.1264185830950737 Validation Loss: 0.7566407918930054\n",
      "Epoch 8379: Training Loss: 0.12669555097818375 Validation Loss: 0.7564645409584045\n",
      "Epoch 8380: Training Loss: 0.12657647331555685 Validation Loss: 0.7565047740936279\n",
      "Epoch 8381: Training Loss: 0.1264836018284162 Validation Loss: 0.7563576102256775\n",
      "Epoch 8382: Training Loss: 0.12658988436063132 Validation Loss: 0.7561792135238647\n",
      "Epoch 8383: Training Loss: 0.1269154747327169 Validation Loss: 0.7560409903526306\n",
      "Epoch 8384: Training Loss: 0.12750458220640817 Validation Loss: 0.7564745545387268\n",
      "Epoch 8385: Training Loss: 0.12677558759848276 Validation Loss: 0.7563117742538452\n",
      "Epoch 8386: Training Loss: 0.12695413082838058 Validation Loss: 0.7567706108093262\n",
      "Epoch 8387: Training Loss: 0.12682546923557916 Validation Loss: 0.7568535208702087\n",
      "Epoch 8388: Training Loss: 0.12682742873827615 Validation Loss: 0.7558457255363464\n",
      "Epoch 8389: Training Loss: 0.12665074318647385 Validation Loss: 0.7558895945549011\n",
      "Epoch 8390: Training Loss: 0.1264893983801206 Validation Loss: 0.7560829520225525\n",
      "Epoch 8391: Training Loss: 0.12652302781740823 Validation Loss: 0.7557395696640015\n",
      "Epoch 8392: Training Loss: 0.12656131386756897 Validation Loss: 0.7560272216796875\n",
      "Epoch 8393: Training Loss: 0.1265464797616005 Validation Loss: 0.7559283375740051\n",
      "Epoch 8394: Training Loss: 0.12658609449863434 Validation Loss: 0.7571274042129517\n",
      "Epoch 8395: Training Loss: 0.12653197844823202 Validation Loss: 0.7564141750335693\n",
      "Epoch 8396: Training Loss: 0.1269060199459394 Validation Loss: 0.7567885518074036\n",
      "Epoch 8397: Training Loss: 0.1263111556569735 Validation Loss: 0.7566881775856018\n",
      "Epoch 8398: Training Loss: 0.12663747370243073 Validation Loss: 0.7562279105186462\n",
      "Epoch 8399: Training Loss: 0.12623593459526697 Validation Loss: 0.7564354538917542\n",
      "Epoch 8400: Training Loss: 0.1265334983666738 Validation Loss: 0.757017970085144\n",
      "Epoch 8401: Training Loss: 0.12640267858902612 Validation Loss: 0.7573140263557434\n",
      "Epoch 8402: Training Loss: 0.12652037541071573 Validation Loss: 0.7570647597312927\n",
      "Epoch 8403: Training Loss: 0.127349724372228 Validation Loss: 0.7566527724266052\n",
      "Epoch 8404: Training Loss: 0.1272593860824903 Validation Loss: 0.7564345598220825\n",
      "Epoch 8405: Training Loss: 0.12626813352108002 Validation Loss: 0.7565306425094604\n",
      "Epoch 8406: Training Loss: 0.1261722519993782 Validation Loss: 0.7560417652130127\n",
      "Epoch 8407: Training Loss: 0.12621376166741052 Validation Loss: 0.7556114196777344\n",
      "Epoch 8408: Training Loss: 0.12742826342582703 Validation Loss: 0.7554800510406494\n",
      "Epoch 8409: Training Loss: 0.12672922511895499 Validation Loss: 0.7564038038253784\n",
      "Epoch 8410: Training Loss: 0.12636668980121613 Validation Loss: 0.7570382356643677\n",
      "Epoch 8411: Training Loss: 0.1265433505177498 Validation Loss: 0.7568403482437134\n",
      "Epoch 8412: Training Loss: 0.126712404191494 Validation Loss: 0.7559780478477478\n",
      "Epoch 8413: Training Loss: 0.12635849167903265 Validation Loss: 0.756184995174408\n",
      "Epoch 8414: Training Loss: 0.12601246188084284 Validation Loss: 0.756597101688385\n",
      "Epoch 8415: Training Loss: 0.12695859372615814 Validation Loss: 0.7575308680534363\n",
      "Epoch 8416: Training Loss: 0.12693880250056586 Validation Loss: 0.7574728727340698\n",
      "Epoch 8417: Training Loss: 0.12678289165099463 Validation Loss: 0.7572872638702393\n",
      "Epoch 8418: Training Loss: 0.12605308244625726 Validation Loss: 0.7568109631538391\n",
      "Epoch 8419: Training Loss: 0.12636429568131766 Validation Loss: 0.7562094926834106\n",
      "Epoch 8420: Training Loss: 0.12686814864476523 Validation Loss: 0.7558625340461731\n",
      "Epoch 8421: Training Loss: 0.12695894141991934 Validation Loss: 0.7559852004051208\n",
      "Epoch 8422: Training Loss: 0.12627869099378586 Validation Loss: 0.7558000683784485\n",
      "Epoch 8423: Training Loss: 0.1264867385228475 Validation Loss: 0.7569988965988159\n",
      "Epoch 8424: Training Loss: 0.12607233971357346 Validation Loss: 0.7568433284759521\n",
      "Epoch 8425: Training Loss: 0.12617080410321554 Validation Loss: 0.757352888584137\n",
      "Epoch 8426: Training Loss: 0.12640060236056647 Validation Loss: 0.7569141387939453\n",
      "Epoch 8427: Training Loss: 0.12636003891626993 Validation Loss: 0.756831705570221\n",
      "Epoch 8428: Training Loss: 0.12616950770219168 Validation Loss: 0.7555789351463318\n",
      "Epoch 8429: Training Loss: 0.12609823793172836 Validation Loss: 0.7556502819061279\n",
      "Epoch 8430: Training Loss: 0.1257839798927307 Validation Loss: 0.7558150291442871\n",
      "Epoch 8431: Training Loss: 0.12630480031172434 Validation Loss: 0.7563856244087219\n",
      "Epoch 8432: Training Loss: 0.12660219023625055 Validation Loss: 0.7560327053070068\n",
      "Epoch 8433: Training Loss: 0.1262386590242386 Validation Loss: 0.7564806938171387\n",
      "Epoch 8434: Training Loss: 0.12659365932146707 Validation Loss: 0.7562485337257385\n",
      "Epoch 8435: Training Loss: 0.1261917402346929 Validation Loss: 0.7563909292221069\n",
      "Epoch 8436: Training Loss: 0.1264294683933258 Validation Loss: 0.7567066550254822\n",
      "Epoch 8437: Training Loss: 0.12618786096572876 Validation Loss: 0.7573432922363281\n",
      "Epoch 8438: Training Loss: 0.12617775052785873 Validation Loss: 0.7580004334449768\n",
      "Epoch 8439: Training Loss: 0.12666847805182138 Validation Loss: 0.7579325437545776\n",
      "Epoch 8440: Training Loss: 0.12606954326232275 Validation Loss: 0.7572667002677917\n",
      "Epoch 8441: Training Loss: 0.12638425827026367 Validation Loss: 0.7571057677268982\n",
      "Epoch 8442: Training Loss: 0.12584313253561655 Validation Loss: 0.7564966082572937\n",
      "Epoch 8443: Training Loss: 0.1259269913037618 Validation Loss: 0.7571520805358887\n",
      "Epoch 8444: Training Loss: 0.1259226401646932 Validation Loss: 0.7574158310890198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8445: Training Loss: 0.12562506397565207 Validation Loss: 0.7566689848899841\n",
      "Epoch 8446: Training Loss: 0.12626629819472632 Validation Loss: 0.7568171620368958\n",
      "Epoch 8447: Training Loss: 0.1261267215013504 Validation Loss: 0.7570785880088806\n",
      "Epoch 8448: Training Loss: 0.1261941467722257 Validation Loss: 0.7571688890457153\n",
      "Epoch 8449: Training Loss: 0.12566127628087997 Validation Loss: 0.7574207186698914\n",
      "Epoch 8450: Training Loss: 0.12620721260706583 Validation Loss: 0.7565712332725525\n",
      "Epoch 8451: Training Loss: 0.12613274902105331 Validation Loss: 0.7555620074272156\n",
      "Epoch 8452: Training Loss: 0.1258972759048144 Validation Loss: 0.7560117244720459\n",
      "Epoch 8453: Training Loss: 0.12601473182439804 Validation Loss: 0.7569882869720459\n",
      "Epoch 8454: Training Loss: 0.1259104162454605 Validation Loss: 0.7579101920127869\n",
      "Epoch 8455: Training Loss: 0.12593608846267065 Validation Loss: 0.7582032084465027\n",
      "Epoch 8456: Training Loss: 0.12581103295087814 Validation Loss: 0.7583773136138916\n",
      "Epoch 8457: Training Loss: 0.12633516391118368 Validation Loss: 0.7571955919265747\n",
      "Epoch 8458: Training Loss: 0.12601774434248605 Validation Loss: 0.7571380734443665\n",
      "Epoch 8459: Training Loss: 0.1258205994963646 Validation Loss: 0.7567107081413269\n",
      "Epoch 8460: Training Loss: 0.12629119803508124 Validation Loss: 0.7569030523300171\n",
      "Epoch 8461: Training Loss: 0.12527229885260263 Validation Loss: 0.757097065448761\n",
      "Epoch 8462: Training Loss: 0.12576432277758917 Validation Loss: 0.7572437524795532\n",
      "Epoch 8463: Training Loss: 0.12569801012674967 Validation Loss: 0.7571466565132141\n",
      "Epoch 8464: Training Loss: 0.12583391616741815 Validation Loss: 0.7573795318603516\n",
      "Epoch 8465: Training Loss: 0.12567242234945297 Validation Loss: 0.7564905285835266\n",
      "Epoch 8466: Training Loss: 0.12572475771109262 Validation Loss: 0.7563644051551819\n",
      "Epoch 8467: Training Loss: 0.1255782743295034 Validation Loss: 0.7566870450973511\n",
      "Epoch 8468: Training Loss: 0.12638921290636063 Validation Loss: 0.7565228939056396\n",
      "Epoch 8469: Training Loss: 0.12582636376221976 Validation Loss: 0.7567119002342224\n",
      "Epoch 8470: Training Loss: 0.12575631588697433 Validation Loss: 0.7571898102760315\n",
      "Epoch 8471: Training Loss: 0.1251123547554016 Validation Loss: 0.757374107837677\n",
      "Epoch 8472: Training Loss: 0.12558795511722565 Validation Loss: 0.7577525973320007\n",
      "Epoch 8473: Training Loss: 0.12685032188892365 Validation Loss: 0.7585658431053162\n",
      "Epoch 8474: Training Loss: 0.12558114777008691 Validation Loss: 0.7583946585655212\n",
      "Epoch 8475: Training Loss: 0.12577580163876215 Validation Loss: 0.7580437660217285\n",
      "Epoch 8476: Training Loss: 0.12564336756865183 Validation Loss: 0.7575597167015076\n",
      "Epoch 8477: Training Loss: 0.12597598880529404 Validation Loss: 0.7570100426673889\n",
      "Epoch 8478: Training Loss: 0.12569846957921982 Validation Loss: 0.7579212784767151\n",
      "Epoch 8479: Training Loss: 0.12524766474962234 Validation Loss: 0.7584359645843506\n",
      "Epoch 8480: Training Loss: 0.1254663740595182 Validation Loss: 0.7580044865608215\n",
      "Epoch 8481: Training Loss: 0.12529927988847098 Validation Loss: 0.7579399943351746\n",
      "Epoch 8482: Training Loss: 0.1254616528749466 Validation Loss: 0.7580703496932983\n",
      "Epoch 8483: Training Loss: 0.12651719897985458 Validation Loss: 0.7575609683990479\n",
      "Epoch 8484: Training Loss: 0.12558213621377945 Validation Loss: 0.7566430568695068\n",
      "Epoch 8485: Training Loss: 0.12541917711496353 Validation Loss: 0.7567905187606812\n",
      "Epoch 8486: Training Loss: 0.12551877150932947 Validation Loss: 0.7561770081520081\n",
      "Epoch 8487: Training Loss: 0.12630602469046912 Validation Loss: 0.7574827671051025\n",
      "Epoch 8488: Training Loss: 0.12547075003385544 Validation Loss: 0.7578988075256348\n",
      "Epoch 8489: Training Loss: 0.12568289786577225 Validation Loss: 0.7575497627258301\n",
      "Epoch 8490: Training Loss: 0.1254544978340467 Validation Loss: 0.7577735185623169\n",
      "Epoch 8491: Training Loss: 0.1257108673453331 Validation Loss: 0.7575211524963379\n",
      "Epoch 8492: Training Loss: 0.12505734215180078 Validation Loss: 0.7570967078208923\n",
      "Epoch 8493: Training Loss: 0.12543489784002304 Validation Loss: 0.757348895072937\n",
      "Epoch 8494: Training Loss: 0.12546340624491373 Validation Loss: 0.7580142617225647\n",
      "Epoch 8495: Training Loss: 0.12543897579113641 Validation Loss: 0.7580361366271973\n",
      "Epoch 8496: Training Loss: 0.12551364550987879 Validation Loss: 0.7582733631134033\n",
      "Epoch 8497: Training Loss: 0.1258665770292282 Validation Loss: 0.7583884596824646\n",
      "Epoch 8498: Training Loss: 0.12541000793377557 Validation Loss: 0.7583088278770447\n",
      "Epoch 8499: Training Loss: 0.12482878317435582 Validation Loss: 0.7572250962257385\n",
      "Epoch 8500: Training Loss: 0.1256986161073049 Validation Loss: 0.7571494579315186\n",
      "Epoch 8501: Training Loss: 0.12535690516233444 Validation Loss: 0.7573024034500122\n",
      "Epoch 8502: Training Loss: 0.12553983430067697 Validation Loss: 0.7582638263702393\n",
      "Epoch 8503: Training Loss: 0.12492213398218155 Validation Loss: 0.7582851648330688\n",
      "Epoch 8504: Training Loss: 0.1254138176639875 Validation Loss: 0.75799161195755\n",
      "Epoch 8505: Training Loss: 0.1252302204569181 Validation Loss: 0.7573725581169128\n",
      "Epoch 8506: Training Loss: 0.12520959476629892 Validation Loss: 0.7578334212303162\n",
      "Epoch 8507: Training Loss: 0.12534936517477036 Validation Loss: 0.7582101821899414\n",
      "Epoch 8508: Training Loss: 0.12499559422334035 Validation Loss: 0.7575153708457947\n",
      "Epoch 8509: Training Loss: 0.1253082975745201 Validation Loss: 0.7577189803123474\n",
      "Epoch 8510: Training Loss: 0.12529766062895456 Validation Loss: 0.7580118179321289\n",
      "Epoch 8511: Training Loss: 0.12499180436134338 Validation Loss: 0.7574182152748108\n",
      "Epoch 8512: Training Loss: 0.1251195694009463 Validation Loss: 0.7577033042907715\n",
      "Epoch 8513: Training Loss: 0.12572124600410461 Validation Loss: 0.7570826411247253\n",
      "Epoch 8514: Training Loss: 0.12513931343952814 Validation Loss: 0.7579198479652405\n",
      "Epoch 8515: Training Loss: 0.12528473883867264 Validation Loss: 0.7580894827842712\n",
      "Epoch 8516: Training Loss: 0.12513350198666254 Validation Loss: 0.7585824131965637\n",
      "Epoch 8517: Training Loss: 0.1251077577471733 Validation Loss: 0.7574006915092468\n",
      "Epoch 8518: Training Loss: 0.12571779638528824 Validation Loss: 0.7574979066848755\n",
      "Epoch 8519: Training Loss: 0.1249707539876302 Validation Loss: 0.7578319311141968\n",
      "Epoch 8520: Training Loss: 0.12507749845584235 Validation Loss: 0.7580773234367371\n",
      "Epoch 8521: Training Loss: 0.12511272231737772 Validation Loss: 0.757547914981842\n",
      "Epoch 8522: Training Loss: 0.12504522254069647 Validation Loss: 0.7579960227012634\n",
      "Epoch 8523: Training Loss: 0.12490476419528325 Validation Loss: 0.7585263252258301\n",
      "Epoch 8524: Training Loss: 0.12544896205266318 Validation Loss: 0.7582997679710388\n",
      "Epoch 8525: Training Loss: 0.12499497830867767 Validation Loss: 0.7576947808265686\n",
      "Epoch 8526: Training Loss: 0.12599979589382806 Validation Loss: 0.7581726312637329\n",
      "Epoch 8527: Training Loss: 0.12522601087888083 Validation Loss: 0.7581773996353149\n",
      "Epoch 8528: Training Loss: 0.12538538376490274 Validation Loss: 0.7576501965522766\n",
      "Epoch 8529: Training Loss: 0.12499374151229858 Validation Loss: 0.7579279541969299\n",
      "Epoch 8530: Training Loss: 0.12455873688062032 Validation Loss: 0.7578064203262329\n",
      "Epoch 8531: Training Loss: 0.12544598430395126 Validation Loss: 0.7584387063980103\n",
      "Epoch 8532: Training Loss: 0.12474927802880605 Validation Loss: 0.7586247324943542\n",
      "Epoch 8533: Training Loss: 0.12461576362450917 Validation Loss: 0.7587572336196899\n",
      "Epoch 8534: Training Loss: 0.1249635989467303 Validation Loss: 0.7577557563781738\n",
      "Epoch 8535: Training Loss: 0.12478679915269215 Validation Loss: 0.7578027248382568\n",
      "Epoch 8536: Training Loss: 0.12485657383998235 Validation Loss: 0.7577682733535767\n",
      "Epoch 8537: Training Loss: 0.12487357606490453 Validation Loss: 0.7589156031608582\n",
      "Epoch 8538: Training Loss: 0.12495250254869461 Validation Loss: 0.7582634091377258\n",
      "Epoch 8539: Training Loss: 0.12451579670111339 Validation Loss: 0.7582637667655945\n",
      "Epoch 8540: Training Loss: 0.12485406547784805 Validation Loss: 0.7578140497207642\n",
      "Epoch 8541: Training Loss: 0.12487549086411794 Validation Loss: 0.7581124901771545\n",
      "Epoch 8542: Training Loss: 0.1244962935646375 Validation Loss: 0.7582241296768188\n",
      "Epoch 8543: Training Loss: 0.12468853841225307 Validation Loss: 0.7581906914710999\n",
      "Epoch 8544: Training Loss: 0.12539412826299667 Validation Loss: 0.7583362460136414\n",
      "Epoch 8545: Training Loss: 0.1247160832087199 Validation Loss: 0.7578308582305908\n",
      "Epoch 8546: Training Loss: 0.12496344496806462 Validation Loss: 0.7577706575393677\n",
      "Epoch 8547: Training Loss: 0.12463321288426717 Validation Loss: 0.7580768465995789\n",
      "Epoch 8548: Training Loss: 0.12505763520797095 Validation Loss: 0.7583560943603516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8549: Training Loss: 0.12463630984226863 Validation Loss: 0.7582551836967468\n",
      "Epoch 8550: Training Loss: 0.12490333865086238 Validation Loss: 0.7587025761604309\n",
      "Epoch 8551: Training Loss: 0.1247619738181432 Validation Loss: 0.7587895393371582\n",
      "Epoch 8552: Training Loss: 0.12457202126582463 Validation Loss: 0.7583327293395996\n",
      "Epoch 8553: Training Loss: 0.12542287011941275 Validation Loss: 0.7576318979263306\n",
      "Epoch 8554: Training Loss: 0.12551566461722055 Validation Loss: 0.7580755352973938\n",
      "Epoch 8555: Training Loss: 0.12448695053656895 Validation Loss: 0.7580713033676147\n",
      "Epoch 8556: Training Loss: 0.1246294230222702 Validation Loss: 0.7581833600997925\n",
      "Epoch 8557: Training Loss: 0.1247641642888387 Validation Loss: 0.7584433555603027\n",
      "Epoch 8558: Training Loss: 0.12439196805159251 Validation Loss: 0.7589070796966553\n",
      "Epoch 8559: Training Loss: 0.12443454563617706 Validation Loss: 0.7590221762657166\n",
      "Epoch 8560: Training Loss: 0.12483054399490356 Validation Loss: 0.7578135132789612\n",
      "Epoch 8561: Training Loss: 0.1248248890042305 Validation Loss: 0.7568625807762146\n",
      "Epoch 8562: Training Loss: 0.12473248690366745 Validation Loss: 0.7575730681419373\n",
      "Epoch 8563: Training Loss: 0.12467513978481293 Validation Loss: 0.7581444978713989\n",
      "Epoch 8564: Training Loss: 0.12470701336860657 Validation Loss: 0.7588921189308167\n",
      "Epoch 8565: Training Loss: 0.12466487785180409 Validation Loss: 0.758083164691925\n",
      "Epoch 8566: Training Loss: 0.1245335837205251 Validation Loss: 0.758635938167572\n",
      "Epoch 8567: Training Loss: 0.12453880657752354 Validation Loss: 0.7583200931549072\n",
      "Epoch 8568: Training Loss: 0.12454494088888168 Validation Loss: 0.7582482695579529\n",
      "Epoch 8569: Training Loss: 0.12454936405022939 Validation Loss: 0.7572286128997803\n",
      "Epoch 8570: Training Loss: 0.12423967818419139 Validation Loss: 0.7568236589431763\n",
      "Epoch 8571: Training Loss: 0.12439355254173279 Validation Loss: 0.758194625377655\n",
      "Epoch 8572: Training Loss: 0.12449114272991817 Validation Loss: 0.7588241696357727\n",
      "Epoch 8573: Training Loss: 0.12527558455864587 Validation Loss: 0.759663462638855\n",
      "Epoch 8574: Training Loss: 0.12441425770521164 Validation Loss: 0.7601515054702759\n",
      "Epoch 8575: Training Loss: 0.12459880113601685 Validation Loss: 0.7602362632751465\n",
      "Epoch 8576: Training Loss: 0.12436933567126592 Validation Loss: 0.75932377576828\n",
      "Epoch 8577: Training Loss: 0.12421804418166478 Validation Loss: 0.7590947151184082\n",
      "Epoch 8578: Training Loss: 0.12430970370769501 Validation Loss: 0.758169412612915\n",
      "Epoch 8579: Training Loss: 0.1246750180919965 Validation Loss: 0.7578716278076172\n",
      "Epoch 8580: Training Loss: 0.12497647603352864 Validation Loss: 0.7584194540977478\n",
      "Epoch 8581: Training Loss: 0.12444187204043071 Validation Loss: 0.7588667869567871\n",
      "Epoch 8582: Training Loss: 0.12464766452709834 Validation Loss: 0.7584425806999207\n",
      "Epoch 8583: Training Loss: 0.12434380501508713 Validation Loss: 0.7584585547447205\n",
      "Epoch 8584: Training Loss: 0.12437397241592407 Validation Loss: 0.7581285834312439\n",
      "Epoch 8585: Training Loss: 0.12470721453428268 Validation Loss: 0.7584235072135925\n",
      "Epoch 8586: Training Loss: 0.12423854072888692 Validation Loss: 0.7584189176559448\n",
      "Epoch 8587: Training Loss: 0.12467912336190541 Validation Loss: 0.759353518486023\n",
      "Epoch 8588: Training Loss: 0.12433309356371562 Validation Loss: 0.759181022644043\n",
      "Epoch 8589: Training Loss: 0.1247102494041125 Validation Loss: 0.7588207125663757\n",
      "Epoch 8590: Training Loss: 0.12492598096529643 Validation Loss: 0.759068489074707\n",
      "Epoch 8591: Training Loss: 0.1252539704243342 Validation Loss: 0.7588995099067688\n",
      "Epoch 8592: Training Loss: 0.12396126985549927 Validation Loss: 0.7582021355628967\n",
      "Epoch 8593: Training Loss: 0.12419154743353526 Validation Loss: 0.7577906250953674\n",
      "Epoch 8594: Training Loss: 0.12441437939802806 Validation Loss: 0.7581968903541565\n",
      "Epoch 8595: Training Loss: 0.12439197550217311 Validation Loss: 0.75889652967453\n",
      "Epoch 8596: Training Loss: 0.12444276362657547 Validation Loss: 0.7587217092514038\n",
      "Epoch 8597: Training Loss: 0.12430072824160258 Validation Loss: 0.7585201263427734\n",
      "Epoch 8598: Training Loss: 0.12447827061017354 Validation Loss: 0.759182870388031\n",
      "Epoch 8599: Training Loss: 0.12395915637413661 Validation Loss: 0.7592086791992188\n",
      "Epoch 8600: Training Loss: 0.12418946872154872 Validation Loss: 0.7583597302436829\n",
      "Epoch 8601: Training Loss: 0.12435326725244522 Validation Loss: 0.7593036890029907\n",
      "Epoch 8602: Training Loss: 0.1247473731637001 Validation Loss: 0.7595641016960144\n",
      "Epoch 8603: Training Loss: 0.12391879161198933 Validation Loss: 0.7594002485275269\n",
      "Epoch 8604: Training Loss: 0.12395462642113368 Validation Loss: 0.759016752243042\n",
      "Epoch 8605: Training Loss: 0.12423266967137654 Validation Loss: 0.7586809396743774\n",
      "Epoch 8606: Training Loss: 0.124191219607989 Validation Loss: 0.7585161924362183\n",
      "Epoch 8607: Training Loss: 0.12432578454415004 Validation Loss: 0.7589861154556274\n",
      "Epoch 8608: Training Loss: 0.12413814912239711 Validation Loss: 0.759040355682373\n",
      "Epoch 8609: Training Loss: 0.1241105521718661 Validation Loss: 0.7592517733573914\n",
      "Epoch 8610: Training Loss: 0.12454939633607864 Validation Loss: 0.7591031193733215\n",
      "Epoch 8611: Training Loss: 0.1245341623822848 Validation Loss: 0.7590306401252747\n",
      "Epoch 8612: Training Loss: 0.12420838574568431 Validation Loss: 0.7589774131774902\n",
      "Epoch 8613: Training Loss: 0.12407696743806203 Validation Loss: 0.7589834928512573\n",
      "Epoch 8614: Training Loss: 0.12392414609591167 Validation Loss: 0.7590748071670532\n",
      "Epoch 8615: Training Loss: 0.1239890530705452 Validation Loss: 0.7585015892982483\n",
      "Epoch 8616: Training Loss: 0.12402938306331635 Validation Loss: 0.758398711681366\n",
      "Epoch 8617: Training Loss: 0.12519418199857077 Validation Loss: 0.7578237652778625\n",
      "Epoch 8618: Training Loss: 0.12394892672697704 Validation Loss: 0.7589293122291565\n",
      "Epoch 8619: Training Loss: 0.12390018006165822 Validation Loss: 0.7598042488098145\n",
      "Epoch 8620: Training Loss: 0.12379327168067296 Validation Loss: 0.7603841423988342\n",
      "Epoch 8621: Training Loss: 0.12378524740537007 Validation Loss: 0.7599527835845947\n",
      "Epoch 8622: Training Loss: 0.12374109278122584 Validation Loss: 0.7594444751739502\n",
      "Epoch 8623: Training Loss: 0.1239169438680013 Validation Loss: 0.7591635584831238\n",
      "Epoch 8624: Training Loss: 0.12389343231916428 Validation Loss: 0.7590628862380981\n",
      "Epoch 8625: Training Loss: 0.1240696186820666 Validation Loss: 0.7597676515579224\n",
      "Epoch 8626: Training Loss: 0.1238771602511406 Validation Loss: 0.7597281336784363\n",
      "Epoch 8627: Training Loss: 0.12405957529942195 Validation Loss: 0.7589964270591736\n",
      "Epoch 8628: Training Loss: 0.12421189496914546 Validation Loss: 0.7588775753974915\n",
      "Epoch 8629: Training Loss: 0.12428002059459686 Validation Loss: 0.757886528968811\n",
      "Epoch 8630: Training Loss: 0.12397094815969467 Validation Loss: 0.7587595582008362\n",
      "Epoch 8631: Training Loss: 0.12391130129496257 Validation Loss: 0.7590557336807251\n",
      "Epoch 8632: Training Loss: 0.1239796703060468 Validation Loss: 0.7590333819389343\n",
      "Epoch 8633: Training Loss: 0.12372050682703654 Validation Loss: 0.7590476870536804\n",
      "Epoch 8634: Training Loss: 0.12363759676615398 Validation Loss: 0.7587044835090637\n",
      "Epoch 8635: Training Loss: 0.12453796466191609 Validation Loss: 0.7589594125747681\n",
      "Epoch 8636: Training Loss: 0.12361673762400945 Validation Loss: 0.7589139938354492\n",
      "Epoch 8637: Training Loss: 0.12428191055854161 Validation Loss: 0.758307695388794\n",
      "Epoch 8638: Training Loss: 0.12384848296642303 Validation Loss: 0.758889377117157\n",
      "Epoch 8639: Training Loss: 0.12421045452356339 Validation Loss: 0.7600480914115906\n",
      "Epoch 8640: Training Loss: 0.1239367996652921 Validation Loss: 0.760106086730957\n",
      "Epoch 8641: Training Loss: 0.1236122598250707 Validation Loss: 0.76015704870224\n",
      "Epoch 8642: Training Loss: 0.12370411554972331 Validation Loss: 0.7598733305931091\n",
      "Epoch 8643: Training Loss: 0.1235738421479861 Validation Loss: 0.7592517733573914\n",
      "Epoch 8644: Training Loss: 0.12383960435787837 Validation Loss: 0.7588129043579102\n",
      "Epoch 8645: Training Loss: 0.12360804031292598 Validation Loss: 0.7587810158729553\n",
      "Epoch 8646: Training Loss: 0.12449533492326736 Validation Loss: 0.7598142623901367\n",
      "Epoch 8647: Training Loss: 0.12373974670966466 Validation Loss: 0.7589229941368103\n",
      "Epoch 8648: Training Loss: 0.12391242384910583 Validation Loss: 0.7584625482559204\n",
      "Epoch 8649: Training Loss: 0.12318535149097443 Validation Loss: 0.7586270570755005\n",
      "Epoch 8650: Training Loss: 0.12364482631285985 Validation Loss: 0.7589535713195801\n",
      "Epoch 8651: Training Loss: 0.12389002492030461 Validation Loss: 0.7598468065261841\n",
      "Epoch 8652: Training Loss: 0.12358313550551732 Validation Loss: 0.7599896788597107\n",
      "Epoch 8653: Training Loss: 0.12362524618705113 Validation Loss: 0.760064423084259\n",
      "Epoch 8654: Training Loss: 0.12380213538805644 Validation Loss: 0.7600104212760925\n",
      "Epoch 8655: Training Loss: 0.12382530917723973 Validation Loss: 0.7596830725669861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8656: Training Loss: 0.1235107605655988 Validation Loss: 0.7593672871589661\n",
      "Epoch 8657: Training Loss: 0.12347770979007085 Validation Loss: 0.7588767409324646\n",
      "Epoch 8658: Training Loss: 0.12345600128173828 Validation Loss: 0.759068489074707\n",
      "Epoch 8659: Training Loss: 0.12380870928366979 Validation Loss: 0.7603104710578918\n",
      "Epoch 8660: Training Loss: 0.12345468004544576 Validation Loss: 0.760280966758728\n",
      "Epoch 8661: Training Loss: 0.12346857041120529 Validation Loss: 0.759999692440033\n",
      "Epoch 8662: Training Loss: 0.12348495175441106 Validation Loss: 0.7592162489891052\n",
      "Epoch 8663: Training Loss: 0.12331742544968922 Validation Loss: 0.7593513131141663\n",
      "Epoch 8664: Training Loss: 0.12340476363897324 Validation Loss: 0.7595417499542236\n",
      "Epoch 8665: Training Loss: 0.1232857530315717 Validation Loss: 0.760306715965271\n",
      "Epoch 8666: Training Loss: 0.12342067311207454 Validation Loss: 0.7599543333053589\n",
      "Epoch 8667: Training Loss: 0.12403755883375804 Validation Loss: 0.7597567439079285\n",
      "Epoch 8668: Training Loss: 0.1233874981602033 Validation Loss: 0.7591213583946228\n",
      "Epoch 8669: Training Loss: 0.12339794884125392 Validation Loss: 0.7590332627296448\n",
      "Epoch 8670: Training Loss: 0.12326829880475998 Validation Loss: 0.758793830871582\n",
      "Epoch 8671: Training Loss: 0.12333172808090846 Validation Loss: 0.7589659690856934\n",
      "Epoch 8672: Training Loss: 0.12332428246736526 Validation Loss: 0.759235680103302\n",
      "Epoch 8673: Training Loss: 0.12253990769386292 Validation Loss: 0.7600962519645691\n",
      "Epoch 8674: Training Loss: 0.12333998829126358 Validation Loss: 0.7598393559455872\n",
      "Epoch 8675: Training Loss: 0.12353638062874477 Validation Loss: 0.7599796652793884\n",
      "Epoch 8676: Training Loss: 0.12296354025602341 Validation Loss: 0.7598331570625305\n",
      "Epoch 8677: Training Loss: 0.12310560047626495 Validation Loss: 0.7599238157272339\n",
      "Epoch 8678: Training Loss: 0.12345117330551147 Validation Loss: 0.7593051195144653\n",
      "Epoch 8679: Training Loss: 0.12340306739012401 Validation Loss: 0.7601357102394104\n",
      "Epoch 8680: Training Loss: 0.12284494191408157 Validation Loss: 0.7606010437011719\n",
      "Epoch 8681: Training Loss: 0.12338737150033315 Validation Loss: 0.7608759999275208\n",
      "Epoch 8682: Training Loss: 0.1232965091864268 Validation Loss: 0.7604912519454956\n",
      "Epoch 8683: Training Loss: 0.12344484776258469 Validation Loss: 0.7603721618652344\n",
      "Epoch 8684: Training Loss: 0.12326613316933314 Validation Loss: 0.7602137923240662\n",
      "Epoch 8685: Training Loss: 0.12343995521465938 Validation Loss: 0.7597854733467102\n",
      "Epoch 8686: Training Loss: 0.12355685234069824 Validation Loss: 0.7595766186714172\n",
      "Epoch 8687: Training Loss: 0.12322352081537247 Validation Loss: 0.759360134601593\n",
      "Epoch 8688: Training Loss: 0.12350582083066304 Validation Loss: 0.7588618993759155\n",
      "Epoch 8689: Training Loss: 0.12374354402224223 Validation Loss: 0.7587070465087891\n",
      "Epoch 8690: Training Loss: 0.12314724673827489 Validation Loss: 0.7597067356109619\n",
      "Epoch 8691: Training Loss: 0.1229990969101588 Validation Loss: 0.7600301504135132\n",
      "Epoch 8692: Training Loss: 0.12321767956018448 Validation Loss: 0.7598313689231873\n",
      "Epoch 8693: Training Loss: 0.12339744468530019 Validation Loss: 0.7590935826301575\n",
      "Epoch 8694: Training Loss: 0.12319419533014297 Validation Loss: 0.7597649693489075\n",
      "Epoch 8695: Training Loss: 0.12305190165837605 Validation Loss: 0.7599956393241882\n",
      "Epoch 8696: Training Loss: 0.12359233697255452 Validation Loss: 0.7598652243614197\n",
      "Epoch 8697: Training Loss: 0.12384810547033946 Validation Loss: 0.7607226967811584\n",
      "Epoch 8698: Training Loss: 0.12376165886720021 Validation Loss: 0.7609764933586121\n",
      "Epoch 8699: Training Loss: 0.12304320186376572 Validation Loss: 0.7605040669441223\n",
      "Epoch 8700: Training Loss: 0.12291065355141957 Validation Loss: 0.7599870562553406\n",
      "Epoch 8701: Training Loss: 0.12295372287432353 Validation Loss: 0.7595266103744507\n",
      "Epoch 8702: Training Loss: 0.12281572073698044 Validation Loss: 0.7597489953041077\n",
      "Epoch 8703: Training Loss: 0.12286486476659775 Validation Loss: 0.7598099708557129\n",
      "Epoch 8704: Training Loss: 0.12301002691189449 Validation Loss: 0.7603541612625122\n",
      "Epoch 8705: Training Loss: 0.12305294722318649 Validation Loss: 0.7603858709335327\n",
      "Epoch 8706: Training Loss: 0.12328558415174484 Validation Loss: 0.7601979374885559\n",
      "Epoch 8707: Training Loss: 0.12305193394422531 Validation Loss: 0.7596603035926819\n",
      "Epoch 8708: Training Loss: 0.12301216771205266 Validation Loss: 0.7593368291854858\n",
      "Epoch 8709: Training Loss: 0.12312869479258855 Validation Loss: 0.7593861222267151\n",
      "Epoch 8710: Training Loss: 0.12404125928878784 Validation Loss: 0.7607196569442749\n",
      "Epoch 8711: Training Loss: 0.12324339648087819 Validation Loss: 0.7605795860290527\n",
      "Epoch 8712: Training Loss: 0.12327674279610316 Validation Loss: 0.7607806324958801\n",
      "Epoch 8713: Training Loss: 0.12287146846453349 Validation Loss: 0.760266900062561\n",
      "Epoch 8714: Training Loss: 0.1230424369374911 Validation Loss: 0.7596322298049927\n",
      "Epoch 8715: Training Loss: 0.12278851370016734 Validation Loss: 0.7586321234703064\n",
      "Epoch 8716: Training Loss: 0.12325288852055867 Validation Loss: 0.7602587938308716\n",
      "Epoch 8717: Training Loss: 0.12299924343824387 Validation Loss: 0.7615827918052673\n",
      "Epoch 8718: Training Loss: 0.12282578398784001 Validation Loss: 0.7624480724334717\n",
      "Epoch 8719: Training Loss: 0.1230221539735794 Validation Loss: 0.7622448801994324\n",
      "Epoch 8720: Training Loss: 0.12291114528973897 Validation Loss: 0.7604410648345947\n",
      "Epoch 8721: Training Loss: 0.12273400028546651 Validation Loss: 0.7595935463905334\n",
      "Epoch 8722: Training Loss: 0.12280566245317459 Validation Loss: 0.7588325142860413\n",
      "Epoch 8723: Training Loss: 0.12267894546190898 Validation Loss: 0.7588695287704468\n",
      "Epoch 8724: Training Loss: 0.12281100700298946 Validation Loss: 0.7598854303359985\n",
      "Epoch 8725: Training Loss: 0.12332826604445775 Validation Loss: 0.7606350779533386\n",
      "Epoch 8726: Training Loss: 0.12325568248828252 Validation Loss: 0.7611801624298096\n",
      "Epoch 8727: Training Loss: 0.12268124024073283 Validation Loss: 0.7609757781028748\n",
      "Epoch 8728: Training Loss: 0.12267577151457469 Validation Loss: 0.760750412940979\n",
      "Epoch 8729: Training Loss: 0.12286852051814397 Validation Loss: 0.759820282459259\n",
      "Epoch 8730: Training Loss: 0.12277610848347346 Validation Loss: 0.7595728039741516\n",
      "Epoch 8731: Training Loss: 0.12301851063966751 Validation Loss: 0.7596061825752258\n",
      "Epoch 8732: Training Loss: 0.1226385806997617 Validation Loss: 0.7607570886611938\n",
      "Epoch 8733: Training Loss: 0.12240123003721237 Validation Loss: 0.761328399181366\n",
      "Epoch 8734: Training Loss: 0.12272183100382487 Validation Loss: 0.761475145816803\n",
      "Epoch 8735: Training Loss: 0.12284385661284129 Validation Loss: 0.7621926665306091\n",
      "Epoch 8736: Training Loss: 0.12273862461249034 Validation Loss: 0.7608790993690491\n",
      "Epoch 8737: Training Loss: 0.12264601389567058 Validation Loss: 0.7604040503501892\n",
      "Epoch 8738: Training Loss: 0.12253783891598384 Validation Loss: 0.7598412036895752\n",
      "Epoch 8739: Training Loss: 0.12257184833288193 Validation Loss: 0.7596287727355957\n",
      "Epoch 8740: Training Loss: 0.12243781238794327 Validation Loss: 0.7597909569740295\n",
      "Epoch 8741: Training Loss: 0.12269157667954762 Validation Loss: 0.7608891129493713\n",
      "Epoch 8742: Training Loss: 0.1225788692633311 Validation Loss: 0.761523425579071\n",
      "Epoch 8743: Training Loss: 0.12288295477628708 Validation Loss: 0.7612684369087219\n",
      "Epoch 8744: Training Loss: 0.12264906366666158 Validation Loss: 0.7604764699935913\n",
      "Epoch 8745: Training Loss: 0.1226080134510994 Validation Loss: 0.7598703503608704\n",
      "Epoch 8746: Training Loss: 0.12431580573320389 Validation Loss: 0.7600167393684387\n",
      "Epoch 8747: Training Loss: 0.12258581817150116 Validation Loss: 0.7609023451805115\n",
      "Epoch 8748: Training Loss: 0.1227999081214269 Validation Loss: 0.7608602643013\n",
      "Epoch 8749: Training Loss: 0.12254414210716884 Validation Loss: 0.7610418200492859\n",
      "Epoch 8750: Training Loss: 0.12284884353478749 Validation Loss: 0.7615295052528381\n",
      "Epoch 8751: Training Loss: 0.12247710178295772 Validation Loss: 0.7608532309532166\n",
      "Epoch 8752: Training Loss: 0.12247822682062785 Validation Loss: 0.7605379223823547\n",
      "Epoch 8753: Training Loss: 0.12306381265322368 Validation Loss: 0.7605377435684204\n",
      "Epoch 8754: Training Loss: 0.1223562980691592 Validation Loss: 0.7604640126228333\n",
      "Epoch 8755: Training Loss: 0.12254609664281209 Validation Loss: 0.7615704536437988\n",
      "Epoch 8756: Training Loss: 0.12291219830513 Validation Loss: 0.7613075375556946\n",
      "Epoch 8757: Training Loss: 0.12238940596580505 Validation Loss: 0.7610137462615967\n",
      "Epoch 8758: Training Loss: 0.12261366844177246 Validation Loss: 0.7604860663414001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8759: Training Loss: 0.12248329818248749 Validation Loss: 0.7603937387466431\n",
      "Epoch 8760: Training Loss: 0.12224863469600677 Validation Loss: 0.7601987719535828\n",
      "Epoch 8761: Training Loss: 0.12226136028766632 Validation Loss: 0.7606144547462463\n",
      "Epoch 8762: Training Loss: 0.12265370041131973 Validation Loss: 0.7614948749542236\n",
      "Epoch 8763: Training Loss: 0.12283506741126378 Validation Loss: 0.7618982791900635\n",
      "Epoch 8764: Training Loss: 0.1223965585231781 Validation Loss: 0.7613858580589294\n",
      "Epoch 8765: Training Loss: 0.12244995931784312 Validation Loss: 0.7602060437202454\n",
      "Epoch 8766: Training Loss: 0.12215988834698994 Validation Loss: 0.7603729963302612\n",
      "Epoch 8767: Training Loss: 0.12240876505772273 Validation Loss: 0.759850263595581\n",
      "Epoch 8768: Training Loss: 0.12195249895254771 Validation Loss: 0.7607486248016357\n",
      "Epoch 8769: Training Loss: 0.12214371810356776 Validation Loss: 0.7613301873207092\n",
      "Epoch 8770: Training Loss: 0.12249938398599625 Validation Loss: 0.7618723511695862\n",
      "Epoch 8771: Training Loss: 0.12245164563258489 Validation Loss: 0.7613129615783691\n",
      "Epoch 8772: Training Loss: 0.12249761074781418 Validation Loss: 0.7604947686195374\n",
      "Epoch 8773: Training Loss: 0.12233739097913106 Validation Loss: 0.7604382634162903\n",
      "Epoch 8774: Training Loss: 0.12227464467287064 Validation Loss: 0.7604440450668335\n",
      "Epoch 8775: Training Loss: 0.12246618171532948 Validation Loss: 0.7612743973731995\n",
      "Epoch 8776: Training Loss: 0.12308800717194875 Validation Loss: 0.7603700757026672\n",
      "Epoch 8777: Training Loss: 0.12228910128275554 Validation Loss: 0.7607960104942322\n",
      "Epoch 8778: Training Loss: 0.12218661109606425 Validation Loss: 0.7608252167701721\n",
      "Epoch 8779: Training Loss: 0.12218685944875081 Validation Loss: 0.7613204717636108\n",
      "Epoch 8780: Training Loss: 0.12211726854244868 Validation Loss: 0.7617284655570984\n",
      "Epoch 8781: Training Loss: 0.12282947450876236 Validation Loss: 0.7611863017082214\n",
      "Epoch 8782: Training Loss: 0.12226972977320354 Validation Loss: 0.7604508996009827\n",
      "Epoch 8783: Training Loss: 0.12226157387097676 Validation Loss: 0.7604504227638245\n",
      "Epoch 8784: Training Loss: 0.12225347757339478 Validation Loss: 0.7598606944084167\n",
      "Epoch 8785: Training Loss: 0.12224311133225758 Validation Loss: 0.7606040239334106\n",
      "Epoch 8786: Training Loss: 0.12218662103017171 Validation Loss: 0.7612630128860474\n",
      "Epoch 8787: Training Loss: 0.12189448873202006 Validation Loss: 0.7619521021842957\n",
      "Epoch 8788: Training Loss: 0.12329009175300598 Validation Loss: 0.7624610662460327\n",
      "Epoch 8789: Training Loss: 0.12192519009113312 Validation Loss: 0.7619925737380981\n",
      "Epoch 8790: Training Loss: 0.12225677818059921 Validation Loss: 0.7609640955924988\n",
      "Epoch 8791: Training Loss: 0.12234384566545486 Validation Loss: 0.7603429555892944\n",
      "Epoch 8792: Training Loss: 0.12213116884231567 Validation Loss: 0.7606093883514404\n",
      "Epoch 8793: Training Loss: 0.12226490179697673 Validation Loss: 0.7604951858520508\n",
      "Epoch 8794: Training Loss: 0.12245671699444453 Validation Loss: 0.7606371641159058\n",
      "Epoch 8795: Training Loss: 0.12257740398248036 Validation Loss: 0.7609653472900391\n",
      "Epoch 8796: Training Loss: 0.12230472018321355 Validation Loss: 0.7612887620925903\n",
      "Epoch 8797: Training Loss: 0.12214979032675426 Validation Loss: 0.7622935175895691\n",
      "Epoch 8798: Training Loss: 0.12188361336787541 Validation Loss: 0.7621421813964844\n",
      "Epoch 8799: Training Loss: 0.12295077244440715 Validation Loss: 0.7610276937484741\n",
      "Epoch 8800: Training Loss: 0.12241181482871373 Validation Loss: 0.760848879814148\n",
      "Epoch 8801: Training Loss: 0.12207666287819545 Validation Loss: 0.7614813446998596\n",
      "Epoch 8802: Training Loss: 0.12266450623671214 Validation Loss: 0.7617610096931458\n",
      "Epoch 8803: Training Loss: 0.12193674097458522 Validation Loss: 0.7617486119270325\n",
      "Epoch 8804: Training Loss: 0.12246289600928624 Validation Loss: 0.7616806626319885\n",
      "Epoch 8805: Training Loss: 0.12229236960411072 Validation Loss: 0.7613911032676697\n",
      "Epoch 8806: Training Loss: 0.12195590883493423 Validation Loss: 0.7613562941551208\n",
      "Epoch 8807: Training Loss: 0.12185389548540115 Validation Loss: 0.7614638209342957\n",
      "Epoch 8808: Training Loss: 0.1222116028269132 Validation Loss: 0.7613419890403748\n",
      "Epoch 8809: Training Loss: 0.12189820905526479 Validation Loss: 0.7607580423355103\n",
      "Epoch 8810: Training Loss: 0.12181273351113002 Validation Loss: 0.7604202628135681\n",
      "Epoch 8811: Training Loss: 0.12195336570342381 Validation Loss: 0.7605218887329102\n",
      "Epoch 8812: Training Loss: 0.1218522662917773 Validation Loss: 0.7605324387550354\n",
      "Epoch 8813: Training Loss: 0.12200547258059184 Validation Loss: 0.7608692049980164\n",
      "Epoch 8814: Training Loss: 0.1219663992524147 Validation Loss: 0.7614539265632629\n",
      "Epoch 8815: Training Loss: 0.1217343012491862 Validation Loss: 0.7616830468177795\n",
      "Epoch 8816: Training Loss: 0.12259470919768016 Validation Loss: 0.7619990110397339\n",
      "Epoch 8817: Training Loss: 0.12293739368518193 Validation Loss: 0.7614431381225586\n",
      "Epoch 8818: Training Loss: 0.12230138977368672 Validation Loss: 0.7615520358085632\n",
      "Epoch 8819: Training Loss: 0.12156364073355992 Validation Loss: 0.7622829675674438\n",
      "Epoch 8820: Training Loss: 0.1214868426322937 Validation Loss: 0.7623298168182373\n",
      "Epoch 8821: Training Loss: 0.12167165925105412 Validation Loss: 0.7627622485160828\n",
      "Epoch 8822: Training Loss: 0.1218903015057246 Validation Loss: 0.7620319724082947\n",
      "Epoch 8823: Training Loss: 0.12197885910669963 Validation Loss: 0.7608241438865662\n",
      "Epoch 8824: Training Loss: 0.12164747963349025 Validation Loss: 0.7608290314674377\n",
      "Epoch 8825: Training Loss: 0.12160275628169377 Validation Loss: 0.7607729434967041\n",
      "Epoch 8826: Training Loss: 0.12171331544717152 Validation Loss: 0.760606050491333\n",
      "Epoch 8827: Training Loss: 0.12152851124604543 Validation Loss: 0.7609004974365234\n",
      "Epoch 8828: Training Loss: 0.12160522987445195 Validation Loss: 0.7612967491149902\n",
      "Epoch 8829: Training Loss: 0.12178587168455124 Validation Loss: 0.7607671022415161\n",
      "Epoch 8830: Training Loss: 0.12241645902395248 Validation Loss: 0.7618045806884766\n",
      "Epoch 8831: Training Loss: 0.12167596071958542 Validation Loss: 0.7616532444953918\n",
      "Epoch 8832: Training Loss: 0.12149951855341594 Validation Loss: 0.761626660823822\n",
      "Epoch 8833: Training Loss: 0.12169295052687328 Validation Loss: 0.7612004280090332\n",
      "Epoch 8834: Training Loss: 0.12201921393473943 Validation Loss: 0.7606086134910583\n",
      "Epoch 8835: Training Loss: 0.12145938972632091 Validation Loss: 0.7619044184684753\n",
      "Epoch 8836: Training Loss: 0.12163394937912624 Validation Loss: 0.7626885175704956\n",
      "Epoch 8837: Training Loss: 0.12202986826499303 Validation Loss: 0.7627261877059937\n",
      "Epoch 8838: Training Loss: 0.12162854770819347 Validation Loss: 0.7626934051513672\n",
      "Epoch 8839: Training Loss: 0.12173095345497131 Validation Loss: 0.7626712322235107\n",
      "Epoch 8840: Training Loss: 0.12175858020782471 Validation Loss: 0.7619767189025879\n",
      "Epoch 8841: Training Loss: 0.12161649018526077 Validation Loss: 0.7614615559577942\n",
      "Epoch 8842: Training Loss: 0.12144409368435542 Validation Loss: 0.7612871527671814\n",
      "Epoch 8843: Training Loss: 0.12162552525599797 Validation Loss: 0.7614277601242065\n",
      "Epoch 8844: Training Loss: 0.12160972505807877 Validation Loss: 0.7613921165466309\n",
      "Epoch 8845: Training Loss: 0.12180759757757187 Validation Loss: 0.7624026536941528\n",
      "Epoch 8846: Training Loss: 0.12165413796901703 Validation Loss: 0.7629664540290833\n",
      "Epoch 8847: Training Loss: 0.1217392235994339 Validation Loss: 0.7626679539680481\n",
      "Epoch 8848: Training Loss: 0.12154509375492732 Validation Loss: 0.7613189816474915\n",
      "Epoch 8849: Training Loss: 0.12151961773633957 Validation Loss: 0.7608938217163086\n",
      "Epoch 8850: Training Loss: 0.1215785766641299 Validation Loss: 0.7602103352546692\n",
      "Epoch 8851: Training Loss: 0.12146398921807607 Validation Loss: 0.7613521218299866\n",
      "Epoch 8852: Training Loss: 0.12133486072222392 Validation Loss: 0.762121319770813\n",
      "Epoch 8853: Training Loss: 0.12151880810658137 Validation Loss: 0.7622779607772827\n",
      "Epoch 8854: Training Loss: 0.12135251363118489 Validation Loss: 0.7623484134674072\n",
      "Epoch 8855: Training Loss: 0.12204235792160034 Validation Loss: 0.7616916298866272\n",
      "Epoch 8856: Training Loss: 0.12149778256813686 Validation Loss: 0.7614566683769226\n",
      "Epoch 8857: Training Loss: 0.121643361945947 Validation Loss: 0.762106716632843\n",
      "Epoch 8858: Training Loss: 0.12179295221964519 Validation Loss: 0.7615299224853516\n",
      "Epoch 8859: Training Loss: 0.12144249429305394 Validation Loss: 0.7613294720649719\n",
      "Epoch 8860: Training Loss: 0.1214585651954015 Validation Loss: 0.7626057863235474\n",
      "Epoch 8861: Training Loss: 0.12215440471967061 Validation Loss: 0.7622957229614258\n",
      "Epoch 8862: Training Loss: 0.12185225635766983 Validation Loss: 0.7625954747200012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8863: Training Loss: 0.12139450013637543 Validation Loss: 0.7622471451759338\n",
      "Epoch 8864: Training Loss: 0.12120473136504491 Validation Loss: 0.7619009017944336\n",
      "Epoch 8865: Training Loss: 0.1212181771794955 Validation Loss: 0.7617939114570618\n",
      "Epoch 8866: Training Loss: 0.12220926582813263 Validation Loss: 0.7623392343521118\n",
      "Epoch 8867: Training Loss: 0.12136890987555186 Validation Loss: 0.762224555015564\n",
      "Epoch 8868: Training Loss: 0.121224045753479 Validation Loss: 0.762312650680542\n",
      "Epoch 8869: Training Loss: 0.1213276336590449 Validation Loss: 0.7622653245925903\n",
      "Epoch 8870: Training Loss: 0.12122573951880138 Validation Loss: 0.7620954513549805\n",
      "Epoch 8871: Training Loss: 0.12140453358491261 Validation Loss: 0.7622572779655457\n",
      "Epoch 8872: Training Loss: 0.12140639126300812 Validation Loss: 0.7617375254631042\n",
      "Epoch 8873: Training Loss: 0.121468851963679 Validation Loss: 0.7617252469062805\n",
      "Epoch 8874: Training Loss: 0.12104274332523346 Validation Loss: 0.7615762948989868\n",
      "Epoch 8875: Training Loss: 0.12104050566752751 Validation Loss: 0.7619057893753052\n",
      "Epoch 8876: Training Loss: 0.12097824861605962 Validation Loss: 0.7619882822036743\n",
      "Epoch 8877: Training Loss: 0.12154075254996617 Validation Loss: 0.7618739604949951\n",
      "Epoch 8878: Training Loss: 0.12118758261203766 Validation Loss: 0.7619446516036987\n",
      "Epoch 8879: Training Loss: 0.12100219229857127 Validation Loss: 0.7623217105865479\n",
      "Epoch 8880: Training Loss: 0.1213065708676974 Validation Loss: 0.7627491354942322\n",
      "Epoch 8881: Training Loss: 0.1211068406701088 Validation Loss: 0.7624996304512024\n",
      "Epoch 8882: Training Loss: 0.12141172091166179 Validation Loss: 0.7620766758918762\n",
      "Epoch 8883: Training Loss: 0.12138508011897405 Validation Loss: 0.7625097632408142\n",
      "Epoch 8884: Training Loss: 0.12135218332211177 Validation Loss: 0.7623863220214844\n",
      "Epoch 8885: Training Loss: 0.12151511758565903 Validation Loss: 0.7625996470451355\n",
      "Epoch 8886: Training Loss: 0.12111004441976547 Validation Loss: 0.7626057863235474\n",
      "Epoch 8887: Training Loss: 0.12117826690276463 Validation Loss: 0.7621352076530457\n",
      "Epoch 8888: Training Loss: 0.1211679403980573 Validation Loss: 0.7625955939292908\n",
      "Epoch 8889: Training Loss: 0.12116219848394394 Validation Loss: 0.7622129917144775\n",
      "Epoch 8890: Training Loss: 0.12102208286523819 Validation Loss: 0.762362003326416\n",
      "Epoch 8891: Training Loss: 0.12151468793551128 Validation Loss: 0.7628999948501587\n",
      "Epoch 8892: Training Loss: 0.12166372189919154 Validation Loss: 0.7624885439872742\n",
      "Epoch 8893: Training Loss: 0.12081261972586314 Validation Loss: 0.7623418569564819\n",
      "Epoch 8894: Training Loss: 0.12103894352912903 Validation Loss: 0.7621656060218811\n",
      "Epoch 8895: Training Loss: 0.12144738435745239 Validation Loss: 0.7618979215621948\n",
      "Epoch 8896: Training Loss: 0.12105210622151692 Validation Loss: 0.7622664570808411\n",
      "Epoch 8897: Training Loss: 0.12140303601821263 Validation Loss: 0.7623032331466675\n",
      "Epoch 8898: Training Loss: 0.12133680780728658 Validation Loss: 0.7622206807136536\n",
      "Epoch 8899: Training Loss: 0.12124887853860855 Validation Loss: 0.7623271942138672\n",
      "Epoch 8900: Training Loss: 0.12102565417687099 Validation Loss: 0.7625595331192017\n",
      "Epoch 8901: Training Loss: 0.12078846494356792 Validation Loss: 0.7620784640312195\n",
      "Epoch 8902: Training Loss: 0.12177567432324092 Validation Loss: 0.7626110911369324\n",
      "Epoch 8903: Training Loss: 0.1215111364920934 Validation Loss: 0.7634289860725403\n",
      "Epoch 8904: Training Loss: 0.12093258897463481 Validation Loss: 0.7631639242172241\n",
      "Epoch 8905: Training Loss: 0.12123878548542659 Validation Loss: 0.7625532150268555\n",
      "Epoch 8906: Training Loss: 0.12096922347942989 Validation Loss: 0.7626630663871765\n",
      "Epoch 8907: Training Loss: 0.12112485369046529 Validation Loss: 0.7632089853286743\n",
      "Epoch 8908: Training Loss: 0.12116959939400355 Validation Loss: 0.7630168795585632\n",
      "Epoch 8909: Training Loss: 0.12096258252859116 Validation Loss: 0.7631812691688538\n",
      "Epoch 8910: Training Loss: 0.12206485619147618 Validation Loss: 0.7634012699127197\n",
      "Epoch 8911: Training Loss: 0.12069359173377354 Validation Loss: 0.7632819414138794\n",
      "Epoch 8912: Training Loss: 0.12096236646175385 Validation Loss: 0.7626429200172424\n",
      "Epoch 8913: Training Loss: 0.12071081499258678 Validation Loss: 0.7624855041503906\n",
      "Epoch 8914: Training Loss: 0.12064216782649358 Validation Loss: 0.7628276348114014\n",
      "Epoch 8915: Training Loss: 0.12067825843890508 Validation Loss: 0.7626871466636658\n",
      "Epoch 8916: Training Loss: 0.12104849517345428 Validation Loss: 0.7631629109382629\n",
      "Epoch 8917: Training Loss: 0.12090300023555756 Validation Loss: 0.7632412314414978\n",
      "Epoch 8918: Training Loss: 0.12083292007446289 Validation Loss: 0.7635092735290527\n",
      "Epoch 8919: Training Loss: 0.12081364790598552 Validation Loss: 0.7636522650718689\n",
      "Epoch 8920: Training Loss: 0.1209033081928889 Validation Loss: 0.7631255984306335\n",
      "Epoch 8921: Training Loss: 0.12064349899689357 Validation Loss: 0.7627732753753662\n",
      "Epoch 8922: Training Loss: 0.12107383211453755 Validation Loss: 0.762292742729187\n",
      "Epoch 8923: Training Loss: 0.12123152861992519 Validation Loss: 0.7618985772132874\n",
      "Epoch 8924: Training Loss: 0.12098411470651627 Validation Loss: 0.7630268335342407\n",
      "Epoch 8925: Training Loss: 0.1207611362139384 Validation Loss: 0.7629484534263611\n",
      "Epoch 8926: Training Loss: 0.1207239752014478 Validation Loss: 0.7627089023590088\n",
      "Epoch 8927: Training Loss: 0.12090530743201573 Validation Loss: 0.7622154951095581\n",
      "Epoch 8928: Training Loss: 0.12098154425621033 Validation Loss: 0.76188725233078\n",
      "Epoch 8929: Training Loss: 0.12060427169005077 Validation Loss: 0.7626365423202515\n",
      "Epoch 8930: Training Loss: 0.1206355740626653 Validation Loss: 0.7625890970230103\n",
      "Epoch 8931: Training Loss: 0.12045112748940785 Validation Loss: 0.7629186511039734\n",
      "Epoch 8932: Training Loss: 0.12061116099357605 Validation Loss: 0.763369619846344\n",
      "Epoch 8933: Training Loss: 0.12066270659367244 Validation Loss: 0.7637547850608826\n",
      "Epoch 8934: Training Loss: 0.12085283547639847 Validation Loss: 0.7641676068305969\n",
      "Epoch 8935: Training Loss: 0.12077172845602036 Validation Loss: 0.7633215188980103\n",
      "Epoch 8936: Training Loss: 0.12033583968877792 Validation Loss: 0.762662410736084\n",
      "Epoch 8937: Training Loss: 0.12120191752910614 Validation Loss: 0.7629883289337158\n",
      "Epoch 8938: Training Loss: 0.1208693856994311 Validation Loss: 0.763184130191803\n",
      "Epoch 8939: Training Loss: 0.12074124564727147 Validation Loss: 0.762772798538208\n",
      "Epoch 8940: Training Loss: 0.12070488929748535 Validation Loss: 0.7626733779907227\n",
      "Epoch 8941: Training Loss: 0.12077216058969498 Validation Loss: 0.7627071738243103\n",
      "Epoch 8942: Training Loss: 0.12040698279937108 Validation Loss: 0.7628853917121887\n",
      "Epoch 8943: Training Loss: 0.12051123132308324 Validation Loss: 0.7631251215934753\n",
      "Epoch 8944: Training Loss: 0.12044473489125569 Validation Loss: 0.7629173398017883\n",
      "Epoch 8945: Training Loss: 0.12046175201733907 Validation Loss: 0.7630955576896667\n",
      "Epoch 8946: Training Loss: 0.12035981317361195 Validation Loss: 0.7622092962265015\n",
      "Epoch 8947: Training Loss: 0.12034788727760315 Validation Loss: 0.7623727917671204\n",
      "Epoch 8948: Training Loss: 0.12050325175126393 Validation Loss: 0.7632582187652588\n",
      "Epoch 8949: Training Loss: 0.12068502108256023 Validation Loss: 0.7627121806144714\n",
      "Epoch 8950: Training Loss: 0.12033417820930481 Validation Loss: 0.7632366418838501\n",
      "Epoch 8951: Training Loss: 0.12045275668303172 Validation Loss: 0.762646496295929\n",
      "Epoch 8952: Training Loss: 0.12041670580705006 Validation Loss: 0.7628011703491211\n",
      "Epoch 8953: Training Loss: 0.12048650284608205 Validation Loss: 0.7636541724205017\n",
      "Epoch 8954: Training Loss: 0.12100328256686528 Validation Loss: 0.7641263604164124\n",
      "Epoch 8955: Training Loss: 0.1205026110013326 Validation Loss: 0.7641838192939758\n",
      "Epoch 8956: Training Loss: 0.12103456258773804 Validation Loss: 0.7623472809791565\n",
      "Epoch 8957: Training Loss: 0.12005734940369923 Validation Loss: 0.7622925639152527\n",
      "Epoch 8958: Training Loss: 0.12080327421426773 Validation Loss: 0.7623085975646973\n",
      "Epoch 8959: Training Loss: 0.1203395997484525 Validation Loss: 0.7625249028205872\n",
      "Epoch 8960: Training Loss: 0.12076642364263535 Validation Loss: 0.7628505825996399\n",
      "Epoch 8961: Training Loss: 0.1201845978697141 Validation Loss: 0.7628592848777771\n",
      "Epoch 8962: Training Loss: 0.12020504474639893 Validation Loss: 0.7632991075515747\n",
      "Epoch 8963: Training Loss: 0.120450754960378 Validation Loss: 0.763566255569458\n",
      "Epoch 8964: Training Loss: 0.12026266753673553 Validation Loss: 0.7634483575820923\n",
      "Epoch 8965: Training Loss: 0.12075567245483398 Validation Loss: 0.7634038925170898\n",
      "Epoch 8966: Training Loss: 0.12005884448687236 Validation Loss: 0.7633232474327087\n",
      "Epoch 8967: Training Loss: 0.12015261501073837 Validation Loss: 0.7637453675270081\n",
      "Epoch 8968: Training Loss: 0.12066241602102916 Validation Loss: 0.7643715143203735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8969: Training Loss: 0.12034326791763306 Validation Loss: 0.7639821171760559\n",
      "Epoch 8970: Training Loss: 0.12001536786556244 Validation Loss: 0.7633673548698425\n",
      "Epoch 8971: Training Loss: 0.12096601227919261 Validation Loss: 0.7641876339912415\n",
      "Epoch 8972: Training Loss: 0.12016479671001434 Validation Loss: 0.7634837627410889\n",
      "Epoch 8973: Training Loss: 0.12025312582651775 Validation Loss: 0.7629045248031616\n",
      "Epoch 8974: Training Loss: 0.11995906879504521 Validation Loss: 0.7622994780540466\n",
      "Epoch 8975: Training Loss: 0.12010994801918666 Validation Loss: 0.7623794674873352\n",
      "Epoch 8976: Training Loss: 0.12047004451354344 Validation Loss: 0.7631113529205322\n",
      "Epoch 8977: Training Loss: 0.11979231238365173 Validation Loss: 0.7635236382484436\n",
      "Epoch 8978: Training Loss: 0.12016055981318156 Validation Loss: 0.7636383771896362\n",
      "Epoch 8979: Training Loss: 0.12039909760157268 Validation Loss: 0.7636759281158447\n",
      "Epoch 8980: Training Loss: 0.12009884665409724 Validation Loss: 0.7638573050498962\n",
      "Epoch 8981: Training Loss: 0.11994267751773198 Validation Loss: 0.7638055086135864\n",
      "Epoch 8982: Training Loss: 0.12034685909748077 Validation Loss: 0.7634191513061523\n",
      "Epoch 8983: Training Loss: 0.1202923059463501 Validation Loss: 0.7642441391944885\n",
      "Epoch 8984: Training Loss: 0.12041931102673213 Validation Loss: 0.7638408541679382\n",
      "Epoch 8985: Training Loss: 0.11998836447795232 Validation Loss: 0.7640706300735474\n",
      "Epoch 8986: Training Loss: 0.12053905427455902 Validation Loss: 0.7631990313529968\n",
      "Epoch 8987: Training Loss: 0.12024136632680893 Validation Loss: 0.7636918425559998\n",
      "Epoch 8988: Training Loss: 0.12009691695372264 Validation Loss: 0.763530969619751\n",
      "Epoch 8989: Training Loss: 0.12089368204275767 Validation Loss: 0.762660801410675\n",
      "Epoch 8990: Training Loss: 0.11999999731779099 Validation Loss: 0.7632734775543213\n",
      "Epoch 8991: Training Loss: 0.12008831898371379 Validation Loss: 0.762670636177063\n",
      "Epoch 8992: Training Loss: 0.11999012778202693 Validation Loss: 0.7632520198822021\n",
      "Epoch 8993: Training Loss: 0.12090406566858292 Validation Loss: 0.7639145255088806\n",
      "Epoch 8994: Training Loss: 0.11989062279462814 Validation Loss: 0.7640969753265381\n",
      "Epoch 8995: Training Loss: 0.12007514139016469 Validation Loss: 0.763779878616333\n",
      "Epoch 8996: Training Loss: 0.12004559238751729 Validation Loss: 0.7635859847068787\n",
      "Epoch 8997: Training Loss: 0.12031361212333043 Validation Loss: 0.7636666297912598\n",
      "Epoch 8998: Training Loss: 0.12047438571850459 Validation Loss: 0.7639299631118774\n",
      "Epoch 8999: Training Loss: 0.12028740843137105 Validation Loss: 0.7650887966156006\n",
      "Epoch 9000: Training Loss: 0.11996353417634964 Validation Loss: 0.764276921749115\n",
      "Epoch 9001: Training Loss: 0.11981137096881866 Validation Loss: 0.76362544298172\n",
      "Epoch 9002: Training Loss: 0.1201600432395935 Validation Loss: 0.7634307146072388\n",
      "Epoch 9003: Training Loss: 0.12024882684151332 Validation Loss: 0.7634424567222595\n",
      "Epoch 9004: Training Loss: 0.12071297814448674 Validation Loss: 0.7634759545326233\n",
      "Epoch 9005: Training Loss: 0.12051763633886974 Validation Loss: 0.7641167044639587\n",
      "Epoch 9006: Training Loss: 0.12086418767770131 Validation Loss: 0.7639725804328918\n",
      "Epoch 9007: Training Loss: 0.12035696456829707 Validation Loss: 0.7635383009910583\n",
      "Epoch 9008: Training Loss: 0.12010051310062408 Validation Loss: 0.7629637718200684\n",
      "Epoch 9009: Training Loss: 0.12123185644547145 Validation Loss: 0.763325035572052\n",
      "Epoch 9010: Training Loss: 0.12000217288732529 Validation Loss: 0.764458179473877\n",
      "Epoch 9011: Training Loss: 0.11991412689288457 Validation Loss: 0.7659475207328796\n",
      "Epoch 9012: Training Loss: 0.12005655219157536 Validation Loss: 0.7649787068367004\n",
      "Epoch 9013: Training Loss: 0.12041082481543224 Validation Loss: 0.7644792795181274\n",
      "Epoch 9014: Training Loss: 0.12017980466286342 Validation Loss: 0.7641960382461548\n",
      "Epoch 9015: Training Loss: 0.12001065164804459 Validation Loss: 0.7640361785888672\n",
      "Epoch 9016: Training Loss: 0.119458702703317 Validation Loss: 0.7638146281242371\n",
      "Epoch 9017: Training Loss: 0.12018366903066635 Validation Loss: 0.7636573910713196\n",
      "Epoch 9018: Training Loss: 0.12078952044248581 Validation Loss: 0.7637384533882141\n",
      "Epoch 9019: Training Loss: 0.11976650357246399 Validation Loss: 0.7637330889701843\n",
      "Epoch 9020: Training Loss: 0.12005025645097096 Validation Loss: 0.7640241384506226\n",
      "Epoch 9021: Training Loss: 0.12024196982383728 Validation Loss: 0.7644701600074768\n",
      "Epoch 9022: Training Loss: 0.11974824965000153 Validation Loss: 0.7637117505073547\n",
      "Epoch 9023: Training Loss: 0.11992131918668747 Validation Loss: 0.7642043232917786\n",
      "Epoch 9024: Training Loss: 0.11997262388467789 Validation Loss: 0.7640250325202942\n",
      "Epoch 9025: Training Loss: 0.11949353416760762 Validation Loss: 0.7635429501533508\n",
      "Epoch 9026: Training Loss: 0.11972075700759888 Validation Loss: 0.7636851668357849\n",
      "Epoch 9027: Training Loss: 0.12110207974910736 Validation Loss: 0.7639028429985046\n",
      "Epoch 9028: Training Loss: 0.11982305347919464 Validation Loss: 0.763340950012207\n",
      "Epoch 9029: Training Loss: 0.11967810988426208 Validation Loss: 0.7637804746627808\n",
      "Epoch 9030: Training Loss: 0.11973907798528671 Validation Loss: 0.764013946056366\n",
      "Epoch 9031: Training Loss: 0.1196553831299146 Validation Loss: 0.7642982602119446\n",
      "Epoch 9032: Training Loss: 0.11990349491437276 Validation Loss: 0.7644795775413513\n",
      "Epoch 9033: Training Loss: 0.1197027713060379 Validation Loss: 0.7645242810249329\n",
      "Epoch 9034: Training Loss: 0.11946277568737666 Validation Loss: 0.7644497752189636\n",
      "Epoch 9035: Training Loss: 0.12002803385257721 Validation Loss: 0.7647683620452881\n",
      "Epoch 9036: Training Loss: 0.1192748174071312 Validation Loss: 0.7646752595901489\n",
      "Epoch 9037: Training Loss: 0.11936038484176 Validation Loss: 0.7642828822135925\n",
      "Epoch 9038: Training Loss: 0.11948949595292409 Validation Loss: 0.7638464570045471\n",
      "Epoch 9039: Training Loss: 0.11960926403601964 Validation Loss: 0.7644004821777344\n",
      "Epoch 9040: Training Loss: 0.1195130671064059 Validation Loss: 0.764618456363678\n",
      "Epoch 9041: Training Loss: 0.11972419669230779 Validation Loss: 0.7644690871238708\n",
      "Epoch 9042: Training Loss: 0.11978884041309357 Validation Loss: 0.7642455101013184\n",
      "Epoch 9043: Training Loss: 0.11968295276165009 Validation Loss: 0.7647655010223389\n",
      "Epoch 9044: Training Loss: 0.1195550188422203 Validation Loss: 0.7646247744560242\n",
      "Epoch 9045: Training Loss: 0.11938771853844325 Validation Loss: 0.7639960646629333\n",
      "Epoch 9046: Training Loss: 0.11966156959533691 Validation Loss: 0.764733612537384\n",
      "Epoch 9047: Training Loss: 0.12057887017726898 Validation Loss: 0.7649990320205688\n",
      "Epoch 9048: Training Loss: 0.11981068551540375 Validation Loss: 0.7649925351142883\n",
      "Epoch 9049: Training Loss: 0.1201016828417778 Validation Loss: 0.7634130120277405\n",
      "Epoch 9050: Training Loss: 0.11943364143371582 Validation Loss: 0.7642750144004822\n",
      "Epoch 9051: Training Loss: 0.11926468710104625 Validation Loss: 0.7642325758934021\n",
      "Epoch 9052: Training Loss: 0.11919517070055008 Validation Loss: 0.7641171216964722\n",
      "Epoch 9053: Training Loss: 0.11942608902851741 Validation Loss: 0.7641960978507996\n",
      "Epoch 9054: Training Loss: 0.11970006922880809 Validation Loss: 0.7640860676765442\n",
      "Epoch 9055: Training Loss: 0.11957382659117381 Validation Loss: 0.7642354965209961\n",
      "Epoch 9056: Training Loss: 0.11976593484481175 Validation Loss: 0.7640303373336792\n",
      "Epoch 9057: Training Loss: 0.11943929145733516 Validation Loss: 0.7647548317909241\n",
      "Epoch 9058: Training Loss: 0.11943451315164566 Validation Loss: 0.7646335363388062\n",
      "Epoch 9059: Training Loss: 0.1188822736342748 Validation Loss: 0.765120804309845\n",
      "Epoch 9060: Training Loss: 0.12036653359731038 Validation Loss: 0.7648221254348755\n",
      "Epoch 9061: Training Loss: 0.11955015858014424 Validation Loss: 0.7649232745170593\n",
      "Epoch 9062: Training Loss: 0.11912221709887187 Validation Loss: 0.7653513550758362\n",
      "Epoch 9063: Training Loss: 0.1192349245150884 Validation Loss: 0.7647680044174194\n",
      "Epoch 9064: Training Loss: 0.11954047779242198 Validation Loss: 0.7645947933197021\n",
      "Epoch 9065: Training Loss: 0.11898461232582729 Validation Loss: 0.7644275426864624\n",
      "Epoch 9066: Training Loss: 0.11956065644820531 Validation Loss: 0.7645535469055176\n",
      "Epoch 9067: Training Loss: 0.11958085497220357 Validation Loss: 0.7647717595100403\n",
      "Epoch 9068: Training Loss: 0.11962465693553288 Validation Loss: 0.7641620635986328\n",
      "Epoch 9069: Training Loss: 0.11951514333486557 Validation Loss: 0.764829695224762\n",
      "Epoch 9070: Training Loss: 0.11911997199058533 Validation Loss: 0.7652040719985962\n",
      "Epoch 9071: Training Loss: 0.1196049377322197 Validation Loss: 0.7654022574424744\n",
      "Epoch 9072: Training Loss: 0.1194753348827362 Validation Loss: 0.7649690508842468\n",
      "Epoch 9073: Training Loss: 0.11894695957501729 Validation Loss: 0.7649768590927124\n",
      "Epoch 9074: Training Loss: 0.1193217933177948 Validation Loss: 0.7653202414512634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9075: Training Loss: 0.11932532986005147 Validation Loss: 0.7656985521316528\n",
      "Epoch 9076: Training Loss: 0.11897916346788406 Validation Loss: 0.7648856043815613\n",
      "Epoch 9077: Training Loss: 0.11905836562315623 Validation Loss: 0.7641609311103821\n",
      "Epoch 9078: Training Loss: 0.11906315634648006 Validation Loss: 0.7639978528022766\n",
      "Epoch 9079: Training Loss: 0.11939217895269394 Validation Loss: 0.7638537287712097\n",
      "Epoch 9080: Training Loss: 0.12029250959555308 Validation Loss: 0.7638286352157593\n",
      "Epoch 9081: Training Loss: 0.11986961215734482 Validation Loss: 0.7647687792778015\n",
      "Epoch 9082: Training Loss: 0.11908032993475597 Validation Loss: 0.7653570771217346\n",
      "Epoch 9083: Training Loss: 0.11920290440320969 Validation Loss: 0.7656704187393188\n",
      "Epoch 9084: Training Loss: 0.11939713607231776 Validation Loss: 0.7659823298454285\n",
      "Epoch 9085: Training Loss: 0.11974867681662242 Validation Loss: 0.7645152807235718\n",
      "Epoch 9086: Training Loss: 0.1189424991607666 Validation Loss: 0.7641170024871826\n",
      "Epoch 9087: Training Loss: 0.11931236336628596 Validation Loss: 0.7636703848838806\n",
      "Epoch 9088: Training Loss: 0.1193007876475652 Validation Loss: 0.7645341753959656\n",
      "Epoch 9089: Training Loss: 0.11923410246769588 Validation Loss: 0.7650262713432312\n",
      "Epoch 9090: Training Loss: 0.11899225910504659 Validation Loss: 0.7644346952438354\n",
      "Epoch 9091: Training Loss: 0.1194537952542305 Validation Loss: 0.7643918395042419\n",
      "Epoch 9092: Training Loss: 0.11907580494880676 Validation Loss: 0.7639409303665161\n",
      "Epoch 9093: Training Loss: 0.11919296781222026 Validation Loss: 0.7638003826141357\n",
      "Epoch 9094: Training Loss: 0.11902938783168793 Validation Loss: 0.7644332051277161\n",
      "Epoch 9095: Training Loss: 0.11912195881207784 Validation Loss: 0.7652093768119812\n",
      "Epoch 9096: Training Loss: 0.11922197540601094 Validation Loss: 0.7656828165054321\n",
      "Epoch 9097: Training Loss: 0.11902304490407307 Validation Loss: 0.7653305530548096\n",
      "Epoch 9098: Training Loss: 0.1189184660712878 Validation Loss: 0.7647401690483093\n",
      "Epoch 9099: Training Loss: 0.11910607169071834 Validation Loss: 0.7639904618263245\n",
      "Epoch 9100: Training Loss: 0.1193171814084053 Validation Loss: 0.7640845775604248\n",
      "Epoch 9101: Training Loss: 0.11891055107116699 Validation Loss: 0.7645562887191772\n",
      "Epoch 9102: Training Loss: 0.11884605884552002 Validation Loss: 0.7651470899581909\n",
      "Epoch 9103: Training Loss: 0.11897370715936025 Validation Loss: 0.7652552723884583\n",
      "Epoch 9104: Training Loss: 0.11907150596380234 Validation Loss: 0.7649117708206177\n",
      "Epoch 9105: Training Loss: 0.11940025289853413 Validation Loss: 0.7642037868499756\n",
      "Epoch 9106: Training Loss: 0.11880394319693248 Validation Loss: 0.764022707939148\n",
      "Epoch 9107: Training Loss: 0.11899333447217941 Validation Loss: 0.7649789452552795\n",
      "Epoch 9108: Training Loss: 0.11903062214454015 Validation Loss: 0.7656813859939575\n",
      "Epoch 9109: Training Loss: 0.11843854933977127 Validation Loss: 0.7660850882530212\n",
      "Epoch 9110: Training Loss: 0.11888847996791203 Validation Loss: 0.7658287286758423\n",
      "Epoch 9111: Training Loss: 0.11867337922255199 Validation Loss: 0.7651514410972595\n",
      "Epoch 9112: Training Loss: 0.11888604114452998 Validation Loss: 0.7645688056945801\n",
      "Epoch 9113: Training Loss: 0.11887018630901973 Validation Loss: 0.7646511793136597\n",
      "Epoch 9114: Training Loss: 0.11862511932849884 Validation Loss: 0.7647573947906494\n",
      "Epoch 9115: Training Loss: 0.11870076507329941 Validation Loss: 0.7647861242294312\n",
      "Epoch 9116: Training Loss: 0.11900078008572261 Validation Loss: 0.7652857303619385\n",
      "Epoch 9117: Training Loss: 0.11893682181835175 Validation Loss: 0.7660427689552307\n",
      "Epoch 9118: Training Loss: 0.11902459214131038 Validation Loss: 0.7651837468147278\n",
      "Epoch 9119: Training Loss: 0.11860607067743938 Validation Loss: 0.7642917037010193\n",
      "Epoch 9120: Training Loss: 0.11907952775557835 Validation Loss: 0.7637096643447876\n",
      "Epoch 9121: Training Loss: 0.11864267041285832 Validation Loss: 0.7652712464332581\n",
      "Epoch 9122: Training Loss: 0.11959012101093928 Validation Loss: 0.7662081122398376\n",
      "Epoch 9123: Training Loss: 0.12083529432614644 Validation Loss: 0.7654030323028564\n",
      "Epoch 9124: Training Loss: 0.11864453554153442 Validation Loss: 0.7654035687446594\n",
      "Epoch 9125: Training Loss: 0.11867225666840871 Validation Loss: 0.764933705329895\n",
      "Epoch 9126: Training Loss: 0.11879263818264008 Validation Loss: 0.7646331787109375\n",
      "Epoch 9127: Training Loss: 0.11887229233980179 Validation Loss: 0.7646719813346863\n",
      "Epoch 9128: Training Loss: 0.11886610090732574 Validation Loss: 0.7646868824958801\n",
      "Epoch 9129: Training Loss: 0.11868688960870107 Validation Loss: 0.7650328874588013\n",
      "Epoch 9130: Training Loss: 0.1185533528526624 Validation Loss: 0.7658188939094543\n",
      "Epoch 9131: Training Loss: 0.11871656527121861 Validation Loss: 0.7659167647361755\n",
      "Epoch 9132: Training Loss: 0.11849380284547806 Validation Loss: 0.7657202482223511\n",
      "Epoch 9133: Training Loss: 0.11877920478582382 Validation Loss: 0.7653697729110718\n",
      "Epoch 9134: Training Loss: 0.11914466321468353 Validation Loss: 0.7642034888267517\n",
      "Epoch 9135: Training Loss: 0.11876087884108226 Validation Loss: 0.7638828158378601\n",
      "Epoch 9136: Training Loss: 0.11862740168968837 Validation Loss: 0.7636972665786743\n",
      "Epoch 9137: Training Loss: 0.11866765469312668 Validation Loss: 0.7650038003921509\n",
      "Epoch 9138: Training Loss: 0.11860239257415135 Validation Loss: 0.7657384276390076\n",
      "Epoch 9139: Training Loss: 0.11898322900136311 Validation Loss: 0.7668378949165344\n",
      "Epoch 9140: Training Loss: 0.11881672342618306 Validation Loss: 0.7661156058311462\n",
      "Epoch 9141: Training Loss: 0.11860839525858562 Validation Loss: 0.7658233046531677\n",
      "Epoch 9142: Training Loss: 0.11930149793624878 Validation Loss: 0.7648690938949585\n",
      "Epoch 9143: Training Loss: 0.11894905318816502 Validation Loss: 0.7647435665130615\n",
      "Epoch 9144: Training Loss: 0.11881029854218166 Validation Loss: 0.7652972340583801\n",
      "Epoch 9145: Training Loss: 0.11862481385469437 Validation Loss: 0.7652831077575684\n",
      "Epoch 9146: Training Loss: 0.1182284876704216 Validation Loss: 0.7657352089881897\n",
      "Epoch 9147: Training Loss: 0.11890359222888947 Validation Loss: 0.7654212713241577\n",
      "Epoch 9148: Training Loss: 0.11868333319822948 Validation Loss: 0.7655885219573975\n",
      "Epoch 9149: Training Loss: 0.11879673600196838 Validation Loss: 0.765836238861084\n",
      "Epoch 9150: Training Loss: 0.11951043953498204 Validation Loss: 0.7655414938926697\n",
      "Epoch 9151: Training Loss: 0.11861926565567653 Validation Loss: 0.7666265368461609\n",
      "Epoch 9152: Training Loss: 0.11892553170522054 Validation Loss: 0.7669945955276489\n",
      "Epoch 9153: Training Loss: 0.1184809406598409 Validation Loss: 0.7664942741394043\n",
      "Epoch 9154: Training Loss: 0.11890803277492523 Validation Loss: 0.7659339904785156\n",
      "Epoch 9155: Training Loss: 0.11888338128725688 Validation Loss: 0.765205979347229\n",
      "Epoch 9156: Training Loss: 0.11815248678127925 Validation Loss: 0.7648506164550781\n",
      "Epoch 9157: Training Loss: 0.11819832026958466 Validation Loss: 0.7656058073043823\n",
      "Epoch 9158: Training Loss: 0.11840065817038219 Validation Loss: 0.7658084630966187\n",
      "Epoch 9159: Training Loss: 0.11876918127139409 Validation Loss: 0.7655434608459473\n",
      "Epoch 9160: Training Loss: 0.11879122257232666 Validation Loss: 0.7664838433265686\n",
      "Epoch 9161: Training Loss: 0.11842372765143712 Validation Loss: 0.7660207748413086\n",
      "Epoch 9162: Training Loss: 0.11833256234725316 Validation Loss: 0.7664967775344849\n",
      "Epoch 9163: Training Loss: 0.11847260097662608 Validation Loss: 0.7659426331520081\n",
      "Epoch 9164: Training Loss: 0.11845593402783076 Validation Loss: 0.765712320804596\n",
      "Epoch 9165: Training Loss: 0.11829259991645813 Validation Loss: 0.7660955786705017\n",
      "Epoch 9166: Training Loss: 0.1186117356022199 Validation Loss: 0.7653069496154785\n",
      "Epoch 9167: Training Loss: 0.11947498718897502 Validation Loss: 0.7658507823944092\n",
      "Epoch 9168: Training Loss: 0.11842256287733714 Validation Loss: 0.7662187218666077\n",
      "Epoch 9169: Training Loss: 0.11837177723646164 Validation Loss: 0.7663279175758362\n",
      "Epoch 9170: Training Loss: 0.11901822189490001 Validation Loss: 0.7659195065498352\n",
      "Epoch 9171: Training Loss: 0.11826745420694351 Validation Loss: 0.765346884727478\n",
      "Epoch 9172: Training Loss: 0.11887397865454356 Validation Loss: 0.7654455304145813\n",
      "Epoch 9173: Training Loss: 0.1182945395509402 Validation Loss: 0.7657790184020996\n",
      "Epoch 9174: Training Loss: 0.11837174991766612 Validation Loss: 0.7669071555137634\n",
      "Epoch 9175: Training Loss: 0.11839813739061356 Validation Loss: 0.7666061520576477\n",
      "Epoch 9176: Training Loss: 0.11825376003980637 Validation Loss: 0.7656326293945312\n",
      "Epoch 9177: Training Loss: 0.11813938369353612 Validation Loss: 0.7649329304695129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9178: Training Loss: 0.11832596113284428 Validation Loss: 0.7648513913154602\n",
      "Epoch 9179: Training Loss: 0.11824512978394826 Validation Loss: 0.7648960947990417\n",
      "Epoch 9180: Training Loss: 0.11834556112686793 Validation Loss: 0.7655413746833801\n",
      "Epoch 9181: Training Loss: 0.1186397522687912 Validation Loss: 0.7661353349685669\n",
      "Epoch 9182: Training Loss: 0.11820406715075175 Validation Loss: 0.7660951018333435\n",
      "Epoch 9183: Training Loss: 0.11866840223471324 Validation Loss: 0.7665846347808838\n",
      "Epoch 9184: Training Loss: 0.11823341995477676 Validation Loss: 0.7657650709152222\n",
      "Epoch 9185: Training Loss: 0.11816674222548802 Validation Loss: 0.7661093473434448\n",
      "Epoch 9186: Training Loss: 0.11861214290062587 Validation Loss: 0.7653879523277283\n",
      "Epoch 9187: Training Loss: 0.11839281767606735 Validation Loss: 0.76544189453125\n",
      "Epoch 9188: Training Loss: 0.11805014063914616 Validation Loss: 0.7665268182754517\n",
      "Epoch 9189: Training Loss: 0.11855913947025935 Validation Loss: 0.7668709754943848\n",
      "Epoch 9190: Training Loss: 0.1181749055782954 Validation Loss: 0.7667871713638306\n",
      "Epoch 9191: Training Loss: 0.11874101807673772 Validation Loss: 0.766492486000061\n",
      "Epoch 9192: Training Loss: 0.1185766061147054 Validation Loss: 0.7665056586265564\n",
      "Epoch 9193: Training Loss: 0.11822264144817989 Validation Loss: 0.7663522958755493\n",
      "Epoch 9194: Training Loss: 0.11779828617970149 Validation Loss: 0.7659015655517578\n",
      "Epoch 9195: Training Loss: 0.1182151660323143 Validation Loss: 0.7662657499313354\n",
      "Epoch 9196: Training Loss: 0.11799389372269313 Validation Loss: 0.7663417458534241\n",
      "Epoch 9197: Training Loss: 0.11802599827448527 Validation Loss: 0.765686571598053\n",
      "Epoch 9198: Training Loss: 0.11816774805386861 Validation Loss: 0.7659752368927002\n",
      "Epoch 9199: Training Loss: 0.11825619389613469 Validation Loss: 0.7652955055236816\n",
      "Epoch 9200: Training Loss: 0.11787517368793488 Validation Loss: 0.7656891345977783\n",
      "Epoch 9201: Training Loss: 0.11837200323740642 Validation Loss: 0.7659493088722229\n",
      "Epoch 9202: Training Loss: 0.11796067903439204 Validation Loss: 0.7670934200286865\n",
      "Epoch 9203: Training Loss: 0.11788637439409892 Validation Loss: 0.7670375108718872\n",
      "Epoch 9204: Training Loss: 0.11756419638792674 Validation Loss: 0.7668883204460144\n",
      "Epoch 9205: Training Loss: 0.11896132677793503 Validation Loss: 0.7663905620574951\n",
      "Epoch 9206: Training Loss: 0.11810156454642613 Validation Loss: 0.765493631362915\n",
      "Epoch 9207: Training Loss: 0.11818074434995651 Validation Loss: 0.7654343843460083\n",
      "Epoch 9208: Training Loss: 0.1176408976316452 Validation Loss: 0.766150176525116\n",
      "Epoch 9209: Training Loss: 0.11772944529851277 Validation Loss: 0.766498327255249\n",
      "Epoch 9210: Training Loss: 0.11758338660001755 Validation Loss: 0.766213059425354\n",
      "Epoch 9211: Training Loss: 0.11835771302382152 Validation Loss: 0.765356183052063\n",
      "Epoch 9212: Training Loss: 0.11759779850641887 Validation Loss: 0.7656133770942688\n",
      "Epoch 9213: Training Loss: 0.11833293984333675 Validation Loss: 0.7658224701881409\n",
      "Epoch 9214: Training Loss: 0.11817900836467743 Validation Loss: 0.7657612562179565\n",
      "Epoch 9215: Training Loss: 0.11801143735647202 Validation Loss: 0.7659750580787659\n",
      "Epoch 9216: Training Loss: 0.11801969756682713 Validation Loss: 0.7666118741035461\n",
      "Epoch 9217: Training Loss: 0.11802371094624202 Validation Loss: 0.7661598920822144\n",
      "Epoch 9218: Training Loss: 0.11770915985107422 Validation Loss: 0.7664010524749756\n",
      "Epoch 9219: Training Loss: 0.11827106028795242 Validation Loss: 0.7664098143577576\n",
      "Epoch 9220: Training Loss: 0.11779249459505081 Validation Loss: 0.7662076950073242\n",
      "Epoch 9221: Training Loss: 0.11763677249352138 Validation Loss: 0.766315221786499\n",
      "Epoch 9222: Training Loss: 0.11798438678185146 Validation Loss: 0.7664691209793091\n",
      "Epoch 9223: Training Loss: 0.11793885628382365 Validation Loss: 0.7660868167877197\n",
      "Epoch 9224: Training Loss: 0.11791724463303883 Validation Loss: 0.7664797306060791\n",
      "Epoch 9225: Training Loss: 0.11774731427431107 Validation Loss: 0.7668701410293579\n",
      "Epoch 9226: Training Loss: 0.11788701017697652 Validation Loss: 0.7668251991271973\n",
      "Epoch 9227: Training Loss: 0.11773114403088887 Validation Loss: 0.7663692235946655\n",
      "Epoch 9228: Training Loss: 0.11780327061812083 Validation Loss: 0.7661148905754089\n",
      "Epoch 9229: Training Loss: 0.11809155593315761 Validation Loss: 0.7673956751823425\n",
      "Epoch 9230: Training Loss: 0.11763878911733627 Validation Loss: 0.7667431831359863\n",
      "Epoch 9231: Training Loss: 0.11784754941860835 Validation Loss: 0.767501950263977\n",
      "Epoch 9232: Training Loss: 0.11762018501758575 Validation Loss: 0.7671597599983215\n",
      "Epoch 9233: Training Loss: 0.11801772316296895 Validation Loss: 0.7660200595855713\n",
      "Epoch 9234: Training Loss: 0.11754704266786575 Validation Loss: 0.7659364342689514\n",
      "Epoch 9235: Training Loss: 0.11787186314662297 Validation Loss: 0.7656127214431763\n",
      "Epoch 9236: Training Loss: 0.11764666686455409 Validation Loss: 0.7667165994644165\n",
      "Epoch 9237: Training Loss: 0.11767896761496861 Validation Loss: 0.7664045095443726\n",
      "Epoch 9238: Training Loss: 0.11766301343838374 Validation Loss: 0.7667748332023621\n",
      "Epoch 9239: Training Loss: 0.11844468613465627 Validation Loss: 0.7666376829147339\n",
      "Epoch 9240: Training Loss: 0.11764845997095108 Validation Loss: 0.7670838832855225\n",
      "Epoch 9241: Training Loss: 0.1175959159930547 Validation Loss: 0.7665475010871887\n",
      "Epoch 9242: Training Loss: 0.11800787101189296 Validation Loss: 0.767228364944458\n",
      "Epoch 9243: Training Loss: 0.11779787143071492 Validation Loss: 0.766979992389679\n",
      "Epoch 9244: Training Loss: 0.11771270136038463 Validation Loss: 0.7669075131416321\n",
      "Epoch 9245: Training Loss: 0.1178676684697469 Validation Loss: 0.7673255205154419\n",
      "Epoch 9246: Training Loss: 0.11785133679707845 Validation Loss: 0.7665717601776123\n",
      "Epoch 9247: Training Loss: 0.1182048146923383 Validation Loss: 0.7659223675727844\n",
      "Epoch 9248: Training Loss: 0.11753023167451222 Validation Loss: 0.7668315172195435\n",
      "Epoch 9249: Training Loss: 0.1177274410923322 Validation Loss: 0.7670387029647827\n",
      "Epoch 9250: Training Loss: 0.11778738349676132 Validation Loss: 0.7671086192131042\n",
      "Epoch 9251: Training Loss: 0.11730898420015971 Validation Loss: 0.7666121125221252\n",
      "Epoch 9252: Training Loss: 0.11760155856609344 Validation Loss: 0.7662293910980225\n",
      "Epoch 9253: Training Loss: 0.11760420352220535 Validation Loss: 0.7667350172996521\n",
      "Epoch 9254: Training Loss: 0.1176745096842448 Validation Loss: 0.767134428024292\n",
      "Epoch 9255: Training Loss: 0.11767405271530151 Validation Loss: 0.7670630216598511\n",
      "Epoch 9256: Training Loss: 0.11744019389152527 Validation Loss: 0.7671928405761719\n",
      "Epoch 9257: Training Loss: 0.11759126683076222 Validation Loss: 0.7669132947921753\n",
      "Epoch 9258: Training Loss: 0.11781303336222966 Validation Loss: 0.7665790319442749\n",
      "Epoch 9259: Training Loss: 0.11742807924747467 Validation Loss: 0.7666770219802856\n",
      "Epoch 9260: Training Loss: 0.11757298310597737 Validation Loss: 0.7662786841392517\n",
      "Epoch 9261: Training Loss: 0.11787702143192291 Validation Loss: 0.7668291330337524\n",
      "Epoch 9262: Training Loss: 0.11804629117250443 Validation Loss: 0.7665442228317261\n",
      "Epoch 9263: Training Loss: 0.11767053852478664 Validation Loss: 0.7661746740341187\n",
      "Epoch 9264: Training Loss: 0.11775652319192886 Validation Loss: 0.765842854976654\n",
      "Epoch 9265: Training Loss: 0.11740687241156895 Validation Loss: 0.7669808864593506\n",
      "Epoch 9266: Training Loss: 0.11753986030817032 Validation Loss: 0.7673349380493164\n",
      "Epoch 9267: Training Loss: 0.11799273391564687 Validation Loss: 0.7673344016075134\n",
      "Epoch 9268: Training Loss: 0.11724874128897984 Validation Loss: 0.7671816945075989\n",
      "Epoch 9269: Training Loss: 0.11733213067054749 Validation Loss: 0.7670320272445679\n",
      "Epoch 9270: Training Loss: 0.11773415406545003 Validation Loss: 0.7669952511787415\n",
      "Epoch 9271: Training Loss: 0.1177016148964564 Validation Loss: 0.7667789459228516\n",
      "Epoch 9272: Training Loss: 0.11751215656598409 Validation Loss: 0.7664335370063782\n",
      "Epoch 9273: Training Loss: 0.11723657945791881 Validation Loss: 0.7668704390525818\n",
      "Epoch 9274: Training Loss: 0.117502065996329 Validation Loss: 0.7673113942146301\n",
      "Epoch 9275: Training Loss: 0.1174784004688263 Validation Loss: 0.7677372097969055\n",
      "Epoch 9276: Training Loss: 0.11732847740252812 Validation Loss: 0.7677059173583984\n",
      "Epoch 9277: Training Loss: 0.1172379453976949 Validation Loss: 0.767660915851593\n",
      "Epoch 9278: Training Loss: 0.11741821219523747 Validation Loss: 0.7669681906700134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9279: Training Loss: 0.11765387902657191 Validation Loss: 0.7660099267959595\n",
      "Epoch 9280: Training Loss: 0.11758826673030853 Validation Loss: 0.7667838335037231\n",
      "Epoch 9281: Training Loss: 0.11694598197937012 Validation Loss: 0.767559826374054\n",
      "Epoch 9282: Training Loss: 0.117705004910628 Validation Loss: 0.7676306366920471\n",
      "Epoch 9283: Training Loss: 0.11741809795300166 Validation Loss: 0.7666967511177063\n",
      "Epoch 9284: Training Loss: 0.11761176586151123 Validation Loss: 0.7664427757263184\n",
      "Epoch 9285: Training Loss: 0.11734992762406667 Validation Loss: 0.7671955823898315\n",
      "Epoch 9286: Training Loss: 0.11727594832579295 Validation Loss: 0.7675106525421143\n",
      "Epoch 9287: Training Loss: 0.11748693386713664 Validation Loss: 0.7680457234382629\n",
      "Epoch 9288: Training Loss: 0.11697644243637721 Validation Loss: 0.7678892612457275\n",
      "Epoch 9289: Training Loss: 0.11712578435738881 Validation Loss: 0.7678903341293335\n",
      "Epoch 9290: Training Loss: 0.11740053693453471 Validation Loss: 0.7668015360832214\n",
      "Epoch 9291: Training Loss: 0.11708652724822362 Validation Loss: 0.7667514681816101\n",
      "Epoch 9292: Training Loss: 0.11730353037516277 Validation Loss: 0.7676632404327393\n",
      "Epoch 9293: Training Loss: 0.1174610157807668 Validation Loss: 0.7679614424705505\n",
      "Epoch 9294: Training Loss: 0.11734277258316676 Validation Loss: 0.7673472166061401\n",
      "Epoch 9295: Training Loss: 0.11723761260509491 Validation Loss: 0.7675004005432129\n",
      "Epoch 9296: Training Loss: 0.11760228623946507 Validation Loss: 0.7669558525085449\n",
      "Epoch 9297: Training Loss: 0.11735512067874272 Validation Loss: 0.7673430442810059\n",
      "Epoch 9298: Training Loss: 0.11719916015863419 Validation Loss: 0.7683534622192383\n",
      "Epoch 9299: Training Loss: 0.11720451712608337 Validation Loss: 0.7680540680885315\n",
      "Epoch 9300: Training Loss: 0.11711593965689342 Validation Loss: 0.7678617835044861\n",
      "Epoch 9301: Training Loss: 0.11698580533266068 Validation Loss: 0.7676271200180054\n",
      "Epoch 9302: Training Loss: 0.11722264687220256 Validation Loss: 0.7673735022544861\n",
      "Epoch 9303: Training Loss: 0.11701848109563191 Validation Loss: 0.7666820883750916\n",
      "Epoch 9304: Training Loss: 0.11678015689055125 Validation Loss: 0.7666674256324768\n",
      "Epoch 9305: Training Loss: 0.11721666157245636 Validation Loss: 0.7672334313392639\n",
      "Epoch 9306: Training Loss: 0.11701969554026921 Validation Loss: 0.7671687006950378\n",
      "Epoch 9307: Training Loss: 0.1172336811820666 Validation Loss: 0.7675291299819946\n",
      "Epoch 9308: Training Loss: 0.11709989110628764 Validation Loss: 0.768013060092926\n",
      "Epoch 9309: Training Loss: 0.11701005697250366 Validation Loss: 0.7673779129981995\n",
      "Epoch 9310: Training Loss: 0.11688382178544998 Validation Loss: 0.7677037715911865\n",
      "Epoch 9311: Training Loss: 0.11705184231201808 Validation Loss: 0.7671857476234436\n",
      "Epoch 9312: Training Loss: 0.11675085872411728 Validation Loss: 0.7670151591300964\n",
      "Epoch 9313: Training Loss: 0.11786341418822606 Validation Loss: 0.7675300240516663\n",
      "Epoch 9314: Training Loss: 0.11735222488641739 Validation Loss: 0.767027735710144\n",
      "Epoch 9315: Training Loss: 0.11679231127103169 Validation Loss: 0.7672186493873596\n",
      "Epoch 9316: Training Loss: 0.11721982558568318 Validation Loss: 0.7679075598716736\n",
      "Epoch 9317: Training Loss: 0.11734010030825932 Validation Loss: 0.7681949734687805\n",
      "Epoch 9318: Training Loss: 0.1171930159131686 Validation Loss: 0.7681646943092346\n",
      "Epoch 9319: Training Loss: 0.1169963429371516 Validation Loss: 0.7679846882820129\n",
      "Epoch 9320: Training Loss: 0.11683530608812968 Validation Loss: 0.7678358554840088\n",
      "Epoch 9321: Training Loss: 0.11686523258686066 Validation Loss: 0.7683177590370178\n",
      "Epoch 9322: Training Loss: 0.11732461800177892 Validation Loss: 0.7684277892112732\n",
      "Epoch 9323: Training Loss: 0.11679306377967198 Validation Loss: 0.7682309746742249\n",
      "Epoch 9324: Training Loss: 0.11700053761402766 Validation Loss: 0.7686795592308044\n",
      "Epoch 9325: Training Loss: 0.11837514489889145 Validation Loss: 0.768619954586029\n",
      "Epoch 9326: Training Loss: 0.11676067610581715 Validation Loss: 0.7679357528686523\n",
      "Epoch 9327: Training Loss: 0.11675914376974106 Validation Loss: 0.7680583596229553\n",
      "Epoch 9328: Training Loss: 0.11708695441484451 Validation Loss: 0.7675622701644897\n",
      "Epoch 9329: Training Loss: 0.11667824039856593 Validation Loss: 0.7683464288711548\n",
      "Epoch 9330: Training Loss: 0.11707418908675511 Validation Loss: 0.7676588892936707\n",
      "Epoch 9331: Training Loss: 0.11688412974278133 Validation Loss: 0.7681097984313965\n",
      "Epoch 9332: Training Loss: 0.11679963022470474 Validation Loss: 0.7679397463798523\n",
      "Epoch 9333: Training Loss: 0.11690106987953186 Validation Loss: 0.7679204344749451\n",
      "Epoch 9334: Training Loss: 0.11788285026947658 Validation Loss: 0.7672321200370789\n",
      "Epoch 9335: Training Loss: 0.11690863470236461 Validation Loss: 0.7676040530204773\n",
      "Epoch 9336: Training Loss: 0.11676235248645146 Validation Loss: 0.7679716944694519\n",
      "Epoch 9337: Training Loss: 0.11697178333997726 Validation Loss: 0.7680873274803162\n",
      "Epoch 9338: Training Loss: 0.11687375108400981 Validation Loss: 0.7687236070632935\n",
      "Epoch 9339: Training Loss: 0.11693187057971954 Validation Loss: 0.7682580947875977\n",
      "Epoch 9340: Training Loss: 0.1165636678536733 Validation Loss: 0.7682510018348694\n",
      "Epoch 9341: Training Loss: 0.11690727869669597 Validation Loss: 0.7682424783706665\n",
      "Epoch 9342: Training Loss: 0.11665092160304387 Validation Loss: 0.7676448822021484\n",
      "Epoch 9343: Training Loss: 0.11689421782890956 Validation Loss: 0.7675053477287292\n",
      "Epoch 9344: Training Loss: 0.1168259506424268 Validation Loss: 0.767167329788208\n",
      "Epoch 9345: Training Loss: 0.11676433930794398 Validation Loss: 0.7675582766532898\n",
      "Epoch 9346: Training Loss: 0.11715697993834813 Validation Loss: 0.7673527598381042\n",
      "Epoch 9347: Training Loss: 0.11674906561772029 Validation Loss: 0.7677556872367859\n",
      "Epoch 9348: Training Loss: 0.11670551697413127 Validation Loss: 0.7682942748069763\n",
      "Epoch 9349: Training Loss: 0.11649066706498463 Validation Loss: 0.7683572769165039\n",
      "Epoch 9350: Training Loss: 0.1168879543741544 Validation Loss: 0.7679656744003296\n",
      "Epoch 9351: Training Loss: 0.11646585663159688 Validation Loss: 0.7677612900733948\n",
      "Epoch 9352: Training Loss: 0.11692110200723012 Validation Loss: 0.7673569917678833\n",
      "Epoch 9353: Training Loss: 0.11669919143120448 Validation Loss: 0.7672675848007202\n",
      "Epoch 9354: Training Loss: 0.11667713522911072 Validation Loss: 0.767913281917572\n",
      "Epoch 9355: Training Loss: 0.11664706965287526 Validation Loss: 0.7680705189704895\n",
      "Epoch 9356: Training Loss: 0.11657542238632838 Validation Loss: 0.7678108811378479\n",
      "Epoch 9357: Training Loss: 0.11719849457343419 Validation Loss: 0.7684712409973145\n",
      "Epoch 9358: Training Loss: 0.1165941779812177 Validation Loss: 0.7679932713508606\n",
      "Epoch 9359: Training Loss: 0.11709284534056981 Validation Loss: 0.7677285671234131\n",
      "Epoch 9360: Training Loss: 0.11686086903015773 Validation Loss: 0.7675700783729553\n",
      "Epoch 9361: Training Loss: 0.11686840405066808 Validation Loss: 0.7671787142753601\n",
      "Epoch 9362: Training Loss: 0.11675089597702026 Validation Loss: 0.7681485414505005\n",
      "Epoch 9363: Training Loss: 0.11656816552082698 Validation Loss: 0.7685739994049072\n",
      "Epoch 9364: Training Loss: 0.11672552426656087 Validation Loss: 0.7689560651779175\n",
      "Epoch 9365: Training Loss: 0.11713320265213649 Validation Loss: 0.7680850625038147\n",
      "Epoch 9366: Training Loss: 0.1160753717025121 Validation Loss: 0.7682725191116333\n",
      "Epoch 9367: Training Loss: 0.11620163172483444 Validation Loss: 0.7677972912788391\n",
      "Epoch 9368: Training Loss: 0.11688733100891113 Validation Loss: 0.7677575349807739\n",
      "Epoch 9369: Training Loss: 0.11679882804552714 Validation Loss: 0.7681125998497009\n",
      "Epoch 9370: Training Loss: 0.11647301912307739 Validation Loss: 0.7682340741157532\n",
      "Epoch 9371: Training Loss: 0.11624837666749954 Validation Loss: 0.7682052850723267\n",
      "Epoch 9372: Training Loss: 0.11668138702710469 Validation Loss: 0.7693673372268677\n",
      "Epoch 9373: Training Loss: 0.11663803954919179 Validation Loss: 0.7690908312797546\n",
      "Epoch 9374: Training Loss: 0.1168229306737582 Validation Loss: 0.7681812644004822\n",
      "Epoch 9375: Training Loss: 0.11624803642431895 Validation Loss: 0.767744243144989\n",
      "Epoch 9376: Training Loss: 0.11658119906981786 Validation Loss: 0.7677966356277466\n",
      "Epoch 9377: Training Loss: 0.11646114786465962 Validation Loss: 0.7674391865730286\n",
      "Epoch 9378: Training Loss: 0.11637126157681148 Validation Loss: 0.7671635150909424\n",
      "Epoch 9379: Training Loss: 0.11667570471763611 Validation Loss: 0.7680796980857849\n",
      "Epoch 9380: Training Loss: 0.11646961917479833 Validation Loss: 0.7687550783157349\n",
      "Epoch 9381: Training Loss: 0.11660983661810558 Validation Loss: 0.7686915397644043\n",
      "Epoch 9382: Training Loss: 0.11590087165435155 Validation Loss: 0.7680419087409973\n",
      "Epoch 9383: Training Loss: 0.11658671498298645 Validation Loss: 0.7688962817192078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9384: Training Loss: 0.11677328993876775 Validation Loss: 0.7686795592308044\n",
      "Epoch 9385: Training Loss: 0.11629173159599304 Validation Loss: 0.7685990333557129\n",
      "Epoch 9386: Training Loss: 0.11617894719044368 Validation Loss: 0.7686120271682739\n",
      "Epoch 9387: Training Loss: 0.11683239042758942 Validation Loss: 0.7684224247932434\n",
      "Epoch 9388: Training Loss: 0.11649593710899353 Validation Loss: 0.7691344022750854\n",
      "Epoch 9389: Training Loss: 0.11632661024729411 Validation Loss: 0.7696660757064819\n",
      "Epoch 9390: Training Loss: 0.11640424033006032 Validation Loss: 0.7689416408538818\n",
      "Epoch 9391: Training Loss: 0.11626694599787395 Validation Loss: 0.7689709067344666\n",
      "Epoch 9392: Training Loss: 0.11668064445257187 Validation Loss: 0.7690988183021545\n",
      "Epoch 9393: Training Loss: 0.11638805518547694 Validation Loss: 0.7693231105804443\n",
      "Epoch 9394: Training Loss: 0.11613987634579341 Validation Loss: 0.7692726254463196\n",
      "Epoch 9395: Training Loss: 0.11638437211513519 Validation Loss: 0.7683065533638\n",
      "Epoch 9396: Training Loss: 0.11616869767506917 Validation Loss: 0.7680264711380005\n",
      "Epoch 9397: Training Loss: 0.11628868679205577 Validation Loss: 0.7681215405464172\n",
      "Epoch 9398: Training Loss: 0.11615304897228877 Validation Loss: 0.7684106230735779\n",
      "Epoch 9399: Training Loss: 0.1160966008901596 Validation Loss: 0.7680535912513733\n",
      "Epoch 9400: Training Loss: 0.11619851738214493 Validation Loss: 0.7677505612373352\n",
      "Epoch 9401: Training Loss: 0.11619800577561061 Validation Loss: 0.7688932418823242\n",
      "Epoch 9402: Training Loss: 0.11641461898883183 Validation Loss: 0.7688890099525452\n",
      "Epoch 9403: Training Loss: 0.1158737267057101 Validation Loss: 0.7690433263778687\n",
      "Epoch 9404: Training Loss: 0.11651178946097691 Validation Loss: 0.7693215012550354\n",
      "Epoch 9405: Training Loss: 0.1160701389114062 Validation Loss: 0.7690814733505249\n",
      "Epoch 9406: Training Loss: 0.1164746955037117 Validation Loss: 0.7686600685119629\n",
      "Epoch 9407: Training Loss: 0.11599583675463994 Validation Loss: 0.7681419253349304\n",
      "Epoch 9408: Training Loss: 0.1157676378885905 Validation Loss: 0.7684913277626038\n",
      "Epoch 9409: Training Loss: 0.11631689220666885 Validation Loss: 0.7679025530815125\n",
      "Epoch 9410: Training Loss: 0.11588846892118454 Validation Loss: 0.7682844996452332\n",
      "Epoch 9411: Training Loss: 0.11614382515350978 Validation Loss: 0.7689305543899536\n",
      "Epoch 9412: Training Loss: 0.11620928595463435 Validation Loss: 0.7696072459220886\n",
      "Epoch 9413: Training Loss: 0.1162334755063057 Validation Loss: 0.7691481113433838\n",
      "Epoch 9414: Training Loss: 0.11623895913362503 Validation Loss: 0.7678526043891907\n",
      "Epoch 9415: Training Loss: 0.11615717162688573 Validation Loss: 0.7683346271514893\n",
      "Epoch 9416: Training Loss: 0.11608919004599254 Validation Loss: 0.7685123085975647\n",
      "Epoch 9417: Training Loss: 0.11611217260360718 Validation Loss: 0.7675533294677734\n",
      "Epoch 9418: Training Loss: 0.11612121760845184 Validation Loss: 0.768722653388977\n",
      "Epoch 9419: Training Loss: 0.11578986545403798 Validation Loss: 0.7692061066627502\n",
      "Epoch 9420: Training Loss: 0.1160397008061409 Validation Loss: 0.7690004110336304\n",
      "Epoch 9421: Training Loss: 0.11583542327086131 Validation Loss: 0.7693939805030823\n",
      "Epoch 9422: Training Loss: 0.11592315882444382 Validation Loss: 0.7698695659637451\n",
      "Epoch 9423: Training Loss: 0.11604637155930202 Validation Loss: 0.7698655724525452\n",
      "Epoch 9424: Training Loss: 0.11585763345162074 Validation Loss: 0.7691014409065247\n",
      "Epoch 9425: Training Loss: 0.11632486432790756 Validation Loss: 0.7689296007156372\n",
      "Epoch 9426: Training Loss: 0.11600722124179204 Validation Loss: 0.7687427401542664\n",
      "Epoch 9427: Training Loss: 0.11630012840032578 Validation Loss: 0.7699887156486511\n",
      "Epoch 9428: Training Loss: 0.11678698658943176 Validation Loss: 0.768806517124176\n",
      "Epoch 9429: Training Loss: 0.11600910623868306 Validation Loss: 0.7690205574035645\n",
      "Epoch 9430: Training Loss: 0.1159244825442632 Validation Loss: 0.7685449719429016\n",
      "Epoch 9431: Training Loss: 0.11585017293691635 Validation Loss: 0.7684363722801208\n",
      "Epoch 9432: Training Loss: 0.11595624188582103 Validation Loss: 0.7692794799804688\n",
      "Epoch 9433: Training Loss: 0.11607334762811661 Validation Loss: 0.7692702412605286\n",
      "Epoch 9434: Training Loss: 0.11598581324021022 Validation Loss: 0.7686961889266968\n",
      "Epoch 9435: Training Loss: 0.11557340373595555 Validation Loss: 0.7690064907073975\n",
      "Epoch 9436: Training Loss: 0.11585285017887752 Validation Loss: 0.7691256999969482\n",
      "Epoch 9437: Training Loss: 0.11592166125774384 Validation Loss: 0.7693802118301392\n",
      "Epoch 9438: Training Loss: 0.1157939260204633 Validation Loss: 0.7692391872406006\n",
      "Epoch 9439: Training Loss: 0.11625120043754578 Validation Loss: 0.7689033150672913\n",
      "Epoch 9440: Training Loss: 0.11623508234818776 Validation Loss: 0.7691212892532349\n",
      "Epoch 9441: Training Loss: 0.11597796529531479 Validation Loss: 0.7697243690490723\n",
      "Epoch 9442: Training Loss: 0.11637095858653386 Validation Loss: 0.7698202729225159\n",
      "Epoch 9443: Training Loss: 0.115994393825531 Validation Loss: 0.7694123387336731\n",
      "Epoch 9444: Training Loss: 0.11589664717515309 Validation Loss: 0.769087553024292\n",
      "Epoch 9445: Training Loss: 0.11615825444459915 Validation Loss: 0.7700743079185486\n",
      "Epoch 9446: Training Loss: 0.11583004146814346 Validation Loss: 0.7702246308326721\n",
      "Epoch 9447: Training Loss: 0.1157928854227066 Validation Loss: 0.7695005536079407\n",
      "Epoch 9448: Training Loss: 0.11582209914922714 Validation Loss: 0.768913209438324\n",
      "Epoch 9449: Training Loss: 0.11586427440245946 Validation Loss: 0.768744945526123\n",
      "Epoch 9450: Training Loss: 0.11601699392000835 Validation Loss: 0.7693055868148804\n",
      "Epoch 9451: Training Loss: 0.11581429839134216 Validation Loss: 0.7699166536331177\n",
      "Epoch 9452: Training Loss: 0.11568786948919296 Validation Loss: 0.7702957391738892\n",
      "Epoch 9453: Training Loss: 0.1160215934117635 Validation Loss: 0.7699126601219177\n",
      "Epoch 9454: Training Loss: 0.11586454262336095 Validation Loss: 0.7688820362091064\n",
      "Epoch 9455: Training Loss: 0.11574557671944301 Validation Loss: 0.7684689164161682\n",
      "Epoch 9456: Training Loss: 0.11620589345693588 Validation Loss: 0.7690675854682922\n",
      "Epoch 9457: Training Loss: 0.11581773807605107 Validation Loss: 0.7691417932510376\n",
      "Epoch 9458: Training Loss: 0.11541880418856938 Validation Loss: 0.7694830298423767\n",
      "Epoch 9459: Training Loss: 0.11651276797056198 Validation Loss: 0.7701252698898315\n",
      "Epoch 9460: Training Loss: 0.11563839763402939 Validation Loss: 0.770044207572937\n",
      "Epoch 9461: Training Loss: 0.11553903420766194 Validation Loss: 0.7691075205802917\n",
      "Epoch 9462: Training Loss: 0.11557117601235707 Validation Loss: 0.7686756253242493\n",
      "Epoch 9463: Training Loss: 0.11578439176082611 Validation Loss: 0.7684130072593689\n",
      "Epoch 9464: Training Loss: 0.11615407715241115 Validation Loss: 0.7693500518798828\n",
      "Epoch 9465: Training Loss: 0.11607581377029419 Validation Loss: 0.7696405649185181\n",
      "Epoch 9466: Training Loss: 0.11601910988489787 Validation Loss: 0.7709653973579407\n",
      "Epoch 9467: Training Loss: 0.11606559654076894 Validation Loss: 0.7709169387817383\n",
      "Epoch 9468: Training Loss: 0.11570180455843608 Validation Loss: 0.7698387503623962\n",
      "Epoch 9469: Training Loss: 0.11582716554403305 Validation Loss: 0.7693660855293274\n",
      "Epoch 9470: Training Loss: 0.11546243975559871 Validation Loss: 0.7686301469802856\n",
      "Epoch 9471: Training Loss: 0.11557552963495255 Validation Loss: 0.7687192559242249\n",
      "Epoch 9472: Training Loss: 0.1154886136452357 Validation Loss: 0.7697109580039978\n",
      "Epoch 9473: Training Loss: 0.11585015555222829 Validation Loss: 0.7697124481201172\n",
      "Epoch 9474: Training Loss: 0.1153988242149353 Validation Loss: 0.7703986763954163\n",
      "Epoch 9475: Training Loss: 0.11550016701221466 Validation Loss: 0.770131528377533\n",
      "Epoch 9476: Training Loss: 0.11568326751391093 Validation Loss: 0.769875168800354\n",
      "Epoch 9477: Training Loss: 0.11550788581371307 Validation Loss: 0.7690499424934387\n",
      "Epoch 9478: Training Loss: 0.1158441777030627 Validation Loss: 0.7685928344726562\n",
      "Epoch 9479: Training Loss: 0.11568114161491394 Validation Loss: 0.7689369916915894\n",
      "Epoch 9480: Training Loss: 0.11542932937542598 Validation Loss: 0.7695646286010742\n",
      "Epoch 9481: Training Loss: 0.11576337615648906 Validation Loss: 0.7701903581619263\n",
      "Epoch 9482: Training Loss: 0.11547458171844482 Validation Loss: 0.770026683807373\n",
      "Epoch 9483: Training Loss: 0.11601351698239644 Validation Loss: 0.7697275876998901\n",
      "Epoch 9484: Training Loss: 0.11565440396467845 Validation Loss: 0.7692832946777344\n",
      "Epoch 9485: Training Loss: 0.11538633455832799 Validation Loss: 0.7695028781890869\n",
      "Epoch 9486: Training Loss: 0.11544771989186604 Validation Loss: 0.7700842022895813\n",
      "Epoch 9487: Training Loss: 0.11611338704824448 Validation Loss: 0.7712258696556091\n",
      "Epoch 9488: Training Loss: 0.11600140233834584 Validation Loss: 0.7707173228263855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9489: Training Loss: 0.11581553022066753 Validation Loss: 0.7693236470222473\n",
      "Epoch 9490: Training Loss: 0.11545561750729878 Validation Loss: 0.7690924406051636\n",
      "Epoch 9491: Training Loss: 0.11557115862766902 Validation Loss: 0.7692563533782959\n",
      "Epoch 9492: Training Loss: 0.11561456074317296 Validation Loss: 0.7693681120872498\n",
      "Epoch 9493: Training Loss: 0.11546881000200908 Validation Loss: 0.7697544693946838\n",
      "Epoch 9494: Training Loss: 0.11537908514340718 Validation Loss: 0.7699965238571167\n",
      "Epoch 9495: Training Loss: 0.11553871134916942 Validation Loss: 0.7700135707855225\n",
      "Epoch 9496: Training Loss: 0.11547272652387619 Validation Loss: 0.7702980041503906\n",
      "Epoch 9497: Training Loss: 0.11521364251772563 Validation Loss: 0.7692567706108093\n",
      "Epoch 9498: Training Loss: 0.11520811667044957 Validation Loss: 0.768707811832428\n",
      "Epoch 9499: Training Loss: 0.1157854696114858 Validation Loss: 0.7680762410163879\n",
      "Epoch 9500: Training Loss: 0.11554475128650665 Validation Loss: 0.7684834003448486\n",
      "Epoch 9501: Training Loss: 0.11565722276767094 Validation Loss: 0.7699756026268005\n",
      "Epoch 9502: Training Loss: 0.11562504371007283 Validation Loss: 0.7719292640686035\n",
      "Epoch 9503: Training Loss: 0.11537839720646541 Validation Loss: 0.7717333436012268\n",
      "Epoch 9504: Training Loss: 0.11603794991970062 Validation Loss: 0.7711160182952881\n",
      "Epoch 9505: Training Loss: 0.11534232397874196 Validation Loss: 0.7708060145378113\n",
      "Epoch 9506: Training Loss: 0.11533629894256592 Validation Loss: 0.7699763178825378\n",
      "Epoch 9507: Training Loss: 0.11521550764640172 Validation Loss: 0.7700043320655823\n",
      "Epoch 9508: Training Loss: 0.11514576524496078 Validation Loss: 0.7697606086730957\n",
      "Epoch 9509: Training Loss: 0.11531399935483932 Validation Loss: 0.7694025039672852\n",
      "Epoch 9510: Training Loss: 0.11550777405500412 Validation Loss: 0.7701437473297119\n",
      "Epoch 9511: Training Loss: 0.11550921201705933 Validation Loss: 0.7702427506446838\n",
      "Epoch 9512: Training Loss: 0.1154237116376559 Validation Loss: 0.7696793079376221\n",
      "Epoch 9513: Training Loss: 0.11526987950007121 Validation Loss: 0.7704700231552124\n",
      "Epoch 9514: Training Loss: 0.11512691030899684 Validation Loss: 0.7704681754112244\n",
      "Epoch 9515: Training Loss: 0.11512413869301479 Validation Loss: 0.7704247236251831\n",
      "Epoch 9516: Training Loss: 0.11512247969706853 Validation Loss: 0.7703261971473694\n",
      "Epoch 9517: Training Loss: 0.11609579374392827 Validation Loss: 0.7692062258720398\n",
      "Epoch 9518: Training Loss: 0.11522478361924489 Validation Loss: 0.7694223523139954\n",
      "Epoch 9519: Training Loss: 0.11532447983821233 Validation Loss: 0.769517183303833\n",
      "Epoch 9520: Training Loss: 0.11520271996657054 Validation Loss: 0.7698960304260254\n",
      "Epoch 9521: Training Loss: 0.11519657572110494 Validation Loss: 0.7704607248306274\n",
      "Epoch 9522: Training Loss: 0.1152225211262703 Validation Loss: 0.7698291540145874\n",
      "Epoch 9523: Training Loss: 0.11531182875235875 Validation Loss: 0.769521951675415\n",
      "Epoch 9524: Training Loss: 0.11501679321130116 Validation Loss: 0.7704318761825562\n",
      "Epoch 9525: Training Loss: 0.11514573792616527 Validation Loss: 0.7704410552978516\n",
      "Epoch 9526: Training Loss: 0.11522269745667775 Validation Loss: 0.7709747552871704\n",
      "Epoch 9527: Training Loss: 0.11510577549537022 Validation Loss: 0.7715501189231873\n",
      "Epoch 9528: Training Loss: 0.11483790725469589 Validation Loss: 0.7706542611122131\n",
      "Epoch 9529: Training Loss: 0.115198348959287 Validation Loss: 0.7700538635253906\n",
      "Epoch 9530: Training Loss: 0.11517882347106934 Validation Loss: 0.7695660591125488\n",
      "Epoch 9531: Training Loss: 0.11573509126901627 Validation Loss: 0.7695153951644897\n",
      "Epoch 9532: Training Loss: 0.11512387543916702 Validation Loss: 0.7699165344238281\n",
      "Epoch 9533: Training Loss: 0.11537499229113261 Validation Loss: 0.7699227929115295\n",
      "Epoch 9534: Training Loss: 0.11574262132247289 Validation Loss: 0.7708744406700134\n",
      "Epoch 9535: Training Loss: 0.11518971125284831 Validation Loss: 0.7703680992126465\n",
      "Epoch 9536: Training Loss: 0.11520178616046906 Validation Loss: 0.7711271643638611\n",
      "Epoch 9537: Training Loss: 0.11563216149806976 Validation Loss: 0.7706748843193054\n",
      "Epoch 9538: Training Loss: 0.11514416337013245 Validation Loss: 0.7709482312202454\n",
      "Epoch 9539: Training Loss: 0.11498170594374339 Validation Loss: 0.7708670496940613\n",
      "Epoch 9540: Training Loss: 0.11499586949745814 Validation Loss: 0.7712571024894714\n",
      "Epoch 9541: Training Loss: 0.11543025076389313 Validation Loss: 0.7707956433296204\n",
      "Epoch 9542: Training Loss: 0.11505816131830215 Validation Loss: 0.7702615261077881\n",
      "Epoch 9543: Training Loss: 0.1149134486913681 Validation Loss: 0.7704342603683472\n",
      "Epoch 9544: Training Loss: 0.11523965994517009 Validation Loss: 0.7708305716514587\n",
      "Epoch 9545: Training Loss: 0.11491659531990688 Validation Loss: 0.7699510455131531\n",
      "Epoch 9546: Training Loss: 0.11490893612305324 Validation Loss: 0.7698900103569031\n",
      "Epoch 9547: Training Loss: 0.11544402937094371 Validation Loss: 0.770058274269104\n",
      "Epoch 9548: Training Loss: 0.11535552144050598 Validation Loss: 0.7703946828842163\n",
      "Epoch 9549: Training Loss: 0.11503880470991135 Validation Loss: 0.77015221118927\n",
      "Epoch 9550: Training Loss: 0.11498120923837025 Validation Loss: 0.7707568407058716\n",
      "Epoch 9551: Training Loss: 0.11501363913218181 Validation Loss: 0.771762490272522\n",
      "Epoch 9552: Training Loss: 0.11556119968493779 Validation Loss: 0.7720435857772827\n",
      "Epoch 9553: Training Loss: 0.11530121912558873 Validation Loss: 0.7703585028648376\n",
      "Epoch 9554: Training Loss: 0.1160765141248703 Validation Loss: 0.7697519659996033\n",
      "Epoch 9555: Training Loss: 0.11539567013581593 Validation Loss: 0.7694063782691956\n",
      "Epoch 9556: Training Loss: 0.11475110550721486 Validation Loss: 0.7702763080596924\n",
      "Epoch 9557: Training Loss: 0.11480283240477245 Validation Loss: 0.7705783247947693\n",
      "Epoch 9558: Training Loss: 0.11501140395800273 Validation Loss: 0.7708982825279236\n",
      "Epoch 9559: Training Loss: 0.11509791016578674 Validation Loss: 0.7709932327270508\n",
      "Epoch 9560: Training Loss: 0.11484956989685695 Validation Loss: 0.769840657711029\n",
      "Epoch 9561: Training Loss: 0.1151942362387975 Validation Loss: 0.7694434523582458\n",
      "Epoch 9562: Training Loss: 0.11496016631523769 Validation Loss: 0.7700757384300232\n",
      "Epoch 9563: Training Loss: 0.11481785774230957 Validation Loss: 0.770620584487915\n",
      "Epoch 9564: Training Loss: 0.11497002591689427 Validation Loss: 0.7712173461914062\n",
      "Epoch 9565: Training Loss: 0.11476882050434749 Validation Loss: 0.7712694406509399\n",
      "Epoch 9566: Training Loss: 0.1149053896466891 Validation Loss: 0.7708495259284973\n",
      "Epoch 9567: Training Loss: 0.11474549522002538 Validation Loss: 0.7709523439407349\n",
      "Epoch 9568: Training Loss: 0.11645177006721497 Validation Loss: 0.771348774433136\n",
      "Epoch 9569: Training Loss: 0.11470382660627365 Validation Loss: 0.7712040543556213\n",
      "Epoch 9570: Training Loss: 0.11502192417780559 Validation Loss: 0.7711244821548462\n",
      "Epoch 9571: Training Loss: 0.11583727101484935 Validation Loss: 0.7708552479743958\n",
      "Epoch 9572: Training Loss: 0.11511510362227757 Validation Loss: 0.7705785036087036\n",
      "Epoch 9573: Training Loss: 0.11469169706106186 Validation Loss: 0.7705568075180054\n",
      "Epoch 9574: Training Loss: 0.11469582716623943 Validation Loss: 0.7707926034927368\n",
      "Epoch 9575: Training Loss: 0.1147052173813184 Validation Loss: 0.7705425024032593\n",
      "Epoch 9576: Training Loss: 0.11436756451924641 Validation Loss: 0.7704436182975769\n",
      "Epoch 9577: Training Loss: 0.11500405271848042 Validation Loss: 0.7709063291549683\n",
      "Epoch 9578: Training Loss: 0.11466323832670848 Validation Loss: 0.7709226012229919\n",
      "Epoch 9579: Training Loss: 0.11466979732116063 Validation Loss: 0.7708778381347656\n",
      "Epoch 9580: Training Loss: 0.11474570880333583 Validation Loss: 0.7707155346870422\n",
      "Epoch 9581: Training Loss: 0.11435548216104507 Validation Loss: 0.7709624767303467\n",
      "Epoch 9582: Training Loss: 0.11464617401361465 Validation Loss: 0.7706072926521301\n",
      "Epoch 9583: Training Loss: 0.11464668810367584 Validation Loss: 0.7714092135429382\n",
      "Epoch 9584: Training Loss: 0.11506034682194392 Validation Loss: 0.7709356546401978\n",
      "Epoch 9585: Training Loss: 0.1145319864153862 Validation Loss: 0.7704759836196899\n",
      "Epoch 9586: Training Loss: 0.11492125689983368 Validation Loss: 0.7702299356460571\n",
      "Epoch 9587: Training Loss: 0.11465548227230708 Validation Loss: 0.770843505859375\n",
      "Epoch 9588: Training Loss: 0.11501029382149379 Validation Loss: 0.7715354561805725\n",
      "Epoch 9589: Training Loss: 0.11461135496695836 Validation Loss: 0.7720600962638855\n",
      "Epoch 9590: Training Loss: 0.11470089852809906 Validation Loss: 0.7713504433631897\n",
      "Epoch 9591: Training Loss: 0.11449697613716125 Validation Loss: 0.7707515358924866\n",
      "Epoch 9592: Training Loss: 0.11453567196925481 Validation Loss: 0.7709018588066101\n",
      "Epoch 9593: Training Loss: 0.11451073239247005 Validation Loss: 0.7703130841255188\n",
      "Epoch 9594: Training Loss: 0.11505334575970967 Validation Loss: 0.7707310318946838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9595: Training Loss: 0.11452424029509227 Validation Loss: 0.7707446217536926\n",
      "Epoch 9596: Training Loss: 0.11458376795053482 Validation Loss: 0.7713789939880371\n",
      "Epoch 9597: Training Loss: 0.11475928127765656 Validation Loss: 0.7710806131362915\n",
      "Epoch 9598: Training Loss: 0.11448163787523906 Validation Loss: 0.7718614339828491\n",
      "Epoch 9599: Training Loss: 0.11447357137997945 Validation Loss: 0.7721525430679321\n",
      "Epoch 9600: Training Loss: 0.11444100489219029 Validation Loss: 0.7712637782096863\n",
      "Epoch 9601: Training Loss: 0.11437055716911952 Validation Loss: 0.7706796526908875\n",
      "Epoch 9602: Training Loss: 0.11449449012676875 Validation Loss: 0.7702018618583679\n",
      "Epoch 9603: Training Loss: 0.11475621908903122 Validation Loss: 0.7709025740623474\n",
      "Epoch 9604: Training Loss: 0.11473307261864345 Validation Loss: 0.7704722881317139\n",
      "Epoch 9605: Training Loss: 0.11508020510276158 Validation Loss: 0.7709147334098816\n",
      "Epoch 9606: Training Loss: 0.11450120061635971 Validation Loss: 0.7710890173912048\n",
      "Epoch 9607: Training Loss: 0.11464061588048935 Validation Loss: 0.7719614505767822\n",
      "Epoch 9608: Training Loss: 0.11442515254020691 Validation Loss: 0.7715954780578613\n",
      "Epoch 9609: Training Loss: 0.11445810894171397 Validation Loss: 0.7717627882957458\n",
      "Epoch 9610: Training Loss: 0.11421545346577962 Validation Loss: 0.7708083391189575\n",
      "Epoch 9611: Training Loss: 0.11438470830519994 Validation Loss: 0.7706690430641174\n",
      "Epoch 9612: Training Loss: 0.11429294943809509 Validation Loss: 0.7714560627937317\n",
      "Epoch 9613: Training Loss: 0.11447962373495102 Validation Loss: 0.7710391879081726\n",
      "Epoch 9614: Training Loss: 0.11432971556981404 Validation Loss: 0.771446943283081\n",
      "Epoch 9615: Training Loss: 0.1143034224708875 Validation Loss: 0.7718353271484375\n",
      "Epoch 9616: Training Loss: 0.11447601268688838 Validation Loss: 0.7716000080108643\n",
      "Epoch 9617: Training Loss: 0.11438680440187454 Validation Loss: 0.7712448835372925\n",
      "Epoch 9618: Training Loss: 0.11527606099843979 Validation Loss: 0.7706254124641418\n",
      "Epoch 9619: Training Loss: 0.11435304085413615 Validation Loss: 0.7713180184364319\n",
      "Epoch 9620: Training Loss: 0.11421961585680644 Validation Loss: 0.77102130651474\n",
      "Epoch 9621: Training Loss: 0.11441647013028462 Validation Loss: 0.7717599868774414\n",
      "Epoch 9622: Training Loss: 0.11414084831873576 Validation Loss: 0.7710191011428833\n",
      "Epoch 9623: Training Loss: 0.1144477774699529 Validation Loss: 0.7709523439407349\n",
      "Epoch 9624: Training Loss: 0.11420238266388576 Validation Loss: 0.770509660243988\n",
      "Epoch 9625: Training Loss: 0.11456572512785594 Validation Loss: 0.7709665298461914\n",
      "Epoch 9626: Training Loss: 0.11456182847420375 Validation Loss: 0.7718784809112549\n",
      "Epoch 9627: Training Loss: 0.1144084557890892 Validation Loss: 0.7716007232666016\n",
      "Epoch 9628: Training Loss: 0.11397345612446468 Validation Loss: 0.771350622177124\n",
      "Epoch 9629: Training Loss: 0.11446266869703929 Validation Loss: 0.7715816497802734\n",
      "Epoch 9630: Training Loss: 0.11419496685266495 Validation Loss: 0.7722005248069763\n",
      "Epoch 9631: Training Loss: 0.11395073682069778 Validation Loss: 0.7716278433799744\n",
      "Epoch 9632: Training Loss: 0.11454132944345474 Validation Loss: 0.771367609500885\n",
      "Epoch 9633: Training Loss: 0.11420779178539912 Validation Loss: 0.770590603351593\n",
      "Epoch 9634: Training Loss: 0.11424387246370316 Validation Loss: 0.7706609964370728\n",
      "Epoch 9635: Training Loss: 0.1142473891377449 Validation Loss: 0.7704746723175049\n",
      "Epoch 9636: Training Loss: 0.11430065333843231 Validation Loss: 0.7713408470153809\n",
      "Epoch 9637: Training Loss: 0.1142951175570488 Validation Loss: 0.7716248631477356\n",
      "Epoch 9638: Training Loss: 0.11445643256107967 Validation Loss: 0.7721390724182129\n",
      "Epoch 9639: Training Loss: 0.1142167995373408 Validation Loss: 0.7716423869132996\n",
      "Epoch 9640: Training Loss: 0.11497481912374496 Validation Loss: 0.7712609171867371\n",
      "Epoch 9641: Training Loss: 0.11404494196176529 Validation Loss: 0.7713542580604553\n",
      "Epoch 9642: Training Loss: 0.11415594071149826 Validation Loss: 0.7709925174713135\n",
      "Epoch 9643: Training Loss: 0.1146186242500941 Validation Loss: 0.7714420557022095\n",
      "Epoch 9644: Training Loss: 0.11430807411670685 Validation Loss: 0.7714284062385559\n",
      "Epoch 9645: Training Loss: 0.11431654294331868 Validation Loss: 0.7724839448928833\n",
      "Epoch 9646: Training Loss: 0.11432590335607529 Validation Loss: 0.7721751928329468\n",
      "Epoch 9647: Training Loss: 0.1139125054081281 Validation Loss: 0.7722750902175903\n",
      "Epoch 9648: Training Loss: 0.11434303969144821 Validation Loss: 0.77266925573349\n",
      "Epoch 9649: Training Loss: 0.11416797588268916 Validation Loss: 0.7720840573310852\n",
      "Epoch 9650: Training Loss: 0.1145053505897522 Validation Loss: 0.771705687046051\n",
      "Epoch 9651: Training Loss: 0.11442611614863078 Validation Loss: 0.7715977430343628\n",
      "Epoch 9652: Training Loss: 0.11425920327504475 Validation Loss: 0.7715778946876526\n",
      "Epoch 9653: Training Loss: 0.11425206313530605 Validation Loss: 0.7724670767784119\n",
      "Epoch 9654: Training Loss: 0.11403543998797734 Validation Loss: 0.7722436189651489\n",
      "Epoch 9655: Training Loss: 0.11392817894617717 Validation Loss: 0.7719476222991943\n",
      "Epoch 9656: Training Loss: 0.114372119307518 Validation Loss: 0.7718885540962219\n",
      "Epoch 9657: Training Loss: 0.11400483548641205 Validation Loss: 0.7716319561004639\n",
      "Epoch 9658: Training Loss: 0.11401843776305516 Validation Loss: 0.7712955474853516\n",
      "Epoch 9659: Training Loss: 0.11412204802036285 Validation Loss: 0.7715224027633667\n",
      "Epoch 9660: Training Loss: 0.11376689622799556 Validation Loss: 0.7713790535926819\n",
      "Epoch 9661: Training Loss: 0.11399090538422267 Validation Loss: 0.7715527415275574\n",
      "Epoch 9662: Training Loss: 0.11410574863354365 Validation Loss: 0.7719307541847229\n",
      "Epoch 9663: Training Loss: 0.11472338438034058 Validation Loss: 0.7725475430488586\n",
      "Epoch 9664: Training Loss: 0.11387926836808522 Validation Loss: 0.7726389765739441\n",
      "Epoch 9665: Training Loss: 0.1144867514570554 Validation Loss: 0.7721518278121948\n",
      "Epoch 9666: Training Loss: 0.11389703800280888 Validation Loss: 0.7711864709854126\n",
      "Epoch 9667: Training Loss: 0.11386523147424062 Validation Loss: 0.771037220954895\n",
      "Epoch 9668: Training Loss: 0.11406491448481877 Validation Loss: 0.7714771628379822\n",
      "Epoch 9669: Training Loss: 0.11419230947891872 Validation Loss: 0.7725054621696472\n",
      "Epoch 9670: Training Loss: 0.11387340227762859 Validation Loss: 0.7715167999267578\n",
      "Epoch 9671: Training Loss: 0.113849143187205 Validation Loss: 0.7719014286994934\n",
      "Epoch 9672: Training Loss: 0.11405844241380692 Validation Loss: 0.7721191048622131\n",
      "Epoch 9673: Training Loss: 0.11458200961351395 Validation Loss: 0.7721962928771973\n",
      "Epoch 9674: Training Loss: 0.11394233008225758 Validation Loss: 0.7718351483345032\n",
      "Epoch 9675: Training Loss: 0.11349158734083176 Validation Loss: 0.7718961834907532\n",
      "Epoch 9676: Training Loss: 0.11399321009715398 Validation Loss: 0.7721810340881348\n",
      "Epoch 9677: Training Loss: 0.1139450545112292 Validation Loss: 0.7712478041648865\n",
      "Epoch 9678: Training Loss: 0.11383458475271861 Validation Loss: 0.77192223072052\n",
      "Epoch 9679: Training Loss: 0.11430814117193222 Validation Loss: 0.7723063230514526\n",
      "Epoch 9680: Training Loss: 0.11383878688017528 Validation Loss: 0.7724379301071167\n",
      "Epoch 9681: Training Loss: 0.11383677770694096 Validation Loss: 0.7720560431480408\n",
      "Epoch 9682: Training Loss: 0.11371709158023198 Validation Loss: 0.7721759080886841\n",
      "Epoch 9683: Training Loss: 0.11376577864090602 Validation Loss: 0.7717685103416443\n",
      "Epoch 9684: Training Loss: 0.114839772383372 Validation Loss: 0.7718525528907776\n",
      "Epoch 9685: Training Loss: 0.11389651149511337 Validation Loss: 0.7726424336433411\n",
      "Epoch 9686: Training Loss: 0.11409661422173183 Validation Loss: 0.7731642723083496\n",
      "Epoch 9687: Training Loss: 0.11474346121152242 Validation Loss: 0.7723852396011353\n",
      "Epoch 9688: Training Loss: 0.11366468171278636 Validation Loss: 0.7726132273674011\n",
      "Epoch 9689: Training Loss: 0.11407590905825298 Validation Loss: 0.772534191608429\n",
      "Epoch 9690: Training Loss: 0.11404119928677876 Validation Loss: 0.7724714875221252\n",
      "Epoch 9691: Training Loss: 0.11405675858259201 Validation Loss: 0.772977888584137\n",
      "Epoch 9692: Training Loss: 0.11408442010482152 Validation Loss: 0.772560715675354\n",
      "Epoch 9693: Training Loss: 0.11398421972990036 Validation Loss: 0.7722793221473694\n",
      "Epoch 9694: Training Loss: 0.11368924379348755 Validation Loss: 0.7719399333000183\n",
      "Epoch 9695: Training Loss: 0.11315928399562836 Validation Loss: 0.7722535133361816\n",
      "Epoch 9696: Training Loss: 0.11359605193138123 Validation Loss: 0.7725843787193298\n",
      "Epoch 9697: Training Loss: 0.1137428159515063 Validation Loss: 0.7725511789321899\n",
      "Epoch 9698: Training Loss: 0.11354498068491618 Validation Loss: 0.7722986936569214\n",
      "Epoch 9699: Training Loss: 0.11383116990327835 Validation Loss: 0.7729618549346924\n",
      "Epoch 9700: Training Loss: 0.11363484213749568 Validation Loss: 0.7726777791976929\n",
      "Epoch 9701: Training Loss: 0.11377487828334172 Validation Loss: 0.772718071937561\n",
      "Epoch 9702: Training Loss: 0.11375369876623154 Validation Loss: 0.773135244846344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9703: Training Loss: 0.11350342631340027 Validation Loss: 0.7723121047019958\n",
      "Epoch 9704: Training Loss: 0.11346262693405151 Validation Loss: 0.772378146648407\n",
      "Epoch 9705: Training Loss: 0.1134891336162885 Validation Loss: 0.7719346284866333\n",
      "Epoch 9706: Training Loss: 0.11318362752596538 Validation Loss: 0.7720720767974854\n",
      "Epoch 9707: Training Loss: 0.113896610836188 Validation Loss: 0.7714846730232239\n",
      "Epoch 9708: Training Loss: 0.11373198529084523 Validation Loss: 0.7721301913261414\n",
      "Epoch 9709: Training Loss: 0.11325930058956146 Validation Loss: 0.7715823650360107\n",
      "Epoch 9710: Training Loss: 0.11351647973060608 Validation Loss: 0.7721901535987854\n",
      "Epoch 9711: Training Loss: 0.11370837440093358 Validation Loss: 0.7720257043838501\n",
      "Epoch 9712: Training Loss: 0.1136033187309901 Validation Loss: 0.7722005248069763\n",
      "Epoch 9713: Training Loss: 0.11387604971726735 Validation Loss: 0.7717371582984924\n",
      "Epoch 9714: Training Loss: 0.11353566497564316 Validation Loss: 0.7725389003753662\n",
      "Epoch 9715: Training Loss: 0.11426414052645366 Validation Loss: 0.7727829813957214\n",
      "Epoch 9716: Training Loss: 0.11463286727666855 Validation Loss: 0.7722054123878479\n",
      "Epoch 9717: Training Loss: 0.11339079091946284 Validation Loss: 0.7726027369499207\n",
      "Epoch 9718: Training Loss: 0.11370354145765305 Validation Loss: 0.7735987901687622\n",
      "Epoch 9719: Training Loss: 0.11346310625473659 Validation Loss: 0.7733666300773621\n",
      "Epoch 9720: Training Loss: 0.11381718267997105 Validation Loss: 0.7729955911636353\n",
      "Epoch 9721: Training Loss: 0.11337327708800633 Validation Loss: 0.7728078365325928\n",
      "Epoch 9722: Training Loss: 0.11368856330712636 Validation Loss: 0.771875262260437\n",
      "Epoch 9723: Training Loss: 0.11339942614237468 Validation Loss: 0.7721115946769714\n",
      "Epoch 9724: Training Loss: 0.1133514146010081 Validation Loss: 0.772203266620636\n",
      "Epoch 9725: Training Loss: 0.11380773534377416 Validation Loss: 0.7729827165603638\n",
      "Epoch 9726: Training Loss: 0.11354953795671463 Validation Loss: 0.773629903793335\n",
      "Epoch 9727: Training Loss: 0.11418173958857854 Validation Loss: 0.7729990482330322\n",
      "Epoch 9728: Training Loss: 0.11334686229626338 Validation Loss: 0.7725580930709839\n",
      "Epoch 9729: Training Loss: 0.11370807141065598 Validation Loss: 0.7721291184425354\n",
      "Epoch 9730: Training Loss: 0.11340169856945674 Validation Loss: 0.7720813751220703\n",
      "Epoch 9731: Training Loss: 0.1137818694114685 Validation Loss: 0.7725878953933716\n",
      "Epoch 9732: Training Loss: 0.11342162390549977 Validation Loss: 0.772882878780365\n",
      "Epoch 9733: Training Loss: 0.11328687767187755 Validation Loss: 0.7730499505996704\n",
      "Epoch 9734: Training Loss: 0.11324144651492436 Validation Loss: 0.7734471559524536\n",
      "Epoch 9735: Training Loss: 0.1142144575715065 Validation Loss: 0.7729132771492004\n",
      "Epoch 9736: Training Loss: 0.11316792418559392 Validation Loss: 0.7731724381446838\n",
      "Epoch 9737: Training Loss: 0.11389644692341487 Validation Loss: 0.7723029255867004\n",
      "Epoch 9738: Training Loss: 0.11352814982334773 Validation Loss: 0.7717282772064209\n",
      "Epoch 9739: Training Loss: 0.11346183220545451 Validation Loss: 0.7717203497886658\n",
      "Epoch 9740: Training Loss: 0.11371644834677379 Validation Loss: 0.7724425792694092\n",
      "Epoch 9741: Training Loss: 0.11323610444863637 Validation Loss: 0.772253155708313\n",
      "Epoch 9742: Training Loss: 0.11318386346101761 Validation Loss: 0.7728803753852844\n",
      "Epoch 9743: Training Loss: 0.11352472752332687 Validation Loss: 0.7735612988471985\n",
      "Epoch 9744: Training Loss: 0.11331458389759064 Validation Loss: 0.7737863063812256\n",
      "Epoch 9745: Training Loss: 0.11377231528361638 Validation Loss: 0.7729089260101318\n",
      "Epoch 9746: Training Loss: 0.11333124587933223 Validation Loss: 0.7738714218139648\n",
      "Epoch 9747: Training Loss: 0.11342745274305344 Validation Loss: 0.7728965282440186\n",
      "Epoch 9748: Training Loss: 0.11328513423601787 Validation Loss: 0.7728153467178345\n",
      "Epoch 9749: Training Loss: 0.11313501248757045 Validation Loss: 0.7728073596954346\n",
      "Epoch 9750: Training Loss: 0.11336975793043773 Validation Loss: 0.7732082009315491\n",
      "Epoch 9751: Training Loss: 0.11321580658356349 Validation Loss: 0.7729285955429077\n",
      "Epoch 9752: Training Loss: 0.11339980363845825 Validation Loss: 0.7732011079788208\n",
      "Epoch 9753: Training Loss: 0.11314329008261363 Validation Loss: 0.7733349800109863\n",
      "Epoch 9754: Training Loss: 0.11344054092963536 Validation Loss: 0.7732002139091492\n",
      "Epoch 9755: Training Loss: 0.1130397046605746 Validation Loss: 0.7725129723548889\n",
      "Epoch 9756: Training Loss: 0.11315773924191792 Validation Loss: 0.7728707194328308\n",
      "Epoch 9757: Training Loss: 0.11305495103200276 Validation Loss: 0.7719570398330688\n",
      "Epoch 9758: Training Loss: 0.1134146402279536 Validation Loss: 0.7710963487625122\n",
      "Epoch 9759: Training Loss: 0.11323477576176326 Validation Loss: 0.7724780440330505\n",
      "Epoch 9760: Training Loss: 0.1130701353152593 Validation Loss: 0.7732411026954651\n",
      "Epoch 9761: Training Loss: 0.11347219347953796 Validation Loss: 0.7732695937156677\n",
      "Epoch 9762: Training Loss: 0.11344098796447118 Validation Loss: 0.7736123204231262\n",
      "Epoch 9763: Training Loss: 0.1132836863398552 Validation Loss: 0.7730991840362549\n",
      "Epoch 9764: Training Loss: 0.11317726224660873 Validation Loss: 0.7728705406188965\n",
      "Epoch 9765: Training Loss: 0.11347903311252594 Validation Loss: 0.7725099325180054\n",
      "Epoch 9766: Training Loss: 0.11299673467874527 Validation Loss: 0.7722912430763245\n",
      "Epoch 9767: Training Loss: 0.11294528593619664 Validation Loss: 0.7726752161979675\n",
      "Epoch 9768: Training Loss: 0.11298574010531108 Validation Loss: 0.7732515931129456\n",
      "Epoch 9769: Training Loss: 0.11332980046669643 Validation Loss: 0.773817241191864\n",
      "Epoch 9770: Training Loss: 0.11313395698865254 Validation Loss: 0.7736353874206543\n",
      "Epoch 9771: Training Loss: 0.1133419672648112 Validation Loss: 0.7743657827377319\n",
      "Epoch 9772: Training Loss: 0.11290255188941956 Validation Loss: 0.7738836407661438\n",
      "Epoch 9773: Training Loss: 0.11365589996178944 Validation Loss: 0.7739294767379761\n",
      "Epoch 9774: Training Loss: 0.11306850612163544 Validation Loss: 0.7725850939750671\n",
      "Epoch 9775: Training Loss: 0.11270427703857422 Validation Loss: 0.7718755006790161\n",
      "Epoch 9776: Training Loss: 0.11308528234561284 Validation Loss: 0.7722323536872864\n",
      "Epoch 9777: Training Loss: 0.11317725479602814 Validation Loss: 0.7733368277549744\n",
      "Epoch 9778: Training Loss: 0.1128482073545456 Validation Loss: 0.7732797265052795\n",
      "Epoch 9779: Training Loss: 0.11275652796030045 Validation Loss: 0.7734606266021729\n",
      "Epoch 9780: Training Loss: 0.11319538454214732 Validation Loss: 0.773984968662262\n",
      "Epoch 9781: Training Loss: 0.1128487338622411 Validation Loss: 0.7730879187583923\n",
      "Epoch 9782: Training Loss: 0.11302412549654643 Validation Loss: 0.7731741070747375\n",
      "Epoch 9783: Training Loss: 0.11331367492675781 Validation Loss: 0.772455632686615\n",
      "Epoch 9784: Training Loss: 0.11308728903532028 Validation Loss: 0.7720540761947632\n",
      "Epoch 9785: Training Loss: 0.1134346475203832 Validation Loss: 0.7730271220207214\n",
      "Epoch 9786: Training Loss: 0.1132538840174675 Validation Loss: 0.7743933796882629\n",
      "Epoch 9787: Training Loss: 0.11269394805034001 Validation Loss: 0.7745312452316284\n",
      "Epoch 9788: Training Loss: 0.11303751667340596 Validation Loss: 0.7739478349685669\n",
      "Epoch 9789: Training Loss: 0.11355177809794743 Validation Loss: 0.7746797204017639\n",
      "Epoch 9790: Training Loss: 0.11312543352444966 Validation Loss: 0.7735639810562134\n",
      "Epoch 9791: Training Loss: 0.11297638962666194 Validation Loss: 0.7730683088302612\n",
      "Epoch 9792: Training Loss: 0.11263743291298549 Validation Loss: 0.7725260853767395\n",
      "Epoch 9793: Training Loss: 0.11302429437637329 Validation Loss: 0.7728273272514343\n",
      "Epoch 9794: Training Loss: 0.11308479805787404 Validation Loss: 0.77390456199646\n",
      "Epoch 9795: Training Loss: 0.11376748234033585 Validation Loss: 0.7742868661880493\n",
      "Epoch 9796: Training Loss: 0.11295060565074284 Validation Loss: 0.7736798524856567\n",
      "Epoch 9797: Training Loss: 0.11291176080703735 Validation Loss: 0.7734341621398926\n",
      "Epoch 9798: Training Loss: 0.11317479610443115 Validation Loss: 0.7734429836273193\n",
      "Epoch 9799: Training Loss: 0.1126711368560791 Validation Loss: 0.7733496427536011\n",
      "Epoch 9800: Training Loss: 0.11330761263767879 Validation Loss: 0.7734172344207764\n",
      "Epoch 9801: Training Loss: 0.11298832545677821 Validation Loss: 0.7740728259086609\n",
      "Epoch 9802: Training Loss: 0.113070214788119 Validation Loss: 0.7742543816566467\n",
      "Epoch 9803: Training Loss: 0.11299502849578857 Validation Loss: 0.7742738127708435\n",
      "Epoch 9804: Training Loss: 0.11287605514129002 Validation Loss: 0.7735922336578369\n",
      "Epoch 9805: Training Loss: 0.11305355032285054 Validation Loss: 0.7733652591705322\n",
      "Epoch 9806: Training Loss: 0.11288390556971233 Validation Loss: 0.7734234929084778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9807: Training Loss: 0.11311432719230652 Validation Loss: 0.7735924124717712\n",
      "Epoch 9808: Training Loss: 0.11282845338185628 Validation Loss: 0.7732844948768616\n",
      "Epoch 9809: Training Loss: 0.11284030228853226 Validation Loss: 0.7727470993995667\n",
      "Epoch 9810: Training Loss: 0.11266434441010158 Validation Loss: 0.7736964225769043\n",
      "Epoch 9811: Training Loss: 0.11266726007064183 Validation Loss: 0.7744007706642151\n",
      "Epoch 9812: Training Loss: 0.11264591912428538 Validation Loss: 0.774042546749115\n",
      "Epoch 9813: Training Loss: 0.11271318048238754 Validation Loss: 0.7742005586624146\n",
      "Epoch 9814: Training Loss: 0.11294448375701904 Validation Loss: 0.7745554447174072\n",
      "Epoch 9815: Training Loss: 0.11309409886598587 Validation Loss: 0.7741492390632629\n",
      "Epoch 9816: Training Loss: 0.11297870427370071 Validation Loss: 0.7740723490715027\n",
      "Epoch 9817: Training Loss: 0.11346329251925151 Validation Loss: 0.773606538772583\n",
      "Epoch 9818: Training Loss: 0.11282804608345032 Validation Loss: 0.773158073425293\n",
      "Epoch 9819: Training Loss: 0.11262662460406621 Validation Loss: 0.7732953429222107\n",
      "Epoch 9820: Training Loss: 0.11321692665417989 Validation Loss: 0.7741920351982117\n",
      "Epoch 9821: Training Loss: 0.1129106804728508 Validation Loss: 0.7744402289390564\n",
      "Epoch 9822: Training Loss: 0.11289313435554504 Validation Loss: 0.7746267318725586\n",
      "Epoch 9823: Training Loss: 0.11285396416982015 Validation Loss: 0.7744171023368835\n",
      "Epoch 9824: Training Loss: 0.11326734473307927 Validation Loss: 0.7736814618110657\n",
      "Epoch 9825: Training Loss: 0.11290917297204335 Validation Loss: 0.7733246684074402\n",
      "Epoch 9826: Training Loss: 0.11285948256651561 Validation Loss: 0.7740505933761597\n",
      "Epoch 9827: Training Loss: 0.1125415712594986 Validation Loss: 0.7735583186149597\n",
      "Epoch 9828: Training Loss: 0.11290519187847774 Validation Loss: 0.7746273875236511\n",
      "Epoch 9829: Training Loss: 0.11262860894203186 Validation Loss: 0.7747674584388733\n",
      "Epoch 9830: Training Loss: 0.11260909587144852 Validation Loss: 0.7742968797683716\n",
      "Epoch 9831: Training Loss: 0.11264268308877945 Validation Loss: 0.7736926078796387\n",
      "Epoch 9832: Training Loss: 0.11260563880205154 Validation Loss: 0.7739817500114441\n",
      "Epoch 9833: Training Loss: 0.11250520745913188 Validation Loss: 0.7742474675178528\n",
      "Epoch 9834: Training Loss: 0.11294091989596684 Validation Loss: 0.7746533155441284\n",
      "Epoch 9835: Training Loss: 0.11254144708315532 Validation Loss: 0.7746350169181824\n",
      "Epoch 9836: Training Loss: 0.11264825363953908 Validation Loss: 0.7738015651702881\n",
      "Epoch 9837: Training Loss: 0.11254108200470607 Validation Loss: 0.7735111713409424\n",
      "Epoch 9838: Training Loss: 0.11274576435486476 Validation Loss: 0.7737801671028137\n",
      "Epoch 9839: Training Loss: 0.11271561682224274 Validation Loss: 0.7747563719749451\n",
      "Epoch 9840: Training Loss: 0.11227526267369588 Validation Loss: 0.7746719717979431\n",
      "Epoch 9841: Training Loss: 0.1129378726085027 Validation Loss: 0.7742246389389038\n",
      "Epoch 9842: Training Loss: 0.11246709773937862 Validation Loss: 0.773583710193634\n",
      "Epoch 9843: Training Loss: 0.11254982898632686 Validation Loss: 0.7732947468757629\n",
      "Epoch 9844: Training Loss: 0.1134303758541743 Validation Loss: 0.7739935517311096\n",
      "Epoch 9845: Training Loss: 0.11268203457196553 Validation Loss: 0.77436363697052\n",
      "Epoch 9846: Training Loss: 0.11238270501295726 Validation Loss: 0.7746124863624573\n",
      "Epoch 9847: Training Loss: 0.11349528034528096 Validation Loss: 0.774695098400116\n",
      "Epoch 9848: Training Loss: 0.11248825738827388 Validation Loss: 0.7754631638526917\n",
      "Epoch 9849: Training Loss: 0.11299789200226466 Validation Loss: 0.7752277851104736\n",
      "Epoch 9850: Training Loss: 0.11238992462555568 Validation Loss: 0.7752495408058167\n",
      "Epoch 9851: Training Loss: 0.1121777097384135 Validation Loss: 0.7747623324394226\n",
      "Epoch 9852: Training Loss: 0.11321753511826198 Validation Loss: 0.7742686867713928\n",
      "Epoch 9853: Training Loss: 0.11268830051024754 Validation Loss: 0.7743855714797974\n",
      "Epoch 9854: Training Loss: 0.11249917248884837 Validation Loss: 0.7740634679794312\n",
      "Epoch 9855: Training Loss: 0.11258209993441899 Validation Loss: 0.774284839630127\n",
      "Epoch 9856: Training Loss: 0.11306914935509364 Validation Loss: 0.7751756310462952\n",
      "Epoch 9857: Training Loss: 0.11251405129830043 Validation Loss: 0.7745552062988281\n",
      "Epoch 9858: Training Loss: 0.11237475275993347 Validation Loss: 0.774199366569519\n",
      "Epoch 9859: Training Loss: 0.11248697588841121 Validation Loss: 0.7747741341590881\n",
      "Epoch 9860: Training Loss: 0.11225126683712006 Validation Loss: 0.7744596600532532\n",
      "Epoch 9861: Training Loss: 0.11227572460969289 Validation Loss: 0.774040162563324\n",
      "Epoch 9862: Training Loss: 0.11203169822692871 Validation Loss: 0.7743631601333618\n",
      "Epoch 9863: Training Loss: 0.11264977355798085 Validation Loss: 0.7747490406036377\n",
      "Epoch 9864: Training Loss: 0.11208259811004002 Validation Loss: 0.774726152420044\n",
      "Epoch 9865: Training Loss: 0.1124633178114891 Validation Loss: 0.7751957774162292\n",
      "Epoch 9866: Training Loss: 0.11280612895886104 Validation Loss: 0.7753264904022217\n",
      "Epoch 9867: Training Loss: 0.1125890463590622 Validation Loss: 0.774080753326416\n",
      "Epoch 9868: Training Loss: 0.11229901760816574 Validation Loss: 0.7742153406143188\n",
      "Epoch 9869: Training Loss: 0.11229735612869263 Validation Loss: 0.7740620970726013\n",
      "Epoch 9870: Training Loss: 0.11240064104398091 Validation Loss: 0.7735228538513184\n",
      "Epoch 9871: Training Loss: 0.11189440389474233 Validation Loss: 0.7749876976013184\n",
      "Epoch 9872: Training Loss: 0.11232288430134456 Validation Loss: 0.7752974033355713\n",
      "Epoch 9873: Training Loss: 0.11270053933064143 Validation Loss: 0.7749807238578796\n",
      "Epoch 9874: Training Loss: 0.11220817764600118 Validation Loss: 0.7747183442115784\n",
      "Epoch 9875: Training Loss: 0.11239475756883621 Validation Loss: 0.7749249935150146\n",
      "Epoch 9876: Training Loss: 0.11251455048720042 Validation Loss: 0.7749958634376526\n",
      "Epoch 9877: Training Loss: 0.11224840581417084 Validation Loss: 0.77508145570755\n",
      "Epoch 9878: Training Loss: 0.11233746508757274 Validation Loss: 0.7750486731529236\n",
      "Epoch 9879: Training Loss: 0.1120258296529452 Validation Loss: 0.7749263644218445\n",
      "Epoch 9880: Training Loss: 0.11232534050941467 Validation Loss: 0.7740947008132935\n",
      "Epoch 9881: Training Loss: 0.11288169771432877 Validation Loss: 0.7734643220901489\n",
      "Epoch 9882: Training Loss: 0.11212343722581863 Validation Loss: 0.7740503549575806\n",
      "Epoch 9883: Training Loss: 0.1118267426888148 Validation Loss: 0.7753949165344238\n",
      "Epoch 9884: Training Loss: 0.1121844897667567 Validation Loss: 0.7756696939468384\n",
      "Epoch 9885: Training Loss: 0.11229836444060008 Validation Loss: 0.7754347324371338\n",
      "Epoch 9886: Training Loss: 0.11246580382188161 Validation Loss: 0.7745274901390076\n",
      "Epoch 9887: Training Loss: 0.11222450931866963 Validation Loss: 0.7746171951293945\n",
      "Epoch 9888: Training Loss: 0.1121617928147316 Validation Loss: 0.7738136053085327\n",
      "Epoch 9889: Training Loss: 0.1125132938226064 Validation Loss: 0.7738320231437683\n",
      "Epoch 9890: Training Loss: 0.11261599510908127 Validation Loss: 0.7741509079933167\n",
      "Epoch 9891: Training Loss: 0.11202334612607956 Validation Loss: 0.7744368314743042\n",
      "Epoch 9892: Training Loss: 0.11191928883393605 Validation Loss: 0.7749133110046387\n",
      "Epoch 9893: Training Loss: 0.11235963304837544 Validation Loss: 0.7750847935676575\n",
      "Epoch 9894: Training Loss: 0.11210220555464427 Validation Loss: 0.7753968834877014\n",
      "Epoch 9895: Training Loss: 0.11223120242357254 Validation Loss: 0.7754571437835693\n",
      "Epoch 9896: Training Loss: 0.11211203783750534 Validation Loss: 0.7759395241737366\n",
      "Epoch 9897: Training Loss: 0.11185981581608455 Validation Loss: 0.7757141590118408\n",
      "Epoch 9898: Training Loss: 0.11219998449087143 Validation Loss: 0.7748643755912781\n",
      "Epoch 9899: Training Loss: 0.11158236612876256 Validation Loss: 0.7753511071205139\n",
      "Epoch 9900: Training Loss: 0.11199603726466496 Validation Loss: 0.7751783132553101\n",
      "Epoch 9901: Training Loss: 0.11197545876105626 Validation Loss: 0.7747809290885925\n",
      "Epoch 9902: Training Loss: 0.11264254152774811 Validation Loss: 0.7751366496086121\n",
      "Epoch 9903: Training Loss: 0.11169099062681198 Validation Loss: 0.7748879790306091\n",
      "Epoch 9904: Training Loss: 0.11192334443330765 Validation Loss: 0.7750335931777954\n",
      "Epoch 9905: Training Loss: 0.1122204785545667 Validation Loss: 0.7751545310020447\n",
      "Epoch 9906: Training Loss: 0.11199117203553517 Validation Loss: 0.7744355797767639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9907: Training Loss: 0.11233266691366832 Validation Loss: 0.7745788097381592\n",
      "Epoch 9908: Training Loss: 0.11185264835755031 Validation Loss: 0.7742666006088257\n",
      "Epoch 9909: Training Loss: 0.11153668661912282 Validation Loss: 0.7747313976287842\n",
      "Epoch 9910: Training Loss: 0.1118450661500295 Validation Loss: 0.7747489809989929\n",
      "Epoch 9911: Training Loss: 0.11183577527602513 Validation Loss: 0.7751033306121826\n",
      "Epoch 9912: Training Loss: 0.1121147448817889 Validation Loss: 0.7752863764762878\n",
      "Epoch 9913: Training Loss: 0.11179311573505402 Validation Loss: 0.7756487131118774\n",
      "Epoch 9914: Training Loss: 0.1120363970597585 Validation Loss: 0.7746997475624084\n",
      "Epoch 9915: Training Loss: 0.11217125505208969 Validation Loss: 0.7745954394340515\n",
      "Epoch 9916: Training Loss: 0.11275199304024379 Validation Loss: 0.7750989198684692\n",
      "Epoch 9917: Training Loss: 0.11181692282358806 Validation Loss: 0.7758508324623108\n",
      "Epoch 9918: Training Loss: 0.1121175189812978 Validation Loss: 0.7759792804718018\n",
      "Epoch 9919: Training Loss: 0.11183572560548782 Validation Loss: 0.7767195701599121\n",
      "Epoch 9920: Training Loss: 0.11202090978622437 Validation Loss: 0.7764925360679626\n",
      "Epoch 9921: Training Loss: 0.112052155037721 Validation Loss: 0.7760558724403381\n",
      "Epoch 9922: Training Loss: 0.11169207096099854 Validation Loss: 0.7753530144691467\n",
      "Epoch 9923: Training Loss: 0.11246629804372787 Validation Loss: 0.7749083042144775\n",
      "Epoch 9924: Training Loss: 0.11190965274969737 Validation Loss: 0.7749437689781189\n",
      "Epoch 9925: Training Loss: 0.11196918785572052 Validation Loss: 0.7751914858818054\n",
      "Epoch 9926: Training Loss: 0.11201377958059311 Validation Loss: 0.7750155925750732\n",
      "Epoch 9927: Training Loss: 0.11209759612878163 Validation Loss: 0.7750870585441589\n",
      "Epoch 9928: Training Loss: 0.11249751845995586 Validation Loss: 0.7745118737220764\n",
      "Epoch 9929: Training Loss: 0.11184653639793396 Validation Loss: 0.774269163608551\n",
      "Epoch 9930: Training Loss: 0.11218863974014918 Validation Loss: 0.7746083736419678\n",
      "Epoch 9931: Training Loss: 0.11210466921329498 Validation Loss: 0.7747960686683655\n",
      "Epoch 9932: Training Loss: 0.11191664884487788 Validation Loss: 0.7757412791252136\n",
      "Epoch 9933: Training Loss: 0.11236158758401871 Validation Loss: 0.7770704627037048\n",
      "Epoch 9934: Training Loss: 0.11205944667259853 Validation Loss: 0.7766579985618591\n",
      "Epoch 9935: Training Loss: 0.11267100274562836 Validation Loss: 0.7755270600318909\n",
      "Epoch 9936: Training Loss: 0.11225173374017079 Validation Loss: 0.7750355005264282\n",
      "Epoch 9937: Training Loss: 0.11175336440404256 Validation Loss: 0.7761728763580322\n",
      "Epoch 9938: Training Loss: 0.11208318422238032 Validation Loss: 0.7762485146522522\n",
      "Epoch 9939: Training Loss: 0.11169222990671794 Validation Loss: 0.7758551239967346\n",
      "Epoch 9940: Training Loss: 0.1118495340148608 Validation Loss: 0.7755358219146729\n",
      "Epoch 9941: Training Loss: 0.11191064119338989 Validation Loss: 0.7758700251579285\n",
      "Epoch 9942: Training Loss: 0.11182304223378499 Validation Loss: 0.7757869362831116\n",
      "Epoch 9943: Training Loss: 0.11229652166366577 Validation Loss: 0.7758649587631226\n",
      "Epoch 9944: Training Loss: 0.11180773129065831 Validation Loss: 0.7750048041343689\n",
      "Epoch 9945: Training Loss: 0.11177939673264821 Validation Loss: 0.7759196162223816\n",
      "Epoch 9946: Training Loss: 0.11160015066464742 Validation Loss: 0.7751849293708801\n",
      "Epoch 9947: Training Loss: 0.11161015679438908 Validation Loss: 0.7749767899513245\n",
      "Epoch 9948: Training Loss: 0.11184992889563243 Validation Loss: 0.7758163213729858\n",
      "Epoch 9949: Training Loss: 0.11160900195439656 Validation Loss: 0.7763716578483582\n",
      "Epoch 9950: Training Loss: 0.11187099665403366 Validation Loss: 0.7762981653213501\n",
      "Epoch 9951: Training Loss: 0.11175050089756648 Validation Loss: 0.7757546901702881\n",
      "Epoch 9952: Training Loss: 0.111622154712677 Validation Loss: 0.7756463885307312\n",
      "Epoch 9953: Training Loss: 0.111653005083402 Validation Loss: 0.7749144434928894\n",
      "Epoch 9954: Training Loss: 0.1119203915198644 Validation Loss: 0.7749008536338806\n",
      "Epoch 9955: Training Loss: 0.11191269755363464 Validation Loss: 0.7757753729820251\n",
      "Epoch 9956: Training Loss: 0.11151171724001567 Validation Loss: 0.7758912444114685\n",
      "Epoch 9957: Training Loss: 0.11151417096455891 Validation Loss: 0.7757008671760559\n",
      "Epoch 9958: Training Loss: 0.11244894812504451 Validation Loss: 0.7760155200958252\n",
      "Epoch 9959: Training Loss: 0.11159664889176686 Validation Loss: 0.7761304974555969\n",
      "Epoch 9960: Training Loss: 0.11171389122804005 Validation Loss: 0.7759243845939636\n",
      "Epoch 9961: Training Loss: 0.11125985284646352 Validation Loss: 0.7754208445549011\n",
      "Epoch 9962: Training Loss: 0.11169002950191498 Validation Loss: 0.7759090662002563\n",
      "Epoch 9963: Training Loss: 0.11180274933576584 Validation Loss: 0.7757900953292847\n",
      "Epoch 9964: Training Loss: 0.11145904411872228 Validation Loss: 0.7759147882461548\n",
      "Epoch 9965: Training Loss: 0.11224739501873653 Validation Loss: 0.7756955027580261\n",
      "Epoch 9966: Training Loss: 0.11157498011986415 Validation Loss: 0.7754312753677368\n",
      "Epoch 9967: Training Loss: 0.11150836199522018 Validation Loss: 0.7756277322769165\n",
      "Epoch 9968: Training Loss: 0.11153180648883183 Validation Loss: 0.7758352160453796\n",
      "Epoch 9969: Training Loss: 0.11146468172470729 Validation Loss: 0.7759844660758972\n",
      "Epoch 9970: Training Loss: 0.11173441509405772 Validation Loss: 0.7752125859260559\n",
      "Epoch 9971: Training Loss: 0.1119385485847791 Validation Loss: 0.7754467725753784\n",
      "Epoch 9972: Training Loss: 0.11140027393897374 Validation Loss: 0.7753307819366455\n",
      "Epoch 9973: Training Loss: 0.11160685122013092 Validation Loss: 0.7757139205932617\n",
      "Epoch 9974: Training Loss: 0.11144946018854777 Validation Loss: 0.7755290865898132\n",
      "Epoch 9975: Training Loss: 0.11187605063120525 Validation Loss: 0.7761443853378296\n",
      "Epoch 9976: Training Loss: 0.11165001740058263 Validation Loss: 0.7759074568748474\n",
      "Epoch 9977: Training Loss: 0.11136686305205028 Validation Loss: 0.7756270170211792\n",
      "Epoch 9978: Training Loss: 0.11166104674339294 Validation Loss: 0.7751771211624146\n",
      "Epoch 9979: Training Loss: 0.11129155506690343 Validation Loss: 0.7753829956054688\n",
      "Epoch 9980: Training Loss: 0.11127020666996638 Validation Loss: 0.7754306197166443\n",
      "Epoch 9981: Training Loss: 0.11241052796443303 Validation Loss: 0.7769168019294739\n",
      "Epoch 9982: Training Loss: 0.1119024579723676 Validation Loss: 0.7773424983024597\n",
      "Epoch 9983: Training Loss: 0.1113562136888504 Validation Loss: 0.7767543196678162\n",
      "Epoch 9984: Training Loss: 0.11119071890910466 Validation Loss: 0.7762174606323242\n",
      "Epoch 9985: Training Loss: 0.11201540132363637 Validation Loss: 0.7760974764823914\n",
      "Epoch 9986: Training Loss: 0.11186792701482773 Validation Loss: 0.7762670516967773\n",
      "Epoch 9987: Training Loss: 0.11174749086300532 Validation Loss: 0.7753262519836426\n",
      "Epoch 9988: Training Loss: 0.11148589352766673 Validation Loss: 0.7755303382873535\n",
      "Epoch 9989: Training Loss: 0.1114758054415385 Validation Loss: 0.775596022605896\n",
      "Epoch 9990: Training Loss: 0.11179858197768529 Validation Loss: 0.7767196297645569\n",
      "Epoch 9991: Training Loss: 0.1116046408812205 Validation Loss: 0.7764790058135986\n",
      "Epoch 9992: Training Loss: 0.11162807544072469 Validation Loss: 0.7759873867034912\n",
      "Epoch 9993: Training Loss: 0.11122924586137135 Validation Loss: 0.7754002809524536\n",
      "Epoch 9994: Training Loss: 0.11196066687504451 Validation Loss: 0.7749007940292358\n",
      "Epoch 9995: Training Loss: 0.11155187090237935 Validation Loss: 0.7753564119338989\n",
      "Epoch 9996: Training Loss: 0.1110265702009201 Validation Loss: 0.7756645083427429\n",
      "Epoch 9997: Training Loss: 0.11118611445029576 Validation Loss: 0.7759117484092712\n",
      "Epoch 9998: Training Loss: 0.11163520564635594 Validation Loss: 0.7761236429214478\n",
      "Epoch 9999: Training Loss: 0.11157858620087306 Validation Loss: 0.7766420841217041\n",
      "Epoch 10000: Training Loss: 0.1114044114947319 Validation Loss: 0.7765269875526428\n",
      "Epoch 10001: Training Loss: 0.11107155680656433 Validation Loss: 0.7763885259628296\n",
      "Epoch 10002: Training Loss: 0.11187904824813207 Validation Loss: 0.776017963886261\n",
      "Epoch 10003: Training Loss: 0.11134239286184311 Validation Loss: 0.7759272456169128\n",
      "Epoch 10004: Training Loss: 0.1112297351161639 Validation Loss: 0.7773538827896118\n",
      "Epoch 10005: Training Loss: 0.11111931751171748 Validation Loss: 0.7774313688278198\n",
      "Epoch 10006: Training Loss: 0.11145154138406117 Validation Loss: 0.7762362360954285\n",
      "Epoch 10007: Training Loss: 0.11141207069158554 Validation Loss: 0.7768972516059875\n",
      "Epoch 10008: Training Loss: 0.11217821637789409 Validation Loss: 0.7767395973205566\n",
      "Epoch 10009: Training Loss: 0.11096851030985515 Validation Loss: 0.7770809531211853\n",
      "Epoch 10010: Training Loss: 0.11121065666278203 Validation Loss: 0.7767768502235413\n",
      "Epoch 10011: Training Loss: 0.11114014933506648 Validation Loss: 0.7767210006713867\n",
      "Epoch 10012: Training Loss: 0.11109484980503719 Validation Loss: 0.7765432000160217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10013: Training Loss: 0.11177637428045273 Validation Loss: 0.7770284414291382\n",
      "Epoch 10014: Training Loss: 0.11130780975023906 Validation Loss: 0.776035726070404\n",
      "Epoch 10015: Training Loss: 0.11143153409163158 Validation Loss: 0.775730550289154\n",
      "Epoch 10016: Training Loss: 0.11108372112115224 Validation Loss: 0.7761635184288025\n",
      "Epoch 10017: Training Loss: 0.1112819289167722 Validation Loss: 0.7768117785453796\n",
      "Epoch 10018: Training Loss: 0.11112058411041896 Validation Loss: 0.7765091061592102\n",
      "Epoch 10019: Training Loss: 0.11102070659399033 Validation Loss: 0.7767553925514221\n",
      "Epoch 10020: Training Loss: 0.11111666758855183 Validation Loss: 0.7765807509422302\n",
      "Epoch 10021: Training Loss: 0.11184914658466975 Validation Loss: 0.7760806083679199\n",
      "Epoch 10022: Training Loss: 0.11129037290811539 Validation Loss: 0.7753521203994751\n",
      "Epoch 10023: Training Loss: 0.1115164781610171 Validation Loss: 0.7756531238555908\n",
      "Epoch 10024: Training Loss: 0.11105165133873622 Validation Loss: 0.7760602831840515\n",
      "Epoch 10025: Training Loss: 0.11111792922019958 Validation Loss: 0.776218831539154\n",
      "Epoch 10026: Training Loss: 0.11081302911043167 Validation Loss: 0.776406466960907\n",
      "Epoch 10027: Training Loss: 0.11121182888746262 Validation Loss: 0.7768948078155518\n",
      "Epoch 10028: Training Loss: 0.11125734945138295 Validation Loss: 0.777347981929779\n",
      "Epoch 10029: Training Loss: 0.1110849380493164 Validation Loss: 0.7766770124435425\n",
      "Epoch 10030: Training Loss: 0.11145050823688507 Validation Loss: 0.7768265008926392\n",
      "Epoch 10031: Training Loss: 0.11108789841334026 Validation Loss: 0.7762463688850403\n",
      "Epoch 10032: Training Loss: 0.11150592565536499 Validation Loss: 0.7765212059020996\n",
      "Epoch 10033: Training Loss: 0.11114819347858429 Validation Loss: 0.7766672968864441\n",
      "Epoch 10034: Training Loss: 0.111246091624101 Validation Loss: 0.7777283787727356\n",
      "Epoch 10035: Training Loss: 0.1112108180920283 Validation Loss: 0.7779046297073364\n",
      "Epoch 10036: Training Loss: 0.11141602446635564 Validation Loss: 0.7773397564888\n",
      "Epoch 10037: Training Loss: 0.11117624243100484 Validation Loss: 0.7769078612327576\n",
      "Epoch 10038: Training Loss: 0.11173469324906667 Validation Loss: 0.7761000394821167\n",
      "Epoch 10039: Training Loss: 0.1117071658372879 Validation Loss: 0.7756544947624207\n",
      "Epoch 10040: Training Loss: 0.11100190381209056 Validation Loss: 0.7765533924102783\n",
      "Epoch 10041: Training Loss: 0.11092173308134079 Validation Loss: 0.7767729759216309\n",
      "Epoch 10042: Training Loss: 0.11105300734440486 Validation Loss: 0.7774883508682251\n",
      "Epoch 10043: Training Loss: 0.11099434147278468 Validation Loss: 0.7769806385040283\n",
      "Epoch 10044: Training Loss: 0.11130383610725403 Validation Loss: 0.7768082022666931\n",
      "Epoch 10045: Training Loss: 0.111351877450943 Validation Loss: 0.776201069355011\n",
      "Epoch 10046: Training Loss: 0.1113561565677325 Validation Loss: 0.7755717635154724\n",
      "Epoch 10047: Training Loss: 0.1110149472951889 Validation Loss: 0.7769170999526978\n",
      "Epoch 10048: Training Loss: 0.11105582118034363 Validation Loss: 0.7777553796768188\n",
      "Epoch 10049: Training Loss: 0.111496701836586 Validation Loss: 0.7779454588890076\n",
      "Epoch 10050: Training Loss: 0.11081725855668385 Validation Loss: 0.7776920199394226\n",
      "Epoch 10051: Training Loss: 0.11091882983843486 Validation Loss: 0.7769186496734619\n",
      "Epoch 10052: Training Loss: 0.11119805773099263 Validation Loss: 0.7760419845581055\n",
      "Epoch 10053: Training Loss: 0.11127512902021408 Validation Loss: 0.7759917974472046\n",
      "Epoch 10054: Training Loss: 0.11128418644269307 Validation Loss: 0.7765592336654663\n",
      "Epoch 10055: Training Loss: 0.11068257441123326 Validation Loss: 0.7772216796875\n",
      "Epoch 10056: Training Loss: 0.11070605119069417 Validation Loss: 0.7775817513465881\n",
      "Epoch 10057: Training Loss: 0.11141632497310638 Validation Loss: 0.77685546875\n",
      "Epoch 10058: Training Loss: 0.11100249985853831 Validation Loss: 0.7766789197921753\n",
      "Epoch 10059: Training Loss: 0.11108216891686122 Validation Loss: 0.7770136594772339\n",
      "Epoch 10060: Training Loss: 0.11148248116175334 Validation Loss: 0.7774967551231384\n",
      "Epoch 10061: Training Loss: 0.11157771448294322 Validation Loss: 0.7771800756454468\n",
      "Epoch 10062: Training Loss: 0.11091602842013042 Validation Loss: 0.7770907282829285\n",
      "Epoch 10063: Training Loss: 0.11115257690350215 Validation Loss: 0.7767094969749451\n",
      "Epoch 10064: Training Loss: 0.110672727227211 Validation Loss: 0.7758877873420715\n",
      "Epoch 10065: Training Loss: 0.11082717031240463 Validation Loss: 0.7761149406433105\n",
      "Epoch 10066: Training Loss: 0.11096847057342529 Validation Loss: 0.7767635583877563\n",
      "Epoch 10067: Training Loss: 0.11107724159955978 Validation Loss: 0.777379035949707\n",
      "Epoch 10068: Training Loss: 0.11071084688107173 Validation Loss: 0.7771883010864258\n",
      "Epoch 10069: Training Loss: 0.11104397227366765 Validation Loss: 0.7768115401268005\n",
      "Epoch 10070: Training Loss: 0.11076383541027705 Validation Loss: 0.7766510248184204\n",
      "Epoch 10071: Training Loss: 0.11130924026171367 Validation Loss: 0.7773696184158325\n",
      "Epoch 10072: Training Loss: 0.11084522306919098 Validation Loss: 0.77691650390625\n",
      "Epoch 10073: Training Loss: 0.11087646335363388 Validation Loss: 0.7774983644485474\n",
      "Epoch 10074: Training Loss: 0.11073544124762218 Validation Loss: 0.7777777910232544\n",
      "Epoch 10075: Training Loss: 0.11114932596683502 Validation Loss: 0.7767075300216675\n",
      "Epoch 10076: Training Loss: 0.11100997030735016 Validation Loss: 0.776593804359436\n",
      "Epoch 10077: Training Loss: 0.11078169445196788 Validation Loss: 0.7771856784820557\n",
      "Epoch 10078: Training Loss: 0.11082831521828969 Validation Loss: 0.7765343189239502\n",
      "Epoch 10079: Training Loss: 0.11094296475251515 Validation Loss: 0.7771599888801575\n",
      "Epoch 10080: Training Loss: 0.11069287111361821 Validation Loss: 0.777550458908081\n",
      "Epoch 10081: Training Loss: 0.11092160890499751 Validation Loss: 0.7778263688087463\n",
      "Epoch 10082: Training Loss: 0.11069724957148235 Validation Loss: 0.7772788405418396\n",
      "Epoch 10083: Training Loss: 0.1108478307723999 Validation Loss: 0.7769638895988464\n",
      "Epoch 10084: Training Loss: 0.11090982953707378 Validation Loss: 0.7768715620040894\n",
      "Epoch 10085: Training Loss: 0.11061827838420868 Validation Loss: 0.7766544222831726\n",
      "Epoch 10086: Training Loss: 0.11060148229201634 Validation Loss: 0.7774326801300049\n",
      "Epoch 10087: Training Loss: 0.11041220277547836 Validation Loss: 0.7779401540756226\n",
      "Epoch 10088: Training Loss: 0.11053503553072612 Validation Loss: 0.7774219512939453\n",
      "Epoch 10089: Training Loss: 0.1105602855483691 Validation Loss: 0.7769989371299744\n",
      "Epoch 10090: Training Loss: 0.1111839363972346 Validation Loss: 0.7759028077125549\n",
      "Epoch 10091: Training Loss: 0.11106175432602565 Validation Loss: 0.7760556936264038\n",
      "Epoch 10092: Training Loss: 0.11051417390505473 Validation Loss: 0.7763469815254211\n",
      "Epoch 10093: Training Loss: 0.1106775055329005 Validation Loss: 0.7771279215812683\n",
      "Epoch 10094: Training Loss: 0.11066560198863347 Validation Loss: 0.7776641845703125\n",
      "Epoch 10095: Training Loss: 0.1105636606613795 Validation Loss: 0.7781723737716675\n",
      "Epoch 10096: Training Loss: 0.11104747156302135 Validation Loss: 0.7774449586868286\n",
      "Epoch 10097: Training Loss: 0.11084073781967163 Validation Loss: 0.7778985500335693\n",
      "Epoch 10098: Training Loss: 0.11064202338457108 Validation Loss: 0.777339518070221\n",
      "Epoch 10099: Training Loss: 0.1107069303592046 Validation Loss: 0.7776222229003906\n",
      "Epoch 10100: Training Loss: 0.1106348584095637 Validation Loss: 0.7780207395553589\n",
      "Epoch 10101: Training Loss: 0.11098543057839076 Validation Loss: 0.7773160934448242\n",
      "Epoch 10102: Training Loss: 0.11091383298238118 Validation Loss: 0.777198851108551\n",
      "Epoch 10103: Training Loss: 0.11051628738641739 Validation Loss: 0.77806156873703\n",
      "Epoch 10104: Training Loss: 0.11079472303390503 Validation Loss: 0.7772254943847656\n",
      "Epoch 10105: Training Loss: 0.11066542069117229 Validation Loss: 0.7776486277580261\n",
      "Epoch 10106: Training Loss: 0.11085119843482971 Validation Loss: 0.7782745361328125\n",
      "Epoch 10107: Training Loss: 0.11058160662651062 Validation Loss: 0.7774121165275574\n",
      "Epoch 10108: Training Loss: 0.1107818881670634 Validation Loss: 0.7778059244155884\n",
      "Epoch 10109: Training Loss: 0.11049230148394902 Validation Loss: 0.7773171067237854\n",
      "Epoch 10110: Training Loss: 0.11050113042195638 Validation Loss: 0.7770925760269165\n",
      "Epoch 10111: Training Loss: 0.1104932576417923 Validation Loss: 0.7768186330795288\n",
      "Epoch 10112: Training Loss: 0.11081226915121078 Validation Loss: 0.7768545746803284\n",
      "Epoch 10113: Training Loss: 0.11052825798590978 Validation Loss: 0.7770440578460693\n",
      "Epoch 10114: Training Loss: 0.11040043085813522 Validation Loss: 0.7769025564193726\n",
      "Epoch 10115: Training Loss: 0.11016021420558293 Validation Loss: 0.7769748568534851\n",
      "Epoch 10116: Training Loss: 0.11123246451218922 Validation Loss: 0.7774592638015747\n",
      "Epoch 10117: Training Loss: 0.11068496604760487 Validation Loss: 0.7781252861022949\n",
      "Epoch 10118: Training Loss: 0.11086467653512955 Validation Loss: 0.7779811024665833\n",
      "Epoch 10119: Training Loss: 0.11053881049156189 Validation Loss: 0.7775334715843201\n",
      "Epoch 10120: Training Loss: 0.11054643740256627 Validation Loss: 0.7783089876174927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10121: Training Loss: 0.11049572378396988 Validation Loss: 0.7778632044792175\n",
      "Epoch 10122: Training Loss: 0.11049416412909825 Validation Loss: 0.7780598998069763\n",
      "Epoch 10123: Training Loss: 0.11056724935770035 Validation Loss: 0.7781919836997986\n",
      "Epoch 10124: Training Loss: 0.11066071440776189 Validation Loss: 0.7772502899169922\n",
      "Epoch 10125: Training Loss: 0.11050409078598022 Validation Loss: 0.7775028944015503\n",
      "Epoch 10126: Training Loss: 0.11032635221878688 Validation Loss: 0.7772492170333862\n",
      "Epoch 10127: Training Loss: 0.11085980385541916 Validation Loss: 0.7779121994972229\n",
      "Epoch 10128: Training Loss: 0.11080676317214966 Validation Loss: 0.7776195406913757\n",
      "Epoch 10129: Training Loss: 0.1107815479238828 Validation Loss: 0.7771645188331604\n",
      "Epoch 10130: Training Loss: 0.11035941044489543 Validation Loss: 0.7773311138153076\n",
      "Epoch 10131: Training Loss: 0.11052124450604121 Validation Loss: 0.7779695987701416\n",
      "Epoch 10132: Training Loss: 0.1102360337972641 Validation Loss: 0.7781603932380676\n",
      "Epoch 10133: Training Loss: 0.11036934951941173 Validation Loss: 0.7778332233428955\n",
      "Epoch 10134: Training Loss: 0.11030769348144531 Validation Loss: 0.7779862284660339\n",
      "Epoch 10135: Training Loss: 0.10988556345303853 Validation Loss: 0.7774944305419922\n",
      "Epoch 10136: Training Loss: 0.110383373995622 Validation Loss: 0.7779973745346069\n",
      "Epoch 10137: Training Loss: 0.11047779271999995 Validation Loss: 0.7779128551483154\n",
      "Epoch 10138: Training Loss: 0.1102093756198883 Validation Loss: 0.7777506113052368\n",
      "Epoch 10139: Training Loss: 0.11045607676108678 Validation Loss: 0.7785657644271851\n",
      "Epoch 10140: Training Loss: 0.11031758785247803 Validation Loss: 0.7783757448196411\n",
      "Epoch 10141: Training Loss: 0.11022631575663884 Validation Loss: 0.7775665521621704\n",
      "Epoch 10142: Training Loss: 0.11088257779677708 Validation Loss: 0.7766614556312561\n",
      "Epoch 10143: Training Loss: 0.1102728620171547 Validation Loss: 0.7768863439559937\n",
      "Epoch 10144: Training Loss: 0.11021468291680019 Validation Loss: 0.7787424921989441\n",
      "Epoch 10145: Training Loss: 0.110106460750103 Validation Loss: 0.7787870168685913\n",
      "Epoch 10146: Training Loss: 0.11073819796244304 Validation Loss: 0.7782813906669617\n",
      "Epoch 10147: Training Loss: 0.11032199114561081 Validation Loss: 0.7779991030693054\n",
      "Epoch 10148: Training Loss: 0.10997509956359863 Validation Loss: 0.777334451675415\n",
      "Epoch 10149: Training Loss: 0.11025985330343246 Validation Loss: 0.77739018201828\n",
      "Epoch 10150: Training Loss: 0.10999061663945515 Validation Loss: 0.7779648900032043\n",
      "Epoch 10151: Training Loss: 0.11006419112284978 Validation Loss: 0.7776670455932617\n",
      "Epoch 10152: Training Loss: 0.11103286345799764 Validation Loss: 0.7768391966819763\n",
      "Epoch 10153: Training Loss: 0.11021922528743744 Validation Loss: 0.778188943862915\n",
      "Epoch 10154: Training Loss: 0.1099312777320544 Validation Loss: 0.7789730429649353\n",
      "Epoch 10155: Training Loss: 0.11024022847414017 Validation Loss: 0.7795146703720093\n",
      "Epoch 10156: Training Loss: 0.11035241931676865 Validation Loss: 0.7790637612342834\n",
      "Epoch 10157: Training Loss: 0.11017928272485733 Validation Loss: 0.7779932618141174\n",
      "Epoch 10158: Training Loss: 0.11015115181605022 Validation Loss: 0.7775683403015137\n",
      "Epoch 10159: Training Loss: 0.11008548488219579 Validation Loss: 0.7777877449989319\n",
      "Epoch 10160: Training Loss: 0.11027582734823227 Validation Loss: 0.7784080505371094\n",
      "Epoch 10161: Training Loss: 0.10987639178832372 Validation Loss: 0.7782191634178162\n",
      "Epoch 10162: Training Loss: 0.11021481702725093 Validation Loss: 0.777580201625824\n",
      "Epoch 10163: Training Loss: 0.1102299615740776 Validation Loss: 0.7774056196212769\n",
      "Epoch 10164: Training Loss: 0.11050977309544881 Validation Loss: 0.7779351472854614\n",
      "Epoch 10165: Training Loss: 0.11097547163565953 Validation Loss: 0.7778779864311218\n",
      "Epoch 10166: Training Loss: 0.11065661907196045 Validation Loss: 0.7779203057289124\n",
      "Epoch 10167: Training Loss: 0.10999289155006409 Validation Loss: 0.7778193950653076\n",
      "Epoch 10168: Training Loss: 0.10976160814364751 Validation Loss: 0.7781955003738403\n",
      "Epoch 10169: Training Loss: 0.109926238656044 Validation Loss: 0.7780287265777588\n",
      "Epoch 10170: Training Loss: 0.10966385155916214 Validation Loss: 0.777914822101593\n",
      "Epoch 10171: Training Loss: 0.11024471372365952 Validation Loss: 0.7781213521957397\n",
      "Epoch 10172: Training Loss: 0.10994728406270345 Validation Loss: 0.7784383893013\n",
      "Epoch 10173: Training Loss: 0.11037711799144745 Validation Loss: 0.7784005403518677\n",
      "Epoch 10174: Training Loss: 0.1104704091946284 Validation Loss: 0.7775929570198059\n",
      "Epoch 10175: Training Loss: 0.11023286233345668 Validation Loss: 0.7784397006034851\n",
      "Epoch 10176: Training Loss: 0.10987795144319534 Validation Loss: 0.7790386080741882\n",
      "Epoch 10177: Training Loss: 0.11011006931463878 Validation Loss: 0.7790986895561218\n",
      "Epoch 10178: Training Loss: 0.10996418943007787 Validation Loss: 0.7780546545982361\n",
      "Epoch 10179: Training Loss: 0.11003946761290233 Validation Loss: 0.7785987854003906\n",
      "Epoch 10180: Training Loss: 0.10986298819382985 Validation Loss: 0.7789120674133301\n",
      "Epoch 10181: Training Loss: 0.10996631284554799 Validation Loss: 0.7785300016403198\n",
      "Epoch 10182: Training Loss: 0.10983419418334961 Validation Loss: 0.7785762548446655\n",
      "Epoch 10183: Training Loss: 0.11000526944796245 Validation Loss: 0.7786171436309814\n",
      "Epoch 10184: Training Loss: 0.10992127160231273 Validation Loss: 0.7782130241394043\n",
      "Epoch 10185: Training Loss: 0.10988615204890569 Validation Loss: 0.7790027856826782\n",
      "Epoch 10186: Training Loss: 0.10987787942091624 Validation Loss: 0.7790387868881226\n",
      "Epoch 10187: Training Loss: 0.10927716145912807 Validation Loss: 0.7790814638137817\n",
      "Epoch 10188: Training Loss: 0.10952261586983998 Validation Loss: 0.7781367897987366\n",
      "Epoch 10189: Training Loss: 0.10986385742823283 Validation Loss: 0.7777783870697021\n",
      "Epoch 10190: Training Loss: 0.10988298306862514 Validation Loss: 0.7776022553443909\n",
      "Epoch 10191: Training Loss: 0.10978694260120392 Validation Loss: 0.77798992395401\n",
      "Epoch 10192: Training Loss: 0.11066558957099915 Validation Loss: 0.7784210443496704\n",
      "Epoch 10193: Training Loss: 0.11007393648227055 Validation Loss: 0.7782765626907349\n",
      "Epoch 10194: Training Loss: 0.1095179592569669 Validation Loss: 0.7783527970314026\n",
      "Epoch 10195: Training Loss: 0.11025704940160115 Validation Loss: 0.7785105109214783\n",
      "Epoch 10196: Training Loss: 0.10992274930079778 Validation Loss: 0.7788306474685669\n",
      "Epoch 10197: Training Loss: 0.11009798447291057 Validation Loss: 0.7789302468299866\n",
      "Epoch 10198: Training Loss: 0.10961314290761948 Validation Loss: 0.7796717286109924\n",
      "Epoch 10199: Training Loss: 0.10970820238192876 Validation Loss: 0.7800232768058777\n",
      "Epoch 10200: Training Loss: 0.10986877729495366 Validation Loss: 0.7801241278648376\n",
      "Epoch 10201: Training Loss: 0.11085998515288036 Validation Loss: 0.7803285121917725\n",
      "Epoch 10202: Training Loss: 0.10977420955896378 Validation Loss: 0.7789140343666077\n",
      "Epoch 10203: Training Loss: 0.11004461348056793 Validation Loss: 0.7778432369232178\n",
      "Epoch 10204: Training Loss: 0.10969579468170802 Validation Loss: 0.777564287185669\n",
      "Epoch 10205: Training Loss: 0.10958061615626018 Validation Loss: 0.778240978717804\n",
      "Epoch 10206: Training Loss: 0.11012536535660426 Validation Loss: 0.7785171866416931\n",
      "Epoch 10207: Training Loss: 0.11062991867462794 Validation Loss: 0.7790314555168152\n",
      "Epoch 10208: Training Loss: 0.10975609471400578 Validation Loss: 0.7794704437255859\n",
      "Epoch 10209: Training Loss: 0.10967925190925598 Validation Loss: 0.7794331908226013\n",
      "Epoch 10210: Training Loss: 0.10968991617361705 Validation Loss: 0.7793919444084167\n",
      "Epoch 10211: Training Loss: 0.11007930338382721 Validation Loss: 0.779492974281311\n",
      "Epoch 10212: Training Loss: 0.10975170383850734 Validation Loss: 0.7796269059181213\n",
      "Epoch 10213: Training Loss: 0.10985241333643596 Validation Loss: 0.7792624235153198\n",
      "Epoch 10214: Training Loss: 0.10982255389293034 Validation Loss: 0.7788351774215698\n",
      "Epoch 10215: Training Loss: 0.11031868308782578 Validation Loss: 0.7786458134651184\n",
      "Epoch 10216: Training Loss: 0.10963846991459529 Validation Loss: 0.7782755494117737\n",
      "Epoch 10217: Training Loss: 0.1093544860680898 Validation Loss: 0.778025209903717\n",
      "Epoch 10218: Training Loss: 0.1102602407336235 Validation Loss: 0.7789429426193237\n",
      "Epoch 10219: Training Loss: 0.10996827483177185 Validation Loss: 0.778201162815094\n",
      "Epoch 10220: Training Loss: 0.10948869834343593 Validation Loss: 0.7785633206367493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10221: Training Loss: 0.10958330829938252 Validation Loss: 0.778976559638977\n",
      "Epoch 10222: Training Loss: 0.10975805421670277 Validation Loss: 0.779059886932373\n",
      "Epoch 10223: Training Loss: 0.10958298295736313 Validation Loss: 0.7779983878135681\n",
      "Epoch 10224: Training Loss: 0.10970162351926167 Validation Loss: 0.7787576913833618\n",
      "Epoch 10225: Training Loss: 0.11009546617666881 Validation Loss: 0.7793386578559875\n",
      "Epoch 10226: Training Loss: 0.1100504423181216 Validation Loss: 0.7799012064933777\n",
      "Epoch 10227: Training Loss: 0.10969776908556621 Validation Loss: 0.7805311679840088\n",
      "Epoch 10228: Training Loss: 0.11010167002677917 Validation Loss: 0.7795442342758179\n",
      "Epoch 10229: Training Loss: 0.10931606590747833 Validation Loss: 0.7782527208328247\n",
      "Epoch 10230: Training Loss: 0.11098191142082214 Validation Loss: 0.7779979705810547\n",
      "Epoch 10231: Training Loss: 0.10953458646933238 Validation Loss: 0.7783066630363464\n",
      "Epoch 10232: Training Loss: 0.10951123386621475 Validation Loss: 0.7784322500228882\n",
      "Epoch 10233: Training Loss: 0.10908867667118709 Validation Loss: 0.779254674911499\n",
      "Epoch 10234: Training Loss: 0.10934094339609146 Validation Loss: 0.7796183228492737\n",
      "Epoch 10235: Training Loss: 0.10962076485157013 Validation Loss: 0.779423713684082\n",
      "Epoch 10236: Training Loss: 0.10941865543524425 Validation Loss: 0.7788529396057129\n",
      "Epoch 10237: Training Loss: 0.10949137061834335 Validation Loss: 0.7790828943252563\n",
      "Epoch 10238: Training Loss: 0.11027709643046062 Validation Loss: 0.7802940011024475\n",
      "Epoch 10239: Training Loss: 0.10943856587012608 Validation Loss: 0.7805710434913635\n",
      "Epoch 10240: Training Loss: 0.10949117441972096 Validation Loss: 0.7803758382797241\n",
      "Epoch 10241: Training Loss: 0.10983170568943024 Validation Loss: 0.7797145247459412\n",
      "Epoch 10242: Training Loss: 0.11012599617242813 Validation Loss: 0.7789912819862366\n",
      "Epoch 10243: Training Loss: 0.10950060188770294 Validation Loss: 0.7786905169487\n",
      "Epoch 10244: Training Loss: 0.1094528039296468 Validation Loss: 0.7793114185333252\n",
      "Epoch 10245: Training Loss: 0.10998694350322087 Validation Loss: 0.7790382504463196\n",
      "Epoch 10246: Training Loss: 0.10930916666984558 Validation Loss: 0.7792074084281921\n",
      "Epoch 10247: Training Loss: 0.10957040637731552 Validation Loss: 0.7797241806983948\n",
      "Epoch 10248: Training Loss: 0.10965229322512944 Validation Loss: 0.7790585160255432\n",
      "Epoch 10249: Training Loss: 0.10933773467938106 Validation Loss: 0.7791969776153564\n",
      "Epoch 10250: Training Loss: 0.10974132021268208 Validation Loss: 0.7788006067276001\n",
      "Epoch 10251: Training Loss: 0.10980487118164699 Validation Loss: 0.7790534496307373\n",
      "Epoch 10252: Training Loss: 0.10951274633407593 Validation Loss: 0.7802906632423401\n",
      "Epoch 10253: Training Loss: 0.11012224107980728 Validation Loss: 0.7808166146278381\n",
      "Epoch 10254: Training Loss: 0.11021166046460469 Validation Loss: 0.7802199721336365\n",
      "Epoch 10255: Training Loss: 0.11024705072244008 Validation Loss: 0.7793876528739929\n",
      "Epoch 10256: Training Loss: 0.10948581248521805 Validation Loss: 0.7783814072608948\n",
      "Epoch 10257: Training Loss: 0.109125517308712 Validation Loss: 0.7771308422088623\n",
      "Epoch 10258: Training Loss: 0.10972038408120473 Validation Loss: 0.7773429155349731\n",
      "Epoch 10259: Training Loss: 0.10966520756483078 Validation Loss: 0.7781341075897217\n",
      "Epoch 10260: Training Loss: 0.10941311468680699 Validation Loss: 0.7789162993431091\n",
      "Epoch 10261: Training Loss: 0.10961032162110011 Validation Loss: 0.7798558473587036\n",
      "Epoch 10262: Training Loss: 0.1095790018637975 Validation Loss: 0.779420793056488\n",
      "Epoch 10263: Training Loss: 0.10970013588666916 Validation Loss: 0.7799035310745239\n",
      "Epoch 10264: Training Loss: 0.10924102365970612 Validation Loss: 0.7803318500518799\n",
      "Epoch 10265: Training Loss: 0.10936358819405238 Validation Loss: 0.7797678112983704\n",
      "Epoch 10266: Training Loss: 0.10933322956164677 Validation Loss: 0.7800915241241455\n",
      "Epoch 10267: Training Loss: 0.10928381731112798 Validation Loss: 0.7797547578811646\n",
      "Epoch 10268: Training Loss: 0.10974349578221639 Validation Loss: 0.7798669934272766\n",
      "Epoch 10269: Training Loss: 0.10968864212433498 Validation Loss: 0.7795169353485107\n",
      "Epoch 10270: Training Loss: 0.10919893532991409 Validation Loss: 0.7794564962387085\n",
      "Epoch 10271: Training Loss: 0.10933812707662582 Validation Loss: 0.7795186638832092\n",
      "Epoch 10272: Training Loss: 0.10938223451375961 Validation Loss: 0.7799344062805176\n",
      "Epoch 10273: Training Loss: 0.10916407157977422 Validation Loss: 0.7797913551330566\n",
      "Epoch 10274: Training Loss: 0.10948290427525838 Validation Loss: 0.7795547246932983\n",
      "Epoch 10275: Training Loss: 0.11027994255224864 Validation Loss: 0.7795042395591736\n",
      "Epoch 10276: Training Loss: 0.10915873696406682 Validation Loss: 0.7794926166534424\n",
      "Epoch 10277: Training Loss: 0.10979883124430974 Validation Loss: 0.7797232270240784\n",
      "Epoch 10278: Training Loss: 0.10928993920485179 Validation Loss: 0.7797994017601013\n",
      "Epoch 10279: Training Loss: 0.1093611791729927 Validation Loss: 0.7794195413589478\n",
      "Epoch 10280: Training Loss: 0.10909714053074519 Validation Loss: 0.7791296243667603\n",
      "Epoch 10281: Training Loss: 0.1092933863401413 Validation Loss: 0.7792069315910339\n",
      "Epoch 10282: Training Loss: 0.10913872222105662 Validation Loss: 0.7793974280357361\n",
      "Epoch 10283: Training Loss: 0.11016013224919637 Validation Loss: 0.7797644734382629\n",
      "Epoch 10284: Training Loss: 0.10915587345759074 Validation Loss: 0.7797779440879822\n",
      "Epoch 10285: Training Loss: 0.10975527763366699 Validation Loss: 0.779340386390686\n",
      "Epoch 10286: Training Loss: 0.10920737435420354 Validation Loss: 0.7790510058403015\n",
      "Epoch 10287: Training Loss: 0.10930652171373367 Validation Loss: 0.7785094380378723\n",
      "Epoch 10288: Training Loss: 0.10936099042495091 Validation Loss: 0.7804540991783142\n",
      "Epoch 10289: Training Loss: 0.10935188829898834 Validation Loss: 0.7807889580726624\n",
      "Epoch 10290: Training Loss: 0.10913169632355373 Validation Loss: 0.7807095646858215\n",
      "Epoch 10291: Training Loss: 0.10914773245652516 Validation Loss: 0.7805756330490112\n",
      "Epoch 10292: Training Loss: 0.10925094286600749 Validation Loss: 0.7794121503829956\n",
      "Epoch 10293: Training Loss: 0.1092098280787468 Validation Loss: 0.7795553803443909\n",
      "Epoch 10294: Training Loss: 0.10919408748547237 Validation Loss: 0.7802863717079163\n",
      "Epoch 10295: Training Loss: 0.10896080732345581 Validation Loss: 0.7806746363639832\n",
      "Epoch 10296: Training Loss: 0.10919770846764247 Validation Loss: 0.779072105884552\n",
      "Epoch 10297: Training Loss: 0.10916364192962646 Validation Loss: 0.7787432670593262\n",
      "Epoch 10298: Training Loss: 0.10922905306021373 Validation Loss: 0.7783296704292297\n",
      "Epoch 10299: Training Loss: 0.10921085129181544 Validation Loss: 0.7796075344085693\n",
      "Epoch 10300: Training Loss: 0.10918773214022319 Validation Loss: 0.7800782918930054\n",
      "Epoch 10301: Training Loss: 0.10944722592830658 Validation Loss: 0.7797079086303711\n",
      "Epoch 10302: Training Loss: 0.10908163338899612 Validation Loss: 0.7801053524017334\n",
      "Epoch 10303: Training Loss: 0.10929623742898305 Validation Loss: 0.7806031107902527\n",
      "Epoch 10304: Training Loss: 0.1092633381485939 Validation Loss: 0.7806456089019775\n",
      "Epoch 10305: Training Loss: 0.10915149748325348 Validation Loss: 0.7805103659629822\n",
      "Epoch 10306: Training Loss: 0.10891110201676686 Validation Loss: 0.7804336547851562\n",
      "Epoch 10307: Training Loss: 0.10860939820607503 Validation Loss: 0.7806175351142883\n",
      "Epoch 10308: Training Loss: 0.10923296213150024 Validation Loss: 0.7802217602729797\n",
      "Epoch 10309: Training Loss: 0.10918806493282318 Validation Loss: 0.7799379229545593\n",
      "Epoch 10310: Training Loss: 0.10904028763373692 Validation Loss: 0.779474139213562\n",
      "Epoch 10311: Training Loss: 0.10909561564524968 Validation Loss: 0.7796815037727356\n",
      "Epoch 10312: Training Loss: 0.10896394650141399 Validation Loss: 0.7800540328025818\n",
      "Epoch 10313: Training Loss: 0.10896654178698857 Validation Loss: 0.7798197269439697\n",
      "Epoch 10314: Training Loss: 0.10897442450126012 Validation Loss: 0.7803162336349487\n",
      "Epoch 10315: Training Loss: 0.10905330628156662 Validation Loss: 0.7802850008010864\n",
      "Epoch 10316: Training Loss: 0.10890347013870876 Validation Loss: 0.7794654965400696\n",
      "Epoch 10317: Training Loss: 0.10907356937726338 Validation Loss: 0.7794578671455383\n",
      "Epoch 10318: Training Loss: 0.10922984033823013 Validation Loss: 0.7792491316795349\n",
      "Epoch 10319: Training Loss: 0.10917801409959793 Validation Loss: 0.7791635990142822\n",
      "Epoch 10320: Training Loss: 0.10900476326545079 Validation Loss: 0.7795692086219788\n",
      "Epoch 10321: Training Loss: 0.10917975505193074 Validation Loss: 0.7795169949531555\n",
      "Epoch 10322: Training Loss: 0.10901972154776256 Validation Loss: 0.7794126868247986\n",
      "Epoch 10323: Training Loss: 0.10884397725264232 Validation Loss: 0.7802401781082153\n",
      "Epoch 10324: Training Loss: 0.10887633264064789 Validation Loss: 0.7805228233337402\n",
      "Epoch 10325: Training Loss: 0.10892200469970703 Validation Loss: 0.7804797887802124\n",
      "Epoch 10326: Training Loss: 0.1089198167125384 Validation Loss: 0.7814319729804993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10327: Training Loss: 0.10869504759709041 Validation Loss: 0.7810176014900208\n",
      "Epoch 10328: Training Loss: 0.10882063210010529 Validation Loss: 0.7809104919433594\n",
      "Epoch 10329: Training Loss: 0.10904077688852946 Validation Loss: 0.7803276777267456\n",
      "Epoch 10330: Training Loss: 0.10890312989552815 Validation Loss: 0.7795758843421936\n",
      "Epoch 10331: Training Loss: 0.10901406904061635 Validation Loss: 0.7802436947822571\n",
      "Epoch 10332: Training Loss: 0.10887319097916286 Validation Loss: 0.7800279259681702\n",
      "Epoch 10333: Training Loss: 0.10887291530768077 Validation Loss: 0.7793845534324646\n",
      "Epoch 10334: Training Loss: 0.10904233405987422 Validation Loss: 0.7801061868667603\n",
      "Epoch 10335: Training Loss: 0.1087996835509936 Validation Loss: 0.7814721465110779\n",
      "Epoch 10336: Training Loss: 0.10876298447450002 Validation Loss: 0.7809168100357056\n",
      "Epoch 10337: Training Loss: 0.10917355120182037 Validation Loss: 0.7812914848327637\n",
      "Epoch 10338: Training Loss: 0.1085122674703598 Validation Loss: 0.7809953689575195\n",
      "Epoch 10339: Training Loss: 0.10892741630474727 Validation Loss: 0.7802797555923462\n",
      "Epoch 10340: Training Loss: 0.10886071125666301 Validation Loss: 0.7799715995788574\n",
      "Epoch 10341: Training Loss: 0.1099102720618248 Validation Loss: 0.7796603441238403\n",
      "Epoch 10342: Training Loss: 0.10885490228732426 Validation Loss: 0.7793849110603333\n",
      "Epoch 10343: Training Loss: 0.10895432035128276 Validation Loss: 0.7802103161811829\n",
      "Epoch 10344: Training Loss: 0.10878687103589375 Validation Loss: 0.7807846069335938\n",
      "Epoch 10345: Training Loss: 0.10883761942386627 Validation Loss: 0.7807205319404602\n",
      "Epoch 10346: Training Loss: 0.10875971615314484 Validation Loss: 0.7804442644119263\n",
      "Epoch 10347: Training Loss: 0.10861160357793172 Validation Loss: 0.780289351940155\n",
      "Epoch 10348: Training Loss: 0.1093386635184288 Validation Loss: 0.7800441980361938\n",
      "Epoch 10349: Training Loss: 0.10865982621908188 Validation Loss: 0.7805634140968323\n",
      "Epoch 10350: Training Loss: 0.10883537431557973 Validation Loss: 0.7811415195465088\n",
      "Epoch 10351: Training Loss: 0.10871577511231105 Validation Loss: 0.7809122800827026\n",
      "Epoch 10352: Training Loss: 0.10883444796005885 Validation Loss: 0.7807561755180359\n",
      "Epoch 10353: Training Loss: 0.1092606857419014 Validation Loss: 0.780152440071106\n",
      "Epoch 10354: Training Loss: 0.10849654922882716 Validation Loss: 0.7799516916275024\n",
      "Epoch 10355: Training Loss: 0.10868580142656963 Validation Loss: 0.7803534269332886\n",
      "Epoch 10356: Training Loss: 0.10864963134129842 Validation Loss: 0.7801453471183777\n",
      "Epoch 10357: Training Loss: 0.10875080525875092 Validation Loss: 0.780750572681427\n",
      "Epoch 10358: Training Loss: 0.10997932900985082 Validation Loss: 0.7814772129058838\n",
      "Epoch 10359: Training Loss: 0.10896451274553935 Validation Loss: 0.7816759347915649\n",
      "Epoch 10360: Training Loss: 0.10877860089143117 Validation Loss: 0.7809582948684692\n",
      "Epoch 10361: Training Loss: 0.10855218023061752 Validation Loss: 0.7806075811386108\n",
      "Epoch 10362: Training Loss: 0.10883088409900665 Validation Loss: 0.7805238366127014\n",
      "Epoch 10363: Training Loss: 0.1087586482365926 Validation Loss: 0.7805687189102173\n",
      "Epoch 10364: Training Loss: 0.10864398131767909 Validation Loss: 0.7806844711303711\n",
      "Epoch 10365: Training Loss: 0.10877195000648499 Validation Loss: 0.7809420824050903\n",
      "Epoch 10366: Training Loss: 0.10922652979691823 Validation Loss: 0.781392514705658\n",
      "Epoch 10367: Training Loss: 0.10867871344089508 Validation Loss: 0.7807060480117798\n",
      "Epoch 10368: Training Loss: 0.10857171565294266 Validation Loss: 0.7801279425621033\n",
      "Epoch 10369: Training Loss: 0.10852114111185074 Validation Loss: 0.7804715633392334\n",
      "Epoch 10370: Training Loss: 0.10908315082391103 Validation Loss: 0.7801028490066528\n",
      "Epoch 10371: Training Loss: 0.10858090718587239 Validation Loss: 0.7800750732421875\n",
      "Epoch 10372: Training Loss: 0.10857052852710088 Validation Loss: 0.780276358127594\n",
      "Epoch 10373: Training Loss: 0.1086005965868632 Validation Loss: 0.7811551690101624\n",
      "Epoch 10374: Training Loss: 0.1094837412238121 Validation Loss: 0.7811642289161682\n",
      "Epoch 10375: Training Loss: 0.10904166847467422 Validation Loss: 0.7819076776504517\n",
      "Epoch 10376: Training Loss: 0.10852695008118947 Validation Loss: 0.7819189429283142\n",
      "Epoch 10377: Training Loss: 0.10867429027954738 Validation Loss: 0.7817633152008057\n",
      "Epoch 10378: Training Loss: 0.10850240290164948 Validation Loss: 0.7812129259109497\n",
      "Epoch 10379: Training Loss: 0.10848838835954666 Validation Loss: 0.781098484992981\n",
      "Epoch 10380: Training Loss: 0.10867965469757716 Validation Loss: 0.7808442711830139\n",
      "Epoch 10381: Training Loss: 0.10845822344223659 Validation Loss: 0.7808508276939392\n",
      "Epoch 10382: Training Loss: 0.10767214745283127 Validation Loss: 0.7813474535942078\n",
      "Epoch 10383: Training Loss: 0.10840386400620143 Validation Loss: 0.7812485098838806\n",
      "Epoch 10384: Training Loss: 0.10885887096325557 Validation Loss: 0.7823780179023743\n",
      "Epoch 10385: Training Loss: 0.10854236284891765 Validation Loss: 0.7815110683441162\n",
      "Epoch 10386: Training Loss: 0.10829840848843257 Validation Loss: 0.7803431749343872\n",
      "Epoch 10387: Training Loss: 0.10821085423231125 Validation Loss: 0.7800639271736145\n",
      "Epoch 10388: Training Loss: 0.10867391526699066 Validation Loss: 0.7794880270957947\n",
      "Epoch 10389: Training Loss: 0.10848037401835124 Validation Loss: 0.7799942493438721\n",
      "Epoch 10390: Training Loss: 0.10860226054986317 Validation Loss: 0.7803354859352112\n",
      "Epoch 10391: Training Loss: 0.10868729650974274 Validation Loss: 0.7819392085075378\n",
      "Epoch 10392: Training Loss: 0.10875849674145381 Validation Loss: 0.782260537147522\n",
      "Epoch 10393: Training Loss: 0.10854104657967885 Validation Loss: 0.7820208668708801\n",
      "Epoch 10394: Training Loss: 0.10833298414945602 Validation Loss: 0.7813114523887634\n",
      "Epoch 10395: Training Loss: 0.108390673995018 Validation Loss: 0.7804790139198303\n",
      "Epoch 10396: Training Loss: 0.10851428161064784 Validation Loss: 0.7799414396286011\n",
      "Epoch 10397: Training Loss: 0.10864762713511784 Validation Loss: 0.7800626754760742\n",
      "Epoch 10398: Training Loss: 0.10837322721878688 Validation Loss: 0.7805666923522949\n",
      "Epoch 10399: Training Loss: 0.10830959429343541 Validation Loss: 0.781578004360199\n",
      "Epoch 10400: Training Loss: 0.10848365227381389 Validation Loss: 0.7822071313858032\n",
      "Epoch 10401: Training Loss: 0.10875742634137471 Validation Loss: 0.7824774980545044\n",
      "Epoch 10402: Training Loss: 0.1082638328274091 Validation Loss: 0.7821180820465088\n",
      "Epoch 10403: Training Loss: 0.10843874017397563 Validation Loss: 0.7806819677352905\n",
      "Epoch 10404: Training Loss: 0.10858175903558731 Validation Loss: 0.7808337211608887\n",
      "Epoch 10405: Training Loss: 0.10867030173540115 Validation Loss: 0.7805175185203552\n",
      "Epoch 10406: Training Loss: 0.10850012550751369 Validation Loss: 0.7799515724182129\n",
      "Epoch 10407: Training Loss: 0.10866962373256683 Validation Loss: 0.7797317504882812\n",
      "Epoch 10408: Training Loss: 0.10829148441553116 Validation Loss: 0.7809819579124451\n",
      "Epoch 10409: Training Loss: 0.10835396995147069 Validation Loss: 0.7818734049797058\n",
      "Epoch 10410: Training Loss: 0.10888358702262242 Validation Loss: 0.7818018794059753\n",
      "Epoch 10411: Training Loss: 0.1085844337940216 Validation Loss: 0.7819361686706543\n",
      "Epoch 10412: Training Loss: 0.10840656856695811 Validation Loss: 0.7818351984024048\n",
      "Epoch 10413: Training Loss: 0.10835186143716176 Validation Loss: 0.7823306918144226\n",
      "Epoch 10414: Training Loss: 0.10829427093267441 Validation Loss: 0.782612144947052\n",
      "Epoch 10415: Training Loss: 0.10838072498639424 Validation Loss: 0.7820001244544983\n",
      "Epoch 10416: Training Loss: 0.10858000814914703 Validation Loss: 0.7809793949127197\n",
      "Epoch 10417: Training Loss: 0.10845711330572765 Validation Loss: 0.7812337279319763\n",
      "Epoch 10418: Training Loss: 0.10793298482894897 Validation Loss: 0.7816478610038757\n",
      "Epoch 10419: Training Loss: 0.10936535398165385 Validation Loss: 0.7820682525634766\n",
      "Epoch 10420: Training Loss: 0.10818115125099818 Validation Loss: 0.7821139097213745\n",
      "Epoch 10421: Training Loss: 0.1081839178999265 Validation Loss: 0.7816653251647949\n",
      "Epoch 10422: Training Loss: 0.10822666933139165 Validation Loss: 0.7813655138015747\n",
      "Epoch 10423: Training Loss: 0.10831701258818309 Validation Loss: 0.7808594703674316\n",
      "Epoch 10424: Training Loss: 0.10803601890802383 Validation Loss: 0.7812662124633789\n",
      "Epoch 10425: Training Loss: 0.1083838219443957 Validation Loss: 0.781308114528656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10426: Training Loss: 0.1079948494831721 Validation Loss: 0.781652569770813\n",
      "Epoch 10427: Training Loss: 0.10898178815841675 Validation Loss: 0.7822597622871399\n",
      "Epoch 10428: Training Loss: 0.10809017966190974 Validation Loss: 0.7821770310401917\n",
      "Epoch 10429: Training Loss: 0.10849066078662872 Validation Loss: 0.7822190523147583\n",
      "Epoch 10430: Training Loss: 0.1083379586537679 Validation Loss: 0.7812051177024841\n",
      "Epoch 10431: Training Loss: 0.10831649601459503 Validation Loss: 0.7812247276306152\n",
      "Epoch 10432: Training Loss: 0.10834236939748128 Validation Loss: 0.781990647315979\n",
      "Epoch 10433: Training Loss: 0.10843502978483836 Validation Loss: 0.7815560698509216\n",
      "Epoch 10434: Training Loss: 0.10836207369963329 Validation Loss: 0.7823014259338379\n",
      "Epoch 10435: Training Loss: 0.10814902931451797 Validation Loss: 0.7823452353477478\n",
      "Epoch 10436: Training Loss: 0.10838237901528676 Validation Loss: 0.7822251915931702\n",
      "Epoch 10437: Training Loss: 0.10820458332697551 Validation Loss: 0.7813633680343628\n",
      "Epoch 10438: Training Loss: 0.10814425100882848 Validation Loss: 0.7814673781394958\n",
      "Epoch 10439: Training Loss: 0.10831248263518016 Validation Loss: 0.7810109257698059\n",
      "Epoch 10440: Training Loss: 0.10819907734791438 Validation Loss: 0.781998336315155\n",
      "Epoch 10441: Training Loss: 0.1081114262342453 Validation Loss: 0.7821844816207886\n",
      "Epoch 10442: Training Loss: 0.10819250841935475 Validation Loss: 0.7821381688117981\n",
      "Epoch 10443: Training Loss: 0.10841578741868337 Validation Loss: 0.7819531559944153\n",
      "Epoch 10444: Training Loss: 0.10787393897771835 Validation Loss: 0.7812768816947937\n",
      "Epoch 10445: Training Loss: 0.10787465671698253 Validation Loss: 0.7804758548736572\n",
      "Epoch 10446: Training Loss: 0.10882054020961125 Validation Loss: 0.7806477546691895\n",
      "Epoch 10447: Training Loss: 0.10848910858233769 Validation Loss: 0.7810555100440979\n",
      "Epoch 10448: Training Loss: 0.10820847749710083 Validation Loss: 0.7817895412445068\n",
      "Epoch 10449: Training Loss: 0.10810620834430058 Validation Loss: 0.7824252843856812\n",
      "Epoch 10450: Training Loss: 0.10807692011197408 Validation Loss: 0.7830482721328735\n",
      "Epoch 10451: Training Loss: 0.1080842191974322 Validation Loss: 0.7824141979217529\n",
      "Epoch 10452: Training Loss: 0.10851133366425832 Validation Loss: 0.7823628783226013\n",
      "Epoch 10453: Training Loss: 0.10796062151590984 Validation Loss: 0.7817088961601257\n",
      "Epoch 10454: Training Loss: 0.10802880426247914 Validation Loss: 0.7815864086151123\n",
      "Epoch 10455: Training Loss: 0.10784588257471721 Validation Loss: 0.782086968421936\n",
      "Epoch 10456: Training Loss: 0.10815093666315079 Validation Loss: 0.7822871804237366\n",
      "Epoch 10457: Training Loss: 0.10800180335839589 Validation Loss: 0.7825114727020264\n",
      "Epoch 10458: Training Loss: 0.10858888427416484 Validation Loss: 0.7825624942779541\n",
      "Epoch 10459: Training Loss: 0.10799330721298854 Validation Loss: 0.7818319201469421\n",
      "Epoch 10460: Training Loss: 0.10833222667376201 Validation Loss: 0.7814324498176575\n",
      "Epoch 10461: Training Loss: 0.10803728550672531 Validation Loss: 0.7815675139427185\n",
      "Epoch 10462: Training Loss: 0.10798656940460205 Validation Loss: 0.7825865745544434\n",
      "Epoch 10463: Training Loss: 0.10806383192539215 Validation Loss: 0.7829074263572693\n",
      "Epoch 10464: Training Loss: 0.10942197342713673 Validation Loss: 0.7821154594421387\n",
      "Epoch 10465: Training Loss: 0.1084290196498235 Validation Loss: 0.7810958623886108\n",
      "Epoch 10466: Training Loss: 0.10792599866787593 Validation Loss: 0.7809887528419495\n",
      "Epoch 10467: Training Loss: 0.10790806512037913 Validation Loss: 0.7811347246170044\n",
      "Epoch 10468: Training Loss: 0.10781032343705495 Validation Loss: 0.7810980081558228\n",
      "Epoch 10469: Training Loss: 0.108466237783432 Validation Loss: 0.7818290591239929\n",
      "Epoch 10470: Training Loss: 0.10815264532963435 Validation Loss: 0.782633900642395\n",
      "Epoch 10471: Training Loss: 0.10788274804751079 Validation Loss: 0.7825761437416077\n",
      "Epoch 10472: Training Loss: 0.10774119695027669 Validation Loss: 0.782396137714386\n",
      "Epoch 10473: Training Loss: 0.10790596157312393 Validation Loss: 0.7816432118415833\n",
      "Epoch 10474: Training Loss: 0.10796542465686798 Validation Loss: 0.78178471326828\n",
      "Epoch 10475: Training Loss: 0.10789656142393748 Validation Loss: 0.7817910313606262\n",
      "Epoch 10476: Training Loss: 0.10861937453349431 Validation Loss: 0.7818943858146667\n",
      "Epoch 10477: Training Loss: 0.10787447293599446 Validation Loss: 0.7810777425765991\n",
      "Epoch 10478: Training Loss: 0.10829533139864604 Validation Loss: 0.7816691398620605\n",
      "Epoch 10479: Training Loss: 0.10760102917750676 Validation Loss: 0.7823551893234253\n",
      "Epoch 10480: Training Loss: 0.1082180639108022 Validation Loss: 0.7827544808387756\n",
      "Epoch 10481: Training Loss: 0.10819285362958908 Validation Loss: 0.781710684299469\n",
      "Epoch 10482: Training Loss: 0.1076609839995702 Validation Loss: 0.7819461822509766\n",
      "Epoch 10483: Training Loss: 0.10770575950543086 Validation Loss: 0.781171441078186\n",
      "Epoch 10484: Training Loss: 0.10789963603019714 Validation Loss: 0.7814183235168457\n",
      "Epoch 10485: Training Loss: 0.10832386960585912 Validation Loss: 0.7832966446876526\n",
      "Epoch 10486: Training Loss: 0.10766155769427617 Validation Loss: 0.7831754684448242\n",
      "Epoch 10487: Training Loss: 0.10800737639268239 Validation Loss: 0.78367680311203\n",
      "Epoch 10488: Training Loss: 0.1079053282737732 Validation Loss: 0.7819851636886597\n",
      "Epoch 10489: Training Loss: 0.10812635471423467 Validation Loss: 0.7823072671890259\n",
      "Epoch 10490: Training Loss: 0.10794193049271901 Validation Loss: 0.7817395329475403\n",
      "Epoch 10491: Training Loss: 0.10810487220684688 Validation Loss: 0.7817423939704895\n",
      "Epoch 10492: Training Loss: 0.10785876462856929 Validation Loss: 0.7820416688919067\n",
      "Epoch 10493: Training Loss: 0.10791486998399098 Validation Loss: 0.783195436000824\n",
      "Epoch 10494: Training Loss: 0.10899407664934795 Validation Loss: 0.7834557294845581\n",
      "Epoch 10495: Training Loss: 0.10807185123364131 Validation Loss: 0.7827557921409607\n",
      "Epoch 10496: Training Loss: 0.10819211850563686 Validation Loss: 0.7817376255989075\n",
      "Epoch 10497: Training Loss: 0.10776848594347636 Validation Loss: 0.7816173434257507\n",
      "Epoch 10498: Training Loss: 0.10774070769548416 Validation Loss: 0.7820748686790466\n",
      "Epoch 10499: Training Loss: 0.1076505680878957 Validation Loss: 0.7830096483230591\n",
      "Epoch 10500: Training Loss: 0.10824523866176605 Validation Loss: 0.7830901145935059\n",
      "Epoch 10501: Training Loss: 0.10792605578899384 Validation Loss: 0.7823859453201294\n",
      "Epoch 10502: Training Loss: 0.10767929255962372 Validation Loss: 0.7833731770515442\n",
      "Epoch 10503: Training Loss: 0.10790551205476125 Validation Loss: 0.7827668190002441\n",
      "Epoch 10504: Training Loss: 0.10739425321420033 Validation Loss: 0.7827387452125549\n",
      "Epoch 10505: Training Loss: 0.10777220378319423 Validation Loss: 0.7823165059089661\n",
      "Epoch 10506: Training Loss: 0.10769185175498326 Validation Loss: 0.7813374400138855\n",
      "Epoch 10507: Training Loss: 0.10770839204390843 Validation Loss: 0.7818313837051392\n",
      "Epoch 10508: Training Loss: 0.10765500366687775 Validation Loss: 0.782330334186554\n",
      "Epoch 10509: Training Loss: 0.10771255940198898 Validation Loss: 0.7819947004318237\n",
      "Epoch 10510: Training Loss: 0.10837950309117635 Validation Loss: 0.7822422981262207\n",
      "Epoch 10511: Training Loss: 0.10802526275316875 Validation Loss: 0.7823779582977295\n",
      "Epoch 10512: Training Loss: 0.10820676138003667 Validation Loss: 0.7826808094978333\n",
      "Epoch 10513: Training Loss: 0.10789712021748225 Validation Loss: 0.782339334487915\n",
      "Epoch 10514: Training Loss: 0.10727990915377934 Validation Loss: 0.7824075818061829\n",
      "Epoch 10515: Training Loss: 0.1075680082043012 Validation Loss: 0.7824158072471619\n",
      "Epoch 10516: Training Loss: 0.10766762991746266 Validation Loss: 0.7828639149665833\n",
      "Epoch 10517: Training Loss: 0.10772435367107391 Validation Loss: 0.7832244634628296\n",
      "Epoch 10518: Training Loss: 0.10769159098466237 Validation Loss: 0.7828870415687561\n",
      "Epoch 10519: Training Loss: 0.10785287866989772 Validation Loss: 0.7826181054115295\n",
      "Epoch 10520: Training Loss: 0.1075664833188057 Validation Loss: 0.7831262350082397\n",
      "Epoch 10521: Training Loss: 0.10752524683872859 Validation Loss: 0.7835444211959839\n",
      "Epoch 10522: Training Loss: 0.1074368233482043 Validation Loss: 0.7834903597831726\n",
      "Epoch 10523: Training Loss: 0.1073404128352801 Validation Loss: 0.7832489609718323\n",
      "Epoch 10524: Training Loss: 0.10784844060738881 Validation Loss: 0.7828945517539978\n",
      "Epoch 10525: Training Loss: 0.10761760175228119 Validation Loss: 0.7822288870811462\n",
      "Epoch 10526: Training Loss: 0.10757698118686676 Validation Loss: 0.7815889120101929\n",
      "Epoch 10527: Training Loss: 0.10753587385018666 Validation Loss: 0.7817362546920776\n",
      "Epoch 10528: Training Loss: 0.10796294113000234 Validation Loss: 0.7812312245368958\n",
      "Epoch 10529: Training Loss: 0.10761709014574687 Validation Loss: 0.7825900912284851\n",
      "Epoch 10530: Training Loss: 0.10757405310869217 Validation Loss: 0.7831270098686218\n",
      "Epoch 10531: Training Loss: 0.10765778521696727 Validation Loss: 0.782916784286499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10532: Training Loss: 0.10743135213851929 Validation Loss: 0.7831804752349854\n",
      "Epoch 10533: Training Loss: 0.10773836572964986 Validation Loss: 0.7823826670646667\n",
      "Epoch 10534: Training Loss: 0.10749125480651855 Validation Loss: 0.7826322317123413\n",
      "Epoch 10535: Training Loss: 0.10758168498675029 Validation Loss: 0.7832900881767273\n",
      "Epoch 10536: Training Loss: 0.10755053162574768 Validation Loss: 0.7832534313201904\n",
      "Epoch 10537: Training Loss: 0.10768475383520126 Validation Loss: 0.7827759385108948\n",
      "Epoch 10538: Training Loss: 0.10804409285386403 Validation Loss: 0.7824766635894775\n",
      "Epoch 10539: Training Loss: 0.10752985626459122 Validation Loss: 0.7834425568580627\n",
      "Epoch 10540: Training Loss: 0.10741291691859563 Validation Loss: 0.7830743789672852\n",
      "Epoch 10541: Training Loss: 0.10796115547418594 Validation Loss: 0.7830615639686584\n",
      "Epoch 10542: Training Loss: 0.10722080618143082 Validation Loss: 0.7832444906234741\n",
      "Epoch 10543: Training Loss: 0.1081148087978363 Validation Loss: 0.7835724353790283\n",
      "Epoch 10544: Training Loss: 0.10759057352940242 Validation Loss: 0.7840129137039185\n",
      "Epoch 10545: Training Loss: 0.10761647174755733 Validation Loss: 0.7841466069221497\n",
      "Epoch 10546: Training Loss: 0.1075971523920695 Validation Loss: 0.7832305431365967\n",
      "Epoch 10547: Training Loss: 0.10744748016198476 Validation Loss: 0.7830711007118225\n",
      "Epoch 10548: Training Loss: 0.10750008622805278 Validation Loss: 0.7830524444580078\n",
      "Epoch 10549: Training Loss: 0.10721043248971303 Validation Loss: 0.783653974533081\n",
      "Epoch 10550: Training Loss: 0.1074821228782336 Validation Loss: 0.7853008508682251\n",
      "Epoch 10551: Training Loss: 0.1076874906818072 Validation Loss: 0.7852603197097778\n",
      "Epoch 10552: Training Loss: 0.10743444412946701 Validation Loss: 0.7843037247657776\n",
      "Epoch 10553: Training Loss: 0.1072566385070483 Validation Loss: 0.7840597033500671\n",
      "Epoch 10554: Training Loss: 0.10741365204254787 Validation Loss: 0.7824617028236389\n",
      "Epoch 10555: Training Loss: 0.10715913772583008 Validation Loss: 0.7819579839706421\n",
      "Epoch 10556: Training Loss: 0.10765863209962845 Validation Loss: 0.7823156714439392\n",
      "Epoch 10557: Training Loss: 0.10702173411846161 Validation Loss: 0.782849133014679\n",
      "Epoch 10558: Training Loss: 0.10724345594644547 Validation Loss: 0.7833561897277832\n",
      "Epoch 10559: Training Loss: 0.10734866559505463 Validation Loss: 0.7842814922332764\n",
      "Epoch 10560: Training Loss: 0.1075619210799535 Validation Loss: 0.783725380897522\n",
      "Epoch 10561: Training Loss: 0.10781911512215932 Validation Loss: 0.7831884622573853\n",
      "Epoch 10562: Training Loss: 0.10809437185525894 Validation Loss: 0.782595694065094\n",
      "Epoch 10563: Training Loss: 0.10730016976594925 Validation Loss: 0.7829011678695679\n",
      "Epoch 10564: Training Loss: 0.1078854575753212 Validation Loss: 0.7833404541015625\n",
      "Epoch 10565: Training Loss: 0.10767928262551625 Validation Loss: 0.7834736108779907\n",
      "Epoch 10566: Training Loss: 0.10735626767079036 Validation Loss: 0.7831658720970154\n",
      "Epoch 10567: Training Loss: 0.10729694614807765 Validation Loss: 0.7835162878036499\n",
      "Epoch 10568: Training Loss: 0.10723370313644409 Validation Loss: 0.7833799719810486\n",
      "Epoch 10569: Training Loss: 0.10723844170570374 Validation Loss: 0.7832605838775635\n",
      "Epoch 10570: Training Loss: 0.10721896837155025 Validation Loss: 0.7828993201255798\n",
      "Epoch 10571: Training Loss: 0.10734563072522481 Validation Loss: 0.7825943827629089\n",
      "Epoch 10572: Training Loss: 0.10704155017932256 Validation Loss: 0.7826880216598511\n",
      "Epoch 10573: Training Loss: 0.10726236055294673 Validation Loss: 0.7828665971755981\n",
      "Epoch 10574: Training Loss: 0.1071834738055865 Validation Loss: 0.7833341360092163\n",
      "Epoch 10575: Training Loss: 0.10731306423743565 Validation Loss: 0.7828258872032166\n",
      "Epoch 10576: Training Loss: 0.10720144957304001 Validation Loss: 0.7832366228103638\n",
      "Epoch 10577: Training Loss: 0.10712564488252004 Validation Loss: 0.783930242061615\n",
      "Epoch 10578: Training Loss: 0.10727086663246155 Validation Loss: 0.7841037511825562\n",
      "Epoch 10579: Training Loss: 0.10757492482662201 Validation Loss: 0.7851251363754272\n",
      "Epoch 10580: Training Loss: 0.1069502979516983 Validation Loss: 0.7848073244094849\n",
      "Epoch 10581: Training Loss: 0.10724399238824844 Validation Loss: 0.7842575907707214\n",
      "Epoch 10582: Training Loss: 0.10732439160346985 Validation Loss: 0.7834056615829468\n",
      "Epoch 10583: Training Loss: 0.10728368411461513 Validation Loss: 0.7843754291534424\n",
      "Epoch 10584: Training Loss: 0.1064955194791158 Validation Loss: 0.7841145992279053\n",
      "Epoch 10585: Training Loss: 0.10738663127024968 Validation Loss: 0.783993661403656\n",
      "Epoch 10586: Training Loss: 0.10740004976590474 Validation Loss: 0.7836176753044128\n",
      "Epoch 10587: Training Loss: 0.1072203020254771 Validation Loss: 0.7829042673110962\n",
      "Epoch 10588: Training Loss: 0.10693825781345367 Validation Loss: 0.7832203507423401\n",
      "Epoch 10589: Training Loss: 0.10687353213628133 Validation Loss: 0.7829920053482056\n",
      "Epoch 10590: Training Loss: 0.10705997049808502 Validation Loss: 0.783352255821228\n",
      "Epoch 10591: Training Loss: 0.10711132735013962 Validation Loss: 0.783392071723938\n",
      "Epoch 10592: Training Loss: 0.10715651015440623 Validation Loss: 0.7830440998077393\n",
      "Epoch 10593: Training Loss: 0.10767089078823726 Validation Loss: 0.7835752367973328\n",
      "Epoch 10594: Training Loss: 0.1072033445040385 Validation Loss: 0.7842434644699097\n",
      "Epoch 10595: Training Loss: 0.10713581989208858 Validation Loss: 0.7845938205718994\n",
      "Epoch 10596: Training Loss: 0.10737594217061996 Validation Loss: 0.7838795185089111\n",
      "Epoch 10597: Training Loss: 0.10728515932957332 Validation Loss: 0.783488392829895\n",
      "Epoch 10598: Training Loss: 0.10690365235010783 Validation Loss: 0.7835309505462646\n",
      "Epoch 10599: Training Loss: 0.10794733713070552 Validation Loss: 0.7826626300811768\n",
      "Epoch 10600: Training Loss: 0.10714296003182729 Validation Loss: 0.7832643389701843\n",
      "Epoch 10601: Training Loss: 0.10658272852500279 Validation Loss: 0.7843691110610962\n",
      "Epoch 10602: Training Loss: 0.10719658682743709 Validation Loss: 0.7847442030906677\n",
      "Epoch 10603: Training Loss: 0.10712543626626332 Validation Loss: 0.7842181921005249\n",
      "Epoch 10604: Training Loss: 0.10707292209068935 Validation Loss: 0.7842262983322144\n",
      "Epoch 10605: Training Loss: 0.10757624357938766 Validation Loss: 0.7841049432754517\n",
      "Epoch 10606: Training Loss: 0.1069980909427007 Validation Loss: 0.7838935256004333\n",
      "Epoch 10607: Training Loss: 0.10719117273886998 Validation Loss: 0.7828716039657593\n",
      "Epoch 10608: Training Loss: 0.10713119804859161 Validation Loss: 0.7835232615470886\n",
      "Epoch 10609: Training Loss: 0.10717134426037471 Validation Loss: 0.7833383083343506\n",
      "Epoch 10610: Training Loss: 0.10725013663371404 Validation Loss: 0.7829150557518005\n",
      "Epoch 10611: Training Loss: 0.10720222194989522 Validation Loss: 0.7829368710517883\n",
      "Epoch 10612: Training Loss: 0.10700613260269165 Validation Loss: 0.7839930057525635\n",
      "Epoch 10613: Training Loss: 0.10684233158826828 Validation Loss: 0.7846032977104187\n",
      "Epoch 10614: Training Loss: 0.10708106309175491 Validation Loss: 0.7840486168861389\n",
      "Epoch 10615: Training Loss: 0.10694697499275208 Validation Loss: 0.7842833399772644\n",
      "Epoch 10616: Training Loss: 0.10735760132471721 Validation Loss: 0.7832708954811096\n",
      "Epoch 10617: Training Loss: 0.10727629562218984 Validation Loss: 0.7836480736732483\n",
      "Epoch 10618: Training Loss: 0.10728426525990169 Validation Loss: 0.7840402722358704\n",
      "Epoch 10619: Training Loss: 0.10769113649924596 Validation Loss: 0.7838580012321472\n",
      "Epoch 10620: Training Loss: 0.1069311077396075 Validation Loss: 0.7832036018371582\n",
      "Epoch 10621: Training Loss: 0.10681602358818054 Validation Loss: 0.7832618951797485\n",
      "Epoch 10622: Training Loss: 0.1070221761862437 Validation Loss: 0.7830535769462585\n",
      "Epoch 10623: Training Loss: 0.10809521128733952 Validation Loss: 0.7832489609718323\n",
      "Epoch 10624: Training Loss: 0.10688659300406773 Validation Loss: 0.7827361822128296\n",
      "Epoch 10625: Training Loss: 0.1069505587220192 Validation Loss: 0.7835066914558411\n",
      "Epoch 10626: Training Loss: 0.10716696083545685 Validation Loss: 0.7842628359794617\n",
      "Epoch 10627: Training Loss: 0.10725030054648717 Validation Loss: 0.7845535278320312\n",
      "Epoch 10628: Training Loss: 0.10691325863202412 Validation Loss: 0.7847030162811279\n",
      "Epoch 10629: Training Loss: 0.10683546215295792 Validation Loss: 0.7844743132591248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10630: Training Loss: 0.10744283845027287 Validation Loss: 0.7840635776519775\n",
      "Epoch 10631: Training Loss: 0.10701328267653783 Validation Loss: 0.7833282947540283\n",
      "Epoch 10632: Training Loss: 0.10689933101336162 Validation Loss: 0.7841718196868896\n",
      "Epoch 10633: Training Loss: 0.10718787958224614 Validation Loss: 0.7841489315032959\n",
      "Epoch 10634: Training Loss: 0.10711115847031276 Validation Loss: 0.7840049266815186\n",
      "Epoch 10635: Training Loss: 0.1071384424964587 Validation Loss: 0.7848498225212097\n",
      "Epoch 10636: Training Loss: 0.10643988351027171 Validation Loss: 0.7843120098114014\n",
      "Epoch 10637: Training Loss: 0.10788573076327641 Validation Loss: 0.7845505475997925\n",
      "Epoch 10638: Training Loss: 0.10694163540999095 Validation Loss: 0.7840084433555603\n",
      "Epoch 10639: Training Loss: 0.10716318090756734 Validation Loss: 0.7828396558761597\n",
      "Epoch 10640: Training Loss: 0.10702929894129436 Validation Loss: 0.7830374836921692\n",
      "Epoch 10641: Training Loss: 0.10753151774406433 Validation Loss: 0.7841243147850037\n",
      "Epoch 10642: Training Loss: 0.10769140968720119 Validation Loss: 0.7851406335830688\n",
      "Epoch 10643: Training Loss: 0.10679431507984798 Validation Loss: 0.7850711345672607\n",
      "Epoch 10644: Training Loss: 0.10682801405588786 Validation Loss: 0.7838314175605774\n",
      "Epoch 10645: Training Loss: 0.10694445172945659 Validation Loss: 0.7841801643371582\n",
      "Epoch 10646: Training Loss: 0.10633676002422969 Validation Loss: 0.7844595313072205\n",
      "Epoch 10647: Training Loss: 0.106961024304231 Validation Loss: 0.7839614152908325\n",
      "Epoch 10648: Training Loss: 0.10700032114982605 Validation Loss: 0.78420090675354\n",
      "Epoch 10649: Training Loss: 0.10683093468348186 Validation Loss: 0.7847165465354919\n",
      "Epoch 10650: Training Loss: 0.10707950095335643 Validation Loss: 0.7848326563835144\n",
      "Epoch 10651: Training Loss: 0.10754080613454182 Validation Loss: 0.7851971983909607\n",
      "Epoch 10652: Training Loss: 0.1067636211713155 Validation Loss: 0.7850345969200134\n",
      "Epoch 10653: Training Loss: 0.10677601645390193 Validation Loss: 0.7854762673377991\n",
      "Epoch 10654: Training Loss: 0.10707549502452214 Validation Loss: 0.7857519388198853\n",
      "Epoch 10655: Training Loss: 0.10733823229869206 Validation Loss: 0.7840150594711304\n",
      "Epoch 10656: Training Loss: 0.10698414593935013 Validation Loss: 0.7826933860778809\n",
      "Epoch 10657: Training Loss: 0.1073215181628863 Validation Loss: 0.7826111912727356\n",
      "Epoch 10658: Training Loss: 0.10670994967222214 Validation Loss: 0.7828387022018433\n",
      "Epoch 10659: Training Loss: 0.10711647321780522 Validation Loss: 0.7834674715995789\n",
      "Epoch 10660: Training Loss: 0.10697669039169948 Validation Loss: 0.7841807007789612\n",
      "Epoch 10661: Training Loss: 0.1065057838956515 Validation Loss: 0.7843639850616455\n",
      "Epoch 10662: Training Loss: 0.10684282581011455 Validation Loss: 0.7846028804779053\n",
      "Epoch 10663: Training Loss: 0.1068256547053655 Validation Loss: 0.7842965722084045\n",
      "Epoch 10664: Training Loss: 0.10649227847655614 Validation Loss: 0.7838056087493896\n",
      "Epoch 10665: Training Loss: 0.10694870352745056 Validation Loss: 0.7843109965324402\n",
      "Epoch 10666: Training Loss: 0.10660576820373535 Validation Loss: 0.7853624820709229\n",
      "Epoch 10667: Training Loss: 0.1064723754922549 Validation Loss: 0.7855808734893799\n",
      "Epoch 10668: Training Loss: 0.10633845875660579 Validation Loss: 0.7849804759025574\n",
      "Epoch 10669: Training Loss: 0.10698196788628896 Validation Loss: 0.7845602631568909\n",
      "Epoch 10670: Training Loss: 0.10710498690605164 Validation Loss: 0.7852532267570496\n",
      "Epoch 10671: Training Loss: 0.10741897424062093 Validation Loss: 0.7847581505775452\n",
      "Epoch 10672: Training Loss: 0.10657116274038951 Validation Loss: 0.7841312885284424\n",
      "Epoch 10673: Training Loss: 0.1065879041949908 Validation Loss: 0.7843661308288574\n",
      "Epoch 10674: Training Loss: 0.1066614141066869 Validation Loss: 0.7848855257034302\n",
      "Epoch 10675: Training Loss: 0.10674145817756653 Validation Loss: 0.7846046686172485\n",
      "Epoch 10676: Training Loss: 0.10648090640703838 Validation Loss: 0.7846056818962097\n",
      "Epoch 10677: Training Loss: 0.10667229940493901 Validation Loss: 0.7843168377876282\n",
      "Epoch 10678: Training Loss: 0.10774261007706325 Validation Loss: 0.7835912108421326\n",
      "Epoch 10679: Training Loss: 0.10663797209660213 Validation Loss: 0.7848343849182129\n",
      "Epoch 10680: Training Loss: 0.10710950195789337 Validation Loss: 0.7859015464782715\n",
      "Epoch 10681: Training Loss: 0.10696770002444585 Validation Loss: 0.7839429974555969\n",
      "Epoch 10682: Training Loss: 0.1064041331410408 Validation Loss: 0.7841140031814575\n",
      "Epoch 10683: Training Loss: 0.1067328800757726 Validation Loss: 0.784349262714386\n",
      "Epoch 10684: Training Loss: 0.10652208824952443 Validation Loss: 0.7843198180198669\n",
      "Epoch 10685: Training Loss: 0.10665478805700938 Validation Loss: 0.7845411896705627\n",
      "Epoch 10686: Training Loss: 0.1067306399345398 Validation Loss: 0.784706175327301\n",
      "Epoch 10687: Training Loss: 0.10639813790718715 Validation Loss: 0.7847283482551575\n",
      "Epoch 10688: Training Loss: 0.10663921634356181 Validation Loss: 0.784616231918335\n",
      "Epoch 10689: Training Loss: 0.10713355739911397 Validation Loss: 0.7849835157394409\n",
      "Epoch 10690: Training Loss: 0.106651671230793 Validation Loss: 0.7848114371299744\n",
      "Epoch 10691: Training Loss: 0.10655051718155543 Validation Loss: 0.7852943539619446\n",
      "Epoch 10692: Training Loss: 0.10650841891765594 Validation Loss: 0.7852264642715454\n",
      "Epoch 10693: Training Loss: 0.10654024283091228 Validation Loss: 0.7842394113540649\n",
      "Epoch 10694: Training Loss: 0.10638909041881561 Validation Loss: 0.7841665148735046\n",
      "Epoch 10695: Training Loss: 0.10671640435854594 Validation Loss: 0.7836804389953613\n",
      "Epoch 10696: Training Loss: 0.10675358772277832 Validation Loss: 0.7841866612434387\n",
      "Epoch 10697: Training Loss: 0.10640819867451985 Validation Loss: 0.7850833535194397\n",
      "Epoch 10698: Training Loss: 0.10651323447624843 Validation Loss: 0.7851651906967163\n",
      "Epoch 10699: Training Loss: 0.10646163920561473 Validation Loss: 0.7855618000030518\n",
      "Epoch 10700: Training Loss: 0.10651478916406631 Validation Loss: 0.7849027514457703\n",
      "Epoch 10701: Training Loss: 0.10628009339173634 Validation Loss: 0.7850986123085022\n",
      "Epoch 10702: Training Loss: 0.10637977719306946 Validation Loss: 0.785120964050293\n",
      "Epoch 10703: Training Loss: 0.10646146287520726 Validation Loss: 0.7843981981277466\n",
      "Epoch 10704: Training Loss: 0.10741272817055385 Validation Loss: 0.7852548360824585\n",
      "Epoch 10705: Training Loss: 0.10640159746011098 Validation Loss: 0.7847978472709656\n",
      "Epoch 10706: Training Loss: 0.10685475418965022 Validation Loss: 0.7856036424636841\n",
      "Epoch 10707: Training Loss: 0.10678208867708842 Validation Loss: 0.7856013178825378\n",
      "Epoch 10708: Training Loss: 0.10638281206289928 Validation Loss: 0.7854105234146118\n",
      "Epoch 10709: Training Loss: 0.10623575001955032 Validation Loss: 0.7852486968040466\n",
      "Epoch 10710: Training Loss: 0.10633845875660579 Validation Loss: 0.7847667336463928\n",
      "Epoch 10711: Training Loss: 0.10642419010400772 Validation Loss: 0.7847610712051392\n",
      "Epoch 10712: Training Loss: 0.1060212900241216 Validation Loss: 0.7848963141441345\n",
      "Epoch 10713: Training Loss: 0.10597114264965057 Validation Loss: 0.7847121357917786\n",
      "Epoch 10714: Training Loss: 0.10661302506923676 Validation Loss: 0.7850555777549744\n",
      "Epoch 10715: Training Loss: 0.10620289544264476 Validation Loss: 0.7856162190437317\n",
      "Epoch 10716: Training Loss: 0.10616611937681834 Validation Loss: 0.7854140996932983\n",
      "Epoch 10717: Training Loss: 0.10637557506561279 Validation Loss: 0.7851746678352356\n",
      "Epoch 10718: Training Loss: 0.10661184042692184 Validation Loss: 0.7855923771858215\n",
      "Epoch 10719: Training Loss: 0.10646299024422963 Validation Loss: 0.785362184047699\n",
      "Epoch 10720: Training Loss: 0.10620059818029404 Validation Loss: 0.784990668296814\n",
      "Epoch 10721: Training Loss: 0.10617938141028087 Validation Loss: 0.7851902842521667\n",
      "Epoch 10722: Training Loss: 0.1064688836534818 Validation Loss: 0.7861974239349365\n",
      "Epoch 10723: Training Loss: 0.1063937172293663 Validation Loss: 0.7854219675064087\n",
      "Epoch 10724: Training Loss: 0.1071200246612231 Validation Loss: 0.7858720421791077\n",
      "Epoch 10725: Training Loss: 0.10609280566374461 Validation Loss: 0.7857805490493774\n",
      "Epoch 10726: Training Loss: 0.1067521870136261 Validation Loss: 0.7859291434288025\n",
      "Epoch 10727: Training Loss: 0.10620085150003433 Validation Loss: 0.7851065397262573\n",
      "Epoch 10728: Training Loss: 0.1062051331003507 Validation Loss: 0.7848579287528992\n",
      "Epoch 10729: Training Loss: 0.10634782165288925 Validation Loss: 0.7846870422363281\n",
      "Epoch 10730: Training Loss: 0.10616900275150935 Validation Loss: 0.7850143909454346\n",
      "Epoch 10731: Training Loss: 0.10625275472799937 Validation Loss: 0.7854269742965698\n",
      "Epoch 10732: Training Loss: 0.10688365002473195 Validation Loss: 0.7855896353721619\n",
      "Epoch 10733: Training Loss: 0.10657109568516414 Validation Loss: 0.7865031361579895\n",
      "Epoch 10734: Training Loss: 0.10653705894947052 Validation Loss: 0.7862669825553894\n",
      "Epoch 10735: Training Loss: 0.10605086882909139 Validation Loss: 0.7850929498672485\n",
      "Epoch 10736: Training Loss: 0.10725828756888707 Validation Loss: 0.7854686379432678\n",
      "Epoch 10737: Training Loss: 0.10610181838274002 Validation Loss: 0.7849034667015076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10738: Training Loss: 0.1062675192952156 Validation Loss: 0.7844130396842957\n",
      "Epoch 10739: Training Loss: 0.10634846985340118 Validation Loss: 0.7856987714767456\n",
      "Epoch 10740: Training Loss: 0.10603111733992894 Validation Loss: 0.7857317328453064\n",
      "Epoch 10741: Training Loss: 0.10623171180486679 Validation Loss: 0.7855209112167358\n",
      "Epoch 10742: Training Loss: 0.1062141830722491 Validation Loss: 0.7850618362426758\n",
      "Epoch 10743: Training Loss: 0.10674517850081126 Validation Loss: 0.7853808999061584\n",
      "Epoch 10744: Training Loss: 0.10615755120913188 Validation Loss: 0.7862054705619812\n",
      "Epoch 10745: Training Loss: 0.10580387463172276 Validation Loss: 0.785956859588623\n",
      "Epoch 10746: Training Loss: 0.10613185167312622 Validation Loss: 0.7859483957290649\n",
      "Epoch 10747: Training Loss: 0.10622510562340419 Validation Loss: 0.7860124111175537\n",
      "Epoch 10748: Training Loss: 0.10618515809377034 Validation Loss: 0.7861604690551758\n",
      "Epoch 10749: Training Loss: 0.10633825262387593 Validation Loss: 0.7862117886543274\n",
      "Epoch 10750: Training Loss: 0.10623282690842946 Validation Loss: 0.7861180305480957\n",
      "Epoch 10751: Training Loss: 0.10613766064246495 Validation Loss: 0.7850747108459473\n",
      "Epoch 10752: Training Loss: 0.10606688261032104 Validation Loss: 0.7851660251617432\n",
      "Epoch 10753: Training Loss: 0.10640019923448563 Validation Loss: 0.7855427265167236\n",
      "Epoch 10754: Training Loss: 0.10605162382125854 Validation Loss: 0.7856317162513733\n",
      "Epoch 10755: Training Loss: 0.10617174456516902 Validation Loss: 0.7848212122917175\n",
      "Epoch 10756: Training Loss: 0.10642832517623901 Validation Loss: 0.7852998971939087\n",
      "Epoch 10757: Training Loss: 0.10622592518726985 Validation Loss: 0.7860118746757507\n",
      "Epoch 10758: Training Loss: 0.10597964872916539 Validation Loss: 0.7862892150878906\n",
      "Epoch 10759: Training Loss: 0.10625745604435603 Validation Loss: 0.7863865494728088\n",
      "Epoch 10760: Training Loss: 0.1060970773299535 Validation Loss: 0.7861781120300293\n",
      "Epoch 10761: Training Loss: 0.10599932571252187 Validation Loss: 0.7859352827072144\n",
      "Epoch 10762: Training Loss: 0.106001911063989 Validation Loss: 0.7856172323226929\n",
      "Epoch 10763: Training Loss: 0.10608146836360295 Validation Loss: 0.7866926789283752\n",
      "Epoch 10764: Training Loss: 0.10583734760681789 Validation Loss: 0.786815881729126\n",
      "Epoch 10765: Training Loss: 0.1065882220864296 Validation Loss: 0.7860085368156433\n",
      "Epoch 10766: Training Loss: 0.10626625766356786 Validation Loss: 0.7863932251930237\n",
      "Epoch 10767: Training Loss: 0.10613626738389333 Validation Loss: 0.7858379483222961\n",
      "Epoch 10768: Training Loss: 0.10630272577206294 Validation Loss: 0.78480464220047\n",
      "Epoch 10769: Training Loss: 0.10599438349405925 Validation Loss: 0.7842203378677368\n",
      "Epoch 10770: Training Loss: 0.10604557891686757 Validation Loss: 0.7846294045448303\n",
      "Epoch 10771: Training Loss: 0.1058419942855835 Validation Loss: 0.7856808304786682\n",
      "Epoch 10772: Training Loss: 0.10608172168334325 Validation Loss: 0.7859084010124207\n",
      "Epoch 10773: Training Loss: 0.10601300497849782 Validation Loss: 0.7863970398902893\n",
      "Epoch 10774: Training Loss: 0.10590801884730656 Validation Loss: 0.7857553958892822\n",
      "Epoch 10775: Training Loss: 0.10590578367312749 Validation Loss: 0.7857027649879456\n",
      "Epoch 10776: Training Loss: 0.106377179423968 Validation Loss: 0.7852584719657898\n",
      "Epoch 10777: Training Loss: 0.10658964763085048 Validation Loss: 0.7859718203544617\n",
      "Epoch 10778: Training Loss: 0.10638796786467235 Validation Loss: 0.7866467833518982\n",
      "Epoch 10779: Training Loss: 0.10622991621494293 Validation Loss: 0.7861316204071045\n",
      "Epoch 10780: Training Loss: 0.10604650030533473 Validation Loss: 0.7859926223754883\n",
      "Epoch 10781: Training Loss: 0.10588356852531433 Validation Loss: 0.7858785390853882\n",
      "Epoch 10782: Training Loss: 0.10582911223173141 Validation Loss: 0.7859063744544983\n",
      "Epoch 10783: Training Loss: 0.10638107856114705 Validation Loss: 0.7850570678710938\n",
      "Epoch 10784: Training Loss: 0.10584719230731328 Validation Loss: 0.7849817872047424\n",
      "Epoch 10785: Training Loss: 0.10546288887659709 Validation Loss: 0.7858917713165283\n",
      "Epoch 10786: Training Loss: 0.10597238441308339 Validation Loss: 0.7871959209442139\n",
      "Epoch 10787: Training Loss: 0.10637893776098888 Validation Loss: 0.7873749136924744\n",
      "Epoch 10788: Training Loss: 0.10605563720067342 Validation Loss: 0.7860404253005981\n",
      "Epoch 10789: Training Loss: 0.10567048192024231 Validation Loss: 0.7855165600776672\n",
      "Epoch 10790: Training Loss: 0.10613230367501576 Validation Loss: 0.7849363088607788\n",
      "Epoch 10791: Training Loss: 0.10666543493668239 Validation Loss: 0.7853648066520691\n",
      "Epoch 10792: Training Loss: 0.10580483327309291 Validation Loss: 0.7859838008880615\n",
      "Epoch 10793: Training Loss: 0.10610610743363698 Validation Loss: 0.7863612771034241\n",
      "Epoch 10794: Training Loss: 0.10641956329345703 Validation Loss: 0.7862774729728699\n",
      "Epoch 10795: Training Loss: 0.10611819227536519 Validation Loss: 0.785861611366272\n",
      "Epoch 10796: Training Loss: 0.10570885241031647 Validation Loss: 0.7866828441619873\n",
      "Epoch 10797: Training Loss: 0.1055489107966423 Validation Loss: 0.7875010967254639\n",
      "Epoch 10798: Training Loss: 0.10580515364805858 Validation Loss: 0.7863909006118774\n",
      "Epoch 10799: Training Loss: 0.10704500476519267 Validation Loss: 0.7864594459533691\n",
      "Epoch 10800: Training Loss: 0.10607697317997615 Validation Loss: 0.786496639251709\n",
      "Epoch 10801: Training Loss: 0.10569801678260167 Validation Loss: 0.7855866551399231\n",
      "Epoch 10802: Training Loss: 0.10647316773732503 Validation Loss: 0.7854865193367004\n",
      "Epoch 10803: Training Loss: 0.1059236650665601 Validation Loss: 0.7857406735420227\n",
      "Epoch 10804: Training Loss: 0.10575238366921742 Validation Loss: 0.7864676117897034\n",
      "Epoch 10805: Training Loss: 0.1057774821917216 Validation Loss: 0.7868178486824036\n",
      "Epoch 10806: Training Loss: 0.10612090428670247 Validation Loss: 0.787156879901886\n",
      "Epoch 10807: Training Loss: 0.10569303979476292 Validation Loss: 0.7870566844940186\n",
      "Epoch 10808: Training Loss: 0.10560985157887141 Validation Loss: 0.7867136001586914\n",
      "Epoch 10809: Training Loss: 0.10585215191046397 Validation Loss: 0.7862575054168701\n",
      "Epoch 10810: Training Loss: 0.10554683208465576 Validation Loss: 0.7858086228370667\n",
      "Epoch 10811: Training Loss: 0.10552756239970525 Validation Loss: 0.7863168120384216\n",
      "Epoch 10812: Training Loss: 0.1059720516204834 Validation Loss: 0.7855720520019531\n",
      "Epoch 10813: Training Loss: 0.1060531015197436 Validation Loss: 0.7857509255409241\n",
      "Epoch 10814: Training Loss: 0.10636640588442485 Validation Loss: 0.7860636711120605\n",
      "Epoch 10815: Training Loss: 0.1056930844982465 Validation Loss: 0.785764753818512\n",
      "Epoch 10816: Training Loss: 0.10636263340711594 Validation Loss: 0.7868152260780334\n",
      "Epoch 10817: Training Loss: 0.10520141075054805 Validation Loss: 0.7867593765258789\n",
      "Epoch 10818: Training Loss: 0.10582112769285838 Validation Loss: 0.7871048450469971\n",
      "Epoch 10819: Training Loss: 0.10559837520122528 Validation Loss: 0.7868932485580444\n",
      "Epoch 10820: Training Loss: 0.10570397228002548 Validation Loss: 0.7866459488868713\n",
      "Epoch 10821: Training Loss: 0.10576081275939941 Validation Loss: 0.7868039011955261\n",
      "Epoch 10822: Training Loss: 0.10588206350803375 Validation Loss: 0.7865450382232666\n",
      "Epoch 10823: Training Loss: 0.10703559219837189 Validation Loss: 0.7863743901252747\n",
      "Epoch 10824: Training Loss: 0.1061338484287262 Validation Loss: 0.7866688370704651\n",
      "Epoch 10825: Training Loss: 0.10564441482226054 Validation Loss: 0.7860181927680969\n",
      "Epoch 10826: Training Loss: 0.10648140807946523 Validation Loss: 0.7861881256103516\n",
      "Epoch 10827: Training Loss: 0.10567512114842732 Validation Loss: 0.7860473990440369\n",
      "Epoch 10828: Training Loss: 0.10624599705139796 Validation Loss: 0.7858831882476807\n",
      "Epoch 10829: Training Loss: 0.10567584385474522 Validation Loss: 0.7861723303794861\n",
      "Epoch 10830: Training Loss: 0.1059580718477567 Validation Loss: 0.7866396903991699\n",
      "Epoch 10831: Training Loss: 0.1056193287173907 Validation Loss: 0.7869672179222107\n",
      "Epoch 10832: Training Loss: 0.10574722786744435 Validation Loss: 0.7874635457992554\n",
      "Epoch 10833: Training Loss: 0.10550315429766972 Validation Loss: 0.7867134213447571\n",
      "Epoch 10834: Training Loss: 0.10520576437314351 Validation Loss: 0.7868404388427734\n",
      "Epoch 10835: Training Loss: 0.10579432795445125 Validation Loss: 0.7864906787872314\n",
      "Epoch 10836: Training Loss: 0.10563202450672786 Validation Loss: 0.7861412763595581\n",
      "Epoch 10837: Training Loss: 0.10598543534676234 Validation Loss: 0.7865722179412842\n",
      "Epoch 10838: Training Loss: 0.105551411708196 Validation Loss: 0.7878367304801941\n",
      "Epoch 10839: Training Loss: 0.10562742253144582 Validation Loss: 0.7874950170516968\n",
      "Epoch 10840: Training Loss: 0.10499744862318039 Validation Loss: 0.7875760793685913\n",
      "Epoch 10841: Training Loss: 0.10561167448759079 Validation Loss: 0.7867657542228699\n",
      "Epoch 10842: Training Loss: 0.10556791226069133 Validation Loss: 0.786620557308197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10843: Training Loss: 0.10549455384413402 Validation Loss: 0.7865248918533325\n",
      "Epoch 10844: Training Loss: 0.10571212321519852 Validation Loss: 0.7874597311019897\n",
      "Epoch 10845: Training Loss: 0.10560018569231033 Validation Loss: 0.7881526350975037\n",
      "Epoch 10846: Training Loss: 0.10549949109554291 Validation Loss: 0.7875597476959229\n",
      "Epoch 10847: Training Loss: 0.10568234324455261 Validation Loss: 0.7869688868522644\n",
      "Epoch 10848: Training Loss: 0.10571944961945216 Validation Loss: 0.7865523099899292\n",
      "Epoch 10849: Training Loss: 0.10652925074100494 Validation Loss: 0.7863277792930603\n",
      "Epoch 10850: Training Loss: 0.10554687430461247 Validation Loss: 0.7869284749031067\n",
      "Epoch 10851: Training Loss: 0.10606980820496877 Validation Loss: 0.7865409851074219\n",
      "Epoch 10852: Training Loss: 0.1056602771083514 Validation Loss: 0.78626948595047\n",
      "Epoch 10853: Training Loss: 0.10517184187968572 Validation Loss: 0.7863285541534424\n",
      "Epoch 10854: Training Loss: 0.1057344600558281 Validation Loss: 0.7862297892570496\n",
      "Epoch 10855: Training Loss: 0.10543100039164226 Validation Loss: 0.7869097590446472\n",
      "Epoch 10856: Training Loss: 0.10565957427024841 Validation Loss: 0.7865999937057495\n",
      "Epoch 10857: Training Loss: 0.1056394949555397 Validation Loss: 0.7875233292579651\n",
      "Epoch 10858: Training Loss: 0.10550847897926967 Validation Loss: 0.7877404093742371\n",
      "Epoch 10859: Training Loss: 0.10534096757570903 Validation Loss: 0.787649929523468\n",
      "Epoch 10860: Training Loss: 0.10593032588561375 Validation Loss: 0.7875620722770691\n",
      "Epoch 10861: Training Loss: 0.1055779680609703 Validation Loss: 0.7879745960235596\n",
      "Epoch 10862: Training Loss: 0.10549300909042358 Validation Loss: 0.7874056100845337\n",
      "Epoch 10863: Training Loss: 0.10538586477438609 Validation Loss: 0.7872808575630188\n",
      "Epoch 10864: Training Loss: 0.10555352022250493 Validation Loss: 0.7874280214309692\n",
      "Epoch 10865: Training Loss: 0.10547919571399689 Validation Loss: 0.7870206832885742\n",
      "Epoch 10866: Training Loss: 0.1053089772661527 Validation Loss: 0.7864556908607483\n",
      "Epoch 10867: Training Loss: 0.10547356307506561 Validation Loss: 0.786906361579895\n",
      "Epoch 10868: Training Loss: 0.10543351620435715 Validation Loss: 0.7865260243415833\n",
      "Epoch 10869: Training Loss: 0.1062045693397522 Validation Loss: 0.7863940000534058\n",
      "Epoch 10870: Training Loss: 0.10562161852916081 Validation Loss: 0.786747932434082\n",
      "Epoch 10871: Training Loss: 0.1058304434021314 Validation Loss: 0.7876059412956238\n",
      "Epoch 10872: Training Loss: 0.10519993305206299 Validation Loss: 0.7880289554595947\n",
      "Epoch 10873: Training Loss: 0.10546670854091644 Validation Loss: 0.788699209690094\n",
      "Epoch 10874: Training Loss: 0.1054788405696551 Validation Loss: 0.7879325747489929\n",
      "Epoch 10875: Training Loss: 0.10565027842919032 Validation Loss: 0.7870638370513916\n",
      "Epoch 10876: Training Loss: 0.10546786586443584 Validation Loss: 0.7867532968521118\n",
      "Epoch 10877: Training Loss: 0.10517101734876633 Validation Loss: 0.7874447107315063\n",
      "Epoch 10878: Training Loss: 0.10568901648124059 Validation Loss: 0.7874265909194946\n",
      "Epoch 10879: Training Loss: 0.1056856041153272 Validation Loss: 0.7885017991065979\n",
      "Epoch 10880: Training Loss: 0.10550655176242192 Validation Loss: 0.7879520654678345\n",
      "Epoch 10881: Training Loss: 0.1054815873503685 Validation Loss: 0.787748396396637\n",
      "Epoch 10882: Training Loss: 0.10518060872952144 Validation Loss: 0.7864179015159607\n",
      "Epoch 10883: Training Loss: 0.10561490803956985 Validation Loss: 0.786236584186554\n",
      "Epoch 10884: Training Loss: 0.10547212262948354 Validation Loss: 0.7870672941207886\n",
      "Epoch 10885: Training Loss: 0.10533990214268367 Validation Loss: 0.7869333028793335\n",
      "Epoch 10886: Training Loss: 0.10493726034959157 Validation Loss: 0.7872714400291443\n",
      "Epoch 10887: Training Loss: 0.10508496810992558 Validation Loss: 0.7871940732002258\n",
      "Epoch 10888: Training Loss: 0.10520894080400467 Validation Loss: 0.7865145802497864\n",
      "Epoch 10889: Training Loss: 0.10494714975357056 Validation Loss: 0.7871782183647156\n",
      "Epoch 10890: Training Loss: 0.10511975983778636 Validation Loss: 0.787548303604126\n",
      "Epoch 10891: Training Loss: 0.10524981220563252 Validation Loss: 0.7879980206489563\n",
      "Epoch 10892: Training Loss: 0.10530684391657512 Validation Loss: 0.7880252599716187\n",
      "Epoch 10893: Training Loss: 0.10569277902444203 Validation Loss: 0.7873783707618713\n",
      "Epoch 10894: Training Loss: 0.10578982532024384 Validation Loss: 0.7872154712677002\n",
      "Epoch 10895: Training Loss: 0.10529182354609172 Validation Loss: 0.787589967250824\n",
      "Epoch 10896: Training Loss: 0.10563986748456955 Validation Loss: 0.7880837917327881\n",
      "Epoch 10897: Training Loss: 0.10508995999892552 Validation Loss: 0.7881574630737305\n",
      "Epoch 10898: Training Loss: 0.10560477276643117 Validation Loss: 0.7876067757606506\n",
      "Epoch 10899: Training Loss: 0.1052090326944987 Validation Loss: 0.7870369553565979\n",
      "Epoch 10900: Training Loss: 0.1055619219938914 Validation Loss: 0.7873714566230774\n",
      "Epoch 10901: Training Loss: 0.10531628380219142 Validation Loss: 0.7873877286911011\n",
      "Epoch 10902: Training Loss: 0.10540689279635747 Validation Loss: 0.7879526615142822\n",
      "Epoch 10903: Training Loss: 0.10554997622966766 Validation Loss: 0.7872908711433411\n",
      "Epoch 10904: Training Loss: 0.10529202471176784 Validation Loss: 0.7877867817878723\n",
      "Epoch 10905: Training Loss: 0.10499534755945206 Validation Loss: 0.788202166557312\n",
      "Epoch 10906: Training Loss: 0.1053369293610255 Validation Loss: 0.7879860401153564\n",
      "Epoch 10907: Training Loss: 0.10536144425471623 Validation Loss: 0.7878941297531128\n",
      "Epoch 10908: Training Loss: 0.10535715023676555 Validation Loss: 0.7880958318710327\n",
      "Epoch 10909: Training Loss: 0.10517816493908565 Validation Loss: 0.7870515584945679\n",
      "Epoch 10910: Training Loss: 0.10526053607463837 Validation Loss: 0.787665069103241\n",
      "Epoch 10911: Training Loss: 0.10542183617750804 Validation Loss: 0.7870050072669983\n",
      "Epoch 10912: Training Loss: 0.1053256243467331 Validation Loss: 0.7869060635566711\n",
      "Epoch 10913: Training Loss: 0.10506773740053177 Validation Loss: 0.7877933979034424\n",
      "Epoch 10914: Training Loss: 0.10536771019299825 Validation Loss: 0.7876307964324951\n",
      "Epoch 10915: Training Loss: 0.10535623629887898 Validation Loss: 0.7875210046768188\n",
      "Epoch 10916: Training Loss: 0.1050603265563647 Validation Loss: 0.7879161834716797\n",
      "Epoch 10917: Training Loss: 0.10528708000977834 Validation Loss: 0.7877689003944397\n",
      "Epoch 10918: Training Loss: 0.10510790348052979 Validation Loss: 0.7877978682518005\n",
      "Epoch 10919: Training Loss: 0.1056171456972758 Validation Loss: 0.787437379360199\n",
      "Epoch 10920: Training Loss: 0.10486510892709096 Validation Loss: 0.7877196669578552\n",
      "Epoch 10921: Training Loss: 0.10519486665725708 Validation Loss: 0.7879347801208496\n",
      "Epoch 10922: Training Loss: 0.10519044349590938 Validation Loss: 0.7881985306739807\n",
      "Epoch 10923: Training Loss: 0.10560245563586552 Validation Loss: 0.7877334356307983\n",
      "Epoch 10924: Training Loss: 0.10494847844044368 Validation Loss: 0.7869163155555725\n",
      "Epoch 10925: Training Loss: 0.10505421459674835 Validation Loss: 0.7871214151382446\n",
      "Epoch 10926: Training Loss: 0.10502129296461742 Validation Loss: 0.7870461940765381\n",
      "Epoch 10927: Training Loss: 0.10497259845336278 Validation Loss: 0.7876035571098328\n",
      "Epoch 10928: Training Loss: 0.10493060946464539 Validation Loss: 0.7876007556915283\n",
      "Epoch 10929: Training Loss: 0.10488531986872356 Validation Loss: 0.7881008386611938\n",
      "Epoch 10930: Training Loss: 0.10480190068483353 Validation Loss: 0.7876354455947876\n",
      "Epoch 10931: Training Loss: 0.10543989886840184 Validation Loss: 0.7866301536560059\n",
      "Epoch 10932: Training Loss: 0.10506308078765869 Validation Loss: 0.7873895168304443\n",
      "Epoch 10933: Training Loss: 0.10503055900335312 Validation Loss: 0.7882902026176453\n",
      "Epoch 10934: Training Loss: 0.10513690114021301 Validation Loss: 0.7882760167121887\n",
      "Epoch 10935: Training Loss: 0.10526326050360997 Validation Loss: 0.7887635231018066\n",
      "Epoch 10936: Training Loss: 0.10529062400261562 Validation Loss: 0.7892937660217285\n",
      "Epoch 10937: Training Loss: 0.1055424635608991 Validation Loss: 0.7899412512779236\n",
      "Epoch 10938: Training Loss: 0.10477611422538757 Validation Loss: 0.7898963689804077\n",
      "Epoch 10939: Training Loss: 0.10508834073940913 Validation Loss: 0.7883399724960327\n",
      "Epoch 10940: Training Loss: 0.10532307376464208 Validation Loss: 0.7877737879753113\n",
      "Epoch 10941: Training Loss: 0.10481563707192738 Validation Loss: 0.7869216203689575\n",
      "Epoch 10942: Training Loss: 0.10491349299748738 Validation Loss: 0.7868415713310242\n",
      "Epoch 10943: Training Loss: 0.10496500879526138 Validation Loss: 0.7872021794319153\n",
      "Epoch 10944: Training Loss: 0.1049043337504069 Validation Loss: 0.7878100872039795\n",
      "Epoch 10945: Training Loss: 0.10490722954273224 Validation Loss: 0.7886138558387756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10946: Training Loss: 0.10488371054331462 Validation Loss: 0.7883754968643188\n",
      "Epoch 10947: Training Loss: 0.10496264199415843 Validation Loss: 0.7889634966850281\n",
      "Epoch 10948: Training Loss: 0.10494911670684814 Validation Loss: 0.7885794639587402\n",
      "Epoch 10949: Training Loss: 0.10506175955136617 Validation Loss: 0.7879289388656616\n",
      "Epoch 10950: Training Loss: 0.10501803706089656 Validation Loss: 0.7875566482543945\n",
      "Epoch 10951: Training Loss: 0.1048954576253891 Validation Loss: 0.7877565622329712\n",
      "Epoch 10952: Training Loss: 0.10482256611188252 Validation Loss: 0.7872145175933838\n",
      "Epoch 10953: Training Loss: 0.10510378827651341 Validation Loss: 0.7867975831031799\n",
      "Epoch 10954: Training Loss: 0.10478293399016063 Validation Loss: 0.7872456908226013\n",
      "Epoch 10955: Training Loss: 0.10489566624164581 Validation Loss: 0.7872183322906494\n",
      "Epoch 10956: Training Loss: 0.1053263396024704 Validation Loss: 0.7879153490066528\n",
      "Epoch 10957: Training Loss: 0.10526723166306813 Validation Loss: 0.7880879044532776\n",
      "Epoch 10958: Training Loss: 0.10491543511549632 Validation Loss: 0.7871773838996887\n",
      "Epoch 10959: Training Loss: 0.10484154274066289 Validation Loss: 0.7873594164848328\n",
      "Epoch 10960: Training Loss: 0.1046362817287445 Validation Loss: 0.7886314988136292\n",
      "Epoch 10961: Training Loss: 0.10505107045173645 Validation Loss: 0.788952648639679\n",
      "Epoch 10962: Training Loss: 0.10450845211744308 Validation Loss: 0.7897146344184875\n",
      "Epoch 10963: Training Loss: 0.104251263042291 Validation Loss: 0.7893868088722229\n",
      "Epoch 10964: Training Loss: 0.10476280252138774 Validation Loss: 0.7889882326126099\n",
      "Epoch 10965: Training Loss: 0.10488343983888626 Validation Loss: 0.7885375022888184\n",
      "Epoch 10966: Training Loss: 0.10511175791422527 Validation Loss: 0.7884837985038757\n",
      "Epoch 10967: Training Loss: 0.10476150860389073 Validation Loss: 0.7882450222969055\n",
      "Epoch 10968: Training Loss: 0.1048610011736552 Validation Loss: 0.7880324721336365\n",
      "Epoch 10969: Training Loss: 0.10512987772623698 Validation Loss: 0.7889933586120605\n",
      "Epoch 10970: Training Loss: 0.10470477491617203 Validation Loss: 0.7895671725273132\n",
      "Epoch 10971: Training Loss: 0.10500973711411159 Validation Loss: 0.7892279028892517\n",
      "Epoch 10972: Training Loss: 0.10437620927890141 Validation Loss: 0.7888323068618774\n",
      "Epoch 10973: Training Loss: 0.10474394013484319 Validation Loss: 0.7882899641990662\n",
      "Epoch 10974: Training Loss: 0.10459951808055241 Validation Loss: 0.7878413796424866\n",
      "Epoch 10975: Training Loss: 0.10462157179911931 Validation Loss: 0.7874705195426941\n",
      "Epoch 10976: Training Loss: 0.10475497196118037 Validation Loss: 0.7868252992630005\n",
      "Epoch 10977: Training Loss: 0.10473504414161046 Validation Loss: 0.7872901558876038\n",
      "Epoch 10978: Training Loss: 0.1046680212020874 Validation Loss: 0.7883292436599731\n",
      "Epoch 10979: Training Loss: 0.10505373279253642 Validation Loss: 0.7883021831512451\n",
      "Epoch 10980: Training Loss: 0.10507100075483322 Validation Loss: 0.789135754108429\n",
      "Epoch 10981: Training Loss: 0.10499297579129536 Validation Loss: 0.7889171242713928\n",
      "Epoch 10982: Training Loss: 0.105109304189682 Validation Loss: 0.7895535230636597\n",
      "Epoch 10983: Training Loss: 0.1049411694208781 Validation Loss: 0.7890802025794983\n",
      "Epoch 10984: Training Loss: 0.10451094061136246 Validation Loss: 0.7886213660240173\n",
      "Epoch 10985: Training Loss: 0.10471293330192566 Validation Loss: 0.7882992029190063\n",
      "Epoch 10986: Training Loss: 0.10482976833979289 Validation Loss: 0.78846675157547\n",
      "Epoch 10987: Training Loss: 0.1049855425953865 Validation Loss: 0.7890516519546509\n",
      "Epoch 10988: Training Loss: 0.10468699783086777 Validation Loss: 0.7893643975257874\n",
      "Epoch 10989: Training Loss: 0.10486262043317159 Validation Loss: 0.7889835834503174\n",
      "Epoch 10990: Training Loss: 0.1050014818708102 Validation Loss: 0.7882983684539795\n",
      "Epoch 10991: Training Loss: 0.10449488212664922 Validation Loss: 0.7880181074142456\n",
      "Epoch 10992: Training Loss: 0.10558299471934636 Validation Loss: 0.78785640001297\n",
      "Epoch 10993: Training Loss: 0.10466960072517395 Validation Loss: 0.7886414527893066\n",
      "Epoch 10994: Training Loss: 0.10487968722979228 Validation Loss: 0.7889971733093262\n",
      "Epoch 10995: Training Loss: 0.10469719519217809 Validation Loss: 0.7887005805969238\n",
      "Epoch 10996: Training Loss: 0.10469839721918106 Validation Loss: 0.7891893982887268\n",
      "Epoch 10997: Training Loss: 0.10455754399299622 Validation Loss: 0.7887822985649109\n",
      "Epoch 10998: Training Loss: 0.10448200752337773 Validation Loss: 0.7883134484291077\n",
      "Epoch 10999: Training Loss: 0.10509088387091954 Validation Loss: 0.7889698147773743\n",
      "Epoch 11000: Training Loss: 0.1045973002910614 Validation Loss: 0.7889561653137207\n",
      "Epoch 11001: Training Loss: 0.10444305837154388 Validation Loss: 0.7890437245368958\n",
      "Epoch 11002: Training Loss: 0.10451741019884746 Validation Loss: 0.7886908054351807\n",
      "Epoch 11003: Training Loss: 0.1045714020729065 Validation Loss: 0.7886236310005188\n",
      "Epoch 11004: Training Loss: 0.10400703052679698 Validation Loss: 0.7886432409286499\n",
      "Epoch 11005: Training Loss: 0.1043200393517812 Validation Loss: 0.78824383020401\n",
      "Epoch 11006: Training Loss: 0.10476319243510564 Validation Loss: 0.7879489064216614\n",
      "Epoch 11007: Training Loss: 0.10475744058688481 Validation Loss: 0.7878495454788208\n",
      "Epoch 11008: Training Loss: 0.10472274323304494 Validation Loss: 0.7880726456642151\n",
      "Epoch 11009: Training Loss: 0.10451509306828181 Validation Loss: 0.7888875603675842\n",
      "Epoch 11010: Training Loss: 0.10512276738882065 Validation Loss: 0.7906676530838013\n",
      "Epoch 11011: Training Loss: 0.10435174405574799 Validation Loss: 0.790743887424469\n",
      "Epoch 11012: Training Loss: 0.10451201597849528 Validation Loss: 0.7893972992897034\n",
      "Epoch 11013: Training Loss: 0.10432435820500056 Validation Loss: 0.7892360091209412\n",
      "Epoch 11014: Training Loss: 0.10454416523377101 Validation Loss: 0.7889714241027832\n",
      "Epoch 11015: Training Loss: 0.10468123356501262 Validation Loss: 0.7890860438346863\n",
      "Epoch 11016: Training Loss: 0.10453744481007259 Validation Loss: 0.7886576056480408\n",
      "Epoch 11017: Training Loss: 0.10454734414815903 Validation Loss: 0.7880173325538635\n",
      "Epoch 11018: Training Loss: 0.10448047518730164 Validation Loss: 0.7880335450172424\n",
      "Epoch 11019: Training Loss: 0.10418804734945297 Validation Loss: 0.7885188460350037\n",
      "Epoch 11020: Training Loss: 0.10491471489270528 Validation Loss: 0.7893192768096924\n",
      "Epoch 11021: Training Loss: 0.10452780624230702 Validation Loss: 0.7896283268928528\n",
      "Epoch 11022: Training Loss: 0.10431964943806331 Validation Loss: 0.7897674441337585\n",
      "Epoch 11023: Training Loss: 0.10444457580645879 Validation Loss: 0.7887616753578186\n",
      "Epoch 11024: Training Loss: 0.10450634111960729 Validation Loss: 0.7894773483276367\n",
      "Epoch 11025: Training Loss: 0.1056325634320577 Validation Loss: 0.789527177810669\n",
      "Epoch 11026: Training Loss: 0.10476430505514145 Validation Loss: 0.7892905473709106\n",
      "Epoch 11027: Training Loss: 0.10459136714537938 Validation Loss: 0.7882486581802368\n",
      "Epoch 11028: Training Loss: 0.10455501576264699 Validation Loss: 0.7884504199028015\n",
      "Epoch 11029: Training Loss: 0.1052107314268748 Validation Loss: 0.7894771695137024\n",
      "Epoch 11030: Training Loss: 0.1048978865146637 Validation Loss: 0.7899813652038574\n",
      "Epoch 11031: Training Loss: 0.10451735556125641 Validation Loss: 0.7895789742469788\n",
      "Epoch 11032: Training Loss: 0.10430848846832912 Validation Loss: 0.7892054915428162\n",
      "Epoch 11033: Training Loss: 0.10493524372577667 Validation Loss: 0.7896185517311096\n",
      "Epoch 11034: Training Loss: 0.10421360780795415 Validation Loss: 0.7893733382225037\n",
      "Epoch 11035: Training Loss: 0.10411608219146729 Validation Loss: 0.7891163229942322\n",
      "Epoch 11036: Training Loss: 0.10459136962890625 Validation Loss: 0.7881675958633423\n",
      "Epoch 11037: Training Loss: 0.10457479705413182 Validation Loss: 0.7900910377502441\n",
      "Epoch 11038: Training Loss: 0.10432993372281392 Validation Loss: 0.7905078530311584\n",
      "Epoch 11039: Training Loss: 0.1043888380130132 Validation Loss: 0.7903492450714111\n",
      "Epoch 11040: Training Loss: 0.10444786647955577 Validation Loss: 0.7899740934371948\n",
      "Epoch 11041: Training Loss: 0.10464634249607722 Validation Loss: 0.7896075248718262\n",
      "Epoch 11042: Training Loss: 0.1043819064895312 Validation Loss: 0.7892683744430542\n",
      "Epoch 11043: Training Loss: 0.10415516793727875 Validation Loss: 0.7891263961791992\n",
      "Epoch 11044: Training Loss: 0.10454258074363072 Validation Loss: 0.7897392511367798\n",
      "Epoch 11045: Training Loss: 0.10431402673323949 Validation Loss: 0.7893190383911133\n",
      "Epoch 11046: Training Loss: 0.10427996267875035 Validation Loss: 0.7895243167877197\n",
      "Epoch 11047: Training Loss: 0.10435629884401958 Validation Loss: 0.789899468421936\n",
      "Epoch 11048: Training Loss: 0.10467351973056793 Validation Loss: 0.7897056937217712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11049: Training Loss: 0.10437567283709843 Validation Loss: 0.7903706431388855\n",
      "Epoch 11050: Training Loss: 0.1046154722571373 Validation Loss: 0.7902870178222656\n",
      "Epoch 11051: Training Loss: 0.10449293007453282 Validation Loss: 0.7895457148551941\n",
      "Epoch 11052: Training Loss: 0.1043510635693868 Validation Loss: 0.7894341945648193\n",
      "Epoch 11053: Training Loss: 0.10417016098896663 Validation Loss: 0.7892867922782898\n",
      "Epoch 11054: Training Loss: 0.10406715919574101 Validation Loss: 0.7891052961349487\n",
      "Epoch 11055: Training Loss: 0.10441907246907552 Validation Loss: 0.7887447476387024\n",
      "Epoch 11056: Training Loss: 0.1045483946800232 Validation Loss: 0.7891141176223755\n",
      "Epoch 11057: Training Loss: 0.10455086827278137 Validation Loss: 0.7890176177024841\n",
      "Epoch 11058: Training Loss: 0.10424697895844777 Validation Loss: 0.7888790965080261\n",
      "Epoch 11059: Training Loss: 0.10406407962242763 Validation Loss: 0.788642406463623\n",
      "Epoch 11060: Training Loss: 0.1045704260468483 Validation Loss: 0.7892158031463623\n",
      "Epoch 11061: Training Loss: 0.10420478383700053 Validation Loss: 0.7891013622283936\n",
      "Epoch 11062: Training Loss: 0.10436973969141643 Validation Loss: 0.789322018623352\n",
      "Epoch 11063: Training Loss: 0.10418383777141571 Validation Loss: 0.7904067635536194\n",
      "Epoch 11064: Training Loss: 0.10454009970029195 Validation Loss: 0.7892494797706604\n",
      "Epoch 11065: Training Loss: 0.10438605397939682 Validation Loss: 0.789235532283783\n",
      "Epoch 11066: Training Loss: 0.10451388110717137 Validation Loss: 0.7900081276893616\n",
      "Epoch 11067: Training Loss: 0.10396085679531097 Validation Loss: 0.7902123928070068\n",
      "Epoch 11068: Training Loss: 0.1042718415458997 Validation Loss: 0.7901502847671509\n",
      "Epoch 11069: Training Loss: 0.1041050652662913 Validation Loss: 0.7895276546478271\n",
      "Epoch 11070: Training Loss: 0.10513762633005778 Validation Loss: 0.7903538942337036\n",
      "Epoch 11071: Training Loss: 0.10374112923940022 Validation Loss: 0.7893943190574646\n",
      "Epoch 11072: Training Loss: 0.10423330465952556 Validation Loss: 0.7897186875343323\n",
      "Epoch 11073: Training Loss: 0.10434892276922862 Validation Loss: 0.7904048562049866\n",
      "Epoch 11074: Training Loss: 0.104410024980704 Validation Loss: 0.7902488112449646\n",
      "Epoch 11075: Training Loss: 0.10412787149349849 Validation Loss: 0.7900527715682983\n",
      "Epoch 11076: Training Loss: 0.10435022165377934 Validation Loss: 0.7889252305030823\n",
      "Epoch 11077: Training Loss: 0.10428682714700699 Validation Loss: 0.7885720729827881\n",
      "Epoch 11078: Training Loss: 0.10411358873049419 Validation Loss: 0.7886877655982971\n",
      "Epoch 11079: Training Loss: 0.1041240468621254 Validation Loss: 0.788692057132721\n",
      "Epoch 11080: Training Loss: 0.10426538437604904 Validation Loss: 0.7905067205429077\n",
      "Epoch 11081: Training Loss: 0.10445799430211385 Validation Loss: 0.7901594638824463\n",
      "Epoch 11082: Training Loss: 0.10453630238771439 Validation Loss: 0.7903735637664795\n",
      "Epoch 11083: Training Loss: 0.10438928008079529 Validation Loss: 0.7897590398788452\n",
      "Epoch 11084: Training Loss: 0.10402062038580577 Validation Loss: 0.7894213795661926\n",
      "Epoch 11085: Training Loss: 0.10410753389199574 Validation Loss: 0.7883584499359131\n",
      "Epoch 11086: Training Loss: 0.10400524983803432 Validation Loss: 0.7893207669258118\n",
      "Epoch 11087: Training Loss: 0.10407935082912445 Validation Loss: 0.7898297309875488\n",
      "Epoch 11088: Training Loss: 0.10431348532438278 Validation Loss: 0.7892813086509705\n",
      "Epoch 11089: Training Loss: 0.10405667374531428 Validation Loss: 0.7900682687759399\n",
      "Epoch 11090: Training Loss: 0.10425029198328654 Validation Loss: 0.7900871634483337\n",
      "Epoch 11091: Training Loss: 0.10488028079271317 Validation Loss: 0.7896063923835754\n",
      "Epoch 11092: Training Loss: 0.10391363749901454 Validation Loss: 0.7898972034454346\n",
      "Epoch 11093: Training Loss: 0.10477925091981888 Validation Loss: 0.7902727127075195\n",
      "Epoch 11094: Training Loss: 0.10410585502783458 Validation Loss: 0.7912964820861816\n",
      "Epoch 11095: Training Loss: 0.10418296853701274 Validation Loss: 0.7910139560699463\n",
      "Epoch 11096: Training Loss: 0.10405515382687251 Validation Loss: 0.7899011969566345\n",
      "Epoch 11097: Training Loss: 0.10408111661672592 Validation Loss: 0.7901345491409302\n",
      "Epoch 11098: Training Loss: 0.10397074371576309 Validation Loss: 0.7900571227073669\n",
      "Epoch 11099: Training Loss: 0.10446951538324356 Validation Loss: 0.7897183299064636\n",
      "Epoch 11100: Training Loss: 0.10369424273570378 Validation Loss: 0.7896947264671326\n",
      "Epoch 11101: Training Loss: 0.10438007364670436 Validation Loss: 0.7888038754463196\n",
      "Epoch 11102: Training Loss: 0.10414468497037888 Validation Loss: 0.7886917591094971\n",
      "Epoch 11103: Training Loss: 0.1037962610522906 Validation Loss: 0.7891395092010498\n",
      "Epoch 11104: Training Loss: 0.10423311591148376 Validation Loss: 0.789422333240509\n",
      "Epoch 11105: Training Loss: 0.10406162838141124 Validation Loss: 0.7899110913276672\n",
      "Epoch 11106: Training Loss: 0.104205255707105 Validation Loss: 0.7903196811676025\n",
      "Epoch 11107: Training Loss: 0.10389097779989243 Validation Loss: 0.7895258665084839\n",
      "Epoch 11108: Training Loss: 0.10408766071001689 Validation Loss: 0.789412260055542\n",
      "Epoch 11109: Training Loss: 0.10465913265943527 Validation Loss: 0.789878785610199\n",
      "Epoch 11110: Training Loss: 0.10401453326145808 Validation Loss: 0.7898611426353455\n",
      "Epoch 11111: Training Loss: 0.10384486118952434 Validation Loss: 0.7893962860107422\n",
      "Epoch 11112: Training Loss: 0.1040509765346845 Validation Loss: 0.7890809178352356\n",
      "Epoch 11113: Training Loss: 0.10388395190238953 Validation Loss: 0.7896643280982971\n",
      "Epoch 11114: Training Loss: 0.10412536809841792 Validation Loss: 0.7909716963768005\n",
      "Epoch 11115: Training Loss: 0.10418259352445602 Validation Loss: 0.7912863492965698\n",
      "Epoch 11116: Training Loss: 0.1039029061794281 Validation Loss: 0.7905102372169495\n",
      "Epoch 11117: Training Loss: 0.10391002396742503 Validation Loss: 0.7901790142059326\n",
      "Epoch 11118: Training Loss: 0.10412863393624623 Validation Loss: 0.7895100116729736\n",
      "Epoch 11119: Training Loss: 0.10385726392269135 Validation Loss: 0.7899577617645264\n",
      "Epoch 11120: Training Loss: 0.10414520651102066 Validation Loss: 0.7908270955085754\n",
      "Epoch 11121: Training Loss: 0.10406833638747533 Validation Loss: 0.7910753488540649\n",
      "Epoch 11122: Training Loss: 0.10394613941510518 Validation Loss: 0.7913074493408203\n",
      "Epoch 11123: Training Loss: 0.1033322811126709 Validation Loss: 0.7916809320449829\n",
      "Epoch 11124: Training Loss: 0.10414992024501164 Validation Loss: 0.7909185886383057\n",
      "Epoch 11125: Training Loss: 0.10479732106129329 Validation Loss: 0.7899277806282043\n",
      "Epoch 11126: Training Loss: 0.10464602708816528 Validation Loss: 0.7894595265388489\n",
      "Epoch 11127: Training Loss: 0.10483964780966441 Validation Loss: 0.788861870765686\n",
      "Epoch 11128: Training Loss: 0.10398368785778682 Validation Loss: 0.7907236218452454\n",
      "Epoch 11129: Training Loss: 0.10401633381843567 Validation Loss: 0.7913020849227905\n",
      "Epoch 11130: Training Loss: 0.1038719043135643 Validation Loss: 0.7906504273414612\n",
      "Epoch 11131: Training Loss: 0.10416121035814285 Validation Loss: 0.7909135818481445\n",
      "Epoch 11132: Training Loss: 0.1039395605524381 Validation Loss: 0.7901833653450012\n",
      "Epoch 11133: Training Loss: 0.10402848819891612 Validation Loss: 0.7904210090637207\n",
      "Epoch 11134: Training Loss: 0.10443385938803355 Validation Loss: 0.7910292148590088\n",
      "Epoch 11135: Training Loss: 0.10352221876382828 Validation Loss: 0.7912889122962952\n",
      "Epoch 11136: Training Loss: 0.1036118393143018 Validation Loss: 0.7904621958732605\n",
      "Epoch 11137: Training Loss: 0.10384920487801234 Validation Loss: 0.7895768284797668\n",
      "Epoch 11138: Training Loss: 0.10374518732229869 Validation Loss: 0.7894552946090698\n",
      "Epoch 11139: Training Loss: 0.10377496232589085 Validation Loss: 0.7895737886428833\n",
      "Epoch 11140: Training Loss: 0.10341594864924748 Validation Loss: 0.7892267107963562\n",
      "Epoch 11141: Training Loss: 0.10391133526961009 Validation Loss: 0.7900672554969788\n",
      "Epoch 11142: Training Loss: 0.10347235451141994 Validation Loss: 0.7904360890388489\n",
      "Epoch 11143: Training Loss: 0.1036164363225301 Validation Loss: 0.7909096479415894\n",
      "Epoch 11144: Training Loss: 0.10390438387791316 Validation Loss: 0.7914729714393616\n",
      "Epoch 11145: Training Loss: 0.10368221749862035 Validation Loss: 0.7909754514694214\n",
      "Epoch 11146: Training Loss: 0.10371625423431396 Validation Loss: 0.7898972034454346\n",
      "Epoch 11147: Training Loss: 0.10460464656352997 Validation Loss: 0.790272057056427\n",
      "Epoch 11148: Training Loss: 0.10451605667670567 Validation Loss: 0.7900890111923218\n",
      "Epoch 11149: Training Loss: 0.10423245280981064 Validation Loss: 0.7916278839111328\n",
      "Epoch 11150: Training Loss: 0.10423010836044948 Validation Loss: 0.7910506725311279\n",
      "Epoch 11151: Training Loss: 0.10367966443300247 Validation Loss: 0.7911679148674011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11152: Training Loss: 0.10367895662784576 Validation Loss: 0.7905567288398743\n",
      "Epoch 11153: Training Loss: 0.10473150263230006 Validation Loss: 0.7899630069732666\n",
      "Epoch 11154: Training Loss: 0.10416993747154872 Validation Loss: 0.7902362942695618\n",
      "Epoch 11155: Training Loss: 0.10375372568766277 Validation Loss: 0.7896188497543335\n",
      "Epoch 11156: Training Loss: 0.10382116834322612 Validation Loss: 0.7905756235122681\n",
      "Epoch 11157: Training Loss: 0.10385783761739731 Validation Loss: 0.7905248403549194\n",
      "Epoch 11158: Training Loss: 0.10389533638954163 Validation Loss: 0.7917822003364563\n",
      "Epoch 11159: Training Loss: 0.1035696342587471 Validation Loss: 0.7927302122116089\n",
      "Epoch 11160: Training Loss: 0.1033138061563174 Validation Loss: 0.7923710346221924\n",
      "Epoch 11161: Training Loss: 0.103681946794192 Validation Loss: 0.790995180606842\n",
      "Epoch 11162: Training Loss: 0.10374953597784042 Validation Loss: 0.7908543348312378\n",
      "Epoch 11163: Training Loss: 0.10388891647259395 Validation Loss: 0.7909048199653625\n",
      "Epoch 11164: Training Loss: 0.10417335977156957 Validation Loss: 0.7909755706787109\n",
      "Epoch 11165: Training Loss: 0.10362342745065689 Validation Loss: 0.7906216382980347\n",
      "Epoch 11166: Training Loss: 0.10388891150554021 Validation Loss: 0.7902011275291443\n",
      "Epoch 11167: Training Loss: 0.10378558437029521 Validation Loss: 0.7911158800125122\n",
      "Epoch 11168: Training Loss: 0.10392378518978755 Validation Loss: 0.790757954120636\n",
      "Epoch 11169: Training Loss: 0.10339168459177017 Validation Loss: 0.7917358875274658\n",
      "Epoch 11170: Training Loss: 0.10350722322861354 Validation Loss: 0.7909731864929199\n",
      "Epoch 11171: Training Loss: 0.10428773363431294 Validation Loss: 0.7904258370399475\n",
      "Epoch 11172: Training Loss: 0.10350238531827927 Validation Loss: 0.7904177308082581\n",
      "Epoch 11173: Training Loss: 0.1039248729745547 Validation Loss: 0.7900124788284302\n",
      "Epoch 11174: Training Loss: 0.10433151572942734 Validation Loss: 0.7896396517753601\n",
      "Epoch 11175: Training Loss: 0.10359599192937215 Validation Loss: 0.7894017696380615\n",
      "Epoch 11176: Training Loss: 0.10351494203011195 Validation Loss: 0.7900162935256958\n",
      "Epoch 11177: Training Loss: 0.10357205073038737 Validation Loss: 0.7915118336677551\n",
      "Epoch 11178: Training Loss: 0.10347820073366165 Validation Loss: 0.7927225828170776\n",
      "Epoch 11179: Training Loss: 0.1035541519522667 Validation Loss: 0.7930665612220764\n",
      "Epoch 11180: Training Loss: 0.10353764643271764 Validation Loss: 0.7922800183296204\n",
      "Epoch 11181: Training Loss: 0.10362692177295685 Validation Loss: 0.7911561131477356\n",
      "Epoch 11182: Training Loss: 0.103752334912618 Validation Loss: 0.7899249792098999\n",
      "Epoch 11183: Training Loss: 0.10358098397652309 Validation Loss: 0.7897502183914185\n",
      "Epoch 11184: Training Loss: 0.1039384553829829 Validation Loss: 0.7903227210044861\n",
      "Epoch 11185: Training Loss: 0.10349573443333308 Validation Loss: 0.7904455661773682\n",
      "Epoch 11186: Training Loss: 0.10342006633679073 Validation Loss: 0.790835440158844\n",
      "Epoch 11187: Training Loss: 0.10345407327016194 Validation Loss: 0.7910482883453369\n",
      "Epoch 11188: Training Loss: 0.104092242817084 Validation Loss: 0.792794942855835\n",
      "Epoch 11189: Training Loss: 0.10365642855564754 Validation Loss: 0.7917928099632263\n",
      "Epoch 11190: Training Loss: 0.10427031914393108 Validation Loss: 0.7903135418891907\n",
      "Epoch 11191: Training Loss: 0.10340843598047893 Validation Loss: 0.7900161147117615\n",
      "Epoch 11192: Training Loss: 0.10323434074719746 Validation Loss: 0.7905561923980713\n",
      "Epoch 11193: Training Loss: 0.10330519825220108 Validation Loss: 0.7915407419204712\n",
      "Epoch 11194: Training Loss: 0.10373345762491226 Validation Loss: 0.791668176651001\n",
      "Epoch 11195: Training Loss: 0.10344863931337993 Validation Loss: 0.792755126953125\n",
      "Epoch 11196: Training Loss: 0.10384315500656764 Validation Loss: 0.7918375730514526\n",
      "Epoch 11197: Training Loss: 0.10343589882055919 Validation Loss: 0.7918662428855896\n",
      "Epoch 11198: Training Loss: 0.10351929316918056 Validation Loss: 0.7916578650474548\n",
      "Epoch 11199: Training Loss: 0.10365439454714458 Validation Loss: 0.7920581102371216\n",
      "Epoch 11200: Training Loss: 0.10369541496038437 Validation Loss: 0.7905085682868958\n",
      "Epoch 11201: Training Loss: 0.10339819143215816 Validation Loss: 0.7909461855888367\n",
      "Epoch 11202: Training Loss: 0.10350253184636433 Validation Loss: 0.7908214330673218\n",
      "Epoch 11203: Training Loss: 0.10338916877905528 Validation Loss: 0.7908538579940796\n",
      "Epoch 11204: Training Loss: 0.10358796020348866 Validation Loss: 0.7917636036872864\n",
      "Epoch 11205: Training Loss: 0.10369223604599635 Validation Loss: 0.7916669249534607\n",
      "Epoch 11206: Training Loss: 0.10357851535081863 Validation Loss: 0.7909026741981506\n",
      "Epoch 11207: Training Loss: 0.1031790201862653 Validation Loss: 0.7901579737663269\n",
      "Epoch 11208: Training Loss: 0.10365218172470729 Validation Loss: 0.7913955450057983\n",
      "Epoch 11209: Training Loss: 0.10337044547001521 Validation Loss: 0.7915455102920532\n",
      "Epoch 11210: Training Loss: 0.10334767401218414 Validation Loss: 0.7918041944503784\n",
      "Epoch 11211: Training Loss: 0.10351171841224034 Validation Loss: 0.7915008664131165\n",
      "Epoch 11212: Training Loss: 0.10368188967307408 Validation Loss: 0.7917971611022949\n",
      "Epoch 11213: Training Loss: 0.1032569408416748 Validation Loss: 0.7914787530899048\n",
      "Epoch 11214: Training Loss: 0.10340951383113861 Validation Loss: 0.7901532649993896\n",
      "Epoch 11215: Training Loss: 0.10361878822247188 Validation Loss: 0.7900390625\n",
      "Epoch 11216: Training Loss: 0.10325708736975987 Validation Loss: 0.7900790572166443\n",
      "Epoch 11217: Training Loss: 0.10371460268894832 Validation Loss: 0.7905178666114807\n",
      "Epoch 11218: Training Loss: 0.10304537167151769 Validation Loss: 0.7917290329933167\n",
      "Epoch 11219: Training Loss: 0.10332037508487701 Validation Loss: 0.7923115491867065\n",
      "Epoch 11220: Training Loss: 0.1032697856426239 Validation Loss: 0.7926209568977356\n",
      "Epoch 11221: Training Loss: 0.10310765107472737 Validation Loss: 0.7924070358276367\n",
      "Epoch 11222: Training Loss: 0.10357055564721425 Validation Loss: 0.791122555732727\n",
      "Epoch 11223: Training Loss: 0.10353424896796544 Validation Loss: 0.7912967801094055\n",
      "Epoch 11224: Training Loss: 0.10337305565675099 Validation Loss: 0.7919523119926453\n",
      "Epoch 11225: Training Loss: 0.10332357635100682 Validation Loss: 0.7916569709777832\n",
      "Epoch 11226: Training Loss: 0.1032856007417043 Validation Loss: 0.791799008846283\n",
      "Epoch 11227: Training Loss: 0.103277621169885 Validation Loss: 0.7916423678398132\n",
      "Epoch 11228: Training Loss: 0.1030778909722964 Validation Loss: 0.791381299495697\n",
      "Epoch 11229: Training Loss: 0.10326191782951355 Validation Loss: 0.7917799353599548\n",
      "Epoch 11230: Training Loss: 0.10333674401044846 Validation Loss: 0.7917647361755371\n",
      "Epoch 11231: Training Loss: 0.10328883677721024 Validation Loss: 0.7920536398887634\n",
      "Epoch 11232: Training Loss: 0.10382752617200215 Validation Loss: 0.7915925979614258\n",
      "Epoch 11233: Training Loss: 0.10328534990549088 Validation Loss: 0.7912027835845947\n",
      "Epoch 11234: Training Loss: 0.1036503439148267 Validation Loss: 0.7911675572395325\n",
      "Epoch 11235: Training Loss: 0.10323164860407512 Validation Loss: 0.791790246963501\n",
      "Epoch 11236: Training Loss: 0.1032164494196574 Validation Loss: 0.7916446924209595\n",
      "Epoch 11237: Training Loss: 0.10350596904754639 Validation Loss: 0.790230393409729\n",
      "Epoch 11238: Training Loss: 0.10315887133280437 Validation Loss: 0.7900978326797485\n",
      "Epoch 11239: Training Loss: 0.10335071633259456 Validation Loss: 0.7911310195922852\n",
      "Epoch 11240: Training Loss: 0.103401484588782 Validation Loss: 0.7923787832260132\n",
      "Epoch 11241: Training Loss: 0.10416962206363678 Validation Loss: 0.7928944826126099\n",
      "Epoch 11242: Training Loss: 0.10366717725992203 Validation Loss: 0.7931617498397827\n",
      "Epoch 11243: Training Loss: 0.10337818662325542 Validation Loss: 0.7927501797676086\n",
      "Epoch 11244: Training Loss: 0.1030530979235967 Validation Loss: 0.7921698093414307\n",
      "Epoch 11245: Training Loss: 0.1032990242044131 Validation Loss: 0.7915862798690796\n",
      "Epoch 11246: Training Loss: 0.10309107353289922 Validation Loss: 0.7912734746932983\n",
      "Epoch 11247: Training Loss: 0.10330980271100998 Validation Loss: 0.791588544845581\n",
      "Epoch 11248: Training Loss: 0.10331043849388759 Validation Loss: 0.7917653322219849\n",
      "Epoch 11249: Training Loss: 0.10350769758224487 Validation Loss: 0.7911421060562134\n",
      "Epoch 11250: Training Loss: 0.10426544398069382 Validation Loss: 0.7914307713508606\n",
      "Epoch 11251: Training Loss: 0.10321372002363205 Validation Loss: 0.7926445007324219\n",
      "Epoch 11252: Training Loss: 0.10287601500749588 Validation Loss: 0.7925776243209839\n",
      "Epoch 11253: Training Loss: 0.10333304107189178 Validation Loss: 0.7931497097015381\n",
      "Epoch 11254: Training Loss: 0.10303124288717906 Validation Loss: 0.7922639846801758\n",
      "Epoch 11255: Training Loss: 0.10317019124825795 Validation Loss: 0.7916334271430969\n",
      "Epoch 11256: Training Loss: 0.10307670136292775 Validation Loss: 0.791394054889679\n",
      "Epoch 11257: Training Loss: 0.10424670080343883 Validation Loss: 0.7925564646720886\n",
      "Epoch 11258: Training Loss: 0.10301241527001063 Validation Loss: 0.7919530868530273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11259: Training Loss: 0.10321824004252751 Validation Loss: 0.7925807237625122\n",
      "Epoch 11260: Training Loss: 0.10293669998645782 Validation Loss: 0.7922449707984924\n",
      "Epoch 11261: Training Loss: 0.10299242287874222 Validation Loss: 0.7917525172233582\n",
      "Epoch 11262: Training Loss: 0.1034851794441541 Validation Loss: 0.7924841046333313\n",
      "Epoch 11263: Training Loss: 0.10293635229269664 Validation Loss: 0.7926086783409119\n",
      "Epoch 11264: Training Loss: 0.10350107898314793 Validation Loss: 0.7922155857086182\n",
      "Epoch 11265: Training Loss: 0.10375167926152547 Validation Loss: 0.7920882701873779\n",
      "Epoch 11266: Training Loss: 0.10405256350835164 Validation Loss: 0.7918719053268433\n",
      "Epoch 11267: Training Loss: 0.10351693878571193 Validation Loss: 0.7923739552497864\n",
      "Epoch 11268: Training Loss: 0.10297008355458577 Validation Loss: 0.7925001978874207\n",
      "Epoch 11269: Training Loss: 0.1037588616212209 Validation Loss: 0.7913951277732849\n",
      "Epoch 11270: Training Loss: 0.1031370759010315 Validation Loss: 0.7917290925979614\n",
      "Epoch 11271: Training Loss: 0.10320636630058289 Validation Loss: 0.7924332618713379\n",
      "Epoch 11272: Training Loss: 0.10306777556737264 Validation Loss: 0.7910826802253723\n",
      "Epoch 11273: Training Loss: 0.10329535106817882 Validation Loss: 0.7915675044059753\n",
      "Epoch 11274: Training Loss: 0.10302131126324336 Validation Loss: 0.7920625805854797\n",
      "Epoch 11275: Training Loss: 0.10323963314294815 Validation Loss: 0.7932772636413574\n",
      "Epoch 11276: Training Loss: 0.10303813219070435 Validation Loss: 0.7930944561958313\n",
      "Epoch 11277: Training Loss: 0.10302171607812245 Validation Loss: 0.793229341506958\n",
      "Epoch 11278: Training Loss: 0.1029253179828326 Validation Loss: 0.7930084466934204\n",
      "Epoch 11279: Training Loss: 0.10312252988417943 Validation Loss: 0.792064368724823\n",
      "Epoch 11280: Training Loss: 0.10303694258133571 Validation Loss: 0.7923370599746704\n",
      "Epoch 11281: Training Loss: 0.10282156864802043 Validation Loss: 0.7923662066459656\n",
      "Epoch 11282: Training Loss: 0.1028527319431305 Validation Loss: 0.791853666305542\n",
      "Epoch 11283: Training Loss: 0.10292308529218037 Validation Loss: 0.7928261756896973\n",
      "Epoch 11284: Training Loss: 0.10291695594787598 Validation Loss: 0.793107271194458\n",
      "Epoch 11285: Training Loss: 0.10302817573149999 Validation Loss: 0.7914958596229553\n",
      "Epoch 11286: Training Loss: 0.10378548751274745 Validation Loss: 0.7914671897888184\n",
      "Epoch 11287: Training Loss: 0.10309443871180217 Validation Loss: 0.7912389636039734\n",
      "Epoch 11288: Training Loss: 0.10356037318706512 Validation Loss: 0.7920467853546143\n",
      "Epoch 11289: Training Loss: 0.10280248522758484 Validation Loss: 0.7926957011222839\n",
      "Epoch 11290: Training Loss: 0.10313254843155543 Validation Loss: 0.793374240398407\n",
      "Epoch 11291: Training Loss: 0.10369504243135452 Validation Loss: 0.793225109577179\n",
      "Epoch 11292: Training Loss: 0.10295403003692627 Validation Loss: 0.7928952574729919\n",
      "Epoch 11293: Training Loss: 0.1030443509419759 Validation Loss: 0.7921516299247742\n",
      "Epoch 11294: Training Loss: 0.10320734977722168 Validation Loss: 0.7918785214424133\n",
      "Epoch 11295: Training Loss: 0.10265408953030904 Validation Loss: 0.7922223806381226\n",
      "Epoch 11296: Training Loss: 0.10288695494333903 Validation Loss: 0.7925279140472412\n",
      "Epoch 11297: Training Loss: 0.1030303214987119 Validation Loss: 0.7920194268226624\n",
      "Epoch 11298: Training Loss: 0.10319419950246811 Validation Loss: 0.7928943037986755\n",
      "Epoch 11299: Training Loss: 0.10343185315529506 Validation Loss: 0.7918241024017334\n",
      "Epoch 11300: Training Loss: 0.10316452880700429 Validation Loss: 0.7934693098068237\n",
      "Epoch 11301: Training Loss: 0.10292970140775044 Validation Loss: 0.7941411137580872\n",
      "Epoch 11302: Training Loss: 0.10307783136765163 Validation Loss: 0.7934709787368774\n",
      "Epoch 11303: Training Loss: 0.10350498557090759 Validation Loss: 0.7917189002037048\n",
      "Epoch 11304: Training Loss: 0.10310163100560506 Validation Loss: 0.791933000087738\n",
      "Epoch 11305: Training Loss: 0.10371666898330052 Validation Loss: 0.7907036542892456\n",
      "Epoch 11306: Training Loss: 0.10354143381118774 Validation Loss: 0.791342556476593\n",
      "Epoch 11307: Training Loss: 0.10323948661486308 Validation Loss: 0.7926804423332214\n",
      "Epoch 11308: Training Loss: 0.10285065819819768 Validation Loss: 0.7930037379264832\n",
      "Epoch 11309: Training Loss: 0.10324090719223022 Validation Loss: 0.7923535108566284\n",
      "Epoch 11310: Training Loss: 0.10305189838012059 Validation Loss: 0.7924719452857971\n",
      "Epoch 11311: Training Loss: 0.10338789224624634 Validation Loss: 0.7929074168205261\n",
      "Epoch 11312: Training Loss: 0.10279644032319386 Validation Loss: 0.7930619120597839\n",
      "Epoch 11313: Training Loss: 0.10306982696056366 Validation Loss: 0.7927995920181274\n",
      "Epoch 11314: Training Loss: 0.10329285015662511 Validation Loss: 0.7932572364807129\n",
      "Epoch 11315: Training Loss: 0.10312790175278981 Validation Loss: 0.7948489189147949\n",
      "Epoch 11316: Training Loss: 0.10276655852794647 Validation Loss: 0.7934317588806152\n",
      "Epoch 11317: Training Loss: 0.10284168769915898 Validation Loss: 0.7925097942352295\n",
      "Epoch 11318: Training Loss: 0.10269335409005483 Validation Loss: 0.7918848395347595\n",
      "Epoch 11319: Training Loss: 0.10267739246288936 Validation Loss: 0.791344940662384\n",
      "Epoch 11320: Training Loss: 0.10280454903841019 Validation Loss: 0.7910928130149841\n",
      "Epoch 11321: Training Loss: 0.1028972715139389 Validation Loss: 0.7921285033226013\n",
      "Epoch 11322: Training Loss: 0.10234118749698003 Validation Loss: 0.7925043106079102\n",
      "Epoch 11323: Training Loss: 0.1032390942176183 Validation Loss: 0.7940384149551392\n",
      "Epoch 11324: Training Loss: 0.10353779544432958 Validation Loss: 0.7934284806251526\n",
      "Epoch 11325: Training Loss: 0.1026541789372762 Validation Loss: 0.792484700679779\n",
      "Epoch 11326: Training Loss: 0.102684552470843 Validation Loss: 0.7923275232315063\n",
      "Epoch 11327: Training Loss: 0.1025058850646019 Validation Loss: 0.7926679849624634\n",
      "Epoch 11328: Training Loss: 0.10270109524329503 Validation Loss: 0.7926043272018433\n",
      "Epoch 11329: Training Loss: 0.1025603214899699 Validation Loss: 0.7925304174423218\n",
      "Epoch 11330: Training Loss: 0.10295790433883667 Validation Loss: 0.7931186556816101\n",
      "Epoch 11331: Training Loss: 0.10265013823906581 Validation Loss: 0.7929072976112366\n",
      "Epoch 11332: Training Loss: 0.1029934212565422 Validation Loss: 0.7920916676521301\n",
      "Epoch 11333: Training Loss: 0.10234000533819199 Validation Loss: 0.7919390201568604\n",
      "Epoch 11334: Training Loss: 0.1033366322517395 Validation Loss: 0.7916736006736755\n",
      "Epoch 11335: Training Loss: 0.10267464816570282 Validation Loss: 0.7924468517303467\n",
      "Epoch 11336: Training Loss: 0.10314314315716426 Validation Loss: 0.7930698990821838\n",
      "Epoch 11337: Training Loss: 0.10236411541700363 Validation Loss: 0.7939028143882751\n",
      "Epoch 11338: Training Loss: 0.10260933389266332 Validation Loss: 0.7942011952400208\n",
      "Epoch 11339: Training Loss: 0.1036926656961441 Validation Loss: 0.7939057350158691\n",
      "Epoch 11340: Training Loss: 0.10270273933808009 Validation Loss: 0.7935721278190613\n",
      "Epoch 11341: Training Loss: 0.10277957965930302 Validation Loss: 0.7940128445625305\n",
      "Epoch 11342: Training Loss: 0.10272124658028285 Validation Loss: 0.7926397323608398\n",
      "Epoch 11343: Training Loss: 0.10272236665089925 Validation Loss: 0.7932921051979065\n",
      "Epoch 11344: Training Loss: 0.10344504068295161 Validation Loss: 0.7927671074867249\n",
      "Epoch 11345: Training Loss: 0.10265028476715088 Validation Loss: 0.7923734784126282\n",
      "Epoch 11346: Training Loss: 0.1029043768843015 Validation Loss: 0.7934513688087463\n",
      "Epoch 11347: Training Loss: 0.10262184590101242 Validation Loss: 0.7935678958892822\n",
      "Epoch 11348: Training Loss: 0.10268005480368932 Validation Loss: 0.792949914932251\n",
      "Epoch 11349: Training Loss: 0.1033305252591769 Validation Loss: 0.7931694984436035\n",
      "Epoch 11350: Training Loss: 0.10333655526240666 Validation Loss: 0.7923421263694763\n",
      "Epoch 11351: Training Loss: 0.10262654225031535 Validation Loss: 0.7919848561286926\n",
      "Epoch 11352: Training Loss: 0.10258866349856059 Validation Loss: 0.7924377918243408\n",
      "Epoch 11353: Training Loss: 0.10256288448969524 Validation Loss: 0.7928014397621155\n",
      "Epoch 11354: Training Loss: 0.10258200267950694 Validation Loss: 0.7933465242385864\n",
      "Epoch 11355: Training Loss: 0.1026479775706927 Validation Loss: 0.7939087748527527\n",
      "Epoch 11356: Training Loss: 0.10247467458248138 Validation Loss: 0.7940741777420044\n",
      "Epoch 11357: Training Loss: 0.1026135931412379 Validation Loss: 0.7935445308685303\n",
      "Epoch 11358: Training Loss: 0.10274353126684825 Validation Loss: 0.7923709750175476\n",
      "Epoch 11359: Training Loss: 0.10276620835065842 Validation Loss: 0.7922886610031128\n",
      "Epoch 11360: Training Loss: 0.10249293843905131 Validation Loss: 0.7925623059272766\n",
      "Epoch 11361: Training Loss: 0.10239373644193013 Validation Loss: 0.7934601902961731\n",
      "Epoch 11362: Training Loss: 0.10265397528807323 Validation Loss: 0.7941204905509949\n",
      "Epoch 11363: Training Loss: 0.10217100878556569 Validation Loss: 0.794072687625885\n",
      "Epoch 11364: Training Loss: 0.10256598393122356 Validation Loss: 0.793476402759552\n",
      "Epoch 11365: Training Loss: 0.10247624168793361 Validation Loss: 0.792538583278656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11366: Training Loss: 0.10232443610827129 Validation Loss: 0.7932249903678894\n",
      "Epoch 11367: Training Loss: 0.1024908795952797 Validation Loss: 0.7933529019355774\n",
      "Epoch 11368: Training Loss: 0.10240981727838516 Validation Loss: 0.7930757999420166\n",
      "Epoch 11369: Training Loss: 0.10238848626613617 Validation Loss: 0.7926597595214844\n",
      "Epoch 11370: Training Loss: 0.1024669458468755 Validation Loss: 0.7930080890655518\n",
      "Epoch 11371: Training Loss: 0.1030447135368983 Validation Loss: 0.793377161026001\n",
      "Epoch 11372: Training Loss: 0.1022487183411916 Validation Loss: 0.7936540842056274\n",
      "Epoch 11373: Training Loss: 0.10251534233490626 Validation Loss: 0.7935975193977356\n",
      "Epoch 11374: Training Loss: 0.10246550291776657 Validation Loss: 0.793386697769165\n",
      "Epoch 11375: Training Loss: 0.10289680709441502 Validation Loss: 0.7926573157310486\n",
      "Epoch 11376: Training Loss: 0.10252192616462708 Validation Loss: 0.7927604913711548\n",
      "Epoch 11377: Training Loss: 0.10274853060642879 Validation Loss: 0.7933914065361023\n",
      "Epoch 11378: Training Loss: 0.10249618192513783 Validation Loss: 0.7935110926628113\n",
      "Epoch 11379: Training Loss: 0.10206207136313121 Validation Loss: 0.7939746975898743\n",
      "Epoch 11380: Training Loss: 0.10278116911649704 Validation Loss: 0.7931244969367981\n",
      "Epoch 11381: Training Loss: 0.10286534329255421 Validation Loss: 0.7931728363037109\n",
      "Epoch 11382: Training Loss: 0.10253064334392548 Validation Loss: 0.7933596968650818\n",
      "Epoch 11383: Training Loss: 0.10235735525687535 Validation Loss: 0.7933007478713989\n",
      "Epoch 11384: Training Loss: 0.10237575074036916 Validation Loss: 0.7926674485206604\n",
      "Epoch 11385: Training Loss: 0.10239264369010925 Validation Loss: 0.7935330271720886\n",
      "Epoch 11386: Training Loss: 0.10237366457780202 Validation Loss: 0.7937259078025818\n",
      "Epoch 11387: Training Loss: 0.10262801746527354 Validation Loss: 0.7940484285354614\n",
      "Epoch 11388: Training Loss: 0.10234337548414867 Validation Loss: 0.7938846349716187\n",
      "Epoch 11389: Training Loss: 0.10234895348548889 Validation Loss: 0.7939335107803345\n",
      "Epoch 11390: Training Loss: 0.10285858313242595 Validation Loss: 0.7929442524909973\n",
      "Epoch 11391: Training Loss: 0.102474940319856 Validation Loss: 0.7923994064331055\n",
      "Epoch 11392: Training Loss: 0.10251932839552562 Validation Loss: 0.7919319868087769\n",
      "Epoch 11393: Training Loss: 0.10231853773196538 Validation Loss: 0.7928143739700317\n",
      "Epoch 11394: Training Loss: 0.10242540886004765 Validation Loss: 0.7937089204788208\n",
      "Epoch 11395: Training Loss: 0.1021399274468422 Validation Loss: 0.7937682867050171\n",
      "Epoch 11396: Training Loss: 0.10242586086193721 Validation Loss: 0.7939097881317139\n",
      "Epoch 11397: Training Loss: 0.10238110522429149 Validation Loss: 0.794508159160614\n",
      "Epoch 11398: Training Loss: 0.10291742285092671 Validation Loss: 0.793842613697052\n",
      "Epoch 11399: Training Loss: 0.10232982536156972 Validation Loss: 0.7936785817146301\n",
      "Epoch 11400: Training Loss: 0.10245564828316371 Validation Loss: 0.7939702272415161\n",
      "Epoch 11401: Training Loss: 0.10240783542394638 Validation Loss: 0.7942284345626831\n",
      "Epoch 11402: Training Loss: 0.10220668464899063 Validation Loss: 0.7936602830886841\n",
      "Epoch 11403: Training Loss: 0.10228926688432693 Validation Loss: 0.7934591174125671\n",
      "Epoch 11404: Training Loss: 0.10245043784379959 Validation Loss: 0.7939164042472839\n",
      "Epoch 11405: Training Loss: 0.1020651509364446 Validation Loss: 0.7930290102958679\n",
      "Epoch 11406: Training Loss: 0.10230295360088348 Validation Loss: 0.7932494282722473\n",
      "Epoch 11407: Training Loss: 0.10229688137769699 Validation Loss: 0.7940915822982788\n",
      "Epoch 11408: Training Loss: 0.10241613785425822 Validation Loss: 0.7945019602775574\n",
      "Epoch 11409: Training Loss: 0.10243776192267735 Validation Loss: 0.7948818206787109\n",
      "Epoch 11410: Training Loss: 0.10213878999153773 Validation Loss: 0.7945893406867981\n",
      "Epoch 11411: Training Loss: 0.10230296105146408 Validation Loss: 0.7936192154884338\n",
      "Epoch 11412: Training Loss: 0.10265085597832997 Validation Loss: 0.7930842638015747\n",
      "Epoch 11413: Training Loss: 0.10279983282089233 Validation Loss: 0.7939660549163818\n",
      "Epoch 11414: Training Loss: 0.10232091943422954 Validation Loss: 0.7933304309844971\n",
      "Epoch 11415: Training Loss: 0.10206772883733113 Validation Loss: 0.7935197353363037\n",
      "Epoch 11416: Training Loss: 0.10235242297252019 Validation Loss: 0.7940207123756409\n",
      "Epoch 11417: Training Loss: 0.10310522715250652 Validation Loss: 0.7935588359832764\n",
      "Epoch 11418: Training Loss: 0.10234481344620387 Validation Loss: 0.7933518290519714\n",
      "Epoch 11419: Training Loss: 0.10222854216893514 Validation Loss: 0.7934973239898682\n",
      "Epoch 11420: Training Loss: 0.10236080239216487 Validation Loss: 0.7951769232749939\n",
      "Epoch 11421: Training Loss: 0.10249154021342595 Validation Loss: 0.7952671051025391\n",
      "Epoch 11422: Training Loss: 0.1024039809902509 Validation Loss: 0.794462263584137\n",
      "Epoch 11423: Training Loss: 0.10185963908831279 Validation Loss: 0.7947465777397156\n",
      "Epoch 11424: Training Loss: 0.10216193397839864 Validation Loss: 0.7941058874130249\n",
      "Epoch 11425: Training Loss: 0.10239453613758087 Validation Loss: 0.7933251261711121\n",
      "Epoch 11426: Training Loss: 0.10405879716078441 Validation Loss: 0.7938694953918457\n",
      "Epoch 11427: Training Loss: 0.10232678552468617 Validation Loss: 0.7935043573379517\n",
      "Epoch 11428: Training Loss: 0.10217158496379852 Validation Loss: 0.7939022779464722\n",
      "Epoch 11429: Training Loss: 0.10217630863189697 Validation Loss: 0.7935651540756226\n",
      "Epoch 11430: Training Loss: 0.10204431662956874 Validation Loss: 0.7929014563560486\n",
      "Epoch 11431: Training Loss: 0.10252000391483307 Validation Loss: 0.7934457063674927\n",
      "Epoch 11432: Training Loss: 0.10240444540977478 Validation Loss: 0.7938635349273682\n",
      "Epoch 11433: Training Loss: 0.1023402065038681 Validation Loss: 0.7949866056442261\n",
      "Epoch 11434: Training Loss: 0.10243117809295654 Validation Loss: 0.7951034903526306\n",
      "Epoch 11435: Training Loss: 0.10206981499989827 Validation Loss: 0.7946903109550476\n",
      "Epoch 11436: Training Loss: 0.10207627216974895 Validation Loss: 0.7934367656707764\n",
      "Epoch 11437: Training Loss: 0.10206111520528793 Validation Loss: 0.7934736013412476\n",
      "Epoch 11438: Training Loss: 0.1018618568778038 Validation Loss: 0.7932708263397217\n",
      "Epoch 11439: Training Loss: 0.102223073442777 Validation Loss: 0.7939547896385193\n",
      "Epoch 11440: Training Loss: 0.10253713528315227 Validation Loss: 0.7948864698410034\n",
      "Epoch 11441: Training Loss: 0.10227408508459727 Validation Loss: 0.7956495881080627\n",
      "Epoch 11442: Training Loss: 0.10245863348245621 Validation Loss: 0.794689953327179\n",
      "Epoch 11443: Training Loss: 0.1020193099975586 Validation Loss: 0.7947385311126709\n",
      "Epoch 11444: Training Loss: 0.10235617061456044 Validation Loss: 0.7942602038383484\n",
      "Epoch 11445: Training Loss: 0.10181168963511784 Validation Loss: 0.7947801947593689\n",
      "Epoch 11446: Training Loss: 0.10220053046941757 Validation Loss: 0.7935543060302734\n",
      "Epoch 11447: Training Loss: 0.10227324068546295 Validation Loss: 0.7936079502105713\n",
      "Epoch 11448: Training Loss: 0.1020690028866132 Validation Loss: 0.79399573802948\n",
      "Epoch 11449: Training Loss: 0.10223603745301564 Validation Loss: 0.7943997979164124\n",
      "Epoch 11450: Training Loss: 0.1019560694694519 Validation Loss: 0.7945418953895569\n",
      "Epoch 11451: Training Loss: 0.10197928547859192 Validation Loss: 0.7945383787155151\n",
      "Epoch 11452: Training Loss: 0.10216311117013295 Validation Loss: 0.7944988012313843\n",
      "Epoch 11453: Training Loss: 0.10277483612298965 Validation Loss: 0.7932981848716736\n",
      "Epoch 11454: Training Loss: 0.10186583548784256 Validation Loss: 0.7943091988563538\n",
      "Epoch 11455: Training Loss: 0.10205296675364177 Validation Loss: 0.794735312461853\n",
      "Epoch 11456: Training Loss: 0.1023844579855601 Validation Loss: 0.7946599721908569\n",
      "Epoch 11457: Training Loss: 0.1026868000626564 Validation Loss: 0.7957387566566467\n",
      "Epoch 11458: Training Loss: 0.10181440661350886 Validation Loss: 0.7950462698936462\n",
      "Epoch 11459: Training Loss: 0.10268454253673553 Validation Loss: 0.7935715913772583\n",
      "Epoch 11460: Training Loss: 0.10211700201034546 Validation Loss: 0.7929975986480713\n",
      "Epoch 11461: Training Loss: 0.10261937479178111 Validation Loss: 0.7944262027740479\n",
      "Epoch 11462: Training Loss: 0.10184568663438161 Validation Loss: 0.7946619391441345\n",
      "Epoch 11463: Training Loss: 0.10236829022566478 Validation Loss: 0.7945727109909058\n",
      "Epoch 11464: Training Loss: 0.10219320158163707 Validation Loss: 0.7942533493041992\n",
      "Epoch 11465: Training Loss: 0.10193744550148646 Validation Loss: 0.794296383857727\n",
      "Epoch 11466: Training Loss: 0.10191735128561656 Validation Loss: 0.7935309410095215\n",
      "Epoch 11467: Training Loss: 0.10207619269688924 Validation Loss: 0.7936471700668335\n",
      "Epoch 11468: Training Loss: 0.10225722938776016 Validation Loss: 0.793247401714325\n",
      "Epoch 11469: Training Loss: 0.10228574275970459 Validation Loss: 0.7941061854362488\n",
      "Epoch 11470: Training Loss: 0.10208453983068466 Validation Loss: 0.7945312261581421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11471: Training Loss: 0.10155364871025085 Validation Loss: 0.794429361820221\n",
      "Epoch 11472: Training Loss: 0.10182815541823705 Validation Loss: 0.7948845028877258\n",
      "Epoch 11473: Training Loss: 0.10198867321014404 Validation Loss: 0.7947894334793091\n",
      "Epoch 11474: Training Loss: 0.10192000865936279 Validation Loss: 0.7950006127357483\n",
      "Epoch 11475: Training Loss: 0.10232034077246983 Validation Loss: 0.7947260737419128\n",
      "Epoch 11476: Training Loss: 0.1018240675330162 Validation Loss: 0.7946227788925171\n",
      "Epoch 11477: Training Loss: 0.10169102996587753 Validation Loss: 0.7955158948898315\n",
      "Epoch 11478: Training Loss: 0.10222659508387248 Validation Loss: 0.7957408428192139\n",
      "Epoch 11479: Training Loss: 0.1024381344517072 Validation Loss: 0.7945206165313721\n",
      "Epoch 11480: Training Loss: 0.10219869762659073 Validation Loss: 0.7949060797691345\n",
      "Epoch 11481: Training Loss: 0.10349545627832413 Validation Loss: 0.7960007190704346\n",
      "Epoch 11482: Training Loss: 0.10218384861946106 Validation Loss: 0.7955106496810913\n",
      "Epoch 11483: Training Loss: 0.10217430939277013 Validation Loss: 0.7947248220443726\n",
      "Epoch 11484: Training Loss: 0.10174194475015004 Validation Loss: 0.7948055863380432\n",
      "Epoch 11485: Training Loss: 0.10181783139705658 Validation Loss: 0.7953724265098572\n",
      "Epoch 11486: Training Loss: 0.10191867252190907 Validation Loss: 0.7946644425392151\n",
      "Epoch 11487: Training Loss: 0.10178661594788234 Validation Loss: 0.7948620915412903\n",
      "Epoch 11488: Training Loss: 0.10180940975745519 Validation Loss: 0.7944144606590271\n",
      "Epoch 11489: Training Loss: 0.10201774040857951 Validation Loss: 0.7938424348831177\n",
      "Epoch 11490: Training Loss: 0.10226537038882573 Validation Loss: 0.7934322357177734\n",
      "Epoch 11491: Training Loss: 0.10183033595482509 Validation Loss: 0.7937582731246948\n",
      "Epoch 11492: Training Loss: 0.10184002916018169 Validation Loss: 0.794876217842102\n",
      "Epoch 11493: Training Loss: 0.10141182690858841 Validation Loss: 0.7953915596008301\n",
      "Epoch 11494: Training Loss: 0.1024075448513031 Validation Loss: 0.7955305576324463\n",
      "Epoch 11495: Training Loss: 0.10198695957660675 Validation Loss: 0.795543372631073\n",
      "Epoch 11496: Training Loss: 0.10208388914664586 Validation Loss: 0.7958869934082031\n",
      "Epoch 11497: Training Loss: 0.10165353367726009 Validation Loss: 0.7959380745887756\n",
      "Epoch 11498: Training Loss: 0.10209109385808308 Validation Loss: 0.7947416305541992\n",
      "Epoch 11499: Training Loss: 0.10286605358123779 Validation Loss: 0.7938675880432129\n",
      "Epoch 11500: Training Loss: 0.10227325558662415 Validation Loss: 0.793742299079895\n",
      "Epoch 11501: Training Loss: 0.10181271284818649 Validation Loss: 0.7944393754005432\n",
      "Epoch 11502: Training Loss: 0.10180912911891937 Validation Loss: 0.7960824370384216\n",
      "Epoch 11503: Training Loss: 0.10200318445761998 Validation Loss: 0.7963963150978088\n",
      "Epoch 11504: Training Loss: 0.10176839182774226 Validation Loss: 0.794701099395752\n",
      "Epoch 11505: Training Loss: 0.10162348796923955 Validation Loss: 0.7934430837631226\n",
      "Epoch 11506: Training Loss: 0.10163094103336334 Validation Loss: 0.7930658459663391\n",
      "Epoch 11507: Training Loss: 0.10173699011405309 Validation Loss: 0.7944015264511108\n",
      "Epoch 11508: Training Loss: 0.10151403397321701 Validation Loss: 0.7957603931427002\n",
      "Epoch 11509: Training Loss: 0.10141991327206294 Validation Loss: 0.7964783310890198\n",
      "Epoch 11510: Training Loss: 0.10222349067529042 Validation Loss: 0.7959522008895874\n",
      "Epoch 11511: Training Loss: 0.10163767635822296 Validation Loss: 0.795418918132782\n",
      "Epoch 11512: Training Loss: 0.10198104878266652 Validation Loss: 0.7959700226783752\n",
      "Epoch 11513: Training Loss: 0.10216334213813145 Validation Loss: 0.7951307892799377\n",
      "Epoch 11514: Training Loss: 0.10180796434481938 Validation Loss: 0.7945889830589294\n",
      "Epoch 11515: Training Loss: 0.10125032564004262 Validation Loss: 0.7956879734992981\n",
      "Epoch 11516: Training Loss: 0.10155184318621953 Validation Loss: 0.7962129712104797\n",
      "Epoch 11517: Training Loss: 0.10219686478376389 Validation Loss: 0.7946920990943909\n",
      "Epoch 11518: Training Loss: 0.10180989404519399 Validation Loss: 0.7942659854888916\n",
      "Epoch 11519: Training Loss: 0.1018228754401207 Validation Loss: 0.7954789996147156\n",
      "Epoch 11520: Training Loss: 0.10151011993487676 Validation Loss: 0.7950863838195801\n",
      "Epoch 11521: Training Loss: 0.10155271987120311 Validation Loss: 0.7950040698051453\n",
      "Epoch 11522: Training Loss: 0.10139704495668411 Validation Loss: 0.7948253750801086\n",
      "Epoch 11523: Training Loss: 0.1016065130631129 Validation Loss: 0.7951263189315796\n",
      "Epoch 11524: Training Loss: 0.10172135631243388 Validation Loss: 0.795200526714325\n",
      "Epoch 11525: Training Loss: 0.10263783733050029 Validation Loss: 0.7946478128433228\n",
      "Epoch 11526: Training Loss: 0.10161759704351425 Validation Loss: 0.7947698831558228\n",
      "Epoch 11527: Training Loss: 0.1015744258960088 Validation Loss: 0.795260488986969\n",
      "Epoch 11528: Training Loss: 0.10166694472233455 Validation Loss: 0.7956235408782959\n",
      "Epoch 11529: Training Loss: 0.10176380475362141 Validation Loss: 0.7961480617523193\n",
      "Epoch 11530: Training Loss: 0.1017986610531807 Validation Loss: 0.7965263724327087\n",
      "Epoch 11531: Training Loss: 0.10198581715424855 Validation Loss: 0.7967263460159302\n",
      "Epoch 11532: Training Loss: 0.10206528007984161 Validation Loss: 0.7966095805168152\n",
      "Epoch 11533: Training Loss: 0.10154034445683162 Validation Loss: 0.7959662079811096\n",
      "Epoch 11534: Training Loss: 0.10178108016649882 Validation Loss: 0.7962868809700012\n",
      "Epoch 11535: Training Loss: 0.10153654217720032 Validation Loss: 0.7960656881332397\n",
      "Epoch 11536: Training Loss: 0.10153831789890926 Validation Loss: 0.7952356934547424\n",
      "Epoch 11537: Training Loss: 0.1018966833750407 Validation Loss: 0.7933017015457153\n",
      "Epoch 11538: Training Loss: 0.10179197788238525 Validation Loss: 0.7944092750549316\n",
      "Epoch 11539: Training Loss: 0.10167299459377925 Validation Loss: 0.7950254678726196\n",
      "Epoch 11540: Training Loss: 0.10165744523207347 Validation Loss: 0.7950615286827087\n",
      "Epoch 11541: Training Loss: 0.10156087825695674 Validation Loss: 0.7957351207733154\n",
      "Epoch 11542: Training Loss: 0.10134059687455495 Validation Loss: 0.795107901096344\n",
      "Epoch 11543: Training Loss: 0.10145954539378484 Validation Loss: 0.795360803604126\n",
      "Epoch 11544: Training Loss: 0.1015836422642072 Validation Loss: 0.7949788570404053\n",
      "Epoch 11545: Training Loss: 0.10140437136093776 Validation Loss: 0.7947224974632263\n",
      "Epoch 11546: Training Loss: 0.10154559711615245 Validation Loss: 0.7946754097938538\n",
      "Epoch 11547: Training Loss: 0.10202204684416454 Validation Loss: 0.7949387431144714\n",
      "Epoch 11548: Training Loss: 0.10147407650947571 Validation Loss: 0.7952982187271118\n",
      "Epoch 11549: Training Loss: 0.10131033758322398 Validation Loss: 0.7959181666374207\n",
      "Epoch 11550: Training Loss: 0.1018425499399503 Validation Loss: 0.7956090569496155\n",
      "Epoch 11551: Training Loss: 0.10157103836536407 Validation Loss: 0.7964887022972107\n",
      "Epoch 11552: Training Loss: 0.10165506601333618 Validation Loss: 0.7969447374343872\n",
      "Epoch 11553: Training Loss: 0.10148706038792928 Validation Loss: 0.7957484126091003\n",
      "Epoch 11554: Training Loss: 0.10171795139710109 Validation Loss: 0.794894814491272\n",
      "Epoch 11555: Training Loss: 0.10183186580737431 Validation Loss: 0.794857919216156\n",
      "Epoch 11556: Training Loss: 0.10146873692671458 Validation Loss: 0.7957255244255066\n",
      "Epoch 11557: Training Loss: 0.10120532164971034 Validation Loss: 0.7963968515396118\n",
      "Epoch 11558: Training Loss: 0.10141045848528545 Validation Loss: 0.7959542274475098\n",
      "Epoch 11559: Training Loss: 0.10155267268419266 Validation Loss: 0.7956209182739258\n",
      "Epoch 11560: Training Loss: 0.10228755325078964 Validation Loss: 0.7952945232391357\n",
      "Epoch 11561: Training Loss: 0.10134209444125493 Validation Loss: 0.7953330278396606\n",
      "Epoch 11562: Training Loss: 0.10171819229920705 Validation Loss: 0.7961394190788269\n",
      "Epoch 11563: Training Loss: 0.10146404057741165 Validation Loss: 0.7962033748626709\n",
      "Epoch 11564: Training Loss: 0.10156984875599544 Validation Loss: 0.7961855530738831\n",
      "Epoch 11565: Training Loss: 0.1016934538880984 Validation Loss: 0.7964521050453186\n",
      "Epoch 11566: Training Loss: 0.10149213423331578 Validation Loss: 0.7955946922302246\n",
      "Epoch 11567: Training Loss: 0.10151572773853938 Validation Loss: 0.7954765558242798\n",
      "Epoch 11568: Training Loss: 0.10130103180805843 Validation Loss: 0.7955955862998962\n",
      "Epoch 11569: Training Loss: 0.10108804702758789 Validation Loss: 0.7961609959602356\n",
      "Epoch 11570: Training Loss: 0.10186364998420079 Validation Loss: 0.7965847849845886\n",
      "Epoch 11571: Training Loss: 0.10159647713104884 Validation Loss: 0.7966445684432983\n",
      "Epoch 11572: Training Loss: 0.10141214231650035 Validation Loss: 0.796156644821167\n",
      "Epoch 11573: Training Loss: 0.10139753172794978 Validation Loss: 0.7954757213592529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11574: Training Loss: 0.1020021140575409 Validation Loss: 0.7944262027740479\n",
      "Epoch 11575: Training Loss: 0.10147280742724736 Validation Loss: 0.7948576807975769\n",
      "Epoch 11576: Training Loss: 0.10147792349259059 Validation Loss: 0.7949466109275818\n",
      "Epoch 11577: Training Loss: 0.10146837184826533 Validation Loss: 0.7949755191802979\n",
      "Epoch 11578: Training Loss: 0.10199993352095287 Validation Loss: 0.796301007270813\n",
      "Epoch 11579: Training Loss: 0.10162687053283055 Validation Loss: 0.7962083220481873\n",
      "Epoch 11580: Training Loss: 0.10149774203697841 Validation Loss: 0.7955703735351562\n",
      "Epoch 11581: Training Loss: 0.10139391819636027 Validation Loss: 0.7961319088935852\n",
      "Epoch 11582: Training Loss: 0.1012822762131691 Validation Loss: 0.7964481711387634\n",
      "Epoch 11583: Training Loss: 0.1025404433409373 Validation Loss: 0.796443521976471\n",
      "Epoch 11584: Training Loss: 0.10151113818089168 Validation Loss: 0.7955588102340698\n",
      "Epoch 11585: Training Loss: 0.10118600974480312 Validation Loss: 0.7948158383369446\n",
      "Epoch 11586: Training Loss: 0.10131776581207912 Validation Loss: 0.7947130799293518\n",
      "Epoch 11587: Training Loss: 0.10144791503747304 Validation Loss: 0.7957509160041809\n",
      "Epoch 11588: Training Loss: 0.10178239395221074 Validation Loss: 0.7956318259239197\n",
      "Epoch 11589: Training Loss: 0.10134290158748627 Validation Loss: 0.7959229350090027\n",
      "Epoch 11590: Training Loss: 0.10111268113056819 Validation Loss: 0.7964745163917542\n",
      "Epoch 11591: Training Loss: 0.10152639696995418 Validation Loss: 0.7966891527175903\n",
      "Epoch 11592: Training Loss: 0.10149129976828893 Validation Loss: 0.7957196831703186\n",
      "Epoch 11593: Training Loss: 0.1015213926633199 Validation Loss: 0.7964391708374023\n",
      "Epoch 11594: Training Loss: 0.10170507431030273 Validation Loss: 0.7966272234916687\n",
      "Epoch 11595: Training Loss: 0.10130206992228825 Validation Loss: 0.7967220544815063\n",
      "Epoch 11596: Training Loss: 0.10131554305553436 Validation Loss: 0.7975050806999207\n",
      "Epoch 11597: Training Loss: 0.10111132760842641 Validation Loss: 0.797333836555481\n",
      "Epoch 11598: Training Loss: 0.10144059111674626 Validation Loss: 0.7957667112350464\n",
      "Epoch 11599: Training Loss: 0.10125039766232173 Validation Loss: 0.7955949902534485\n",
      "Epoch 11600: Training Loss: 0.10145064940055211 Validation Loss: 0.7946906089782715\n",
      "Epoch 11601: Training Loss: 0.10173135250806808 Validation Loss: 0.795098602771759\n",
      "Epoch 11602: Training Loss: 0.10121184835831325 Validation Loss: 0.7955252528190613\n",
      "Epoch 11603: Training Loss: 0.10110149035851161 Validation Loss: 0.7963151335716248\n",
      "Epoch 11604: Training Loss: 0.10119797537724178 Validation Loss: 0.7966648936271667\n",
      "Epoch 11605: Training Loss: 0.1010610858599345 Validation Loss: 0.7970138192176819\n",
      "Epoch 11606: Training Loss: 0.10121604303518932 Validation Loss: 0.7966004610061646\n",
      "Epoch 11607: Training Loss: 0.10123669107755025 Validation Loss: 0.7967281341552734\n",
      "Epoch 11608: Training Loss: 0.1008624682823817 Validation Loss: 0.7959789633750916\n",
      "Epoch 11609: Training Loss: 0.10125516106685002 Validation Loss: 0.7956181764602661\n",
      "Epoch 11610: Training Loss: 0.10118790467580159 Validation Loss: 0.796002209186554\n",
      "Epoch 11611: Training Loss: 0.10113564381996791 Validation Loss: 0.7960761189460754\n",
      "Epoch 11612: Training Loss: 0.10125492513179779 Validation Loss: 0.797087550163269\n",
      "Epoch 11613: Training Loss: 0.10121730715036392 Validation Loss: 0.7968419194221497\n",
      "Epoch 11614: Training Loss: 0.10115824143091838 Validation Loss: 0.7962605953216553\n",
      "Epoch 11615: Training Loss: 0.10121776163578033 Validation Loss: 0.796109676361084\n",
      "Epoch 11616: Training Loss: 0.10169800122578938 Validation Loss: 0.7955687642097473\n",
      "Epoch 11617: Training Loss: 0.1012971227367719 Validation Loss: 0.7960209846496582\n",
      "Epoch 11618: Training Loss: 0.10111706703901291 Validation Loss: 0.7955909371376038\n",
      "Epoch 11619: Training Loss: 0.10134402165810268 Validation Loss: 0.7959153652191162\n",
      "Epoch 11620: Training Loss: 0.10101877897977829 Validation Loss: 0.7953624725341797\n",
      "Epoch 11621: Training Loss: 0.10108689963817596 Validation Loss: 0.7959153056144714\n",
      "Epoch 11622: Training Loss: 0.10172828783591588 Validation Loss: 0.7972448468208313\n",
      "Epoch 11623: Training Loss: 0.10138736913601558 Validation Loss: 0.7984262108802795\n",
      "Epoch 11624: Training Loss: 0.10112053404251735 Validation Loss: 0.7980538606643677\n",
      "Epoch 11625: Training Loss: 0.10108837982018788 Validation Loss: 0.7974867820739746\n",
      "Epoch 11626: Training Loss: 0.10098677376906078 Validation Loss: 0.7964730262756348\n",
      "Epoch 11627: Training Loss: 0.10123860836029053 Validation Loss: 0.796278715133667\n",
      "Epoch 11628: Training Loss: 0.10137727111577988 Validation Loss: 0.796514093875885\n",
      "Epoch 11629: Training Loss: 0.1009995440642039 Validation Loss: 0.7958327531814575\n",
      "Epoch 11630: Training Loss: 0.10101574907700221 Validation Loss: 0.7953062057495117\n",
      "Epoch 11631: Training Loss: 0.10214909414450328 Validation Loss: 0.7961542010307312\n",
      "Epoch 11632: Training Loss: 0.10142035782337189 Validation Loss: 0.796027421951294\n",
      "Epoch 11633: Training Loss: 0.1009887953599294 Validation Loss: 0.7963026165962219\n",
      "Epoch 11634: Training Loss: 0.10109092791875203 Validation Loss: 0.7967711687088013\n",
      "Epoch 11635: Training Loss: 0.10110977540413539 Validation Loss: 0.7962046265602112\n",
      "Epoch 11636: Training Loss: 0.1012405554453532 Validation Loss: 0.7969729900360107\n",
      "Epoch 11637: Training Loss: 0.10176132619380951 Validation Loss: 0.7963886260986328\n",
      "Epoch 11638: Training Loss: 0.10147383064031601 Validation Loss: 0.7971296906471252\n",
      "Epoch 11639: Training Loss: 0.10129730155070622 Validation Loss: 0.7967248558998108\n",
      "Epoch 11640: Training Loss: 0.10095324118932088 Validation Loss: 0.7970415353775024\n",
      "Epoch 11641: Training Loss: 0.10111961265405019 Validation Loss: 0.7965575456619263\n",
      "Epoch 11642: Training Loss: 0.10136148581902187 Validation Loss: 0.7959262728691101\n",
      "Epoch 11643: Training Loss: 0.10097569475571315 Validation Loss: 0.7962808609008789\n",
      "Epoch 11644: Training Loss: 0.10096379121144612 Validation Loss: 0.796783447265625\n",
      "Epoch 11645: Training Loss: 0.10112677762905757 Validation Loss: 0.7972935438156128\n",
      "Epoch 11646: Training Loss: 0.10139764100313187 Validation Loss: 0.7980860471725464\n",
      "Epoch 11647: Training Loss: 0.1011889527241389 Validation Loss: 0.7972131371498108\n",
      "Epoch 11648: Training Loss: 0.10101875414450963 Validation Loss: 0.7961212992668152\n",
      "Epoch 11649: Training Loss: 0.10110523054997127 Validation Loss: 0.7962417006492615\n",
      "Epoch 11650: Training Loss: 0.10071752965450287 Validation Loss: 0.796703040599823\n",
      "Epoch 11651: Training Loss: 0.10115669916073482 Validation Loss: 0.7966861128807068\n",
      "Epoch 11652: Training Loss: 0.10086735586325328 Validation Loss: 0.7967349886894226\n",
      "Epoch 11653: Training Loss: 0.10092315077781677 Validation Loss: 0.7960678339004517\n",
      "Epoch 11654: Training Loss: 0.10104526331027348 Validation Loss: 0.7957286834716797\n",
      "Epoch 11655: Training Loss: 0.10103234400351842 Validation Loss: 0.7966655492782593\n",
      "Epoch 11656: Training Loss: 0.10102295875549316 Validation Loss: 0.796709418296814\n",
      "Epoch 11657: Training Loss: 0.10130479683478673 Validation Loss: 0.7964871525764465\n",
      "Epoch 11658: Training Loss: 0.1007169559597969 Validation Loss: 0.7970584630966187\n",
      "Epoch 11659: Training Loss: 0.10116703808307648 Validation Loss: 0.7977272868156433\n",
      "Epoch 11660: Training Loss: 0.10111018518606822 Validation Loss: 0.7977595329284668\n",
      "Epoch 11661: Training Loss: 0.10143710921208064 Validation Loss: 0.7971212863922119\n",
      "Epoch 11662: Training Loss: 0.10110070059696834 Validation Loss: 0.795936107635498\n",
      "Epoch 11663: Training Loss: 0.10091996192932129 Validation Loss: 0.7955470085144043\n",
      "Epoch 11664: Training Loss: 0.10092028727134068 Validation Loss: 0.7965430021286011\n",
      "Epoch 11665: Training Loss: 0.10135342677434285 Validation Loss: 0.7975085973739624\n",
      "Epoch 11666: Training Loss: 0.10111835847298305 Validation Loss: 0.7983180284500122\n",
      "Epoch 11667: Training Loss: 0.10136277476946513 Validation Loss: 0.7984781265258789\n",
      "Epoch 11668: Training Loss: 0.10130349546670914 Validation Loss: 0.7978273034095764\n",
      "Epoch 11669: Training Loss: 0.10107076168060303 Validation Loss: 0.7959734797477722\n",
      "Epoch 11670: Training Loss: 0.10093760987122853 Validation Loss: 0.7959902882575989\n",
      "Epoch 11671: Training Loss: 0.100889191031456 Validation Loss: 0.7966849207878113\n",
      "Epoch 11672: Training Loss: 0.10107820729414622 Validation Loss: 0.797639787197113\n",
      "Epoch 11673: Training Loss: 0.10106870035330455 Validation Loss: 0.7986348867416382\n",
      "Epoch 11674: Training Loss: 0.10149369388818741 Validation Loss: 0.7975746393203735\n",
      "Epoch 11675: Training Loss: 0.10103143254915874 Validation Loss: 0.7967427372932434\n",
      "Epoch 11676: Training Loss: 0.10101305196682613 Validation Loss: 0.795403003692627\n",
      "Epoch 11677: Training Loss: 0.10088607917229335 Validation Loss: 0.7952072024345398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11678: Training Loss: 0.10079536338647206 Validation Loss: 0.796073317527771\n",
      "Epoch 11679: Training Loss: 0.10143713653087616 Validation Loss: 0.7980939745903015\n",
      "Epoch 11680: Training Loss: 0.10070452094078064 Validation Loss: 0.7976842522621155\n",
      "Epoch 11681: Training Loss: 0.10108755777279536 Validation Loss: 0.7972772121429443\n",
      "Epoch 11682: Training Loss: 0.10121096670627594 Validation Loss: 0.796699047088623\n",
      "Epoch 11683: Training Loss: 0.10104039808114369 Validation Loss: 0.7973735928535461\n",
      "Epoch 11684: Training Loss: 0.10084709028402965 Validation Loss: 0.7973572611808777\n",
      "Epoch 11685: Training Loss: 0.10079717387755711 Validation Loss: 0.7974562048912048\n",
      "Epoch 11686: Training Loss: 0.1007391835252444 Validation Loss: 0.7977542281150818\n",
      "Epoch 11687: Training Loss: 0.10074639568726222 Validation Loss: 0.7976948618888855\n",
      "Epoch 11688: Training Loss: 0.1003328412771225 Validation Loss: 0.7966479659080505\n",
      "Epoch 11689: Training Loss: 0.10089503725369771 Validation Loss: 0.7965258359909058\n",
      "Epoch 11690: Training Loss: 0.10080751031637192 Validation Loss: 0.7971780300140381\n",
      "Epoch 11691: Training Loss: 0.1008645271261533 Validation Loss: 0.7978125214576721\n",
      "Epoch 11692: Training Loss: 0.1006842131416003 Validation Loss: 0.7980167865753174\n",
      "Epoch 11693: Training Loss: 0.1007811427116394 Validation Loss: 0.7981573343276978\n",
      "Epoch 11694: Training Loss: 0.10112647463877995 Validation Loss: 0.797569215297699\n",
      "Epoch 11695: Training Loss: 0.1009409228960673 Validation Loss: 0.7976632118225098\n",
      "Epoch 11696: Training Loss: 0.10097092390060425 Validation Loss: 0.7963765859603882\n",
      "Epoch 11697: Training Loss: 0.10049468527237575 Validation Loss: 0.7956159114837646\n",
      "Epoch 11698: Training Loss: 0.10089253882567088 Validation Loss: 0.7959974408149719\n",
      "Epoch 11699: Training Loss: 0.1008426199356715 Validation Loss: 0.7966877818107605\n",
      "Epoch 11700: Training Loss: 0.10083672652641933 Validation Loss: 0.7971161603927612\n",
      "Epoch 11701: Training Loss: 0.10091481109460194 Validation Loss: 0.7967225313186646\n",
      "Epoch 11702: Training Loss: 0.10138547917207082 Validation Loss: 0.7962511777877808\n",
      "Epoch 11703: Training Loss: 0.10085490345954895 Validation Loss: 0.7979647517204285\n",
      "Epoch 11704: Training Loss: 0.1011462261279424 Validation Loss: 0.7982929944992065\n",
      "Epoch 11705: Training Loss: 0.10074818382660548 Validation Loss: 0.798187792301178\n",
      "Epoch 11706: Training Loss: 0.10092921058336894 Validation Loss: 0.7982194423675537\n",
      "Epoch 11707: Training Loss: 0.10125357657670975 Validation Loss: 0.7973459362983704\n",
      "Epoch 11708: Training Loss: 0.10084620863199234 Validation Loss: 0.7964722514152527\n",
      "Epoch 11709: Training Loss: 0.10065795977910359 Validation Loss: 0.7976126670837402\n",
      "Epoch 11710: Training Loss: 0.10083573559919994 Validation Loss: 0.7988386750221252\n",
      "Epoch 11711: Training Loss: 0.10107684880495071 Validation Loss: 0.7985262870788574\n",
      "Epoch 11712: Training Loss: 0.1006796583533287 Validation Loss: 0.7978196144104004\n",
      "Epoch 11713: Training Loss: 0.1005728542804718 Validation Loss: 0.7964525818824768\n",
      "Epoch 11714: Training Loss: 0.10056411723295848 Validation Loss: 0.7965483069419861\n",
      "Epoch 11715: Training Loss: 0.10113680362701416 Validation Loss: 0.797785222530365\n",
      "Epoch 11716: Training Loss: 0.10129697372515996 Validation Loss: 0.7973071336746216\n",
      "Epoch 11717: Training Loss: 0.10140762726465861 Validation Loss: 0.7984161972999573\n",
      "Epoch 11718: Training Loss: 0.1005797063310941 Validation Loss: 0.7985659837722778\n",
      "Epoch 11719: Training Loss: 0.10109957059224446 Validation Loss: 0.7984579205513\n",
      "Epoch 11720: Training Loss: 0.10088282823562622 Validation Loss: 0.7978035807609558\n",
      "Epoch 11721: Training Loss: 0.10091064622004826 Validation Loss: 0.7972772121429443\n",
      "Epoch 11722: Training Loss: 0.10071857025225957 Validation Loss: 0.7970169186592102\n",
      "Epoch 11723: Training Loss: 0.1005813255906105 Validation Loss: 0.7973898649215698\n",
      "Epoch 11724: Training Loss: 0.10149213423331578 Validation Loss: 0.7983771562576294\n",
      "Epoch 11725: Training Loss: 0.10037993391354878 Validation Loss: 0.7980465292930603\n",
      "Epoch 11726: Training Loss: 0.10060872634251912 Validation Loss: 0.7979034781455994\n",
      "Epoch 11727: Training Loss: 0.10049500316381454 Validation Loss: 0.797601044178009\n",
      "Epoch 11728: Training Loss: 0.10072094698746999 Validation Loss: 0.797164797782898\n",
      "Epoch 11729: Training Loss: 0.10061857601006825 Validation Loss: 0.7971411943435669\n",
      "Epoch 11730: Training Loss: 0.10071290284395218 Validation Loss: 0.7978793978691101\n",
      "Epoch 11731: Training Loss: 0.10045979420344035 Validation Loss: 0.7983328700065613\n",
      "Epoch 11732: Training Loss: 0.1009347140789032 Validation Loss: 0.798418402671814\n",
      "Epoch 11733: Training Loss: 0.10141604642073314 Validation Loss: 0.7983185052871704\n",
      "Epoch 11734: Training Loss: 0.10076904296875 Validation Loss: 0.7975510954856873\n",
      "Epoch 11735: Training Loss: 0.10093297809362411 Validation Loss: 0.797854483127594\n",
      "Epoch 11736: Training Loss: 0.10042000810305278 Validation Loss: 0.7979167699813843\n",
      "Epoch 11737: Training Loss: 0.10119034598271053 Validation Loss: 0.7980614900588989\n",
      "Epoch 11738: Training Loss: 0.10044338305791219 Validation Loss: 0.7978143692016602\n",
      "Epoch 11739: Training Loss: 0.10075368980566661 Validation Loss: 0.7982202172279358\n",
      "Epoch 11740: Training Loss: 0.10045224924882253 Validation Loss: 0.796956479549408\n",
      "Epoch 11741: Training Loss: 0.10079143444697063 Validation Loss: 0.7974147200584412\n",
      "Epoch 11742: Training Loss: 0.10074945787588756 Validation Loss: 0.7974513173103333\n",
      "Epoch 11743: Training Loss: 0.10069292535384496 Validation Loss: 0.7968356013298035\n",
      "Epoch 11744: Training Loss: 0.10033770153919856 Validation Loss: 0.796927809715271\n",
      "Epoch 11745: Training Loss: 0.10041654358307521 Validation Loss: 0.797160267829895\n",
      "Epoch 11746: Training Loss: 0.10108627627293269 Validation Loss: 0.7977288365364075\n",
      "Epoch 11747: Training Loss: 0.10097776850064595 Validation Loss: 0.7993533611297607\n",
      "Epoch 11748: Training Loss: 0.1006409302353859 Validation Loss: 0.8002574443817139\n",
      "Epoch 11749: Training Loss: 0.1006578579545021 Validation Loss: 0.7993545532226562\n",
      "Epoch 11750: Training Loss: 0.10036861151456833 Validation Loss: 0.7980207800865173\n",
      "Epoch 11751: Training Loss: 0.10070610543092091 Validation Loss: 0.7969750761985779\n",
      "Epoch 11752: Training Loss: 0.10005790740251541 Validation Loss: 0.796879231929779\n",
      "Epoch 11753: Training Loss: 0.10073271890481313 Validation Loss: 0.7971270680427551\n",
      "Epoch 11754: Training Loss: 0.10093003511428833 Validation Loss: 0.7983989715576172\n",
      "Epoch 11755: Training Loss: 0.10058054824670155 Validation Loss: 0.799187958240509\n",
      "Epoch 11756: Training Loss: 0.10081951071818669 Validation Loss: 0.7994522452354431\n",
      "Epoch 11757: Training Loss: 0.10071009397506714 Validation Loss: 0.8000363111495972\n",
      "Epoch 11758: Training Loss: 0.1004135434826215 Validation Loss: 0.7992113828659058\n",
      "Epoch 11759: Training Loss: 0.10040265570084254 Validation Loss: 0.7990132570266724\n",
      "Epoch 11760: Training Loss: 0.10048015664021175 Validation Loss: 0.797589898109436\n",
      "Epoch 11761: Training Loss: 0.10043911635875702 Validation Loss: 0.7969101667404175\n",
      "Epoch 11762: Training Loss: 0.1005185917019844 Validation Loss: 0.7967127561569214\n",
      "Epoch 11763: Training Loss: 0.10085245221853256 Validation Loss: 0.7974467873573303\n",
      "Epoch 11764: Training Loss: 0.1006956398487091 Validation Loss: 0.7981142401695251\n",
      "Epoch 11765: Training Loss: 0.10068127264579137 Validation Loss: 0.79925936460495\n",
      "Epoch 11766: Training Loss: 0.10030924777189891 Validation Loss: 0.7982738614082336\n",
      "Epoch 11767: Training Loss: 0.09988716493050258 Validation Loss: 0.7979747653007507\n",
      "Epoch 11768: Training Loss: 0.100772425532341 Validation Loss: 0.7981999516487122\n",
      "Epoch 11769: Training Loss: 0.10009690870841344 Validation Loss: 0.7981739044189453\n",
      "Epoch 11770: Training Loss: 0.10068555424610774 Validation Loss: 0.7978814244270325\n",
      "Epoch 11771: Training Loss: 0.10044291118780772 Validation Loss: 0.7978752255439758\n",
      "Epoch 11772: Training Loss: 0.10081261644760768 Validation Loss: 0.7994352579116821\n",
      "Epoch 11773: Training Loss: 0.10017397254705429 Validation Loss: 0.7993150949478149\n",
      "Epoch 11774: Training Loss: 0.10113418847322464 Validation Loss: 0.799368679523468\n",
      "Epoch 11775: Training Loss: 0.10035035014152527 Validation Loss: 0.7989740371704102\n",
      "Epoch 11776: Training Loss: 0.1003517434000969 Validation Loss: 0.7988203167915344\n",
      "Epoch 11777: Training Loss: 0.1009989654024442 Validation Loss: 0.7981139421463013\n",
      "Epoch 11778: Training Loss: 0.10041029502948125 Validation Loss: 0.7983384728431702\n",
      "Epoch 11779: Training Loss: 0.10083524386088054 Validation Loss: 0.7986259460449219\n",
      "Epoch 11780: Training Loss: 0.10030367225408554 Validation Loss: 0.7985666394233704\n",
      "Epoch 11781: Training Loss: 0.10042502234379451 Validation Loss: 0.7980723977088928\n",
      "Epoch 11782: Training Loss: 0.10033504913250606 Validation Loss: 0.7977958917617798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11783: Training Loss: 0.10033965110778809 Validation Loss: 0.7982428073883057\n",
      "Epoch 11784: Training Loss: 0.10072206457455952 Validation Loss: 0.7985108494758606\n",
      "Epoch 11785: Training Loss: 0.10022161404291789 Validation Loss: 0.798177182674408\n",
      "Epoch 11786: Training Loss: 0.10032587001721065 Validation Loss: 0.7984891533851624\n",
      "Epoch 11787: Training Loss: 0.10041329264640808 Validation Loss: 0.7983555793762207\n",
      "Epoch 11788: Training Loss: 0.1002683440844218 Validation Loss: 0.7983623743057251\n",
      "Epoch 11789: Training Loss: 0.10050456722577412 Validation Loss: 0.7982192635536194\n",
      "Epoch 11790: Training Loss: 0.10062236835559209 Validation Loss: 0.7981674075126648\n",
      "Epoch 11791: Training Loss: 0.10048049439986546 Validation Loss: 0.7976065278053284\n",
      "Epoch 11792: Training Loss: 0.10024287054936092 Validation Loss: 0.7979763746261597\n",
      "Epoch 11793: Training Loss: 0.10066632429758708 Validation Loss: 0.7989028692245483\n",
      "Epoch 11794: Training Loss: 0.10031822075446446 Validation Loss: 0.7992185354232788\n",
      "Epoch 11795: Training Loss: 0.1003270149230957 Validation Loss: 0.7983366847038269\n",
      "Epoch 11796: Training Loss: 0.10006096462408702 Validation Loss: 0.7989113330841064\n",
      "Epoch 11797: Training Loss: 0.10028185695409775 Validation Loss: 0.7989404797554016\n",
      "Epoch 11798: Training Loss: 0.10067422191301982 Validation Loss: 0.7985444664955139\n",
      "Epoch 11799: Training Loss: 0.10034181425968806 Validation Loss: 0.7981541156768799\n",
      "Epoch 11800: Training Loss: 0.10007345676422119 Validation Loss: 0.7987683415412903\n",
      "Epoch 11801: Training Loss: 0.10081135233243306 Validation Loss: 0.7993074059486389\n",
      "Epoch 11802: Training Loss: 0.10134841501712799 Validation Loss: 0.7998127937316895\n",
      "Epoch 11803: Training Loss: 0.10023212184508641 Validation Loss: 0.7987020611763\n",
      "Epoch 11804: Training Loss: 0.10002343108256657 Validation Loss: 0.7990840077400208\n",
      "Epoch 11805: Training Loss: 0.10014559576908748 Validation Loss: 0.7988830208778381\n",
      "Epoch 11806: Training Loss: 0.10005904237429301 Validation Loss: 0.7978375554084778\n",
      "Epoch 11807: Training Loss: 0.09999010960261027 Validation Loss: 0.7970184683799744\n",
      "Epoch 11808: Training Loss: 0.10021355996529262 Validation Loss: 0.7982900142669678\n",
      "Epoch 11809: Training Loss: 0.10033731410900752 Validation Loss: 0.7993074059486389\n",
      "Epoch 11810: Training Loss: 0.10046807676553726 Validation Loss: 0.8001058101654053\n",
      "Epoch 11811: Training Loss: 0.10010643551747005 Validation Loss: 0.7991531491279602\n",
      "Epoch 11812: Training Loss: 0.1012649933497111 Validation Loss: 0.7981306910514832\n",
      "Epoch 11813: Training Loss: 0.10042532781759898 Validation Loss: 0.7977789044380188\n",
      "Epoch 11814: Training Loss: 0.09998756150404613 Validation Loss: 0.7980228066444397\n",
      "Epoch 11815: Training Loss: 0.10012070337931316 Validation Loss: 0.7982863187789917\n",
      "Epoch 11816: Training Loss: 0.10020073751608531 Validation Loss: 0.7986018657684326\n",
      "Epoch 11817: Training Loss: 0.10007956872383754 Validation Loss: 0.7982404828071594\n",
      "Epoch 11818: Training Loss: 0.10010319203138351 Validation Loss: 0.7978290915489197\n",
      "Epoch 11819: Training Loss: 0.09970999509096146 Validation Loss: 0.7992432713508606\n",
      "Epoch 11820: Training Loss: 0.10046473642190297 Validation Loss: 0.7987071871757507\n",
      "Epoch 11821: Training Loss: 0.10006874799728394 Validation Loss: 0.7994651794433594\n",
      "Epoch 11822: Training Loss: 0.10010081777969997 Validation Loss: 0.7995025515556335\n",
      "Epoch 11823: Training Loss: 0.10001025597254436 Validation Loss: 0.7995184659957886\n",
      "Epoch 11824: Training Loss: 0.10008091231187184 Validation Loss: 0.7993273735046387\n",
      "Epoch 11825: Training Loss: 0.10070029894510905 Validation Loss: 0.7991544008255005\n",
      "Epoch 11826: Training Loss: 0.10076155265172322 Validation Loss: 0.7991843819618225\n",
      "Epoch 11827: Training Loss: 0.10004736483097076 Validation Loss: 0.7976626753807068\n",
      "Epoch 11828: Training Loss: 0.1003103107213974 Validation Loss: 0.7976884841918945\n",
      "Epoch 11829: Training Loss: 0.10014981279770534 Validation Loss: 0.7973949909210205\n",
      "Epoch 11830: Training Loss: 0.10027411083380382 Validation Loss: 0.7981212139129639\n",
      "Epoch 11831: Training Loss: 0.10015546530485153 Validation Loss: 0.7994242906570435\n",
      "Epoch 11832: Training Loss: 0.10059560338656108 Validation Loss: 0.7996863722801208\n",
      "Epoch 11833: Training Loss: 0.10026084383328755 Validation Loss: 0.7984291315078735\n",
      "Epoch 11834: Training Loss: 0.10013583799203236 Validation Loss: 0.7987251281738281\n",
      "Epoch 11835: Training Loss: 0.10029781113068263 Validation Loss: 0.7993548512458801\n",
      "Epoch 11836: Training Loss: 0.1000329280893008 Validation Loss: 0.7996587157249451\n",
      "Epoch 11837: Training Loss: 0.09968552738428116 Validation Loss: 0.7989463210105896\n",
      "Epoch 11838: Training Loss: 0.10027156521876653 Validation Loss: 0.7997931838035583\n",
      "Epoch 11839: Training Loss: 0.10068630178769429 Validation Loss: 0.8000532388687134\n",
      "Epoch 11840: Training Loss: 0.09961613267660141 Validation Loss: 0.7994663715362549\n",
      "Epoch 11841: Training Loss: 0.10008433957894643 Validation Loss: 0.7991345524787903\n",
      "Epoch 11842: Training Loss: 0.10037524253129959 Validation Loss: 0.7985615134239197\n",
      "Epoch 11843: Training Loss: 0.10027389228343964 Validation Loss: 0.7998770475387573\n",
      "Epoch 11844: Training Loss: 0.10037606209516525 Validation Loss: 0.7999918460845947\n",
      "Epoch 11845: Training Loss: 0.10006408641735713 Validation Loss: 0.8001240491867065\n",
      "Epoch 11846: Training Loss: 0.10044829299052556 Validation Loss: 0.7993866205215454\n",
      "Epoch 11847: Training Loss: 0.10015049080053966 Validation Loss: 0.7990244030952454\n",
      "Epoch 11848: Training Loss: 0.09995235006014506 Validation Loss: 0.7977206707000732\n",
      "Epoch 11849: Training Loss: 0.1002497027317683 Validation Loss: 0.7981370687484741\n",
      "Epoch 11850: Training Loss: 0.0999471793572108 Validation Loss: 0.798776388168335\n",
      "Epoch 11851: Training Loss: 0.10020570208628972 Validation Loss: 0.7985655665397644\n",
      "Epoch 11852: Training Loss: 0.10035326828559239 Validation Loss: 0.7983226776123047\n",
      "Epoch 11853: Training Loss: 0.09988299508889516 Validation Loss: 0.7979852557182312\n",
      "Epoch 11854: Training Loss: 0.09965204944213231 Validation Loss: 0.7990903854370117\n",
      "Epoch 11855: Training Loss: 0.09997584422429402 Validation Loss: 0.8001409769058228\n",
      "Epoch 11856: Training Loss: 0.09994688878456752 Validation Loss: 0.7995353937149048\n",
      "Epoch 11857: Training Loss: 0.09998856236537297 Validation Loss: 0.7992194294929504\n",
      "Epoch 11858: Training Loss: 0.09996290008227031 Validation Loss: 0.7997964024543762\n",
      "Epoch 11859: Training Loss: 0.10067257781823476 Validation Loss: 0.7994540333747864\n",
      "Epoch 11860: Training Loss: 0.10002147157986958 Validation Loss: 0.8001002669334412\n",
      "Epoch 11861: Training Loss: 0.09992037216822307 Validation Loss: 0.7993135452270508\n",
      "Epoch 11862: Training Loss: 0.10023547957340877 Validation Loss: 0.7986280918121338\n",
      "Epoch 11863: Training Loss: 0.09993883222341537 Validation Loss: 0.798953652381897\n",
      "Epoch 11864: Training Loss: 0.09983476251363754 Validation Loss: 0.7989722490310669\n",
      "Epoch 11865: Training Loss: 0.10019157578547795 Validation Loss: 0.7988136410713196\n",
      "Epoch 11866: Training Loss: 0.09994492431481679 Validation Loss: 0.7999621033668518\n",
      "Epoch 11867: Training Loss: 0.09968797117471695 Validation Loss: 0.7998232245445251\n",
      "Epoch 11868: Training Loss: 0.10009100039800008 Validation Loss: 0.7994646430015564\n",
      "Epoch 11869: Training Loss: 0.09998870144287746 Validation Loss: 0.7998549938201904\n",
      "Epoch 11870: Training Loss: 0.09981049845616023 Validation Loss: 0.799250602722168\n",
      "Epoch 11871: Training Loss: 0.09992815802494685 Validation Loss: 0.7995091676712036\n",
      "Epoch 11872: Training Loss: 0.09994687636693318 Validation Loss: 0.799433171749115\n",
      "Epoch 11873: Training Loss: 0.1003283013900121 Validation Loss: 0.7988991737365723\n",
      "Epoch 11874: Training Loss: 0.10046818852424622 Validation Loss: 0.7993612289428711\n",
      "Epoch 11875: Training Loss: 0.10031978040933609 Validation Loss: 0.8011336326599121\n",
      "Epoch 11876: Training Loss: 0.10001641263564427 Validation Loss: 0.8006510138511658\n",
      "Epoch 11877: Training Loss: 0.09986933320760727 Validation Loss: 0.7998505234718323\n",
      "Epoch 11878: Training Loss: 0.09980668872594833 Validation Loss: 0.7990594506263733\n",
      "Epoch 11879: Training Loss: 0.09988681475321452 Validation Loss: 0.7987928986549377\n",
      "Epoch 11880: Training Loss: 0.0998889406522115 Validation Loss: 0.7993623614311218\n",
      "Epoch 11881: Training Loss: 0.0997221569220225 Validation Loss: 0.7999918460845947\n",
      "Epoch 11882: Training Loss: 0.10000884781281154 Validation Loss: 0.800483763217926\n",
      "Epoch 11883: Training Loss: 0.10034054269393285 Validation Loss: 0.7990288734436035\n",
      "Epoch 11884: Training Loss: 0.0999433770775795 Validation Loss: 0.7989908456802368\n",
      "Epoch 11885: Training Loss: 0.10005013644695282 Validation Loss: 0.7998097538948059\n",
      "Epoch 11886: Training Loss: 0.09971931328376134 Validation Loss: 0.7992628812789917\n",
      "Epoch 11887: Training Loss: 0.09982038040955861 Validation Loss: 0.7990004420280457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11888: Training Loss: 0.09988487760225932 Validation Loss: 0.7989793419837952\n",
      "Epoch 11889: Training Loss: 0.10012680788834889 Validation Loss: 0.799582302570343\n",
      "Epoch 11890: Training Loss: 0.10002934436003368 Validation Loss: 0.7999169230461121\n",
      "Epoch 11891: Training Loss: 0.10078706343968709 Validation Loss: 0.7998411655426025\n",
      "Epoch 11892: Training Loss: 0.09998265157143275 Validation Loss: 0.8002303242683411\n",
      "Epoch 11893: Training Loss: 0.09999038775761922 Validation Loss: 0.8006976842880249\n",
      "Epoch 11894: Training Loss: 0.09947661062081654 Validation Loss: 0.8007609248161316\n",
      "Epoch 11895: Training Loss: 0.1001441478729248 Validation Loss: 0.7996960878372192\n",
      "Epoch 11896: Training Loss: 0.09982037792603175 Validation Loss: 0.8004726767539978\n",
      "Epoch 11897: Training Loss: 0.0999148041009903 Validation Loss: 0.8000145554542542\n",
      "Epoch 11898: Training Loss: 0.09978294372558594 Validation Loss: 0.7994006276130676\n",
      "Epoch 11899: Training Loss: 0.09969918678204219 Validation Loss: 0.7990092635154724\n",
      "Epoch 11900: Training Loss: 0.09950997680425644 Validation Loss: 0.7999492883682251\n",
      "Epoch 11901: Training Loss: 0.09992630283037822 Validation Loss: 0.8008175492286682\n",
      "Epoch 11902: Training Loss: 0.10017800082763036 Validation Loss: 0.7996306419372559\n",
      "Epoch 11903: Training Loss: 0.09967756023009618 Validation Loss: 0.7994843125343323\n",
      "Epoch 11904: Training Loss: 0.09994906932115555 Validation Loss: 0.8009858131408691\n",
      "Epoch 11905: Training Loss: 0.09981117645899455 Validation Loss: 0.8006463646888733\n",
      "Epoch 11906: Training Loss: 0.09982146074374516 Validation Loss: 0.8005520105361938\n",
      "Epoch 11907: Training Loss: 0.10014340281486511 Validation Loss: 0.8006958961486816\n",
      "Epoch 11908: Training Loss: 0.09973299503326416 Validation Loss: 0.7997613549232483\n",
      "Epoch 11909: Training Loss: 0.09990070015192032 Validation Loss: 0.7991635799407959\n",
      "Epoch 11910: Training Loss: 0.099770983060201 Validation Loss: 0.7991586327552795\n",
      "Epoch 11911: Training Loss: 0.09966710458199184 Validation Loss: 0.799125075340271\n",
      "Epoch 11912: Training Loss: 0.09947545578082402 Validation Loss: 0.7998848557472229\n",
      "Epoch 11913: Training Loss: 0.1000692422191302 Validation Loss: 0.7994804382324219\n",
      "Epoch 11914: Training Loss: 0.09977125376462936 Validation Loss: 0.7994663119316101\n",
      "Epoch 11915: Training Loss: 0.0997680773337682 Validation Loss: 0.7992773056030273\n",
      "Epoch 11916: Training Loss: 0.09957245737314224 Validation Loss: 0.7988219857215881\n",
      "Epoch 11917: Training Loss: 0.09962698072195053 Validation Loss: 0.7998275756835938\n",
      "Epoch 11918: Training Loss: 0.09999111046393712 Validation Loss: 0.7998310923576355\n",
      "Epoch 11919: Training Loss: 0.09991546968619029 Validation Loss: 0.8005356192588806\n",
      "Epoch 11920: Training Loss: 0.09988054633140564 Validation Loss: 0.7994509339332581\n",
      "Epoch 11921: Training Loss: 0.10016182313362758 Validation Loss: 0.8009412884712219\n",
      "Epoch 11922: Training Loss: 0.09958622852961223 Validation Loss: 0.801150381565094\n",
      "Epoch 11923: Training Loss: 0.09960709015528361 Validation Loss: 0.8008092641830444\n",
      "Epoch 11924: Training Loss: 0.09961018214623134 Validation Loss: 0.8001875877380371\n",
      "Epoch 11925: Training Loss: 0.09936906894048055 Validation Loss: 0.799807608127594\n",
      "Epoch 11926: Training Loss: 0.09995387991269429 Validation Loss: 0.7996605038642883\n",
      "Epoch 11927: Training Loss: 0.09958110253016154 Validation Loss: 0.8001092076301575\n",
      "Epoch 11928: Training Loss: 0.09960048645734787 Validation Loss: 0.7997322678565979\n",
      "Epoch 11929: Training Loss: 0.09959179908037186 Validation Loss: 0.7997289299964905\n",
      "Epoch 11930: Training Loss: 0.09980172663927078 Validation Loss: 0.800467312335968\n",
      "Epoch 11931: Training Loss: 0.09962930530309677 Validation Loss: 0.8004178404808044\n",
      "Epoch 11932: Training Loss: 0.10002412895361583 Validation Loss: 0.8006051182746887\n",
      "Epoch 11933: Training Loss: 0.09981133043766022 Validation Loss: 0.8009780049324036\n",
      "Epoch 11934: Training Loss: 0.09941394627094269 Validation Loss: 0.8006716370582581\n",
      "Epoch 11935: Training Loss: 0.09955164790153503 Validation Loss: 0.8009542226791382\n",
      "Epoch 11936: Training Loss: 0.09972993781169255 Validation Loss: 0.8006616830825806\n",
      "Epoch 11937: Training Loss: 0.10070169965426128 Validation Loss: 0.8005367517471313\n",
      "Epoch 11938: Training Loss: 0.09934442738691966 Validation Loss: 0.7996814250946045\n",
      "Epoch 11939: Training Loss: 0.09959403425455093 Validation Loss: 0.8003965616226196\n",
      "Epoch 11940: Training Loss: 0.0996622343858083 Validation Loss: 0.8000101447105408\n",
      "Epoch 11941: Training Loss: 0.09982890387376149 Validation Loss: 0.7994598746299744\n",
      "Epoch 11942: Training Loss: 0.09952918191750844 Validation Loss: 0.7995920181274414\n",
      "Epoch 11943: Training Loss: 0.09956569721301396 Validation Loss: 0.8005071878433228\n",
      "Epoch 11944: Training Loss: 0.10016503185033798 Validation Loss: 0.8009858727455139\n",
      "Epoch 11945: Training Loss: 0.09959921737511952 Validation Loss: 0.8013951778411865\n",
      "Epoch 11946: Training Loss: 0.09991692006587982 Validation Loss: 0.8022288084030151\n",
      "Epoch 11947: Training Loss: 0.09958897282679875 Validation Loss: 0.8011459708213806\n",
      "Epoch 11948: Training Loss: 0.09991895407438278 Validation Loss: 0.7997353672981262\n",
      "Epoch 11949: Training Loss: 0.09941131869951884 Validation Loss: 0.7994996905326843\n",
      "Epoch 11950: Training Loss: 0.10027894874413808 Validation Loss: 0.801179826259613\n",
      "Epoch 11951: Training Loss: 0.09966290493806203 Validation Loss: 0.8018819689750671\n",
      "Epoch 11952: Training Loss: 0.09953983376423518 Validation Loss: 0.8015180826187134\n",
      "Epoch 11953: Training Loss: 0.09960884849230449 Validation Loss: 0.7998570203781128\n",
      "Epoch 11954: Training Loss: 0.09967154016097386 Validation Loss: 0.7991019487380981\n",
      "Epoch 11955: Training Loss: 0.09975746522347133 Validation Loss: 0.7995767593383789\n",
      "Epoch 11956: Training Loss: 0.0996166542172432 Validation Loss: 0.800093948841095\n",
      "Epoch 11957: Training Loss: 0.09978053718805313 Validation Loss: 0.8007276058197021\n",
      "Epoch 11958: Training Loss: 0.09971177577972412 Validation Loss: 0.8003039360046387\n",
      "Epoch 11959: Training Loss: 0.09965044756730397 Validation Loss: 0.8006224036216736\n",
      "Epoch 11960: Training Loss: 0.09952125201622646 Validation Loss: 0.8003535866737366\n",
      "Epoch 11961: Training Loss: 0.09978922208150227 Validation Loss: 0.7999852299690247\n",
      "Epoch 11962: Training Loss: 0.0994952271382014 Validation Loss: 0.79996258020401\n",
      "Epoch 11963: Training Loss: 0.09944485872983932 Validation Loss: 0.8004965782165527\n",
      "Epoch 11964: Training Loss: 0.0994677444299062 Validation Loss: 0.7998713850975037\n",
      "Epoch 11965: Training Loss: 0.09951968242724736 Validation Loss: 0.8005357384681702\n",
      "Epoch 11966: Training Loss: 0.0994604875644048 Validation Loss: 0.8011220693588257\n",
      "Epoch 11967: Training Loss: 0.09955867131551106 Validation Loss: 0.8016638159751892\n",
      "Epoch 11968: Training Loss: 0.09973250081141789 Validation Loss: 0.8012591600418091\n",
      "Epoch 11969: Training Loss: 0.09946262588103612 Validation Loss: 0.8018338084220886\n",
      "Epoch 11970: Training Loss: 0.1002719874183337 Validation Loss: 0.8018203973770142\n",
      "Epoch 11971: Training Loss: 0.0994425689180692 Validation Loss: 0.8010160326957703\n",
      "Epoch 11972: Training Loss: 0.09926898280779521 Validation Loss: 0.8015016913414001\n",
      "Epoch 11973: Training Loss: 0.09921443959077199 Validation Loss: 0.8012371063232422\n",
      "Epoch 11974: Training Loss: 0.0994189406434695 Validation Loss: 0.8010367751121521\n",
      "Epoch 11975: Training Loss: 0.09938024232784907 Validation Loss: 0.8006154894828796\n",
      "Epoch 11976: Training Loss: 0.09926298260688782 Validation Loss: 0.7998119592666626\n",
      "Epoch 11977: Training Loss: 0.09957528859376907 Validation Loss: 0.7994529008865356\n",
      "Epoch 11978: Training Loss: 0.09936586022377014 Validation Loss: 0.7996694445610046\n",
      "Epoch 11979: Training Loss: 0.09948037564754486 Validation Loss: 0.8016475439071655\n",
      "Epoch 11980: Training Loss: 0.09944708893696468 Validation Loss: 0.801906406879425\n",
      "Epoch 11981: Training Loss: 0.10007808854182561 Validation Loss: 0.8018449544906616\n",
      "Epoch 11982: Training Loss: 0.09946851680676143 Validation Loss: 0.80027836561203\n",
      "Epoch 11983: Training Loss: 0.09942020227511723 Validation Loss: 0.7997442483901978\n",
      "Epoch 11984: Training Loss: 0.09977639714876811 Validation Loss: 0.7991752028465271\n",
      "Epoch 11985: Training Loss: 0.09948562582333882 Validation Loss: 0.8005509972572327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11986: Training Loss: 0.09932461380958557 Validation Loss: 0.8017446994781494\n",
      "Epoch 11987: Training Loss: 0.09920324385166168 Validation Loss: 0.8021970391273499\n",
      "Epoch 11988: Training Loss: 0.09969018151362737 Validation Loss: 0.8026530146598816\n",
      "Epoch 11989: Training Loss: 0.09976449857155482 Validation Loss: 0.8010250329971313\n",
      "Epoch 11990: Training Loss: 0.09940222650766373 Validation Loss: 0.800118088722229\n",
      "Epoch 11991: Training Loss: 0.09947124868631363 Validation Loss: 0.800383985042572\n",
      "Epoch 11992: Training Loss: 0.0992938553293546 Validation Loss: 0.7996143102645874\n",
      "Epoch 11993: Training Loss: 0.09935313959916432 Validation Loss: 0.8007020354270935\n",
      "Epoch 11994: Training Loss: 0.09938249240318935 Validation Loss: 0.801638662815094\n",
      "Epoch 11995: Training Loss: 0.09940348813931148 Validation Loss: 0.8015342354774475\n",
      "Epoch 11996: Training Loss: 0.09902587036291759 Validation Loss: 0.8013666868209839\n",
      "Epoch 11997: Training Loss: 0.09933024148146312 Validation Loss: 0.8010591864585876\n",
      "Epoch 11998: Training Loss: 0.09938967476288478 Validation Loss: 0.8006911873817444\n",
      "Epoch 11999: Training Loss: 0.09950737903515498 Validation Loss: 0.8003976941108704\n",
      "Epoch 12000: Training Loss: 0.09923873593409856 Validation Loss: 0.8007941246032715\n",
      "Epoch 12001: Training Loss: 0.09907908240954082 Validation Loss: 0.8015637993812561\n",
      "Epoch 12002: Training Loss: 0.09980560094118118 Validation Loss: 0.8023191690444946\n",
      "Epoch 12003: Training Loss: 0.09918722013632457 Validation Loss: 0.8020881414413452\n",
      "Epoch 12004: Training Loss: 0.09971119960149129 Validation Loss: 0.8011587262153625\n",
      "Epoch 12005: Training Loss: 0.09929593155781428 Validation Loss: 0.8008529543876648\n",
      "Epoch 12006: Training Loss: 0.09943868468205135 Validation Loss: 0.8002185821533203\n",
      "Epoch 12007: Training Loss: 0.09924566000699997 Validation Loss: 0.8006682991981506\n",
      "Epoch 12008: Training Loss: 0.09976126253604889 Validation Loss: 0.8020773530006409\n",
      "Epoch 12009: Training Loss: 0.09926949193080266 Validation Loss: 0.8021430969238281\n",
      "Epoch 12010: Training Loss: 0.09931473185618718 Validation Loss: 0.8021453022956848\n",
      "Epoch 12011: Training Loss: 0.09963531295458476 Validation Loss: 0.8007094860076904\n",
      "Epoch 12012: Training Loss: 0.09929188589255016 Validation Loss: 0.800775408744812\n",
      "Epoch 12013: Training Loss: 0.09945576389630635 Validation Loss: 0.8007868528366089\n",
      "Epoch 12014: Training Loss: 0.09946756809949875 Validation Loss: 0.8016771078109741\n",
      "Epoch 12015: Training Loss: 0.09941592067480087 Validation Loss: 0.8008453249931335\n",
      "Epoch 12016: Training Loss: 0.0992414802312851 Validation Loss: 0.8001609444618225\n",
      "Epoch 12017: Training Loss: 0.0991708015402158 Validation Loss: 0.8012515306472778\n",
      "Epoch 12018: Training Loss: 0.09920696417490642 Validation Loss: 0.8025305271148682\n",
      "Epoch 12019: Training Loss: 0.0990525633096695 Validation Loss: 0.8019075989723206\n",
      "Epoch 12020: Training Loss: 0.09924211353063583 Validation Loss: 0.8023989796638489\n",
      "Epoch 12021: Training Loss: 0.09949290752410889 Validation Loss: 0.8022720217704773\n",
      "Epoch 12022: Training Loss: 0.09925438463687897 Validation Loss: 0.8013144731521606\n",
      "Epoch 12023: Training Loss: 0.0994102085630099 Validation Loss: 0.8016453385353088\n",
      "Epoch 12024: Training Loss: 0.09928083419799805 Validation Loss: 0.8012956976890564\n",
      "Epoch 12025: Training Loss: 0.09917567918697993 Validation Loss: 0.8008348941802979\n",
      "Epoch 12026: Training Loss: 0.09949419895807902 Validation Loss: 0.8018433451652527\n",
      "Epoch 12027: Training Loss: 0.09964718172947566 Validation Loss: 0.8031155467033386\n",
      "Epoch 12028: Training Loss: 0.099518949786822 Validation Loss: 0.8029381632804871\n",
      "Epoch 12029: Training Loss: 0.09905298550923665 Validation Loss: 0.8018189072608948\n",
      "Epoch 12030: Training Loss: 0.09900241841872533 Validation Loss: 0.8013597130775452\n",
      "Epoch 12031: Training Loss: 0.09975424905618031 Validation Loss: 0.800899088382721\n",
      "Epoch 12032: Training Loss: 0.0990604634086291 Validation Loss: 0.8005837798118591\n",
      "Epoch 12033: Training Loss: 0.09914885212977727 Validation Loss: 0.8010266423225403\n",
      "Epoch 12034: Training Loss: 0.09914130965868632 Validation Loss: 0.8002488017082214\n",
      "Epoch 12035: Training Loss: 0.09924224515755971 Validation Loss: 0.8010806441307068\n",
      "Epoch 12036: Training Loss: 0.09928138057390849 Validation Loss: 0.8017986416816711\n",
      "Epoch 12037: Training Loss: 0.09916206697622935 Validation Loss: 0.8017871379852295\n",
      "Epoch 12038: Training Loss: 0.09907387445370357 Validation Loss: 0.8011188507080078\n",
      "Epoch 12039: Training Loss: 0.098691130677859 Validation Loss: 0.8014394044876099\n",
      "Epoch 12040: Training Loss: 0.09883057822783788 Validation Loss: 0.8014737963676453\n",
      "Epoch 12041: Training Loss: 0.09976631651322047 Validation Loss: 0.8014686703681946\n",
      "Epoch 12042: Training Loss: 0.09900041669607162 Validation Loss: 0.8018954992294312\n",
      "Epoch 12043: Training Loss: 0.09917495399713516 Validation Loss: 0.8021783232688904\n",
      "Epoch 12044: Training Loss: 0.09910429517428081 Validation Loss: 0.8020157217979431\n",
      "Epoch 12045: Training Loss: 0.09903672834237416 Validation Loss: 0.8019795417785645\n",
      "Epoch 12046: Training Loss: 0.09896443784236908 Validation Loss: 0.801210343837738\n",
      "Epoch 12047: Training Loss: 0.09946269790331523 Validation Loss: 0.8012799620628357\n",
      "Epoch 12048: Training Loss: 0.09916763007640839 Validation Loss: 0.8010746836662292\n",
      "Epoch 12049: Training Loss: 0.0991408700744311 Validation Loss: 0.8004558682441711\n",
      "Epoch 12050: Training Loss: 0.0994299128651619 Validation Loss: 0.8014081716537476\n",
      "Epoch 12051: Training Loss: 0.09915214280287425 Validation Loss: 0.8013111352920532\n",
      "Epoch 12052: Training Loss: 0.09884137411912282 Validation Loss: 0.8016679286956787\n",
      "Epoch 12053: Training Loss: 0.09915360063314438 Validation Loss: 0.8024442195892334\n",
      "Epoch 12054: Training Loss: 0.09943800667921703 Validation Loss: 0.8037645816802979\n",
      "Epoch 12055: Training Loss: 0.09910157322883606 Validation Loss: 0.801806628704071\n",
      "Epoch 12056: Training Loss: 0.09994722654422124 Validation Loss: 0.8005832433700562\n",
      "Epoch 12057: Training Loss: 0.09884850432475407 Validation Loss: 0.8006769418716431\n",
      "Epoch 12058: Training Loss: 0.09918469687302907 Validation Loss: 0.8008931279182434\n",
      "Epoch 12059: Training Loss: 0.09896131853262584 Validation Loss: 0.8020339608192444\n",
      "Epoch 12060: Training Loss: 0.09902329494555791 Validation Loss: 0.803023636341095\n",
      "Epoch 12061: Training Loss: 0.0989559938510259 Validation Loss: 0.8028404712677002\n",
      "Epoch 12062: Training Loss: 0.09877291321754456 Validation Loss: 0.80162113904953\n",
      "Epoch 12063: Training Loss: 0.09968405216932297 Validation Loss: 0.8007823824882507\n",
      "Epoch 12064: Training Loss: 0.09928193191687266 Validation Loss: 0.8001231551170349\n",
      "Epoch 12065: Training Loss: 0.09920643518368404 Validation Loss: 0.8013200163841248\n",
      "Epoch 12066: Training Loss: 0.09892306228478749 Validation Loss: 0.8009290099143982\n",
      "Epoch 12067: Training Loss: 0.09899481137593587 Validation Loss: 0.8014428615570068\n",
      "Epoch 12068: Training Loss: 0.09936198343833287 Validation Loss: 0.8014721274375916\n",
      "Epoch 12069: Training Loss: 0.09876817713181178 Validation Loss: 0.8027327060699463\n",
      "Epoch 12070: Training Loss: 0.0988187591234843 Validation Loss: 0.8024317026138306\n",
      "Epoch 12071: Training Loss: 0.09889886528253555 Validation Loss: 0.8030877709388733\n",
      "Epoch 12072: Training Loss: 0.09955006589492162 Validation Loss: 0.8016872406005859\n",
      "Epoch 12073: Training Loss: 0.09881558269262314 Validation Loss: 0.8020920753479004\n",
      "Epoch 12074: Training Loss: 0.09897895654042561 Validation Loss: 0.8026643991470337\n",
      "Epoch 12075: Training Loss: 0.09896757205327351 Validation Loss: 0.8035297393798828\n",
      "Epoch 12076: Training Loss: 0.09865182389815648 Validation Loss: 0.801852285861969\n",
      "Epoch 12077: Training Loss: 0.09916842728853226 Validation Loss: 0.8015722632408142\n",
      "Epoch 12078: Training Loss: 0.09906393786271413 Validation Loss: 0.8006446957588196\n",
      "Epoch 12079: Training Loss: 0.0993639628092448 Validation Loss: 0.8022964000701904\n",
      "Epoch 12080: Training Loss: 0.09882928679386775 Validation Loss: 0.8020408153533936\n",
      "Epoch 12081: Training Loss: 0.09889650096495946 Validation Loss: 0.8025367259979248\n",
      "Epoch 12082: Training Loss: 0.09913011143604915 Validation Loss: 0.8030650615692139\n",
      "Epoch 12083: Training Loss: 0.09903099139531453 Validation Loss: 0.8014641404151917\n",
      "Epoch 12084: Training Loss: 0.09859083344539006 Validation Loss: 0.8009346723556519\n",
      "Epoch 12085: Training Loss: 0.09895630925893784 Validation Loss: 0.801633894443512\n",
      "Epoch 12086: Training Loss: 0.09886190543572108 Validation Loss: 0.8013789653778076\n",
      "Epoch 12087: Training Loss: 0.09909275422493617 Validation Loss: 0.8016089797019958\n",
      "Epoch 12088: Training Loss: 0.09944588442643483 Validation Loss: 0.8029047250747681\n",
      "Epoch 12089: Training Loss: 0.09940228114525478 Validation Loss: 0.8030534386634827\n",
      "Epoch 12090: Training Loss: 0.09912057717641194 Validation Loss: 0.8023400902748108\n",
      "Epoch 12091: Training Loss: 0.09888491034507751 Validation Loss: 0.801007091999054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12092: Training Loss: 0.09965149561564128 Validation Loss: 0.800159215927124\n",
      "Epoch 12093: Training Loss: 0.09881771107514699 Validation Loss: 0.8007925152778625\n",
      "Epoch 12094: Training Loss: 0.09895220398902893 Validation Loss: 0.8027592897415161\n",
      "Epoch 12095: Training Loss: 0.09888586650292079 Validation Loss: 0.8032130002975464\n",
      "Epoch 12096: Training Loss: 0.0987214595079422 Validation Loss: 0.8022282719612122\n",
      "Epoch 12097: Training Loss: 0.09881072243054707 Validation Loss: 0.8010948300361633\n",
      "Epoch 12098: Training Loss: 0.09879315147797267 Validation Loss: 0.8014187812805176\n",
      "Epoch 12099: Training Loss: 0.09897969911495845 Validation Loss: 0.8022199869155884\n",
      "Epoch 12100: Training Loss: 0.09880397965510686 Validation Loss: 0.8022074103355408\n",
      "Epoch 12101: Training Loss: 0.0987627034385999 Validation Loss: 0.8015706539154053\n",
      "Epoch 12102: Training Loss: 0.09880703936020534 Validation Loss: 0.801329493522644\n",
      "Epoch 12103: Training Loss: 0.09891747683286667 Validation Loss: 0.8023533821105957\n",
      "Epoch 12104: Training Loss: 0.09907546142737071 Validation Loss: 0.8025269508361816\n",
      "Epoch 12105: Training Loss: 0.09853120396534602 Validation Loss: 0.802379310131073\n",
      "Epoch 12106: Training Loss: 0.09867155800263087 Validation Loss: 0.8030243515968323\n",
      "Epoch 12107: Training Loss: 0.09883454690376918 Validation Loss: 0.8030814528465271\n",
      "Epoch 12108: Training Loss: 0.09890441099802653 Validation Loss: 0.8023194670677185\n",
      "Epoch 12109: Training Loss: 0.09888978054126103 Validation Loss: 0.8016965389251709\n",
      "Epoch 12110: Training Loss: 0.09914437929789226 Validation Loss: 0.801537036895752\n",
      "Epoch 12111: Training Loss: 0.09888691703478496 Validation Loss: 0.802249014377594\n",
      "Epoch 12112: Training Loss: 0.09871901571750641 Validation Loss: 0.802422821521759\n",
      "Epoch 12113: Training Loss: 0.09880772233009338 Validation Loss: 0.8020010590553284\n",
      "Epoch 12114: Training Loss: 0.09870353837807973 Validation Loss: 0.8021517395973206\n",
      "Epoch 12115: Training Loss: 0.09860026836395264 Validation Loss: 0.8025878071784973\n",
      "Epoch 12116: Training Loss: 0.09889069199562073 Validation Loss: 0.8020477294921875\n",
      "Epoch 12117: Training Loss: 0.09872811784346898 Validation Loss: 0.8018823862075806\n",
      "Epoch 12118: Training Loss: 0.09883872916301091 Validation Loss: 0.8025361895561218\n",
      "Epoch 12119: Training Loss: 0.09882927934328715 Validation Loss: 0.8030834794044495\n",
      "Epoch 12120: Training Loss: 0.09863700966040294 Validation Loss: 0.8024501800537109\n",
      "Epoch 12121: Training Loss: 0.09892474114894867 Validation Loss: 0.8018770217895508\n",
      "Epoch 12122: Training Loss: 0.09882164498170216 Validation Loss: 0.8023942112922668\n",
      "Epoch 12123: Training Loss: 0.09865988790988922 Validation Loss: 0.8021582365036011\n",
      "Epoch 12124: Training Loss: 0.09886733442544937 Validation Loss: 0.803200900554657\n",
      "Epoch 12125: Training Loss: 0.09883822252353032 Validation Loss: 0.8029711842536926\n",
      "Epoch 12126: Training Loss: 0.09881596763928731 Validation Loss: 0.8026165962219238\n",
      "Epoch 12127: Training Loss: 0.09928528964519501 Validation Loss: 0.8021042346954346\n",
      "Epoch 12128: Training Loss: 0.0993108054002126 Validation Loss: 0.8017419576644897\n",
      "Epoch 12129: Training Loss: 0.09873344500859578 Validation Loss: 0.8021023869514465\n",
      "Epoch 12130: Training Loss: 0.09823933988809586 Validation Loss: 0.8025968670845032\n",
      "Epoch 12131: Training Loss: 0.09895183145999908 Validation Loss: 0.8028615117073059\n",
      "Epoch 12132: Training Loss: 0.09898318598667781 Validation Loss: 0.8043732047080994\n",
      "Epoch 12133: Training Loss: 0.0987051675717036 Validation Loss: 0.8031596541404724\n",
      "Epoch 12134: Training Loss: 0.09894000242153804 Validation Loss: 0.8017199635505676\n",
      "Epoch 12135: Training Loss: 0.09855738778909047 Validation Loss: 0.8020387887954712\n",
      "Epoch 12136: Training Loss: 0.09852680563926697 Validation Loss: 0.8022480010986328\n",
      "Epoch 12137: Training Loss: 0.09922974556684494 Validation Loss: 0.8037660121917725\n",
      "Epoch 12138: Training Loss: 0.09869776666164398 Validation Loss: 0.8028088808059692\n",
      "Epoch 12139: Training Loss: 0.09859612087408702 Validation Loss: 0.802483856678009\n",
      "Epoch 12140: Training Loss: 0.09855282306671143 Validation Loss: 0.8017274141311646\n",
      "Epoch 12141: Training Loss: 0.09884705394506454 Validation Loss: 0.80217045545578\n",
      "Epoch 12142: Training Loss: 0.09862874199946721 Validation Loss: 0.8021289706230164\n",
      "Epoch 12143: Training Loss: 0.09859711925188701 Validation Loss: 0.8030007481575012\n",
      "Epoch 12144: Training Loss: 0.09901910275220871 Validation Loss: 0.8029263019561768\n",
      "Epoch 12145: Training Loss: 0.0984079937140147 Validation Loss: 0.8028572201728821\n",
      "Epoch 12146: Training Loss: 0.09851965308189392 Validation Loss: 0.8025309443473816\n",
      "Epoch 12147: Training Loss: 0.0987555980682373 Validation Loss: 0.8019732236862183\n",
      "Epoch 12148: Training Loss: 0.09847902009884517 Validation Loss: 0.8028673529624939\n",
      "Epoch 12149: Training Loss: 0.09885368992884953 Validation Loss: 0.8030132055282593\n",
      "Epoch 12150: Training Loss: 0.09875209877888362 Validation Loss: 0.8030867576599121\n",
      "Epoch 12151: Training Loss: 0.09861679126818974 Validation Loss: 0.8031876087188721\n",
      "Epoch 12152: Training Loss: 0.09919776519139607 Validation Loss: 0.8032662868499756\n",
      "Epoch 12153: Training Loss: 0.09887814025084178 Validation Loss: 0.8015400171279907\n",
      "Epoch 12154: Training Loss: 0.09848704189062119 Validation Loss: 0.8017429709434509\n",
      "Epoch 12155: Training Loss: 0.0984847644964854 Validation Loss: 0.8020623922348022\n",
      "Epoch 12156: Training Loss: 0.0986253023147583 Validation Loss: 0.8032651543617249\n",
      "Epoch 12157: Training Loss: 0.09879536181688309 Validation Loss: 0.8038606643676758\n",
      "Epoch 12158: Training Loss: 0.09893781691789627 Validation Loss: 0.8040751218795776\n",
      "Epoch 12159: Training Loss: 0.09839368611574173 Validation Loss: 0.8029575943946838\n",
      "Epoch 12160: Training Loss: 0.09842124084631602 Validation Loss: 0.8029685020446777\n",
      "Epoch 12161: Training Loss: 0.09859073410431544 Validation Loss: 0.8023862838745117\n",
      "Epoch 12162: Training Loss: 0.0987999935944875 Validation Loss: 0.8032694458961487\n",
      "Epoch 12163: Training Loss: 0.09838152925173442 Validation Loss: 0.8032689094543457\n",
      "Epoch 12164: Training Loss: 0.0984972318013509 Validation Loss: 0.8037793040275574\n",
      "Epoch 12165: Training Loss: 0.09855996072292328 Validation Loss: 0.8032217621803284\n",
      "Epoch 12166: Training Loss: 0.09850967178742091 Validation Loss: 0.802812397480011\n",
      "Epoch 12167: Training Loss: 0.09912201762199402 Validation Loss: 0.8036845326423645\n",
      "Epoch 12168: Training Loss: 0.0990154395500819 Validation Loss: 0.8033655881881714\n",
      "Epoch 12169: Training Loss: 0.09864458690087001 Validation Loss: 0.8029428124427795\n",
      "Epoch 12170: Training Loss: 0.09866869946320851 Validation Loss: 0.8021475672721863\n",
      "Epoch 12171: Training Loss: 0.09894593805074692 Validation Loss: 0.8032974004745483\n",
      "Epoch 12172: Training Loss: 0.09852240234613419 Validation Loss: 0.8040477633476257\n",
      "Epoch 12173: Training Loss: 0.09830942501624425 Validation Loss: 0.8043385744094849\n",
      "Epoch 12174: Training Loss: 0.09878326704104741 Validation Loss: 0.804929256439209\n",
      "Epoch 12175: Training Loss: 0.09821750471989314 Validation Loss: 0.804326593875885\n",
      "Epoch 12176: Training Loss: 0.0985417515039444 Validation Loss: 0.8026297092437744\n",
      "Epoch 12177: Training Loss: 0.09840194135904312 Validation Loss: 0.8024182915687561\n",
      "Epoch 12178: Training Loss: 0.09860577434301376 Validation Loss: 0.8016061782836914\n",
      "Epoch 12179: Training Loss: 0.09891187647978465 Validation Loss: 0.8030946254730225\n",
      "Epoch 12180: Training Loss: 0.0985796923438708 Validation Loss: 0.804295539855957\n",
      "Epoch 12181: Training Loss: 0.09879313906033833 Validation Loss: 0.804940402507782\n",
      "Epoch 12182: Training Loss: 0.09842066466808319 Validation Loss: 0.8047927618026733\n",
      "Epoch 12183: Training Loss: 0.09885700543721516 Validation Loss: 0.8036815524101257\n",
      "Epoch 12184: Training Loss: 0.09845912208159764 Validation Loss: 0.8033317923545837\n",
      "Epoch 12185: Training Loss: 0.09837765743335088 Validation Loss: 0.8027550578117371\n",
      "Epoch 12186: Training Loss: 0.0983617976307869 Validation Loss: 0.8028484582901001\n",
      "Epoch 12187: Training Loss: 0.09809102614720662 Validation Loss: 0.8028971552848816\n",
      "Epoch 12188: Training Loss: 0.09836844851573308 Validation Loss: 0.8041033148765564\n",
      "Epoch 12189: Training Loss: 0.09857260187466939 Validation Loss: 0.8041520118713379\n",
      "Epoch 12190: Training Loss: 0.09851546088854472 Validation Loss: 0.8033227920532227\n",
      "Epoch 12191: Training Loss: 0.09860120713710785 Validation Loss: 0.8026242256164551\n",
      "Epoch 12192: Training Loss: 0.09847743064165115 Validation Loss: 0.8025953769683838\n",
      "Epoch 12193: Training Loss: 0.09902119388182958 Validation Loss: 0.8038784861564636\n",
      "Epoch 12194: Training Loss: 0.09854163229465485 Validation Loss: 0.8043090105056763\n",
      "Epoch 12195: Training Loss: 0.09845783809820811 Validation Loss: 0.8047149181365967\n",
      "Epoch 12196: Training Loss: 0.09860129406054814 Validation Loss: 0.8027193546295166\n",
      "Epoch 12197: Training Loss: 0.09859753400087357 Validation Loss: 0.8019838333129883\n",
      "Epoch 12198: Training Loss: 0.09907417247692744 Validation Loss: 0.8025909662246704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12199: Training Loss: 0.0982426901658376 Validation Loss: 0.8021460771560669\n",
      "Epoch 12200: Training Loss: 0.09891328712304433 Validation Loss: 0.8048015236854553\n",
      "Epoch 12201: Training Loss: 0.0986994778116544 Validation Loss: 0.8045356273651123\n",
      "Epoch 12202: Training Loss: 0.09891428301731746 Validation Loss: 0.8031277656555176\n",
      "Epoch 12203: Training Loss: 0.09883077939351399 Validation Loss: 0.802535891532898\n",
      "Epoch 12204: Training Loss: 0.09834287812312444 Validation Loss: 0.802419900894165\n",
      "Epoch 12205: Training Loss: 0.09851126372814178 Validation Loss: 0.8030567169189453\n",
      "Epoch 12206: Training Loss: 0.09867494304974873 Validation Loss: 0.8033800721168518\n",
      "Epoch 12207: Training Loss: 0.09903828551371892 Validation Loss: 0.8037583827972412\n",
      "Epoch 12208: Training Loss: 0.09891119847695033 Validation Loss: 0.8044196963310242\n",
      "Epoch 12209: Training Loss: 0.09854235748449962 Validation Loss: 0.8041495084762573\n",
      "Epoch 12210: Training Loss: 0.09851397822300594 Validation Loss: 0.8039463758468628\n",
      "Epoch 12211: Training Loss: 0.09839530785878499 Validation Loss: 0.8036367893218994\n",
      "Epoch 12212: Training Loss: 0.09870082139968872 Validation Loss: 0.8043389916419983\n",
      "Epoch 12213: Training Loss: 0.0983579158782959 Validation Loss: 0.804510235786438\n",
      "Epoch 12214: Training Loss: 0.09837494045495987 Validation Loss: 0.8030917644500732\n",
      "Epoch 12215: Training Loss: 0.09874421606461208 Validation Loss: 0.8032800555229187\n",
      "Epoch 12216: Training Loss: 0.09835394720236461 Validation Loss: 0.803461492061615\n",
      "Epoch 12217: Training Loss: 0.09892792751391728 Validation Loss: 0.8043310642242432\n",
      "Epoch 12218: Training Loss: 0.09827742477258046 Validation Loss: 0.8035011291503906\n",
      "Epoch 12219: Training Loss: 0.09850061188141505 Validation Loss: 0.8034101724624634\n",
      "Epoch 12220: Training Loss: 0.09829801321029663 Validation Loss: 0.8031041622161865\n",
      "Epoch 12221: Training Loss: 0.09858120232820511 Validation Loss: 0.8033101558685303\n",
      "Epoch 12222: Training Loss: 0.0981958881020546 Validation Loss: 0.8040959239006042\n",
      "Epoch 12223: Training Loss: 0.09829320013523102 Validation Loss: 0.8045698404312134\n",
      "Epoch 12224: Training Loss: 0.09815320620934169 Validation Loss: 0.8044725656509399\n",
      "Epoch 12225: Training Loss: 0.09938336163759232 Validation Loss: 0.8037014603614807\n",
      "Epoch 12226: Training Loss: 0.0982940544684728 Validation Loss: 0.8032045364379883\n",
      "Epoch 12227: Training Loss: 0.09832244118054707 Validation Loss: 0.8036552667617798\n",
      "Epoch 12228: Training Loss: 0.09864798436562221 Validation Loss: 0.8037039637565613\n",
      "Epoch 12229: Training Loss: 0.09879045188426971 Validation Loss: 0.8056291937828064\n",
      "Epoch 12230: Training Loss: 0.09843566517035167 Validation Loss: 0.8055958151817322\n",
      "Epoch 12231: Training Loss: 0.09822073827187221 Validation Loss: 0.8054027557373047\n",
      "Epoch 12232: Training Loss: 0.09836549311876297 Validation Loss: 0.8035419583320618\n",
      "Epoch 12233: Training Loss: 0.09846251457929611 Validation Loss: 0.8032230734825134\n",
      "Epoch 12234: Training Loss: 0.09807600826025009 Validation Loss: 0.8030593395233154\n",
      "Epoch 12235: Training Loss: 0.09835072110096614 Validation Loss: 0.803396999835968\n",
      "Epoch 12236: Training Loss: 0.09808751444021861 Validation Loss: 0.8042339086532593\n",
      "Epoch 12237: Training Loss: 0.09831238786379497 Validation Loss: 0.805370032787323\n",
      "Epoch 12238: Training Loss: 0.09848356495300929 Validation Loss: 0.8047255277633667\n",
      "Epoch 12239: Training Loss: 0.0980811541279157 Validation Loss: 0.8043873906135559\n",
      "Epoch 12240: Training Loss: 0.09825430810451508 Validation Loss: 0.8026949763298035\n",
      "Epoch 12241: Training Loss: 0.09828030566374461 Validation Loss: 0.8018273711204529\n",
      "Epoch 12242: Training Loss: 0.098392387231191 Validation Loss: 0.8024674654006958\n",
      "Epoch 12243: Training Loss: 0.09864234675963719 Validation Loss: 0.8023390173912048\n",
      "Epoch 12244: Training Loss: 0.09856294343868892 Validation Loss: 0.8046093583106995\n",
      "Epoch 12245: Training Loss: 0.09816907594601314 Validation Loss: 0.8044548034667969\n",
      "Epoch 12246: Training Loss: 0.09814036637544632 Validation Loss: 0.80423504114151\n",
      "Epoch 12247: Training Loss: 0.09832526743412018 Validation Loss: 0.8035295009613037\n",
      "Epoch 12248: Training Loss: 0.09872691333293915 Validation Loss: 0.8046207427978516\n",
      "Epoch 12249: Training Loss: 0.0983997533718745 Validation Loss: 0.803763210773468\n",
      "Epoch 12250: Training Loss: 0.0982860525449117 Validation Loss: 0.8041704893112183\n",
      "Epoch 12251: Training Loss: 0.09845929344495137 Validation Loss: 0.8037055730819702\n",
      "Epoch 12252: Training Loss: 0.09824387480815251 Validation Loss: 0.8037124276161194\n",
      "Epoch 12253: Training Loss: 0.09808370222647984 Validation Loss: 0.8042530417442322\n",
      "Epoch 12254: Training Loss: 0.09804865966240565 Validation Loss: 0.8048988580703735\n",
      "Epoch 12255: Training Loss: 0.0979355921347936 Validation Loss: 0.8047035932540894\n",
      "Epoch 12256: Training Loss: 0.09790182610352834 Validation Loss: 0.8044179081916809\n",
      "Epoch 12257: Training Loss: 0.09885802368323009 Validation Loss: 0.8036783933639526\n",
      "Epoch 12258: Training Loss: 0.09796492010354996 Validation Loss: 0.8036870360374451\n",
      "Epoch 12259: Training Loss: 0.09879475583632787 Validation Loss: 0.8035921454429626\n",
      "Epoch 12260: Training Loss: 0.09865929434696834 Validation Loss: 0.8039641976356506\n",
      "Epoch 12261: Training Loss: 0.09799999743700027 Validation Loss: 0.8028658628463745\n",
      "Epoch 12262: Training Loss: 0.0982166404525439 Validation Loss: 0.8030250072479248\n",
      "Epoch 12263: Training Loss: 0.09809570511182149 Validation Loss: 0.8037630915641785\n",
      "Epoch 12264: Training Loss: 0.09826234479745229 Validation Loss: 0.805212140083313\n",
      "Epoch 12265: Training Loss: 0.0983552560210228 Validation Loss: 0.805997908115387\n",
      "Epoch 12266: Training Loss: 0.09870530168215434 Validation Loss: 0.8045572638511658\n",
      "Epoch 12267: Training Loss: 0.09802871942520142 Validation Loss: 0.804616391658783\n",
      "Epoch 12268: Training Loss: 0.09860535214344661 Validation Loss: 0.8044856786727905\n",
      "Epoch 12269: Training Loss: 0.09852822373310725 Validation Loss: 0.8044586181640625\n",
      "Epoch 12270: Training Loss: 0.09814335157473882 Validation Loss: 0.8043774366378784\n",
      "Epoch 12271: Training Loss: 0.09785148253043492 Validation Loss: 0.804344654083252\n",
      "Epoch 12272: Training Loss: 0.09848431249459584 Validation Loss: 0.8034374117851257\n",
      "Epoch 12273: Training Loss: 0.09794671088457108 Validation Loss: 0.8036064505577087\n",
      "Epoch 12274: Training Loss: 0.0980609878897667 Validation Loss: 0.8037986159324646\n",
      "Epoch 12275: Training Loss: 0.09797896196444829 Validation Loss: 0.8037000894546509\n",
      "Epoch 12276: Training Loss: 0.09795803080002467 Validation Loss: 0.8039858937263489\n",
      "Epoch 12277: Training Loss: 0.09814873337745667 Validation Loss: 0.8040410280227661\n",
      "Epoch 12278: Training Loss: 0.0980762466788292 Validation Loss: 0.8053890466690063\n",
      "Epoch 12279: Training Loss: 0.09789935251077016 Validation Loss: 0.8051450252532959\n",
      "Epoch 12280: Training Loss: 0.09807377556959788 Validation Loss: 0.8042472004890442\n",
      "Epoch 12281: Training Loss: 0.09805746376514435 Validation Loss: 0.8037756681442261\n",
      "Epoch 12282: Training Loss: 0.09791542341311772 Validation Loss: 0.8033292889595032\n",
      "Epoch 12283: Training Loss: 0.09788903345664342 Validation Loss: 0.8038550615310669\n",
      "Epoch 12284: Training Loss: 0.09801927953958511 Validation Loss: 0.8039188385009766\n",
      "Epoch 12285: Training Loss: 0.0980379581451416 Validation Loss: 0.8049013614654541\n",
      "Epoch 12286: Training Loss: 0.09784507006406784 Validation Loss: 0.8054149150848389\n",
      "Epoch 12287: Training Loss: 0.09782692541678746 Validation Loss: 0.804840624332428\n",
      "Epoch 12288: Training Loss: 0.09815745552380879 Validation Loss: 0.8040982484817505\n",
      "Epoch 12289: Training Loss: 0.09803308049837749 Validation Loss: 0.8041848540306091\n",
      "Epoch 12290: Training Loss: 0.09808032711346944 Validation Loss: 0.804029643535614\n",
      "Epoch 12291: Training Loss: 0.09771133214235306 Validation Loss: 0.8039535880088806\n",
      "Epoch 12292: Training Loss: 0.09788915266593297 Validation Loss: 0.8044338226318359\n",
      "Epoch 12293: Training Loss: 0.0980484386285146 Validation Loss: 0.8053948283195496\n",
      "Epoch 12294: Training Loss: 0.09809164206186931 Validation Loss: 0.805435299873352\n",
      "Epoch 12295: Training Loss: 0.0979648157954216 Validation Loss: 0.8045571446418762\n",
      "Epoch 12296: Training Loss: 0.0980113297700882 Validation Loss: 0.8048450946807861\n",
      "Epoch 12297: Training Loss: 0.09774875889221828 Validation Loss: 0.8047311305999756\n",
      "Epoch 12298: Training Loss: 0.0977599894007047 Validation Loss: 0.8047364950180054\n",
      "Epoch 12299: Training Loss: 0.09814660251140594 Validation Loss: 0.8052052855491638\n",
      "Epoch 12300: Training Loss: 0.0979107196132342 Validation Loss: 0.8048696517944336\n",
      "Epoch 12301: Training Loss: 0.09789667278528214 Validation Loss: 0.8054164052009583\n",
      "Epoch 12302: Training Loss: 0.09817688167095184 Validation Loss: 0.8055658340454102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12303: Training Loss: 0.09847389409939449 Validation Loss: 0.8043764233589172\n",
      "Epoch 12304: Training Loss: 0.0983504205942154 Validation Loss: 0.8053300380706787\n",
      "Epoch 12305: Training Loss: 0.09829890727996826 Validation Loss: 0.8046756386756897\n",
      "Epoch 12306: Training Loss: 0.09791176517804463 Validation Loss: 0.8050970435142517\n",
      "Epoch 12307: Training Loss: 0.0981223185857137 Validation Loss: 0.8061990141868591\n",
      "Epoch 12308: Training Loss: 0.0979554404815038 Validation Loss: 0.8049710988998413\n",
      "Epoch 12309: Training Loss: 0.09795073419809341 Validation Loss: 0.804280698299408\n",
      "Epoch 12310: Training Loss: 0.09774656842152278 Validation Loss: 0.8036261796951294\n",
      "Epoch 12311: Training Loss: 0.09767039368549983 Validation Loss: 0.8032631874084473\n",
      "Epoch 12312: Training Loss: 0.09790248423814774 Validation Loss: 0.8037376403808594\n",
      "Epoch 12313: Training Loss: 0.09792938580115636 Validation Loss: 0.8047000765800476\n",
      "Epoch 12314: Training Loss: 0.09809401383002599 Validation Loss: 0.8059694170951843\n",
      "Epoch 12315: Training Loss: 0.0980043535431226 Validation Loss: 0.8056684732437134\n",
      "Epoch 12316: Training Loss: 0.09788306305805843 Validation Loss: 0.805915117263794\n",
      "Epoch 12317: Training Loss: 0.09776776532332103 Validation Loss: 0.8052577972412109\n",
      "Epoch 12318: Training Loss: 0.09846420337756474 Validation Loss: 0.8036986589431763\n",
      "Epoch 12319: Training Loss: 0.0979326715071996 Validation Loss: 0.8038654923439026\n",
      "Epoch 12320: Training Loss: 0.09798885136842728 Validation Loss: 0.8045929670333862\n",
      "Epoch 12321: Training Loss: 0.0979281837741534 Validation Loss: 0.8055480718612671\n",
      "Epoch 12322: Training Loss: 0.09747117757797241 Validation Loss: 0.8050201535224915\n",
      "Epoch 12323: Training Loss: 0.0980498418211937 Validation Loss: 0.8050746321678162\n",
      "Epoch 12324: Training Loss: 0.09773976604143779 Validation Loss: 0.8050075173377991\n",
      "Epoch 12325: Training Loss: 0.09863784164190292 Validation Loss: 0.803064227104187\n",
      "Epoch 12326: Training Loss: 0.09772730867067973 Validation Loss: 0.8030975461006165\n",
      "Epoch 12327: Training Loss: 0.09788867831230164 Validation Loss: 0.8041921854019165\n",
      "Epoch 12328: Training Loss: 0.0978807881474495 Validation Loss: 0.8060457110404968\n",
      "Epoch 12329: Training Loss: 0.09818705668052037 Validation Loss: 0.8056349754333496\n",
      "Epoch 12330: Training Loss: 0.09795323510964711 Validation Loss: 0.8063444495201111\n",
      "Epoch 12331: Training Loss: 0.09727511554956436 Validation Loss: 0.8065537810325623\n",
      "Epoch 12332: Training Loss: 0.09794233242670695 Validation Loss: 0.805754542350769\n",
      "Epoch 12333: Training Loss: 0.09847459693749745 Validation Loss: 0.80565345287323\n",
      "Epoch 12334: Training Loss: 0.09766740103562672 Validation Loss: 0.8053554892539978\n",
      "Epoch 12335: Training Loss: 0.0982691968480746 Validation Loss: 0.8051908612251282\n",
      "Epoch 12336: Training Loss: 0.098286934196949 Validation Loss: 0.8051594495773315\n",
      "Epoch 12337: Training Loss: 0.09768816083669662 Validation Loss: 0.8051495552062988\n",
      "Epoch 12338: Training Loss: 0.09774830689032872 Validation Loss: 0.8049483895301819\n",
      "Epoch 12339: Training Loss: 0.09768349180618922 Validation Loss: 0.8050014972686768\n",
      "Epoch 12340: Training Loss: 0.09790221601724625 Validation Loss: 0.8045039176940918\n",
      "Epoch 12341: Training Loss: 0.09774901966253917 Validation Loss: 0.8035224080085754\n",
      "Epoch 12342: Training Loss: 0.09774345407883327 Validation Loss: 0.8043103218078613\n",
      "Epoch 12343: Training Loss: 0.09789980947971344 Validation Loss: 0.8051713109016418\n",
      "Epoch 12344: Training Loss: 0.09787214795748393 Validation Loss: 0.8053531646728516\n",
      "Epoch 12345: Training Loss: 0.09800541152556737 Validation Loss: 0.8051354289054871\n",
      "Epoch 12346: Training Loss: 0.09775370856126149 Validation Loss: 0.8052526712417603\n",
      "Epoch 12347: Training Loss: 0.09779038528601329 Validation Loss: 0.806113600730896\n",
      "Epoch 12348: Training Loss: 0.09758430967728297 Validation Loss: 0.806645393371582\n",
      "Epoch 12349: Training Loss: 0.09775434682766597 Validation Loss: 0.8057360649108887\n",
      "Epoch 12350: Training Loss: 0.09829266369342804 Validation Loss: 0.805124044418335\n",
      "Epoch 12351: Training Loss: 0.09817660351594289 Validation Loss: 0.8040880560874939\n",
      "Epoch 12352: Training Loss: 0.09773047516743343 Validation Loss: 0.8051332235336304\n",
      "Epoch 12353: Training Loss: 0.09790502240260442 Validation Loss: 0.80692458152771\n",
      "Epoch 12354: Training Loss: 0.0978163555264473 Validation Loss: 0.8061501383781433\n",
      "Epoch 12355: Training Loss: 0.09780558198690414 Validation Loss: 0.805169403553009\n",
      "Epoch 12356: Training Loss: 0.09808344642321269 Validation Loss: 0.8043870329856873\n",
      "Epoch 12357: Training Loss: 0.0970390463868777 Validation Loss: 0.8046997785568237\n",
      "Epoch 12358: Training Loss: 0.097674660384655 Validation Loss: 0.805355966091156\n",
      "Epoch 12359: Training Loss: 0.09786265840133031 Validation Loss: 0.8062406182289124\n",
      "Epoch 12360: Training Loss: 0.09700896590948105 Validation Loss: 0.806981086730957\n",
      "Epoch 12361: Training Loss: 0.09784538795550664 Validation Loss: 0.8063838481903076\n",
      "Epoch 12362: Training Loss: 0.09807021915912628 Validation Loss: 0.8052154183387756\n",
      "Epoch 12363: Training Loss: 0.09745418032010396 Validation Loss: 0.8041293025016785\n",
      "Epoch 12364: Training Loss: 0.09750671188036601 Validation Loss: 0.8046532869338989\n",
      "Epoch 12365: Training Loss: 0.09739656994740169 Validation Loss: 0.8046409487724304\n",
      "Epoch 12366: Training Loss: 0.09764807174603145 Validation Loss: 0.8048573732376099\n",
      "Epoch 12367: Training Loss: 0.09782104939222336 Validation Loss: 0.8055798411369324\n",
      "Epoch 12368: Training Loss: 0.09779859085877736 Validation Loss: 0.8063442707061768\n",
      "Epoch 12369: Training Loss: 0.09776215255260468 Validation Loss: 0.8053999543190002\n",
      "Epoch 12370: Training Loss: 0.0978007862965266 Validation Loss: 0.8059083223342896\n",
      "Epoch 12371: Training Loss: 0.098530612885952 Validation Loss: 0.8051968216896057\n",
      "Epoch 12372: Training Loss: 0.09752954045931499 Validation Loss: 0.8049181699752808\n",
      "Epoch 12373: Training Loss: 0.0978035032749176 Validation Loss: 0.8062419891357422\n",
      "Epoch 12374: Training Loss: 0.09773455808560054 Validation Loss: 0.8056868314743042\n",
      "Epoch 12375: Training Loss: 0.09759388118982315 Validation Loss: 0.8062089681625366\n",
      "Epoch 12376: Training Loss: 0.09773712356885274 Validation Loss: 0.8062785863876343\n",
      "Epoch 12377: Training Loss: 0.09754790862401326 Validation Loss: 0.8059316277503967\n",
      "Epoch 12378: Training Loss: 0.09732357660929362 Validation Loss: 0.8058146238327026\n",
      "Epoch 12379: Training Loss: 0.0978134348988533 Validation Loss: 0.8062902092933655\n",
      "Epoch 12380: Training Loss: 0.09756344060103099 Validation Loss: 0.8057092428207397\n",
      "Epoch 12381: Training Loss: 0.09757813811302185 Validation Loss: 0.8054786920547485\n",
      "Epoch 12382: Training Loss: 0.0980853761235873 Validation Loss: 0.8054255843162537\n",
      "Epoch 12383: Training Loss: 0.09767131259044011 Validation Loss: 0.8054125308990479\n",
      "Epoch 12384: Training Loss: 0.09764187534650166 Validation Loss: 0.8057703375816345\n",
      "Epoch 12385: Training Loss: 0.09751856078704198 Validation Loss: 0.8061453104019165\n",
      "Epoch 12386: Training Loss: 0.09747766703367233 Validation Loss: 0.8062911629676819\n",
      "Epoch 12387: Training Loss: 0.09787704050540924 Validation Loss: 0.8062434792518616\n",
      "Epoch 12388: Training Loss: 0.09757047146558762 Validation Loss: 0.8056828379631042\n",
      "Epoch 12389: Training Loss: 0.09746741006771724 Validation Loss: 0.8051596879959106\n",
      "Epoch 12390: Training Loss: 0.09743504971265793 Validation Loss: 0.8047719597816467\n",
      "Epoch 12391: Training Loss: 0.09742085387309392 Validation Loss: 0.8053117990493774\n",
      "Epoch 12392: Training Loss: 0.09763617813587189 Validation Loss: 0.8066437244415283\n",
      "Epoch 12393: Training Loss: 0.0981065755089124 Validation Loss: 0.8066688776016235\n",
      "Epoch 12394: Training Loss: 0.0975160871942838 Validation Loss: 0.8064389824867249\n",
      "Epoch 12395: Training Loss: 0.09729014833768208 Validation Loss: 0.8063582181930542\n",
      "Epoch 12396: Training Loss: 0.09715763727823894 Validation Loss: 0.8054644465446472\n",
      "Epoch 12397: Training Loss: 0.09762101372083028 Validation Loss: 0.8056579232215881\n",
      "Epoch 12398: Training Loss: 0.09782324482997258 Validation Loss: 0.8050329089164734\n",
      "Epoch 12399: Training Loss: 0.09804060061772664 Validation Loss: 0.8045240640640259\n",
      "Epoch 12400: Training Loss: 0.09756191819906235 Validation Loss: 0.8056026101112366\n",
      "Epoch 12401: Training Loss: 0.09802727897961934 Validation Loss: 0.806396484375\n",
      "Epoch 12402: Training Loss: 0.09742744266986847 Validation Loss: 0.8056093454360962\n",
      "Epoch 12403: Training Loss: 0.09772270917892456 Validation Loss: 0.8049551844596863\n",
      "Epoch 12404: Training Loss: 0.0973195880651474 Validation Loss: 0.8048110604286194\n",
      "Epoch 12405: Training Loss: 0.09763133774201076 Validation Loss: 0.8052956461906433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12406: Training Loss: 0.0975959946711858 Validation Loss: 0.8062382936477661\n",
      "Epoch 12407: Training Loss: 0.09735140204429626 Validation Loss: 0.8072142601013184\n",
      "Epoch 12408: Training Loss: 0.09772432347138722 Validation Loss: 0.8080763220787048\n",
      "Epoch 12409: Training Loss: 0.09750223159790039 Validation Loss: 0.8065977096557617\n",
      "Epoch 12410: Training Loss: 0.09736461440722148 Validation Loss: 0.8053418397903442\n",
      "Epoch 12411: Training Loss: 0.09751689434051514 Validation Loss: 0.8048311471939087\n",
      "Epoch 12412: Training Loss: 0.09759024779001872 Validation Loss: 0.8056057095527649\n",
      "Epoch 12413: Training Loss: 0.09860339512427647 Validation Loss: 0.8068478107452393\n",
      "Epoch 12414: Training Loss: 0.09746176501115163 Validation Loss: 0.8070270419120789\n",
      "Epoch 12415: Training Loss: 0.09754470239082973 Validation Loss: 0.8069473505020142\n",
      "Epoch 12416: Training Loss: 0.09809023886919022 Validation Loss: 0.8057944178581238\n",
      "Epoch 12417: Training Loss: 0.09721073756615321 Validation Loss: 0.8054753541946411\n",
      "Epoch 12418: Training Loss: 0.09723599006732304 Validation Loss: 0.8055227398872375\n",
      "Epoch 12419: Training Loss: 0.09748399754365285 Validation Loss: 0.8064642548561096\n",
      "Epoch 12420: Training Loss: 0.09747617443402608 Validation Loss: 0.8064084053039551\n",
      "Epoch 12421: Training Loss: 0.0975225493311882 Validation Loss: 0.806155264377594\n",
      "Epoch 12422: Training Loss: 0.09751364837090175 Validation Loss: 0.8058165311813354\n",
      "Epoch 12423: Training Loss: 0.09751885881026585 Validation Loss: 0.8060999512672424\n",
      "Epoch 12424: Training Loss: 0.09739117821057637 Validation Loss: 0.8054437637329102\n",
      "Epoch 12425: Training Loss: 0.09722710897525151 Validation Loss: 0.8059068918228149\n",
      "Epoch 12426: Training Loss: 0.09793266902367274 Validation Loss: 0.8068578243255615\n",
      "Epoch 12427: Training Loss: 0.09737685322761536 Validation Loss: 0.8063457608222961\n",
      "Epoch 12428: Training Loss: 0.09730283170938492 Validation Loss: 0.8057783246040344\n",
      "Epoch 12429: Training Loss: 0.09739251186450322 Validation Loss: 0.8068501353263855\n",
      "Epoch 12430: Training Loss: 0.0977419763803482 Validation Loss: 0.8057094216346741\n",
      "Epoch 12431: Training Loss: 0.0975089743733406 Validation Loss: 0.8052232265472412\n",
      "Epoch 12432: Training Loss: 0.09754152844349544 Validation Loss: 0.805865466594696\n",
      "Epoch 12433: Training Loss: 0.09785374999046326 Validation Loss: 0.8053587675094604\n",
      "Epoch 12434: Training Loss: 0.0970797911286354 Validation Loss: 0.8063645958900452\n",
      "Epoch 12435: Training Loss: 0.0974293624361356 Validation Loss: 0.8068903684616089\n",
      "Epoch 12436: Training Loss: 0.0977178265651067 Validation Loss: 0.8075466156005859\n",
      "Epoch 12437: Training Loss: 0.09741633633772533 Validation Loss: 0.8069949746131897\n",
      "Epoch 12438: Training Loss: 0.09720537066459656 Validation Loss: 0.8061607480049133\n",
      "Epoch 12439: Training Loss: 0.09736222525437672 Validation Loss: 0.8068486452102661\n",
      "Epoch 12440: Training Loss: 0.09730328619480133 Validation Loss: 0.8062477707862854\n",
      "Epoch 12441: Training Loss: 0.09768423189719518 Validation Loss: 0.8065829873085022\n",
      "Epoch 12442: Training Loss: 0.09750618288914363 Validation Loss: 0.8069509863853455\n",
      "Epoch 12443: Training Loss: 0.0971670572956403 Validation Loss: 0.8065914511680603\n",
      "Epoch 12444: Training Loss: 0.0976307491461436 Validation Loss: 0.8074437379837036\n",
      "Epoch 12445: Training Loss: 0.09732065598169963 Validation Loss: 0.8062177896499634\n",
      "Epoch 12446: Training Loss: 0.0973978117108345 Validation Loss: 0.8055702447891235\n",
      "Epoch 12447: Training Loss: 0.09753565738598506 Validation Loss: 0.8057073950767517\n",
      "Epoch 12448: Training Loss: 0.09742640455563863 Validation Loss: 0.8052983283996582\n",
      "Epoch 12449: Training Loss: 0.09758065144220988 Validation Loss: 0.8055440783500671\n",
      "Epoch 12450: Training Loss: 0.09799292435248692 Validation Loss: 0.8058089017868042\n",
      "Epoch 12451: Training Loss: 0.09737163037061691 Validation Loss: 0.8069474697113037\n",
      "Epoch 12452: Training Loss: 0.09722026685873668 Validation Loss: 0.8072220683097839\n",
      "Epoch 12453: Training Loss: 0.09766281892855962 Validation Loss: 0.8074178695678711\n",
      "Epoch 12454: Training Loss: 0.09745484342177708 Validation Loss: 0.806117594242096\n",
      "Epoch 12455: Training Loss: 0.09752330432335536 Validation Loss: 0.8069425821304321\n",
      "Epoch 12456: Training Loss: 0.09717416763305664 Validation Loss: 0.8056980967521667\n",
      "Epoch 12457: Training Loss: 0.09712229669094086 Validation Loss: 0.8047208189964294\n",
      "Epoch 12458: Training Loss: 0.09739871819814046 Validation Loss: 0.80557781457901\n",
      "Epoch 12459: Training Loss: 0.0969231848915418 Validation Loss: 0.806813657283783\n",
      "Epoch 12460: Training Loss: 0.09719124188025792 Validation Loss: 0.8075703382492065\n",
      "Epoch 12461: Training Loss: 0.09758240729570389 Validation Loss: 0.8078792691230774\n",
      "Epoch 12462: Training Loss: 0.09735872348149617 Validation Loss: 0.8058114647865295\n",
      "Epoch 12463: Training Loss: 0.09702118982871373 Validation Loss: 0.8059495687484741\n",
      "Epoch 12464: Training Loss: 0.09728785355885823 Validation Loss: 0.8064632415771484\n",
      "Epoch 12465: Training Loss: 0.09742965052525203 Validation Loss: 0.8070204257965088\n",
      "Epoch 12466: Training Loss: 0.0971989780664444 Validation Loss: 0.8060285449028015\n",
      "Epoch 12467: Training Loss: 0.09738009174664815 Validation Loss: 0.8062594532966614\n",
      "Epoch 12468: Training Loss: 0.09733288238445918 Validation Loss: 0.8059521913528442\n",
      "Epoch 12469: Training Loss: 0.09721367557843526 Validation Loss: 0.8067090511322021\n",
      "Epoch 12470: Training Loss: 0.09749979277451833 Validation Loss: 0.8070363998413086\n",
      "Epoch 12471: Training Loss: 0.0971234639485677 Validation Loss: 0.8081001043319702\n",
      "Epoch 12472: Training Loss: 0.09723028540611267 Validation Loss: 0.8079338669776917\n",
      "Epoch 12473: Training Loss: 0.09726084768772125 Validation Loss: 0.8081865310668945\n",
      "Epoch 12474: Training Loss: 0.09729078908761342 Validation Loss: 0.8077322840690613\n",
      "Epoch 12475: Training Loss: 0.09750496596097946 Validation Loss: 0.8058276176452637\n",
      "Epoch 12476: Training Loss: 0.0972408006588618 Validation Loss: 0.8059583306312561\n",
      "Epoch 12477: Training Loss: 0.09720987578233083 Validation Loss: 0.8059161901473999\n",
      "Epoch 12478: Training Loss: 0.09724046538273494 Validation Loss: 0.8067908883094788\n",
      "Epoch 12479: Training Loss: 0.09725828220446904 Validation Loss: 0.8060988783836365\n",
      "Epoch 12480: Training Loss: 0.09710871428251266 Validation Loss: 0.8065747022628784\n",
      "Epoch 12481: Training Loss: 0.09778363505999248 Validation Loss: 0.8066391348838806\n",
      "Epoch 12482: Training Loss: 0.09754578024148941 Validation Loss: 0.8076465129852295\n",
      "Epoch 12483: Training Loss: 0.09759483983119328 Validation Loss: 0.8069595098495483\n",
      "Epoch 12484: Training Loss: 0.09741738190253575 Validation Loss: 0.8075241446495056\n",
      "Epoch 12485: Training Loss: 0.09706416229406993 Validation Loss: 0.8069315552711487\n",
      "Epoch 12486: Training Loss: 0.09713134666283925 Validation Loss: 0.8068103790283203\n",
      "Epoch 12487: Training Loss: 0.09750668704509735 Validation Loss: 0.8061956167221069\n",
      "Epoch 12488: Training Loss: 0.09724479417006175 Validation Loss: 0.8060581684112549\n",
      "Epoch 12489: Training Loss: 0.09807442873716354 Validation Loss: 0.8073424100875854\n",
      "Epoch 12490: Training Loss: 0.09707228342692058 Validation Loss: 0.8076146841049194\n",
      "Epoch 12491: Training Loss: 0.09701500087976456 Validation Loss: 0.8075860738754272\n",
      "Epoch 12492: Training Loss: 0.09764041751623154 Validation Loss: 0.8083623647689819\n",
      "Epoch 12493: Training Loss: 0.09800306210915248 Validation Loss: 0.8068830370903015\n",
      "Epoch 12494: Training Loss: 0.09717511882384618 Validation Loss: 0.8062477707862854\n",
      "Epoch 12495: Training Loss: 0.09738262742757797 Validation Loss: 0.8057712912559509\n",
      "Epoch 12496: Training Loss: 0.09701849271853764 Validation Loss: 0.8068621158599854\n",
      "Epoch 12497: Training Loss: 0.09771509220202763 Validation Loss: 0.8078558444976807\n",
      "Epoch 12498: Training Loss: 0.09710730363925298 Validation Loss: 0.8074448108673096\n",
      "Epoch 12499: Training Loss: 0.09773692240317662 Validation Loss: 0.8063544034957886\n",
      "Epoch 12500: Training Loss: 0.0973527083794276 Validation Loss: 0.8067123889923096\n",
      "Epoch 12501: Training Loss: 0.09715439875920613 Validation Loss: 0.8077945709228516\n",
      "Epoch 12502: Training Loss: 0.09701755146185558 Validation Loss: 0.8075298070907593\n",
      "Epoch 12503: Training Loss: 0.09735353042682011 Validation Loss: 0.807796835899353\n",
      "Epoch 12504: Training Loss: 0.09697067985932033 Validation Loss: 0.8075757622718811\n",
      "Epoch 12505: Training Loss: 0.09712923318147659 Validation Loss: 0.8066201210021973\n",
      "Epoch 12506: Training Loss: 0.09711036334435146 Validation Loss: 0.807050347328186\n",
      "Epoch 12507: Training Loss: 0.09708081930875778 Validation Loss: 0.8070665597915649\n",
      "Epoch 12508: Training Loss: 0.09681675831476848 Validation Loss: 0.8072730302810669\n",
      "Epoch 12509: Training Loss: 0.09697142740090688 Validation Loss: 0.8075597286224365\n",
      "Epoch 12510: Training Loss: 0.09733434518178304 Validation Loss: 0.8074256777763367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12511: Training Loss: 0.09744692593812943 Validation Loss: 0.8078433871269226\n",
      "Epoch 12512: Training Loss: 0.09715343763430913 Validation Loss: 0.8068605065345764\n",
      "Epoch 12513: Training Loss: 0.09666949262221654 Validation Loss: 0.8060302138328552\n",
      "Epoch 12514: Training Loss: 0.09695961823066075 Validation Loss: 0.8064623475074768\n",
      "Epoch 12515: Training Loss: 0.09700037787357967 Validation Loss: 0.8067782521247864\n",
      "Epoch 12516: Training Loss: 0.09693217029174168 Validation Loss: 0.8071978092193604\n",
      "Epoch 12517: Training Loss: 0.09704583138227463 Validation Loss: 0.8070653080940247\n",
      "Epoch 12518: Training Loss: 0.09672769904136658 Validation Loss: 0.8070281147956848\n",
      "Epoch 12519: Training Loss: 0.09732056657473247 Validation Loss: 0.8079381585121155\n",
      "Epoch 12520: Training Loss: 0.09715899328390758 Validation Loss: 0.8071374893188477\n",
      "Epoch 12521: Training Loss: 0.09686131775379181 Validation Loss: 0.8062230348587036\n",
      "Epoch 12522: Training Loss: 0.09676714241504669 Validation Loss: 0.8061122894287109\n",
      "Epoch 12523: Training Loss: 0.09705150872468948 Validation Loss: 0.8065703511238098\n",
      "Epoch 12524: Training Loss: 0.0971104974548022 Validation Loss: 0.8081844449043274\n",
      "Epoch 12525: Training Loss: 0.09677975376447041 Validation Loss: 0.8084301352500916\n",
      "Epoch 12526: Training Loss: 0.09706675509611766 Validation Loss: 0.8077709078788757\n",
      "Epoch 12527: Training Loss: 0.09685231000185013 Validation Loss: 0.8089565634727478\n",
      "Epoch 12528: Training Loss: 0.09710866212844849 Validation Loss: 0.8077443838119507\n",
      "Epoch 12529: Training Loss: 0.0970137392481168 Validation Loss: 0.8072913289070129\n",
      "Epoch 12530: Training Loss: 0.09697001924117406 Validation Loss: 0.8072542548179626\n",
      "Epoch 12531: Training Loss: 0.09657130887111028 Validation Loss: 0.8071991205215454\n",
      "Epoch 12532: Training Loss: 0.09695198635260265 Validation Loss: 0.8068374395370483\n",
      "Epoch 12533: Training Loss: 0.09721517314513524 Validation Loss: 0.8088998198509216\n",
      "Epoch 12534: Training Loss: 0.09692587703466415 Validation Loss: 0.80728679895401\n",
      "Epoch 12535: Training Loss: 0.09754631419976552 Validation Loss: 0.8067277669906616\n",
      "Epoch 12536: Training Loss: 0.09670305003722508 Validation Loss: 0.8074731826782227\n",
      "Epoch 12537: Training Loss: 0.09716173509756725 Validation Loss: 0.8070428371429443\n",
      "Epoch 12538: Training Loss: 0.09694179395834605 Validation Loss: 0.8074446320533752\n",
      "Epoch 12539: Training Loss: 0.09693239629268646 Validation Loss: 0.8077751398086548\n",
      "Epoch 12540: Training Loss: 0.09651000052690506 Validation Loss: 0.8076319098472595\n",
      "Epoch 12541: Training Loss: 0.09684839844703674 Validation Loss: 0.8072900176048279\n",
      "Epoch 12542: Training Loss: 0.09688039124011993 Validation Loss: 0.8073090314865112\n",
      "Epoch 12543: Training Loss: 0.09727217257022858 Validation Loss: 0.8069380521774292\n",
      "Epoch 12544: Training Loss: 0.0969608227411906 Validation Loss: 0.8071662187576294\n",
      "Epoch 12545: Training Loss: 0.0968568002184232 Validation Loss: 0.8078765273094177\n",
      "Epoch 12546: Training Loss: 0.09755230198303859 Validation Loss: 0.8091543912887573\n",
      "Epoch 12547: Training Loss: 0.09741322447856267 Validation Loss: 0.808418869972229\n",
      "Epoch 12548: Training Loss: 0.09714994331200917 Validation Loss: 0.807838499546051\n",
      "Epoch 12549: Training Loss: 0.09679659952720006 Validation Loss: 0.8073222041130066\n",
      "Epoch 12550: Training Loss: 0.09673368185758591 Validation Loss: 0.8067231774330139\n",
      "Epoch 12551: Training Loss: 0.09670845170815785 Validation Loss: 0.8078500032424927\n",
      "Epoch 12552: Training Loss: 0.09646592289209366 Validation Loss: 0.8079254031181335\n",
      "Epoch 12553: Training Loss: 0.09680742025375366 Validation Loss: 0.8071491718292236\n",
      "Epoch 12554: Training Loss: 0.09701593468586604 Validation Loss: 0.8074864149093628\n",
      "Epoch 12555: Training Loss: 0.09686886519193649 Validation Loss: 0.8078292608261108\n",
      "Epoch 12556: Training Loss: 0.09681085248788197 Validation Loss: 0.8078031539916992\n",
      "Epoch 12557: Training Loss: 0.09686740736166637 Validation Loss: 0.8076987266540527\n",
      "Epoch 12558: Training Loss: 0.09673583755890529 Validation Loss: 0.8079476356506348\n",
      "Epoch 12559: Training Loss: 0.09710364540417989 Validation Loss: 0.8078094720840454\n",
      "Epoch 12560: Training Loss: 0.09678774078687032 Validation Loss: 0.8071703910827637\n",
      "Epoch 12561: Training Loss: 0.0965472807486852 Validation Loss: 0.8081579208374023\n",
      "Epoch 12562: Training Loss: 0.09671249240636826 Validation Loss: 0.8081672191619873\n",
      "Epoch 12563: Training Loss: 0.09684543559948604 Validation Loss: 0.808482825756073\n",
      "Epoch 12564: Training Loss: 0.09679044783115387 Validation Loss: 0.8082953095436096\n",
      "Epoch 12565: Training Loss: 0.09712941696246465 Validation Loss: 0.8079350590705872\n",
      "Epoch 12566: Training Loss: 0.0972964217265447 Validation Loss: 0.8064941167831421\n",
      "Epoch 12567: Training Loss: 0.09678290287653606 Validation Loss: 0.8073356747627258\n",
      "Epoch 12568: Training Loss: 0.0966854989528656 Validation Loss: 0.8079014420509338\n",
      "Epoch 12569: Training Loss: 0.09713680545488994 Validation Loss: 0.8075802326202393\n",
      "Epoch 12570: Training Loss: 0.09696266800165176 Validation Loss: 0.807843804359436\n",
      "Epoch 12571: Training Loss: 0.09691184510787328 Validation Loss: 0.8068444132804871\n",
      "Epoch 12572: Training Loss: 0.09666511664787929 Validation Loss: 0.8074237108230591\n",
      "Epoch 12573: Training Loss: 0.09697753687699635 Validation Loss: 0.8081042766571045\n",
      "Epoch 12574: Training Loss: 0.09697820991277695 Validation Loss: 0.8083363175392151\n",
      "Epoch 12575: Training Loss: 0.09668367852767308 Validation Loss: 0.8080019950866699\n",
      "Epoch 12576: Training Loss: 0.09696395446856816 Validation Loss: 0.8086350560188293\n",
      "Epoch 12577: Training Loss: 0.09714927524328232 Validation Loss: 0.8083358407020569\n",
      "Epoch 12578: Training Loss: 0.09684255222479503 Validation Loss: 0.8081910014152527\n",
      "Epoch 12579: Training Loss: 0.09707435717185338 Validation Loss: 0.8083682060241699\n",
      "Epoch 12580: Training Loss: 0.09637602418661118 Validation Loss: 0.8085629940032959\n",
      "Epoch 12581: Training Loss: 0.09664679318666458 Validation Loss: 0.808057963848114\n",
      "Epoch 12582: Training Loss: 0.09651246418555577 Validation Loss: 0.8077937364578247\n",
      "Epoch 12583: Training Loss: 0.09671791642904282 Validation Loss: 0.8069888353347778\n",
      "Epoch 12584: Training Loss: 0.09677699704964955 Validation Loss: 0.8071436882019043\n",
      "Epoch 12585: Training Loss: 0.09672440588474274 Validation Loss: 0.8083900809288025\n",
      "Epoch 12586: Training Loss: 0.09714378168185551 Validation Loss: 0.808784008026123\n",
      "Epoch 12587: Training Loss: 0.09668577959140141 Validation Loss: 0.8067927360534668\n",
      "Epoch 12588: Training Loss: 0.09643368671337764 Validation Loss: 0.8071759939193726\n",
      "Epoch 12589: Training Loss: 0.09655964126189549 Validation Loss: 0.8073848485946655\n",
      "Epoch 12590: Training Loss: 0.09671817223230998 Validation Loss: 0.8072496056556702\n",
      "Epoch 12591: Training Loss: 0.09660762796799342 Validation Loss: 0.8082125186920166\n",
      "Epoch 12592: Training Loss: 0.09732131411631902 Validation Loss: 0.8092136979103088\n",
      "Epoch 12593: Training Loss: 0.09658955285946529 Validation Loss: 0.8083763122558594\n",
      "Epoch 12594: Training Loss: 0.09656779964764912 Validation Loss: 0.8084622025489807\n",
      "Epoch 12595: Training Loss: 0.09714493652184804 Validation Loss: 0.8073303699493408\n",
      "Epoch 12596: Training Loss: 0.09690226117769878 Validation Loss: 0.8067721128463745\n",
      "Epoch 12597: Training Loss: 0.0966767892241478 Validation Loss: 0.8080820441246033\n",
      "Epoch 12598: Training Loss: 0.09686208764712016 Validation Loss: 0.808497428894043\n",
      "Epoch 12599: Training Loss: 0.09658477703730266 Validation Loss: 0.8087962865829468\n",
      "Epoch 12600: Training Loss: 0.09671538571516673 Validation Loss: 0.8089863657951355\n",
      "Epoch 12601: Training Loss: 0.09670178095499675 Validation Loss: 0.8092378973960876\n",
      "Epoch 12602: Training Loss: 0.09711312750975291 Validation Loss: 0.8086073398590088\n",
      "Epoch 12603: Training Loss: 0.0964428832133611 Validation Loss: 0.80777508020401\n",
      "Epoch 12604: Training Loss: 0.09675085792938869 Validation Loss: 0.8075052499771118\n",
      "Epoch 12605: Training Loss: 0.09653009722630183 Validation Loss: 0.8078222870826721\n",
      "Epoch 12606: Training Loss: 0.0979601467649142 Validation Loss: 0.8081814646720886\n",
      "Epoch 12607: Training Loss: 0.0968439703186353 Validation Loss: 0.8076703548431396\n",
      "Epoch 12608: Training Loss: 0.09688902149597804 Validation Loss: 0.80714350938797\n",
      "Epoch 12609: Training Loss: 0.09652566661437352 Validation Loss: 0.8079496622085571\n",
      "Epoch 12610: Training Loss: 0.09666215876738231 Validation Loss: 0.8088104128837585\n",
      "Epoch 12611: Training Loss: 0.09669681390126546 Validation Loss: 0.8084379434585571\n",
      "Epoch 12612: Training Loss: 0.09634873022635777 Validation Loss: 0.8088256120681763\n",
      "Epoch 12613: Training Loss: 0.0971338078379631 Validation Loss: 0.8083156943321228\n",
      "Epoch 12614: Training Loss: 0.09647805492083232 Validation Loss: 0.8081612586975098\n",
      "Epoch 12615: Training Loss: 0.09656925002733867 Validation Loss: 0.8076990842819214\n",
      "Epoch 12616: Training Loss: 0.09692648301521938 Validation Loss: 0.8080419898033142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12617: Training Loss: 0.0963420644402504 Validation Loss: 0.8087161183357239\n",
      "Epoch 12618: Training Loss: 0.09695619344711304 Validation Loss: 0.8082976341247559\n",
      "Epoch 12619: Training Loss: 0.09741958479086558 Validation Loss: 0.8080001473426819\n",
      "Epoch 12620: Training Loss: 0.09719359626372655 Validation Loss: 0.8088449835777283\n",
      "Epoch 12621: Training Loss: 0.0973330835501353 Validation Loss: 0.8082135319709778\n",
      "Epoch 12622: Training Loss: 0.09621713558832805 Validation Loss: 0.8084955811500549\n",
      "Epoch 12623: Training Loss: 0.09652671962976456 Validation Loss: 0.8086739182472229\n",
      "Epoch 12624: Training Loss: 0.09662398199240367 Validation Loss: 0.8090904951095581\n",
      "Epoch 12625: Training Loss: 0.09677052994569142 Validation Loss: 0.808651864528656\n",
      "Epoch 12626: Training Loss: 0.09705736984809239 Validation Loss: 0.807796061038971\n",
      "Epoch 12627: Training Loss: 0.09627358863751094 Validation Loss: 0.808431088924408\n",
      "Epoch 12628: Training Loss: 0.0965289647380511 Validation Loss: 0.8089728355407715\n",
      "Epoch 12629: Training Loss: 0.0965244248509407 Validation Loss: 0.8093791007995605\n",
      "Epoch 12630: Training Loss: 0.09655690689881642 Validation Loss: 0.8097907900810242\n",
      "Epoch 12631: Training Loss: 0.09644084672133128 Validation Loss: 0.8085651993751526\n",
      "Epoch 12632: Training Loss: 0.09646408011515935 Validation Loss: 0.8077908754348755\n",
      "Epoch 12633: Training Loss: 0.0965867613752683 Validation Loss: 0.8092198371887207\n",
      "Epoch 12634: Training Loss: 0.09705391029516856 Validation Loss: 0.8087449669837952\n",
      "Epoch 12635: Training Loss: 0.09691273917754491 Validation Loss: 0.8085575103759766\n",
      "Epoch 12636: Training Loss: 0.09641640881697337 Validation Loss: 0.8082674145698547\n",
      "Epoch 12637: Training Loss: 0.09655581166346867 Validation Loss: 0.8079206347465515\n",
      "Epoch 12638: Training Loss: 0.0964495986700058 Validation Loss: 0.8084040880203247\n",
      "Epoch 12639: Training Loss: 0.09632690995931625 Validation Loss: 0.8081282377243042\n",
      "Epoch 12640: Training Loss: 0.09656132757663727 Validation Loss: 0.8072723746299744\n",
      "Epoch 12641: Training Loss: 0.09691315641005833 Validation Loss: 0.808449923992157\n",
      "Epoch 12642: Training Loss: 0.09637573609749477 Validation Loss: 0.8093520402908325\n",
      "Epoch 12643: Training Loss: 0.09650953362385432 Validation Loss: 0.8090092539787292\n",
      "Epoch 12644: Training Loss: 0.09639790405829747 Validation Loss: 0.8099728226661682\n",
      "Epoch 12645: Training Loss: 0.0965619112054507 Validation Loss: 0.8086634874343872\n",
      "Epoch 12646: Training Loss: 0.09656198571125667 Validation Loss: 0.8081367015838623\n",
      "Epoch 12647: Training Loss: 0.09668716291586558 Validation Loss: 0.8088347315788269\n",
      "Epoch 12648: Training Loss: 0.09634342789649963 Validation Loss: 0.8094085454940796\n",
      "Epoch 12649: Training Loss: 0.09730177869399388 Validation Loss: 0.8097130656242371\n",
      "Epoch 12650: Training Loss: 0.09656753639380138 Validation Loss: 0.8088153004646301\n",
      "Epoch 12651: Training Loss: 0.09663590292135875 Validation Loss: 0.8079053163528442\n",
      "Epoch 12652: Training Loss: 0.09644005695978801 Validation Loss: 0.8091312050819397\n",
      "Epoch 12653: Training Loss: 0.09661746770143509 Validation Loss: 0.8089185357093811\n",
      "Epoch 12654: Training Loss: 0.09646820773681004 Validation Loss: 0.8091824650764465\n",
      "Epoch 12655: Training Loss: 0.09643752872943878 Validation Loss: 0.8095113635063171\n",
      "Epoch 12656: Training Loss: 0.09640416999657948 Validation Loss: 0.8096785545349121\n",
      "Epoch 12657: Training Loss: 0.09644259512424469 Validation Loss: 0.8091639280319214\n",
      "Epoch 12658: Training Loss: 0.0964943344394366 Validation Loss: 0.8098480105400085\n",
      "Epoch 12659: Training Loss: 0.09635127087434132 Validation Loss: 0.8093442320823669\n",
      "Epoch 12660: Training Loss: 0.09671631455421448 Validation Loss: 0.8080896139144897\n",
      "Epoch 12661: Training Loss: 0.09673678874969482 Validation Loss: 0.8075549006462097\n",
      "Epoch 12662: Training Loss: 0.09643343836069107 Validation Loss: 0.8076727986335754\n",
      "Epoch 12663: Training Loss: 0.09647530068953832 Validation Loss: 0.8087195754051208\n",
      "Epoch 12664: Training Loss: 0.09634943803151448 Validation Loss: 0.81034255027771\n",
      "Epoch 12665: Training Loss: 0.09694687773784001 Validation Loss: 0.8100996017456055\n",
      "Epoch 12666: Training Loss: 0.096286175151666 Validation Loss: 0.8091939091682434\n",
      "Epoch 12667: Training Loss: 0.09652274598677953 Validation Loss: 0.8083986043930054\n",
      "Epoch 12668: Training Loss: 0.09631732602914174 Validation Loss: 0.8085802793502808\n",
      "Epoch 12669: Training Loss: 0.09613166004419327 Validation Loss: 0.8087360858917236\n",
      "Epoch 12670: Training Loss: 0.09627766658862431 Validation Loss: 0.8095415234565735\n",
      "Epoch 12671: Training Loss: 0.09643863638242085 Validation Loss: 0.8088749051094055\n",
      "Epoch 12672: Training Loss: 0.09666464974482854 Validation Loss: 0.8085902333259583\n",
      "Epoch 12673: Training Loss: 0.09669545541206996 Validation Loss: 0.8082460761070251\n",
      "Epoch 12674: Training Loss: 0.09625974545876186 Validation Loss: 0.8094592094421387\n",
      "Epoch 12675: Training Loss: 0.0964095542828242 Validation Loss: 0.8087546825408936\n",
      "Epoch 12676: Training Loss: 0.0961797907948494 Validation Loss: 0.8088921904563904\n",
      "Epoch 12677: Training Loss: 0.0961676687002182 Validation Loss: 0.8085196018218994\n",
      "Epoch 12678: Training Loss: 0.09627750515937805 Validation Loss: 0.8081801533699036\n",
      "Epoch 12679: Training Loss: 0.09626522908608119 Validation Loss: 0.8085861206054688\n",
      "Epoch 12680: Training Loss: 0.09616680939992268 Validation Loss: 0.8093135356903076\n",
      "Epoch 12681: Training Loss: 0.09647211680809657 Validation Loss: 0.8110100626945496\n",
      "Epoch 12682: Training Loss: 0.09636144091685613 Validation Loss: 0.8108505606651306\n",
      "Epoch 12683: Training Loss: 0.09629248082637787 Validation Loss: 0.8103163242340088\n",
      "Epoch 12684: Training Loss: 0.09649780144294103 Validation Loss: 0.8080729246139526\n",
      "Epoch 12685: Training Loss: 0.0961337462067604 Validation Loss: 0.8078194856643677\n",
      "Epoch 12686: Training Loss: 0.09623492509126663 Validation Loss: 0.807748019695282\n",
      "Epoch 12687: Training Loss: 0.09645318984985352 Validation Loss: 0.8086196780204773\n",
      "Epoch 12688: Training Loss: 0.09649311006069183 Validation Loss: 0.8100559115409851\n",
      "Epoch 12689: Training Loss: 0.09641520430644353 Validation Loss: 0.8098461627960205\n",
      "Epoch 12690: Training Loss: 0.09610195209582646 Validation Loss: 0.8095174431800842\n",
      "Epoch 12691: Training Loss: 0.09618524213631947 Validation Loss: 0.8092586994171143\n",
      "Epoch 12692: Training Loss: 0.09630048523346584 Validation Loss: 0.8079649209976196\n",
      "Epoch 12693: Training Loss: 0.09627831230560939 Validation Loss: 0.8076848983764648\n",
      "Epoch 12694: Training Loss: 0.09753924608230591 Validation Loss: 0.809258222579956\n",
      "Epoch 12695: Training Loss: 0.09669361263513565 Validation Loss: 0.8106276988983154\n",
      "Epoch 12696: Training Loss: 0.09615703423817952 Validation Loss: 0.8104158043861389\n",
      "Epoch 12697: Training Loss: 0.0964170719186465 Validation Loss: 0.8098824620246887\n",
      "Epoch 12698: Training Loss: 0.09614333758751552 Validation Loss: 0.8091775178909302\n",
      "Epoch 12699: Training Loss: 0.09614866971969604 Validation Loss: 0.808040201663971\n",
      "Epoch 12700: Training Loss: 0.09618417918682098 Validation Loss: 0.8073577284812927\n",
      "Epoch 12701: Training Loss: 0.09637427081664403 Validation Loss: 0.8078399896621704\n",
      "Epoch 12702: Training Loss: 0.09637614091237386 Validation Loss: 0.8091396689414978\n",
      "Epoch 12703: Training Loss: 0.09630020459493001 Validation Loss: 0.8097174763679504\n",
      "Epoch 12704: Training Loss: 0.09622566650311153 Validation Loss: 0.8105313777923584\n",
      "Epoch 12705: Training Loss: 0.09658034145832062 Validation Loss: 0.8110600113868713\n",
      "Epoch 12706: Training Loss: 0.09611539791027705 Validation Loss: 0.8094226717948914\n",
      "Epoch 12707: Training Loss: 0.09652343392372131 Validation Loss: 0.8082022666931152\n",
      "Epoch 12708: Training Loss: 0.09598508973916371 Validation Loss: 0.8079347014427185\n",
      "Epoch 12709: Training Loss: 0.09633589535951614 Validation Loss: 0.809691846370697\n",
      "Epoch 12710: Training Loss: 0.09586025774478912 Validation Loss: 0.8106905221939087\n",
      "Epoch 12711: Training Loss: 0.09594339380661647 Validation Loss: 0.8098488450050354\n",
      "Epoch 12712: Training Loss: 0.09621102611223857 Validation Loss: 0.8095718026161194\n",
      "Epoch 12713: Training Loss: 0.09617039561271667 Validation Loss: 0.8095600008964539\n",
      "Epoch 12714: Training Loss: 0.09588222205638885 Validation Loss: 0.8096997737884521\n",
      "Epoch 12715: Training Loss: 0.09617086748282115 Validation Loss: 0.8093984127044678\n",
      "Epoch 12716: Training Loss: 0.09613940119743347 Validation Loss: 0.8094351291656494\n",
      "Epoch 12717: Training Loss: 0.09629059086243312 Validation Loss: 0.8104825615882874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12718: Training Loss: 0.09627774606148402 Validation Loss: 0.8089063167572021\n",
      "Epoch 12719: Training Loss: 0.09654273341099422 Validation Loss: 0.8085973858833313\n",
      "Epoch 12720: Training Loss: 0.09621237715085347 Validation Loss: 0.8090887665748596\n",
      "Epoch 12721: Training Loss: 0.09656426558891933 Validation Loss: 0.8108853697776794\n",
      "Epoch 12722: Training Loss: 0.09619103868802388 Validation Loss: 0.810980498790741\n",
      "Epoch 12723: Training Loss: 0.0965062752366066 Validation Loss: 0.8096250891685486\n",
      "Epoch 12724: Training Loss: 0.09604725738366444 Validation Loss: 0.8088694214820862\n",
      "Epoch 12725: Training Loss: 0.09652037421862285 Validation Loss: 0.8098760843276978\n",
      "Epoch 12726: Training Loss: 0.09653708587090175 Validation Loss: 0.8102427124977112\n",
      "Epoch 12727: Training Loss: 0.09602328141530354 Validation Loss: 0.8107921481132507\n",
      "Epoch 12728: Training Loss: 0.09658566613992055 Validation Loss: 0.8103100061416626\n",
      "Epoch 12729: Training Loss: 0.09642064819733302 Validation Loss: 0.8100264668464661\n",
      "Epoch 12730: Training Loss: 0.09633110960324605 Validation Loss: 0.8104115128517151\n",
      "Epoch 12731: Training Loss: 0.09619274983803432 Validation Loss: 0.8098887801170349\n",
      "Epoch 12732: Training Loss: 0.09610754748185475 Validation Loss: 0.8089373111724854\n",
      "Epoch 12733: Training Loss: 0.09631648659706116 Validation Loss: 0.8094870448112488\n",
      "Epoch 12734: Training Loss: 0.09607940912246704 Validation Loss: 0.809362530708313\n",
      "Epoch 12735: Training Loss: 0.09597888588905334 Validation Loss: 0.8100979328155518\n",
      "Epoch 12736: Training Loss: 0.09592199077208836 Validation Loss: 0.8106476068496704\n",
      "Epoch 12737: Training Loss: 0.09626999994119008 Validation Loss: 0.8108078837394714\n",
      "Epoch 12738: Training Loss: 0.0959954559803009 Validation Loss: 0.8103157877922058\n",
      "Epoch 12739: Training Loss: 0.09592661013205846 Validation Loss: 0.8106223940849304\n",
      "Epoch 12740: Training Loss: 0.09602658450603485 Validation Loss: 0.8096156120300293\n",
      "Epoch 12741: Training Loss: 0.0959370161096255 Validation Loss: 0.808830201625824\n",
      "Epoch 12742: Training Loss: 0.09615772217512131 Validation Loss: 0.808114230632782\n",
      "Epoch 12743: Training Loss: 0.09589092930157979 Validation Loss: 0.8086878657341003\n",
      "Epoch 12744: Training Loss: 0.09654878079891205 Validation Loss: 0.8108683824539185\n",
      "Epoch 12745: Training Loss: 0.0965648740530014 Validation Loss: 0.8101045489311218\n",
      "Epoch 12746: Training Loss: 0.09591590116421382 Validation Loss: 0.8108400106430054\n",
      "Epoch 12747: Training Loss: 0.09623665610949199 Validation Loss: 0.8111063241958618\n",
      "Epoch 12748: Training Loss: 0.09570410599311192 Validation Loss: 0.8102158308029175\n",
      "Epoch 12749: Training Loss: 0.09599710255861282 Validation Loss: 0.8106533288955688\n",
      "Epoch 12750: Training Loss: 0.09662071118752162 Validation Loss: 0.8109648823738098\n",
      "Epoch 12751: Training Loss: 0.09592376401027043 Validation Loss: 0.812767744064331\n",
      "Epoch 12752: Training Loss: 0.0966106007496516 Validation Loss: 0.8130656480789185\n",
      "Epoch 12753: Training Loss: 0.09604975332816441 Validation Loss: 0.8113202452659607\n",
      "Epoch 12754: Training Loss: 0.09643776714801788 Validation Loss: 0.8102418780326843\n",
      "Epoch 12755: Training Loss: 0.09591552118460338 Validation Loss: 0.8092025518417358\n",
      "Epoch 12756: Training Loss: 0.09605403244495392 Validation Loss: 0.8080011606216431\n",
      "Epoch 12757: Training Loss: 0.09605641414721806 Validation Loss: 0.8086153268814087\n",
      "Epoch 12758: Training Loss: 0.09614789982636769 Validation Loss: 0.8093680143356323\n",
      "Epoch 12759: Training Loss: 0.09649757047494252 Validation Loss: 0.8107662200927734\n",
      "Epoch 12760: Training Loss: 0.09626263380050659 Validation Loss: 0.8099379539489746\n",
      "Epoch 12761: Training Loss: 0.09599908192952473 Validation Loss: 0.8099430203437805\n",
      "Epoch 12762: Training Loss: 0.0969870959719022 Validation Loss: 0.808826208114624\n",
      "Epoch 12763: Training Loss: 0.09637034436066945 Validation Loss: 0.8087146282196045\n",
      "Epoch 12764: Training Loss: 0.09590860704580943 Validation Loss: 0.8101747035980225\n",
      "Epoch 12765: Training Loss: 0.09581290433804195 Validation Loss: 0.8105460405349731\n",
      "Epoch 12766: Training Loss: 0.09596724559863408 Validation Loss: 0.8100209832191467\n",
      "Epoch 12767: Training Loss: 0.09576349457105 Validation Loss: 0.8102142214775085\n",
      "Epoch 12768: Training Loss: 0.09633275866508484 Validation Loss: 0.8101645708084106\n",
      "Epoch 12769: Training Loss: 0.09574303527673085 Validation Loss: 0.810270369052887\n",
      "Epoch 12770: Training Loss: 0.0961490049958229 Validation Loss: 0.8101394772529602\n",
      "Epoch 12771: Training Loss: 0.09638195236523946 Validation Loss: 0.809938371181488\n",
      "Epoch 12772: Training Loss: 0.09567055851221085 Validation Loss: 0.8107166290283203\n",
      "Epoch 12773: Training Loss: 0.09644571940104167 Validation Loss: 0.810123860836029\n",
      "Epoch 12774: Training Loss: 0.0964529166618983 Validation Loss: 0.81110018491745\n",
      "Epoch 12775: Training Loss: 0.09600472450256348 Validation Loss: 0.8110244870185852\n",
      "Epoch 12776: Training Loss: 0.09571050107479095 Validation Loss: 0.810985803604126\n",
      "Epoch 12777: Training Loss: 0.09599766631921132 Validation Loss: 0.8104808926582336\n",
      "Epoch 12778: Training Loss: 0.09593724459409714 Validation Loss: 0.8097008466720581\n",
      "Epoch 12779: Training Loss: 0.09589969118436177 Validation Loss: 0.8108884692192078\n",
      "Epoch 12780: Training Loss: 0.09636997679869334 Validation Loss: 0.8118939399719238\n",
      "Epoch 12781: Training Loss: 0.09640224277973175 Validation Loss: 0.8112771511077881\n",
      "Epoch 12782: Training Loss: 0.09590629488229752 Validation Loss: 0.8093461990356445\n",
      "Epoch 12783: Training Loss: 0.0962713286280632 Validation Loss: 0.809199869632721\n",
      "Epoch 12784: Training Loss: 0.09590499599774678 Validation Loss: 0.8090236783027649\n",
      "Epoch 12785: Training Loss: 0.09601402531067531 Validation Loss: 0.8103395700454712\n",
      "Epoch 12786: Training Loss: 0.09587506204843521 Validation Loss: 0.8118836879730225\n",
      "Epoch 12787: Training Loss: 0.09576457490523656 Validation Loss: 0.81228107213974\n",
      "Epoch 12788: Training Loss: 0.0962559108932813 Validation Loss: 0.8106433153152466\n",
      "Epoch 12789: Training Loss: 0.09615406145652135 Validation Loss: 0.8095126152038574\n",
      "Epoch 12790: Training Loss: 0.09615551183621089 Validation Loss: 0.8093169927597046\n",
      "Epoch 12791: Training Loss: 0.09634142369031906 Validation Loss: 0.8097324967384338\n",
      "Epoch 12792: Training Loss: 0.09588258216778438 Validation Loss: 0.8111867308616638\n",
      "Epoch 12793: Training Loss: 0.0956617643435796 Validation Loss: 0.8111286759376526\n",
      "Epoch 12794: Training Loss: 0.09587710599104564 Validation Loss: 0.8107223510742188\n",
      "Epoch 12795: Training Loss: 0.09608939786752065 Validation Loss: 0.809909462928772\n",
      "Epoch 12796: Training Loss: 0.09592131525278091 Validation Loss: 0.8100308775901794\n",
      "Epoch 12797: Training Loss: 0.09584388633569081 Validation Loss: 0.8108625411987305\n",
      "Epoch 12798: Training Loss: 0.09590378900369008 Validation Loss: 0.8114285469055176\n",
      "Epoch 12799: Training Loss: 0.09587674836317699 Validation Loss: 0.8109428286552429\n",
      "Epoch 12800: Training Loss: 0.09589894364277522 Validation Loss: 0.8101820945739746\n",
      "Epoch 12801: Training Loss: 0.09624484181404114 Validation Loss: 0.8099783062934875\n",
      "Epoch 12802: Training Loss: 0.09583518902460735 Validation Loss: 0.8106788992881775\n",
      "Epoch 12803: Training Loss: 0.09580580145120621 Validation Loss: 0.8115836977958679\n",
      "Epoch 12804: Training Loss: 0.0965893988807996 Validation Loss: 0.8120924234390259\n",
      "Epoch 12805: Training Loss: 0.09579916795094807 Validation Loss: 0.8114383220672607\n",
      "Epoch 12806: Training Loss: 0.09570524096488953 Validation Loss: 0.8107134103775024\n",
      "Epoch 12807: Training Loss: 0.09570252150297165 Validation Loss: 0.8100677132606506\n",
      "Epoch 12808: Training Loss: 0.09606077522039413 Validation Loss: 0.8096590042114258\n",
      "Epoch 12809: Training Loss: 0.09580572197834651 Validation Loss: 0.8103001713752747\n",
      "Epoch 12810: Training Loss: 0.09592971205711365 Validation Loss: 0.8114442229270935\n",
      "Epoch 12811: Training Loss: 0.0958235537012418 Validation Loss: 0.8117175102233887\n",
      "Epoch 12812: Training Loss: 0.09579970687627792 Validation Loss: 0.8109487891197205\n",
      "Epoch 12813: Training Loss: 0.09600381056467693 Validation Loss: 0.8097292184829712\n",
      "Epoch 12814: Training Loss: 0.09576933085918427 Validation Loss: 0.8100088834762573\n",
      "Epoch 12815: Training Loss: 0.09573050340016682 Validation Loss: 0.8106687068939209\n",
      "Epoch 12816: Training Loss: 0.09578214585781097 Validation Loss: 0.810625433921814\n",
      "Epoch 12817: Training Loss: 0.09567229946454366 Validation Loss: 0.8105602860450745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12818: Training Loss: 0.09571755180756251 Validation Loss: 0.8108188509941101\n",
      "Epoch 12819: Training Loss: 0.09559495250384013 Validation Loss: 0.8116758465766907\n",
      "Epoch 12820: Training Loss: 0.09661947190761566 Validation Loss: 0.8123230934143066\n",
      "Epoch 12821: Training Loss: 0.09618932008743286 Validation Loss: 0.8114038705825806\n",
      "Epoch 12822: Training Loss: 0.09589974582195282 Validation Loss: 0.8100476264953613\n",
      "Epoch 12823: Training Loss: 0.09619672348101933 Validation Loss: 0.8096063733100891\n",
      "Epoch 12824: Training Loss: 0.09604879965384801 Validation Loss: 0.8095893859863281\n",
      "Epoch 12825: Training Loss: 0.09568578501542409 Validation Loss: 0.8105549812316895\n",
      "Epoch 12826: Training Loss: 0.0956910823782285 Validation Loss: 0.811129629611969\n",
      "Epoch 12827: Training Loss: 0.09626312553882599 Validation Loss: 0.8115400671958923\n",
      "Epoch 12828: Training Loss: 0.09600746631622314 Validation Loss: 0.8124322295188904\n",
      "Epoch 12829: Training Loss: 0.09583974132935207 Validation Loss: 0.8117462396621704\n",
      "Epoch 12830: Training Loss: 0.09581874807675679 Validation Loss: 0.8099057674407959\n",
      "Epoch 12831: Training Loss: 0.0957594911257426 Validation Loss: 0.8095563650131226\n",
      "Epoch 12832: Training Loss: 0.09589120248953502 Validation Loss: 0.8097168803215027\n",
      "Epoch 12833: Training Loss: 0.09593786795934041 Validation Loss: 0.8114680647850037\n",
      "Epoch 12834: Training Loss: 0.09596133480469386 Validation Loss: 0.8130928874015808\n",
      "Epoch 12835: Training Loss: 0.0955258881052335 Validation Loss: 0.8118265271186829\n",
      "Epoch 12836: Training Loss: 0.0960529272754987 Validation Loss: 0.8105988502502441\n",
      "Epoch 12837: Training Loss: 0.09620396296183269 Validation Loss: 0.8101286292076111\n",
      "Epoch 12838: Training Loss: 0.09580020606517792 Validation Loss: 0.8090860247612\n",
      "Epoch 12839: Training Loss: 0.09553919484217961 Validation Loss: 0.8099268674850464\n",
      "Epoch 12840: Training Loss: 0.09574401378631592 Validation Loss: 0.8101555705070496\n",
      "Epoch 12841: Training Loss: 0.09552876899639766 Validation Loss: 0.810929000377655\n",
      "Epoch 12842: Training Loss: 0.09557458261648814 Validation Loss: 0.8111344575881958\n",
      "Epoch 12843: Training Loss: 0.09571909656127293 Validation Loss: 0.81120765209198\n",
      "Epoch 12844: Training Loss: 0.09586430092652638 Validation Loss: 0.8106200695037842\n",
      "Epoch 12845: Training Loss: 0.09574040522178014 Validation Loss: 0.8101982474327087\n",
      "Epoch 12846: Training Loss: 0.09575385351975758 Validation Loss: 0.8117491006851196\n",
      "Epoch 12847: Training Loss: 0.09543484697739284 Validation Loss: 0.812364935874939\n",
      "Epoch 12848: Training Loss: 0.09575616071621577 Validation Loss: 0.8126934170722961\n",
      "Epoch 12849: Training Loss: 0.09572657446066539 Validation Loss: 0.8126041293144226\n",
      "Epoch 12850: Training Loss: 0.09541816016038258 Validation Loss: 0.8109774589538574\n",
      "Epoch 12851: Training Loss: 0.09532906860113144 Validation Loss: 0.810731053352356\n",
      "Epoch 12852: Training Loss: 0.09558695554733276 Validation Loss: 0.8110955357551575\n",
      "Epoch 12853: Training Loss: 0.09571515768766403 Validation Loss: 0.811672568321228\n",
      "Epoch 12854: Training Loss: 0.09550831963618596 Validation Loss: 0.8118876218795776\n",
      "Epoch 12855: Training Loss: 0.09540416300296783 Validation Loss: 0.8110727071762085\n",
      "Epoch 12856: Training Loss: 0.09558433045943578 Validation Loss: 0.8111402988433838\n",
      "Epoch 12857: Training Loss: 0.09578101833661397 Validation Loss: 0.8111701607704163\n",
      "Epoch 12858: Training Loss: 0.09583828349908192 Validation Loss: 0.8098064064979553\n",
      "Epoch 12859: Training Loss: 0.09577494611342748 Validation Loss: 0.8093780875205994\n",
      "Epoch 12860: Training Loss: 0.09579880783955257 Validation Loss: 0.8096126317977905\n",
      "Epoch 12861: Training Loss: 0.09566610803206761 Validation Loss: 0.8113812804222107\n",
      "Epoch 12862: Training Loss: 0.09527756522099178 Validation Loss: 0.8120435476303101\n",
      "Epoch 12863: Training Loss: 0.095491424202919 Validation Loss: 0.812730610370636\n",
      "Epoch 12864: Training Loss: 0.09577646603186925 Validation Loss: 0.8124734163284302\n",
      "Epoch 12865: Training Loss: 0.09568497290213902 Validation Loss: 0.8106654286384583\n",
      "Epoch 12866: Training Loss: 0.09577428052822749 Validation Loss: 0.8103689551353455\n",
      "Epoch 12867: Training Loss: 0.09580023090044658 Validation Loss: 0.8103389143943787\n",
      "Epoch 12868: Training Loss: 0.09549180914958318 Validation Loss: 0.8106330037117004\n",
      "Epoch 12869: Training Loss: 0.09555808206399281 Validation Loss: 0.8109578490257263\n",
      "Epoch 12870: Training Loss: 0.09548251082499822 Validation Loss: 0.8122817277908325\n",
      "Epoch 12871: Training Loss: 0.09608049690723419 Validation Loss: 0.8128977417945862\n",
      "Epoch 12872: Training Loss: 0.09558621793985367 Validation Loss: 0.8115604519844055\n",
      "Epoch 12873: Training Loss: 0.0955686221520106 Validation Loss: 0.8106740117073059\n",
      "Epoch 12874: Training Loss: 0.09567451973756154 Validation Loss: 0.809963583946228\n",
      "Epoch 12875: Training Loss: 0.0961326112349828 Validation Loss: 0.8107127547264099\n",
      "Epoch 12876: Training Loss: 0.09566433976093928 Validation Loss: 0.8120982050895691\n",
      "Epoch 12877: Training Loss: 0.09566004325946172 Validation Loss: 0.8132875561714172\n",
      "Epoch 12878: Training Loss: 0.09573645393053691 Validation Loss: 0.8129947185516357\n",
      "Epoch 12879: Training Loss: 0.09537243843078613 Validation Loss: 0.8123320937156677\n",
      "Epoch 12880: Training Loss: 0.09525624414285024 Validation Loss: 0.8126347064971924\n",
      "Epoch 12881: Training Loss: 0.0953958531220754 Validation Loss: 0.8113179802894592\n",
      "Epoch 12882: Training Loss: 0.09499304244915645 Validation Loss: 0.8114028573036194\n",
      "Epoch 12883: Training Loss: 0.09541674951712291 Validation Loss: 0.8116832971572876\n",
      "Epoch 12884: Training Loss: 0.095462866127491 Validation Loss: 0.8122552037239075\n",
      "Epoch 12885: Training Loss: 0.09562816967566808 Validation Loss: 0.8116453289985657\n",
      "Epoch 12886: Training Loss: 0.09558877100547154 Validation Loss: 0.8116650581359863\n",
      "Epoch 12887: Training Loss: 0.09585300336281459 Validation Loss: 0.8105883598327637\n",
      "Epoch 12888: Training Loss: 0.0954815646012624 Validation Loss: 0.8107016086578369\n",
      "Epoch 12889: Training Loss: 0.09546695649623871 Validation Loss: 0.8110650181770325\n",
      "Epoch 12890: Training Loss: 0.0955491413672765 Validation Loss: 0.8127155303955078\n",
      "Epoch 12891: Training Loss: 0.09508598844210307 Validation Loss: 0.8118175268173218\n",
      "Epoch 12892: Training Loss: 0.09561548382043839 Validation Loss: 0.8108773231506348\n",
      "Epoch 12893: Training Loss: 0.09557582934697469 Validation Loss: 0.8108818531036377\n",
      "Epoch 12894: Training Loss: 0.09562238057454427 Validation Loss: 0.8114691376686096\n",
      "Epoch 12895: Training Loss: 0.09534924477338791 Validation Loss: 0.8116519451141357\n",
      "Epoch 12896: Training Loss: 0.09541456401348114 Validation Loss: 0.8112822771072388\n",
      "Epoch 12897: Training Loss: 0.09555587669213612 Validation Loss: 0.810792863368988\n",
      "Epoch 12898: Training Loss: 0.09603786716858546 Validation Loss: 0.8108131885528564\n",
      "Epoch 12899: Training Loss: 0.09595923374096553 Validation Loss: 0.8122648000717163\n",
      "Epoch 12900: Training Loss: 0.0956504022081693 Validation Loss: 0.8124377131462097\n",
      "Epoch 12901: Training Loss: 0.09584054599205653 Validation Loss: 0.8127544522285461\n",
      "Epoch 12902: Training Loss: 0.09522876143455505 Validation Loss: 0.8127268552780151\n",
      "Epoch 12903: Training Loss: 0.09563175092140834 Validation Loss: 0.8122031688690186\n",
      "Epoch 12904: Training Loss: 0.09549633413553238 Validation Loss: 0.8121075630187988\n",
      "Epoch 12905: Training Loss: 0.09529279172420502 Validation Loss: 0.8127619028091431\n",
      "Epoch 12906: Training Loss: 0.09549348801374435 Validation Loss: 0.812301516532898\n",
      "Epoch 12907: Training Loss: 0.09547555943330129 Validation Loss: 0.8128260374069214\n",
      "Epoch 12908: Training Loss: 0.09540663907925288 Validation Loss: 0.8125627636909485\n",
      "Epoch 12909: Training Loss: 0.09537870933612187 Validation Loss: 0.8123536705970764\n",
      "Epoch 12910: Training Loss: 0.09512438625097275 Validation Loss: 0.8119615316390991\n",
      "Epoch 12911: Training Loss: 0.09537863234678905 Validation Loss: 0.8119675517082214\n",
      "Epoch 12912: Training Loss: 0.09632535527149837 Validation Loss: 0.8112083077430725\n",
      "Epoch 12913: Training Loss: 0.09532567858695984 Validation Loss: 0.8120150566101074\n",
      "Epoch 12914: Training Loss: 0.09551590929428737 Validation Loss: 0.8116990923881531\n",
      "Epoch 12915: Training Loss: 0.09520658602317174 Validation Loss: 0.8113948702812195\n",
      "Epoch 12916: Training Loss: 0.09568449358145396 Validation Loss: 0.8123597502708435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12917: Training Loss: 0.09508421768744786 Validation Loss: 0.81156325340271\n",
      "Epoch 12918: Training Loss: 0.09544101605812709 Validation Loss: 0.8114849925041199\n",
      "Epoch 12919: Training Loss: 0.09532801061868668 Validation Loss: 0.811211884021759\n",
      "Epoch 12920: Training Loss: 0.09555285175641377 Validation Loss: 0.8112382292747498\n",
      "Epoch 12921: Training Loss: 0.0953821986913681 Validation Loss: 0.8111907243728638\n",
      "Epoch 12922: Training Loss: 0.0952766165137291 Validation Loss: 0.8125202655792236\n",
      "Epoch 12923: Training Loss: 0.09599284331003825 Validation Loss: 0.8126457929611206\n",
      "Epoch 12924: Training Loss: 0.09538095196088155 Validation Loss: 0.8120107054710388\n",
      "Epoch 12925: Training Loss: 0.09531645725170772 Validation Loss: 0.8122836947441101\n",
      "Epoch 12926: Training Loss: 0.09529993683099747 Validation Loss: 0.8127204775810242\n",
      "Epoch 12927: Training Loss: 0.0954945981502533 Validation Loss: 0.8129730224609375\n",
      "Epoch 12928: Training Loss: 0.09526769568522771 Validation Loss: 0.8122761249542236\n",
      "Epoch 12929: Training Loss: 0.0953237513701121 Validation Loss: 0.811295211315155\n",
      "Epoch 12930: Training Loss: 0.09594666709502538 Validation Loss: 0.8115652799606323\n",
      "Epoch 12931: Training Loss: 0.09544009963671367 Validation Loss: 0.8126376867294312\n",
      "Epoch 12932: Training Loss: 0.09525105108817418 Validation Loss: 0.8128871321678162\n",
      "Epoch 12933: Training Loss: 0.09538809458414714 Validation Loss: 0.813020646572113\n",
      "Epoch 12934: Training Loss: 0.09558511525392532 Validation Loss: 0.8139172792434692\n",
      "Epoch 12935: Training Loss: 0.09572735925515492 Validation Loss: 0.813004732131958\n",
      "Epoch 12936: Training Loss: 0.09518576661745708 Validation Loss: 0.8120601773262024\n",
      "Epoch 12937: Training Loss: 0.0949432502190272 Validation Loss: 0.8115026354789734\n",
      "Epoch 12938: Training Loss: 0.09494920323292415 Validation Loss: 0.8121368885040283\n",
      "Epoch 12939: Training Loss: 0.09517743935187657 Validation Loss: 0.8116759061813354\n",
      "Epoch 12940: Training Loss: 0.09503407031297684 Validation Loss: 0.8117751479148865\n",
      "Epoch 12941: Training Loss: 0.09530834605296452 Validation Loss: 0.8116803765296936\n",
      "Epoch 12942: Training Loss: 0.095675028860569 Validation Loss: 0.8129454255104065\n",
      "Epoch 12943: Training Loss: 0.09532291690508525 Validation Loss: 0.8128156661987305\n",
      "Epoch 12944: Training Loss: 0.09516972055037816 Validation Loss: 0.8126715421676636\n",
      "Epoch 12945: Training Loss: 0.09532104680935542 Validation Loss: 0.8117485642433167\n",
      "Epoch 12946: Training Loss: 0.09513280043999355 Validation Loss: 0.8110591769218445\n",
      "Epoch 12947: Training Loss: 0.09538158774375916 Validation Loss: 0.8119895458221436\n",
      "Epoch 12948: Training Loss: 0.09524185458819072 Validation Loss: 0.8121331334114075\n",
      "Epoch 12949: Training Loss: 0.09518561760584514 Validation Loss: 0.812350869178772\n",
      "Epoch 12950: Training Loss: 0.09557696183522542 Validation Loss: 0.8122889995574951\n",
      "Epoch 12951: Training Loss: 0.09562707444032033 Validation Loss: 0.8125240206718445\n",
      "Epoch 12952: Training Loss: 0.09616825481255849 Validation Loss: 0.8135258555412292\n",
      "Epoch 12953: Training Loss: 0.09552505612373352 Validation Loss: 0.8140666484832764\n",
      "Epoch 12954: Training Loss: 0.09558106462160747 Validation Loss: 0.8131001591682434\n",
      "Epoch 12955: Training Loss: 0.0950225368142128 Validation Loss: 0.8127544522285461\n",
      "Epoch 12956: Training Loss: 0.09578239421049754 Validation Loss: 0.8122539520263672\n",
      "Epoch 12957: Training Loss: 0.09533852835496266 Validation Loss: 0.8127160668373108\n",
      "Epoch 12958: Training Loss: 0.09530780961116155 Validation Loss: 0.8134604096412659\n",
      "Epoch 12959: Training Loss: 0.09569491942723592 Validation Loss: 0.8136982917785645\n",
      "Epoch 12960: Training Loss: 0.09520719697078069 Validation Loss: 0.8131667971611023\n",
      "Epoch 12961: Training Loss: 0.09563887119293213 Validation Loss: 0.8115344047546387\n",
      "Epoch 12962: Training Loss: 0.0950652261575063 Validation Loss: 0.8115787506103516\n",
      "Epoch 12963: Training Loss: 0.09523633867502213 Validation Loss: 0.8122415542602539\n",
      "Epoch 12964: Training Loss: 0.09501774112383525 Validation Loss: 0.8127635717391968\n",
      "Epoch 12965: Training Loss: 0.0950433611869812 Validation Loss: 0.8129360675811768\n",
      "Epoch 12966: Training Loss: 0.09511035929123561 Validation Loss: 0.8130574822425842\n",
      "Epoch 12967: Training Loss: 0.09565263489882152 Validation Loss: 0.8136615753173828\n",
      "Epoch 12968: Training Loss: 0.09536599616209666 Validation Loss: 0.812617838382721\n",
      "Epoch 12969: Training Loss: 0.09575085093577702 Validation Loss: 0.8123064637184143\n",
      "Epoch 12970: Training Loss: 0.09513546029726665 Validation Loss: 0.8129768371582031\n",
      "Epoch 12971: Training Loss: 0.09531022608280182 Validation Loss: 0.8130558729171753\n",
      "Epoch 12972: Training Loss: 0.09509498625993729 Validation Loss: 0.8127967119216919\n",
      "Epoch 12973: Training Loss: 0.09522443761428197 Validation Loss: 0.8129686117172241\n",
      "Epoch 12974: Training Loss: 0.09507716198762257 Validation Loss: 0.8127498030662537\n",
      "Epoch 12975: Training Loss: 0.0950807382663091 Validation Loss: 0.8124750256538391\n",
      "Epoch 12976: Training Loss: 0.09496907889842987 Validation Loss: 0.8118194937705994\n",
      "Epoch 12977: Training Loss: 0.09516976773738861 Validation Loss: 0.8116227388381958\n",
      "Epoch 12978: Training Loss: 0.0947950929403305 Validation Loss: 0.8126144409179688\n",
      "Epoch 12979: Training Loss: 0.0952059601744016 Validation Loss: 0.8134404420852661\n",
      "Epoch 12980: Training Loss: 0.09516697873671849 Validation Loss: 0.812506914138794\n",
      "Epoch 12981: Training Loss: 0.09533739338318507 Validation Loss: 0.8125554323196411\n",
      "Epoch 12982: Training Loss: 0.09510611494382222 Validation Loss: 0.8123986721038818\n",
      "Epoch 12983: Training Loss: 0.09545694043238957 Validation Loss: 0.8116949200630188\n",
      "Epoch 12984: Training Loss: 0.09501444796721141 Validation Loss: 0.8124823570251465\n",
      "Epoch 12985: Training Loss: 0.0954950029651324 Validation Loss: 0.8124478459358215\n",
      "Epoch 12986: Training Loss: 0.09500970443089803 Validation Loss: 0.8128641843795776\n",
      "Epoch 12987: Training Loss: 0.09514554093281428 Validation Loss: 0.8129651546478271\n",
      "Epoch 12988: Training Loss: 0.09496244539817174 Validation Loss: 0.8125189542770386\n",
      "Epoch 12989: Training Loss: 0.09516541908184688 Validation Loss: 0.811784029006958\n",
      "Epoch 12990: Training Loss: 0.0952867642045021 Validation Loss: 0.8116974234580994\n",
      "Epoch 12991: Training Loss: 0.09539852788050969 Validation Loss: 0.8124958276748657\n",
      "Epoch 12992: Training Loss: 0.09500510742266972 Validation Loss: 0.813875138759613\n",
      "Epoch 12993: Training Loss: 0.09515339881181717 Validation Loss: 0.8148923516273499\n",
      "Epoch 12994: Training Loss: 0.0954456627368927 Validation Loss: 0.8156723380088806\n",
      "Epoch 12995: Training Loss: 0.09479630490144093 Validation Loss: 0.8145689964294434\n",
      "Epoch 12996: Training Loss: 0.09512125452359517 Validation Loss: 0.8135220408439636\n",
      "Epoch 12997: Training Loss: 0.09501090397437413 Validation Loss: 0.8125905990600586\n",
      "Epoch 12998: Training Loss: 0.09536173691352208 Validation Loss: 0.8119470477104187\n",
      "Epoch 12999: Training Loss: 0.0956342617670695 Validation Loss: 0.8126035928726196\n",
      "Epoch 13000: Training Loss: 0.09475748240947723 Validation Loss: 0.8134326934814453\n",
      "Epoch 13001: Training Loss: 0.09509295225143433 Validation Loss: 0.8140021562576294\n",
      "Epoch 13002: Training Loss: 0.09592579801877339 Validation Loss: 0.8130427598953247\n",
      "Epoch 13003: Training Loss: 0.09482263525327046 Validation Loss: 0.8123961687088013\n",
      "Epoch 13004: Training Loss: 0.09515813489754994 Validation Loss: 0.8122528791427612\n",
      "Epoch 13005: Training Loss: 0.09487999478975932 Validation Loss: 0.812220573425293\n",
      "Epoch 13006: Training Loss: 0.09488632778326671 Validation Loss: 0.8122435808181763\n",
      "Epoch 13007: Training Loss: 0.09556649128595988 Validation Loss: 0.8123508095741272\n",
      "Epoch 13008: Training Loss: 0.09503876914580663 Validation Loss: 0.813554048538208\n",
      "Epoch 13009: Training Loss: 0.09503495693206787 Validation Loss: 0.814500093460083\n",
      "Epoch 13010: Training Loss: 0.09520064294338226 Validation Loss: 0.8137916326522827\n",
      "Epoch 13011: Training Loss: 0.09562100718418758 Validation Loss: 0.8133227825164795\n",
      "Epoch 13012: Training Loss: 0.09515728304783504 Validation Loss: 0.8116560578346252\n",
      "Epoch 13013: Training Loss: 0.09518899023532867 Validation Loss: 0.811120867729187\n",
      "Epoch 13014: Training Loss: 0.0950164223710696 Validation Loss: 0.8116887807846069\n",
      "Epoch 13015: Training Loss: 0.09507334232330322 Validation Loss: 0.8123619556427002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13016: Training Loss: 0.09492071717977524 Validation Loss: 0.8140869140625\n",
      "Epoch 13017: Training Loss: 0.09531586368878682 Validation Loss: 0.8139224648475647\n",
      "Epoch 13018: Training Loss: 0.09509700785080592 Validation Loss: 0.8138968348503113\n",
      "Epoch 13019: Training Loss: 0.09508614242076874 Validation Loss: 0.8133350014686584\n",
      "Epoch 13020: Training Loss: 0.09643679360548656 Validation Loss: 0.8131737112998962\n",
      "Epoch 13021: Training Loss: 0.09464004387458165 Validation Loss: 0.8132590055465698\n",
      "Epoch 13022: Training Loss: 0.09496793895959854 Validation Loss: 0.8127901554107666\n",
      "Epoch 13023: Training Loss: 0.09483546763658524 Validation Loss: 0.813798189163208\n",
      "Epoch 13024: Training Loss: 0.09478731205066045 Validation Loss: 0.8142335414886475\n",
      "Epoch 13025: Training Loss: 0.09490609914064407 Validation Loss: 0.8136216998100281\n",
      "Epoch 13026: Training Loss: 0.09506113827228546 Validation Loss: 0.8132238984107971\n",
      "Epoch 13027: Training Loss: 0.09486414988835652 Validation Loss: 0.8133273720741272\n",
      "Epoch 13028: Training Loss: 0.09483390549818675 Validation Loss: 0.8134082555770874\n",
      "Epoch 13029: Training Loss: 0.09484221786260605 Validation Loss: 0.8126533031463623\n",
      "Epoch 13030: Training Loss: 0.09474839270114899 Validation Loss: 0.8122588396072388\n",
      "Epoch 13031: Training Loss: 0.09547278036673863 Validation Loss: 0.8127195239067078\n",
      "Epoch 13032: Training Loss: 0.0949382409453392 Validation Loss: 0.8125049471855164\n",
      "Epoch 13033: Training Loss: 0.0952260469396909 Validation Loss: 0.812567412853241\n",
      "Epoch 13034: Training Loss: 0.09495199471712112 Validation Loss: 0.8133606314659119\n",
      "Epoch 13035: Training Loss: 0.09503035247325897 Validation Loss: 0.8136037588119507\n",
      "Epoch 13036: Training Loss: 0.09487673391898473 Validation Loss: 0.813334047794342\n",
      "Epoch 13037: Training Loss: 0.09508427480856578 Validation Loss: 0.8136723041534424\n",
      "Epoch 13038: Training Loss: 0.0947473794221878 Validation Loss: 0.8143707513809204\n",
      "Epoch 13039: Training Loss: 0.09485441197951634 Validation Loss: 0.8142205476760864\n",
      "Epoch 13040: Training Loss: 0.09519747893015544 Validation Loss: 0.8147189617156982\n",
      "Epoch 13041: Training Loss: 0.09539550294478734 Validation Loss: 0.8147310018539429\n",
      "Epoch 13042: Training Loss: 0.09482079744338989 Validation Loss: 0.8135372996330261\n",
      "Epoch 13043: Training Loss: 0.09477682411670685 Validation Loss: 0.8135960698127747\n",
      "Epoch 13044: Training Loss: 0.09500696510076523 Validation Loss: 0.8130066990852356\n",
      "Epoch 13045: Training Loss: 0.09459375590085983 Validation Loss: 0.8134446740150452\n",
      "Epoch 13046: Training Loss: 0.0956532433629036 Validation Loss: 0.8142524361610413\n",
      "Epoch 13047: Training Loss: 0.09473939488331477 Validation Loss: 0.8137140274047852\n",
      "Epoch 13048: Training Loss: 0.09591830273469289 Validation Loss: 0.8121237754821777\n",
      "Epoch 13049: Training Loss: 0.09496572365363438 Validation Loss: 0.8135172128677368\n",
      "Epoch 13050: Training Loss: 0.09485521167516708 Validation Loss: 0.814290463924408\n",
      "Epoch 13051: Training Loss: 0.09491204718748729 Validation Loss: 0.8143191933631897\n",
      "Epoch 13052: Training Loss: 0.09476194034020106 Validation Loss: 0.8135265707969666\n",
      "Epoch 13053: Training Loss: 0.09459252903858821 Validation Loss: 0.8136133551597595\n",
      "Epoch 13054: Training Loss: 0.09487699220577876 Validation Loss: 0.8133561015129089\n",
      "Epoch 13055: Training Loss: 0.0948695441087087 Validation Loss: 0.813430666923523\n",
      "Epoch 13056: Training Loss: 0.09512131412823994 Validation Loss: 0.813119113445282\n",
      "Epoch 13057: Training Loss: 0.09523583700259526 Validation Loss: 0.8130964040756226\n",
      "Epoch 13058: Training Loss: 0.09506538510322571 Validation Loss: 0.8135419487953186\n",
      "Epoch 13059: Training Loss: 0.09454238911469777 Validation Loss: 0.8140656352043152\n",
      "Epoch 13060: Training Loss: 0.0948960358897845 Validation Loss: 0.8139930367469788\n",
      "Epoch 13061: Training Loss: 0.0947041188677152 Validation Loss: 0.8142488598823547\n",
      "Epoch 13062: Training Loss: 0.0951752985517184 Validation Loss: 0.8129542469978333\n",
      "Epoch 13063: Training Loss: 0.09484967092672984 Validation Loss: 0.8122268915176392\n",
      "Epoch 13064: Training Loss: 0.0952554742495219 Validation Loss: 0.8128193616867065\n",
      "Epoch 13065: Training Loss: 0.09503989666700363 Validation Loss: 0.8138443827629089\n",
      "Epoch 13066: Training Loss: 0.09455768764019012 Validation Loss: 0.8141934871673584\n",
      "Epoch 13067: Training Loss: 0.09467488278945287 Validation Loss: 0.8143389821052551\n",
      "Epoch 13068: Training Loss: 0.09487312287092209 Validation Loss: 0.8139774799346924\n",
      "Epoch 13069: Training Loss: 0.09483099480470021 Validation Loss: 0.8144847750663757\n",
      "Epoch 13070: Training Loss: 0.09507011870543162 Validation Loss: 0.8143625855445862\n",
      "Epoch 13071: Training Loss: 0.09535453220208485 Validation Loss: 0.8136507272720337\n",
      "Epoch 13072: Training Loss: 0.09496091802914937 Validation Loss: 0.8139181137084961\n",
      "Epoch 13073: Training Loss: 0.09464314083258311 Validation Loss: 0.8137985467910767\n",
      "Epoch 13074: Training Loss: 0.09492595742146175 Validation Loss: 0.8144338130950928\n",
      "Epoch 13075: Training Loss: 0.09474872052669525 Validation Loss: 0.8152192831039429\n",
      "Epoch 13076: Training Loss: 0.0951455608010292 Validation Loss: 0.8144200444221497\n",
      "Epoch 13077: Training Loss: 0.09466320276260376 Validation Loss: 0.8139551877975464\n",
      "Epoch 13078: Training Loss: 0.09486990173657735 Validation Loss: 0.813651442527771\n",
      "Epoch 13079: Training Loss: 0.09467953443527222 Validation Loss: 0.8134987354278564\n",
      "Epoch 13080: Training Loss: 0.09502621243397395 Validation Loss: 0.8143001794815063\n",
      "Epoch 13081: Training Loss: 0.0951497678955396 Validation Loss: 0.8147248029708862\n",
      "Epoch 13082: Training Loss: 0.0947468454639117 Validation Loss: 0.8148173093795776\n",
      "Epoch 13083: Training Loss: 0.09458769609530766 Validation Loss: 0.8141714334487915\n",
      "Epoch 13084: Training Loss: 0.09480374306440353 Validation Loss: 0.8136741518974304\n",
      "Epoch 13085: Training Loss: 0.09497640033562978 Validation Loss: 0.8121095299720764\n",
      "Epoch 13086: Training Loss: 0.09464950114488602 Validation Loss: 0.8127894997596741\n",
      "Epoch 13087: Training Loss: 0.09494683891534805 Validation Loss: 0.8140078186988831\n",
      "Epoch 13088: Training Loss: 0.09471838921308517 Validation Loss: 0.815031886100769\n",
      "Epoch 13089: Training Loss: 0.09481018284956615 Validation Loss: 0.8140471577644348\n",
      "Epoch 13090: Training Loss: 0.09482225527366002 Validation Loss: 0.8138671517372131\n",
      "Epoch 13091: Training Loss: 0.09468021988868713 Validation Loss: 0.8135493993759155\n",
      "Epoch 13092: Training Loss: 0.09466943393150966 Validation Loss: 0.8146495223045349\n",
      "Epoch 13093: Training Loss: 0.09444518635670344 Validation Loss: 0.8140743970870972\n",
      "Epoch 13094: Training Loss: 0.09461731712023418 Validation Loss: 0.8141402006149292\n",
      "Epoch 13095: Training Loss: 0.0948370173573494 Validation Loss: 0.8142899870872498\n",
      "Epoch 13096: Training Loss: 0.09462495644887288 Validation Loss: 0.8143231868743896\n",
      "Epoch 13097: Training Loss: 0.09477344155311584 Validation Loss: 0.8143795728683472\n",
      "Epoch 13098: Training Loss: 0.09497159471114476 Validation Loss: 0.8131148219108582\n",
      "Epoch 13099: Training Loss: 0.0946248397231102 Validation Loss: 0.8134094476699829\n",
      "Epoch 13100: Training Loss: 0.09451945374409358 Validation Loss: 0.8145753741264343\n",
      "Epoch 13101: Training Loss: 0.09500626971324284 Validation Loss: 0.8146296739578247\n",
      "Epoch 13102: Training Loss: 0.09447684387365977 Validation Loss: 0.8144325017929077\n",
      "Epoch 13103: Training Loss: 0.09443557262420654 Validation Loss: 0.8146953582763672\n",
      "Epoch 13104: Training Loss: 0.0951078434785207 Validation Loss: 0.813836932182312\n",
      "Epoch 13105: Training Loss: 0.09468663980563481 Validation Loss: 0.8139317631721497\n",
      "Epoch 13106: Training Loss: 0.09459881732861201 Validation Loss: 0.8147256374359131\n",
      "Epoch 13107: Training Loss: 0.09461592386166255 Validation Loss: 0.8150662779808044\n",
      "Epoch 13108: Training Loss: 0.09506049007177353 Validation Loss: 0.8146727681159973\n",
      "Epoch 13109: Training Loss: 0.09484747797250748 Validation Loss: 0.8132948279380798\n",
      "Epoch 13110: Training Loss: 0.09467092901468277 Validation Loss: 0.8137108683586121\n",
      "Epoch 13111: Training Loss: 0.09443113456169765 Validation Loss: 0.8138622045516968\n",
      "Epoch 13112: Training Loss: 0.09546250601609547 Validation Loss: 0.8145862817764282\n",
      "Epoch 13113: Training Loss: 0.0947083830833435 Validation Loss: 0.8139694333076477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13114: Training Loss: 0.0944799135128657 Validation Loss: 0.8141725659370422\n",
      "Epoch 13115: Training Loss: 0.09472136447827022 Validation Loss: 0.8139453530311584\n",
      "Epoch 13116: Training Loss: 0.09452684968709946 Validation Loss: 0.8145126700401306\n",
      "Epoch 13117: Training Loss: 0.09447949628035228 Validation Loss: 0.81448894739151\n",
      "Epoch 13118: Training Loss: 0.0945620263616244 Validation Loss: 0.8151988387107849\n",
      "Epoch 13119: Training Loss: 0.09439316391944885 Validation Loss: 0.8148724436759949\n",
      "Epoch 13120: Training Loss: 0.0946271022160848 Validation Loss: 0.8142438530921936\n",
      "Epoch 13121: Training Loss: 0.09489270796378453 Validation Loss: 0.8134024143218994\n",
      "Epoch 13122: Training Loss: 0.09419432282447815 Validation Loss: 0.8135370016098022\n",
      "Epoch 13123: Training Loss: 0.09427487601836522 Validation Loss: 0.814223051071167\n",
      "Epoch 13124: Training Loss: 0.09503111243247986 Validation Loss: 0.8147023320198059\n",
      "Epoch 13125: Training Loss: 0.09457667420307796 Validation Loss: 0.8145233988761902\n",
      "Epoch 13126: Training Loss: 0.09445187697807948 Validation Loss: 0.8148176074028015\n",
      "Epoch 13127: Training Loss: 0.09476674099763234 Validation Loss: 0.8146572113037109\n",
      "Epoch 13128: Training Loss: 0.09536246955394745 Validation Loss: 0.8144398331642151\n",
      "Epoch 13129: Training Loss: 0.09453557431697845 Validation Loss: 0.8148629665374756\n",
      "Epoch 13130: Training Loss: 0.09444852421681087 Validation Loss: 0.8149949908256531\n",
      "Epoch 13131: Training Loss: 0.09478300561507542 Validation Loss: 0.8151153922080994\n",
      "Epoch 13132: Training Loss: 0.09457018971443176 Validation Loss: 0.8155745267868042\n",
      "Epoch 13133: Training Loss: 0.09468200554450353 Validation Loss: 0.8168115019798279\n",
      "Epoch 13134: Training Loss: 0.09484374274810155 Validation Loss: 0.8158362507820129\n",
      "Epoch 13135: Training Loss: 0.09448515127102534 Validation Loss: 0.8154363036155701\n",
      "Epoch 13136: Training Loss: 0.09473738819360733 Validation Loss: 0.8149064779281616\n",
      "Epoch 13137: Training Loss: 0.09457053989171982 Validation Loss: 0.8144683241844177\n",
      "Epoch 13138: Training Loss: 0.09458156178394954 Validation Loss: 0.8141411542892456\n",
      "Epoch 13139: Training Loss: 0.09452792753775914 Validation Loss: 0.8145570755004883\n",
      "Epoch 13140: Training Loss: 0.09450480341911316 Validation Loss: 0.8151444792747498\n",
      "Epoch 13141: Training Loss: 0.09456151723861694 Validation Loss: 0.8159438371658325\n",
      "Epoch 13142: Training Loss: 0.0948082705338796 Validation Loss: 0.8153240084648132\n",
      "Epoch 13143: Training Loss: 0.09448246657848358 Validation Loss: 0.8140959739685059\n",
      "Epoch 13144: Training Loss: 0.0944069077571233 Validation Loss: 0.8129006028175354\n",
      "Epoch 13145: Training Loss: 0.09453160812457402 Validation Loss: 0.8130171298980713\n",
      "Epoch 13146: Training Loss: 0.09467626363039017 Validation Loss: 0.8134479522705078\n",
      "Epoch 13147: Training Loss: 0.09458942959705989 Validation Loss: 0.8153723478317261\n",
      "Epoch 13148: Training Loss: 0.09440476447343826 Validation Loss: 0.815850019454956\n",
      "Epoch 13149: Training Loss: 0.09453066686789195 Validation Loss: 0.8160787224769592\n",
      "Epoch 13150: Training Loss: 0.09460450957218806 Validation Loss: 0.8151498436927795\n",
      "Epoch 13151: Training Loss: 0.09443352619806926 Validation Loss: 0.8143823146820068\n",
      "Epoch 13152: Training Loss: 0.09480444838603337 Validation Loss: 0.814551591873169\n",
      "Epoch 13153: Training Loss: 0.09442898382743199 Validation Loss: 0.814518392086029\n",
      "Epoch 13154: Training Loss: 0.09446366628011067 Validation Loss: 0.8140292763710022\n",
      "Epoch 13155: Training Loss: 0.09438926974932353 Validation Loss: 0.8143892884254456\n",
      "Epoch 13156: Training Loss: 0.09438174217939377 Validation Loss: 0.8145826458930969\n",
      "Epoch 13157: Training Loss: 0.09468952566385269 Validation Loss: 0.8152058124542236\n",
      "Epoch 13158: Training Loss: 0.09444231788317363 Validation Loss: 0.8146564960479736\n",
      "Epoch 13159: Training Loss: 0.09445328762133916 Validation Loss: 0.8140602707862854\n",
      "Epoch 13160: Training Loss: 0.09479257464408875 Validation Loss: 0.8138995170593262\n",
      "Epoch 13161: Training Loss: 0.09429678072532018 Validation Loss: 0.8151445388793945\n",
      "Epoch 13162: Training Loss: 0.09451607118050258 Validation Loss: 0.8146673440933228\n",
      "Epoch 13163: Training Loss: 0.09494185199340184 Validation Loss: 0.8155568242073059\n",
      "Epoch 13164: Training Loss: 0.0944219504793485 Validation Loss: 0.8155831098556519\n",
      "Epoch 13165: Training Loss: 0.09448747833569844 Validation Loss: 0.8144828677177429\n",
      "Epoch 13166: Training Loss: 0.09421385576327641 Validation Loss: 0.8131219148635864\n",
      "Epoch 13167: Training Loss: 0.0944744125008583 Validation Loss: 0.8141287565231323\n",
      "Epoch 13168: Training Loss: 0.09462260454893112 Validation Loss: 0.8153412938117981\n",
      "Epoch 13169: Training Loss: 0.09511993080377579 Validation Loss: 0.8166727423667908\n",
      "Epoch 13170: Training Loss: 0.09461694459120433 Validation Loss: 0.8152425289154053\n",
      "Epoch 13171: Training Loss: 0.09426544109980266 Validation Loss: 0.8145348429679871\n",
      "Epoch 13172: Training Loss: 0.09431729465723038 Validation Loss: 0.8138737678527832\n",
      "Epoch 13173: Training Loss: 0.09407787770032883 Validation Loss: 0.8141350746154785\n",
      "Epoch 13174: Training Loss: 0.09447873880465825 Validation Loss: 0.8143565654754639\n",
      "Epoch 13175: Training Loss: 0.0946112722158432 Validation Loss: 0.8157482147216797\n",
      "Epoch 13176: Training Loss: 0.09415589769681294 Validation Loss: 0.8162738084793091\n",
      "Epoch 13177: Training Loss: 0.09467703352371852 Validation Loss: 0.8161216974258423\n",
      "Epoch 13178: Training Loss: 0.09469414999087651 Validation Loss: 0.8159334063529968\n",
      "Epoch 13179: Training Loss: 0.09503596027692159 Validation Loss: 0.8153842687606812\n",
      "Epoch 13180: Training Loss: 0.09463035811980565 Validation Loss: 0.814037561416626\n",
      "Epoch 13181: Training Loss: 0.09431389967600505 Validation Loss: 0.8149450421333313\n",
      "Epoch 13182: Training Loss: 0.09450867275396983 Validation Loss: 0.8151246905326843\n",
      "Epoch 13183: Training Loss: 0.09436234831809998 Validation Loss: 0.8151179552078247\n",
      "Epoch 13184: Training Loss: 0.0941560169061025 Validation Loss: 0.8143378496170044\n",
      "Epoch 13185: Training Loss: 0.09451632698376973 Validation Loss: 0.8146691918373108\n",
      "Epoch 13186: Training Loss: 0.09444751838843028 Validation Loss: 0.8150184154510498\n",
      "Epoch 13187: Training Loss: 0.09449472278356552 Validation Loss: 0.8154808282852173\n",
      "Epoch 13188: Training Loss: 0.09487047294775645 Validation Loss: 0.8155075311660767\n",
      "Epoch 13189: Training Loss: 0.09443209071954091 Validation Loss: 0.815339982509613\n",
      "Epoch 13190: Training Loss: 0.09435966114203136 Validation Loss: 0.8153209090232849\n",
      "Epoch 13191: Training Loss: 0.09470762560764949 Validation Loss: 0.8145499229431152\n",
      "Epoch 13192: Training Loss: 0.0946089078982671 Validation Loss: 0.815731406211853\n",
      "Epoch 13193: Training Loss: 0.09392088403304417 Validation Loss: 0.8158254027366638\n",
      "Epoch 13194: Training Loss: 0.09427931904792786 Validation Loss: 0.8156382441520691\n",
      "Epoch 13195: Training Loss: 0.09464853753646214 Validation Loss: 0.8161726593971252\n",
      "Epoch 13196: Training Loss: 0.09431962668895721 Validation Loss: 0.8160062432289124\n",
      "Epoch 13197: Training Loss: 0.09526397039492925 Validation Loss: 0.8163454532623291\n",
      "Epoch 13198: Training Loss: 0.09420206149419148 Validation Loss: 0.8158661723136902\n",
      "Epoch 13199: Training Loss: 0.09397117048501968 Validation Loss: 0.8150662183761597\n",
      "Epoch 13200: Training Loss: 0.09452719241380692 Validation Loss: 0.8141976594924927\n",
      "Epoch 13201: Training Loss: 0.09397601832946141 Validation Loss: 0.8151071667671204\n",
      "Epoch 13202: Training Loss: 0.094299611945947 Validation Loss: 0.8166416883468628\n",
      "Epoch 13203: Training Loss: 0.09418004751205444 Validation Loss: 0.8160708546638489\n",
      "Epoch 13204: Training Loss: 0.0943464661637942 Validation Loss: 0.8159188032150269\n",
      "Epoch 13205: Training Loss: 0.09427434454361598 Validation Loss: 0.815176248550415\n",
      "Epoch 13206: Training Loss: 0.09511584043502808 Validation Loss: 0.8155439496040344\n",
      "Epoch 13207: Training Loss: 0.09426367531220119 Validation Loss: 0.8156049251556396\n",
      "Epoch 13208: Training Loss: 0.09410493075847626 Validation Loss: 0.8152123689651489\n",
      "Epoch 13209: Training Loss: 0.09436068435509999 Validation Loss: 0.8147650361061096\n",
      "Epoch 13210: Training Loss: 0.09425727029641469 Validation Loss: 0.8144605755805969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13211: Training Loss: 0.09439179797967275 Validation Loss: 0.8148745894432068\n",
      "Epoch 13212: Training Loss: 0.09392595539490382 Validation Loss: 0.815516471862793\n",
      "Epoch 13213: Training Loss: 0.09406915307044983 Validation Loss: 0.8152008056640625\n",
      "Epoch 13214: Training Loss: 0.09461166461308797 Validation Loss: 0.8148072957992554\n",
      "Epoch 13215: Training Loss: 0.09382111330827077 Validation Loss: 0.8152697086334229\n",
      "Epoch 13216: Training Loss: 0.0941379964351654 Validation Loss: 0.8157053589820862\n",
      "Epoch 13217: Training Loss: 0.09433325131734212 Validation Loss: 0.8153800368309021\n",
      "Epoch 13218: Training Loss: 0.09398962805668513 Validation Loss: 0.8159652352333069\n",
      "Epoch 13219: Training Loss: 0.09431398163239162 Validation Loss: 0.8161205649375916\n",
      "Epoch 13220: Training Loss: 0.09481511513392131 Validation Loss: 0.816373884677887\n",
      "Epoch 13221: Training Loss: 0.09414655963579814 Validation Loss: 0.8169033527374268\n",
      "Epoch 13222: Training Loss: 0.09436569859584172 Validation Loss: 0.8163538575172424\n",
      "Epoch 13223: Training Loss: 0.09426240374644597 Validation Loss: 0.8151887059211731\n",
      "Epoch 13224: Training Loss: 0.09428271899620692 Validation Loss: 0.8151183724403381\n",
      "Epoch 13225: Training Loss: 0.09468807528416316 Validation Loss: 0.8164698481559753\n",
      "Epoch 13226: Training Loss: 0.09436418861150742 Validation Loss: 0.8146682977676392\n",
      "Epoch 13227: Training Loss: 0.09494966020186742 Validation Loss: 0.8144416809082031\n",
      "Epoch 13228: Training Loss: 0.09364207337299983 Validation Loss: 0.8144030570983887\n",
      "Epoch 13229: Training Loss: 0.0944415032863617 Validation Loss: 0.8140586018562317\n",
      "Epoch 13230: Training Loss: 0.09433415780464809 Validation Loss: 0.8157781362533569\n",
      "Epoch 13231: Training Loss: 0.09403492758671443 Validation Loss: 0.8152212500572205\n",
      "Epoch 13232: Training Loss: 0.0940447524189949 Validation Loss: 0.8146352767944336\n",
      "Epoch 13233: Training Loss: 0.09432936956485112 Validation Loss: 0.8151004910469055\n",
      "Epoch 13234: Training Loss: 0.09432521710793178 Validation Loss: 0.8151118159294128\n",
      "Epoch 13235: Training Loss: 0.09565416475137074 Validation Loss: 0.8169097304344177\n",
      "Epoch 13236: Training Loss: 0.09458163877328236 Validation Loss: 0.8182543516159058\n",
      "Epoch 13237: Training Loss: 0.09422176827987035 Validation Loss: 0.8171983957290649\n",
      "Epoch 13238: Training Loss: 0.09418528278668721 Validation Loss: 0.8165772557258606\n",
      "Epoch 13239: Training Loss: 0.0942175363500913 Validation Loss: 0.8172777891159058\n",
      "Epoch 13240: Training Loss: 0.09419736514488856 Validation Loss: 0.8167886734008789\n",
      "Epoch 13241: Training Loss: 0.09450269490480423 Validation Loss: 0.8165188431739807\n",
      "Epoch 13242: Training Loss: 0.09409634272257487 Validation Loss: 0.8155865669250488\n",
      "Epoch 13243: Training Loss: 0.09418419500192006 Validation Loss: 0.8154195547103882\n",
      "Epoch 13244: Training Loss: 0.09427917251984279 Validation Loss: 0.8151279091835022\n",
      "Epoch 13245: Training Loss: 0.09395973881085713 Validation Loss: 0.8152709007263184\n",
      "Epoch 13246: Training Loss: 0.09460189938545227 Validation Loss: 0.8143821954727173\n",
      "Epoch 13247: Training Loss: 0.09447833895683289 Validation Loss: 0.8154557347297668\n",
      "Epoch 13248: Training Loss: 0.09390903760989507 Validation Loss: 0.8153316378593445\n",
      "Epoch 13249: Training Loss: 0.09425052503744762 Validation Loss: 0.8154575228691101\n",
      "Epoch 13250: Training Loss: 0.09397693475087483 Validation Loss: 0.8155834674835205\n",
      "Epoch 13251: Training Loss: 0.09434267381827037 Validation Loss: 0.8154258728027344\n",
      "Epoch 13252: Training Loss: 0.0949128990372022 Validation Loss: 0.8163439631462097\n",
      "Epoch 13253: Training Loss: 0.09403092414140701 Validation Loss: 0.8164015412330627\n",
      "Epoch 13254: Training Loss: 0.0937506506840388 Validation Loss: 0.8160697221755981\n",
      "Epoch 13255: Training Loss: 0.09417170037825902 Validation Loss: 0.8161771893501282\n",
      "Epoch 13256: Training Loss: 0.09416341036558151 Validation Loss: 0.8151259422302246\n",
      "Epoch 13257: Training Loss: 0.0942620038986206 Validation Loss: 0.8168320059776306\n",
      "Epoch 13258: Training Loss: 0.09401731938123703 Validation Loss: 0.8173293471336365\n",
      "Epoch 13259: Training Loss: 0.09449714918931325 Validation Loss: 0.8166943192481995\n",
      "Epoch 13260: Training Loss: 0.09408058474461238 Validation Loss: 0.81651371717453\n",
      "Epoch 13261: Training Loss: 0.09418302526076634 Validation Loss: 0.8160268664360046\n",
      "Epoch 13262: Training Loss: 0.09400442242622375 Validation Loss: 0.8154182434082031\n",
      "Epoch 13263: Training Loss: 0.09414783616860707 Validation Loss: 0.8146228194236755\n",
      "Epoch 13264: Training Loss: 0.09450621157884598 Validation Loss: 0.8143373131752014\n",
      "Epoch 13265: Training Loss: 0.093958280980587 Validation Loss: 0.8152437806129456\n",
      "Epoch 13266: Training Loss: 0.09441729138294856 Validation Loss: 0.8164685368537903\n",
      "Epoch 13267: Training Loss: 0.09429363658030827 Validation Loss: 0.8166041970252991\n",
      "Epoch 13268: Training Loss: 0.09419562915960948 Validation Loss: 0.8168542981147766\n",
      "Epoch 13269: Training Loss: 0.09428439786036809 Validation Loss: 0.8161720037460327\n",
      "Epoch 13270: Training Loss: 0.09420038511355718 Validation Loss: 0.8150841593742371\n",
      "Epoch 13271: Training Loss: 0.09418180584907532 Validation Loss: 0.8151583075523376\n",
      "Epoch 13272: Training Loss: 0.09401268263657887 Validation Loss: 0.8158493041992188\n",
      "Epoch 13273: Training Loss: 0.09390294055143993 Validation Loss: 0.8161733746528625\n",
      "Epoch 13274: Training Loss: 0.09393978615601857 Validation Loss: 0.8172534108161926\n",
      "Epoch 13275: Training Loss: 0.09407864511013031 Validation Loss: 0.816332995891571\n",
      "Epoch 13276: Training Loss: 0.09385866175095241 Validation Loss: 0.8160845041275024\n",
      "Epoch 13277: Training Loss: 0.09428428610165913 Validation Loss: 0.8162251710891724\n",
      "Epoch 13278: Training Loss: 0.09407850354909897 Validation Loss: 0.8163610100746155\n",
      "Epoch 13279: Training Loss: 0.09432398279507954 Validation Loss: 0.8168694376945496\n",
      "Epoch 13280: Training Loss: 0.09388076762358348 Validation Loss: 0.8156235218048096\n",
      "Epoch 13281: Training Loss: 0.09407051901022594 Validation Loss: 0.8153319954872131\n",
      "Epoch 13282: Training Loss: 0.09420593579610188 Validation Loss: 0.8156014084815979\n",
      "Epoch 13283: Training Loss: 0.0944007287422816 Validation Loss: 0.8159834146499634\n",
      "Epoch 13284: Training Loss: 0.09393422305583954 Validation Loss: 0.8170360326766968\n",
      "Epoch 13285: Training Loss: 0.09417898456255595 Validation Loss: 0.8170862793922424\n",
      "Epoch 13286: Training Loss: 0.09442470222711563 Validation Loss: 0.8159273862838745\n",
      "Epoch 13287: Training Loss: 0.09429522355397542 Validation Loss: 0.8161906003952026\n",
      "Epoch 13288: Training Loss: 0.09422471622625987 Validation Loss: 0.8160166144371033\n",
      "Epoch 13289: Training Loss: 0.09400221208731334 Validation Loss: 0.8155397772789001\n",
      "Epoch 13290: Training Loss: 0.09393199533224106 Validation Loss: 0.8158950209617615\n",
      "Epoch 13291: Training Loss: 0.09380421787500381 Validation Loss: 0.8177376985549927\n",
      "Epoch 13292: Training Loss: 0.09414225071668625 Validation Loss: 0.8183573484420776\n",
      "Epoch 13293: Training Loss: 0.09400240083535512 Validation Loss: 0.8179099559783936\n",
      "Epoch 13294: Training Loss: 0.09422207872072856 Validation Loss: 0.8156362771987915\n",
      "Epoch 13295: Training Loss: 0.09390475352605183 Validation Loss: 0.8153789639472961\n",
      "Epoch 13296: Training Loss: 0.09383580088615417 Validation Loss: 0.8152479529380798\n",
      "Epoch 13297: Training Loss: 0.0942337488134702 Validation Loss: 0.8153682351112366\n",
      "Epoch 13298: Training Loss: 0.09395010024309158 Validation Loss: 0.8158104419708252\n",
      "Epoch 13299: Training Loss: 0.09422320127487183 Validation Loss: 0.8176835179328918\n",
      "Epoch 13300: Training Loss: 0.09444827338059743 Validation Loss: 0.8185380697250366\n",
      "Epoch 13301: Training Loss: 0.09396759172280629 Validation Loss: 0.817650556564331\n",
      "Epoch 13302: Training Loss: 0.09398968269427617 Validation Loss: 0.8158925175666809\n",
      "Epoch 13303: Training Loss: 0.09387166053056717 Validation Loss: 0.8163806796073914\n",
      "Epoch 13304: Training Loss: 0.09375739345947902 Validation Loss: 0.8171244859695435\n",
      "Epoch 13305: Training Loss: 0.09394970287879308 Validation Loss: 0.8178384900093079\n",
      "Epoch 13306: Training Loss: 0.09405678013960521 Validation Loss: 0.816096305847168\n",
      "Epoch 13307: Training Loss: 0.09349654614925385 Validation Loss: 0.8163897395133972\n",
      "Epoch 13308: Training Loss: 0.09405241906642914 Validation Loss: 0.8157782554626465\n",
      "Epoch 13309: Training Loss: 0.09382925182580948 Validation Loss: 0.8164173364639282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13310: Training Loss: 0.09407697369654973 Validation Loss: 0.8170353770256042\n",
      "Epoch 13311: Training Loss: 0.09386840462684631 Validation Loss: 0.8177565932273865\n",
      "Epoch 13312: Training Loss: 0.09360281626383464 Validation Loss: 0.8178704380989075\n",
      "Epoch 13313: Training Loss: 0.0937550316254298 Validation Loss: 0.8175513744354248\n",
      "Epoch 13314: Training Loss: 0.09389180938402812 Validation Loss: 0.8173837661743164\n",
      "Epoch 13315: Training Loss: 0.09376287460327148 Validation Loss: 0.8167588114738464\n",
      "Epoch 13316: Training Loss: 0.09352762997150421 Validation Loss: 0.8162273168563843\n",
      "Epoch 13317: Training Loss: 0.09424432118733723 Validation Loss: 0.8153818249702454\n",
      "Epoch 13318: Training Loss: 0.09394754966100057 Validation Loss: 0.8160783648490906\n",
      "Epoch 13319: Training Loss: 0.09441044678290685 Validation Loss: 0.8171143531799316\n",
      "Epoch 13320: Training Loss: 0.09395543485879898 Validation Loss: 0.8165541887283325\n",
      "Epoch 13321: Training Loss: 0.09414127469062805 Validation Loss: 0.8158566951751709\n",
      "Epoch 13322: Training Loss: 0.093964122235775 Validation Loss: 0.8159039616584778\n",
      "Epoch 13323: Training Loss: 0.0945609596868356 Validation Loss: 0.8169291615486145\n",
      "Epoch 13324: Training Loss: 0.09390811870495479 Validation Loss: 0.8169260621070862\n",
      "Epoch 13325: Training Loss: 0.09385984142621358 Validation Loss: 0.8159946799278259\n",
      "Epoch 13326: Training Loss: 0.09387996047735214 Validation Loss: 0.8156037330627441\n",
      "Epoch 13327: Training Loss: 0.09386451045672099 Validation Loss: 0.816830575466156\n",
      "Epoch 13328: Training Loss: 0.0934521754582723 Validation Loss: 0.8170061111450195\n",
      "Epoch 13329: Training Loss: 0.09432272364695866 Validation Loss: 0.8165643811225891\n",
      "Epoch 13330: Training Loss: 0.09412667651971181 Validation Loss: 0.8164907097816467\n",
      "Epoch 13331: Training Loss: 0.09358206639687221 Validation Loss: 0.8167325258255005\n",
      "Epoch 13332: Training Loss: 0.09387531131505966 Validation Loss: 0.8158363699913025\n",
      "Epoch 13333: Training Loss: 0.09419861684242885 Validation Loss: 0.8164839148521423\n",
      "Epoch 13334: Training Loss: 0.09398702532052994 Validation Loss: 0.817777693271637\n",
      "Epoch 13335: Training Loss: 0.09388238688309987 Validation Loss: 0.8176811933517456\n",
      "Epoch 13336: Training Loss: 0.09378824383020401 Validation Loss: 0.8180150389671326\n",
      "Epoch 13337: Training Loss: 0.09433057407538097 Validation Loss: 0.8174366354942322\n",
      "Epoch 13338: Training Loss: 0.09376945346593857 Validation Loss: 0.8175865411758423\n",
      "Epoch 13339: Training Loss: 0.09412017961343129 Validation Loss: 0.817041277885437\n",
      "Epoch 13340: Training Loss: 0.0938167745868365 Validation Loss: 0.8165658712387085\n",
      "Epoch 13341: Training Loss: 0.09373849133650462 Validation Loss: 0.8169429898262024\n",
      "Epoch 13342: Training Loss: 0.09373397131760915 Validation Loss: 0.8173621892929077\n",
      "Epoch 13343: Training Loss: 0.09387482702732086 Validation Loss: 0.8169124722480774\n",
      "Epoch 13344: Training Loss: 0.0938489759961764 Validation Loss: 0.8172882795333862\n",
      "Epoch 13345: Training Loss: 0.09401778131723404 Validation Loss: 0.8162471055984497\n",
      "Epoch 13346: Training Loss: 0.09515435496966045 Validation Loss: 0.8149619698524475\n",
      "Epoch 13347: Training Loss: 0.09383024523655574 Validation Loss: 0.8162928223609924\n",
      "Epoch 13348: Training Loss: 0.09391252199808757 Validation Loss: 0.8175447583198547\n",
      "Epoch 13349: Training Loss: 0.09385875364144643 Validation Loss: 0.816657304763794\n",
      "Epoch 13350: Training Loss: 0.09377916405598323 Validation Loss: 0.816853940486908\n",
      "Epoch 13351: Training Loss: 0.09391860663890839 Validation Loss: 0.816932737827301\n",
      "Epoch 13352: Training Loss: 0.09367422014474869 Validation Loss: 0.8167545199394226\n",
      "Epoch 13353: Training Loss: 0.0938531110684077 Validation Loss: 0.8161996006965637\n",
      "Epoch 13354: Training Loss: 0.0939394732316335 Validation Loss: 0.816431999206543\n",
      "Epoch 13355: Training Loss: 0.09375109275182088 Validation Loss: 0.8181937336921692\n",
      "Epoch 13356: Training Loss: 0.09352775911490123 Validation Loss: 0.8178418278694153\n",
      "Epoch 13357: Training Loss: 0.09421037882566452 Validation Loss: 0.8174747824668884\n",
      "Epoch 13358: Training Loss: 0.09368332972129186 Validation Loss: 0.8179348111152649\n",
      "Epoch 13359: Training Loss: 0.09393063187599182 Validation Loss: 0.8169947862625122\n",
      "Epoch 13360: Training Loss: 0.09380324681599934 Validation Loss: 0.8159931898117065\n",
      "Epoch 13361: Training Loss: 0.09367925922075908 Validation Loss: 0.8156366348266602\n",
      "Epoch 13362: Training Loss: 0.09386702378590901 Validation Loss: 0.8174979090690613\n",
      "Epoch 13363: Training Loss: 0.09373973309993744 Validation Loss: 0.8174159526824951\n",
      "Epoch 13364: Training Loss: 0.09390068302551906 Validation Loss: 0.8171565532684326\n",
      "Epoch 13365: Training Loss: 0.0936730330189069 Validation Loss: 0.81741863489151\n",
      "Epoch 13366: Training Loss: 0.09354619433482488 Validation Loss: 0.8174076676368713\n",
      "Epoch 13367: Training Loss: 0.09360669801632564 Validation Loss: 0.8166561722755432\n",
      "Epoch 13368: Training Loss: 0.093964288632075 Validation Loss: 0.8167498111724854\n",
      "Epoch 13369: Training Loss: 0.0936448002854983 Validation Loss: 0.8168514966964722\n",
      "Epoch 13370: Training Loss: 0.09380920479695003 Validation Loss: 0.8175584673881531\n",
      "Epoch 13371: Training Loss: 0.09369200716416042 Validation Loss: 0.8182141780853271\n",
      "Epoch 13372: Training Loss: 0.09350239237149556 Validation Loss: 0.8174607157707214\n",
      "Epoch 13373: Training Loss: 0.09363517413536708 Validation Loss: 0.8175774216651917\n",
      "Epoch 13374: Training Loss: 0.09369311233361562 Validation Loss: 0.8175500631332397\n",
      "Epoch 13375: Training Loss: 0.09372857461373012 Validation Loss: 0.8179470896720886\n",
      "Epoch 13376: Training Loss: 0.09341731915871303 Validation Loss: 0.8173105716705322\n",
      "Epoch 13377: Training Loss: 0.09434569130341212 Validation Loss: 0.8166961669921875\n",
      "Epoch 13378: Training Loss: 0.09357507526874542 Validation Loss: 0.815788209438324\n",
      "Epoch 13379: Training Loss: 0.0937618687748909 Validation Loss: 0.8169830441474915\n",
      "Epoch 13380: Training Loss: 0.09378566592931747 Validation Loss: 0.8173133134841919\n",
      "Epoch 13381: Training Loss: 0.09363549202680588 Validation Loss: 0.8173331022262573\n",
      "Epoch 13382: Training Loss: 0.09406367937723796 Validation Loss: 0.8192174434661865\n",
      "Epoch 13383: Training Loss: 0.09391631931066513 Validation Loss: 0.8180997967720032\n",
      "Epoch 13384: Training Loss: 0.09455495824416478 Validation Loss: 0.8172125816345215\n",
      "Epoch 13385: Training Loss: 0.09363382558027904 Validation Loss: 0.8164458870887756\n",
      "Epoch 13386: Training Loss: 0.09388019392887752 Validation Loss: 0.8163586854934692\n",
      "Epoch 13387: Training Loss: 0.09373533974091212 Validation Loss: 0.8182613253593445\n",
      "Epoch 13388: Training Loss: 0.09344775726397832 Validation Loss: 0.818698525428772\n",
      "Epoch 13389: Training Loss: 0.09377795457839966 Validation Loss: 0.8181509375572205\n",
      "Epoch 13390: Training Loss: 0.0936114490032196 Validation Loss: 0.8166770935058594\n",
      "Epoch 13391: Training Loss: 0.093888724843661 Validation Loss: 0.8165464401245117\n",
      "Epoch 13392: Training Loss: 0.0942980299393336 Validation Loss: 0.8160266876220703\n",
      "Epoch 13393: Training Loss: 0.09348504493633907 Validation Loss: 0.8161351680755615\n",
      "Epoch 13394: Training Loss: 0.09368283053239186 Validation Loss: 0.8167114853858948\n",
      "Epoch 13395: Training Loss: 0.09373666842778523 Validation Loss: 0.8172115087509155\n",
      "Epoch 13396: Training Loss: 0.09371628363927205 Validation Loss: 0.8173010349273682\n",
      "Epoch 13397: Training Loss: 0.09355828414360683 Validation Loss: 0.8174752593040466\n",
      "Epoch 13398: Training Loss: 0.09357582529385884 Validation Loss: 0.8182658553123474\n",
      "Epoch 13399: Training Loss: 0.09409211824337642 Validation Loss: 0.8189061284065247\n",
      "Epoch 13400: Training Loss: 0.09344212959210078 Validation Loss: 0.8182979226112366\n",
      "Epoch 13401: Training Loss: 0.09377370526393254 Validation Loss: 0.8172550797462463\n",
      "Epoch 13402: Training Loss: 0.09345921625693639 Validation Loss: 0.8174203038215637\n",
      "Epoch 13403: Training Loss: 0.09345534443855286 Validation Loss: 0.8169175386428833\n",
      "Epoch 13404: Training Loss: 0.0935204749306043 Validation Loss: 0.8174178600311279\n",
      "Epoch 13405: Training Loss: 0.09350985288619995 Validation Loss: 0.8173925876617432\n",
      "Epoch 13406: Training Loss: 0.09363375107447307 Validation Loss: 0.8184270858764648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13407: Training Loss: 0.09346272548039754 Validation Loss: 0.8181930780410767\n",
      "Epoch 13408: Training Loss: 0.09349078685045242 Validation Loss: 0.8169925212860107\n",
      "Epoch 13409: Training Loss: 0.09449150413274765 Validation Loss: 0.8169270753860474\n",
      "Epoch 13410: Training Loss: 0.09351491679747899 Validation Loss: 0.8173430562019348\n",
      "Epoch 13411: Training Loss: 0.09351539115111034 Validation Loss: 0.8180926442146301\n",
      "Epoch 13412: Training Loss: 0.09329894681771596 Validation Loss: 0.8188284039497375\n",
      "Epoch 13413: Training Loss: 0.09358683228492737 Validation Loss: 0.8192444443702698\n",
      "Epoch 13414: Training Loss: 0.09364009151856105 Validation Loss: 0.8188758492469788\n",
      "Epoch 13415: Training Loss: 0.09356700877348582 Validation Loss: 0.8170450925827026\n",
      "Epoch 13416: Training Loss: 0.09348476926485698 Validation Loss: 0.8176523447036743\n",
      "Epoch 13417: Training Loss: 0.0935554951429367 Validation Loss: 0.8166700005531311\n",
      "Epoch 13418: Training Loss: 0.09384760012229283 Validation Loss: 0.8166652917861938\n",
      "Epoch 13419: Training Loss: 0.09382783124844234 Validation Loss: 0.8184483647346497\n",
      "Epoch 13420: Training Loss: 0.09389362235864003 Validation Loss: 0.8190696835517883\n",
      "Epoch 13421: Training Loss: 0.09339963396390279 Validation Loss: 0.8184945583343506\n",
      "Epoch 13422: Training Loss: 0.09380050996939342 Validation Loss: 0.8174395561218262\n",
      "Epoch 13423: Training Loss: 0.09347647180159886 Validation Loss: 0.8165844678878784\n",
      "Epoch 13424: Training Loss: 0.0935027947028478 Validation Loss: 0.8167068362236023\n",
      "Epoch 13425: Training Loss: 0.09397718558708827 Validation Loss: 0.8172115087509155\n",
      "Epoch 13426: Training Loss: 0.09387996296087901 Validation Loss: 0.8174598217010498\n",
      "Epoch 13427: Training Loss: 0.09352266043424606 Validation Loss: 0.8184623122215271\n",
      "Epoch 13428: Training Loss: 0.09353615591923396 Validation Loss: 0.8195459842681885\n",
      "Epoch 13429: Training Loss: 0.09360839923222859 Validation Loss: 0.8180881142616272\n",
      "Epoch 13430: Training Loss: 0.09364035973946254 Validation Loss: 0.8176479935646057\n",
      "Epoch 13431: Training Loss: 0.0933952455719312 Validation Loss: 0.8170996308326721\n",
      "Epoch 13432: Training Loss: 0.09385959059000015 Validation Loss: 0.8162270188331604\n",
      "Epoch 13433: Training Loss: 0.09344935168822606 Validation Loss: 0.8167824149131775\n",
      "Epoch 13434: Training Loss: 0.09354294588168462 Validation Loss: 0.8179600834846497\n",
      "Epoch 13435: Training Loss: 0.09356022377808888 Validation Loss: 0.819322943687439\n",
      "Epoch 13436: Training Loss: 0.09370427081982295 Validation Loss: 0.8191044330596924\n",
      "Epoch 13437: Training Loss: 0.09349917123715083 Validation Loss: 0.8174442052841187\n",
      "Epoch 13438: Training Loss: 0.09332503130038579 Validation Loss: 0.8169037699699402\n",
      "Epoch 13439: Training Loss: 0.09387974192698796 Validation Loss: 0.8171816468238831\n",
      "Epoch 13440: Training Loss: 0.09407681971788406 Validation Loss: 0.8191477656364441\n",
      "Epoch 13441: Training Loss: 0.0934753989179929 Validation Loss: 0.8196297287940979\n",
      "Epoch 13442: Training Loss: 0.09301029145717621 Validation Loss: 0.8185204863548279\n",
      "Epoch 13443: Training Loss: 0.09346008052428563 Validation Loss: 0.8184814453125\n",
      "Epoch 13444: Training Loss: 0.09371742606163025 Validation Loss: 0.8180927634239197\n",
      "Epoch 13445: Training Loss: 0.09348669151465099 Validation Loss: 0.8169217109680176\n",
      "Epoch 13446: Training Loss: 0.09386095652977626 Validation Loss: 0.816461980342865\n",
      "Epoch 13447: Training Loss: 0.09339886158704758 Validation Loss: 0.8176717162132263\n",
      "Epoch 13448: Training Loss: 0.09348206222057343 Validation Loss: 0.8185848593711853\n",
      "Epoch 13449: Training Loss: 0.09340972950061162 Validation Loss: 0.8189263343811035\n",
      "Epoch 13450: Training Loss: 0.09352995951970418 Validation Loss: 0.8184024095535278\n",
      "Epoch 13451: Training Loss: 0.09359854459762573 Validation Loss: 0.8175424337387085\n",
      "Epoch 13452: Training Loss: 0.09334354847669601 Validation Loss: 0.8175294399261475\n",
      "Epoch 13453: Training Loss: 0.09338157624006271 Validation Loss: 0.8178920149803162\n",
      "Epoch 13454: Training Loss: 0.09323129554589589 Validation Loss: 0.8178091049194336\n",
      "Epoch 13455: Training Loss: 0.09360254059235255 Validation Loss: 0.8194127678871155\n",
      "Epoch 13456: Training Loss: 0.09327013045549393 Validation Loss: 0.8184435367584229\n",
      "Epoch 13457: Training Loss: 0.09340386092662811 Validation Loss: 0.8186467885971069\n",
      "Epoch 13458: Training Loss: 0.09382000813881557 Validation Loss: 0.8183591961860657\n",
      "Epoch 13459: Training Loss: 0.09402293215195338 Validation Loss: 0.8172860741615295\n",
      "Epoch 13460: Training Loss: 0.09321744243303935 Validation Loss: 0.817409873008728\n",
      "Epoch 13461: Training Loss: 0.0933549553155899 Validation Loss: 0.8180874586105347\n",
      "Epoch 13462: Training Loss: 0.0933188150326411 Validation Loss: 0.8181803226470947\n",
      "Epoch 13463: Training Loss: 0.09347108503182729 Validation Loss: 0.8190430998802185\n",
      "Epoch 13464: Training Loss: 0.09325605879227321 Validation Loss: 0.8190193772315979\n",
      "Epoch 13465: Training Loss: 0.09319222221771876 Validation Loss: 0.8187590837478638\n",
      "Epoch 13466: Training Loss: 0.09328017632166545 Validation Loss: 0.8186882138252258\n",
      "Epoch 13467: Training Loss: 0.09336164096991222 Validation Loss: 0.8177831768989563\n",
      "Epoch 13468: Training Loss: 0.09339111546675365 Validation Loss: 0.8189934492111206\n",
      "Epoch 13469: Training Loss: 0.0932190145055453 Validation Loss: 0.818671703338623\n",
      "Epoch 13470: Training Loss: 0.0927022248506546 Validation Loss: 0.8182613849639893\n",
      "Epoch 13471: Training Loss: 0.09299346307913463 Validation Loss: 0.8183757662773132\n",
      "Epoch 13472: Training Loss: 0.09318371117115021 Validation Loss: 0.8180135488510132\n",
      "Epoch 13473: Training Loss: 0.09328950196504593 Validation Loss: 0.8181728720664978\n",
      "Epoch 13474: Training Loss: 0.09330187489589055 Validation Loss: 0.8190011382102966\n",
      "Epoch 13475: Training Loss: 0.09331236034631729 Validation Loss: 0.819537341594696\n",
      "Epoch 13476: Training Loss: 0.09333652257919312 Validation Loss: 0.8180816173553467\n",
      "Epoch 13477: Training Loss: 0.09324063857396443 Validation Loss: 0.8173221945762634\n",
      "Epoch 13478: Training Loss: 0.09320206195116043 Validation Loss: 0.8178911805152893\n",
      "Epoch 13479: Training Loss: 0.09270808845758438 Validation Loss: 0.8182689547538757\n",
      "Epoch 13480: Training Loss: 0.09328110267718633 Validation Loss: 0.8180176615715027\n",
      "Epoch 13481: Training Loss: 0.09349045405785243 Validation Loss: 0.8191781044006348\n",
      "Epoch 13482: Training Loss: 0.09324787060419719 Validation Loss: 0.8187860250473022\n",
      "Epoch 13483: Training Loss: 0.0931380068262418 Validation Loss: 0.8182562589645386\n",
      "Epoch 13484: Training Loss: 0.09352743128935496 Validation Loss: 0.8174606561660767\n",
      "Epoch 13485: Training Loss: 0.09318337837855022 Validation Loss: 0.8169139623641968\n",
      "Epoch 13486: Training Loss: 0.09315533190965652 Validation Loss: 0.8180029392242432\n",
      "Epoch 13487: Training Loss: 0.09349269916613896 Validation Loss: 0.8185111880302429\n",
      "Epoch 13488: Training Loss: 0.09324591358502705 Validation Loss: 0.8182072043418884\n",
      "Epoch 13489: Training Loss: 0.09322121739387512 Validation Loss: 0.8184020519256592\n",
      "Epoch 13490: Training Loss: 0.09280815223852794 Validation Loss: 0.8191726803779602\n",
      "Epoch 13491: Training Loss: 0.09338836868604024 Validation Loss: 0.8196439743041992\n",
      "Epoch 13492: Training Loss: 0.09326171626647313 Validation Loss: 0.8193517327308655\n",
      "Epoch 13493: Training Loss: 0.09336868673563004 Validation Loss: 0.8182950615882874\n",
      "Epoch 13494: Training Loss: 0.09329699228207271 Validation Loss: 0.8189125657081604\n",
      "Epoch 13495: Training Loss: 0.09316587448120117 Validation Loss: 0.8189067840576172\n",
      "Epoch 13496: Training Loss: 0.0932147850592931 Validation Loss: 0.8185909986495972\n",
      "Epoch 13497: Training Loss: 0.09316286196311314 Validation Loss: 0.8178606033325195\n",
      "Epoch 13498: Training Loss: 0.09268258512020111 Validation Loss: 0.8183227777481079\n",
      "Epoch 13499: Training Loss: 0.09307413796583812 Validation Loss: 0.8191524147987366\n",
      "Epoch 13500: Training Loss: 0.09308321525653203 Validation Loss: 0.8194870948791504\n",
      "Epoch 13501: Training Loss: 0.09322470674912135 Validation Loss: 0.818389356136322\n",
      "Epoch 13502: Training Loss: 0.09397672116756439 Validation Loss: 0.8173866271972656\n",
      "Epoch 13503: Training Loss: 0.09318257371584575 Validation Loss: 0.8163670301437378\n",
      "Epoch 13504: Training Loss: 0.09325127551952998 Validation Loss: 0.8166856169700623\n",
      "Epoch 13505: Training Loss: 0.09331037600835164 Validation Loss: 0.8181612491607666\n",
      "Epoch 13506: Training Loss: 0.0931922843058904 Validation Loss: 0.8189549446105957\n",
      "Epoch 13507: Training Loss: 0.09340064227581024 Validation Loss: 0.8201136589050293\n",
      "Epoch 13508: Training Loss: 0.09302204350630443 Validation Loss: 0.820152223110199\n",
      "Epoch 13509: Training Loss: 0.09326541423797607 Validation Loss: 0.8188670873641968\n",
      "Epoch 13510: Training Loss: 0.09337885181109111 Validation Loss: 0.818660318851471\n",
      "Epoch 13511: Training Loss: 0.09315549582242966 Validation Loss: 0.8188478350639343\n",
      "Epoch 13512: Training Loss: 0.0934492548306783 Validation Loss: 0.8187593221664429\n",
      "Epoch 13513: Training Loss: 0.09321641673644383 Validation Loss: 0.8193777799606323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13514: Training Loss: 0.09395242234071095 Validation Loss: 0.8197667598724365\n",
      "Epoch 13515: Training Loss: 0.09332939734061559 Validation Loss: 0.8184890747070312\n",
      "Epoch 13516: Training Loss: 0.0929769625266393 Validation Loss: 0.8179588913917542\n",
      "Epoch 13517: Training Loss: 0.09319866448640823 Validation Loss: 0.8189314603805542\n",
      "Epoch 13518: Training Loss: 0.09305975337823232 Validation Loss: 0.8192518949508667\n",
      "Epoch 13519: Training Loss: 0.09322136888901393 Validation Loss: 0.8199828267097473\n",
      "Epoch 13520: Training Loss: 0.09331473211447398 Validation Loss: 0.8194844126701355\n",
      "Epoch 13521: Training Loss: 0.09337092439333598 Validation Loss: 0.8182414174079895\n",
      "Epoch 13522: Training Loss: 0.09304622064034145 Validation Loss: 0.818574070930481\n",
      "Epoch 13523: Training Loss: 0.0933946743607521 Validation Loss: 0.8184628486633301\n",
      "Epoch 13524: Training Loss: 0.09316553175449371 Validation Loss: 0.8191575407981873\n",
      "Epoch 13525: Training Loss: 0.09306527922550838 Validation Loss: 0.8193522095680237\n",
      "Epoch 13526: Training Loss: 0.09360483288764954 Validation Loss: 0.8187456727027893\n",
      "Epoch 13527: Training Loss: 0.09306423862775166 Validation Loss: 0.8198105692863464\n",
      "Epoch 13528: Training Loss: 0.09335619459549586 Validation Loss: 0.8197251558303833\n",
      "Epoch 13529: Training Loss: 0.09333669890960057 Validation Loss: 0.8200345635414124\n",
      "Epoch 13530: Training Loss: 0.09390738109747569 Validation Loss: 0.819904088973999\n",
      "Epoch 13531: Training Loss: 0.09328317642211914 Validation Loss: 0.8194862604141235\n",
      "Epoch 13532: Training Loss: 0.09291343887646993 Validation Loss: 0.8195285201072693\n",
      "Epoch 13533: Training Loss: 0.09396627297004063 Validation Loss: 0.8195009827613831\n",
      "Epoch 13534: Training Loss: 0.0935820738474528 Validation Loss: 0.8192000389099121\n",
      "Epoch 13535: Training Loss: 0.09316600610812505 Validation Loss: 0.818528413772583\n",
      "Epoch 13536: Training Loss: 0.09307131667931874 Validation Loss: 0.8178825378417969\n",
      "Epoch 13537: Training Loss: 0.09328232953945796 Validation Loss: 0.8177635669708252\n",
      "Epoch 13538: Training Loss: 0.09325237323840459 Validation Loss: 0.8175493478775024\n",
      "Epoch 13539: Training Loss: 0.09295280029376347 Validation Loss: 0.819355845451355\n",
      "Epoch 13540: Training Loss: 0.09297628700733185 Validation Loss: 0.8191746473312378\n",
      "Epoch 13541: Training Loss: 0.09315388898054759 Validation Loss: 0.8184698224067688\n",
      "Epoch 13542: Training Loss: 0.09311592827240626 Validation Loss: 0.8181025981903076\n",
      "Epoch 13543: Training Loss: 0.09324989219506581 Validation Loss: 0.8187999725341797\n",
      "Epoch 13544: Training Loss: 0.09298495203256607 Validation Loss: 0.819419801235199\n",
      "Epoch 13545: Training Loss: 0.09287129342556 Validation Loss: 0.8190286159515381\n",
      "Epoch 13546: Training Loss: 0.09358765681584676 Validation Loss: 0.819694459438324\n",
      "Epoch 13547: Training Loss: 0.09347931295633316 Validation Loss: 0.8207836747169495\n",
      "Epoch 13548: Training Loss: 0.09270558754603068 Validation Loss: 0.8199050426483154\n",
      "Epoch 13549: Training Loss: 0.09313986450433731 Validation Loss: 0.8196070790290833\n",
      "Epoch 13550: Training Loss: 0.09309077262878418 Validation Loss: 0.8200573921203613\n",
      "Epoch 13551: Training Loss: 0.09265164782603581 Validation Loss: 0.8196244239807129\n",
      "Epoch 13552: Training Loss: 0.09354060391585033 Validation Loss: 0.8184586763381958\n",
      "Epoch 13553: Training Loss: 0.09300077954928081 Validation Loss: 0.8191219568252563\n",
      "Epoch 13554: Training Loss: 0.0928852582971255 Validation Loss: 0.8194116353988647\n",
      "Epoch 13555: Training Loss: 0.09299290925264359 Validation Loss: 0.8194878697395325\n",
      "Epoch 13556: Training Loss: 0.09285705536603928 Validation Loss: 0.8194679021835327\n",
      "Epoch 13557: Training Loss: 0.09294869502385457 Validation Loss: 0.8192340731620789\n",
      "Epoch 13558: Training Loss: 0.09323890010515849 Validation Loss: 0.8194913268089294\n",
      "Epoch 13559: Training Loss: 0.09301310529311498 Validation Loss: 0.8197742104530334\n",
      "Epoch 13560: Training Loss: 0.0930912693341573 Validation Loss: 0.818821907043457\n",
      "Epoch 13561: Training Loss: 0.09298527240753174 Validation Loss: 0.819360613822937\n",
      "Epoch 13562: Training Loss: 0.09326117237408955 Validation Loss: 0.8184821009635925\n",
      "Epoch 13563: Training Loss: 0.09293757379055023 Validation Loss: 0.8186269402503967\n",
      "Epoch 13564: Training Loss: 0.09404465556144714 Validation Loss: 0.8183726668357849\n",
      "Epoch 13565: Training Loss: 0.09332175056139629 Validation Loss: 0.8176908493041992\n",
      "Epoch 13566: Training Loss: 0.09268510341644287 Validation Loss: 0.8189014792442322\n",
      "Epoch 13567: Training Loss: 0.09298016875982285 Validation Loss: 0.8201751708984375\n",
      "Epoch 13568: Training Loss: 0.09293956061204274 Validation Loss: 0.819346010684967\n",
      "Epoch 13569: Training Loss: 0.09329442928234737 Validation Loss: 0.818865180015564\n",
      "Epoch 13570: Training Loss: 0.09291420876979828 Validation Loss: 0.819147527217865\n",
      "Epoch 13571: Training Loss: 0.09312489628791809 Validation Loss: 0.8195634484291077\n",
      "Epoch 13572: Training Loss: 0.09329823404550552 Validation Loss: 0.8204928636550903\n",
      "Epoch 13573: Training Loss: 0.09289851288000743 Validation Loss: 0.8191940784454346\n",
      "Epoch 13574: Training Loss: 0.09274373203516006 Validation Loss: 0.8191965818405151\n",
      "Epoch 13575: Training Loss: 0.09346519162257512 Validation Loss: 0.8188852071762085\n",
      "Epoch 13576: Training Loss: 0.09322065611680348 Validation Loss: 0.818259060382843\n",
      "Epoch 13577: Training Loss: 0.09309192498524983 Validation Loss: 0.8189601898193359\n",
      "Epoch 13578: Training Loss: 0.09306729584932327 Validation Loss: 0.8198627233505249\n",
      "Epoch 13579: Training Loss: 0.09325952331225078 Validation Loss: 0.8199646472930908\n",
      "Epoch 13580: Training Loss: 0.09289885560671489 Validation Loss: 0.8197335600852966\n",
      "Epoch 13581: Training Loss: 0.09288728733857472 Validation Loss: 0.8192071914672852\n",
      "Epoch 13582: Training Loss: 0.09294263273477554 Validation Loss: 0.8195940256118774\n",
      "Epoch 13583: Training Loss: 0.09319354842106502 Validation Loss: 0.8202534914016724\n",
      "Epoch 13584: Training Loss: 0.09258507192134857 Validation Loss: 0.820289671421051\n",
      "Epoch 13585: Training Loss: 0.09324700385332108 Validation Loss: 0.8205335140228271\n",
      "Epoch 13586: Training Loss: 0.09290743867556255 Validation Loss: 0.8188971281051636\n",
      "Epoch 13587: Training Loss: 0.09308486183484395 Validation Loss: 0.8183820843696594\n",
      "Epoch 13588: Training Loss: 0.09284502764542897 Validation Loss: 0.8188105821609497\n",
      "Epoch 13589: Training Loss: 0.09317372739315033 Validation Loss: 0.8195375204086304\n",
      "Epoch 13590: Training Loss: 0.0931352029244105 Validation Loss: 0.8204472064971924\n",
      "Epoch 13591: Training Loss: 0.09305515885353088 Validation Loss: 0.8207927346229553\n",
      "Epoch 13592: Training Loss: 0.09292320907115936 Validation Loss: 0.8206338286399841\n",
      "Epoch 13593: Training Loss: 0.09300812582174937 Validation Loss: 0.8203399181365967\n",
      "Epoch 13594: Training Loss: 0.09269830336173375 Validation Loss: 0.820186197757721\n",
      "Epoch 13595: Training Loss: 0.09303013980388641 Validation Loss: 0.820276141166687\n",
      "Epoch 13596: Training Loss: 0.09290134410063426 Validation Loss: 0.8203842043876648\n",
      "Epoch 13597: Training Loss: 0.092988816400369 Validation Loss: 0.819404125213623\n",
      "Epoch 13598: Training Loss: 0.09298502653837204 Validation Loss: 0.8182761073112488\n",
      "Epoch 13599: Training Loss: 0.092804121474425 Validation Loss: 0.8185130953788757\n",
      "Epoch 13600: Training Loss: 0.09290021161238353 Validation Loss: 0.818294107913971\n",
      "Epoch 13601: Training Loss: 0.09348065654436748 Validation Loss: 0.8193633556365967\n",
      "Epoch 13602: Training Loss: 0.09290079275767009 Validation Loss: 0.8197153806686401\n",
      "Epoch 13603: Training Loss: 0.092979001502196 Validation Loss: 0.8197519779205322\n",
      "Epoch 13604: Training Loss: 0.09283068527777989 Validation Loss: 0.820842981338501\n",
      "Epoch 13605: Training Loss: 0.09286565830310185 Validation Loss: 0.820989727973938\n",
      "Epoch 13606: Training Loss: 0.09286295125881831 Validation Loss: 0.820817232131958\n",
      "Epoch 13607: Training Loss: 0.09283016870419185 Validation Loss: 0.8197471499443054\n",
      "Epoch 13608: Training Loss: 0.09308404475450516 Validation Loss: 0.8205353617668152\n",
      "Epoch 13609: Training Loss: 0.09295470267534256 Validation Loss: 0.8213029503822327\n",
      "Epoch 13610: Training Loss: 0.09351062029600143 Validation Loss: 0.8207380175590515\n",
      "Epoch 13611: Training Loss: 0.09352488567431767 Validation Loss: 0.8205454349517822\n",
      "Epoch 13612: Training Loss: 0.09301488598187764 Validation Loss: 0.819898247718811\n",
      "Epoch 13613: Training Loss: 0.09296100089947383 Validation Loss: 0.8208928108215332\n",
      "Epoch 13614: Training Loss: 0.09418243914842606 Validation Loss: 0.8197973966598511\n",
      "Epoch 13615: Training Loss: 0.09286372115214665 Validation Loss: 0.8205534219741821\n",
      "Epoch 13616: Training Loss: 0.09304112692674 Validation Loss: 0.8198271989822388\n",
      "Epoch 13617: Training Loss: 0.09268234421809514 Validation Loss: 0.8185604214668274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13618: Training Loss: 0.09339548150698344 Validation Loss: 0.8189504146575928\n",
      "Epoch 13619: Training Loss: 0.09273318946361542 Validation Loss: 0.8187637329101562\n",
      "Epoch 13620: Training Loss: 0.09247400611639023 Validation Loss: 0.81947922706604\n",
      "Epoch 13621: Training Loss: 0.09278278797864914 Validation Loss: 0.8201618790626526\n",
      "Epoch 13622: Training Loss: 0.09286314249038696 Validation Loss: 0.8195278644561768\n",
      "Epoch 13623: Training Loss: 0.09313544134298961 Validation Loss: 0.8194870948791504\n",
      "Epoch 13624: Training Loss: 0.09249022354682286 Validation Loss: 0.819802463054657\n",
      "Epoch 13625: Training Loss: 0.09268724918365479 Validation Loss: 0.8211514949798584\n",
      "Epoch 13626: Training Loss: 0.09275888899962108 Validation Loss: 0.8208269476890564\n",
      "Epoch 13627: Training Loss: 0.09307269006967545 Validation Loss: 0.8205105662345886\n",
      "Epoch 13628: Training Loss: 0.0933026671409607 Validation Loss: 0.8202940225601196\n",
      "Epoch 13629: Training Loss: 0.09309119482835133 Validation Loss: 0.8205006122589111\n",
      "Epoch 13630: Training Loss: 0.09270595759153366 Validation Loss: 0.8198049664497375\n",
      "Epoch 13631: Training Loss: 0.09333839764197667 Validation Loss: 0.8203876614570618\n",
      "Epoch 13632: Training Loss: 0.09284493823846181 Validation Loss: 0.8204337358474731\n",
      "Epoch 13633: Training Loss: 0.09246211002270381 Validation Loss: 0.8206079006195068\n",
      "Epoch 13634: Training Loss: 0.09268078456322353 Validation Loss: 0.8204014897346497\n",
      "Epoch 13635: Training Loss: 0.09291947881380717 Validation Loss: 0.8205796480178833\n",
      "Epoch 13636: Training Loss: 0.09243225306272507 Validation Loss: 0.8195487856864929\n",
      "Epoch 13637: Training Loss: 0.09268637498219807 Validation Loss: 0.8190633654594421\n",
      "Epoch 13638: Training Loss: 0.09277269492546718 Validation Loss: 0.8198176622390747\n",
      "Epoch 13639: Training Loss: 0.09283915162086487 Validation Loss: 0.8198575377464294\n",
      "Epoch 13640: Training Loss: 0.09275333831707637 Validation Loss: 0.8196755647659302\n",
      "Epoch 13641: Training Loss: 0.09309477855761845 Validation Loss: 0.8208191394805908\n",
      "Epoch 13642: Training Loss: 0.09267554183801015 Validation Loss: 0.8214406967163086\n",
      "Epoch 13643: Training Loss: 0.093476968506972 Validation Loss: 0.8223116397857666\n",
      "Epoch 13644: Training Loss: 0.09296532720327377 Validation Loss: 0.8216614127159119\n",
      "Epoch 13645: Training Loss: 0.09313805152972539 Validation Loss: 0.8193792700767517\n",
      "Epoch 13646: Training Loss: 0.09325232356786728 Validation Loss: 0.8194179534912109\n",
      "Epoch 13647: Training Loss: 0.09291257212559383 Validation Loss: 0.8202720880508423\n",
      "Epoch 13648: Training Loss: 0.09308208773533504 Validation Loss: 0.8203381896018982\n",
      "Epoch 13649: Training Loss: 0.09275096406539281 Validation Loss: 0.8198840022087097\n",
      "Epoch 13650: Training Loss: 0.09302852551142375 Validation Loss: 0.8191741704940796\n",
      "Epoch 13651: Training Loss: 0.0925624668598175 Validation Loss: 0.8204821944236755\n",
      "Epoch 13652: Training Loss: 0.09244864185651143 Validation Loss: 0.8209699988365173\n",
      "Epoch 13653: Training Loss: 0.09293066958586375 Validation Loss: 0.8206468224525452\n",
      "Epoch 13654: Training Loss: 0.09278277059396108 Validation Loss: 0.8211634755134583\n",
      "Epoch 13655: Training Loss: 0.09282039354244868 Validation Loss: 0.8197066187858582\n",
      "Epoch 13656: Training Loss: 0.09302264948685963 Validation Loss: 0.8200603127479553\n",
      "Epoch 13657: Training Loss: 0.09262972076733907 Validation Loss: 0.8193323016166687\n",
      "Epoch 13658: Training Loss: 0.09295412649710973 Validation Loss: 0.8206709027290344\n",
      "Epoch 13659: Training Loss: 0.09261151651541392 Validation Loss: 0.8218482732772827\n",
      "Epoch 13660: Training Loss: 0.09333571543296178 Validation Loss: 0.8217682242393494\n",
      "Epoch 13661: Training Loss: 0.09239344795544942 Validation Loss: 0.8216290473937988\n",
      "Epoch 13662: Training Loss: 0.09269643574953079 Validation Loss: 0.820669412612915\n",
      "Epoch 13663: Training Loss: 0.09279921402533849 Validation Loss: 0.8213338255882263\n",
      "Epoch 13664: Training Loss: 0.09261070936918259 Validation Loss: 0.8209291100502014\n",
      "Epoch 13665: Training Loss: 0.09278309096892674 Validation Loss: 0.8215895891189575\n",
      "Epoch 13666: Training Loss: 0.09341912219921748 Validation Loss: 0.8203303813934326\n",
      "Epoch 13667: Training Loss: 0.09295391043027242 Validation Loss: 0.8211948275566101\n",
      "Epoch 13668: Training Loss: 0.09256989757219951 Validation Loss: 0.8212262392044067\n",
      "Epoch 13669: Training Loss: 0.09269388020038605 Validation Loss: 0.8211547136306763\n",
      "Epoch 13670: Training Loss: 0.09272933254639308 Validation Loss: 0.8208305239677429\n",
      "Epoch 13671: Training Loss: 0.09271400421857834 Validation Loss: 0.8213847279548645\n",
      "Epoch 13672: Training Loss: 0.09271620213985443 Validation Loss: 0.8206915855407715\n",
      "Epoch 13673: Training Loss: 0.09279550611972809 Validation Loss: 0.820738673210144\n",
      "Epoch 13674: Training Loss: 0.09237055728832881 Validation Loss: 0.8201239705085754\n",
      "Epoch 13675: Training Loss: 0.09299529095490773 Validation Loss: 0.8205851316452026\n",
      "Epoch 13676: Training Loss: 0.09278156359990437 Validation Loss: 0.8214254379272461\n",
      "Epoch 13677: Training Loss: 0.0927682916323344 Validation Loss: 0.8208571672439575\n",
      "Epoch 13678: Training Loss: 0.09275123725334804 Validation Loss: 0.8205733895301819\n",
      "Epoch 13679: Training Loss: 0.09280890723069508 Validation Loss: 0.8203340172767639\n",
      "Epoch 13680: Training Loss: 0.0930339793364207 Validation Loss: 0.8202329277992249\n",
      "Epoch 13681: Training Loss: 0.09286316484212875 Validation Loss: 0.820381760597229\n",
      "Epoch 13682: Training Loss: 0.09273467461268108 Validation Loss: 0.8218273520469666\n",
      "Epoch 13683: Training Loss: 0.09257530669371287 Validation Loss: 0.822287380695343\n",
      "Epoch 13684: Training Loss: 0.09218483169873555 Validation Loss: 0.8219015002250671\n",
      "Epoch 13685: Training Loss: 0.09261711438496907 Validation Loss: 0.8216351270675659\n",
      "Epoch 13686: Training Loss: 0.09276267389456432 Validation Loss: 0.8205193877220154\n",
      "Epoch 13687: Training Loss: 0.09283353885014851 Validation Loss: 0.8205379843711853\n",
      "Epoch 13688: Training Loss: 0.0925309881567955 Validation Loss: 0.8203807473182678\n",
      "Epoch 13689: Training Loss: 0.09279266496499379 Validation Loss: 0.8201947212219238\n",
      "Epoch 13690: Training Loss: 0.09246625006198883 Validation Loss: 0.8214676976203918\n",
      "Epoch 13691: Training Loss: 0.09247814367214839 Validation Loss: 0.822036862373352\n",
      "Epoch 13692: Training Loss: 0.09252617756525676 Validation Loss: 0.8216286897659302\n",
      "Epoch 13693: Training Loss: 0.09260843445857365 Validation Loss: 0.8216678500175476\n",
      "Epoch 13694: Training Loss: 0.09243424236774445 Validation Loss: 0.8209115266799927\n",
      "Epoch 13695: Training Loss: 0.0925826405485471 Validation Loss: 0.8211147785186768\n",
      "Epoch 13696: Training Loss: 0.09302712231874466 Validation Loss: 0.8211614489555359\n",
      "Epoch 13697: Training Loss: 0.09236416469017665 Validation Loss: 0.8214207887649536\n",
      "Epoch 13698: Training Loss: 0.09248106429974239 Validation Loss: 0.8210585117340088\n",
      "Epoch 13699: Training Loss: 0.09259399026632309 Validation Loss: 0.820966362953186\n",
      "Epoch 13700: Training Loss: 0.09252477685610454 Validation Loss: 0.8212790489196777\n",
      "Epoch 13701: Training Loss: 0.09267557909091313 Validation Loss: 0.820441484451294\n",
      "Epoch 13702: Training Loss: 0.09226212898890178 Validation Loss: 0.8219320774078369\n",
      "Epoch 13703: Training Loss: 0.09235870589812596 Validation Loss: 0.8217906355857849\n",
      "Epoch 13704: Training Loss: 0.09300953646500905 Validation Loss: 0.8218532204627991\n",
      "Epoch 13705: Training Loss: 0.09257655839125316 Validation Loss: 0.82167649269104\n",
      "Epoch 13706: Training Loss: 0.09247530500094096 Validation Loss: 0.8196396231651306\n",
      "Epoch 13707: Training Loss: 0.09258936097224553 Validation Loss: 0.8190521001815796\n",
      "Epoch 13708: Training Loss: 0.09243336071570714 Validation Loss: 0.820705235004425\n",
      "Epoch 13709: Training Loss: 0.09319694836934407 Validation Loss: 0.8204210996627808\n",
      "Epoch 13710: Training Loss: 0.09304040918747584 Validation Loss: 0.8217000961303711\n",
      "Epoch 13711: Training Loss: 0.09241356203953426 Validation Loss: 0.8219819664955139\n",
      "Epoch 13712: Training Loss: 0.09266745795806249 Validation Loss: 0.8223761916160583\n",
      "Epoch 13713: Training Loss: 0.09237070133288701 Validation Loss: 0.8217806220054626\n",
      "Epoch 13714: Training Loss: 0.09262784322102864 Validation Loss: 0.8207662105560303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13715: Training Loss: 0.09388553599516551 Validation Loss: 0.8215946555137634\n",
      "Epoch 13716: Training Loss: 0.0927243505915006 Validation Loss: 0.8211005330085754\n",
      "Epoch 13717: Training Loss: 0.09252573549747467 Validation Loss: 0.8210245966911316\n",
      "Epoch 13718: Training Loss: 0.09262492010990779 Validation Loss: 0.8212908506393433\n",
      "Epoch 13719: Training Loss: 0.09229495127995808 Validation Loss: 0.821538507938385\n",
      "Epoch 13720: Training Loss: 0.09339487552642822 Validation Loss: 0.8224942684173584\n",
      "Epoch 13721: Training Loss: 0.09239762773116429 Validation Loss: 0.8214415311813354\n",
      "Epoch 13722: Training Loss: 0.09271846463282903 Validation Loss: 0.8202580213546753\n",
      "Epoch 13723: Training Loss: 0.09282300372918446 Validation Loss: 0.8202900886535645\n",
      "Epoch 13724: Training Loss: 0.09252137194077174 Validation Loss: 0.8196603059768677\n",
      "Epoch 13725: Training Loss: 0.09298252562681834 Validation Loss: 0.8202176094055176\n",
      "Epoch 13726: Training Loss: 0.09238370756308238 Validation Loss: 0.8216639161109924\n",
      "Epoch 13727: Training Loss: 0.09258325894673665 Validation Loss: 0.821574866771698\n",
      "Epoch 13728: Training Loss: 0.09247142573197682 Validation Loss: 0.8215427994728088\n",
      "Epoch 13729: Training Loss: 0.09242880096038182 Validation Loss: 0.8210445642471313\n",
      "Epoch 13730: Training Loss: 0.09246976673603058 Validation Loss: 0.8220872282981873\n",
      "Epoch 13731: Training Loss: 0.09256064643462499 Validation Loss: 0.8224245309829712\n",
      "Epoch 13732: Training Loss: 0.09253027786811192 Validation Loss: 0.8215007781982422\n",
      "Epoch 13733: Training Loss: 0.09243017435073853 Validation Loss: 0.8211424350738525\n",
      "Epoch 13734: Training Loss: 0.09241821368535359 Validation Loss: 0.8204506635665894\n",
      "Epoch 13735: Training Loss: 0.09241353223721187 Validation Loss: 0.8200473785400391\n",
      "Epoch 13736: Training Loss: 0.09260182827711105 Validation Loss: 0.8210763335227966\n",
      "Epoch 13737: Training Loss: 0.09239103396733601 Validation Loss: 0.8212209343910217\n",
      "Epoch 13738: Training Loss: 0.09210921823978424 Validation Loss: 0.8219762444496155\n",
      "Epoch 13739: Training Loss: 0.09236029535531998 Validation Loss: 0.822337806224823\n",
      "Epoch 13740: Training Loss: 0.09252479672431946 Validation Loss: 0.8221592903137207\n",
      "Epoch 13741: Training Loss: 0.09260180592536926 Validation Loss: 0.8220235705375671\n",
      "Epoch 13742: Training Loss: 0.09236391882101695 Validation Loss: 0.8212066292762756\n",
      "Epoch 13743: Training Loss: 0.09233883768320084 Validation Loss: 0.8199255466461182\n",
      "Epoch 13744: Training Loss: 0.09306363761425018 Validation Loss: 0.8205127716064453\n",
      "Epoch 13745: Training Loss: 0.09219548106193542 Validation Loss: 0.8208231925964355\n",
      "Epoch 13746: Training Loss: 0.09244913359483083 Validation Loss: 0.820751965045929\n",
      "Epoch 13747: Training Loss: 0.09229121605555217 Validation Loss: 0.8210172057151794\n",
      "Epoch 13748: Training Loss: 0.09221410751342773 Validation Loss: 0.8214799761772156\n",
      "Epoch 13749: Training Loss: 0.09257326026757558 Validation Loss: 0.8210311532020569\n",
      "Epoch 13750: Training Loss: 0.09235834081967671 Validation Loss: 0.8221809267997742\n",
      "Epoch 13751: Training Loss: 0.09265732516845067 Validation Loss: 0.8223388195037842\n",
      "Epoch 13752: Training Loss: 0.0925070767601331 Validation Loss: 0.8227102160453796\n",
      "Epoch 13753: Training Loss: 0.09234754989544551 Validation Loss: 0.8227673768997192\n",
      "Epoch 13754: Training Loss: 0.09236437578996022 Validation Loss: 0.8222717642784119\n",
      "Epoch 13755: Training Loss: 0.09242957085371017 Validation Loss: 0.8219208121299744\n",
      "Epoch 13756: Training Loss: 0.09227931251128514 Validation Loss: 0.8210015296936035\n",
      "Epoch 13757: Training Loss: 0.09218765050172806 Validation Loss: 0.8211196660995483\n",
      "Epoch 13758: Training Loss: 0.09243077784776688 Validation Loss: 0.8205832242965698\n",
      "Epoch 13759: Training Loss: 0.09229491651058197 Validation Loss: 0.8209089040756226\n",
      "Epoch 13760: Training Loss: 0.09224238246679306 Validation Loss: 0.8215388655662537\n",
      "Epoch 13761: Training Loss: 0.09224416067202885 Validation Loss: 0.822417676448822\n",
      "Epoch 13762: Training Loss: 0.09248595933119456 Validation Loss: 0.8228318095207214\n",
      "Epoch 13763: Training Loss: 0.09251169115304947 Validation Loss: 0.8209398984909058\n",
      "Epoch 13764: Training Loss: 0.09223395337661107 Validation Loss: 0.8206926584243774\n",
      "Epoch 13765: Training Loss: 0.09253535668055217 Validation Loss: 0.820833146572113\n",
      "Epoch 13766: Training Loss: 0.0923848624030749 Validation Loss: 0.820375919342041\n",
      "Epoch 13767: Training Loss: 0.0924396812915802 Validation Loss: 0.8211233019828796\n",
      "Epoch 13768: Training Loss: 0.09199394037326176 Validation Loss: 0.8224257230758667\n",
      "Epoch 13769: Training Loss: 0.09218783428271611 Validation Loss: 0.8228634595870972\n",
      "Epoch 13770: Training Loss: 0.09222103903690974 Validation Loss: 0.8233941197395325\n",
      "Epoch 13771: Training Loss: 0.09228521088759105 Validation Loss: 0.823321521282196\n",
      "Epoch 13772: Training Loss: 0.09231233348449071 Validation Loss: 0.8225138783454895\n",
      "Epoch 13773: Training Loss: 0.09234637518723805 Validation Loss: 0.8216265439987183\n",
      "Epoch 13774: Training Loss: 0.09184636175632477 Validation Loss: 0.8214031457901001\n",
      "Epoch 13775: Training Loss: 0.09237062186002731 Validation Loss: 0.8207386136054993\n",
      "Epoch 13776: Training Loss: 0.09262625376383464 Validation Loss: 0.8222330212593079\n",
      "Epoch 13777: Training Loss: 0.09233920276165009 Validation Loss: 0.8222549557685852\n",
      "Epoch 13778: Training Loss: 0.09213389207919438 Validation Loss: 0.8224725127220154\n",
      "Epoch 13779: Training Loss: 0.09218997259934743 Validation Loss: 0.822223961353302\n",
      "Epoch 13780: Training Loss: 0.09226734687884648 Validation Loss: 0.8217779397964478\n",
      "Epoch 13781: Training Loss: 0.0925450250506401 Validation Loss: 0.820853054523468\n",
      "Epoch 13782: Training Loss: 0.09230949729681015 Validation Loss: 0.8212150931358337\n",
      "Epoch 13783: Training Loss: 0.09241824100414912 Validation Loss: 0.8216793537139893\n",
      "Epoch 13784: Training Loss: 0.09208974987268448 Validation Loss: 0.8219591975212097\n",
      "Epoch 13785: Training Loss: 0.09359539051850636 Validation Loss: 0.8234014511108398\n",
      "Epoch 13786: Training Loss: 0.09249923626581828 Validation Loss: 0.8231215476989746\n",
      "Epoch 13787: Training Loss: 0.09223873416582744 Validation Loss: 0.8221772909164429\n",
      "Epoch 13788: Training Loss: 0.09232291827599208 Validation Loss: 0.821657121181488\n",
      "Epoch 13789: Training Loss: 0.09242054323355357 Validation Loss: 0.8225117325782776\n",
      "Epoch 13790: Training Loss: 0.0921140859524409 Validation Loss: 0.821531355381012\n",
      "Epoch 13791: Training Loss: 0.09271604816118877 Validation Loss: 0.8215612769126892\n",
      "Epoch 13792: Training Loss: 0.09217043220996857 Validation Loss: 0.8213973045349121\n",
      "Epoch 13793: Training Loss: 0.0922775740424792 Validation Loss: 0.8210560083389282\n",
      "Epoch 13794: Training Loss: 0.09238635748624802 Validation Loss: 0.8219233155250549\n",
      "Epoch 13795: Training Loss: 0.09216974924008052 Validation Loss: 0.8226197957992554\n",
      "Epoch 13796: Training Loss: 0.09217961132526398 Validation Loss: 0.8235712051391602\n",
      "Epoch 13797: Training Loss: 0.09237657735745113 Validation Loss: 0.8226839303970337\n",
      "Epoch 13798: Training Loss: 0.09218108157316844 Validation Loss: 0.8218263387680054\n",
      "Epoch 13799: Training Loss: 0.09246456374724706 Validation Loss: 0.8211867213249207\n",
      "Epoch 13800: Training Loss: 0.09245086709658305 Validation Loss: 0.8210883736610413\n",
      "Epoch 13801: Training Loss: 0.09243520100911458 Validation Loss: 0.8226537704467773\n",
      "Epoch 13802: Training Loss: 0.09217678507169087 Validation Loss: 0.8232616186141968\n",
      "Epoch 13803: Training Loss: 0.09239281217257182 Validation Loss: 0.8230050206184387\n",
      "Epoch 13804: Training Loss: 0.09215821325778961 Validation Loss: 0.8221105933189392\n",
      "Epoch 13805: Training Loss: 0.09229719638824463 Validation Loss: 0.8209640383720398\n",
      "Epoch 13806: Training Loss: 0.09202924122412999 Validation Loss: 0.8211547136306763\n",
      "Epoch 13807: Training Loss: 0.0922875056664149 Validation Loss: 0.8224459886550903\n",
      "Epoch 13808: Training Loss: 0.09240362544854482 Validation Loss: 0.8226872682571411\n",
      "Epoch 13809: Training Loss: 0.09203595916430156 Validation Loss: 0.82313072681427\n",
      "Epoch 13810: Training Loss: 0.09209043284257253 Validation Loss: 0.8231669664382935\n",
      "Epoch 13811: Training Loss: 0.09224426994721095 Validation Loss: 0.8220859169960022\n",
      "Epoch 13812: Training Loss: 0.09229267885287602 Validation Loss: 0.8226206302642822\n",
      "Epoch 13813: Training Loss: 0.09277645498514175 Validation Loss: 0.8231229186058044\n",
      "Epoch 13814: Training Loss: 0.09230413536230724 Validation Loss: 0.8226626515388489\n",
      "Epoch 13815: Training Loss: 0.0922942782441775 Validation Loss: 0.8224421143531799\n",
      "Epoch 13816: Training Loss: 0.09219665328661601 Validation Loss: 0.8237287998199463\n",
      "Epoch 13817: Training Loss: 0.09266345699628194 Validation Loss: 0.8237171173095703\n",
      "Epoch 13818: Training Loss: 0.09222959230343501 Validation Loss: 0.8230937123298645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13819: Training Loss: 0.09208092590173085 Validation Loss: 0.8228335976600647\n",
      "Epoch 13820: Training Loss: 0.09298783044020335 Validation Loss: 0.8218706250190735\n",
      "Epoch 13821: Training Loss: 0.09219402819871902 Validation Loss: 0.821773111820221\n",
      "Epoch 13822: Training Loss: 0.09209396441777547 Validation Loss: 0.8223205804824829\n",
      "Epoch 13823: Training Loss: 0.09218501547972362 Validation Loss: 0.8226364850997925\n",
      "Epoch 13824: Training Loss: 0.0920524795850118 Validation Loss: 0.8227311372756958\n",
      "Epoch 13825: Training Loss: 0.09224147349596024 Validation Loss: 0.8232670426368713\n",
      "Epoch 13826: Training Loss: 0.09201467037200928 Validation Loss: 0.8229312300682068\n",
      "Epoch 13827: Training Loss: 0.09283621857563655 Validation Loss: 0.8215461373329163\n",
      "Epoch 13828: Training Loss: 0.09186398237943649 Validation Loss: 0.8213609457015991\n",
      "Epoch 13829: Training Loss: 0.09222198277711868 Validation Loss: 0.8214027881622314\n",
      "Epoch 13830: Training Loss: 0.09288651744524638 Validation Loss: 0.8221322298049927\n",
      "Epoch 13831: Training Loss: 0.09193387130896251 Validation Loss: 0.8221491575241089\n",
      "Epoch 13832: Training Loss: 0.09207414587338765 Validation Loss: 0.8225273489952087\n",
      "Epoch 13833: Training Loss: 0.0920379509528478 Validation Loss: 0.8224515914916992\n",
      "Epoch 13834: Training Loss: 0.09222699701786041 Validation Loss: 0.822360098361969\n",
      "Epoch 13835: Training Loss: 0.09182042131821315 Validation Loss: 0.8232652544975281\n",
      "Epoch 13836: Training Loss: 0.0922195240855217 Validation Loss: 0.8231622576713562\n",
      "Epoch 13837: Training Loss: 0.09207304567098618 Validation Loss: 0.8233972787857056\n",
      "Epoch 13838: Training Loss: 0.09186899910370509 Validation Loss: 0.8242374658584595\n",
      "Epoch 13839: Training Loss: 0.09216080605983734 Validation Loss: 0.8239149451255798\n",
      "Epoch 13840: Training Loss: 0.09192014982302983 Validation Loss: 0.8228003978729248\n",
      "Epoch 13841: Training Loss: 0.0926451509197553 Validation Loss: 0.8217534422874451\n",
      "Epoch 13842: Training Loss: 0.09271824111541112 Validation Loss: 0.8215014338493347\n",
      "Epoch 13843: Training Loss: 0.0925251767039299 Validation Loss: 0.8222482800483704\n",
      "Epoch 13844: Training Loss: 0.09203858921925227 Validation Loss: 0.8218885660171509\n",
      "Epoch 13845: Training Loss: 0.0924713263909022 Validation Loss: 0.8235803246498108\n",
      "Epoch 13846: Training Loss: 0.09109552204608917 Validation Loss: 0.8224751949310303\n",
      "Epoch 13847: Training Loss: 0.09235209226608276 Validation Loss: 0.8240044713020325\n",
      "Epoch 13848: Training Loss: 0.09223457922538121 Validation Loss: 0.8229715824127197\n",
      "Epoch 13849: Training Loss: 0.09213462720314662 Validation Loss: 0.8217067718505859\n",
      "Epoch 13850: Training Loss: 0.09270594269037247 Validation Loss: 0.822134256362915\n",
      "Epoch 13851: Training Loss: 0.09203201035658519 Validation Loss: 0.8230535387992859\n",
      "Epoch 13852: Training Loss: 0.09267520159482956 Validation Loss: 0.823641300201416\n",
      "Epoch 13853: Training Loss: 0.09274875621000926 Validation Loss: 0.8227000832557678\n",
      "Epoch 13854: Training Loss: 0.09205119560162227 Validation Loss: 0.8221392035484314\n",
      "Epoch 13855: Training Loss: 0.09184712171554565 Validation Loss: 0.8222740292549133\n",
      "Epoch 13856: Training Loss: 0.09201124807198842 Validation Loss: 0.8221309185028076\n",
      "Epoch 13857: Training Loss: 0.09191370755434036 Validation Loss: 0.8223926424980164\n",
      "Epoch 13858: Training Loss: 0.09258636832237244 Validation Loss: 0.8230586051940918\n",
      "Epoch 13859: Training Loss: 0.09245881934960683 Validation Loss: 0.8248233795166016\n",
      "Epoch 13860: Training Loss: 0.09221004446347554 Validation Loss: 0.8244106769561768\n",
      "Epoch 13861: Training Loss: 0.09202483793099721 Validation Loss: 0.8231645822525024\n",
      "Epoch 13862: Training Loss: 0.091971588631471 Validation Loss: 0.8219826221466064\n",
      "Epoch 13863: Training Loss: 0.09267562876145045 Validation Loss: 0.8226696252822876\n",
      "Epoch 13864: Training Loss: 0.092240904768308 Validation Loss: 0.8235076069831848\n",
      "Epoch 13865: Training Loss: 0.09238111972808838 Validation Loss: 0.8235610127449036\n",
      "Epoch 13866: Training Loss: 0.0921322504679362 Validation Loss: 0.8235264420509338\n",
      "Epoch 13867: Training Loss: 0.09276943653821945 Validation Loss: 0.822763204574585\n",
      "Epoch 13868: Training Loss: 0.09204546610514323 Validation Loss: 0.8218489289283752\n",
      "Epoch 13869: Training Loss: 0.0922732725739479 Validation Loss: 0.8220056295394897\n",
      "Epoch 13870: Training Loss: 0.09215955684582393 Validation Loss: 0.8222966194152832\n",
      "Epoch 13871: Training Loss: 0.09177638341983159 Validation Loss: 0.8228242993354797\n",
      "Epoch 13872: Training Loss: 0.09152298420667648 Validation Loss: 0.8231189250946045\n",
      "Epoch 13873: Training Loss: 0.09173062692085902 Validation Loss: 0.8240976929664612\n",
      "Epoch 13874: Training Loss: 0.09190462529659271 Validation Loss: 0.8243760466575623\n",
      "Epoch 13875: Training Loss: 0.09221749007701874 Validation Loss: 0.8237102031707764\n",
      "Epoch 13876: Training Loss: 0.09205128500858943 Validation Loss: 0.8225212097167969\n",
      "Epoch 13877: Training Loss: 0.09192560364802678 Validation Loss: 0.8221170902252197\n",
      "Epoch 13878: Training Loss: 0.09188771744569142 Validation Loss: 0.8215888142585754\n",
      "Epoch 13879: Training Loss: 0.0919455016652743 Validation Loss: 0.8230038285255432\n",
      "Epoch 13880: Training Loss: 0.09240105499823888 Validation Loss: 0.8231853246688843\n",
      "Epoch 13881: Training Loss: 0.09191642204920451 Validation Loss: 0.8236035704612732\n",
      "Epoch 13882: Training Loss: 0.09198880692323048 Validation Loss: 0.8234241604804993\n",
      "Epoch 13883: Training Loss: 0.09225821246703465 Validation Loss: 0.8240417242050171\n",
      "Epoch 13884: Training Loss: 0.09197788685560226 Validation Loss: 0.8238335251808167\n",
      "Epoch 13885: Training Loss: 0.09192104389270146 Validation Loss: 0.8239219784736633\n",
      "Epoch 13886: Training Loss: 0.09186164786418279 Validation Loss: 0.82245272397995\n",
      "Epoch 13887: Training Loss: 0.09207356721162796 Validation Loss: 0.8219204545021057\n",
      "Epoch 13888: Training Loss: 0.09205230573813121 Validation Loss: 0.8217601776123047\n",
      "Epoch 13889: Training Loss: 0.09227526684602101 Validation Loss: 0.8236713409423828\n",
      "Epoch 13890: Training Loss: 0.09226453304290771 Validation Loss: 0.824471116065979\n",
      "Epoch 13891: Training Loss: 0.09194389233986537 Validation Loss: 0.8247542977333069\n",
      "Epoch 13892: Training Loss: 0.09207271784543991 Validation Loss: 0.8230326771736145\n",
      "Epoch 13893: Training Loss: 0.09244060764710109 Validation Loss: 0.8231692910194397\n",
      "Epoch 13894: Training Loss: 0.09189671526352565 Validation Loss: 0.8229873776435852\n",
      "Epoch 13895: Training Loss: 0.09174387405316035 Validation Loss: 0.8234933018684387\n",
      "Epoch 13896: Training Loss: 0.09181516617536545 Validation Loss: 0.8231496214866638\n",
      "Epoch 13897: Training Loss: 0.09183479845523834 Validation Loss: 0.8237361311912537\n",
      "Epoch 13898: Training Loss: 0.09159442534049352 Validation Loss: 0.8241798877716064\n",
      "Epoch 13899: Training Loss: 0.09181602795918782 Validation Loss: 0.8243348002433777\n",
      "Epoch 13900: Training Loss: 0.09220844507217407 Validation Loss: 0.8222953081130981\n",
      "Epoch 13901: Training Loss: 0.09165983647108078 Validation Loss: 0.8218408823013306\n",
      "Epoch 13902: Training Loss: 0.09205412119626999 Validation Loss: 0.8214789032936096\n",
      "Epoch 13903: Training Loss: 0.0917494719227155 Validation Loss: 0.822986900806427\n",
      "Epoch 13904: Training Loss: 0.09217473119497299 Validation Loss: 0.8249220848083496\n",
      "Epoch 13905: Training Loss: 0.0918435553709666 Validation Loss: 0.8251779675483704\n",
      "Epoch 13906: Training Loss: 0.09171667446692784 Validation Loss: 0.8249455094337463\n",
      "Epoch 13907: Training Loss: 0.0917828877766927 Validation Loss: 0.8241321444511414\n",
      "Epoch 13908: Training Loss: 0.09213602542877197 Validation Loss: 0.8217927813529968\n",
      "Epoch 13909: Training Loss: 0.09166089196999867 Validation Loss: 0.8227505087852478\n",
      "Epoch 13910: Training Loss: 0.09194175153970718 Validation Loss: 0.8230834007263184\n",
      "Epoch 13911: Training Loss: 0.0922409122188886 Validation Loss: 0.824034571647644\n",
      "Epoch 13912: Training Loss: 0.09180388102928798 Validation Loss: 0.8243433237075806\n",
      "Epoch 13913: Training Loss: 0.09178932507832845 Validation Loss: 0.8236538767814636\n",
      "Epoch 13914: Training Loss: 0.09194519619146983 Validation Loss: 0.8230510354042053\n",
      "Epoch 13915: Training Loss: 0.09154537568489711 Validation Loss: 0.8221533298492432\n",
      "Epoch 13916: Training Loss: 0.09202732145786285 Validation Loss: 0.8219030499458313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13917: Training Loss: 0.092169851064682 Validation Loss: 0.8233916759490967\n",
      "Epoch 13918: Training Loss: 0.09271237005790074 Validation Loss: 0.8251715302467346\n",
      "Epoch 13919: Training Loss: 0.09179476896921794 Validation Loss: 0.8248240947723389\n",
      "Epoch 13920: Training Loss: 0.09203155090411504 Validation Loss: 0.8241854906082153\n",
      "Epoch 13921: Training Loss: 0.09214336176713307 Validation Loss: 0.8245245218276978\n",
      "Epoch 13922: Training Loss: 0.09208556264638901 Validation Loss: 0.8241617679595947\n",
      "Epoch 13923: Training Loss: 0.09175417572259903 Validation Loss: 0.823421061038971\n",
      "Epoch 13924: Training Loss: 0.09284949426849683 Validation Loss: 0.8229251503944397\n",
      "Epoch 13925: Training Loss: 0.09186772257089615 Validation Loss: 0.8229947090148926\n",
      "Epoch 13926: Training Loss: 0.09193831185499828 Validation Loss: 0.8231375217437744\n",
      "Epoch 13927: Training Loss: 0.09153250853220622 Validation Loss: 0.8235958814620972\n",
      "Epoch 13928: Training Loss: 0.09153702855110168 Validation Loss: 0.823343813419342\n",
      "Epoch 13929: Training Loss: 0.09162254879872005 Validation Loss: 0.8236882090568542\n",
      "Epoch 13930: Training Loss: 0.09224077314138412 Validation Loss: 0.8233458399772644\n",
      "Epoch 13931: Training Loss: 0.09178007394075394 Validation Loss: 0.8238295912742615\n",
      "Epoch 13932: Training Loss: 0.09173726787169774 Validation Loss: 0.8233973979949951\n",
      "Epoch 13933: Training Loss: 0.09162302315235138 Validation Loss: 0.8230011463165283\n",
      "Epoch 13934: Training Loss: 0.09164891640345256 Validation Loss: 0.8219274282455444\n",
      "Epoch 13935: Training Loss: 0.09177125990390778 Validation Loss: 0.8222180604934692\n",
      "Epoch 13936: Training Loss: 0.09202427913745244 Validation Loss: 0.8244460821151733\n",
      "Epoch 13937: Training Loss: 0.09183177848656972 Validation Loss: 0.8252628445625305\n",
      "Epoch 13938: Training Loss: 0.09179780383904775 Validation Loss: 0.825183629989624\n",
      "Epoch 13939: Training Loss: 0.09196301798025767 Validation Loss: 0.8237289786338806\n",
      "Epoch 13940: Training Loss: 0.09181924909353256 Validation Loss: 0.8238494992256165\n",
      "Epoch 13941: Training Loss: 0.09212884058554967 Validation Loss: 0.8233539462089539\n",
      "Epoch 13942: Training Loss: 0.09176030258337657 Validation Loss: 0.8230496644973755\n",
      "Epoch 13943: Training Loss: 0.09171959261099498 Validation Loss: 0.8235782980918884\n",
      "Epoch 13944: Training Loss: 0.09175839523474376 Validation Loss: 0.8245099782943726\n",
      "Epoch 13945: Training Loss: 0.09171809007724126 Validation Loss: 0.8251534700393677\n",
      "Epoch 13946: Training Loss: 0.09174160659313202 Validation Loss: 0.8248428106307983\n",
      "Epoch 13947: Training Loss: 0.0916650320092837 Validation Loss: 0.823764443397522\n",
      "Epoch 13948: Training Loss: 0.09175141404072444 Validation Loss: 0.8236098289489746\n",
      "Epoch 13949: Training Loss: 0.09174974759419759 Validation Loss: 0.8234954476356506\n",
      "Epoch 13950: Training Loss: 0.09166291356086731 Validation Loss: 0.8230022192001343\n",
      "Epoch 13951: Training Loss: 0.09149694442749023 Validation Loss: 0.8235189914703369\n",
      "Epoch 13952: Training Loss: 0.09156648069620132 Validation Loss: 0.8237029910087585\n",
      "Epoch 13953: Training Loss: 0.09185213595628738 Validation Loss: 0.8230516910552979\n",
      "Epoch 13954: Training Loss: 0.09158103664716084 Validation Loss: 0.8229901790618896\n",
      "Epoch 13955: Training Loss: 0.09215208142995834 Validation Loss: 0.8245115280151367\n",
      "Epoch 13956: Training Loss: 0.09159541875123978 Validation Loss: 0.8239941596984863\n",
      "Epoch 13957: Training Loss: 0.09164048234621684 Validation Loss: 0.8241456747055054\n",
      "Epoch 13958: Training Loss: 0.09225937227408092 Validation Loss: 0.8243795037269592\n",
      "Epoch 13959: Training Loss: 0.09159287561972936 Validation Loss: 0.824009120464325\n",
      "Epoch 13960: Training Loss: 0.09160533299048741 Validation Loss: 0.8237903714179993\n",
      "Epoch 13961: Training Loss: 0.09217401345570882 Validation Loss: 0.8226326704025269\n",
      "Epoch 13962: Training Loss: 0.09183672815561295 Validation Loss: 0.8230224251747131\n",
      "Epoch 13963: Training Loss: 0.09171110143264134 Validation Loss: 0.8235930800437927\n",
      "Epoch 13964: Training Loss: 0.09253026048342387 Validation Loss: 0.8255136609077454\n",
      "Epoch 13965: Training Loss: 0.09198457499345143 Validation Loss: 0.8247205018997192\n",
      "Epoch 13966: Training Loss: 0.09211039046446483 Validation Loss: 0.8253990411758423\n",
      "Epoch 13967: Training Loss: 0.09181651473045349 Validation Loss: 0.8239010572433472\n",
      "Epoch 13968: Training Loss: 0.09166069825490315 Validation Loss: 0.8232024312019348\n",
      "Epoch 13969: Training Loss: 0.09152195354302724 Validation Loss: 0.8246175050735474\n",
      "Epoch 13970: Training Loss: 0.09249735375245412 Validation Loss: 0.8254635334014893\n",
      "Epoch 13971: Training Loss: 0.09177748362223308 Validation Loss: 0.825234591960907\n",
      "Epoch 13972: Training Loss: 0.0920989140868187 Validation Loss: 0.8243030905723572\n",
      "Epoch 13973: Training Loss: 0.09165609627962112 Validation Loss: 0.8244316577911377\n",
      "Epoch 13974: Training Loss: 0.09159386654694875 Validation Loss: 0.8235949277877808\n",
      "Epoch 13975: Training Loss: 0.09161897748708725 Validation Loss: 0.8229289650917053\n",
      "Epoch 13976: Training Loss: 0.09158744911352794 Validation Loss: 0.8230586051940918\n",
      "Epoch 13977: Training Loss: 0.09182331959406535 Validation Loss: 0.8224833607673645\n",
      "Epoch 13978: Training Loss: 0.09171375880638759 Validation Loss: 0.8231199383735657\n",
      "Epoch 13979: Training Loss: 0.09163863708575566 Validation Loss: 0.8242720365524292\n",
      "Epoch 13980: Training Loss: 0.09217506895462672 Validation Loss: 0.8241514563560486\n",
      "Epoch 13981: Training Loss: 0.0917014330625534 Validation Loss: 0.8255857229232788\n",
      "Epoch 13982: Training Loss: 0.09179320434729259 Validation Loss: 0.8237139582633972\n",
      "Epoch 13983: Training Loss: 0.09133392075697581 Validation Loss: 0.8236027359962463\n",
      "Epoch 13984: Training Loss: 0.09120239565769832 Validation Loss: 0.8243919610977173\n",
      "Epoch 13985: Training Loss: 0.09185100098450978 Validation Loss: 0.8254547119140625\n",
      "Epoch 13986: Training Loss: 0.09188159306844075 Validation Loss: 0.8248699307441711\n",
      "Epoch 13987: Training Loss: 0.09190283715724945 Validation Loss: 0.8243808150291443\n",
      "Epoch 13988: Training Loss: 0.09166565040747325 Validation Loss: 0.8237752318382263\n",
      "Epoch 13989: Training Loss: 0.09197208036979039 Validation Loss: 0.8225718140602112\n",
      "Epoch 13990: Training Loss: 0.09182520707448323 Validation Loss: 0.8248133063316345\n",
      "Epoch 13991: Training Loss: 0.09169623504082362 Validation Loss: 0.8256639838218689\n",
      "Epoch 13992: Training Loss: 0.09198650221029918 Validation Loss: 0.8253965973854065\n",
      "Epoch 13993: Training Loss: 0.09137605130672455 Validation Loss: 0.8256382346153259\n",
      "Epoch 13994: Training Loss: 0.09161929041147232 Validation Loss: 0.8254198431968689\n",
      "Epoch 13995: Training Loss: 0.09166071812311809 Validation Loss: 0.8246955275535583\n",
      "Epoch 13996: Training Loss: 0.0915329431494077 Validation Loss: 0.8237636089324951\n",
      "Epoch 13997: Training Loss: 0.09152165551980336 Validation Loss: 0.8227778077125549\n",
      "Epoch 13998: Training Loss: 0.09180923799673717 Validation Loss: 0.8241046071052551\n",
      "Epoch 13999: Training Loss: 0.09163354337215424 Validation Loss: 0.8244113922119141\n",
      "Epoch 14000: Training Loss: 0.09163932005564372 Validation Loss: 0.8236050009727478\n",
      "Epoch 14001: Training Loss: 0.09123938033978145 Validation Loss: 0.8240755200386047\n",
      "Epoch 14002: Training Loss: 0.0912695253888766 Validation Loss: 0.8233227133750916\n",
      "Epoch 14003: Training Loss: 0.09142393122116725 Validation Loss: 0.8234962224960327\n",
      "Epoch 14004: Training Loss: 0.09147859861453374 Validation Loss: 0.8240357041358948\n",
      "Epoch 14005: Training Loss: 0.09161871423323949 Validation Loss: 0.8251610994338989\n",
      "Epoch 14006: Training Loss: 0.09169398993253708 Validation Loss: 0.8254145383834839\n",
      "Epoch 14007: Training Loss: 0.09185619652271271 Validation Loss: 0.8249706029891968\n",
      "Epoch 14008: Training Loss: 0.09238651394844055 Validation Loss: 0.8252921104431152\n",
      "Epoch 14009: Training Loss: 0.09162529309590657 Validation Loss: 0.8254645466804504\n",
      "Epoch 14010: Training Loss: 0.09135908136765163 Validation Loss: 0.8245024085044861\n",
      "Epoch 14011: Training Loss: 0.0915266325076421 Validation Loss: 0.8241753578186035\n",
      "Epoch 14012: Training Loss: 0.09168599297602971 Validation Loss: 0.822765052318573\n",
      "Epoch 14013: Training Loss: 0.09155729164679845 Validation Loss: 0.8229445219039917\n",
      "Epoch 14014: Training Loss: 0.09125115970770518 Validation Loss: 0.8248926997184753\n",
      "Epoch 14015: Training Loss: 0.0914316972096761 Validation Loss: 0.8260345458984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14016: Training Loss: 0.09201346337795258 Validation Loss: 0.8258811831474304\n",
      "Epoch 14017: Training Loss: 0.0915402223666509 Validation Loss: 0.8240777254104614\n",
      "Epoch 14018: Training Loss: 0.09184453388055165 Validation Loss: 0.8231545090675354\n",
      "Epoch 14019: Training Loss: 0.09115803738435109 Validation Loss: 0.8236392140388489\n",
      "Epoch 14020: Training Loss: 0.09182959298292796 Validation Loss: 0.8246643543243408\n",
      "Epoch 14021: Training Loss: 0.09157911439736684 Validation Loss: 0.8251708745956421\n",
      "Epoch 14022: Training Loss: 0.0913032740354538 Validation Loss: 0.8247485160827637\n",
      "Epoch 14023: Training Loss: 0.0913488045334816 Validation Loss: 0.8246151804924011\n",
      "Epoch 14024: Training Loss: 0.0917012169957161 Validation Loss: 0.8240567445755005\n",
      "Epoch 14025: Training Loss: 0.09156136214733124 Validation Loss: 0.8242003321647644\n",
      "Epoch 14026: Training Loss: 0.09183876464764278 Validation Loss: 0.825772762298584\n",
      "Epoch 14027: Training Loss: 0.09126172214746475 Validation Loss: 0.8246427774429321\n",
      "Epoch 14028: Training Loss: 0.09141572813193004 Validation Loss: 0.8246239423751831\n",
      "Epoch 14029: Training Loss: 0.09207851191361745 Validation Loss: 0.8248066306114197\n",
      "Epoch 14030: Training Loss: 0.09145672619342804 Validation Loss: 0.8244278430938721\n",
      "Epoch 14031: Training Loss: 0.0912639672557513 Validation Loss: 0.8239297866821289\n",
      "Epoch 14032: Training Loss: 0.09298669546842575 Validation Loss: 0.8252176642417908\n",
      "Epoch 14033: Training Loss: 0.0913475975394249 Validation Loss: 0.8249566555023193\n",
      "Epoch 14034: Training Loss: 0.0912474791208903 Validation Loss: 0.8251597881317139\n",
      "Epoch 14035: Training Loss: 0.09195572137832642 Validation Loss: 0.8242607712745667\n",
      "Epoch 14036: Training Loss: 0.09131530423959096 Validation Loss: 0.8242614269256592\n",
      "Epoch 14037: Training Loss: 0.09142020841439565 Validation Loss: 0.8241682648658752\n",
      "Epoch 14038: Training Loss: 0.09148800869782765 Validation Loss: 0.8235246539115906\n",
      "Epoch 14039: Training Loss: 0.0913355549176534 Validation Loss: 0.8243763446807861\n",
      "Epoch 14040: Training Loss: 0.09110321601231892 Validation Loss: 0.8253692984580994\n",
      "Epoch 14041: Training Loss: 0.09138731161753337 Validation Loss: 0.825520932674408\n",
      "Epoch 14042: Training Loss: 0.09119042257467906 Validation Loss: 0.825222373008728\n",
      "Epoch 14043: Training Loss: 0.09171884258588155 Validation Loss: 0.824072539806366\n",
      "Epoch 14044: Training Loss: 0.09158702194690704 Validation Loss: 0.8244513869285583\n",
      "Epoch 14045: Training Loss: 0.09174561748902003 Validation Loss: 0.8259291648864746\n",
      "Epoch 14046: Training Loss: 0.09157639741897583 Validation Loss: 0.8271211981773376\n",
      "Epoch 14047: Training Loss: 0.09147486587365468 Validation Loss: 0.8258594870567322\n",
      "Epoch 14048: Training Loss: 0.09093300501505534 Validation Loss: 0.8248282074928284\n",
      "Epoch 14049: Training Loss: 0.09142085413138072 Validation Loss: 0.8252331018447876\n",
      "Epoch 14050: Training Loss: 0.0915646106004715 Validation Loss: 0.8260396718978882\n",
      "Epoch 14051: Training Loss: 0.09154061476389568 Validation Loss: 0.8246822357177734\n",
      "Epoch 14052: Training Loss: 0.09147311995426814 Validation Loss: 0.8242661952972412\n",
      "Epoch 14053: Training Loss: 0.09128877520561218 Validation Loss: 0.8246525526046753\n",
      "Epoch 14054: Training Loss: 0.09146573146184285 Validation Loss: 0.8254925608634949\n",
      "Epoch 14055: Training Loss: 0.09155634293953578 Validation Loss: 0.8259552717208862\n",
      "Epoch 14056: Training Loss: 0.0918492004275322 Validation Loss: 0.8252018094062805\n",
      "Epoch 14057: Training Loss: 0.09122920036315918 Validation Loss: 0.8248271942138672\n",
      "Epoch 14058: Training Loss: 0.09130757053693135 Validation Loss: 0.8248956799507141\n",
      "Epoch 14059: Training Loss: 0.0913860375682513 Validation Loss: 0.8242681622505188\n",
      "Epoch 14060: Training Loss: 0.09133092562357585 Validation Loss: 0.8238608241081238\n",
      "Epoch 14061: Training Loss: 0.09115008513132732 Validation Loss: 0.8240898251533508\n",
      "Epoch 14062: Training Loss: 0.09145643313725789 Validation Loss: 0.8251795172691345\n",
      "Epoch 14063: Training Loss: 0.09165909389654796 Validation Loss: 0.8262965679168701\n",
      "Epoch 14064: Training Loss: 0.09143492331107457 Validation Loss: 0.8258136510848999\n",
      "Epoch 14065: Training Loss: 0.09147883454958598 Validation Loss: 0.8254865407943726\n",
      "Epoch 14066: Training Loss: 0.091323621571064 Validation Loss: 0.824379026889801\n",
      "Epoch 14067: Training Loss: 0.09154887000719707 Validation Loss: 0.8244891166687012\n",
      "Epoch 14068: Training Loss: 0.09134543687105179 Validation Loss: 0.824525773525238\n",
      "Epoch 14069: Training Loss: 0.09126340101162593 Validation Loss: 0.8253172039985657\n",
      "Epoch 14070: Training Loss: 0.09145681808392207 Validation Loss: 0.8262821435928345\n",
      "Epoch 14071: Training Loss: 0.09207296619812648 Validation Loss: 0.8257980346679688\n",
      "Epoch 14072: Training Loss: 0.09118790924549103 Validation Loss: 0.8256088495254517\n",
      "Epoch 14073: Training Loss: 0.09133742998043697 Validation Loss: 0.824920117855072\n",
      "Epoch 14074: Training Loss: 0.09118756651878357 Validation Loss: 0.8253899812698364\n",
      "Epoch 14075: Training Loss: 0.09136137366294861 Validation Loss: 0.8250338435173035\n",
      "Epoch 14076: Training Loss: 0.09133579581975937 Validation Loss: 0.824502170085907\n",
      "Epoch 14077: Training Loss: 0.09129985670248668 Validation Loss: 0.8249359130859375\n",
      "Epoch 14078: Training Loss: 0.09123703837394714 Validation Loss: 0.8247800469398499\n",
      "Epoch 14079: Training Loss: 0.09128412852684657 Validation Loss: 0.8245032429695129\n",
      "Epoch 14080: Training Loss: 0.09112381935119629 Validation Loss: 0.825610339641571\n",
      "Epoch 14081: Training Loss: 0.09149868289629619 Validation Loss: 0.8258970975875854\n",
      "Epoch 14082: Training Loss: 0.09133328000704448 Validation Loss: 0.8257948160171509\n",
      "Epoch 14083: Training Loss: 0.09168179581562678 Validation Loss: 0.8259395360946655\n",
      "Epoch 14084: Training Loss: 0.09121319403251012 Validation Loss: 0.8251456618309021\n",
      "Epoch 14085: Training Loss: 0.09147953242063522 Validation Loss: 0.8254902362823486\n",
      "Epoch 14086: Training Loss: 0.09124108652273814 Validation Loss: 0.825275719165802\n",
      "Epoch 14087: Training Loss: 0.09134184072415034 Validation Loss: 0.8249372243881226\n",
      "Epoch 14088: Training Loss: 0.0913429061571757 Validation Loss: 0.8247137665748596\n",
      "Epoch 14089: Training Loss: 0.09177008022864659 Validation Loss: 0.8252072334289551\n",
      "Epoch 14090: Training Loss: 0.09129017343123753 Validation Loss: 0.8255747556686401\n",
      "Epoch 14091: Training Loss: 0.09143652766942978 Validation Loss: 0.8251112699508667\n",
      "Epoch 14092: Training Loss: 0.09137354791164398 Validation Loss: 0.8258104920387268\n",
      "Epoch 14093: Training Loss: 0.09157369037469228 Validation Loss: 0.82628333568573\n",
      "Epoch 14094: Training Loss: 0.09156586478153865 Validation Loss: 0.8260828256607056\n",
      "Epoch 14095: Training Loss: 0.0913208896915118 Validation Loss: 0.8249938488006592\n",
      "Epoch 14096: Training Loss: 0.0911282127102216 Validation Loss: 0.8244938850402832\n",
      "Epoch 14097: Training Loss: 0.09112300475438435 Validation Loss: 0.8245744705200195\n",
      "Epoch 14098: Training Loss: 0.09125883380572002 Validation Loss: 0.8251028060913086\n",
      "Epoch 14099: Training Loss: 0.09139757355054219 Validation Loss: 0.8255420327186584\n",
      "Epoch 14100: Training Loss: 0.09106118232011795 Validation Loss: 0.8262674808502197\n",
      "Epoch 14101: Training Loss: 0.09122163305679958 Validation Loss: 0.827072024345398\n",
      "Epoch 14102: Training Loss: 0.09165901442368825 Validation Loss: 0.8267150521278381\n",
      "Epoch 14103: Training Loss: 0.09148785223563512 Validation Loss: 0.8250247240066528\n",
      "Epoch 14104: Training Loss: 0.0911761075258255 Validation Loss: 0.8247131109237671\n",
      "Epoch 14105: Training Loss: 0.09153417746225993 Validation Loss: 0.8254122138023376\n",
      "Epoch 14106: Training Loss: 0.09147942066192627 Validation Loss: 0.8252620100975037\n",
      "Epoch 14107: Training Loss: 0.09125260015328725 Validation Loss: 0.8255622982978821\n",
      "Epoch 14108: Training Loss: 0.09170419722795486 Validation Loss: 0.8262864351272583\n",
      "Epoch 14109: Training Loss: 0.09129403531551361 Validation Loss: 0.8265622854232788\n",
      "Epoch 14110: Training Loss: 0.0913512756427129 Validation Loss: 0.8264485597610474\n",
      "Epoch 14111: Training Loss: 0.09159318606058757 Validation Loss: 0.8247212767601013\n",
      "Epoch 14112: Training Loss: 0.09099321564038594 Validation Loss: 0.8254035115242004\n",
      "Epoch 14113: Training Loss: 0.09129379192988078 Validation Loss: 0.8257330060005188\n",
      "Epoch 14114: Training Loss: 0.09122481942176819 Validation Loss: 0.8252313137054443\n",
      "Epoch 14115: Training Loss: 0.09107193102439244 Validation Loss: 0.8250757455825806\n",
      "Epoch 14116: Training Loss: 0.09184705962737401 Validation Loss: 0.8259828090667725\n",
      "Epoch 14117: Training Loss: 0.09128779669602712 Validation Loss: 0.82603520154953\n",
      "Epoch 14118: Training Loss: 0.09119893610477448 Validation Loss: 0.8258534669876099\n",
      "Epoch 14119: Training Loss: 0.09130595624446869 Validation Loss: 0.8257350325584412\n",
      "Epoch 14120: Training Loss: 0.09103104223807652 Validation Loss: 0.824890673160553\n",
      "Epoch 14121: Training Loss: 0.09111129740873973 Validation Loss: 0.8246769905090332\n",
      "Epoch 14122: Training Loss: 0.09115489323933919 Validation Loss: 0.8241373300552368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14123: Training Loss: 0.09113917251427968 Validation Loss: 0.8253322839736938\n",
      "Epoch 14124: Training Loss: 0.09097510327895482 Validation Loss: 0.8254057168960571\n",
      "Epoch 14125: Training Loss: 0.0911718060572942 Validation Loss: 0.8258444666862488\n",
      "Epoch 14126: Training Loss: 0.09096849213043849 Validation Loss: 0.8267093300819397\n",
      "Epoch 14127: Training Loss: 0.0912824496626854 Validation Loss: 0.8271787762641907\n",
      "Epoch 14128: Training Loss: 0.09110104540983836 Validation Loss: 0.8272681832313538\n",
      "Epoch 14129: Training Loss: 0.09145564834276836 Validation Loss: 0.8256366848945618\n",
      "Epoch 14130: Training Loss: 0.09095689157644908 Validation Loss: 0.8257268667221069\n",
      "Epoch 14131: Training Loss: 0.09114506840705872 Validation Loss: 0.8259179592132568\n",
      "Epoch 14132: Training Loss: 0.09073381374279658 Validation Loss: 0.8263756632804871\n",
      "Epoch 14133: Training Loss: 0.09110205868879954 Validation Loss: 0.8263166546821594\n",
      "Epoch 14134: Training Loss: 0.09120175987482071 Validation Loss: 0.8268031477928162\n",
      "Epoch 14135: Training Loss: 0.09091689437627792 Validation Loss: 0.8259028196334839\n",
      "Epoch 14136: Training Loss: 0.09096126755078633 Validation Loss: 0.8254777789115906\n",
      "Epoch 14137: Training Loss: 0.09103920310735703 Validation Loss: 0.8247304558753967\n",
      "Epoch 14138: Training Loss: 0.09108645468950272 Validation Loss: 0.8251712322235107\n",
      "Epoch 14139: Training Loss: 0.09101519236962001 Validation Loss: 0.8265066146850586\n",
      "Epoch 14140: Training Loss: 0.0909189482529958 Validation Loss: 0.8268010020256042\n",
      "Epoch 14141: Training Loss: 0.09120432039101918 Validation Loss: 0.8260073065757751\n",
      "Epoch 14142: Training Loss: 0.09101347128550212 Validation Loss: 0.8258562684059143\n",
      "Epoch 14143: Training Loss: 0.09125304718812306 Validation Loss: 0.826096773147583\n",
      "Epoch 14144: Training Loss: 0.09128400186697642 Validation Loss: 0.8256309032440186\n",
      "Epoch 14145: Training Loss: 0.09117505451043446 Validation Loss: 0.8268841505050659\n",
      "Epoch 14146: Training Loss: 0.09126658737659454 Validation Loss: 0.8264617919921875\n",
      "Epoch 14147: Training Loss: 0.09130200246969859 Validation Loss: 0.8253788352012634\n",
      "Epoch 14148: Training Loss: 0.09093145529429118 Validation Loss: 0.8252002596855164\n",
      "Epoch 14149: Training Loss: 0.0914862925807635 Validation Loss: 0.8254626989364624\n",
      "Epoch 14150: Training Loss: 0.09184171259403229 Validation Loss: 0.8262062668800354\n",
      "Epoch 14151: Training Loss: 0.09079695989688237 Validation Loss: 0.8261899352073669\n",
      "Epoch 14152: Training Loss: 0.09083912521600723 Validation Loss: 0.8267554640769958\n",
      "Epoch 14153: Training Loss: 0.09133715430895488 Validation Loss: 0.8271272778511047\n",
      "Epoch 14154: Training Loss: 0.09123165408770244 Validation Loss: 0.8266140818595886\n",
      "Epoch 14155: Training Loss: 0.09067983676989873 Validation Loss: 0.8260506391525269\n",
      "Epoch 14156: Training Loss: 0.0914115458726883 Validation Loss: 0.8265805840492249\n",
      "Epoch 14157: Training Loss: 0.09110167374213536 Validation Loss: 0.8265345692634583\n",
      "Epoch 14158: Training Loss: 0.09100682536760966 Validation Loss: 0.8260125517845154\n",
      "Epoch 14159: Training Loss: 0.09097493439912796 Validation Loss: 0.8261593580245972\n",
      "Epoch 14160: Training Loss: 0.09076882898807526 Validation Loss: 0.8257462978363037\n",
      "Epoch 14161: Training Loss: 0.09152648101250331 Validation Loss: 0.8243216872215271\n",
      "Epoch 14162: Training Loss: 0.0909459615747134 Validation Loss: 0.8247367143630981\n",
      "Epoch 14163: Training Loss: 0.09092454115549724 Validation Loss: 0.8252110481262207\n",
      "Epoch 14164: Training Loss: 0.09105449418226878 Validation Loss: 0.8267046213150024\n",
      "Epoch 14165: Training Loss: 0.09122266372044881 Validation Loss: 0.8266448378562927\n",
      "Epoch 14166: Training Loss: 0.09115954736868541 Validation Loss: 0.8262653946876526\n",
      "Epoch 14167: Training Loss: 0.09100079536437988 Validation Loss: 0.8262080550193787\n",
      "Epoch 14168: Training Loss: 0.0922806312640508 Validation Loss: 0.8262842893600464\n",
      "Epoch 14169: Training Loss: 0.09069250772396724 Validation Loss: 0.8255298137664795\n",
      "Epoch 14170: Training Loss: 0.09108577917019527 Validation Loss: 0.8250529170036316\n",
      "Epoch 14171: Training Loss: 0.09100876996914546 Validation Loss: 0.8256895542144775\n",
      "Epoch 14172: Training Loss: 0.09107872098684311 Validation Loss: 0.8267755508422852\n",
      "Epoch 14173: Training Loss: 0.09116482237974803 Validation Loss: 0.8262647390365601\n",
      "Epoch 14174: Training Loss: 0.09137661258379619 Validation Loss: 0.82643061876297\n",
      "Epoch 14175: Training Loss: 0.09095280120770137 Validation Loss: 0.8260720372200012\n",
      "Epoch 14176: Training Loss: 0.09105562667051952 Validation Loss: 0.8270488977432251\n",
      "Epoch 14177: Training Loss: 0.09085368861754735 Validation Loss: 0.827765941619873\n",
      "Epoch 14178: Training Loss: 0.09086205810308456 Validation Loss: 0.8269969820976257\n",
      "Epoch 14179: Training Loss: 0.09103496621052425 Validation Loss: 0.8264943957328796\n",
      "Epoch 14180: Training Loss: 0.09184207518895467 Validation Loss: 0.8264339566230774\n",
      "Epoch 14181: Training Loss: 0.09139806280533473 Validation Loss: 0.8262370824813843\n",
      "Epoch 14182: Training Loss: 0.09141064683596294 Validation Loss: 0.8263276219367981\n",
      "Epoch 14183: Training Loss: 0.09087069084246953 Validation Loss: 0.8272913694381714\n",
      "Epoch 14184: Training Loss: 0.09100792557001114 Validation Loss: 0.8281036615371704\n",
      "Epoch 14185: Training Loss: 0.09154199808835983 Validation Loss: 0.8284826874732971\n",
      "Epoch 14186: Training Loss: 0.09095657120148341 Validation Loss: 0.8260951638221741\n",
      "Epoch 14187: Training Loss: 0.09077305595080058 Validation Loss: 0.8243374228477478\n",
      "Epoch 14188: Training Loss: 0.09097077697515488 Validation Loss: 0.8242466449737549\n",
      "Epoch 14189: Training Loss: 0.0911427636941274 Validation Loss: 0.8256001472473145\n",
      "Epoch 14190: Training Loss: 0.09152085085709889 Validation Loss: 0.8284011483192444\n",
      "Epoch 14191: Training Loss: 0.09088623275359471 Validation Loss: 0.8284649848937988\n",
      "Epoch 14192: Training Loss: 0.09117428958415985 Validation Loss: 0.8264364004135132\n",
      "Epoch 14193: Training Loss: 0.09128022193908691 Validation Loss: 0.8263131380081177\n",
      "Epoch 14194: Training Loss: 0.09098487347364426 Validation Loss: 0.8258126378059387\n",
      "Epoch 14195: Training Loss: 0.0910134141643842 Validation Loss: 0.8265619874000549\n",
      "Epoch 14196: Training Loss: 0.09127259999513626 Validation Loss: 0.8268107175827026\n",
      "Epoch 14197: Training Loss: 0.09095504383246104 Validation Loss: 0.8276211023330688\n",
      "Epoch 14198: Training Loss: 0.09117890397707622 Validation Loss: 0.827355682849884\n",
      "Epoch 14199: Training Loss: 0.09058160086472829 Validation Loss: 0.8259036540985107\n",
      "Epoch 14200: Training Loss: 0.09155390659968059 Validation Loss: 0.8258114457130432\n",
      "Epoch 14201: Training Loss: 0.09101437032222748 Validation Loss: 0.8262575268745422\n",
      "Epoch 14202: Training Loss: 0.09081049760182698 Validation Loss: 0.8263148665428162\n",
      "Epoch 14203: Training Loss: 0.09135941416025162 Validation Loss: 0.8270381093025208\n",
      "Epoch 14204: Training Loss: 0.09105148414770763 Validation Loss: 0.8262482285499573\n",
      "Epoch 14205: Training Loss: 0.09090909610191981 Validation Loss: 0.8263726830482483\n",
      "Epoch 14206: Training Loss: 0.09088777750730515 Validation Loss: 0.8265364170074463\n",
      "Epoch 14207: Training Loss: 0.09106430162986119 Validation Loss: 0.8269496560096741\n",
      "Epoch 14208: Training Loss: 0.09084882587194443 Validation Loss: 0.8266692757606506\n",
      "Epoch 14209: Training Loss: 0.09120913594961166 Validation Loss: 0.8269546031951904\n",
      "Epoch 14210: Training Loss: 0.0909397229552269 Validation Loss: 0.8270045518875122\n",
      "Epoch 14211: Training Loss: 0.09140292058388393 Validation Loss: 0.8266704678535461\n",
      "Epoch 14212: Training Loss: 0.09101894249518712 Validation Loss: 0.8259351849555969\n",
      "Epoch 14213: Training Loss: 0.09063278635342915 Validation Loss: 0.8261494636535645\n",
      "Epoch 14214: Training Loss: 0.09136437624692917 Validation Loss: 0.8268755078315735\n",
      "Epoch 14215: Training Loss: 0.090855007370313 Validation Loss: 0.8278054594993591\n",
      "Epoch 14216: Training Loss: 0.09120100736618042 Validation Loss: 0.8268011808395386\n",
      "Epoch 14217: Training Loss: 0.09091563771168391 Validation Loss: 0.8265038728713989\n",
      "Epoch 14218: Training Loss: 0.09090433269739151 Validation Loss: 0.8259965777397156\n",
      "Epoch 14219: Training Loss: 0.09066200504700343 Validation Loss: 0.8267050981521606\n",
      "Epoch 14220: Training Loss: 0.09112006425857544 Validation Loss: 0.8275247812271118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14221: Training Loss: 0.09168563534816106 Validation Loss: 0.8267503380775452\n",
      "Epoch 14222: Training Loss: 0.09103779743115108 Validation Loss: 0.8260690569877625\n",
      "Epoch 14223: Training Loss: 0.09079456329345703 Validation Loss: 0.8262909054756165\n",
      "Epoch 14224: Training Loss: 0.09052072713772456 Validation Loss: 0.827133297920227\n",
      "Epoch 14225: Training Loss: 0.09081628421942393 Validation Loss: 0.8264440894126892\n",
      "Epoch 14226: Training Loss: 0.09050059070189793 Validation Loss: 0.8260660767555237\n",
      "Epoch 14227: Training Loss: 0.09086392819881439 Validation Loss: 0.8266224265098572\n",
      "Epoch 14228: Training Loss: 0.09071772793928783 Validation Loss: 0.826732873916626\n",
      "Epoch 14229: Training Loss: 0.09093573937813441 Validation Loss: 0.8277648687362671\n",
      "Epoch 14230: Training Loss: 0.09126483152310054 Validation Loss: 0.828542172908783\n",
      "Epoch 14231: Training Loss: 0.09076840927203496 Validation Loss: 0.8281843066215515\n",
      "Epoch 14232: Training Loss: 0.09090278049310048 Validation Loss: 0.8273701667785645\n",
      "Epoch 14233: Training Loss: 0.09074168652296066 Validation Loss: 0.8268380165100098\n",
      "Epoch 14234: Training Loss: 0.09096137930949529 Validation Loss: 0.8274450302124023\n",
      "Epoch 14235: Training Loss: 0.09077166765928268 Validation Loss: 0.8268876671791077\n",
      "Epoch 14236: Training Loss: 0.09070662160714467 Validation Loss: 0.8273367881774902\n",
      "Epoch 14237: Training Loss: 0.09066468973954518 Validation Loss: 0.8270115256309509\n",
      "Epoch 14238: Training Loss: 0.09106205900510152 Validation Loss: 0.8268099427223206\n",
      "Epoch 14239: Training Loss: 0.0910790537794431 Validation Loss: 0.8264896869659424\n",
      "Epoch 14240: Training Loss: 0.09081482390562694 Validation Loss: 0.8257390260696411\n",
      "Epoch 14241: Training Loss: 0.09084053337574005 Validation Loss: 0.8260098695755005\n",
      "Epoch 14242: Training Loss: 0.0908425177137057 Validation Loss: 0.8262742757797241\n",
      "Epoch 14243: Training Loss: 0.09072770675023396 Validation Loss: 0.8266913890838623\n",
      "Epoch 14244: Training Loss: 0.09085627893606822 Validation Loss: 0.8271881341934204\n",
      "Epoch 14245: Training Loss: 0.09061283866564433 Validation Loss: 0.8266087770462036\n",
      "Epoch 14246: Training Loss: 0.09080365300178528 Validation Loss: 0.827241063117981\n",
      "Epoch 14247: Training Loss: 0.09071847548087437 Validation Loss: 0.8263838887214661\n",
      "Epoch 14248: Training Loss: 0.0914029876391093 Validation Loss: 0.8258885145187378\n",
      "Epoch 14249: Training Loss: 0.09072032322486241 Validation Loss: 0.8263595104217529\n",
      "Epoch 14250: Training Loss: 0.09091829011837642 Validation Loss: 0.827968180179596\n",
      "Epoch 14251: Training Loss: 0.09067381670077641 Validation Loss: 0.8278297185897827\n",
      "Epoch 14252: Training Loss: 0.0908457338809967 Validation Loss: 0.8275408744812012\n",
      "Epoch 14253: Training Loss: 0.09074494491020839 Validation Loss: 0.8275964856147766\n",
      "Epoch 14254: Training Loss: 0.09066208948691686 Validation Loss: 0.8276366591453552\n",
      "Epoch 14255: Training Loss: 0.09079669415950775 Validation Loss: 0.8278911709785461\n",
      "Epoch 14256: Training Loss: 0.0907477264602979 Validation Loss: 0.8273134827613831\n",
      "Epoch 14257: Training Loss: 0.09090808282295863 Validation Loss: 0.8274211287498474\n",
      "Epoch 14258: Training Loss: 0.09031099577744801 Validation Loss: 0.8272985219955444\n",
      "Epoch 14259: Training Loss: 0.09050500144561131 Validation Loss: 0.8262527585029602\n",
      "Epoch 14260: Training Loss: 0.0907146433989207 Validation Loss: 0.8268427848815918\n",
      "Epoch 14261: Training Loss: 0.09081629415353139 Validation Loss: 0.8265947103500366\n",
      "Epoch 14262: Training Loss: 0.09067658086617787 Validation Loss: 0.8259049654006958\n",
      "Epoch 14263: Training Loss: 0.09103050331274669 Validation Loss: 0.826427161693573\n",
      "Epoch 14264: Training Loss: 0.09084193656841914 Validation Loss: 0.8268380165100098\n",
      "Epoch 14265: Training Loss: 0.09067954123020172 Validation Loss: 0.8267361521720886\n",
      "Epoch 14266: Training Loss: 0.09131319572528203 Validation Loss: 0.8280065059661865\n",
      "Epoch 14267: Training Loss: 0.09084077676137288 Validation Loss: 0.8270495533943176\n",
      "Epoch 14268: Training Loss: 0.09099753946065903 Validation Loss: 0.8265730738639832\n",
      "Epoch 14269: Training Loss: 0.09071899702151616 Validation Loss: 0.8262677192687988\n",
      "Epoch 14270: Training Loss: 0.09102572252353032 Validation Loss: 0.8262050747871399\n",
      "Epoch 14271: Training Loss: 0.09082117428382237 Validation Loss: 0.8277758955955505\n",
      "Epoch 14272: Training Loss: 0.09088945140441258 Validation Loss: 0.8290343880653381\n",
      "Epoch 14273: Training Loss: 0.09072793275117874 Validation Loss: 0.8296563625335693\n",
      "Epoch 14274: Training Loss: 0.09148566921552022 Validation Loss: 0.8294533491134644\n",
      "Epoch 14275: Training Loss: 0.09059559057156245 Validation Loss: 0.8284762501716614\n",
      "Epoch 14276: Training Loss: 0.09097043673197429 Validation Loss: 0.8272968530654907\n",
      "Epoch 14277: Training Loss: 0.09068529307842255 Validation Loss: 0.8264380693435669\n",
      "Epoch 14278: Training Loss: 0.09081373860438664 Validation Loss: 0.8264564871788025\n",
      "Epoch 14279: Training Loss: 0.09069895495971043 Validation Loss: 0.8280987739562988\n",
      "Epoch 14280: Training Loss: 0.0907807524005572 Validation Loss: 0.8291463851928711\n",
      "Epoch 14281: Training Loss: 0.0912072906891505 Validation Loss: 0.8286663889884949\n",
      "Epoch 14282: Training Loss: 0.09133019546667735 Validation Loss: 0.8275327086448669\n",
      "Epoch 14283: Training Loss: 0.09085315962632497 Validation Loss: 0.8264204859733582\n",
      "Epoch 14284: Training Loss: 0.09061414003372192 Validation Loss: 0.8262249827384949\n",
      "Epoch 14285: Training Loss: 0.09074823806683223 Validation Loss: 0.8265843987464905\n",
      "Epoch 14286: Training Loss: 0.09087878465652466 Validation Loss: 0.8265966773033142\n",
      "Epoch 14287: Training Loss: 0.09064196298519771 Validation Loss: 0.8277249932289124\n",
      "Epoch 14288: Training Loss: 0.09050222982962926 Validation Loss: 0.8283655047416687\n",
      "Epoch 14289: Training Loss: 0.09142328053712845 Validation Loss: 0.827848494052887\n",
      "Epoch 14290: Training Loss: 0.09155608713626862 Validation Loss: 0.826383650302887\n",
      "Epoch 14291: Training Loss: 0.09080018351475398 Validation Loss: 0.8269889950752258\n",
      "Epoch 14292: Training Loss: 0.09048142532507579 Validation Loss: 0.8277392983436584\n",
      "Epoch 14293: Training Loss: 0.09078541646401088 Validation Loss: 0.8281932473182678\n",
      "Epoch 14294: Training Loss: 0.09067542105913162 Validation Loss: 0.8281620144844055\n",
      "Epoch 14295: Training Loss: 0.0906047448515892 Validation Loss: 0.828373908996582\n",
      "Epoch 14296: Training Loss: 0.09075208256642024 Validation Loss: 0.8286409378051758\n",
      "Epoch 14297: Training Loss: 0.09056260933478673 Validation Loss: 0.8280960917472839\n",
      "Epoch 14298: Training Loss: 0.09038772185643514 Validation Loss: 0.8271071910858154\n",
      "Epoch 14299: Training Loss: 0.09067356089750926 Validation Loss: 0.8261924386024475\n",
      "Epoch 14300: Training Loss: 0.09113396207491557 Validation Loss: 0.825999915599823\n",
      "Epoch 14301: Training Loss: 0.09077076117197673 Validation Loss: 0.8278476595878601\n",
      "Epoch 14302: Training Loss: 0.09064680586258571 Validation Loss: 0.8280865550041199\n",
      "Epoch 14303: Training Loss: 0.09059768418471019 Validation Loss: 0.8286052942276001\n",
      "Epoch 14304: Training Loss: 0.09061959634224574 Validation Loss: 0.8285979628562927\n",
      "Epoch 14305: Training Loss: 0.09079019476970036 Validation Loss: 0.8286507725715637\n",
      "Epoch 14306: Training Loss: 0.09057126194238663 Validation Loss: 0.8272450566291809\n",
      "Epoch 14307: Training Loss: 0.0905591646830241 Validation Loss: 0.8273332118988037\n",
      "Epoch 14308: Training Loss: 0.09072345246871312 Validation Loss: 0.8283295631408691\n",
      "Epoch 14309: Training Loss: 0.09087975571552913 Validation Loss: 0.8291714787483215\n",
      "Epoch 14310: Training Loss: 0.09058804313341777 Validation Loss: 0.8294088244438171\n",
      "Epoch 14311: Training Loss: 0.09052408238252004 Validation Loss: 0.8292011618614197\n",
      "Epoch 14312: Training Loss: 0.09058032929897308 Validation Loss: 0.8273041844367981\n",
      "Epoch 14313: Training Loss: 0.09046134104331334 Validation Loss: 0.8276328444480896\n",
      "Epoch 14314: Training Loss: 0.0907338485121727 Validation Loss: 0.8266855478286743\n",
      "Epoch 14315: Training Loss: 0.09063327064116795 Validation Loss: 0.8273624777793884\n",
      "Epoch 14316: Training Loss: 0.09094965209563573 Validation Loss: 0.8280925750732422\n",
      "Epoch 14317: Training Loss: 0.09079684068759282 Validation Loss: 0.828474223613739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14318: Training Loss: 0.09064365178346634 Validation Loss: 0.8285312056541443\n",
      "Epoch 14319: Training Loss: 0.09140524143973987 Validation Loss: 0.8292533755302429\n",
      "Epoch 14320: Training Loss: 0.09090323994557063 Validation Loss: 0.8273075222969055\n",
      "Epoch 14321: Training Loss: 0.09064082552989323 Validation Loss: 0.8284870386123657\n",
      "Epoch 14322: Training Loss: 0.09144867211580276 Validation Loss: 0.8280256986618042\n",
      "Epoch 14323: Training Loss: 0.0905734474460284 Validation Loss: 0.828141450881958\n",
      "Epoch 14324: Training Loss: 0.0907582404712836 Validation Loss: 0.8277071118354797\n",
      "Epoch 14325: Training Loss: 0.09045788149038951 Validation Loss: 0.8271054029464722\n",
      "Epoch 14326: Training Loss: 0.09053365141153336 Validation Loss: 0.8268216848373413\n",
      "Epoch 14327: Training Loss: 0.09050126870473225 Validation Loss: 0.8263137936592102\n",
      "Epoch 14328: Training Loss: 0.09105969965457916 Validation Loss: 0.8275051712989807\n",
      "Epoch 14329: Training Loss: 0.09135667483011882 Validation Loss: 0.828421950340271\n",
      "Epoch 14330: Training Loss: 0.09089893599351247 Validation Loss: 0.8284398317337036\n",
      "Epoch 14331: Training Loss: 0.09059302508831024 Validation Loss: 0.8294628858566284\n",
      "Epoch 14332: Training Loss: 0.09039827187856038 Validation Loss: 0.8293110132217407\n",
      "Epoch 14333: Training Loss: 0.09074628353118896 Validation Loss: 0.828575849533081\n",
      "Epoch 14334: Training Loss: 0.09031526496013005 Validation Loss: 0.8280656933784485\n",
      "Epoch 14335: Training Loss: 0.09074849883715312 Validation Loss: 0.8276034593582153\n",
      "Epoch 14336: Training Loss: 0.0911008616288503 Validation Loss: 0.8274945616722107\n",
      "Epoch 14337: Training Loss: 0.08970784644285838 Validation Loss: 0.8282970786094666\n",
      "Epoch 14338: Training Loss: 0.0905165895819664 Validation Loss: 0.8290773034095764\n",
      "Epoch 14339: Training Loss: 0.09099406997362773 Validation Loss: 0.8292083740234375\n",
      "Epoch 14340: Training Loss: 0.09056171774864197 Validation Loss: 0.8276312351226807\n",
      "Epoch 14341: Training Loss: 0.09044031550486882 Validation Loss: 0.8272339701652527\n",
      "Epoch 14342: Training Loss: 0.09051213165124257 Validation Loss: 0.8270589113235474\n",
      "Epoch 14343: Training Loss: 0.09066093961397807 Validation Loss: 0.8267953395843506\n",
      "Epoch 14344: Training Loss: 0.09124836573998134 Validation Loss: 0.8268740177154541\n",
      "Epoch 14345: Training Loss: 0.09040111054976781 Validation Loss: 0.8284114599227905\n",
      "Epoch 14346: Training Loss: 0.09040716290473938 Validation Loss: 0.8288396000862122\n",
      "Epoch 14347: Training Loss: 0.09047841032346089 Validation Loss: 0.8298656344413757\n",
      "Epoch 14348: Training Loss: 0.0906430830558141 Validation Loss: 0.8295104503631592\n",
      "Epoch 14349: Training Loss: 0.09086877355972926 Validation Loss: 0.8299152851104736\n",
      "Epoch 14350: Training Loss: 0.09063832710186641 Validation Loss: 0.8278489112854004\n",
      "Epoch 14351: Training Loss: 0.0908288632829984 Validation Loss: 0.8266656398773193\n",
      "Epoch 14352: Training Loss: 0.09093756477038066 Validation Loss: 0.8269934058189392\n",
      "Epoch 14353: Training Loss: 0.09032114098469417 Validation Loss: 0.8279025554656982\n",
      "Epoch 14354: Training Loss: 0.0903292919198672 Validation Loss: 0.8290450572967529\n",
      "Epoch 14355: Training Loss: 0.09042517344156902 Validation Loss: 0.8297320008277893\n",
      "Epoch 14356: Training Loss: 0.09045290450255077 Validation Loss: 0.8290488719940186\n",
      "Epoch 14357: Training Loss: 0.09096470723549525 Validation Loss: 0.8279299736022949\n",
      "Epoch 14358: Training Loss: 0.09051466981569926 Validation Loss: 0.8269854784011841\n",
      "Epoch 14359: Training Loss: 0.09058636923631032 Validation Loss: 0.8271222114562988\n",
      "Epoch 14360: Training Loss: 0.09035418430964152 Validation Loss: 0.828511118888855\n",
      "Epoch 14361: Training Loss: 0.09030535817146301 Validation Loss: 0.8285135626792908\n",
      "Epoch 14362: Training Loss: 0.09043024728695552 Validation Loss: 0.8293896913528442\n",
      "Epoch 14363: Training Loss: 0.09059671560923259 Validation Loss: 0.8286914825439453\n",
      "Epoch 14364: Training Loss: 0.09026070684194565 Validation Loss: 0.8282325863838196\n",
      "Epoch 14365: Training Loss: 0.09067202856143315 Validation Loss: 0.826699435710907\n",
      "Epoch 14366: Training Loss: 0.09076763192812602 Validation Loss: 0.8272601962089539\n",
      "Epoch 14367: Training Loss: 0.09044978519280751 Validation Loss: 0.8281960487365723\n",
      "Epoch 14368: Training Loss: 0.09070679793755214 Validation Loss: 0.8294018507003784\n",
      "Epoch 14369: Training Loss: 0.09078557789325714 Validation Loss: 0.82987380027771\n",
      "Epoch 14370: Training Loss: 0.09020178268353145 Validation Loss: 0.8288971185684204\n",
      "Epoch 14371: Training Loss: 0.09052432825167973 Validation Loss: 0.8277791738510132\n",
      "Epoch 14372: Training Loss: 0.09090751161177953 Validation Loss: 0.8277703523635864\n",
      "Epoch 14373: Training Loss: 0.0905331348379453 Validation Loss: 0.8291869163513184\n",
      "Epoch 14374: Training Loss: 0.0907658040523529 Validation Loss: 0.8297455906867981\n",
      "Epoch 14375: Training Loss: 0.09048280616601308 Validation Loss: 0.8288105726242065\n",
      "Epoch 14376: Training Loss: 0.09056954582532246 Validation Loss: 0.8298965692520142\n",
      "Epoch 14377: Training Loss: 0.09040794024864833 Validation Loss: 0.8281010985374451\n",
      "Epoch 14378: Training Loss: 0.09041887025038402 Validation Loss: 0.8272972106933594\n",
      "Epoch 14379: Training Loss: 0.09031700839598973 Validation Loss: 0.8276117444038391\n",
      "Epoch 14380: Training Loss: 0.09025613466898601 Validation Loss: 0.8278363347053528\n",
      "Epoch 14381: Training Loss: 0.09054418901602428 Validation Loss: 0.8288037180900574\n",
      "Epoch 14382: Training Loss: 0.09082664797703426 Validation Loss: 0.8290935158729553\n",
      "Epoch 14383: Training Loss: 0.09073516974846522 Validation Loss: 0.8285849690437317\n",
      "Epoch 14384: Training Loss: 0.0907992571592331 Validation Loss: 0.8296873569488525\n",
      "Epoch 14385: Training Loss: 0.09023333340883255 Validation Loss: 0.8292697668075562\n",
      "Epoch 14386: Training Loss: 0.09033556282520294 Validation Loss: 0.8287153840065002\n",
      "Epoch 14387: Training Loss: 0.09020297477642696 Validation Loss: 0.8293094635009766\n",
      "Epoch 14388: Training Loss: 0.09021174659331639 Validation Loss: 0.8284417390823364\n",
      "Epoch 14389: Training Loss: 0.09039391080538432 Validation Loss: 0.8282098770141602\n",
      "Epoch 14390: Training Loss: 0.09075829883416493 Validation Loss: 0.8282219171524048\n",
      "Epoch 14391: Training Loss: 0.0901519979039828 Validation Loss: 0.828432023525238\n",
      "Epoch 14392: Training Loss: 0.0905820628007253 Validation Loss: 0.8287099003791809\n",
      "Epoch 14393: Training Loss: 0.09076123684644699 Validation Loss: 0.8266122937202454\n",
      "Epoch 14394: Training Loss: 0.09034577012062073 Validation Loss: 0.8270362615585327\n",
      "Epoch 14395: Training Loss: 0.09020605186621349 Validation Loss: 0.8273101449012756\n",
      "Epoch 14396: Training Loss: 0.09037759403387706 Validation Loss: 0.8282737731933594\n",
      "Epoch 14397: Training Loss: 0.09030089279015859 Validation Loss: 0.8281942009925842\n",
      "Epoch 14398: Training Loss: 0.09048526734113693 Validation Loss: 0.8291456699371338\n",
      "Epoch 14399: Training Loss: 0.09056483705838521 Validation Loss: 0.8279609084129333\n",
      "Epoch 14400: Training Loss: 0.09034421543280284 Validation Loss: 0.8279917240142822\n",
      "Epoch 14401: Training Loss: 0.09031197428703308 Validation Loss: 0.8282467126846313\n",
      "Epoch 14402: Training Loss: 0.09090746690829594 Validation Loss: 0.8296386003494263\n",
      "Epoch 14403: Training Loss: 0.090847613910834 Validation Loss: 0.8313117027282715\n",
      "Epoch 14404: Training Loss: 0.09037519246339798 Validation Loss: 0.8311475515365601\n",
      "Epoch 14405: Training Loss: 0.09015690783659618 Validation Loss: 0.8310387134552002\n",
      "Epoch 14406: Training Loss: 0.09035779784123103 Validation Loss: 0.8303526043891907\n",
      "Epoch 14407: Training Loss: 0.09036702911059062 Validation Loss: 0.8295209407806396\n",
      "Epoch 14408: Training Loss: 0.09073769052823384 Validation Loss: 0.82756507396698\n",
      "Epoch 14409: Training Loss: 0.09013824661572774 Validation Loss: 0.8279251456260681\n",
      "Epoch 14410: Training Loss: 0.09018929302692413 Validation Loss: 0.8282332420349121\n",
      "Epoch 14411: Training Loss: 0.0904191608230273 Validation Loss: 0.8286698460578918\n",
      "Epoch 14412: Training Loss: 0.0901985044280688 Validation Loss: 0.8287873268127441\n",
      "Epoch 14413: Training Loss: 0.09020945926507314 Validation Loss: 0.8287774920463562\n",
      "Epoch 14414: Training Loss: 0.09032888462146123 Validation Loss: 0.8288083672523499\n",
      "Epoch 14415: Training Loss: 0.09027907252311707 Validation Loss: 0.828400194644928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14416: Training Loss: 0.09020040184259415 Validation Loss: 0.8280088305473328\n",
      "Epoch 14417: Training Loss: 0.08995461463928223 Validation Loss: 0.8276601433753967\n",
      "Epoch 14418: Training Loss: 0.09057783583799998 Validation Loss: 0.8274462223052979\n",
      "Epoch 14419: Training Loss: 0.09024903178215027 Validation Loss: 0.8288240432739258\n",
      "Epoch 14420: Training Loss: 0.09073543796936671 Validation Loss: 0.8307294845581055\n",
      "Epoch 14421: Training Loss: 0.09041805813709895 Validation Loss: 0.8316827416419983\n",
      "Epoch 14422: Training Loss: 0.09032566845417023 Validation Loss: 0.8307578563690186\n",
      "Epoch 14423: Training Loss: 0.09027242660522461 Validation Loss: 0.8295546174049377\n",
      "Epoch 14424: Training Loss: 0.09040471414724986 Validation Loss: 0.8293235898017883\n",
      "Epoch 14425: Training Loss: 0.09064475943644841 Validation Loss: 0.8280215263366699\n",
      "Epoch 14426: Training Loss: 0.09023004521926244 Validation Loss: 0.8283824324607849\n",
      "Epoch 14427: Training Loss: 0.09039402504762013 Validation Loss: 0.8287826776504517\n",
      "Epoch 14428: Training Loss: 0.09066614011923473 Validation Loss: 0.8285853266716003\n",
      "Epoch 14429: Training Loss: 0.0902727022767067 Validation Loss: 0.8293209075927734\n",
      "Epoch 14430: Training Loss: 0.09018150220314662 Validation Loss: 0.8284701108932495\n",
      "Epoch 14431: Training Loss: 0.09021039555470149 Validation Loss: 0.8288843035697937\n",
      "Epoch 14432: Training Loss: 0.09089652200539906 Validation Loss: 0.8300464153289795\n",
      "Epoch 14433: Training Loss: 0.09024562189976375 Validation Loss: 0.8303928375244141\n",
      "Epoch 14434: Training Loss: 0.09048850337664287 Validation Loss: 0.8282588124275208\n",
      "Epoch 14435: Training Loss: 0.09062195817629497 Validation Loss: 0.8275038599967957\n",
      "Epoch 14436: Training Loss: 0.0901880885163943 Validation Loss: 0.8280731439590454\n",
      "Epoch 14437: Training Loss: 0.09036691735188167 Validation Loss: 0.8293132781982422\n",
      "Epoch 14438: Training Loss: 0.09013706197341283 Validation Loss: 0.829630970954895\n",
      "Epoch 14439: Training Loss: 0.09004014482100804 Validation Loss: 0.8297176957130432\n",
      "Epoch 14440: Training Loss: 0.08980058133602142 Validation Loss: 0.8298638463020325\n",
      "Epoch 14441: Training Loss: 0.0902812232573827 Validation Loss: 0.8286459445953369\n",
      "Epoch 14442: Training Loss: 0.09009057283401489 Validation Loss: 0.8283935785293579\n",
      "Epoch 14443: Training Loss: 0.09043797602256139 Validation Loss: 0.8279792666435242\n",
      "Epoch 14444: Training Loss: 0.0916546881198883 Validation Loss: 0.8286094069480896\n",
      "Epoch 14445: Training Loss: 0.08996101717154185 Validation Loss: 0.8290247917175293\n",
      "Epoch 14446: Training Loss: 0.09015652288993199 Validation Loss: 0.8303728699684143\n",
      "Epoch 14447: Training Loss: 0.09064429998397827 Validation Loss: 0.8306775093078613\n",
      "Epoch 14448: Training Loss: 0.0907535453637441 Validation Loss: 0.8297649025917053\n",
      "Epoch 14449: Training Loss: 0.09023566544055939 Validation Loss: 0.8294798731803894\n",
      "Epoch 14450: Training Loss: 0.09068896621465683 Validation Loss: 0.8282354474067688\n",
      "Epoch 14451: Training Loss: 0.09035455683867137 Validation Loss: 0.8280763030052185\n",
      "Epoch 14452: Training Loss: 0.0901775707801183 Validation Loss: 0.8292019963264465\n",
      "Epoch 14453: Training Loss: 0.09018263717492421 Validation Loss: 0.8300135135650635\n",
      "Epoch 14454: Training Loss: 0.09056653082370758 Validation Loss: 0.8306977152824402\n",
      "Epoch 14455: Training Loss: 0.09001992146174113 Validation Loss: 0.8304640650749207\n",
      "Epoch 14456: Training Loss: 0.09002390007177989 Validation Loss: 0.829427182674408\n",
      "Epoch 14457: Training Loss: 0.09021511922279994 Validation Loss: 0.8297784328460693\n",
      "Epoch 14458: Training Loss: 0.0910269816716512 Validation Loss: 0.8297439813613892\n",
      "Epoch 14459: Training Loss: 0.09016034255425136 Validation Loss: 0.8288931250572205\n",
      "Epoch 14460: Training Loss: 0.09005903949340184 Validation Loss: 0.8287864327430725\n",
      "Epoch 14461: Training Loss: 0.09002031634251277 Validation Loss: 0.8293875455856323\n",
      "Epoch 14462: Training Loss: 0.09009942909081776 Validation Loss: 0.8290510177612305\n",
      "Epoch 14463: Training Loss: 0.09079620987176895 Validation Loss: 0.8279213309288025\n",
      "Epoch 14464: Training Loss: 0.09037260214487712 Validation Loss: 0.8286519646644592\n",
      "Epoch 14465: Training Loss: 0.09000654021898906 Validation Loss: 0.8295415043830872\n",
      "Epoch 14466: Training Loss: 0.09013166775306065 Validation Loss: 0.8296612501144409\n",
      "Epoch 14467: Training Loss: 0.0902223934729894 Validation Loss: 0.8294825553894043\n",
      "Epoch 14468: Training Loss: 0.09034688770771027 Validation Loss: 0.8297324776649475\n",
      "Epoch 14469: Training Loss: 0.09022190173467 Validation Loss: 0.8303478360176086\n",
      "Epoch 14470: Training Loss: 0.0902365470925967 Validation Loss: 0.8291327953338623\n",
      "Epoch 14471: Training Loss: 0.0900892789165179 Validation Loss: 0.8287549018859863\n",
      "Epoch 14472: Training Loss: 0.08994481712579727 Validation Loss: 0.8283137679100037\n",
      "Epoch 14473: Training Loss: 0.09012337774038315 Validation Loss: 0.8295766711235046\n",
      "Epoch 14474: Training Loss: 0.0904026875893275 Validation Loss: 0.8291082978248596\n",
      "Epoch 14475: Training Loss: 0.09007694323857625 Validation Loss: 0.8281747698783875\n",
      "Epoch 14476: Training Loss: 0.0904216468334198 Validation Loss: 0.829188346862793\n",
      "Epoch 14477: Training Loss: 0.09001309424638748 Validation Loss: 0.8304911255836487\n",
      "Epoch 14478: Training Loss: 0.09018977731466293 Validation Loss: 0.8305047750473022\n",
      "Epoch 14479: Training Loss: 0.09026547024647395 Validation Loss: 0.8292511701583862\n",
      "Epoch 14480: Training Loss: 0.09050162384907405 Validation Loss: 0.8291895985603333\n",
      "Epoch 14481: Training Loss: 0.09006740401188533 Validation Loss: 0.8295353651046753\n",
      "Epoch 14482: Training Loss: 0.0902823805809021 Validation Loss: 0.8289031982421875\n",
      "Epoch 14483: Training Loss: 0.08996175477902095 Validation Loss: 0.8293916583061218\n",
      "Epoch 14484: Training Loss: 0.08978050698836644 Validation Loss: 0.8305146098136902\n",
      "Epoch 14485: Training Loss: 0.0904572606086731 Validation Loss: 0.8312638998031616\n",
      "Epoch 14486: Training Loss: 0.09011938174565633 Validation Loss: 0.8314299583435059\n",
      "Epoch 14487: Training Loss: 0.08984799434741338 Validation Loss: 0.8308219909667969\n",
      "Epoch 14488: Training Loss: 0.08997628341118495 Validation Loss: 0.8290213346481323\n",
      "Epoch 14489: Training Loss: 0.08998529116312663 Validation Loss: 0.8289985656738281\n",
      "Epoch 14490: Training Loss: 0.09012330075105031 Validation Loss: 0.8291329145431519\n",
      "Epoch 14491: Training Loss: 0.0902633269627889 Validation Loss: 0.8286964893341064\n",
      "Epoch 14492: Training Loss: 0.09007814278205235 Validation Loss: 0.829813539981842\n",
      "Epoch 14493: Training Loss: 0.08998845269282658 Validation Loss: 0.8296021223068237\n",
      "Epoch 14494: Training Loss: 0.0901646042863528 Validation Loss: 0.8307597041130066\n",
      "Epoch 14495: Training Loss: 0.08988476544618607 Validation Loss: 0.8308230042457581\n",
      "Epoch 14496: Training Loss: 0.0897345244884491 Validation Loss: 0.8298407793045044\n",
      "Epoch 14497: Training Loss: 0.08948562542597453 Validation Loss: 0.8291805386543274\n",
      "Epoch 14498: Training Loss: 0.09043755133946736 Validation Loss: 0.8292587995529175\n",
      "Epoch 14499: Training Loss: 0.09000847985347112 Validation Loss: 0.8290149569511414\n",
      "Epoch 14500: Training Loss: 0.09014879415432613 Validation Loss: 0.8291938900947571\n",
      "Epoch 14501: Training Loss: 0.0900011161963145 Validation Loss: 0.83070969581604\n",
      "Epoch 14502: Training Loss: 0.09012488772471745 Validation Loss: 0.8298545479774475\n",
      "Epoch 14503: Training Loss: 0.09004214902718861 Validation Loss: 0.8309900164604187\n",
      "Epoch 14504: Training Loss: 0.09017638365427653 Validation Loss: 0.8293084502220154\n",
      "Epoch 14505: Training Loss: 0.09018784513076146 Validation Loss: 0.8300992250442505\n",
      "Epoch 14506: Training Loss: 0.0899992436170578 Validation Loss: 0.8298512697219849\n",
      "Epoch 14507: Training Loss: 0.09034322947263718 Validation Loss: 0.8309772610664368\n",
      "Epoch 14508: Training Loss: 0.09020943939685822 Validation Loss: 0.8321555852890015\n",
      "Epoch 14509: Training Loss: 0.08979526907205582 Validation Loss: 0.8314239978790283\n",
      "Epoch 14510: Training Loss: 0.09129366775353749 Validation Loss: 0.8306030631065369\n",
      "Epoch 14511: Training Loss: 0.08975607653458913 Validation Loss: 0.830116331577301\n",
      "Epoch 14512: Training Loss: 0.08996663987636566 Validation Loss: 0.8299347758293152\n",
      "Epoch 14513: Training Loss: 0.09033497671286266 Validation Loss: 0.8289940357208252\n",
      "Epoch 14514: Training Loss: 0.09046945969263713 Validation Loss: 0.8282304406166077\n",
      "Epoch 14515: Training Loss: 0.0907472347219785 Validation Loss: 0.8315673470497131\n",
      "Epoch 14516: Training Loss: 0.09011176228523254 Validation Loss: 0.8319616317749023\n",
      "Epoch 14517: Training Loss: 0.08988132327795029 Validation Loss: 0.8299633860588074\n",
      "Epoch 14518: Training Loss: 0.08997734636068344 Validation Loss: 0.82960045337677\n",
      "Epoch 14519: Training Loss: 0.08984977751970291 Validation Loss: 0.8297750353813171\n",
      "Epoch 14520: Training Loss: 0.0899086445569992 Validation Loss: 0.8294677734375\n",
      "Epoch 14521: Training Loss: 0.09000125775734584 Validation Loss: 0.8295331597328186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14522: Training Loss: 0.08987815181414287 Validation Loss: 0.8298935890197754\n",
      "Epoch 14523: Training Loss: 0.09029505401849747 Validation Loss: 0.8312031030654907\n",
      "Epoch 14524: Training Loss: 0.08991867552200954 Validation Loss: 0.8313743472099304\n",
      "Epoch 14525: Training Loss: 0.08991567542155583 Validation Loss: 0.8310025334358215\n",
      "Epoch 14526: Training Loss: 0.08993158489465714 Validation Loss: 0.8297515511512756\n",
      "Epoch 14527: Training Loss: 0.08985071629285812 Validation Loss: 0.8286849856376648\n",
      "Epoch 14528: Training Loss: 0.09008740882078807 Validation Loss: 0.8284350037574768\n",
      "Epoch 14529: Training Loss: 0.09010942528645198 Validation Loss: 0.8288527131080627\n",
      "Epoch 14530: Training Loss: 0.08969485263029735 Validation Loss: 0.8290420770645142\n",
      "Epoch 14531: Training Loss: 0.0899741401274999 Validation Loss: 0.830662727355957\n",
      "Epoch 14532: Training Loss: 0.09000901877880096 Validation Loss: 0.831555187702179\n",
      "Epoch 14533: Training Loss: 0.09011254211266835 Validation Loss: 0.8314653635025024\n",
      "Epoch 14534: Training Loss: 0.08990011115868886 Validation Loss: 0.8297223448753357\n",
      "Epoch 14535: Training Loss: 0.08999318381150563 Validation Loss: 0.8289298415184021\n",
      "Epoch 14536: Training Loss: 0.09003705779711406 Validation Loss: 0.8300169706344604\n",
      "Epoch 14537: Training Loss: 0.09013799081246059 Validation Loss: 0.8308088183403015\n",
      "Epoch 14538: Training Loss: 0.08994501829147339 Validation Loss: 0.8305942416191101\n",
      "Epoch 14539: Training Loss: 0.09018717209498088 Validation Loss: 0.8314840197563171\n",
      "Epoch 14540: Training Loss: 0.0900141770641009 Validation Loss: 0.8309068083763123\n",
      "Epoch 14541: Training Loss: 0.09011722356081009 Validation Loss: 0.8309686183929443\n",
      "Epoch 14542: Training Loss: 0.09013152370850246 Validation Loss: 0.8291165232658386\n",
      "Epoch 14543: Training Loss: 0.09024100005626678 Validation Loss: 0.8300724029541016\n",
      "Epoch 14544: Training Loss: 0.0896292229493459 Validation Loss: 0.8298770189285278\n",
      "Epoch 14545: Training Loss: 0.09032246718804042 Validation Loss: 0.8303247094154358\n",
      "Epoch 14546: Training Loss: 0.08993980785210927 Validation Loss: 0.8306187391281128\n",
      "Epoch 14547: Training Loss: 0.09010475377241771 Validation Loss: 0.8300325870513916\n",
      "Epoch 14548: Training Loss: 0.08997330317894618 Validation Loss: 0.8298237919807434\n",
      "Epoch 14549: Training Loss: 0.08981199562549591 Validation Loss: 0.830168604850769\n",
      "Epoch 14550: Training Loss: 0.09034014244874318 Validation Loss: 0.8299174904823303\n",
      "Epoch 14551: Training Loss: 0.08995756506919861 Validation Loss: 0.8291724324226379\n",
      "Epoch 14552: Training Loss: 0.09015341351429622 Validation Loss: 0.829983115196228\n",
      "Epoch 14553: Training Loss: 0.09056786199410756 Validation Loss: 0.8295411467552185\n",
      "Epoch 14554: Training Loss: 0.08982038249572118 Validation Loss: 0.830826997756958\n",
      "Epoch 14555: Training Loss: 0.09021726499001186 Validation Loss: 0.8320423364639282\n",
      "Epoch 14556: Training Loss: 0.09011013805866241 Validation Loss: 0.8311488032341003\n",
      "Epoch 14557: Training Loss: 0.08978852132956187 Validation Loss: 0.8305351138114929\n",
      "Epoch 14558: Training Loss: 0.09021920959154765 Validation Loss: 0.8294099569320679\n",
      "Epoch 14559: Training Loss: 0.08997339755296707 Validation Loss: 0.8304498195648193\n",
      "Epoch 14560: Training Loss: 0.08986211071411769 Validation Loss: 0.8322862982749939\n",
      "Epoch 14561: Training Loss: 0.09001207848389943 Validation Loss: 0.8311898112297058\n",
      "Epoch 14562: Training Loss: 0.09018266946077347 Validation Loss: 0.8313805460929871\n",
      "Epoch 14563: Training Loss: 0.08988682180643082 Validation Loss: 0.8305632472038269\n",
      "Epoch 14564: Training Loss: 0.09050661077102025 Validation Loss: 0.8305630683898926\n",
      "Epoch 14565: Training Loss: 0.08959148575862248 Validation Loss: 0.8311313390731812\n",
      "Epoch 14566: Training Loss: 0.08969381948312123 Validation Loss: 0.8305866718292236\n",
      "Epoch 14567: Training Loss: 0.09012243151664734 Validation Loss: 0.8294823169708252\n",
      "Epoch 14568: Training Loss: 0.09006967643896739 Validation Loss: 0.8308333158493042\n",
      "Epoch 14569: Training Loss: 0.08997443566719691 Validation Loss: 0.8303720355033875\n",
      "Epoch 14570: Training Loss: 0.08989644298950832 Validation Loss: 0.8298736810684204\n",
      "Epoch 14571: Training Loss: 0.0900206242998441 Validation Loss: 0.8296215534210205\n",
      "Epoch 14572: Training Loss: 0.08956934263308843 Validation Loss: 0.8303894996643066\n",
      "Epoch 14573: Training Loss: 0.08990708490212758 Validation Loss: 0.8318316340446472\n",
      "Epoch 14574: Training Loss: 0.08996007094780605 Validation Loss: 0.8319624662399292\n",
      "Epoch 14575: Training Loss: 0.09004183361927669 Validation Loss: 0.8323572874069214\n",
      "Epoch 14576: Training Loss: 0.08986058582862218 Validation Loss: 0.8311240673065186\n",
      "Epoch 14577: Training Loss: 0.09030766288439433 Validation Loss: 0.8301789164543152\n",
      "Epoch 14578: Training Loss: 0.08971705784400304 Validation Loss: 0.8301271200180054\n",
      "Epoch 14579: Training Loss: 0.0899231160680453 Validation Loss: 0.8301639556884766\n",
      "Epoch 14580: Training Loss: 0.09064724793036778 Validation Loss: 0.8305936455726624\n",
      "Epoch 14581: Training Loss: 0.08981842796007793 Validation Loss: 0.831616997718811\n",
      "Epoch 14582: Training Loss: 0.08983080089092255 Validation Loss: 0.8320996165275574\n",
      "Epoch 14583: Training Loss: 0.08988202859958012 Validation Loss: 0.8318081498146057\n",
      "Epoch 14584: Training Loss: 0.08969537913799286 Validation Loss: 0.8303337097167969\n",
      "Epoch 14585: Training Loss: 0.08970243483781815 Validation Loss: 0.8297426104545593\n",
      "Epoch 14586: Training Loss: 0.08971941967805226 Validation Loss: 0.8311858177185059\n",
      "Epoch 14587: Training Loss: 0.08967654407024384 Validation Loss: 0.8315476179122925\n",
      "Epoch 14588: Training Loss: 0.08976529041926067 Validation Loss: 0.8317092061042786\n",
      "Epoch 14589: Training Loss: 0.08976548165082932 Validation Loss: 0.8305259943008423\n",
      "Epoch 14590: Training Loss: 0.08980570485194524 Validation Loss: 0.8304085731506348\n",
      "Epoch 14591: Training Loss: 0.09043585509061813 Validation Loss: 0.8298367857933044\n",
      "Epoch 14592: Training Loss: 0.08968677371740341 Validation Loss: 0.8305834531784058\n",
      "Epoch 14593: Training Loss: 0.08969058593114217 Validation Loss: 0.8309280872344971\n",
      "Epoch 14594: Training Loss: 0.08954533437887828 Validation Loss: 0.83006352186203\n",
      "Epoch 14595: Training Loss: 0.08992089331150055 Validation Loss: 0.8307924270629883\n",
      "Epoch 14596: Training Loss: 0.08960569153229396 Validation Loss: 0.8306632041931152\n",
      "Epoch 14597: Training Loss: 0.08982165654500325 Validation Loss: 0.8309878706932068\n",
      "Epoch 14598: Training Loss: 0.08979384849468867 Validation Loss: 0.8309645056724548\n",
      "Epoch 14599: Training Loss: 0.08920103063186009 Validation Loss: 0.8311201333999634\n",
      "Epoch 14600: Training Loss: 0.08964144190152486 Validation Loss: 0.8321444392204285\n",
      "Epoch 14601: Training Loss: 0.08968776712814967 Validation Loss: 0.8320174813270569\n",
      "Epoch 14602: Training Loss: 0.09005019317070644 Validation Loss: 0.8312957882881165\n",
      "Epoch 14603: Training Loss: 0.08969178795814514 Validation Loss: 0.8314717411994934\n",
      "Epoch 14604: Training Loss: 0.08980411787827809 Validation Loss: 0.8320546746253967\n",
      "Epoch 14605: Training Loss: 0.08994607875744502 Validation Loss: 0.832715630531311\n",
      "Epoch 14606: Training Loss: 0.08965370804071426 Validation Loss: 0.8310049772262573\n",
      "Epoch 14607: Training Loss: 0.09003141025702159 Validation Loss: 0.8294680714607239\n",
      "Epoch 14608: Training Loss: 0.08952556798855464 Validation Loss: 0.830003023147583\n",
      "Epoch 14609: Training Loss: 0.08974816650152206 Validation Loss: 0.830365777015686\n",
      "Epoch 14610: Training Loss: 0.08972786118586858 Validation Loss: 0.8306649923324585\n",
      "Epoch 14611: Training Loss: 0.08988742033640544 Validation Loss: 0.8313102722167969\n",
      "Epoch 14612: Training Loss: 0.08992163836956024 Validation Loss: 0.8318981528282166\n",
      "Epoch 14613: Training Loss: 0.09067293504873912 Validation Loss: 0.8320618271827698\n",
      "Epoch 14614: Training Loss: 0.08993968119223912 Validation Loss: 0.8313100934028625\n",
      "Epoch 14615: Training Loss: 0.08960439264774323 Validation Loss: 0.8308846354484558\n",
      "Epoch 14616: Training Loss: 0.08998459080855052 Validation Loss: 0.830455482006073\n",
      "Epoch 14617: Training Loss: 0.08970445394515991 Validation Loss: 0.8299862146377563\n",
      "Epoch 14618: Training Loss: 0.08960851281881332 Validation Loss: 0.8306466341018677\n",
      "Epoch 14619: Training Loss: 0.0902648667494456 Validation Loss: 0.8329586386680603\n",
      "Epoch 14620: Training Loss: 0.08981490880250931 Validation Loss: 0.8323906064033508\n",
      "Epoch 14621: Training Loss: 0.08990700046221416 Validation Loss: 0.8304442763328552\n",
      "Epoch 14622: Training Loss: 0.08977320541938145 Validation Loss: 0.8303295373916626\n",
      "Epoch 14623: Training Loss: 0.08973777294158936 Validation Loss: 0.8307092189788818\n",
      "Epoch 14624: Training Loss: 0.0902529036005338 Validation Loss: 0.8309196829795837\n",
      "Epoch 14625: Training Loss: 0.0895636056860288 Validation Loss: 0.8309013247489929\n",
      "Epoch 14626: Training Loss: 0.09002289175987244 Validation Loss: 0.8321448564529419\n",
      "Epoch 14627: Training Loss: 0.08987366159756978 Validation Loss: 0.8313582539558411\n",
      "Epoch 14628: Training Loss: 0.08959977328777313 Validation Loss: 0.8302936553955078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14629: Training Loss: 0.08963559816281001 Validation Loss: 0.8303394913673401\n",
      "Epoch 14630: Training Loss: 0.08963357657194138 Validation Loss: 0.8306695222854614\n",
      "Epoch 14631: Training Loss: 0.08982285112142563 Validation Loss: 0.8318015336990356\n",
      "Epoch 14632: Training Loss: 0.0903268630305926 Validation Loss: 0.8316090703010559\n",
      "Epoch 14633: Training Loss: 0.08970436453819275 Validation Loss: 0.8317116498947144\n",
      "Epoch 14634: Training Loss: 0.08967841416597366 Validation Loss: 0.83141028881073\n",
      "Epoch 14635: Training Loss: 0.08997642497221629 Validation Loss: 0.8308570384979248\n",
      "Epoch 14636: Training Loss: 0.0896116445461909 Validation Loss: 0.832191526889801\n",
      "Epoch 14637: Training Loss: 0.08960949381192525 Validation Loss: 0.8321974277496338\n",
      "Epoch 14638: Training Loss: 0.08960464845101039 Validation Loss: 0.8323137760162354\n",
      "Epoch 14639: Training Loss: 0.08987535287936528 Validation Loss: 0.8308390378952026\n",
      "Epoch 14640: Training Loss: 0.08974230786164601 Validation Loss: 0.8314775228500366\n",
      "Epoch 14641: Training Loss: 0.0895786980787913 Validation Loss: 0.8323624134063721\n",
      "Epoch 14642: Training Loss: 0.09018101791540782 Validation Loss: 0.833634614944458\n",
      "Epoch 14643: Training Loss: 0.0897289291024208 Validation Loss: 0.8328809142112732\n",
      "Epoch 14644: Training Loss: 0.0896561120947202 Validation Loss: 0.8324935436248779\n",
      "Epoch 14645: Training Loss: 0.0896573265393575 Validation Loss: 0.8308958411216736\n",
      "Epoch 14646: Training Loss: 0.08950738112131755 Validation Loss: 0.8294671177864075\n",
      "Epoch 14647: Training Loss: 0.0896449734767278 Validation Loss: 0.8295526504516602\n",
      "Epoch 14648: Training Loss: 0.08981557190418243 Validation Loss: 0.8315195441246033\n",
      "Epoch 14649: Training Loss: 0.0897073894739151 Validation Loss: 0.8317309617996216\n",
      "Epoch 14650: Training Loss: 0.08975916604201 Validation Loss: 0.831570029258728\n",
      "Epoch 14651: Training Loss: 0.0894818181792895 Validation Loss: 0.8317059874534607\n",
      "Epoch 14652: Training Loss: 0.08972303320964177 Validation Loss: 0.8320217728614807\n",
      "Epoch 14653: Training Loss: 0.08949439475933711 Validation Loss: 0.8304728865623474\n",
      "Epoch 14654: Training Loss: 0.08983806769053142 Validation Loss: 0.8310244679450989\n",
      "Epoch 14655: Training Loss: 0.0899377092719078 Validation Loss: 0.8324105739593506\n",
      "Epoch 14656: Training Loss: 0.0893306980530421 Validation Loss: 0.8323781490325928\n",
      "Epoch 14657: Training Loss: 0.08973871419827144 Validation Loss: 0.8315250277519226\n",
      "Epoch 14658: Training Loss: 0.08959671606620152 Validation Loss: 0.8317824602127075\n",
      "Epoch 14659: Training Loss: 0.08966528375943501 Validation Loss: 0.8313316702842712\n",
      "Epoch 14660: Training Loss: 0.09005947162707646 Validation Loss: 0.8318219780921936\n",
      "Epoch 14661: Training Loss: 0.08967061837514241 Validation Loss: 0.8321240544319153\n",
      "Epoch 14662: Training Loss: 0.08952439328034718 Validation Loss: 0.8321323394775391\n",
      "Epoch 14663: Training Loss: 0.089056263367335 Validation Loss: 0.8305038809776306\n",
      "Epoch 14664: Training Loss: 0.09000693758328755 Validation Loss: 0.8304573893547058\n",
      "Epoch 14665: Training Loss: 0.08965774128834407 Validation Loss: 0.8304516077041626\n",
      "Epoch 14666: Training Loss: 0.0900201400121053 Validation Loss: 0.831355631351471\n",
      "Epoch 14667: Training Loss: 0.08954622348149617 Validation Loss: 0.832061231136322\n",
      "Epoch 14668: Training Loss: 0.08954587082068126 Validation Loss: 0.8328141570091248\n",
      "Epoch 14669: Training Loss: 0.08978640288114548 Validation Loss: 0.8326491713523865\n",
      "Epoch 14670: Training Loss: 0.0904913234213988 Validation Loss: 0.8328800201416016\n",
      "Epoch 14671: Training Loss: 0.0898628979921341 Validation Loss: 0.8321999907493591\n",
      "Epoch 14672: Training Loss: 0.08965505411227544 Validation Loss: 0.8313773274421692\n",
      "Epoch 14673: Training Loss: 0.0895054464538892 Validation Loss: 0.8318417072296143\n",
      "Epoch 14674: Training Loss: 0.09016788005828857 Validation Loss: 0.8331750631332397\n",
      "Epoch 14675: Training Loss: 0.0896844466527303 Validation Loss: 0.832286536693573\n",
      "Epoch 14676: Training Loss: 0.09011367460091908 Validation Loss: 0.8314346671104431\n",
      "Epoch 14677: Training Loss: 0.08959524581829707 Validation Loss: 0.8302283883094788\n",
      "Epoch 14678: Training Loss: 0.08968831598758698 Validation Loss: 0.8301476836204529\n",
      "Epoch 14679: Training Loss: 0.08969773103793462 Validation Loss: 0.8312381505966187\n",
      "Epoch 14680: Training Loss: 0.08940795063972473 Validation Loss: 0.8318409323692322\n",
      "Epoch 14681: Training Loss: 0.09009825189908345 Validation Loss: 0.8324264883995056\n",
      "Epoch 14682: Training Loss: 0.08973327030738194 Validation Loss: 0.8336588740348816\n",
      "Epoch 14683: Training Loss: 0.08974068115154903 Validation Loss: 0.8334025144577026\n",
      "Epoch 14684: Training Loss: 0.08941457917292912 Validation Loss: 0.83256596326828\n",
      "Epoch 14685: Training Loss: 0.08932078133026759 Validation Loss: 0.831923246383667\n",
      "Epoch 14686: Training Loss: 0.08974676827589671 Validation Loss: 0.8313743472099304\n",
      "Epoch 14687: Training Loss: 0.08962152153253555 Validation Loss: 0.8314924836158752\n",
      "Epoch 14688: Training Loss: 0.08949006100495656 Validation Loss: 0.8317092061042786\n",
      "Epoch 14689: Training Loss: 0.08993005255858104 Validation Loss: 0.8326350450515747\n",
      "Epoch 14690: Training Loss: 0.08948874473571777 Validation Loss: 0.8328908681869507\n",
      "Epoch 14691: Training Loss: 0.09000479926665624 Validation Loss: 0.8314881324768066\n",
      "Epoch 14692: Training Loss: 0.08976929883162181 Validation Loss: 0.830385148525238\n",
      "Epoch 14693: Training Loss: 0.08985234051942825 Validation Loss: 0.83132004737854\n",
      "Epoch 14694: Training Loss: 0.08974991738796234 Validation Loss: 0.8330922722816467\n",
      "Epoch 14695: Training Loss: 0.08972889681657155 Validation Loss: 0.8336633443832397\n",
      "Epoch 14696: Training Loss: 0.08930753171443939 Validation Loss: 0.833284854888916\n",
      "Epoch 14697: Training Loss: 0.08962404976288478 Validation Loss: 0.8320629000663757\n",
      "Epoch 14698: Training Loss: 0.0894037236769994 Validation Loss: 0.8309427499771118\n",
      "Epoch 14699: Training Loss: 0.08963761478662491 Validation Loss: 0.830289363861084\n",
      "Epoch 14700: Training Loss: 0.0897945985198021 Validation Loss: 0.8319355845451355\n",
      "Epoch 14701: Training Loss: 0.0902877077460289 Validation Loss: 0.8326389193534851\n",
      "Epoch 14702: Training Loss: 0.08934011310338974 Validation Loss: 0.8328611254692078\n",
      "Epoch 14703: Training Loss: 0.08963057150443395 Validation Loss: 0.8326877355575562\n",
      "Epoch 14704: Training Loss: 0.08940609792868297 Validation Loss: 0.831775963306427\n",
      "Epoch 14705: Training Loss: 0.0893198698759079 Validation Loss: 0.8308183550834656\n",
      "Epoch 14706: Training Loss: 0.08973325788974762 Validation Loss: 0.8303706049919128\n",
      "Epoch 14707: Training Loss: 0.08949780464172363 Validation Loss: 0.830615222454071\n",
      "Epoch 14708: Training Loss: 0.08935518066088359 Validation Loss: 0.8321807384490967\n",
      "Epoch 14709: Training Loss: 0.08942622194687526 Validation Loss: 0.8321123123168945\n",
      "Epoch 14710: Training Loss: 0.089716171224912 Validation Loss: 0.8325240015983582\n",
      "Epoch 14711: Training Loss: 0.08970093230406444 Validation Loss: 0.8330320715904236\n",
      "Epoch 14712: Training Loss: 0.0894313355286916 Validation Loss: 0.8318847417831421\n",
      "Epoch 14713: Training Loss: 0.08915694306294124 Validation Loss: 0.8313192129135132\n",
      "Epoch 14714: Training Loss: 0.08935038248697917 Validation Loss: 0.8308593034744263\n",
      "Epoch 14715: Training Loss: 0.09003537644942601 Validation Loss: 0.8322314023971558\n",
      "Epoch 14716: Training Loss: 0.08947251985470454 Validation Loss: 0.8324200510978699\n",
      "Epoch 14717: Training Loss: 0.08938169727722804 Validation Loss: 0.8331689238548279\n",
      "Epoch 14718: Training Loss: 0.09007104982932408 Validation Loss: 0.8321813344955444\n",
      "Epoch 14719: Training Loss: 0.08927645534276962 Validation Loss: 0.8330678939819336\n",
      "Epoch 14720: Training Loss: 0.0898092786471049 Validation Loss: 0.8318578600883484\n",
      "Epoch 14721: Training Loss: 0.08948967357476552 Validation Loss: 0.8305617570877075\n",
      "Epoch 14722: Training Loss: 0.08937581380208333 Validation Loss: 0.8314613699913025\n",
      "Epoch 14723: Training Loss: 0.08943715194861095 Validation Loss: 0.8327683210372925\n",
      "Epoch 14724: Training Loss: 0.08937713752190272 Validation Loss: 0.834183931350708\n",
      "Epoch 14725: Training Loss: 0.08977344383796056 Validation Loss: 0.832942008972168\n",
      "Epoch 14726: Training Loss: 0.08906336625417073 Validation Loss: 0.8310076594352722\n",
      "Epoch 14727: Training Loss: 0.08928686380386353 Validation Loss: 0.8316221237182617\n",
      "Epoch 14728: Training Loss: 0.08955338100592296 Validation Loss: 0.8311793804168701\n",
      "Epoch 14729: Training Loss: 0.08952008684476216 Validation Loss: 0.8310452103614807\n",
      "Epoch 14730: Training Loss: 0.0894614706436793 Validation Loss: 0.8327925205230713\n",
      "Epoch 14731: Training Loss: 0.08965793003638585 Validation Loss: 0.833267867565155\n",
      "Epoch 14732: Training Loss: 0.08955996483564377 Validation Loss: 0.8331594467163086\n",
      "Epoch 14733: Training Loss: 0.08931570996840794 Validation Loss: 0.8327628374099731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14734: Training Loss: 0.0893575797478358 Validation Loss: 0.8331401944160461\n",
      "Epoch 14735: Training Loss: 0.08976914236942928 Validation Loss: 0.8327558040618896\n",
      "Epoch 14736: Training Loss: 0.08999638259410858 Validation Loss: 0.8323557376861572\n",
      "Epoch 14737: Training Loss: 0.08955218146244685 Validation Loss: 0.8320317268371582\n",
      "Epoch 14738: Training Loss: 0.08990253259738286 Validation Loss: 0.8322010040283203\n",
      "Epoch 14739: Training Loss: 0.08968377113342285 Validation Loss: 0.832105815410614\n",
      "Epoch 14740: Training Loss: 0.08948531498511632 Validation Loss: 0.8318750858306885\n",
      "Epoch 14741: Training Loss: 0.0892203797896703 Validation Loss: 0.8317580223083496\n",
      "Epoch 14742: Training Loss: 0.08945487687985103 Validation Loss: 0.8314082622528076\n",
      "Epoch 14743: Training Loss: 0.08964800586303075 Validation Loss: 0.8300338983535767\n",
      "Epoch 14744: Training Loss: 0.0894281913836797 Validation Loss: 0.8317018747329712\n",
      "Epoch 14745: Training Loss: 0.08947357287009557 Validation Loss: 0.8343051671981812\n",
      "Epoch 14746: Training Loss: 0.08961458504199982 Validation Loss: 0.834567129611969\n",
      "Epoch 14747: Training Loss: 0.0894142637650172 Validation Loss: 0.8336243033409119\n",
      "Epoch 14748: Training Loss: 0.08943743010361989 Validation Loss: 0.832528293132782\n",
      "Epoch 14749: Training Loss: 0.0895376056432724 Validation Loss: 0.8326768279075623\n",
      "Epoch 14750: Training Loss: 0.0896432896455129 Validation Loss: 0.8328834176063538\n",
      "Epoch 14751: Training Loss: 0.08885755389928818 Validation Loss: 0.8334376215934753\n",
      "Epoch 14752: Training Loss: 0.08920259277025859 Validation Loss: 0.8345934152603149\n",
      "Epoch 14753: Training Loss: 0.08935798207918803 Validation Loss: 0.8343328237533569\n",
      "Epoch 14754: Training Loss: 0.08969945708910625 Validation Loss: 0.8323855996131897\n",
      "Epoch 14755: Training Loss: 0.08938006311655045 Validation Loss: 0.8312506079673767\n",
      "Epoch 14756: Training Loss: 0.08969074487686157 Validation Loss: 0.8315861225128174\n",
      "Epoch 14757: Training Loss: 0.08956173807382584 Validation Loss: 0.8325853943824768\n",
      "Epoch 14758: Training Loss: 0.0896483560403188 Validation Loss: 0.83311527967453\n",
      "Epoch 14759: Training Loss: 0.0891552393635114 Validation Loss: 0.8331190347671509\n",
      "Epoch 14760: Training Loss: 0.08927251398563385 Validation Loss: 0.8329787254333496\n",
      "Epoch 14761: Training Loss: 0.0899062231183052 Validation Loss: 0.8324207067489624\n",
      "Epoch 14762: Training Loss: 0.08929151048262914 Validation Loss: 0.8333941102027893\n",
      "Epoch 14763: Training Loss: 0.0893290713429451 Validation Loss: 0.8333361148834229\n",
      "Epoch 14764: Training Loss: 0.08995077510674794 Validation Loss: 0.8332659006118774\n",
      "Epoch 14765: Training Loss: 0.0894283081094424 Validation Loss: 0.8328314423561096\n",
      "Epoch 14766: Training Loss: 0.08979224413633347 Validation Loss: 0.8318392038345337\n",
      "Epoch 14767: Training Loss: 0.08918593327204387 Validation Loss: 0.8324641585350037\n",
      "Epoch 14768: Training Loss: 0.08924095084269841 Validation Loss: 0.832591712474823\n",
      "Epoch 14769: Training Loss: 0.08921576539675395 Validation Loss: 0.8324713110923767\n",
      "Epoch 14770: Training Loss: 0.0893511176109314 Validation Loss: 0.8326712846755981\n",
      "Epoch 14771: Training Loss: 0.08922953655322392 Validation Loss: 0.8323155045509338\n",
      "Epoch 14772: Training Loss: 0.09054700036843617 Validation Loss: 0.8316906094551086\n",
      "Epoch 14773: Training Loss: 0.08913141985734303 Validation Loss: 0.831855833530426\n",
      "Epoch 14774: Training Loss: 0.08980119973421097 Validation Loss: 0.8323594331741333\n",
      "Epoch 14775: Training Loss: 0.09053816894690196 Validation Loss: 0.8337118029594421\n",
      "Epoch 14776: Training Loss: 0.08948250859975815 Validation Loss: 0.8342723846435547\n",
      "Epoch 14777: Training Loss: 0.08926936239004135 Validation Loss: 0.833888828754425\n",
      "Epoch 14778: Training Loss: 0.0895563413699468 Validation Loss: 0.8342909216880798\n",
      "Epoch 14779: Training Loss: 0.08929751565059026 Validation Loss: 0.8338901996612549\n",
      "Epoch 14780: Training Loss: 0.08933678766091664 Validation Loss: 0.832588255405426\n",
      "Epoch 14781: Training Loss: 0.08890017867088318 Validation Loss: 0.8321784138679504\n",
      "Epoch 14782: Training Loss: 0.08961841215689977 Validation Loss: 0.8327563405036926\n",
      "Epoch 14783: Training Loss: 0.08937206119298935 Validation Loss: 0.8330262303352356\n",
      "Epoch 14784: Training Loss: 0.08931595832109451 Validation Loss: 0.8339855074882507\n",
      "Epoch 14785: Training Loss: 0.08932792395353317 Validation Loss: 0.8331314921379089\n",
      "Epoch 14786: Training Loss: 0.08920055876175563 Validation Loss: 0.833282470703125\n",
      "Epoch 14787: Training Loss: 0.08942395448684692 Validation Loss: 0.8328195810317993\n",
      "Epoch 14788: Training Loss: 0.08916313201189041 Validation Loss: 0.8325569033622742\n",
      "Epoch 14789: Training Loss: 0.08927866816520691 Validation Loss: 0.833419144153595\n",
      "Epoch 14790: Training Loss: 0.08926137536764145 Validation Loss: 0.8332644104957581\n",
      "Epoch 14791: Training Loss: 0.0889409879843394 Validation Loss: 0.8335742950439453\n",
      "Epoch 14792: Training Loss: 0.09022854268550873 Validation Loss: 0.8331248760223389\n",
      "Epoch 14793: Training Loss: 0.08972373604774475 Validation Loss: 0.8341630697250366\n",
      "Epoch 14794: Training Loss: 0.08971088379621506 Validation Loss: 0.8326958417892456\n",
      "Epoch 14795: Training Loss: 0.08905036995808284 Validation Loss: 0.8332263231277466\n",
      "Epoch 14796: Training Loss: 0.08908883978923161 Validation Loss: 0.8326566815376282\n",
      "Epoch 14797: Training Loss: 0.08896228422721227 Validation Loss: 0.8326950669288635\n",
      "Epoch 14798: Training Loss: 0.08920087665319443 Validation Loss: 0.8327658176422119\n",
      "Epoch 14799: Training Loss: 0.08939831455548604 Validation Loss: 0.832473874092102\n",
      "Epoch 14800: Training Loss: 0.08919040362040202 Validation Loss: 0.8322680592536926\n",
      "Epoch 14801: Training Loss: 0.08923205981651942 Validation Loss: 0.8326506614685059\n",
      "Epoch 14802: Training Loss: 0.08953022956848145 Validation Loss: 0.8337897062301636\n",
      "Epoch 14803: Training Loss: 0.08917921284834544 Validation Loss: 0.8336963653564453\n",
      "Epoch 14804: Training Loss: 0.08908017724752426 Validation Loss: 0.8340816497802734\n",
      "Epoch 14805: Training Loss: 0.08902274320522945 Validation Loss: 0.83366858959198\n",
      "Epoch 14806: Training Loss: 0.08927479883035024 Validation Loss: 0.8337430357933044\n",
      "Epoch 14807: Training Loss: 0.08938893179098766 Validation Loss: 0.8347371816635132\n",
      "Epoch 14808: Training Loss: 0.08917420357465744 Validation Loss: 0.834827184677124\n",
      "Epoch 14809: Training Loss: 0.08963105827569962 Validation Loss: 0.8340132832527161\n",
      "Epoch 14810: Training Loss: 0.08959823101758957 Validation Loss: 0.8319623470306396\n",
      "Epoch 14811: Training Loss: 0.08928617835044861 Validation Loss: 0.8325446248054504\n",
      "Epoch 14812: Training Loss: 0.08884488791227341 Validation Loss: 0.8321446180343628\n",
      "Epoch 14813: Training Loss: 0.08871748050053914 Validation Loss: 0.8311959505081177\n",
      "Epoch 14814: Training Loss: 0.08926883339881897 Validation Loss: 0.8320074677467346\n",
      "Epoch 14815: Training Loss: 0.08928723881642024 Validation Loss: 0.8329173922538757\n",
      "Epoch 14816: Training Loss: 0.08924496918916702 Validation Loss: 0.8324311375617981\n",
      "Epoch 14817: Training Loss: 0.08988086382548015 Validation Loss: 0.8327673077583313\n",
      "Epoch 14818: Training Loss: 0.08915538837512334 Validation Loss: 0.8328831791877747\n",
      "Epoch 14819: Training Loss: 0.08936536063750584 Validation Loss: 0.8339089155197144\n",
      "Epoch 14820: Training Loss: 0.08882347246011098 Validation Loss: 0.8336897492408752\n",
      "Epoch 14821: Training Loss: 0.0899361918369929 Validation Loss: 0.8328903913497925\n",
      "Epoch 14822: Training Loss: 0.08918157716592152 Validation Loss: 0.8335227966308594\n",
      "Epoch 14823: Training Loss: 0.08938491592804591 Validation Loss: 0.833273708820343\n",
      "Epoch 14824: Training Loss: 0.08918393154939015 Validation Loss: 0.8335283994674683\n",
      "Epoch 14825: Training Loss: 0.08897189299265544 Validation Loss: 0.8342633843421936\n",
      "Epoch 14826: Training Loss: 0.08896452188491821 Validation Loss: 0.8350956439971924\n",
      "Epoch 14827: Training Loss: 0.08920657883087794 Validation Loss: 0.8359056115150452\n",
      "Epoch 14828: Training Loss: 0.08894532918930054 Validation Loss: 0.8363568186759949\n",
      "Epoch 14829: Training Loss: 0.08932365228732426 Validation Loss: 0.8346145153045654\n",
      "Epoch 14830: Training Loss: 0.08918865025043488 Validation Loss: 0.8327500224113464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14831: Training Loss: 0.08902627974748611 Validation Loss: 0.8322609663009644\n",
      "Epoch 14832: Training Loss: 0.0890636146068573 Validation Loss: 0.8328208923339844\n",
      "Epoch 14833: Training Loss: 0.0897890105843544 Validation Loss: 0.8322517275810242\n",
      "Epoch 14834: Training Loss: 0.08910136918226878 Validation Loss: 0.8336231112480164\n",
      "Epoch 14835: Training Loss: 0.08904412885506947 Validation Loss: 0.8347533345222473\n",
      "Epoch 14836: Training Loss: 0.08910524348417918 Validation Loss: 0.8345372080802917\n",
      "Epoch 14837: Training Loss: 0.08932321270306905 Validation Loss: 0.8344631195068359\n",
      "Epoch 14838: Training Loss: 0.0899152581890424 Validation Loss: 0.833129346370697\n",
      "Epoch 14839: Training Loss: 0.09020260473092397 Validation Loss: 0.8340454697608948\n",
      "Epoch 14840: Training Loss: 0.0891299123565356 Validation Loss: 0.8339709043502808\n",
      "Epoch 14841: Training Loss: 0.08899734417597453 Validation Loss: 0.8327585458755493\n",
      "Epoch 14842: Training Loss: 0.08901405334472656 Validation Loss: 0.8329305648803711\n",
      "Epoch 14843: Training Loss: 0.08944482356309891 Validation Loss: 0.8319611549377441\n",
      "Epoch 14844: Training Loss: 0.08903429905573527 Validation Loss: 0.8334548473358154\n",
      "Epoch 14845: Training Loss: 0.08895013978083928 Validation Loss: 0.8346287608146667\n",
      "Epoch 14846: Training Loss: 0.08914546420176823 Validation Loss: 0.8356865048408508\n",
      "Epoch 14847: Training Loss: 0.08947863678137462 Validation Loss: 0.8348062634468079\n",
      "Epoch 14848: Training Loss: 0.0889178936680158 Validation Loss: 0.8335899114608765\n",
      "Epoch 14849: Training Loss: 0.08906165758768718 Validation Loss: 0.8332290053367615\n",
      "Epoch 14850: Training Loss: 0.08925243963797887 Validation Loss: 0.8335030674934387\n",
      "Epoch 14851: Training Loss: 0.08925184359153111 Validation Loss: 0.8335526585578918\n",
      "Epoch 14852: Training Loss: 0.08871104568243027 Validation Loss: 0.8332933187484741\n",
      "Epoch 14853: Training Loss: 0.08908512940009435 Validation Loss: 0.8333129286766052\n",
      "Epoch 14854: Training Loss: 0.08894296983877818 Validation Loss: 0.8326164484024048\n",
      "Epoch 14855: Training Loss: 0.08896217991908391 Validation Loss: 0.8332325220108032\n",
      "Epoch 14856: Training Loss: 0.08894406755765279 Validation Loss: 0.8333255052566528\n",
      "Epoch 14857: Training Loss: 0.08917337904373805 Validation Loss: 0.8346025347709656\n",
      "Epoch 14858: Training Loss: 0.08919373899698257 Validation Loss: 0.8339856863021851\n",
      "Epoch 14859: Training Loss: 0.08910097926855087 Validation Loss: 0.832578182220459\n",
      "Epoch 14860: Training Loss: 0.0901887317498525 Validation Loss: 0.8328294157981873\n",
      "Epoch 14861: Training Loss: 0.08890471359093984 Validation Loss: 0.8331778049468994\n",
      "Epoch 14862: Training Loss: 0.08926135053237279 Validation Loss: 0.8353217840194702\n",
      "Epoch 14863: Training Loss: 0.0887341449658076 Validation Loss: 0.8355128169059753\n",
      "Epoch 14864: Training Loss: 0.08914388964573543 Validation Loss: 0.8337417244911194\n",
      "Epoch 14865: Training Loss: 0.08853450665871303 Validation Loss: 0.8331091403961182\n",
      "Epoch 14866: Training Loss: 0.08979061990976334 Validation Loss: 0.8332479596138\n",
      "Epoch 14867: Training Loss: 0.08886425693829854 Validation Loss: 0.8334907293319702\n",
      "Epoch 14868: Training Loss: 0.08883153150478999 Validation Loss: 0.8334397673606873\n",
      "Epoch 14869: Training Loss: 0.08902810265620549 Validation Loss: 0.8336368799209595\n",
      "Epoch 14870: Training Loss: 0.08922066787878673 Validation Loss: 0.8340842127799988\n",
      "Epoch 14871: Training Loss: 0.0889200468858083 Validation Loss: 0.8348275423049927\n",
      "Epoch 14872: Training Loss: 0.08923670649528503 Validation Loss: 0.8352083563804626\n",
      "Epoch 14873: Training Loss: 0.08905723690986633 Validation Loss: 0.8349989056587219\n",
      "Epoch 14874: Training Loss: 0.08890382448832194 Validation Loss: 0.833639919757843\n",
      "Epoch 14875: Training Loss: 0.08905194948116939 Validation Loss: 0.8329401612281799\n",
      "Epoch 14876: Training Loss: 0.08896763622760773 Validation Loss: 0.8327144384384155\n",
      "Epoch 14877: Training Loss: 0.0885191559791565 Validation Loss: 0.8331633806228638\n",
      "Epoch 14878: Training Loss: 0.08889355510473251 Validation Loss: 0.8343528509140015\n",
      "Epoch 14879: Training Loss: 0.08938716103633244 Validation Loss: 0.8363168239593506\n",
      "Epoch 14880: Training Loss: 0.08902234087387721 Validation Loss: 0.8357952833175659\n",
      "Epoch 14881: Training Loss: 0.08886758486429851 Validation Loss: 0.8354141116142273\n",
      "Epoch 14882: Training Loss: 0.08907512327035268 Validation Loss: 0.8330179452896118\n",
      "Epoch 14883: Training Loss: 0.0894715537627538 Validation Loss: 0.8324827551841736\n",
      "Epoch 14884: Training Loss: 0.08917931467294693 Validation Loss: 0.8333243131637573\n",
      "Epoch 14885: Training Loss: 0.08921516438325246 Validation Loss: 0.8354141712188721\n",
      "Epoch 14886: Training Loss: 0.08894709497690201 Validation Loss: 0.8344815969467163\n",
      "Epoch 14887: Training Loss: 0.08938497801621755 Validation Loss: 0.8345580101013184\n",
      "Epoch 14888: Training Loss: 0.0888453871011734 Validation Loss: 0.8334691524505615\n",
      "Epoch 14889: Training Loss: 0.08904511729876201 Validation Loss: 0.8333256244659424\n",
      "Epoch 14890: Training Loss: 0.08902423580487569 Validation Loss: 0.83302241563797\n",
      "Epoch 14891: Training Loss: 0.08904424558083217 Validation Loss: 0.8337162733078003\n",
      "Epoch 14892: Training Loss: 0.08852335810661316 Validation Loss: 0.8346374034881592\n",
      "Epoch 14893: Training Loss: 0.08916845917701721 Validation Loss: 0.8351258039474487\n",
      "Epoch 14894: Training Loss: 0.08881104985872905 Validation Loss: 0.8340614438056946\n",
      "Epoch 14895: Training Loss: 0.0889142279823621 Validation Loss: 0.8341827988624573\n",
      "Epoch 14896: Training Loss: 0.08896529426177342 Validation Loss: 0.8343786001205444\n",
      "Epoch 14897: Training Loss: 0.08920820554097493 Validation Loss: 0.8337945342063904\n",
      "Epoch 14898: Training Loss: 0.08894647657871246 Validation Loss: 0.833595871925354\n",
      "Epoch 14899: Training Loss: 0.0891503170132637 Validation Loss: 0.8327568769454956\n",
      "Epoch 14900: Training Loss: 0.09017835060755412 Validation Loss: 0.833824872970581\n",
      "Epoch 14901: Training Loss: 0.08882071326176326 Validation Loss: 0.8352093696594238\n",
      "Epoch 14902: Training Loss: 0.08890603731075923 Validation Loss: 0.8359326720237732\n",
      "Epoch 14903: Training Loss: 0.0891345019141833 Validation Loss: 0.8353654742240906\n",
      "Epoch 14904: Training Loss: 0.08892570187648137 Validation Loss: 0.834497332572937\n",
      "Epoch 14905: Training Loss: 0.08910481880108516 Validation Loss: 0.8333316445350647\n",
      "Epoch 14906: Training Loss: 0.08921628445386887 Validation Loss: 0.8327115178108215\n",
      "Epoch 14907: Training Loss: 0.08887700488169988 Validation Loss: 0.833075225353241\n",
      "Epoch 14908: Training Loss: 0.08908401429653168 Validation Loss: 0.8356624245643616\n",
      "Epoch 14909: Training Loss: 0.08926411718130112 Validation Loss: 0.8361886143684387\n",
      "Epoch 14910: Training Loss: 0.08913751691579819 Validation Loss: 0.8358910083770752\n",
      "Epoch 14911: Training Loss: 0.08901545157035191 Validation Loss: 0.8355613350868225\n",
      "Epoch 14912: Training Loss: 0.08891066660483678 Validation Loss: 0.8347315192222595\n",
      "Epoch 14913: Training Loss: 0.08883318305015564 Validation Loss: 0.8345465660095215\n",
      "Epoch 14914: Training Loss: 0.08938318987687428 Validation Loss: 0.8334273099899292\n",
      "Epoch 14915: Training Loss: 0.08911774307489395 Validation Loss: 0.8347521424293518\n",
      "Epoch 14916: Training Loss: 0.08889573812484741 Validation Loss: 0.8345065116882324\n",
      "Epoch 14917: Training Loss: 0.08903243392705917 Validation Loss: 0.8351191878318787\n",
      "Epoch 14918: Training Loss: 0.08866480986277263 Validation Loss: 0.8346340656280518\n",
      "Epoch 14919: Training Loss: 0.08885766317447026 Validation Loss: 0.8340011239051819\n",
      "Epoch 14920: Training Loss: 0.08914808928966522 Validation Loss: 0.8332310318946838\n",
      "Epoch 14921: Training Loss: 0.08879311134417851 Validation Loss: 0.8339486122131348\n",
      "Epoch 14922: Training Loss: 0.08906710396210353 Validation Loss: 0.8349379897117615\n",
      "Epoch 14923: Training Loss: 0.08900841325521469 Validation Loss: 0.8350124359130859\n",
      "Epoch 14924: Training Loss: 0.08917008092006047 Validation Loss: 0.8350614309310913\n",
      "Epoch 14925: Training Loss: 0.08874580512444179 Validation Loss: 0.833321213722229\n",
      "Epoch 14926: Training Loss: 0.08880625913540523 Validation Loss: 0.8335234522819519\n",
      "Epoch 14927: Training Loss: 0.08884647736946742 Validation Loss: 0.8347071409225464\n",
      "Epoch 14928: Training Loss: 0.08854753524065018 Validation Loss: 0.8349951505661011\n",
      "Epoch 14929: Training Loss: 0.08882015695174535 Validation Loss: 0.8345543742179871\n",
      "Epoch 14930: Training Loss: 0.0888343056042989 Validation Loss: 0.8347169756889343\n",
      "Epoch 14931: Training Loss: 0.08901004741589229 Validation Loss: 0.8349575400352478\n",
      "Epoch 14932: Training Loss: 0.08889565120140712 Validation Loss: 0.8349313139915466\n",
      "Epoch 14933: Training Loss: 0.08878388504187266 Validation Loss: 0.8351044058799744\n",
      "Epoch 14934: Training Loss: 0.0890621617436409 Validation Loss: 0.8346685767173767\n",
      "Epoch 14935: Training Loss: 0.08887189378341039 Validation Loss: 0.8340426683425903\n",
      "Epoch 14936: Training Loss: 0.08919260402520497 Validation Loss: 0.8335877060890198\n",
      "Epoch 14937: Training Loss: 0.08914351214965184 Validation Loss: 0.8346011638641357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14938: Training Loss: 0.08965527266263962 Validation Loss: 0.8352470993995667\n",
      "Epoch 14939: Training Loss: 0.08860837419827779 Validation Loss: 0.8348225951194763\n",
      "Epoch 14940: Training Loss: 0.08945334578553836 Validation Loss: 0.8353123664855957\n",
      "Epoch 14941: Training Loss: 0.08895560105641682 Validation Loss: 0.8338255286216736\n",
      "Epoch 14942: Training Loss: 0.08884250621000926 Validation Loss: 0.8350280523300171\n",
      "Epoch 14943: Training Loss: 0.08888552834590276 Validation Loss: 0.8351559042930603\n",
      "Epoch 14944: Training Loss: 0.08921308318773906 Validation Loss: 0.8340942859649658\n",
      "Epoch 14945: Training Loss: 0.08895651499430339 Validation Loss: 0.8337408304214478\n",
      "Epoch 14946: Training Loss: 0.0887955551346143 Validation Loss: 0.833669126033783\n",
      "Epoch 14947: Training Loss: 0.0886365423599879 Validation Loss: 0.8345111608505249\n",
      "Epoch 14948: Training Loss: 0.08853031943241756 Validation Loss: 0.8343045711517334\n",
      "Epoch 14949: Training Loss: 0.08879065265258153 Validation Loss: 0.8349922895431519\n",
      "Epoch 14950: Training Loss: 0.08890089144309361 Validation Loss: 0.8367887735366821\n",
      "Epoch 14951: Training Loss: 0.08917665978272755 Validation Loss: 0.8376742005348206\n",
      "Epoch 14952: Training Loss: 0.089036429921786 Validation Loss: 0.8363668322563171\n",
      "Epoch 14953: Training Loss: 0.08877938489119212 Validation Loss: 0.8346441984176636\n",
      "Epoch 14954: Training Loss: 0.08875442296266556 Validation Loss: 0.8335040211677551\n",
      "Epoch 14955: Training Loss: 0.08955666174491246 Validation Loss: 0.8338749408721924\n",
      "Epoch 14956: Training Loss: 0.08895884205897649 Validation Loss: 0.8352795839309692\n",
      "Epoch 14957: Training Loss: 0.08863541732231776 Validation Loss: 0.8349382877349854\n",
      "Epoch 14958: Training Loss: 0.08867279936869939 Validation Loss: 0.8352983593940735\n",
      "Epoch 14959: Training Loss: 0.08888280888398488 Validation Loss: 0.8351555466651917\n",
      "Epoch 14960: Training Loss: 0.08862220744291942 Validation Loss: 0.8347526788711548\n",
      "Epoch 14961: Training Loss: 0.0890302707751592 Validation Loss: 0.8332468271255493\n",
      "Epoch 14962: Training Loss: 0.0887064312895139 Validation Loss: 0.8337560296058655\n",
      "Epoch 14963: Training Loss: 0.08876150598128636 Validation Loss: 0.8346563577651978\n",
      "Epoch 14964: Training Loss: 0.08862900733947754 Validation Loss: 0.8351613879203796\n",
      "Epoch 14965: Training Loss: 0.0886151095231374 Validation Loss: 0.8343608379364014\n",
      "Epoch 14966: Training Loss: 0.08950181057055791 Validation Loss: 0.8336774110794067\n",
      "Epoch 14967: Training Loss: 0.08882519354422887 Validation Loss: 0.8341326713562012\n",
      "Epoch 14968: Training Loss: 0.08878838767608006 Validation Loss: 0.8349148631095886\n",
      "Epoch 14969: Training Loss: 0.08862895270188649 Validation Loss: 0.8357113003730774\n",
      "Epoch 14970: Training Loss: 0.08865751326084137 Validation Loss: 0.8360329270362854\n",
      "Epoch 14971: Training Loss: 0.08881741513808568 Validation Loss: 0.8356665968894958\n",
      "Epoch 14972: Training Loss: 0.0888349711894989 Validation Loss: 0.834816038608551\n",
      "Epoch 14973: Training Loss: 0.08860216786464055 Validation Loss: 0.8341305255889893\n",
      "Epoch 14974: Training Loss: 0.08850960681835811 Validation Loss: 0.8336483836174011\n",
      "Epoch 14975: Training Loss: 0.0886724665760994 Validation Loss: 0.8339337110519409\n",
      "Epoch 14976: Training Loss: 0.08872230350971222 Validation Loss: 0.8344231247901917\n",
      "Epoch 14977: Training Loss: 0.08891564607620239 Validation Loss: 0.8338561654090881\n",
      "Epoch 14978: Training Loss: 0.08882804463307063 Validation Loss: 0.8348259329795837\n",
      "Epoch 14979: Training Loss: 0.08848309268554051 Validation Loss: 0.8358298540115356\n",
      "Epoch 14980: Training Loss: 0.08918019632498424 Validation Loss: 0.835727870464325\n",
      "Epoch 14981: Training Loss: 0.08880078295866649 Validation Loss: 0.8363621830940247\n",
      "Epoch 14982: Training Loss: 0.08924452463785808 Validation Loss: 0.835758626461029\n",
      "Epoch 14983: Training Loss: 0.08882155269384384 Validation Loss: 0.8357184529304504\n",
      "Epoch 14984: Training Loss: 0.08883701016505559 Validation Loss: 0.836408793926239\n",
      "Epoch 14985: Training Loss: 0.08926272888978322 Validation Loss: 0.8351024389266968\n",
      "Epoch 14986: Training Loss: 0.08866588771343231 Validation Loss: 0.8351324200630188\n",
      "Epoch 14987: Training Loss: 0.08864574631055196 Validation Loss: 0.8359936475753784\n",
      "Epoch 14988: Training Loss: 0.08860371510187785 Validation Loss: 0.8349784016609192\n",
      "Epoch 14989: Training Loss: 0.0889764775832494 Validation Loss: 0.8355463743209839\n",
      "Epoch 14990: Training Loss: 0.0882186417778333 Validation Loss: 0.8358336687088013\n",
      "Epoch 14991: Training Loss: 0.08900234351555507 Validation Loss: 0.836005449295044\n",
      "Epoch 14992: Training Loss: 0.0887485146522522 Validation Loss: 0.8346349000930786\n",
      "Epoch 14993: Training Loss: 0.08881191660960515 Validation Loss: 0.8341770172119141\n",
      "Epoch 14994: Training Loss: 0.0887519841392835 Validation Loss: 0.8349917531013489\n",
      "Epoch 14995: Training Loss: 0.08877250552177429 Validation Loss: 0.8353335857391357\n",
      "Epoch 14996: Training Loss: 0.08831986784934998 Validation Loss: 0.8345972299575806\n",
      "Epoch 14997: Training Loss: 0.08880559355020523 Validation Loss: 0.8347612619400024\n",
      "Epoch 14998: Training Loss: 0.08865974595149358 Validation Loss: 0.834812581539154\n",
      "Epoch 14999: Training Loss: 0.08863329142332077 Validation Loss: 0.8352130651473999\n",
      "Epoch 15000: Training Loss: 0.08869122465451558 Validation Loss: 0.8359391093254089\n"
     ]
    }
   ],
   "source": [
    "fit(Xtra, ytra, Xtes, ytes, net, optimizer, criterion, n_epochs, n_batches, device, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data\\\\model_checkpoint.pt'\n",
    "device = torch.device('cpu')\n",
    "net = Net1()\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815489649772644\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    inputs = torch.FloatTensor(Xtes)\n",
    "    labels = torch.tensor(ytes, dtype=torch.long)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net.forward(inputs)\n",
    "    loss = error(outputs, labels) \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "for v,p in enumerate(net.parameters()):\n",
    "    param_dict[v] = p.data.numpy()\n",
    "    \n",
    "W0 = param_dict[0].T\n",
    "b0 = param_dict[1]\n",
    "W1 = param_dict[2].T\n",
    "b1 = param_dict[3]\n",
    "\n",
    "\n",
    "h0 = np.matmul(Xtes, W0) + b0\n",
    "h1 = np.tanh(h0)\n",
    "h2 = np.matmul(h1, W1) + b1\n",
    "h3 = np.exp(h2)\n",
    "o = h3/np.sum(h3,axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 13)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data\\\\param_dict.npy', param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(prob, topics):    \n",
    "    return topics[np.argmax(prob)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "topics = ['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
    "for c in o:\n",
    "    pred_labels.append(y_hat(c, topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = data.Pregunta[btes]\n",
    "labels = data.Tema[btes]\n",
    "ambig = data.ambiguedad[btes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtes = np.column_stack((preguntas, labels, pred_labels, ambig, Xtes, o))\n",
    "#Xtes = np.column_stack((preguntas, labels, pred_labels, Xtes, o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(Xtes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load('Data\\\\vocab.npy', allow_pickle=True)\n",
    "colnames = np.concatenate((['pregunta', 'label', 'pred_label', 'ambig'], vocab, ['n_token', 'perc_greet', 'capacitacion', 'polarity'], ['Jubilacion Patronal', 'Consultoria', \\\n",
    "                                                                                                     'Renuncia/Despido/Desahucio', 'IESS', 'Greeting', \\\n",
    "                                                                                                         'Contacto', 'No Topic', 'Queja', 'Otros servicios', \\\n",
    "                                                                                                 'Charlas/Capacitaciones', 'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']))\n",
    "data2.columns = colnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregunta</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>ambig</th>\n",
       "      <th>buen</th>\n",
       "      <th>dia</th>\n",
       "      <th>mi</th>\n",
       "      <th>nombr</th>\n",
       "      <th>llam</th>\n",
       "      <th>empres</th>\n",
       "      <th>...</th>\n",
       "      <th>IESS</th>\n",
       "      <th>Greeting</th>\n",
       "      <th>Contacto</th>\n",
       "      <th>No Topic</th>\n",
       "      <th>Queja</th>\n",
       "      <th>Otros servicios</th>\n",
       "      <th>Charlas/Capacitaciones</th>\n",
       "      <th>Hi Five</th>\n",
       "      <th>job seeker</th>\n",
       "      <th>Facturacion/Retencion/Cobros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hola qu tal</td>\n",
       "      <td>Greeting</td>\n",
       "      <td>Greeting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0196924</td>\n",
       "      <td>0.575278</td>\n",
       "      <td>0.0117529</td>\n",
       "      <td>0.0418878</td>\n",
       "      <td>0.00780163</td>\n",
       "      <td>0.110079</td>\n",
       "      <td>0.0266009</td>\n",
       "      <td>0.0171291</td>\n",
       "      <td>0.0969172</td>\n",
       "      <td>0.00848771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hola buenos das como calcular un finiquito</td>\n",
       "      <td>Renuncia/Despido/Desahucio</td>\n",
       "      <td>Renuncia/Despido/Desahucio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00467586</td>\n",
       "      <td>0.0397423</td>\n",
       "      <td>0.000463164</td>\n",
       "      <td>6.33128e-06</td>\n",
       "      <td>7.4005e-05</td>\n",
       "      <td>0.000176887</td>\n",
       "      <td>0.000293868</td>\n",
       "      <td>8.49566e-05</td>\n",
       "      <td>0.00118814</td>\n",
       "      <td>0.000725514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buenas tardes mi consulta es para mi padre que...</td>\n",
       "      <td>Jubilacion Patronal</td>\n",
       "      <td>IESS</td>\n",
       "      <td>ambivalente</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939896</td>\n",
       "      <td>0.000385766</td>\n",
       "      <td>0.000399942</td>\n",
       "      <td>0.000635039</td>\n",
       "      <td>4.90935e-05</td>\n",
       "      <td>0.000385507</td>\n",
       "      <td>0.00126023</td>\n",
       "      <td>4.56116e-05</td>\n",
       "      <td>6.7998e-05</td>\n",
       "      <td>7.69816e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  593 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pregunta  \\\n",
       "0                                      Hola qu tal    \n",
       "1        Hola buenos das como calcular un finiquito   \n",
       "2  Buenas tardes mi consulta es para mi padre que...   \n",
       "\n",
       "                        label                  pred_label        ambig buen  \\\n",
       "0                    Greeting                    Greeting          NaN    0   \n",
       "1  Renuncia/Despido/Desahucio  Renuncia/Despido/Desahucio          NaN    1   \n",
       "2         Jubilacion Patronal                        IESS  ambivalente    1   \n",
       "\n",
       "  dia mi nombr llam empres  ...        IESS     Greeting     Contacto  \\\n",
       "0   0  0     0    0      0  ...   0.0196924     0.575278    0.0117529   \n",
       "1   1  0     0    0      0  ...  0.00467586    0.0397423  0.000463164   \n",
       "2   0  0     0    0      0  ...    0.939896  0.000385766  0.000399942   \n",
       "\n",
       "      No Topic        Queja Otros servicios Charlas/Capacitaciones  \\\n",
       "0    0.0418878   0.00780163        0.110079              0.0266009   \n",
       "1  6.33128e-06   7.4005e-05     0.000176887            0.000293868   \n",
       "2  0.000635039  4.90935e-05     0.000385507             0.00126023   \n",
       "\n",
       "       Hi Five  job seeker Facturacion/Retencion/Cobros  \n",
       "0    0.0171291   0.0969172                   0.00848771  \n",
       "1  8.49566e-05  0.00118814                  0.000725514  \n",
       "2  4.56116e-05  6.7998e-05                  7.69816e-05  \n",
       "\n",
       "[3 rows x 593 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('Data\\\\Xtes.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
