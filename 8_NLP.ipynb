{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data\\\\preguntas.csv\", sep=',', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Data\\\\X.npy', allow_pickle=True)\n",
    "y = np.load('Data\\\\y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_oh = np.zeros((y.shape[0], max(y)+1))\n",
    "#y_oh[np.arange(y.shape[0]), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "idx = np.arange(X.shape[0])\n",
    "random.shuffle(idx)  \n",
    "btra = np.random.choice(idx, int(0.8*X.shape[0]), replace=False)\n",
    "btes = [i for i in idx if i not in btra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtra = X[btra]\n",
    "ytra = y[btra]\n",
    "\n",
    "Xtes = X[btes]\n",
    "ytes = y[btes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 571)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, n_batches):  \n",
    "    \n",
    "    random.seed(123)\n",
    "    \n",
    "    batch_size = X.shape[0] // n_batches\n",
    "    \n",
    "    idx = np.arange(X.shape[0])\n",
    "    random.shuffle(idx)    \n",
    "    idx = idx[:n_batches*batch_size]\n",
    "        \n",
    "    for i in range(n_batches):            \n",
    "        bi = np.random.choice(idx, batch_size, replace=False)\n",
    "        X_batch = X[bi]\n",
    "        Y_batch = Y[bi]\n",
    "        idx = [i for i in idx if i not in bi]\n",
    "        yield (X_batch,Y_batch)       \n",
    "        \n",
    "        \n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc11 = nn.Linear(571, 24, bias=True)\n",
    "        self.fc12 = nn.Linear(24, 13, bias=True) \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = torch.tanh(self.fc11(x))\n",
    "        x1 = self.fc12(x1)     \n",
    "        return x1\n",
    "    \n",
    "    \n",
    "def fit(X, Y, Xt, Yt, net, optimizer, error, n_epochs, n_batches, device, PATH, min_val_loss = float('inf')):\n",
    "    \n",
    "    net = net.to(device)    \n",
    "    losses = []    \n",
    "    val_losses = []\n",
    "\n",
    "    val_inputs = torch.FloatTensor(Xt)\n",
    "    val_labels = torch.tensor(Yt, dtype=torch.long)\n",
    "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)  \n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "        running_loss = 0  \n",
    "         \n",
    "        \n",
    "        for batch_x, batch_y in batch_generator(X, Y, n_batches):  \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the inputs\n",
    "            inputs = torch.FloatTensor(batch_x)\n",
    "            labels = torch.tensor(batch_y, dtype=torch.long)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)             \n",
    "                \n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = error(outputs, labels)\n",
    "                        \n",
    "            loss.backward()    #obtain gradients      \n",
    "            optimizer.step()   #optimize\n",
    "                \n",
    "            running_loss += loss.item()      \n",
    "                \n",
    "        running_loss = running_loss/n_batches    \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_outputs = net.forward(val_inputs)\n",
    "            val_loss = error(val_outputs, val_labels) \n",
    "        \n",
    "        losses.append(running_loss)   \n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        \n",
    "        print('Epoch {0}: Training Loss: {1} Validation Loss: {2}'.format(epoch+1, running_loss, val_loss.item()))\n",
    "        \n",
    "        if val_loss.item() < min_val_loss:\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            print('New Checkpoint Saved into PATH')\n",
    "            min_val_loss = val_loss.item()\n",
    "        \n",
    "        \n",
    "def fweights_init_normal(m):     \n",
    "    classname = m.__class__.__name__\n",
    "    torch.manual_seed(0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        #y = 0.0001\n",
    "        m.weight.data.normal_(0, y)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.normal_(0, y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15000\n",
    "lr = 0.001\n",
    "n_batches = 3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH = 'Data\\\\model_checkpoint.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.sum(np.unique(ytes, return_counts=True)[1])/np.unique(ytes, return_counts=True)[1]\n",
    "class_weights = torch.FloatTensor(weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net1()\n",
    "net.apply(fweights_init_normal)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 2.586364984512329 Validation Loss: 2.5788536071777344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2: Training Loss: 2.5733602046966553 Validation Loss: 2.5698306560516357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3: Training Loss: 2.556111971537272 Validation Loss: 2.563422441482544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4: Training Loss: 2.5394643942515054 Validation Loss: 2.5610508918762207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5: Training Loss: 2.525557041168213 Validation Loss: 2.5616748332977295\n",
      "Epoch 6: Training Loss: 2.515100876490275 Validation Loss: 2.5643019676208496\n",
      "Epoch 7: Training Loss: 2.507951021194458 Validation Loss: 2.567497968673706\n",
      "Epoch 8: Training Loss: 2.5006144841512046 Validation Loss: 2.5702309608459473\n",
      "Epoch 9: Training Loss: 2.4945225715637207 Validation Loss: 2.5724704265594482\n",
      "Epoch 10: Training Loss: 2.492986520131429 Validation Loss: 2.5733482837677\n",
      "Epoch 11: Training Loss: 2.489573081334432 Validation Loss: 2.5722885131835938\n",
      "Epoch 12: Training Loss: 2.4859084288279214 Validation Loss: 2.5700228214263916\n",
      "Epoch 13: Training Loss: 2.481499195098877 Validation Loss: 2.5666286945343018\n",
      "Epoch 14: Training Loss: 2.480325619379679 Validation Loss: 2.563897132873535\n",
      "Epoch 15: Training Loss: 2.4774668216705322 Validation Loss: 2.5616509914398193\n",
      "Epoch 16: Training Loss: 2.4741265773773193 Validation Loss: 2.5593252182006836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 17: Training Loss: 2.4727819760640464 Validation Loss: 2.5563113689422607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 18: Training Loss: 2.4700447718302407 Validation Loss: 2.5535714626312256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 19: Training Loss: 2.4688379764556885 Validation Loss: 2.551422119140625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 20: Training Loss: 2.4660093784332275 Validation Loss: 2.549328565597534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 21: Training Loss: 2.462948719660441 Validation Loss: 2.546692132949829\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 22: Training Loss: 2.4620139598846436 Validation Loss: 2.5446813106536865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 23: Training Loss: 2.4604172706604004 Validation Loss: 2.542442798614502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 24: Training Loss: 2.457800547281901 Validation Loss: 2.540152072906494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 25: Training Loss: 2.45623246828715 Validation Loss: 2.5377025604248047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 26: Training Loss: 2.4548317591349282 Validation Loss: 2.535727024078369\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 27: Training Loss: 2.453171888987223 Validation Loss: 2.5341176986694336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 28: Training Loss: 2.4514191150665283 Validation Loss: 2.5325241088867188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 29: Training Loss: 2.4494007428487143 Validation Loss: 2.531139373779297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 30: Training Loss: 2.449148416519165 Validation Loss: 2.5288565158843994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 31: Training Loss: 2.4465795358022056 Validation Loss: 2.527383804321289\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 32: Training Loss: 2.4446698824564614 Validation Loss: 2.525418519973755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 33: Training Loss: 2.440861225128174 Validation Loss: 2.5242693424224854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 34: Training Loss: 2.4387007554372153 Validation Loss: 2.5226168632507324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 35: Training Loss: 2.4395132859547934 Validation Loss: 2.5209479331970215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 36: Training Loss: 2.4367937246958413 Validation Loss: 2.519874095916748\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 37: Training Loss: 2.4355271657307944 Validation Loss: 2.5182175636291504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 38: Training Loss: 2.434030055999756 Validation Loss: 2.5160109996795654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 39: Training Loss: 2.4310304323832193 Validation Loss: 2.5139377117156982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 40: Training Loss: 2.4290839036305747 Validation Loss: 2.512127161026001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 41: Training Loss: 2.427898406982422 Validation Loss: 2.511387348175049\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 42: Training Loss: 2.4260782400767007 Validation Loss: 2.5105648040771484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 43: Training Loss: 2.425825595855713 Validation Loss: 2.5088746547698975\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 44: Training Loss: 2.4224541982014975 Validation Loss: 2.5079126358032227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 45: Training Loss: 2.421330531438192 Validation Loss: 2.507901430130005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 46: Training Loss: 2.4199352264404297 Validation Loss: 2.5072460174560547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 47: Training Loss: 2.418937842051188 Validation Loss: 2.505821943283081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 48: Training Loss: 2.417194048563639 Validation Loss: 2.5047106742858887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 49: Training Loss: 2.414968490600586 Validation Loss: 2.503918170928955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 50: Training Loss: 2.4135613441467285 Validation Loss: 2.502690076828003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 51: Training Loss: 2.4118762811024985 Validation Loss: 2.5009539127349854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 52: Training Loss: 2.4113194147745767 Validation Loss: 2.499382495880127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 53: Training Loss: 2.4103810787200928 Validation Loss: 2.4984405040740967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 54: Training Loss: 2.4079535802205405 Validation Loss: 2.496992826461792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 55: Training Loss: 2.4073182741800943 Validation Loss: 2.495469093322754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 56: Training Loss: 2.40553871790568 Validation Loss: 2.4944844245910645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 57: Training Loss: 2.404252529144287 Validation Loss: 2.4933953285217285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 58: Training Loss: 2.4029152393341064 Validation Loss: 2.492286443710327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 59: Training Loss: 2.4019455909729004 Validation Loss: 2.4914677143096924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 60: Training Loss: 2.3992507457733154 Validation Loss: 2.4901716709136963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 61: Training Loss: 2.398912270863851 Validation Loss: 2.489295482635498\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 62: Training Loss: 2.398218790690104 Validation Loss: 2.488680839538574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 63: Training Loss: 2.395508050918579 Validation Loss: 2.487642765045166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 64: Training Loss: 2.3942626317342124 Validation Loss: 2.4860646724700928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 65: Training Loss: 2.3936891555786133 Validation Loss: 2.484541893005371\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 66: Training Loss: 2.3924070994059243 Validation Loss: 2.48321533203125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 67: Training Loss: 2.390419085820516 Validation Loss: 2.4819040298461914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 68: Training Loss: 2.3893206119537354 Validation Loss: 2.4813132286071777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 69: Training Loss: 2.3875159422556558 Validation Loss: 2.4799892902374268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 70: Training Loss: 2.385978619257609 Validation Loss: 2.4783027172088623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 71: Training Loss: 2.3847692807515464 Validation Loss: 2.4774346351623535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 72: Training Loss: 2.3812032540639243 Validation Loss: 2.476694345474243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 73: Training Loss: 2.382711172103882 Validation Loss: 2.4755401611328125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 74: Training Loss: 2.381259044011434 Validation Loss: 2.474785566329956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 75: Training Loss: 2.3797473112742105 Validation Loss: 2.4735848903656006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 76: Training Loss: 2.378281911214193 Validation Loss: 2.4723315238952637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 77: Training Loss: 2.377792994181315 Validation Loss: 2.47164249420166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 78: Training Loss: 2.3757367928822837 Validation Loss: 2.469890832901001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 79: Training Loss: 2.375004609425863 Validation Loss: 2.4685657024383545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 80: Training Loss: 2.3733420372009277 Validation Loss: 2.467726469039917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 81: Training Loss: 2.371654510498047 Validation Loss: 2.4662232398986816\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: Training Loss: 2.371384382247925 Validation Loss: 2.4648704528808594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 83: Training Loss: 2.3667044639587402 Validation Loss: 2.4639039039611816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 84: Training Loss: 2.3666783968607583 Validation Loss: 2.462888240814209\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 85: Training Loss: 2.3662033081054688 Validation Loss: 2.462228536605835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 86: Training Loss: 2.364403486251831 Validation Loss: 2.460611343383789\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 87: Training Loss: 2.363484541575114 Validation Loss: 2.4604411125183105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 88: Training Loss: 2.3618901570638022 Validation Loss: 2.4594244956970215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 89: Training Loss: 2.360685666402181 Validation Loss: 2.457860231399536\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 90: Training Loss: 2.359774589538574 Validation Loss: 2.4568395614624023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 91: Training Loss: 2.3583215872446694 Validation Loss: 2.4549574851989746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 92: Training Loss: 2.3558797041575112 Validation Loss: 2.454014301300049\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 93: Training Loss: 2.356276512145996 Validation Loss: 2.4532546997070312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 94: Training Loss: 2.3532958030700684 Validation Loss: 2.4513790607452393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 95: Training Loss: 2.352970600128174 Validation Loss: 2.4503822326660156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 96: Training Loss: 2.3514718214670816 Validation Loss: 2.4494893550872803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 97: Training Loss: 2.349245548248291 Validation Loss: 2.448364496231079\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 98: Training Loss: 2.348083257675171 Validation Loss: 2.4463727474212646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 99: Training Loss: 2.346548875172933 Validation Loss: 2.445502281188965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 100: Training Loss: 2.3434695402781167 Validation Loss: 2.4443535804748535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 101: Training Loss: 2.34397562344869 Validation Loss: 2.443507671356201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 102: Training Loss: 2.342538038889567 Validation Loss: 2.44274640083313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 103: Training Loss: 2.341540972391764 Validation Loss: 2.4415814876556396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 104: Training Loss: 2.3404584725697837 Validation Loss: 2.441222906112671\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 105: Training Loss: 2.3378966649373374 Validation Loss: 2.439666748046875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 106: Training Loss: 2.3360676765441895 Validation Loss: 2.438035488128662\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 107: Training Loss: 2.33534304300944 Validation Loss: 2.437603712081909\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 108: Training Loss: 2.335148255030314 Validation Loss: 2.436427116394043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 109: Training Loss: 2.332195202509562 Validation Loss: 2.435698986053467\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 110: Training Loss: 2.331193685531616 Validation Loss: 2.4347290992736816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 111: Training Loss: 2.3288167317708335 Validation Loss: 2.4343581199645996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 112: Training Loss: 2.3296025594075522 Validation Loss: 2.4321177005767822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 113: Training Loss: 2.326873540878296 Validation Loss: 2.430884599685669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 114: Training Loss: 2.3255725701649985 Validation Loss: 2.4293668270111084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 115: Training Loss: 2.324200471242269 Validation Loss: 2.4285027980804443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 116: Training Loss: 2.3230698108673096 Validation Loss: 2.4273509979248047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 117: Training Loss: 2.3213366667429605 Validation Loss: 2.4261064529418945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 118: Training Loss: 2.3209577401479087 Validation Loss: 2.42551326751709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 119: Training Loss: 2.317856470743815 Validation Loss: 2.4246366024017334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 120: Training Loss: 2.317526419957479 Validation Loss: 2.4227652549743652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 121: Training Loss: 2.31655486424764 Validation Loss: 2.4219205379486084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 122: Training Loss: 2.3147152264912925 Validation Loss: 2.420980930328369\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 123: Training Loss: 2.3132731914520264 Validation Loss: 2.420145034790039\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 124: Training Loss: 2.311484177907308 Validation Loss: 2.418893814086914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 125: Training Loss: 2.3109538555145264 Validation Loss: 2.417515754699707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 126: Training Loss: 2.3082364400227866 Validation Loss: 2.416452646255493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 127: Training Loss: 2.307633320490519 Validation Loss: 2.415234088897705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 128: Training Loss: 2.3064822355906167 Validation Loss: 2.4142959117889404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 129: Training Loss: 2.3057892322540283 Validation Loss: 2.413240909576416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 130: Training Loss: 2.3038062254587808 Validation Loss: 2.412231683731079\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 131: Training Loss: 2.3020814259847007 Validation Loss: 2.4110116958618164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 132: Training Loss: 2.300607522328695 Validation Loss: 2.409708261489868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 133: Training Loss: 2.2989160219828286 Validation Loss: 2.407900094985962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 134: Training Loss: 2.298027197519938 Validation Loss: 2.4075214862823486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 135: Training Loss: 2.29663880666097 Validation Loss: 2.40610933303833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 136: Training Loss: 2.294917106628418 Validation Loss: 2.4051427841186523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 137: Training Loss: 2.2944435278574624 Validation Loss: 2.4039790630340576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 138: Training Loss: 2.2921356360117593 Validation Loss: 2.403311252593994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 139: Training Loss: 2.290924390157064 Validation Loss: 2.402200937271118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 140: Training Loss: 2.2917123635609946 Validation Loss: 2.4011313915252686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 141: Training Loss: 2.288269837697347 Validation Loss: 2.399601459503174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 142: Training Loss: 2.2865246136983237 Validation Loss: 2.3979406356811523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 143: Training Loss: 2.2854839166005454 Validation Loss: 2.3965041637420654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 144: Training Loss: 2.2839349110921225 Validation Loss: 2.3950493335723877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 145: Training Loss: 2.2841035525004068 Validation Loss: 2.3933568000793457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 146: Training Loss: 2.282615582148234 Validation Loss: 2.393059253692627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 147: Training Loss: 2.281205415725708 Validation Loss: 2.3915553092956543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 148: Training Loss: 2.2796969413757324 Validation Loss: 2.3910059928894043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 149: Training Loss: 2.277124563852946 Validation Loss: 2.39005708694458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 150: Training Loss: 2.2772165139516196 Validation Loss: 2.388803720474243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 151: Training Loss: 2.2738170623779297 Validation Loss: 2.387451171875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 152: Training Loss: 2.2724974155426025 Validation Loss: 2.386350393295288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 153: Training Loss: 2.271482467651367 Validation Loss: 2.3856210708618164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 154: Training Loss: 2.2700840632120767 Validation Loss: 2.3841307163238525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 155: Training Loss: 2.2705175081888833 Validation Loss: 2.3833041191101074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 156: Training Loss: 2.2671324412027993 Validation Loss: 2.381863594055176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 157: Training Loss: 2.265917936960856 Validation Loss: 2.3800179958343506\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158: Training Loss: 2.264084974924723 Validation Loss: 2.378857374191284\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 159: Training Loss: 2.263110955556234 Validation Loss: 2.37784481048584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 160: Training Loss: 2.262084722518921 Validation Loss: 2.3764636516571045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 161: Training Loss: 2.260392665863037 Validation Loss: 2.375492572784424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 162: Training Loss: 2.258196751276652 Validation Loss: 2.374321460723877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 163: Training Loss: 2.2576348781585693 Validation Loss: 2.373594284057617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 164: Training Loss: 2.2567657629648843 Validation Loss: 2.3718910217285156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 165: Training Loss: 2.2540512084960938 Validation Loss: 2.3708715438842773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 166: Training Loss: 2.2531707286834717 Validation Loss: 2.3704006671905518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 167: Training Loss: 2.25182310740153 Validation Loss: 2.368466377258301\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 168: Training Loss: 2.251420259475708 Validation Loss: 2.367020606994629\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 169: Training Loss: 2.24995223681132 Validation Loss: 2.366131067276001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 170: Training Loss: 2.2474513053894043 Validation Loss: 2.3653464317321777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 171: Training Loss: 2.245843251546224 Validation Loss: 2.3646295070648193\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 172: Training Loss: 2.2444624106089273 Validation Loss: 2.3630385398864746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 173: Training Loss: 2.2440133889516196 Validation Loss: 2.361981153488159\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 174: Training Loss: 2.241404374440511 Validation Loss: 2.360624074935913\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 175: Training Loss: 2.2412405808766684 Validation Loss: 2.3589565753936768\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 176: Training Loss: 2.2382164001464844 Validation Loss: 2.357705593109131\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 177: Training Loss: 2.237454096476237 Validation Loss: 2.355757713317871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 178: Training Loss: 2.235013008117676 Validation Loss: 2.3547985553741455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 179: Training Loss: 2.2344391345977783 Validation Loss: 2.353681802749634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 180: Training Loss: 2.2333513100941977 Validation Loss: 2.352410078048706\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 181: Training Loss: 2.2325048446655273 Validation Loss: 2.3510046005249023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 182: Training Loss: 2.229624032974243 Validation Loss: 2.349964141845703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 183: Training Loss: 2.2295568784077964 Validation Loss: 2.349397659301758\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 184: Training Loss: 2.228370110193888 Validation Loss: 2.349412202835083\n",
      "Epoch 185: Training Loss: 2.2260519663492837 Validation Loss: 2.34879207611084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 186: Training Loss: 2.223902384440104 Validation Loss: 2.3470511436462402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 187: Training Loss: 2.2234605153401694 Validation Loss: 2.345402956008911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 188: Training Loss: 2.221127510070801 Validation Loss: 2.343773603439331\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 189: Training Loss: 2.21943736076355 Validation Loss: 2.3416831493377686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 190: Training Loss: 2.2201098601023355 Validation Loss: 2.3410065174102783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 191: Training Loss: 2.216780185699463 Validation Loss: 2.3399860858917236\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 192: Training Loss: 2.215676705042521 Validation Loss: 2.339026689529419\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 193: Training Loss: 2.2153898080190024 Validation Loss: 2.337359666824341\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 194: Training Loss: 2.213674306869507 Validation Loss: 2.336665153503418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 195: Training Loss: 2.209707021713257 Validation Loss: 2.335411548614502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 196: Training Loss: 2.209782044092814 Validation Loss: 2.334064245223999\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 197: Training Loss: 2.2082064946492515 Validation Loss: 2.3324549198150635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 198: Training Loss: 2.2069894472757974 Validation Loss: 2.3316240310668945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 199: Training Loss: 2.207282304763794 Validation Loss: 2.3300857543945312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 200: Training Loss: 2.2041424910227456 Validation Loss: 2.328852415084839\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 201: Training Loss: 2.2022443612416587 Validation Loss: 2.3275821208953857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 202: Training Loss: 2.2016921838124595 Validation Loss: 2.3270657062530518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 203: Training Loss: 2.1997434298197427 Validation Loss: 2.3253142833709717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 204: Training Loss: 2.197913885116577 Validation Loss: 2.32466197013855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 205: Training Loss: 2.196227709452311 Validation Loss: 2.3234126567840576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 206: Training Loss: 2.195038398106893 Validation Loss: 2.3216843605041504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 207: Training Loss: 2.195272445678711 Validation Loss: 2.3200387954711914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 208: Training Loss: 2.1931889851888022 Validation Loss: 2.3189098834991455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 209: Training Loss: 2.190921942392985 Validation Loss: 2.3180830478668213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 210: Training Loss: 2.1895624001820884 Validation Loss: 2.3167924880981445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 211: Training Loss: 2.1869122982025146 Validation Loss: 2.315734624862671\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 212: Training Loss: 2.186484416325887 Validation Loss: 2.3147709369659424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 213: Training Loss: 2.184786240259806 Validation Loss: 2.3137712478637695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 214: Training Loss: 2.1838688055674234 Validation Loss: 2.3120617866516113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 215: Training Loss: 2.182525157928467 Validation Loss: 2.311016798019409\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 216: Training Loss: 2.179658889770508 Validation Loss: 2.30991530418396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 217: Training Loss: 2.178738832473755 Validation Loss: 2.308474063873291\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 218: Training Loss: 2.177911917368571 Validation Loss: 2.3075671195983887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 219: Training Loss: 2.178437074025472 Validation Loss: 2.307171106338501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 220: Training Loss: 2.1749427318573 Validation Loss: 2.3047170639038086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 221: Training Loss: 2.173982858657837 Validation Loss: 2.303396701812744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 222: Training Loss: 2.1727922757466636 Validation Loss: 2.302335023880005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 223: Training Loss: 2.170271714528402 Validation Loss: 2.301126480102539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 224: Training Loss: 2.167940060297648 Validation Loss: 2.300140380859375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 225: Training Loss: 2.1672295729319253 Validation Loss: 2.298503875732422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 226: Training Loss: 2.165954271952311 Validation Loss: 2.2974507808685303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 227: Training Loss: 2.1649934450785318 Validation Loss: 2.295819044113159\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 228: Training Loss: 2.164478143056234 Validation Loss: 2.294743061065674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 229: Training Loss: 2.1615657011667886 Validation Loss: 2.294032096862793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 230: Training Loss: 2.159999211629232 Validation Loss: 2.2929298877716064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 231: Training Loss: 2.158933639526367 Validation Loss: 2.291318893432617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 232: Training Loss: 2.156691869099935 Validation Loss: 2.289918899536133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 233: Training Loss: 2.1554644107818604 Validation Loss: 2.2881083488464355\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234: Training Loss: 2.1534526348114014 Validation Loss: 2.286699056625366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 235: Training Loss: 2.1508661905924478 Validation Loss: 2.2856452465057373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 236: Training Loss: 2.15017032623291 Validation Loss: 2.284881830215454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 237: Training Loss: 2.150364716847738 Validation Loss: 2.2836098670959473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 238: Training Loss: 2.1498025258382163 Validation Loss: 2.282163143157959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 239: Training Loss: 2.1468629042307534 Validation Loss: 2.281039237976074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 240: Training Loss: 2.1445223490397134 Validation Loss: 2.280268907546997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 241: Training Loss: 2.142122427622477 Validation Loss: 2.279447555541992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 242: Training Loss: 2.141653140385946 Validation Loss: 2.279113292694092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 243: Training Loss: 2.140411297480265 Validation Loss: 2.2772634029388428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 244: Training Loss: 2.141190767288208 Validation Loss: 2.275874376296997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 245: Training Loss: 2.1374473571777344 Validation Loss: 2.274441719055176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 246: Training Loss: 2.1354664961496987 Validation Loss: 2.272569417953491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 247: Training Loss: 2.1343066692352295 Validation Loss: 2.27156925201416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 248: Training Loss: 2.1325058937072754 Validation Loss: 2.2702882289886475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 249: Training Loss: 2.1317596435546875 Validation Loss: 2.2689321041107178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 250: Training Loss: 2.130106051762899 Validation Loss: 2.268052339553833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 251: Training Loss: 2.12809681892395 Validation Loss: 2.266444683074951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 252: Training Loss: 2.127760092417399 Validation Loss: 2.264878034591675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 253: Training Loss: 2.1254546642303467 Validation Loss: 2.264082431793213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 254: Training Loss: 2.124277671178182 Validation Loss: 2.262826919555664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 255: Training Loss: 2.1232279936472573 Validation Loss: 2.2616145610809326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 256: Training Loss: 2.1212021509806314 Validation Loss: 2.2607169151306152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 257: Training Loss: 2.1197144190470376 Validation Loss: 2.259503126144409\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 258: Training Loss: 2.118183453877767 Validation Loss: 2.2583675384521484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 259: Training Loss: 2.116940498352051 Validation Loss: 2.256765842437744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 260: Training Loss: 2.1140310764312744 Validation Loss: 2.255349636077881\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 261: Training Loss: 2.113175868988037 Validation Loss: 2.2541418075561523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 262: Training Loss: 2.1110931237538657 Validation Loss: 2.252462387084961\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 263: Training Loss: 2.110197067260742 Validation Loss: 2.2509446144104004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 264: Training Loss: 2.108729124069214 Validation Loss: 2.2502503395080566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 265: Training Loss: 2.107109785079956 Validation Loss: 2.2486155033111572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 266: Training Loss: 2.1043294270833335 Validation Loss: 2.2476134300231934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 267: Training Loss: 2.105532169342041 Validation Loss: 2.2465460300445557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 268: Training Loss: 2.10311492284139 Validation Loss: 2.244873046875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 269: Training Loss: 2.099003871281942 Validation Loss: 2.244030714035034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 270: Training Loss: 2.099836826324463 Validation Loss: 2.2428739070892334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 271: Training Loss: 2.098937749862671 Validation Loss: 2.2415614128112793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 272: Training Loss: 2.096876859664917 Validation Loss: 2.2402265071868896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 273: Training Loss: 2.0953632990519204 Validation Loss: 2.2388031482696533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 274: Training Loss: 2.0936251481374106 Validation Loss: 2.2374117374420166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 275: Training Loss: 2.0921150843302407 Validation Loss: 2.236513137817383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 276: Training Loss: 2.091179927190145 Validation Loss: 2.2352139949798584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 277: Training Loss: 2.089582363764445 Validation Loss: 2.234403371810913\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 278: Training Loss: 2.087712287902832 Validation Loss: 2.2325117588043213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 279: Training Loss: 2.0866188208262124 Validation Loss: 2.2309460639953613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 280: Training Loss: 2.084754705429077 Validation Loss: 2.2292513847351074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 281: Training Loss: 2.0826762517293296 Validation Loss: 2.2281076908111572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 282: Training Loss: 2.082304318745931 Validation Loss: 2.227130174636841\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 283: Training Loss: 2.079258362452189 Validation Loss: 2.2262959480285645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 284: Training Loss: 2.0790496667226157 Validation Loss: 2.2244255542755127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 285: Training Loss: 2.07688577969869 Validation Loss: 2.2236385345458984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 286: Training Loss: 2.0761001904805503 Validation Loss: 2.2231268882751465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 287: Training Loss: 2.0751312573750815 Validation Loss: 2.2220733165740967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 288: Training Loss: 2.07215150197347 Validation Loss: 2.2207348346710205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 289: Training Loss: 2.071025530497233 Validation Loss: 2.218371629714966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 290: Training Loss: 2.070153554280599 Validation Loss: 2.2166218757629395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 291: Training Loss: 2.0678619543711343 Validation Loss: 2.2143492698669434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 292: Training Loss: 2.066049575805664 Validation Loss: 2.2131693363189697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 293: Training Loss: 2.0647300084431968 Validation Loss: 2.2128593921661377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 294: Training Loss: 2.063028653462728 Validation Loss: 2.2124009132385254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 295: Training Loss: 2.0617748896280923 Validation Loss: 2.211538553237915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 296: Training Loss: 2.061243931452433 Validation Loss: 2.209780216217041\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 297: Training Loss: 2.0591320991516113 Validation Loss: 2.208782196044922\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 298: Training Loss: 2.056915760040283 Validation Loss: 2.207706928253174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 299: Training Loss: 2.0565072695414224 Validation Loss: 2.2053558826446533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 300: Training Loss: 2.055291732152303 Validation Loss: 2.2039029598236084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 301: Training Loss: 2.054550806681315 Validation Loss: 2.2027628421783447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 302: Training Loss: 2.0514351526896157 Validation Loss: 2.201559066772461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 303: Training Loss: 2.0488550662994385 Validation Loss: 2.2005553245544434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 304: Training Loss: 2.048193176587423 Validation Loss: 2.199164628982544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 305: Training Loss: 2.046886603037516 Validation Loss: 2.1982688903808594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 306: Training Loss: 2.0446515878041587 Validation Loss: 2.196946144104004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 307: Training Loss: 2.0443245569864907 Validation Loss: 2.1950790882110596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 308: Training Loss: 2.0410085916519165 Validation Loss: 2.1937220096588135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 309: Training Loss: 2.041724920272827 Validation Loss: 2.19195818901062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 310: Training Loss: 2.0389887491861978 Validation Loss: 2.1909663677215576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 311: Training Loss: 2.0374767780303955 Validation Loss: 2.1899044513702393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 312: Training Loss: 2.03707488377889 Validation Loss: 2.189770460128784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 313: Training Loss: 2.034124771753947 Validation Loss: 2.187915325164795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 314: Training Loss: 2.0324783325195312 Validation Loss: 2.1862964630126953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 315: Training Loss: 2.03130833307902 Validation Loss: 2.185171365737915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 316: Training Loss: 2.0329984029134116 Validation Loss: 2.183957576751709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 317: Training Loss: 2.028276205062866 Validation Loss: 2.1815359592437744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 318: Training Loss: 2.0267388820648193 Validation Loss: 2.1807894706726074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 319: Training Loss: 2.0254492362340293 Validation Loss: 2.1800248622894287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 320: Training Loss: 2.0234665473302207 Validation Loss: 2.1785731315612793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 321: Training Loss: 2.022645274798075 Validation Loss: 2.177443742752075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 322: Training Loss: 2.019993782043457 Validation Loss: 2.177018880844116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 323: Training Loss: 2.019024213155111 Validation Loss: 2.1748857498168945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 324: Training Loss: 2.0187568267186484 Validation Loss: 2.1734235286712646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 325: Training Loss: 2.016366402308146 Validation Loss: 2.171926736831665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 326: Training Loss: 2.014425198237101 Validation Loss: 2.1708245277404785\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 327: Training Loss: 2.0130029916763306 Validation Loss: 2.169553518295288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 328: Training Loss: 2.0099270343780518 Validation Loss: 2.1684248447418213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 329: Training Loss: 2.0094734032948813 Validation Loss: 2.1671981811523438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 330: Training Loss: 2.0080947081247964 Validation Loss: 2.1666038036346436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 331: Training Loss: 2.0069068670272827 Validation Loss: 2.1658132076263428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 332: Training Loss: 2.006424864133199 Validation Loss: 2.164829730987549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 333: Training Loss: 2.004239281018575 Validation Loss: 2.162108898162842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 334: Training Loss: 2.0023502111434937 Validation Loss: 2.1605618000030518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 335: Training Loss: 1.9998126029968262 Validation Loss: 2.158635377883911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 336: Training Loss: 1.9985734621683757 Validation Loss: 2.1574273109436035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 337: Training Loss: 1.9975708723068237 Validation Loss: 2.1564204692840576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 338: Training Loss: 1.9941463073094685 Validation Loss: 2.1559252738952637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 339: Training Loss: 1.994830886522929 Validation Loss: 2.1552810668945312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 340: Training Loss: 1.9936269521713257 Validation Loss: 2.1551480293273926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 341: Training Loss: 1.991419514020284 Validation Loss: 2.1527597904205322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 342: Training Loss: 1.9901209274927776 Validation Loss: 2.150775909423828\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 343: Training Loss: 1.9876125653584797 Validation Loss: 2.149665117263794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 344: Training Loss: 1.988976240158081 Validation Loss: 2.14799165725708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 345: Training Loss: 1.986171801884969 Validation Loss: 2.1462981700897217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 346: Training Loss: 1.984765926996867 Validation Loss: 2.1446566581726074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 347: Training Loss: 1.983567198117574 Validation Loss: 2.143885850906372\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 348: Training Loss: 1.9810020526250203 Validation Loss: 2.1435744762420654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 349: Training Loss: 1.9779049555460613 Validation Loss: 2.142162561416626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 350: Training Loss: 1.9770602385203044 Validation Loss: 2.1403541564941406\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 351: Training Loss: 1.975177009900411 Validation Loss: 2.140108346939087\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 352: Training Loss: 1.9754602511723836 Validation Loss: 2.1385107040405273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 353: Training Loss: 1.97476061185201 Validation Loss: 2.1368730068206787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 354: Training Loss: 1.971693754196167 Validation Loss: 2.1354339122772217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 355: Training Loss: 1.970920483271281 Validation Loss: 2.1343300342559814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 356: Training Loss: 1.9691481192906697 Validation Loss: 2.1333370208740234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 357: Training Loss: 1.9675944248835247 Validation Loss: 2.1311895847320557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 358: Training Loss: 1.965973973274231 Validation Loss: 2.130124092102051\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 359: Training Loss: 1.9666181405385335 Validation Loss: 2.128253698348999\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 360: Training Loss: 1.9617148637771606 Validation Loss: 2.1267080307006836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 361: Training Loss: 1.9635000626246135 Validation Loss: 2.1264662742614746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 362: Training Loss: 1.960874120394389 Validation Loss: 2.1251649856567383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 363: Training Loss: 1.958081046740214 Validation Loss: 2.1240122318267822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 364: Training Loss: 1.9565918445587158 Validation Loss: 2.1230311393737793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 365: Training Loss: 1.9556856552759807 Validation Loss: 2.1220147609710693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 366: Training Loss: 1.9554386536280315 Validation Loss: 2.121168851852417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 367: Training Loss: 1.9526142676671345 Validation Loss: 2.119385242462158\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 368: Training Loss: 1.9494114716847737 Validation Loss: 2.1176483631134033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 369: Training Loss: 1.949086348215739 Validation Loss: 2.1156795024871826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 370: Training Loss: 1.9480866988499959 Validation Loss: 2.1155567169189453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 371: Training Loss: 1.948526660601298 Validation Loss: 2.1141111850738525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 372: Training Loss: 1.9448675711949666 Validation Loss: 2.11338210105896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 373: Training Loss: 1.9434843858083088 Validation Loss: 2.111905574798584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 374: Training Loss: 1.9418328205744426 Validation Loss: 2.110405445098877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 375: Training Loss: 1.9393973747889202 Validation Loss: 2.1088883876800537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 376: Training Loss: 1.9370614687601726 Validation Loss: 2.1076173782348633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 377: Training Loss: 1.9346987009048462 Validation Loss: 2.106736898422241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 378: Training Loss: 1.936292052268982 Validation Loss: 2.105909824371338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 379: Training Loss: 1.9359697500864665 Validation Loss: 2.105278730392456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 380: Training Loss: 1.9325869878133137 Validation Loss: 2.10310959815979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 381: Training Loss: 1.9304331143697102 Validation Loss: 2.1019539833068848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 382: Training Loss: 1.9308970769246419 Validation Loss: 2.0999608039855957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 383: Training Loss: 1.929871678352356 Validation Loss: 2.098115921020508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 384: Training Loss: 1.9295337200164795 Validation Loss: 2.096620559692383\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385: Training Loss: 1.9244977235794067 Validation Loss: 2.095930337905884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 386: Training Loss: 1.9266008536020915 Validation Loss: 2.0953073501586914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 387: Training Loss: 1.9225993156433105 Validation Loss: 2.094078540802002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 388: Training Loss: 1.9214093685150146 Validation Loss: 2.093146562576294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 389: Training Loss: 1.918891151746114 Validation Loss: 2.0925395488739014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 390: Training Loss: 1.9186040163040161 Validation Loss: 2.090456962585449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 391: Training Loss: 1.9164653221766155 Validation Loss: 2.0878422260284424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 392: Training Loss: 1.917290170987447 Validation Loss: 2.08634090423584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 393: Training Loss: 1.9107252359390259 Validation Loss: 2.0853090286254883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 394: Training Loss: 1.9113553365071614 Validation Loss: 2.084414005279541\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 395: Training Loss: 1.9102978308995564 Validation Loss: 2.0841243267059326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 396: Training Loss: 1.9100032647450764 Validation Loss: 2.083911418914795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 397: Training Loss: 1.9083436727523804 Validation Loss: 2.081906795501709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 398: Training Loss: 1.9059138695398967 Validation Loss: 2.080698013305664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 399: Training Loss: 1.9030767281850178 Validation Loss: 2.078958511352539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 400: Training Loss: 1.903499682744344 Validation Loss: 2.077119827270508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 401: Training Loss: 1.901103178660075 Validation Loss: 2.075936794281006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 402: Training Loss: 1.898978590965271 Validation Loss: 2.0752012729644775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 403: Training Loss: 1.898781696955363 Validation Loss: 2.0734219551086426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 404: Training Loss: 1.8973023096720378 Validation Loss: 2.072598695755005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 405: Training Loss: 1.89517080783844 Validation Loss: 2.0722737312316895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 406: Training Loss: 1.8940822680791218 Validation Loss: 2.0709664821624756\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 407: Training Loss: 1.8927300771077473 Validation Loss: 2.0688092708587646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 408: Training Loss: 1.8909839789072673 Validation Loss: 2.0674941539764404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 409: Training Loss: 1.889878789583842 Validation Loss: 2.065101385116577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 410: Training Loss: 1.888785719871521 Validation Loss: 2.0647106170654297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 411: Training Loss: 1.8889235655466716 Validation Loss: 2.0636842250823975\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 412: Training Loss: 1.8848838408788045 Validation Loss: 2.0631189346313477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 413: Training Loss: 1.883933703104655 Validation Loss: 2.0621984004974365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 414: Training Loss: 1.882026235262553 Validation Loss: 2.0609824657440186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 415: Training Loss: 1.8811962207158406 Validation Loss: 2.05967378616333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 416: Training Loss: 1.881156365076701 Validation Loss: 2.057706117630005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 417: Training Loss: 1.8775107860565186 Validation Loss: 2.0570085048675537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 418: Training Loss: 1.8762849569320679 Validation Loss: 2.056541919708252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 419: Training Loss: 1.8740757306416829 Validation Loss: 2.0550403594970703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 420: Training Loss: 1.8746047814687092 Validation Loss: 2.053405523300171\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 421: Training Loss: 1.8720293045043945 Validation Loss: 2.050731897354126\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 422: Training Loss: 1.8725584348042805 Validation Loss: 2.050859212875366\n",
      "Epoch 423: Training Loss: 1.8702900807062786 Validation Loss: 2.0491364002227783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 424: Training Loss: 1.8743530909220378 Validation Loss: 2.047438621520996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 425: Training Loss: 1.865463137626648 Validation Loss: 2.0469799041748047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 426: Training Loss: 1.8638195991516113 Validation Loss: 2.0463156700134277\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 427: Training Loss: 1.8638750712076824 Validation Loss: 2.044602394104004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 428: Training Loss: 1.8623602787653606 Validation Loss: 2.043086051940918\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 429: Training Loss: 1.8600844939549763 Validation Loss: 2.0416829586029053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 430: Training Loss: 1.8589327732721965 Validation Loss: 2.0403149127960205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 431: Training Loss: 1.8581579526265461 Validation Loss: 2.039301872253418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 432: Training Loss: 1.8558449745178223 Validation Loss: 2.0384063720703125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 433: Training Loss: 1.8541735410690308 Validation Loss: 2.036808729171753\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 434: Training Loss: 1.8529991308848064 Validation Loss: 2.035083532333374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 435: Training Loss: 1.8528478542963664 Validation Loss: 2.0337624549865723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 436: Training Loss: 1.851429541905721 Validation Loss: 2.0335071086883545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 437: Training Loss: 1.8467265764872234 Validation Loss: 2.0333526134490967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 438: Training Loss: 1.8484433889389038 Validation Loss: 2.0319554805755615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 439: Training Loss: 1.846524755160014 Validation Loss: 2.030102491378784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 440: Training Loss: 1.844665288925171 Validation Loss: 2.028531074523926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 441: Training Loss: 1.8431875308354695 Validation Loss: 2.027657985687256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 442: Training Loss: 1.8411102692286174 Validation Loss: 2.0256614685058594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 443: Training Loss: 1.8397968610127766 Validation Loss: 2.0239815711975098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 444: Training Loss: 1.839250922203064 Validation Loss: 2.022092580795288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 445: Training Loss: 1.8361166715621948 Validation Loss: 2.0215630531311035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 446: Training Loss: 1.8351081609725952 Validation Loss: 2.0202035903930664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 447: Training Loss: 1.8340221643447876 Validation Loss: 2.019541025161743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 448: Training Loss: 1.8323129018147786 Validation Loss: 2.0198781490325928\n",
      "Epoch 449: Training Loss: 1.8309958378473918 Validation Loss: 2.0196805000305176\n",
      "Epoch 450: Training Loss: 1.8297999302546184 Validation Loss: 2.0170772075653076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 451: Training Loss: 1.828101674715678 Validation Loss: 2.014962673187256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 452: Training Loss: 1.824389378229777 Validation Loss: 2.0142228603363037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 453: Training Loss: 1.8256572484970093 Validation Loss: 2.0125951766967773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 454: Training Loss: 1.8237110773722331 Validation Loss: 2.0113942623138428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 455: Training Loss: 1.8219903707504272 Validation Loss: 2.009889841079712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 456: Training Loss: 1.8216582139333088 Validation Loss: 2.008364677429199\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 457: Training Loss: 1.8197353680928547 Validation Loss: 2.007514715194702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 458: Training Loss: 1.8182278474171956 Validation Loss: 2.0061850547790527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 459: Training Loss: 1.8166832129160564 Validation Loss: 2.0055129528045654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 460: Training Loss: 1.814343015352885 Validation Loss: 2.0044171810150146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 461: Training Loss: 1.8135840892791748 Validation Loss: 2.003561496734619\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 462: Training Loss: 1.8140968084335327 Validation Loss: 2.002363920211792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 463: Training Loss: 1.8115208546320598 Validation Loss: 2.0010056495666504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 464: Training Loss: 1.8152028719584148 Validation Loss: 2.000096559524536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 465: Training Loss: 1.8090282678604126 Validation Loss: 1.9985249042510986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 466: Training Loss: 1.8066437641779582 Validation Loss: 1.9973533153533936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 467: Training Loss: 1.8058093388875325 Validation Loss: 1.9954571723937988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 468: Training Loss: 1.8027113278706868 Validation Loss: 1.9946138858795166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 469: Training Loss: 1.8025635878245037 Validation Loss: 1.993032455444336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 470: Training Loss: 1.8016867240269978 Validation Loss: 1.991660714149475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 471: Training Loss: 1.7984451055526733 Validation Loss: 1.9905719757080078\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 472: Training Loss: 1.7983306248982747 Validation Loss: 1.989493727684021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 473: Training Loss: 1.7986961205800374 Validation Loss: 1.9883472919464111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 474: Training Loss: 1.7960644960403442 Validation Loss: 1.9881067276000977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 475: Training Loss: 1.7950694958368938 Validation Loss: 1.9864331483840942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 476: Training Loss: 1.7926017045974731 Validation Loss: 1.985226035118103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 477: Training Loss: 1.7917587757110596 Validation Loss: 1.9837380647659302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 478: Training Loss: 1.7901148398717244 Validation Loss: 1.9829655885696411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 479: Training Loss: 1.7903389533360798 Validation Loss: 1.9815229177474976\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 480: Training Loss: 1.7910319169362385 Validation Loss: 1.9788885116577148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 481: Training Loss: 1.7859949270884197 Validation Loss: 1.978100299835205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 482: Training Loss: 1.784305731455485 Validation Loss: 1.9770904779434204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 483: Training Loss: 1.7833202282587688 Validation Loss: 1.9763504266738892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 484: Training Loss: 1.7815325260162354 Validation Loss: 1.9763082265853882\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 485: Training Loss: 1.7805966933568318 Validation Loss: 1.9762083292007446\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 486: Training Loss: 1.7782199382781982 Validation Loss: 1.97459876537323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 487: Training Loss: 1.777306358019511 Validation Loss: 1.972307801246643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 488: Training Loss: 1.7774764696757 Validation Loss: 1.9696451425552368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 489: Training Loss: 1.7740209102630615 Validation Loss: 1.9686623811721802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 490: Training Loss: 1.7754490772883098 Validation Loss: 1.967944622039795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 491: Training Loss: 1.772443135579427 Validation Loss: 1.9665579795837402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 492: Training Loss: 1.7690325578053792 Validation Loss: 1.965867280960083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 493: Training Loss: 1.7695419390996296 Validation Loss: 1.964733600616455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 494: Training Loss: 1.7676204442977905 Validation Loss: 1.9634145498275757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 495: Training Loss: 1.7649697065353394 Validation Loss: 1.962438941001892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 496: Training Loss: 1.7653032938639324 Validation Loss: 1.9611796140670776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 497: Training Loss: 1.7638915379842122 Validation Loss: 1.96116304397583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 498: Training Loss: 1.7632710536321003 Validation Loss: 1.9601385593414307\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 499: Training Loss: 1.7590233484903972 Validation Loss: 1.9582561254501343\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 500: Training Loss: 1.7584084669748943 Validation Loss: 1.956692099571228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 501: Training Loss: 1.7575857639312744 Validation Loss: 1.9551174640655518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 502: Training Loss: 1.7551958958307903 Validation Loss: 1.9538962841033936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 503: Training Loss: 1.7546837329864502 Validation Loss: 1.9527844190597534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 504: Training Loss: 1.7563687960306804 Validation Loss: 1.9526480436325073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 505: Training Loss: 1.7530601421991985 Validation Loss: 1.9510352611541748\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 506: Training Loss: 1.7511324882507324 Validation Loss: 1.9491537809371948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 507: Training Loss: 1.7507842779159546 Validation Loss: 1.94878089427948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 508: Training Loss: 1.7483972311019897 Validation Loss: 1.946919322013855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 509: Training Loss: 1.7474088668823242 Validation Loss: 1.9458329677581787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 510: Training Loss: 1.7465515931447346 Validation Loss: 1.944549322128296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 511: Training Loss: 1.7451295455296834 Validation Loss: 1.9446485042572021\n",
      "Epoch 512: Training Loss: 1.7415141264597576 Validation Loss: 1.9438245296478271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 513: Training Loss: 1.7415964206059773 Validation Loss: 1.9421625137329102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 514: Training Loss: 1.74091374874115 Validation Loss: 1.9404315948486328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 515: Training Loss: 1.738658865292867 Validation Loss: 1.9385651350021362\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 516: Training Loss: 1.7385484377543132 Validation Loss: 1.937697410583496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 517: Training Loss: 1.7358264923095703 Validation Loss: 1.9368928670883179\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 518: Training Loss: 1.7362682024637859 Validation Loss: 1.9353833198547363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 519: Training Loss: 1.7333666880925496 Validation Loss: 1.9340651035308838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 520: Training Loss: 1.7314172983169556 Validation Loss: 1.9333807229995728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 521: Training Loss: 1.7301606734593709 Validation Loss: 1.931625247001648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 522: Training Loss: 1.7295036713282268 Validation Loss: 1.9306941032409668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 523: Training Loss: 1.7298575639724731 Validation Loss: 1.9296209812164307\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 524: Training Loss: 1.7267043193181355 Validation Loss: 1.9282814264297485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 525: Training Loss: 1.726022760073344 Validation Loss: 1.9276596307754517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 526: Training Loss: 1.7239126364390056 Validation Loss: 1.927253007888794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 527: Training Loss: 1.7228394349416096 Validation Loss: 1.926445722579956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 528: Training Loss: 1.7213789224624634 Validation Loss: 1.9251806735992432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 529: Training Loss: 1.720243771870931 Validation Loss: 1.922293782234192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 530: Training Loss: 1.718063473701477 Validation Loss: 1.9201327562332153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 531: Training Loss: 1.7195773522059123 Validation Loss: 1.9191398620605469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 532: Training Loss: 1.7166268428166707 Validation Loss: 1.919177770614624\n",
      "Epoch 533: Training Loss: 1.7141629457473755 Validation Loss: 1.9195383787155151\n",
      "Epoch 534: Training Loss: 1.7132518291473389 Validation Loss: 1.9180097579956055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 535: Training Loss: 1.7129431168238323 Validation Loss: 1.91649329662323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 536: Training Loss: 1.7108362118403118 Validation Loss: 1.915837049484253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 537: Training Loss: 1.7094364563624065 Validation Loss: 1.9142554998397827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 538: Training Loss: 1.7073253393173218 Validation Loss: 1.9121806621551514\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 539: Training Loss: 1.7064162492752075 Validation Loss: 1.9103041887283325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 540: Training Loss: 1.705349326133728 Validation Loss: 1.9099329710006714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 541: Training Loss: 1.7054949601491292 Validation Loss: 1.9094411134719849\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 542: Training Loss: 1.703256646792094 Validation Loss: 1.9080332517623901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 543: Training Loss: 1.7044589122136433 Validation Loss: 1.9067211151123047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 544: Training Loss: 1.6998773018519084 Validation Loss: 1.9060029983520508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 545: Training Loss: 1.6982804139455159 Validation Loss: 1.905655026435852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 546: Training Loss: 1.7039990425109863 Validation Loss: 1.904211401939392\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 547: Training Loss: 1.6954976717631023 Validation Loss: 1.9023123979568481\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 548: Training Loss: 1.6948004563649495 Validation Loss: 1.901004433631897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 549: Training Loss: 1.6933650175730388 Validation Loss: 1.8993011713027954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 550: Training Loss: 1.6919434865315754 Validation Loss: 1.8983887434005737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 551: Training Loss: 1.6907146374384563 Validation Loss: 1.8971277475357056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 552: Training Loss: 1.6887234052022297 Validation Loss: 1.896742820739746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 553: Training Loss: 1.6856271028518677 Validation Loss: 1.8964396715164185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 554: Training Loss: 1.6868773698806763 Validation Loss: 1.8955951929092407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 555: Training Loss: 1.6862524350484211 Validation Loss: 1.8931900262832642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 556: Training Loss: 1.6850351095199585 Validation Loss: 1.8908220529556274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 557: Training Loss: 1.6832855145136516 Validation Loss: 1.8898721933364868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 558: Training Loss: 1.6815939744313557 Validation Loss: 1.8891626596450806\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 559: Training Loss: 1.6797646284103394 Validation Loss: 1.887835144996643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 560: Training Loss: 1.67874809106191 Validation Loss: 1.88718581199646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 561: Training Loss: 1.6782714128494263 Validation Loss: 1.886491298675537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 562: Training Loss: 1.6736394166946411 Validation Loss: 1.8854671716690063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 563: Training Loss: 1.6750487089157104 Validation Loss: 1.8842569589614868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 564: Training Loss: 1.6741130352020264 Validation Loss: 1.8836454153060913\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 565: Training Loss: 1.6733346780141194 Validation Loss: 1.882232904434204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 566: Training Loss: 1.6704209248224895 Validation Loss: 1.8799035549163818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 567: Training Loss: 1.6690982580184937 Validation Loss: 1.8793182373046875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 568: Training Loss: 1.6674004395802815 Validation Loss: 1.8783619403839111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 569: Training Loss: 1.6664204200108845 Validation Loss: 1.8777222633361816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 570: Training Loss: 1.6672813892364502 Validation Loss: 1.8766682147979736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 571: Training Loss: 1.6650816202163696 Validation Loss: 1.874647855758667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 572: Training Loss: 1.6653169393539429 Validation Loss: 1.8751304149627686\n",
      "Epoch 573: Training Loss: 1.662744164466858 Validation Loss: 1.8741873502731323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 574: Training Loss: 1.6611011823018391 Validation Loss: 1.8711034059524536\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 575: Training Loss: 1.6610345443089802 Validation Loss: 1.8693263530731201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 576: Training Loss: 1.6568971474965413 Validation Loss: 1.8685888051986694\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 577: Training Loss: 1.6563523610432942 Validation Loss: 1.8686883449554443\n",
      "Epoch 578: Training Loss: 1.6548192103703816 Validation Loss: 1.8682807683944702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 579: Training Loss: 1.653864582379659 Validation Loss: 1.8668831586837769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 580: Training Loss: 1.6543396711349487 Validation Loss: 1.8654924631118774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 581: Training Loss: 1.6523290872573853 Validation Loss: 1.8639041185379028\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 582: Training Loss: 1.6522827943166096 Validation Loss: 1.8623088598251343\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 583: Training Loss: 1.6521953741709392 Validation Loss: 1.8614822626113892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 584: Training Loss: 1.6476030747095745 Validation Loss: 1.861299753189087\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 585: Training Loss: 1.6507444779078166 Validation Loss: 1.8600019216537476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 586: Training Loss: 1.6439272165298462 Validation Loss: 1.858746886253357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 587: Training Loss: 1.6435471773147583 Validation Loss: 1.8576661348342896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 588: Training Loss: 1.6418135563532512 Validation Loss: 1.8556323051452637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 589: Training Loss: 1.6424638430277507 Validation Loss: 1.854899525642395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 590: Training Loss: 1.6405092080434163 Validation Loss: 1.8535088300704956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 591: Training Loss: 1.638258457183838 Validation Loss: 1.8525887727737427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 592: Training Loss: 1.638694206873576 Validation Loss: 1.8513914346694946\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 593: Training Loss: 1.6359669367472331 Validation Loss: 1.8505268096923828\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 594: Training Loss: 1.6352259318033855 Validation Loss: 1.850249171257019\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 595: Training Loss: 1.6339709361394246 Validation Loss: 1.8485833406448364\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 596: Training Loss: 1.6328134536743164 Validation Loss: 1.8473544120788574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 597: Training Loss: 1.6329726775487263 Validation Loss: 1.844975233078003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 598: Training Loss: 1.6294881502787273 Validation Loss: 1.844658374786377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 599: Training Loss: 1.6296602884928386 Validation Loss: 1.8446574211120605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 600: Training Loss: 1.628664493560791 Validation Loss: 1.8440816402435303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 601: Training Loss: 1.6259528398513794 Validation Loss: 1.8432486057281494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 602: Training Loss: 1.625595211982727 Validation Loss: 1.8405234813690186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 603: Training Loss: 1.6236104170481365 Validation Loss: 1.8387945890426636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 604: Training Loss: 1.6257582902908325 Validation Loss: 1.8373839855194092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 605: Training Loss: 1.6212414105733235 Validation Loss: 1.836887240409851\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 606: Training Loss: 1.6211095253626506 Validation Loss: 1.836042881011963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 607: Training Loss: 1.6165519555409749 Validation Loss: 1.83710515499115\n",
      "Epoch 608: Training Loss: 1.6172668139139812 Validation Loss: 1.836033821105957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 609: Training Loss: 1.6149054368336995 Validation Loss: 1.8337050676345825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 610: Training Loss: 1.614816427230835 Validation Loss: 1.831998348236084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 611: Training Loss: 1.6129258076349895 Validation Loss: 1.8303242921829224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 612: Training Loss: 1.6128154595692952 Validation Loss: 1.8285512924194336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 613: Training Loss: 1.6113632520039876 Validation Loss: 1.8284825086593628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 614: Training Loss: 1.6102677583694458 Validation Loss: 1.8277356624603271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 615: Training Loss: 1.6093564828236897 Validation Loss: 1.8270423412322998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 616: Training Loss: 1.60699458916982 Validation Loss: 1.8260289430618286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 617: Training Loss: 1.6084773937861125 Validation Loss: 1.8247603178024292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 618: Training Loss: 1.6056805054346721 Validation Loss: 1.8237113952636719\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 619: Training Loss: 1.6030689080556233 Validation Loss: 1.821947455406189\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 620: Training Loss: 1.6045087178548176 Validation Loss: 1.820102334022522\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 621: Training Loss: 1.6006651322046916 Validation Loss: 1.819892406463623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 622: Training Loss: 1.5994936625162761 Validation Loss: 1.818529486656189\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 623: Training Loss: 1.599440375963847 Validation Loss: 1.8189167976379395\n",
      "Epoch 624: Training Loss: 1.5993614594141643 Validation Loss: 1.8185721635818481\n",
      "Epoch 625: Training Loss: 1.5961546103159587 Validation Loss: 1.8160619735717773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 626: Training Loss: 1.593631664911906 Validation Loss: 1.8140747547149658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 627: Training Loss: 1.5923292636871338 Validation Loss: 1.8135485649108887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 628: Training Loss: 1.5936728715896606 Validation Loss: 1.8128644227981567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 629: Training Loss: 1.5919698079427083 Validation Loss: 1.811964750289917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 630: Training Loss: 1.5898272196451824 Validation Loss: 1.8121240139007568\n",
      "Epoch 631: Training Loss: 1.587815483411153 Validation Loss: 1.8100749254226685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 632: Training Loss: 1.5875170628229778 Validation Loss: 1.8079109191894531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 633: Training Loss: 1.5844914118448894 Validation Loss: 1.8062342405319214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 634: Training Loss: 1.5864276091257732 Validation Loss: 1.8056838512420654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 635: Training Loss: 1.5846460262934368 Validation Loss: 1.804965853691101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 636: Training Loss: 1.5823840697606404 Validation Loss: 1.8045903444290161\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 637: Training Loss: 1.5810151894887288 Validation Loss: 1.80360746383667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 638: Training Loss: 1.5800687472025554 Validation Loss: 1.803026556968689\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 639: Training Loss: 1.5783044894536336 Validation Loss: 1.801621437072754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 640: Training Loss: 1.577302614847819 Validation Loss: 1.8006315231323242\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 641: Training Loss: 1.576714038848877 Validation Loss: 1.79776930809021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 642: Training Loss: 1.5723694562911987 Validation Loss: 1.7960855960845947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 643: Training Loss: 1.5740912755330403 Validation Loss: 1.7954931259155273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 644: Training Loss: 1.574101209640503 Validation Loss: 1.795409917831421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 645: Training Loss: 1.5710492928822835 Validation Loss: 1.7947423458099365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 646: Training Loss: 1.5708452860514324 Validation Loss: 1.7947360277175903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 647: Training Loss: 1.5686997175216675 Validation Loss: 1.793567180633545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 648: Training Loss: 1.5677272876103718 Validation Loss: 1.7924607992172241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 649: Training Loss: 1.5674203236897786 Validation Loss: 1.7909233570098877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 650: Training Loss: 1.5650392373402913 Validation Loss: 1.7893991470336914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 651: Training Loss: 1.5649175643920898 Validation Loss: 1.787166714668274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 652: Training Loss: 1.564040978749593 Validation Loss: 1.7861454486846924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 653: Training Loss: 1.562256654103597 Validation Loss: 1.7848671674728394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 654: Training Loss: 1.560671091079712 Validation Loss: 1.783862590789795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 655: Training Loss: 1.5596139033635457 Validation Loss: 1.7838795185089111\n",
      "Epoch 656: Training Loss: 1.5605568488438923 Validation Loss: 1.783129334449768\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 657: Training Loss: 1.5565909544626872 Validation Loss: 1.7822575569152832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 658: Training Loss: 1.5555121898651123 Validation Loss: 1.7820311784744263\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 659: Training Loss: 1.5543128649393718 Validation Loss: 1.7805265188217163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 660: Training Loss: 1.5533146858215332 Validation Loss: 1.7807658910751343\n",
      "Epoch 661: Training Loss: 1.5525873899459839 Validation Loss: 1.7791380882263184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 662: Training Loss: 1.5506562391916912 Validation Loss: 1.7770400047302246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 663: Training Loss: 1.5499236981074016 Validation Loss: 1.7745107412338257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 664: Training Loss: 1.5481789509455364 Validation Loss: 1.773409366607666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 665: Training Loss: 1.5474183162053425 Validation Loss: 1.7725425958633423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 666: Training Loss: 1.5477569103240967 Validation Loss: 1.772385835647583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 667: Training Loss: 1.546670397122701 Validation Loss: 1.7711557149887085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 668: Training Loss: 1.543211579322815 Validation Loss: 1.7702115774154663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 669: Training Loss: 1.5426985025405884 Validation Loss: 1.7693164348602295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 670: Training Loss: 1.541286547978719 Validation Loss: 1.768857479095459\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 671: Training Loss: 1.540068546930949 Validation Loss: 1.7685415744781494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 672: Training Loss: 1.5389130512873332 Validation Loss: 1.7669172286987305\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 673: Training Loss: 1.5377915302912395 Validation Loss: 1.7649569511413574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 674: Training Loss: 1.5357913573582966 Validation Loss: 1.7642059326171875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 675: Training Loss: 1.5321733951568604 Validation Loss: 1.7631546258926392\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 676: Training Loss: 1.533524791399638 Validation Loss: 1.76164972782135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 677: Training Loss: 1.5333284536997478 Validation Loss: 1.760432243347168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 678: Training Loss: 1.531802773475647 Validation Loss: 1.759320855140686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 679: Training Loss: 1.5294175545374553 Validation Loss: 1.759124755859375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 680: Training Loss: 1.529453198115031 Validation Loss: 1.7590652704238892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 681: Training Loss: 1.5287359952926636 Validation Loss: 1.758239507675171\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 682: Training Loss: 1.5262315273284912 Validation Loss: 1.7572426795959473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 683: Training Loss: 1.525448203086853 Validation Loss: 1.754902958869934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 684: Training Loss: 1.5272998809814453 Validation Loss: 1.7537074089050293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 685: Training Loss: 1.5238419771194458 Validation Loss: 1.753065824508667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 686: Training Loss: 1.5233662923177083 Validation Loss: 1.7523658275604248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 687: Training Loss: 1.5239841143290203 Validation Loss: 1.7506047487258911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 688: Training Loss: 1.5204647779464722 Validation Loss: 1.7496788501739502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 689: Training Loss: 1.519874930381775 Validation Loss: 1.7488176822662354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 690: Training Loss: 1.516373872756958 Validation Loss: 1.7481685876846313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 691: Training Loss: 1.5162027676900227 Validation Loss: 1.7473613023757935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 692: Training Loss: 1.514204700787862 Validation Loss: 1.7464241981506348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 693: Training Loss: 1.5146694580713909 Validation Loss: 1.7446746826171875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 694: Training Loss: 1.5151015917460124 Validation Loss: 1.7432578802108765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 695: Training Loss: 1.510799765586853 Validation Loss: 1.7423371076583862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 696: Training Loss: 1.5112085739771526 Validation Loss: 1.7421809434890747\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 697: Training Loss: 1.5141557455062866 Validation Loss: 1.7414495944976807\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 698: Training Loss: 1.5078002214431763 Validation Loss: 1.7401108741760254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 699: Training Loss: 1.507431149482727 Validation Loss: 1.7386841773986816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 700: Training Loss: 1.5067537625630696 Validation Loss: 1.7369837760925293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 701: Training Loss: 1.5061895449956257 Validation Loss: 1.7368144989013672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 702: Training Loss: 1.5044761498769124 Validation Loss: 1.7355395555496216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 703: Training Loss: 1.503948410352071 Validation Loss: 1.7343246936798096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 704: Training Loss: 1.5011680920918782 Validation Loss: 1.734047293663025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 705: Training Loss: 1.4999855359395344 Validation Loss: 1.7334346771240234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 706: Training Loss: 1.4988007545471191 Validation Loss: 1.7324109077453613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 707: Training Loss: 1.4980024894078572 Validation Loss: 1.7318166494369507\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 708: Training Loss: 1.4968616167704265 Validation Loss: 1.7300289869308472\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 709: Training Loss: 1.4958219528198242 Validation Loss: 1.7284356355667114\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 710: Training Loss: 1.4929359356562297 Validation Loss: 1.7272920608520508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 711: Training Loss: 1.4933853546778362 Validation Loss: 1.726441502571106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 712: Training Loss: 1.4923890431722004 Validation Loss: 1.7254064083099365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 713: Training Loss: 1.48880139986674 Validation Loss: 1.724635362625122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 714: Training Loss: 1.4902661244074504 Validation Loss: 1.7236131429672241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 715: Training Loss: 1.4882105191548665 Validation Loss: 1.723190188407898\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 716: Training Loss: 1.486519455909729 Validation Loss: 1.7218098640441895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 717: Training Loss: 1.4854633808135986 Validation Loss: 1.7207021713256836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 718: Training Loss: 1.4841408729553223 Validation Loss: 1.7190754413604736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 719: Training Loss: 1.4840695063273113 Validation Loss: 1.7183725833892822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 720: Training Loss: 1.4860368569691975 Validation Loss: 1.7166731357574463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 721: Training Loss: 1.482072114944458 Validation Loss: 1.7168546915054321\n",
      "Epoch 722: Training Loss: 1.481707493464152 Validation Loss: 1.715428352355957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 723: Training Loss: 1.4799633423487346 Validation Loss: 1.7143480777740479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 724: Training Loss: 1.479363481203715 Validation Loss: 1.7140511274337769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 725: Training Loss: 1.4781209230422974 Validation Loss: 1.7131850719451904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 726: Training Loss: 1.4762227932612102 Validation Loss: 1.7122751474380493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 727: Training Loss: 1.4750067790349324 Validation Loss: 1.7112910747528076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 728: Training Loss: 1.4774093627929688 Validation Loss: 1.7100529670715332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 729: Training Loss: 1.474372665087382 Validation Loss: 1.7084871530532837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 730: Training Loss: 1.4681148926417034 Validation Loss: 1.7073055505752563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 731: Training Loss: 1.4715555906295776 Validation Loss: 1.7062886953353882\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 732: Training Loss: 1.4700700441996257 Validation Loss: 1.7055326700210571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 733: Training Loss: 1.4689833720525105 Validation Loss: 1.7050888538360596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 734: Training Loss: 1.4671930472056072 Validation Loss: 1.70503568649292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 735: Training Loss: 1.4652179876963298 Validation Loss: 1.7035852670669556\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 736: Training Loss: 1.4665141105651855 Validation Loss: 1.7019575834274292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 737: Training Loss: 1.4613085587819417 Validation Loss: 1.7009880542755127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 738: Training Loss: 1.460161288579305 Validation Loss: 1.6994277238845825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 739: Training Loss: 1.461008350054423 Validation Loss: 1.6981009244918823\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 740: Training Loss: 1.4611035188039143 Validation Loss: 1.6973979473114014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 741: Training Loss: 1.4599302212397258 Validation Loss: 1.6962913274765015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 742: Training Loss: 1.459280252456665 Validation Loss: 1.6966209411621094\n",
      "Epoch 743: Training Loss: 1.457068920135498 Validation Loss: 1.6963744163513184\n",
      "Epoch 744: Training Loss: 1.456186334292094 Validation Loss: 1.6949095726013184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 745: Training Loss: 1.4549135367075603 Validation Loss: 1.6929224729537964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 746: Training Loss: 1.454546054204305 Validation Loss: 1.691435694694519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 747: Training Loss: 1.4519511461257935 Validation Loss: 1.6905796527862549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 748: Training Loss: 1.451678474744161 Validation Loss: 1.689776062965393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 749: Training Loss: 1.4486063718795776 Validation Loss: 1.689605474472046\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 750: Training Loss: 1.4499691327412922 Validation Loss: 1.6889758110046387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 751: Training Loss: 1.4480797449747722 Validation Loss: 1.6878316402435303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 752: Training Loss: 1.4483143885930378 Validation Loss: 1.6873217821121216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 753: Training Loss: 1.4478826522827148 Validation Loss: 1.6852660179138184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 754: Training Loss: 1.4440237681070964 Validation Loss: 1.6840049028396606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 755: Training Loss: 1.4444198608398438 Validation Loss: 1.6828187704086304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 756: Training Loss: 1.4439560174942017 Validation Loss: 1.6822181940078735\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 757: Training Loss: 1.4440652132034302 Validation Loss: 1.6819666624069214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 758: Training Loss: 1.4416942596435547 Validation Loss: 1.681334376335144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 759: Training Loss: 1.4387236833572388 Validation Loss: 1.680514931678772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 760: Training Loss: 1.4385470549265544 Validation Loss: 1.6775929927825928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 761: Training Loss: 1.4390904506047566 Validation Loss: 1.6768279075622559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 762: Training Loss: 1.4390700260798137 Validation Loss: 1.677184820175171\n",
      "Epoch 763: Training Loss: 1.4372093677520752 Validation Loss: 1.6758880615234375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 764: Training Loss: 1.4340931971867878 Validation Loss: 1.6742379665374756\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 765: Training Loss: 1.4332159757614136 Validation Loss: 1.673379898071289\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 766: Training Loss: 1.4323307275772095 Validation Loss: 1.6734291315078735\n",
      "Epoch 767: Training Loss: 1.4295705954233806 Validation Loss: 1.6720285415649414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 768: Training Loss: 1.4301429986953735 Validation Loss: 1.671799898147583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 769: Training Loss: 1.4282657702763875 Validation Loss: 1.671162486076355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 770: Training Loss: 1.429057240486145 Validation Loss: 1.6692522764205933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 771: Training Loss: 1.4262107610702515 Validation Loss: 1.6676222085952759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 772: Training Loss: 1.425203800201416 Validation Loss: 1.6663399934768677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 773: Training Loss: 1.4234741926193237 Validation Loss: 1.6661173105239868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 774: Training Loss: 1.4232906897862752 Validation Loss: 1.6651570796966553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 775: Training Loss: 1.4218285878499348 Validation Loss: 1.6640750169754028\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 776: Training Loss: 1.4207863410313923 Validation Loss: 1.6634304523468018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 777: Training Loss: 1.4202843109766643 Validation Loss: 1.663317084312439\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 778: Training Loss: 1.418558160463969 Validation Loss: 1.662131667137146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 779: Training Loss: 1.4170762300491333 Validation Loss: 1.6605267524719238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 780: Training Loss: 1.4185241063435872 Validation Loss: 1.6600468158721924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 781: Training Loss: 1.4178257783253987 Validation Loss: 1.659023404121399\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 782: Training Loss: 1.414218783378601 Validation Loss: 1.656952977180481\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 783: Training Loss: 1.412797490755717 Validation Loss: 1.6557633876800537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 784: Training Loss: 1.4126441081364949 Validation Loss: 1.65584397315979\n",
      "Epoch 785: Training Loss: 1.4108021259307861 Validation Loss: 1.6548458337783813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 786: Training Loss: 1.410248041152954 Validation Loss: 1.6540892124176025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 787: Training Loss: 1.4089303811391194 Validation Loss: 1.652761697769165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 788: Training Loss: 1.4071415662765503 Validation Loss: 1.651902675628662\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 789: Training Loss: 1.4069332679112752 Validation Loss: 1.6513086557388306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 790: Training Loss: 1.4061030149459839 Validation Loss: 1.6508054733276367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 791: Training Loss: 1.4048244953155518 Validation Loss: 1.6490015983581543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 792: Training Loss: 1.4031378030776978 Validation Loss: 1.6477783918380737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 793: Training Loss: 1.4038968880971272 Validation Loss: 1.6473612785339355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 794: Training Loss: 1.402916709582011 Validation Loss: 1.646546721458435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 795: Training Loss: 1.4013042847315471 Validation Loss: 1.6453781127929688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 796: Training Loss: 1.3990686734517415 Validation Loss: 1.644335150718689\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 797: Training Loss: 1.4012902180353801 Validation Loss: 1.6430336236953735\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 798: Training Loss: 1.3999475638071697 Validation Loss: 1.6421564817428589\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 799: Training Loss: 1.396002729733785 Validation Loss: 1.6418594121932983\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 800: Training Loss: 1.3942292928695679 Validation Loss: 1.6419507265090942\n",
      "Epoch 801: Training Loss: 1.3953098853429158 Validation Loss: 1.642121434211731\n",
      "Epoch 802: Training Loss: 1.393992026646932 Validation Loss: 1.6407885551452637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 803: Training Loss: 1.3924552996953328 Validation Loss: 1.6395559310913086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 804: Training Loss: 1.392424742380778 Validation Loss: 1.6377300024032593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 805: Training Loss: 1.3879314661026 Validation Loss: 1.6356186866760254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 806: Training Loss: 1.3897268772125244 Validation Loss: 1.6335560083389282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 807: Training Loss: 1.3879043261210124 Validation Loss: 1.6328710317611694\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 808: Training Loss: 1.386965274810791 Validation Loss: 1.6329929828643799\n",
      "Epoch 809: Training Loss: 1.3833417097727458 Validation Loss: 1.6326414346694946\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 810: Training Loss: 1.384687860806783 Validation Loss: 1.6316317319869995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 811: Training Loss: 1.384212891260783 Validation Loss: 1.6315535306930542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 812: Training Loss: 1.380281130472819 Validation Loss: 1.6306780576705933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 813: Training Loss: 1.3813320795694988 Validation Loss: 1.6288762092590332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 814: Training Loss: 1.382246494293213 Validation Loss: 1.6276416778564453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 815: Training Loss: 1.377449671427409 Validation Loss: 1.6263540983200073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 816: Training Loss: 1.3772988716761272 Validation Loss: 1.6253329515457153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 817: Training Loss: 1.3756523927052815 Validation Loss: 1.625307321548462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 818: Training Loss: 1.3776335716247559 Validation Loss: 1.6250263452529907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 819: Training Loss: 1.3743306398391724 Validation Loss: 1.6237858533859253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 820: Training Loss: 1.3720781008402507 Validation Loss: 1.6235543489456177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 821: Training Loss: 1.3727986017862956 Validation Loss: 1.6224864721298218\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 822: Training Loss: 1.3730586369832356 Validation Loss: 1.6208522319793701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 823: Training Loss: 1.3706772724787395 Validation Loss: 1.6189677715301514\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 824: Training Loss: 1.3701767524083455 Validation Loss: 1.617821216583252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 825: Training Loss: 1.3680128653844197 Validation Loss: 1.6179029941558838\n",
      "Epoch 826: Training Loss: 1.3675161600112915 Validation Loss: 1.616877555847168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 827: Training Loss: 1.3697202603022258 Validation Loss: 1.616489052772522\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 828: Training Loss: 1.3666555484135945 Validation Loss: 1.6144132614135742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 829: Training Loss: 1.3642208178838093 Validation Loss: 1.613782525062561\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 830: Training Loss: 1.3643183708190918 Validation Loss: 1.613812804222107\n",
      "Epoch 831: Training Loss: 1.362064798672994 Validation Loss: 1.6129937171936035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 832: Training Loss: 1.3606892426808674 Validation Loss: 1.6118731498718262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 833: Training Loss: 1.3603650331497192 Validation Loss: 1.6111282110214233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 834: Training Loss: 1.359397570292155 Validation Loss: 1.6106011867523193\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 835: Training Loss: 1.3579295476277669 Validation Loss: 1.6090646982192993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 836: Training Loss: 1.358453631401062 Validation Loss: 1.6077475547790527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 837: Training Loss: 1.355992595354716 Validation Loss: 1.6067925691604614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 838: Training Loss: 1.3557189305623372 Validation Loss: 1.6060529947280884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 839: Training Loss: 1.3534368673960369 Validation Loss: 1.6053334474563599\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 840: Training Loss: 1.353930910428365 Validation Loss: 1.6043418645858765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 841: Training Loss: 1.3497698704401653 Validation Loss: 1.6032289266586304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 842: Training Loss: 1.3529731432596843 Validation Loss: 1.6026867628097534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 843: Training Loss: 1.3504114151000977 Validation Loss: 1.6018271446228027\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 844: Training Loss: 1.349610686302185 Validation Loss: 1.600836157798767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 845: Training Loss: 1.3484211762746174 Validation Loss: 1.600669264793396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 846: Training Loss: 1.3483665386835735 Validation Loss: 1.6001847982406616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 847: Training Loss: 1.3480831384658813 Validation Loss: 1.598099946975708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 848: Training Loss: 1.344219962755839 Validation Loss: 1.5969202518463135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 849: Training Loss: 1.3444840908050537 Validation Loss: 1.5957252979278564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 850: Training Loss: 1.3416684071222942 Validation Loss: 1.5942394733428955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 851: Training Loss: 1.342549244562785 Validation Loss: 1.5937459468841553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 852: Training Loss: 1.3399500846862793 Validation Loss: 1.5932612419128418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 853: Training Loss: 1.3413303693135579 Validation Loss: 1.5934733152389526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 854: Training Loss: 1.339652180671692 Validation Loss: 1.5927770137786865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 855: Training Loss: 1.3368496894836426 Validation Loss: 1.5915879011154175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 856: Training Loss: 1.3373121817906697 Validation Loss: 1.590923547744751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 857: Training Loss: 1.3374855915705364 Validation Loss: 1.5894852876663208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 858: Training Loss: 1.333390196164449 Validation Loss: 1.5883817672729492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 859: Training Loss: 1.3344996372858684 Validation Loss: 1.5873161554336548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 860: Training Loss: 1.332627534866333 Validation Loss: 1.5862457752227783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 861: Training Loss: 1.3313659032185872 Validation Loss: 1.5853896141052246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 862: Training Loss: 1.3302759726842244 Validation Loss: 1.5847654342651367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 863: Training Loss: 1.329940636952718 Validation Loss: 1.5847184658050537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 864: Training Loss: 1.3292752901713054 Validation Loss: 1.584079623222351\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 865: Training Loss: 1.328986644744873 Validation Loss: 1.5816898345947266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 866: Training Loss: 1.3283021052678425 Validation Loss: 1.5806047916412354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 867: Training Loss: 1.3276170094807942 Validation Loss: 1.579124927520752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 868: Training Loss: 1.3253239790598552 Validation Loss: 1.5789719820022583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 869: Training Loss: 1.3233663241068523 Validation Loss: 1.5784605741500854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 870: Training Loss: 1.323623259862264 Validation Loss: 1.5783408880233765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 871: Training Loss: 1.322664499282837 Validation Loss: 1.5781129598617554\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 872: Training Loss: 1.3236502408981323 Validation Loss: 1.5767245292663574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 873: Training Loss: 1.3193559249242146 Validation Loss: 1.5745917558670044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 874: Training Loss: 1.3184914588928223 Validation Loss: 1.5739970207214355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 875: Training Loss: 1.3191365798314412 Validation Loss: 1.5728851556777954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 876: Training Loss: 1.3190926710764568 Validation Loss: 1.5726202726364136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 877: Training Loss: 1.3153124650319417 Validation Loss: 1.572245478630066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 878: Training Loss: 1.3143622477849324 Validation Loss: 1.571044683456421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 879: Training Loss: 1.3149893681208293 Validation Loss: 1.5700572729110718\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 880: Training Loss: 1.3129520416259766 Validation Loss: 1.5680807828903198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 881: Training Loss: 1.3133968909581502 Validation Loss: 1.567862868309021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 882: Training Loss: 1.3117320537567139 Validation Loss: 1.567812204360962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 883: Training Loss: 1.3094189167022705 Validation Loss: 1.5671688318252563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 884: Training Loss: 1.3088512023289998 Validation Loss: 1.565916895866394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 885: Training Loss: 1.3078546524047852 Validation Loss: 1.5648746490478516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 886: Training Loss: 1.305881381034851 Validation Loss: 1.563746452331543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 887: Training Loss: 1.3059759140014648 Validation Loss: 1.5628983974456787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 888: Training Loss: 1.306403398513794 Validation Loss: 1.5624128580093384\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 889: Training Loss: 1.3038569291432698 Validation Loss: 1.5615897178649902\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 890: Training Loss: 1.3032558759053547 Validation Loss: 1.5603055953979492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 891: Training Loss: 1.3013015588124592 Validation Loss: 1.5596946477890015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 892: Training Loss: 1.3020943800608318 Validation Loss: 1.559334397315979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 893: Training Loss: 1.2996597687403362 Validation Loss: 1.5578898191452026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 894: Training Loss: 1.2999375661214192 Validation Loss: 1.556578516960144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 895: Training Loss: 1.2947869300842285 Validation Loss: 1.5552568435668945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 896: Training Loss: 1.2970187664031982 Validation Loss: 1.5551735162734985\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 897: Training Loss: 1.2959853808085124 Validation Loss: 1.5553110837936401\n",
      "Epoch 898: Training Loss: 1.2955620288848877 Validation Loss: 1.554459571838379\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 899: Training Loss: 1.2935609022776287 Validation Loss: 1.553141713142395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 900: Training Loss: 1.2938801844914753 Validation Loss: 1.5519332885742188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 901: Training Loss: 1.2917960087458293 Validation Loss: 1.550373911857605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 902: Training Loss: 1.2909650405248005 Validation Loss: 1.549683690071106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 903: Training Loss: 1.290490746498108 Validation Loss: 1.5496461391448975\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 904: Training Loss: 1.2884728113810222 Validation Loss: 1.5488286018371582\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 905: Training Loss: 1.287962834040324 Validation Loss: 1.5476267337799072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 906: Training Loss: 1.2888779242833455 Validation Loss: 1.5458804368972778\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 907: Training Loss: 1.2863821188608806 Validation Loss: 1.5453696250915527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 908: Training Loss: 1.2852269013722737 Validation Loss: 1.5447001457214355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 909: Training Loss: 1.2839199701944988 Validation Loss: 1.5436357259750366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 910: Training Loss: 1.2845369180043538 Validation Loss: 1.5430418252944946\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 911: Training Loss: 1.2827592293421428 Validation Loss: 1.5422412157058716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 912: Training Loss: 1.280741016070048 Validation Loss: 1.541893720626831\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 913: Training Loss: 1.2793250878651936 Validation Loss: 1.541830062866211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 914: Training Loss: 1.2792062362035115 Validation Loss: 1.5408493280410767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 915: Training Loss: 1.2791143655776978 Validation Loss: 1.5394244194030762\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 916: Training Loss: 1.277901252110799 Validation Loss: 1.538051724433899\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 917: Training Loss: 1.2782764434814453 Validation Loss: 1.5373735427856445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 918: Training Loss: 1.2747091849644978 Validation Loss: 1.535662055015564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 919: Training Loss: 1.2742313543955486 Validation Loss: 1.5352164506912231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 920: Training Loss: 1.2750086386998494 Validation Loss: 1.5346769094467163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 921: Training Loss: 1.2730494340260823 Validation Loss: 1.5335487127304077\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 922: Training Loss: 1.271312157313029 Validation Loss: 1.5331404209136963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 923: Training Loss: 1.2713414033253987 Validation Loss: 1.5325243473052979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 924: Training Loss: 1.2701749801635742 Validation Loss: 1.5321658849716187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 925: Training Loss: 1.2699586550394695 Validation Loss: 1.5314364433288574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 926: Training Loss: 1.2675825754801433 Validation Loss: 1.5301830768585205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 927: Training Loss: 1.2670119206110637 Validation Loss: 1.5289703607559204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 928: Training Loss: 1.2649065256118774 Validation Loss: 1.5275192260742188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 929: Training Loss: 1.264697511990865 Validation Loss: 1.5266063213348389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 930: Training Loss: 1.2645013729731243 Validation Loss: 1.5256718397140503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 931: Training Loss: 1.2629382212956746 Validation Loss: 1.5254101753234863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 932: Training Loss: 1.262605905532837 Validation Loss: 1.5253467559814453\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 933: Training Loss: 1.2623376846313477 Validation Loss: 1.5251697301864624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 934: Training Loss: 1.2599636316299438 Validation Loss: 1.5243433713912964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 935: Training Loss: 1.2603712876637776 Validation Loss: 1.5228700637817383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 936: Training Loss: 1.2581171989440918 Validation Loss: 1.5210694074630737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 937: Training Loss: 1.257414182027181 Validation Loss: 1.5203505754470825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 938: Training Loss: 1.257883906364441 Validation Loss: 1.5196990966796875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 939: Training Loss: 1.2550179560979207 Validation Loss: 1.518540620803833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 940: Training Loss: 1.254074255625407 Validation Loss: 1.5188926458358765\n",
      "Epoch 941: Training Loss: 1.2539528210957844 Validation Loss: 1.5180763006210327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 942: Training Loss: 1.2542848587036133 Validation Loss: 1.5171632766723633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 943: Training Loss: 1.2512001593907673 Validation Loss: 1.51539146900177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 944: Training Loss: 1.25056791305542 Validation Loss: 1.514072299003601\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 945: Training Loss: 1.2509599924087524 Validation Loss: 1.5135326385498047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 946: Training Loss: 1.2483048836390178 Validation Loss: 1.5124868154525757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 947: Training Loss: 1.250108003616333 Validation Loss: 1.5126899480819702\n",
      "Epoch 948: Training Loss: 1.2468209266662598 Validation Loss: 1.5113712549209595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 949: Training Loss: 1.250046968460083 Validation Loss: 1.5114693641662598\n",
      "Epoch 950: Training Loss: 1.2455647389094036 Validation Loss: 1.5095314979553223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 951: Training Loss: 1.2434467474619548 Validation Loss: 1.5089038610458374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 952: Training Loss: 1.240551749865214 Validation Loss: 1.5080829858779907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 953: Training Loss: 1.2425551811854045 Validation Loss: 1.5081286430358887\n",
      "Epoch 954: Training Loss: 1.2396352688471477 Validation Loss: 1.5080806016921997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 955: Training Loss: 1.239484707514445 Validation Loss: 1.5068470239639282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 956: Training Loss: 1.2408184210459392 Validation Loss: 1.5050538778305054\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 957: Training Loss: 1.2379072904586792 Validation Loss: 1.504003882408142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 958: Training Loss: 1.2380026578903198 Validation Loss: 1.5033215284347534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 959: Training Loss: 1.2362853686014812 Validation Loss: 1.5021165609359741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 960: Training Loss: 1.2370657523473103 Validation Loss: 1.5013762712478638\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 961: Training Loss: 1.2345058917999268 Validation Loss: 1.5006983280181885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 962: Training Loss: 1.235048532485962 Validation Loss: 1.499621033668518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 963: Training Loss: 1.2369962135950725 Validation Loss: 1.4988133907318115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 964: Training Loss: 1.2351492245992024 Validation Loss: 1.4985822439193726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 965: Training Loss: 1.2326478163401287 Validation Loss: 1.4987307786941528\n",
      "Epoch 966: Training Loss: 1.2298167149225872 Validation Loss: 1.4975032806396484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 967: Training Loss: 1.2275906006495159 Validation Loss: 1.4964689016342163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 968: Training Loss: 1.2294716437657673 Validation Loss: 1.494695782661438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 969: Training Loss: 1.228392243385315 Validation Loss: 1.4936778545379639\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 970: Training Loss: 1.226697325706482 Validation Loss: 1.4930099248886108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 971: Training Loss: 1.2246676683425903 Validation Loss: 1.4921860694885254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 972: Training Loss: 1.2266449133555095 Validation Loss: 1.4921045303344727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 973: Training Loss: 1.224565029144287 Validation Loss: 1.4918636083602905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 974: Training Loss: 1.2230615218480427 Validation Loss: 1.4915697574615479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 975: Training Loss: 1.2212011814117432 Validation Loss: 1.4901500940322876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 976: Training Loss: 1.220899264017741 Validation Loss: 1.4888978004455566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 977: Training Loss: 1.2201273838678997 Validation Loss: 1.4878166913986206\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 978: Training Loss: 1.2200879255930583 Validation Loss: 1.4866381883621216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 979: Training Loss: 1.2186081012090046 Validation Loss: 1.4863665103912354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 980: Training Loss: 1.218945821126302 Validation Loss: 1.4861347675323486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 981: Training Loss: 1.2173366943995159 Validation Loss: 1.4844584465026855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 982: Training Loss: 1.2150027751922607 Validation Loss: 1.4848581552505493\n",
      "Epoch 983: Training Loss: 1.2138063510258992 Validation Loss: 1.4842115640640259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 984: Training Loss: 1.2170235713322957 Validation Loss: 1.4824625253677368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 985: Training Loss: 1.211211125055949 Validation Loss: 1.4813741445541382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 986: Training Loss: 1.210954984029134 Validation Loss: 1.4801321029663086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 987: Training Loss: 1.2110188802083333 Validation Loss: 1.4793477058410645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 988: Training Loss: 1.2102797031402588 Validation Loss: 1.4795833826065063\n",
      "Epoch 989: Training Loss: 1.2096108198165894 Validation Loss: 1.4786193370819092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 990: Training Loss: 1.2081565459569295 Validation Loss: 1.4785666465759277\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 991: Training Loss: 1.2073338429133098 Validation Loss: 1.4777557849884033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 992: Training Loss: 1.207257827123006 Validation Loss: 1.475914478302002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 993: Training Loss: 1.2067276239395142 Validation Loss: 1.4746043682098389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 994: Training Loss: 1.2025638818740845 Validation Loss: 1.4741218090057373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 995: Training Loss: 1.20563801129659 Validation Loss: 1.473644495010376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 996: Training Loss: 1.202703833580017 Validation Loss: 1.472434639930725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 997: Training Loss: 1.202412207921346 Validation Loss: 1.4721726179122925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 998: Training Loss: 1.1992957194646199 Validation Loss: 1.4712730646133423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 999: Training Loss: 1.2010303735733032 Validation Loss: 1.4708478450775146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1000: Training Loss: 1.1987582842508953 Validation Loss: 1.4703106880187988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1001: Training Loss: 1.1982410351435344 Validation Loss: 1.4689500331878662\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1002: Training Loss: 1.1977617740631104 Validation Loss: 1.468474268913269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1003: Training Loss: 1.1968282063802083 Validation Loss: 1.4675114154815674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1004: Training Loss: 1.1952472925186157 Validation Loss: 1.466626524925232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1005: Training Loss: 1.1958469947179158 Validation Loss: 1.4656909704208374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1006: Training Loss: 1.1936386028925579 Validation Loss: 1.4655125141143799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1007: Training Loss: 1.1925714413324993 Validation Loss: 1.4641480445861816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1008: Training Loss: 1.1910141309102376 Validation Loss: 1.4632939100265503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1009: Training Loss: 1.191143274307251 Validation Loss: 1.4638983011245728\n",
      "Epoch 1010: Training Loss: 1.189894199371338 Validation Loss: 1.4631283283233643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1011: Training Loss: 1.1916921138763428 Validation Loss: 1.4611214399337769\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1012: Training Loss: 1.1875062386194866 Validation Loss: 1.4602521657943726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1013: Training Loss: 1.1869052648544312 Validation Loss: 1.459004521369934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1014: Training Loss: 1.1878958543141682 Validation Loss: 1.4582037925720215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1015: Training Loss: 1.1880439519882202 Validation Loss: 1.4581940174102783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1016: Training Loss: 1.1852933168411255 Validation Loss: 1.457977533340454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1017: Training Loss: 1.1861902872721355 Validation Loss: 1.4573112726211548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1018: Training Loss: 1.1828135251998901 Validation Loss: 1.4558056592941284\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1019: Training Loss: 1.182599941889445 Validation Loss: 1.4556316137313843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1020: Training Loss: 1.1819591522216797 Validation Loss: 1.4543259143829346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1021: Training Loss: 1.1796483596165974 Validation Loss: 1.4534595012664795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1022: Training Loss: 1.1789251168568928 Validation Loss: 1.4522426128387451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1023: Training Loss: 1.1801188389460247 Validation Loss: 1.4514946937561035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1024: Training Loss: 1.1814430952072144 Validation Loss: 1.450663685798645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1025: Training Loss: 1.1768721342086792 Validation Loss: 1.4504878520965576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1026: Training Loss: 1.1780349810918171 Validation Loss: 1.4498884677886963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1027: Training Loss: 1.1747454802195232 Validation Loss: 1.4494637250900269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1028: Training Loss: 1.1734500726064045 Validation Loss: 1.4482356309890747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1029: Training Loss: 1.1751134792963664 Validation Loss: 1.4461760520935059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1030: Training Loss: 1.1726698875427246 Validation Loss: 1.4457228183746338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1031: Training Loss: 1.1706992785135906 Validation Loss: 1.445414662361145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1032: Training Loss: 1.170156200726827 Validation Loss: 1.4456682205200195\n",
      "Epoch 1033: Training Loss: 1.1686675945917766 Validation Loss: 1.444932460784912\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1034: Training Loss: 1.1681489149729412 Validation Loss: 1.4437874555587769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1035: Training Loss: 1.16845699151357 Validation Loss: 1.4426826238632202\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1036: Training Loss: 1.1667428414026897 Validation Loss: 1.442142128944397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1037: Training Loss: 1.1686872243881226 Validation Loss: 1.4406211376190186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1038: Training Loss: 1.1647839943567913 Validation Loss: 1.4397714138031006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1039: Training Loss: 1.1635500987370808 Validation Loss: 1.4391049146652222\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1040: Training Loss: 1.1651214758555095 Validation Loss: 1.4381219148635864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1041: Training Loss: 1.1638915141423543 Validation Loss: 1.4385981559753418\n",
      "Epoch 1042: Training Loss: 1.1646026372909546 Validation Loss: 1.438291072845459\n",
      "Epoch 1043: Training Loss: 1.1609893242518108 Validation Loss: 1.4366862773895264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1044: Training Loss: 1.1597923437754314 Validation Loss: 1.4353728294372559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1045: Training Loss: 1.1584917704264324 Validation Loss: 1.434889316558838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1046: Training Loss: 1.1585138241449993 Validation Loss: 1.4352154731750488\n",
      "Epoch 1047: Training Loss: 1.1575794617335002 Validation Loss: 1.4343557357788086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1048: Training Loss: 1.1570309400558472 Validation Loss: 1.4332873821258545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1049: Training Loss: 1.155429442723592 Validation Loss: 1.4321473836898804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1050: Training Loss: 1.1541198094685872 Validation Loss: 1.4312248229980469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1051: Training Loss: 1.1533806721369426 Validation Loss: 1.4309860467910767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1052: Training Loss: 1.1529303789138794 Validation Loss: 1.4304312467575073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1053: Training Loss: 1.1517394383748372 Validation Loss: 1.4290578365325928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1054: Training Loss: 1.1521059672037761 Validation Loss: 1.4279091358184814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1055: Training Loss: 1.151227871576945 Validation Loss: 1.4271903038024902\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1056: Training Loss: 1.1514960130055745 Validation Loss: 1.426409363746643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1057: Training Loss: 1.1493897835413616 Validation Loss: 1.4250845909118652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1058: Training Loss: 1.1499310334523518 Validation Loss: 1.4254862070083618\n",
      "Epoch 1059: Training Loss: 1.1473464965820312 Validation Loss: 1.4258769750595093\n",
      "Epoch 1060: Training Loss: 1.14712389310201 Validation Loss: 1.4244105815887451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1061: Training Loss: 1.1453086932500203 Validation Loss: 1.4236537218093872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1062: Training Loss: 1.1443748871485393 Validation Loss: 1.422041416168213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1063: Training Loss: 1.1437262694040935 Validation Loss: 1.4207260608673096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1064: Training Loss: 1.1438032388687134 Validation Loss: 1.4200682640075684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1065: Training Loss: 1.1446613073349 Validation Loss: 1.4201343059539795\n",
      "Epoch 1066: Training Loss: 1.1436317364374797 Validation Loss: 1.4204658269882202\n",
      "Epoch 1067: Training Loss: 1.1415290832519531 Validation Loss: 1.4195505380630493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1068: Training Loss: 1.1395148833592732 Validation Loss: 1.418312907218933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1069: Training Loss: 1.1381433804829915 Validation Loss: 1.4162620306015015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1070: Training Loss: 1.1390154759089153 Validation Loss: 1.4157390594482422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1071: Training Loss: 1.136102517445882 Validation Loss: 1.4146243333816528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1072: Training Loss: 1.1351755062739055 Validation Loss: 1.414846658706665\n",
      "Epoch 1073: Training Loss: 1.1372341712315877 Validation Loss: 1.4150011539459229\n",
      "Epoch 1074: Training Loss: 1.1332455078760784 Validation Loss: 1.4134862422943115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1075: Training Loss: 1.1347089608510335 Validation Loss: 1.4129791259765625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1076: Training Loss: 1.1323658625284831 Validation Loss: 1.4121320247650146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1077: Training Loss: 1.131951133410136 Validation Loss: 1.4117283821105957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1078: Training Loss: 1.131221095720927 Validation Loss: 1.4104050397872925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1079: Training Loss: 1.1310871442159016 Validation Loss: 1.4097176790237427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1080: Training Loss: 1.1306262016296387 Validation Loss: 1.408835768699646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1081: Training Loss: 1.1279430786768596 Validation Loss: 1.40816068649292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1082: Training Loss: 1.127224286397298 Validation Loss: 1.4083073139190674\n",
      "Epoch 1083: Training Loss: 1.1283894379933674 Validation Loss: 1.4075599908828735\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1084: Training Loss: 1.1263504028320312 Validation Loss: 1.4065728187561035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1085: Training Loss: 1.1224043369293213 Validation Loss: 1.4054886102676392\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1086: Training Loss: 1.1242146889368694 Validation Loss: 1.4043753147125244\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1087: Training Loss: 1.1231356064478557 Validation Loss: 1.4032089710235596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1088: Training Loss: 1.1261264483133953 Validation Loss: 1.402732491493225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1089: Training Loss: 1.1223351955413818 Validation Loss: 1.4026336669921875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1090: Training Loss: 1.120069940884908 Validation Loss: 1.4012653827667236\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1091: Training Loss: 1.1196022033691406 Validation Loss: 1.401263952255249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1092: Training Loss: 1.1201305786768596 Validation Loss: 1.4006656408309937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1093: Training Loss: 1.1185189485549927 Validation Loss: 1.400020956993103\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1094: Training Loss: 1.1174277464548747 Validation Loss: 1.3988014459609985\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1095: Training Loss: 1.1161680221557617 Validation Loss: 1.3979945182800293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1096: Training Loss: 1.1169991493225098 Validation Loss: 1.397299885749817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1097: Training Loss: 1.1151197751363118 Validation Loss: 1.39717698097229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1098: Training Loss: 1.1144345204035442 Validation Loss: 1.3963383436203003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1099: Training Loss: 1.1126163403193157 Validation Loss: 1.3952425718307495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1100: Training Loss: 1.1149630943934123 Validation Loss: 1.3942389488220215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1101: Training Loss: 1.1107453902562459 Validation Loss: 1.393566608428955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1102: Training Loss: 1.110912283261617 Validation Loss: 1.3922216892242432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1103: Training Loss: 1.110346754391988 Validation Loss: 1.3920279741287231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1104: Training Loss: 1.108924388885498 Validation Loss: 1.3916715383529663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1105: Training Loss: 1.1108582417170207 Validation Loss: 1.3909121751785278\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1106: Training Loss: 1.1077664295832317 Validation Loss: 1.390714168548584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1107: Training Loss: 1.1072339216868083 Validation Loss: 1.3900741338729858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1108: Training Loss: 1.1063183546066284 Validation Loss: 1.389267921447754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1109: Training Loss: 1.106125036875407 Validation Loss: 1.3885548114776611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1110: Training Loss: 1.1050267219543457 Validation Loss: 1.3868577480316162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1111: Training Loss: 1.102539857228597 Validation Loss: 1.3856831789016724\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1112: Training Loss: 1.1022969086964924 Validation Loss: 1.3853318691253662\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1113: Training Loss: 1.1013193527857463 Validation Loss: 1.3856940269470215\n",
      "Epoch 1114: Training Loss: 1.1050307353337605 Validation Loss: 1.3857636451721191\n",
      "Epoch 1115: Training Loss: 1.1005623737970989 Validation Loss: 1.3845059871673584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1116: Training Loss: 1.0982096195220947 Validation Loss: 1.383862018585205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1117: Training Loss: 1.0984782377878826 Validation Loss: 1.382423996925354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1118: Training Loss: 1.0967875321706135 Validation Loss: 1.3812750577926636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1119: Training Loss: 1.095797101656596 Validation Loss: 1.3804998397827148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1120: Training Loss: 1.0959049860636394 Validation Loss: 1.3795223236083984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1121: Training Loss: 1.0949372847874959 Validation Loss: 1.3790842294692993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1122: Training Loss: 1.0962672630945842 Validation Loss: 1.3796846866607666\n",
      "Epoch 1123: Training Loss: 1.0947144826253254 Validation Loss: 1.3792370557785034\n",
      "Epoch 1124: Training Loss: 1.0920032262802124 Validation Loss: 1.37784743309021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1125: Training Loss: 1.096612850824992 Validation Loss: 1.3761543035507202\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1126: Training Loss: 1.0926254590352376 Validation Loss: 1.3753448724746704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1127: Training Loss: 1.0918507973353069 Validation Loss: 1.3761072158813477\n",
      "Epoch 1128: Training Loss: 1.0912466843922932 Validation Loss: 1.3754444122314453\n",
      "Epoch 1129: Training Loss: 1.0880908568700154 Validation Loss: 1.3744847774505615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1130: Training Loss: 1.0870897769927979 Validation Loss: 1.373608946800232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1131: Training Loss: 1.086353103319804 Validation Loss: 1.3724701404571533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1132: Training Loss: 1.0893099109331768 Validation Loss: 1.3711111545562744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1133: Training Loss: 1.0845880111058552 Validation Loss: 1.3707501888275146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1134: Training Loss: 1.0855424404144287 Validation Loss: 1.3709843158721924\n",
      "Epoch 1135: Training Loss: 1.0843302011489868 Validation Loss: 1.3697333335876465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1136: Training Loss: 1.0821237961451213 Validation Loss: 1.3691763877868652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1137: Training Loss: 1.0821655591328938 Validation Loss: 1.3679441213607788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1138: Training Loss: 1.08121919631958 Validation Loss: 1.3676104545593262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1139: Training Loss: 1.0802699724833171 Validation Loss: 1.366853952407837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1140: Training Loss: 1.0791852474212646 Validation Loss: 1.366529107093811\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1141: Training Loss: 1.0808535019556682 Validation Loss: 1.3661906719207764\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1142: Training Loss: 1.0800161759058635 Validation Loss: 1.3646368980407715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1143: Training Loss: 1.0791281859079997 Validation Loss: 1.3638578653335571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1144: Training Loss: 1.076451261838277 Validation Loss: 1.3626729249954224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1145: Training Loss: 1.0774530172348022 Validation Loss: 1.3627201318740845\n",
      "Epoch 1146: Training Loss: 1.073770801226298 Validation Loss: 1.362036108970642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1147: Training Loss: 1.0740962028503418 Validation Loss: 1.3611539602279663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1148: Training Loss: 1.0758006771405537 Validation Loss: 1.3604427576065063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1149: Training Loss: 1.0726572275161743 Validation Loss: 1.3603967428207397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1150: Training Loss: 1.0725787083307903 Validation Loss: 1.3602012395858765\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1151: Training Loss: 1.0716523726781209 Validation Loss: 1.3589754104614258\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1152: Training Loss: 1.070588231086731 Validation Loss: 1.357588291168213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1153: Training Loss: 1.0708231528600056 Validation Loss: 1.3562253713607788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1154: Training Loss: 1.0695871909459431 Validation Loss: 1.356135606765747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1155: Training Loss: 1.0673332611719768 Validation Loss: 1.355912685394287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1156: Training Loss: 1.0712058941523235 Validation Loss: 1.355513334274292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1157: Training Loss: 1.0687181949615479 Validation Loss: 1.3547837734222412\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1158: Training Loss: 1.0653632084528606 Validation Loss: 1.353684425354004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1159: Training Loss: 1.0650716225306194 Validation Loss: 1.3526619672775269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1160: Training Loss: 1.063731034596761 Validation Loss: 1.3515604734420776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1161: Training Loss: 1.0643359422683716 Validation Loss: 1.3512040376663208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1162: Training Loss: 1.0621438423792522 Validation Loss: 1.3508843183517456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1163: Training Loss: 1.0618709723154705 Validation Loss: 1.350868582725525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1164: Training Loss: 1.0597095489501953 Validation Loss: 1.3495546579360962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1165: Training Loss: 1.05941903591156 Validation Loss: 1.3490495681762695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1166: Training Loss: 1.0606436332066853 Validation Loss: 1.3483690023422241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1167: Training Loss: 1.0580685138702393 Validation Loss: 1.3480960130691528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1168: Training Loss: 1.057170848051707 Validation Loss: 1.3467113971710205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1169: Training Loss: 1.0568040609359741 Validation Loss: 1.3459765911102295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1170: Training Loss: 1.0563368399937947 Validation Loss: 1.3454989194869995\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1171: Training Loss: 1.0544646978378296 Validation Loss: 1.3453751802444458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1172: Training Loss: 1.0537091493606567 Validation Loss: 1.3442010879516602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1173: Training Loss: 1.0535338322321575 Validation Loss: 1.3432044982910156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1174: Training Loss: 1.056729217370351 Validation Loss: 1.3425346612930298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1175: Training Loss: 1.0517659187316895 Validation Loss: 1.3423151969909668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1176: Training Loss: 1.0526548624038696 Validation Loss: 1.341627597808838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1177: Training Loss: 1.050358255704244 Validation Loss: 1.3403507471084595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1178: Training Loss: 1.0494050979614258 Validation Loss: 1.339415192604065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1179: Training Loss: 1.0491796731948853 Validation Loss: 1.3393950462341309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1180: Training Loss: 1.0489588181177776 Validation Loss: 1.3391664028167725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1181: Training Loss: 1.0474422375361125 Validation Loss: 1.3381863832473755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1182: Training Loss: 1.0466100772221882 Validation Loss: 1.3375080823898315\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1183: Training Loss: 1.045114556948344 Validation Loss: 1.3370298147201538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1184: Training Loss: 1.0446906089782715 Validation Loss: 1.3368351459503174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1185: Training Loss: 1.0455399751663208 Validation Loss: 1.3358010053634644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1186: Training Loss: 1.044176499048869 Validation Loss: 1.335106611251831\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1187: Training Loss: 1.0442894101142883 Validation Loss: 1.3334712982177734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1188: Training Loss: 1.0431689818700154 Validation Loss: 1.3330518007278442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1189: Training Loss: 1.041482150554657 Validation Loss: 1.3326101303100586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1190: Training Loss: 1.0408875544865925 Validation Loss: 1.3318986892700195\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1191: Training Loss: 1.040063500404358 Validation Loss: 1.3313915729522705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1192: Training Loss: 1.040126363436381 Validation Loss: 1.3311638832092285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1193: Training Loss: 1.0384264787038167 Validation Loss: 1.3304696083068848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1194: Training Loss: 1.0353585481643677 Validation Loss: 1.330018401145935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1195: Training Loss: 1.0372189283370972 Validation Loss: 1.3303163051605225\n",
      "Epoch 1196: Training Loss: 1.0357050895690918 Validation Loss: 1.328732967376709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1197: Training Loss: 1.035090486208598 Validation Loss: 1.327339768409729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1198: Training Loss: 1.0375558137893677 Validation Loss: 1.3265234231948853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1199: Training Loss: 1.0331597725550334 Validation Loss: 1.3255857229232788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1200: Training Loss: 1.0341549913088481 Validation Loss: 1.3252285718917847\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1201: Training Loss: 1.032896637916565 Validation Loss: 1.3245296478271484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1202: Training Loss: 1.0338144302368164 Validation Loss: 1.3241592645645142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1203: Training Loss: 1.0297919909159343 Validation Loss: 1.3232831954956055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1204: Training Loss: 1.0291441480318706 Validation Loss: 1.3222689628601074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1205: Training Loss: 1.0306400656700134 Validation Loss: 1.3220285177230835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1206: Training Loss: 1.0281199216842651 Validation Loss: 1.3214219808578491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1207: Training Loss: 1.027016560236613 Validation Loss: 1.3216207027435303\n",
      "Epoch 1208: Training Loss: 1.0255017081896465 Validation Loss: 1.3212077617645264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1209: Training Loss: 1.0243573983510335 Validation Loss: 1.32008695602417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1210: Training Loss: 1.024594783782959 Validation Loss: 1.3188601732254028\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1211: Training Loss: 1.026018500328064 Validation Loss: 1.318169355392456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1212: Training Loss: 1.022331456343333 Validation Loss: 1.317348599433899\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1213: Training Loss: 1.0229061444600422 Validation Loss: 1.3169485330581665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1214: Training Loss: 1.0206586718559265 Validation Loss: 1.31582510471344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1215: Training Loss: 1.020614782969157 Validation Loss: 1.3152319192886353\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1216: Training Loss: 1.0206581354141235 Validation Loss: 1.3149157762527466\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1217: Training Loss: 1.0202377637227376 Validation Loss: 1.3140203952789307\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1218: Training Loss: 1.0189736882845561 Validation Loss: 1.3137760162353516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1219: Training Loss: 1.0199682513872783 Validation Loss: 1.313252329826355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1220: Training Loss: 1.0182846585909526 Validation Loss: 1.3123492002487183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1221: Training Loss: 1.0161162813504536 Validation Loss: 1.3119211196899414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1222: Training Loss: 1.0162720282872517 Validation Loss: 1.3112900257110596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1223: Training Loss: 1.018326719601949 Validation Loss: 1.3106690645217896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1224: Training Loss: 1.0141063332557678 Validation Loss: 1.3099713325500488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1225: Training Loss: 1.0151218175888062 Validation Loss: 1.3087323904037476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1226: Training Loss: 1.0128371715545654 Validation Loss: 1.3078957796096802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1227: Training Loss: 1.013762891292572 Validation Loss: 1.3081594705581665\n",
      "Epoch 1228: Training Loss: 1.0114869674046834 Validation Loss: 1.3074371814727783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1229: Training Loss: 1.0111575325330098 Validation Loss: 1.306454062461853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1230: Training Loss: 1.012292702992757 Validation Loss: 1.3063116073608398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1231: Training Loss: 1.0121478239695232 Validation Loss: 1.3054172992706299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1232: Training Loss: 1.0087364315986633 Validation Loss: 1.3044934272766113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1233: Training Loss: 1.0079665978749592 Validation Loss: 1.3033031225204468\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1234: Training Loss: 1.0064827402432759 Validation Loss: 1.3027803897857666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1235: Training Loss: 1.0061815579732258 Validation Loss: 1.302810549736023\n",
      "Epoch 1236: Training Loss: 1.0063647826512654 Validation Loss: 1.302353024482727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1237: Training Loss: 1.0047167539596558 Validation Loss: 1.3022198677062988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1238: Training Loss: 1.005690058072408 Validation Loss: 1.300629734992981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1239: Training Loss: 1.0032934347788494 Validation Loss: 1.3001055717468262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1240: Training Loss: 1.0041417479515076 Validation Loss: 1.2999463081359863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1241: Training Loss: 1.0031076669692993 Validation Loss: 1.299106240272522\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1242: Training Loss: 1.0019824902216594 Validation Loss: 1.2983100414276123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1243: Training Loss: 1.0009700854619343 Validation Loss: 1.2975729703903198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1244: Training Loss: 1.0002854665120442 Validation Loss: 1.2965530157089233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1245: Training Loss: 0.9993935426076254 Validation Loss: 1.2968014478683472\n",
      "Epoch 1246: Training Loss: 0.9981698195139567 Validation Loss: 1.2961891889572144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1247: Training Loss: 0.9976277947425842 Validation Loss: 1.2949262857437134\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1248: Training Loss: 0.9951848189036051 Validation Loss: 1.2940279245376587\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1249: Training Loss: 0.9956955512364706 Validation Loss: 1.2936402559280396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1250: Training Loss: 0.995064377784729 Validation Loss: 1.292575716972351\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1251: Training Loss: 0.997535506884257 Validation Loss: 1.2920911312103271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1252: Training Loss: 0.9949572483698527 Validation Loss: 1.2923389673233032\n",
      "Epoch 1253: Training Loss: 0.9929543733596802 Validation Loss: 1.292546033859253\n",
      "Epoch 1254: Training Loss: 0.9932898680369059 Validation Loss: 1.2911324501037598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1255: Training Loss: 0.9934976696968079 Validation Loss: 1.2897050380706787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1256: Training Loss: 0.992400586605072 Validation Loss: 1.2886885404586792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1257: Training Loss: 0.9899021983146667 Validation Loss: 1.288878321647644\n",
      "Epoch 1258: Training Loss: 0.9898952643076578 Validation Loss: 1.2886220216751099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1259: Training Loss: 0.9895102580388387 Validation Loss: 1.2877662181854248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1260: Training Loss: 0.9885903795560201 Validation Loss: 1.2870410680770874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1261: Training Loss: 0.9876880844434103 Validation Loss: 1.2857266664505005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1262: Training Loss: 0.986405591169993 Validation Loss: 1.285177230834961\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1263: Training Loss: 0.9879339138666788 Validation Loss: 1.2850775718688965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1264: Training Loss: 0.9860953092575073 Validation Loss: 1.2841054201126099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1265: Training Loss: 0.9834227959314982 Validation Loss: 1.284102201461792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1266: Training Loss: 0.9838953614234924 Validation Loss: 1.2836259603500366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1267: Training Loss: 0.9868882099787394 Validation Loss: 1.2831593751907349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1268: Training Loss: 0.9844476381937662 Validation Loss: 1.2817115783691406\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1269: Training Loss: 0.9829755624135336 Validation Loss: 1.2812864780426025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1270: Training Loss: 0.981322169303894 Validation Loss: 1.2804162502288818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1271: Training Loss: 0.979712704817454 Validation Loss: 1.2797963619232178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1272: Training Loss: 0.9796028137207031 Validation Loss: 1.2804226875305176\n",
      "Epoch 1273: Training Loss: 0.9805734157562256 Validation Loss: 1.2794239521026611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1274: Training Loss: 0.9780576030413309 Validation Loss: 1.2784138917922974\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1275: Training Loss: 0.9777268369992574 Validation Loss: 1.2775452136993408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1276: Training Loss: 0.9787025252978007 Validation Loss: 1.2780464887619019\n",
      "Epoch 1277: Training Loss: 0.9770148992538452 Validation Loss: 1.2767521142959595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1278: Training Loss: 0.9755011598269144 Validation Loss: 1.275113821029663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1279: Training Loss: 0.9748941659927368 Validation Loss: 1.2743021249771118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1280: Training Loss: 0.9745931228001913 Validation Loss: 1.2738590240478516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1281: Training Loss: 0.9753554264704386 Validation Loss: 1.2737480401992798\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1282: Training Loss: 0.9723923206329346 Validation Loss: 1.273146629333496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1283: Training Loss: 0.9711927175521851 Validation Loss: 1.2726049423217773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1284: Training Loss: 0.9723517696062723 Validation Loss: 1.2717256546020508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1285: Training Loss: 0.9712798794110616 Validation Loss: 1.271255612373352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1286: Training Loss: 0.9690113663673401 Validation Loss: 1.2706842422485352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1287: Training Loss: 0.9689234892527262 Validation Loss: 1.270613670349121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1288: Training Loss: 0.9673941532770792 Validation Loss: 1.2700241804122925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1289: Training Loss: 0.9690749446551005 Validation Loss: 1.2693760395050049\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1290: Training Loss: 0.9665763179461161 Validation Loss: 1.2686748504638672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1291: Training Loss: 0.9661892255147299 Validation Loss: 1.2681586742401123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1292: Training Loss: 0.9646096428235372 Validation Loss: 1.267083764076233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1293: Training Loss: 0.9646692077318827 Validation Loss: 1.2670365571975708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1294: Training Loss: 0.9635970791180929 Validation Loss: 1.2661921977996826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1295: Training Loss: 0.9628592133522034 Validation Loss: 1.2656667232513428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1296: Training Loss: 0.9624263644218445 Validation Loss: 1.2641935348510742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1297: Training Loss: 0.9611910184224447 Validation Loss: 1.263966679573059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1298: Training Loss: 0.9604686498641968 Validation Loss: 1.2637296915054321\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1299: Training Loss: 0.9603327711423238 Validation Loss: 1.2635694742202759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1300: Training Loss: 0.9597567518552145 Validation Loss: 1.2621855735778809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1301: Training Loss: 0.9585932294527689 Validation Loss: 1.2619407176971436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1302: Training Loss: 0.9586775700251261 Validation Loss: 1.260532021522522\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1303: Training Loss: 0.9561441739400228 Validation Loss: 1.2602447271347046\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1304: Training Loss: 0.9574293494224548 Validation Loss: 1.2598121166229248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1305: Training Loss: 0.9577103853225708 Validation Loss: 1.2595072984695435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1306: Training Loss: 0.956493099530538 Validation Loss: 1.2586041688919067\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1307: Training Loss: 0.9549236297607422 Validation Loss: 1.258215069770813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1308: Training Loss: 0.9553299347559611 Validation Loss: 1.2582876682281494\n",
      "Epoch 1309: Training Loss: 0.9524406790733337 Validation Loss: 1.2573331594467163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1310: Training Loss: 0.9531745115915934 Validation Loss: 1.2570106983184814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1311: Training Loss: 0.9523183107376099 Validation Loss: 1.2565680742263794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1312: Training Loss: 0.9512642621994019 Validation Loss: 1.2557581663131714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1313: Training Loss: 0.9508548378944397 Validation Loss: 1.254372000694275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1314: Training Loss: 0.9508932034174601 Validation Loss: 1.2534637451171875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1315: Training Loss: 0.9497861266136169 Validation Loss: 1.252600073814392\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1316: Training Loss: 0.9482470552126566 Validation Loss: 1.2528246641159058\n",
      "Epoch 1317: Training Loss: 0.9469972451527914 Validation Loss: 1.2525146007537842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1318: Training Loss: 0.9451079567273458 Validation Loss: 1.2524847984313965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1319: Training Loss: 0.9471487601598104 Validation Loss: 1.2517915964126587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1320: Training Loss: 0.9451318383216858 Validation Loss: 1.251267910003662\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1321: Training Loss: 0.9463106989860535 Validation Loss: 1.2497698068618774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1322: Training Loss: 0.9424295624097189 Validation Loss: 1.248872995376587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1323: Training Loss: 0.9440566102663676 Validation Loss: 1.248694658279419\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1324: Training Loss: 0.9422815044720968 Validation Loss: 1.24878990650177\n",
      "Epoch 1325: Training Loss: 0.9432389736175537 Validation Loss: 1.2483047246932983\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1326: Training Loss: 0.9414946238199869 Validation Loss: 1.2469608783721924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1327: Training Loss: 0.9412956436475118 Validation Loss: 1.2460169792175293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1328: Training Loss: 0.9428734381993612 Validation Loss: 1.245218276977539\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1329: Training Loss: 0.9414496620496114 Validation Loss: 1.2449184656143188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1330: Training Loss: 0.9393519560496012 Validation Loss: 1.2445948123931885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1331: Training Loss: 0.9385803540547689 Validation Loss: 1.2439074516296387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1332: Training Loss: 0.9392744898796082 Validation Loss: 1.2435230016708374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1333: Training Loss: 0.9372933904329935 Validation Loss: 1.2426930665969849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1334: Training Loss: 0.9367446899414062 Validation Loss: 1.2417491674423218\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1335: Training Loss: 0.9355153441429138 Validation Loss: 1.2413424253463745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1336: Training Loss: 0.9361345569292704 Validation Loss: 1.2409751415252686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1337: Training Loss: 0.9342467387517294 Validation Loss: 1.2406893968582153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1338: Training Loss: 0.9337304433186849 Validation Loss: 1.2400678396224976\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1339: Training Loss: 0.9326337377230326 Validation Loss: 1.2392719984054565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1340: Training Loss: 0.9327890872955322 Validation Loss: 1.2390220165252686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1341: Training Loss: 0.9312621355056763 Validation Loss: 1.2380492687225342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1342: Training Loss: 0.9310750166575114 Validation Loss: 1.23766028881073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1343: Training Loss: 0.9322048226992289 Validation Loss: 1.2369569540023804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1344: Training Loss: 0.9286075631777445 Validation Loss: 1.2363218069076538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1345: Training Loss: 0.9293018380800883 Validation Loss: 1.2356183528900146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1346: Training Loss: 0.9308852354685465 Validation Loss: 1.2357869148254395\n",
      "Epoch 1347: Training Loss: 0.9273598988850912 Validation Loss: 1.2352828979492188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1348: Training Loss: 0.9274654785792033 Validation Loss: 1.2337663173675537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1349: Training Loss: 0.9264729022979736 Validation Loss: 1.2335811853408813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1350: Training Loss: 0.9254820346832275 Validation Loss: 1.2332884073257446\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1351: Training Loss: 0.9260251323382059 Validation Loss: 1.2327871322631836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1352: Training Loss: 0.924801747004191 Validation Loss: 1.2323448657989502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1353: Training Loss: 0.9244439403216044 Validation Loss: 1.2311450242996216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1354: Training Loss: 0.9228288928667704 Validation Loss: 1.2302194833755493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1355: Training Loss: 0.9209984540939331 Validation Loss: 1.22950279712677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1356: Training Loss: 0.9212646087010702 Validation Loss: 1.2291620969772339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1357: Training Loss: 0.9213996529579163 Validation Loss: 1.2287899255752563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1358: Training Loss: 0.9219353993733724 Validation Loss: 1.2292819023132324\n",
      "Epoch 1359: Training Loss: 0.9206541577974955 Validation Loss: 1.2287472486495972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1360: Training Loss: 0.9197756052017212 Validation Loss: 1.227243423461914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1361: Training Loss: 0.9187218944231669 Validation Loss: 1.2267073392868042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1362: Training Loss: 0.9178875088691711 Validation Loss: 1.226483702659607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1363: Training Loss: 0.9183732072512308 Validation Loss: 1.2258158922195435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1364: Training Loss: 0.9162177046140035 Validation Loss: 1.2249177694320679\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1365: Training Loss: 0.9155713319778442 Validation Loss: 1.2245367765426636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1366: Training Loss: 0.9142942627271017 Validation Loss: 1.2236809730529785\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1367: Training Loss: 0.9140708446502686 Validation Loss: 1.22319495677948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1368: Training Loss: 0.9146056175231934 Validation Loss: 1.2228397130966187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1369: Training Loss: 0.9141785105069479 Validation Loss: 1.222335696220398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1370: Training Loss: 0.9126224319140116 Validation Loss: 1.2216140031814575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1371: Training Loss: 0.9119671583175659 Validation Loss: 1.2209957838058472\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1372: Training Loss: 0.9116137822469076 Validation Loss: 1.2203501462936401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1373: Training Loss: 0.9103155334790548 Validation Loss: 1.2200673818588257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1374: Training Loss: 0.9103356202443441 Validation Loss: 1.2192456722259521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1375: Training Loss: 0.9074716766675314 Validation Loss: 1.219093680381775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1376: Training Loss: 0.9084022045135498 Validation Loss: 1.2186150550842285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1377: Training Loss: 0.9085294405619303 Validation Loss: 1.2174546718597412\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1378: Training Loss: 0.9077179829279581 Validation Loss: 1.2171573638916016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1379: Training Loss: 0.9058792988459269 Validation Loss: 1.2168022394180298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1380: Training Loss: 0.9047784209251404 Validation Loss: 1.2162302732467651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1381: Training Loss: 0.9051488836606344 Validation Loss: 1.2156976461410522\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1382: Training Loss: 0.9076012770334879 Validation Loss: 1.2147002220153809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1383: Training Loss: 0.9061136643091837 Validation Loss: 1.2147510051727295\n",
      "Epoch 1384: Training Loss: 0.9047854145367941 Validation Loss: 1.2139859199523926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1385: Training Loss: 0.9066665768623352 Validation Loss: 1.2131999731063843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1386: Training Loss: 0.9027735590934753 Validation Loss: 1.2131866216659546\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1387: Training Loss: 0.9006762901941935 Validation Loss: 1.2125896215438843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1388: Training Loss: 0.9011451601982117 Validation Loss: 1.211431860923767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1389: Training Loss: 0.900599459807078 Validation Loss: 1.2109721899032593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1390: Training Loss: 0.8985640207926432 Validation Loss: 1.2103302478790283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1391: Training Loss: 0.897923211256663 Validation Loss: 1.2096621990203857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1392: Training Loss: 0.8997917175292969 Validation Loss: 1.2093511819839478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1393: Training Loss: 0.8968853950500488 Validation Loss: 1.2087346315383911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1394: Training Loss: 0.8968333999315897 Validation Loss: 1.2089827060699463\n",
      "Epoch 1395: Training Loss: 0.8968800107638041 Validation Loss: 1.2081124782562256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1396: Training Loss: 0.8956018090248108 Validation Loss: 1.2078781127929688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1397: Training Loss: 0.8950492143630981 Validation Loss: 1.207926869392395\n",
      "Epoch 1398: Training Loss: 0.894422173500061 Validation Loss: 1.2074753046035767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1399: Training Loss: 0.8945538004239401 Validation Loss: 1.2056701183319092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1400: Training Loss: 0.8929800589879354 Validation Loss: 1.204494833946228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1401: Training Loss: 0.8927611708641052 Validation Loss: 1.2036609649658203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1402: Training Loss: 0.8923916816711426 Validation Loss: 1.2030972242355347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1403: Training Loss: 0.8925828536351522 Validation Loss: 1.2030971050262451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1404: Training Loss: 0.8891855676968893 Validation Loss: 1.203597068786621\n",
      "Epoch 1405: Training Loss: 0.8904779156049093 Validation Loss: 1.2035366296768188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1406: Training Loss: 0.8885203798611959 Validation Loss: 1.2025431394577026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1407: Training Loss: 0.8896435896555582 Validation Loss: 1.201217532157898\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1408: Training Loss: 0.8864753047625223 Validation Loss: 1.200613260269165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1409: Training Loss: 0.8880248467127482 Validation Loss: 1.1999034881591797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1410: Training Loss: 0.887867788473765 Validation Loss: 1.1991639137268066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1411: Training Loss: 0.8870940804481506 Validation Loss: 1.1986281871795654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1412: Training Loss: 0.886361300945282 Validation Loss: 1.1982955932617188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1413: Training Loss: 0.8848462700843811 Validation Loss: 1.198203682899475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1414: Training Loss: 0.8839377363522848 Validation Loss: 1.1980104446411133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1415: Training Loss: 0.8844412366549174 Validation Loss: 1.197500467300415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1416: Training Loss: 0.881813665231069 Validation Loss: 1.1966572999954224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1417: Training Loss: 0.8823481400807699 Validation Loss: 1.1952569484710693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1418: Training Loss: 0.8833651145299276 Validation Loss: 1.1945959329605103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1419: Training Loss: 0.8816462953885397 Validation Loss: 1.1943891048431396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1420: Training Loss: 0.8811031977335612 Validation Loss: 1.194846510887146\n",
      "Epoch 1421: Training Loss: 0.8796108961105347 Validation Loss: 1.194273829460144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1422: Training Loss: 0.8792142669359843 Validation Loss: 1.1939157247543335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1423: Training Loss: 0.8784973820050558 Validation Loss: 1.1934789419174194\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1424: Training Loss: 0.8778577645619711 Validation Loss: 1.1928483247756958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1425: Training Loss: 0.8768535455067953 Validation Loss: 1.1916601657867432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1426: Training Loss: 0.8777435620625814 Validation Loss: 1.191077709197998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1427: Training Loss: 0.8780921896298727 Validation Loss: 1.1905884742736816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1428: Training Loss: 0.8749393423398336 Validation Loss: 1.1897437572479248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1429: Training Loss: 0.8754293322563171 Validation Loss: 1.1898596286773682\n",
      "Epoch 1430: Training Loss: 0.8739940722783407 Validation Loss: 1.1888328790664673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1431: Training Loss: 0.8728113373120626 Validation Loss: 1.188125491142273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1432: Training Loss: 0.8731527527173361 Validation Loss: 1.1877530813217163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1433: Training Loss: 0.8754110336303711 Validation Loss: 1.1875158548355103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1434: Training Loss: 0.8741003473599752 Validation Loss: 1.1868915557861328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1435: Training Loss: 0.8712702592213949 Validation Loss: 1.186651349067688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1436: Training Loss: 0.8695303599039713 Validation Loss: 1.1860816478729248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1437: Training Loss: 0.8711231152216593 Validation Loss: 1.1849666833877563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1438: Training Loss: 0.8696282108624777 Validation Loss: 1.1846543550491333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1439: Training Loss: 0.8709538777669271 Validation Loss: 1.1839715242385864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1440: Training Loss: 0.871759315331777 Validation Loss: 1.1839714050292969\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1441: Training Loss: 0.8686497012774149 Validation Loss: 1.1833425760269165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1442: Training Loss: 0.8671225110689799 Validation Loss: 1.1821743249893188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1443: Training Loss: 0.866889476776123 Validation Loss: 1.1817312240600586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1444: Training Loss: 0.8660422166188558 Validation Loss: 1.1816588640213013\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1445: Training Loss: 0.864493171374003 Validation Loss: 1.1809117794036865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1446: Training Loss: 0.8642438848813375 Validation Loss: 1.1801563501358032\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1447: Training Loss: 0.8643634716669718 Validation Loss: 1.1805906295776367\n",
      "Epoch 1448: Training Loss: 0.8629224896430969 Validation Loss: 1.1800780296325684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1449: Training Loss: 0.8635107676188151 Validation Loss: 1.1790602207183838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1450: Training Loss: 0.8614227573076884 Validation Loss: 1.178127646446228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1451: Training Loss: 0.8605592250823975 Validation Loss: 1.1781419515609741\n",
      "Epoch 1452: Training Loss: 0.8600043257077535 Validation Loss: 1.1778312921524048\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1453: Training Loss: 0.8598026235898336 Validation Loss: 1.1776206493377686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1454: Training Loss: 0.8610948522885641 Validation Loss: 1.1773256063461304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1455: Training Loss: 0.8587168057759603 Validation Loss: 1.1765393018722534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1456: Training Loss: 0.8577724893887838 Validation Loss: 1.1754697561264038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1457: Training Loss: 0.8602341612180074 Validation Loss: 1.1744165420532227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1458: Training Loss: 0.8565492828687032 Validation Loss: 1.1736209392547607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1459: Training Loss: 0.8556551138559977 Validation Loss: 1.173435926437378\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1460: Training Loss: 0.8556575973828634 Validation Loss: 1.1733105182647705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1461: Training Loss: 0.8548941016197205 Validation Loss: 1.1729044914245605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1462: Training Loss: 0.8585693637530009 Validation Loss: 1.1727309226989746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1463: Training Loss: 0.8542950550715128 Validation Loss: 1.171986699104309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1464: Training Loss: 0.8525046904881796 Validation Loss: 1.171283483505249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1465: Training Loss: 0.8520557284355164 Validation Loss: 1.1707617044448853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1466: Training Loss: 0.85186239083608 Validation Loss: 1.1699029207229614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1467: Training Loss: 0.8539312879244486 Validation Loss: 1.1693716049194336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1468: Training Loss: 0.8515223264694214 Validation Loss: 1.1691466569900513\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1469: Training Loss: 0.8503422737121582 Validation Loss: 1.1691441535949707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1470: Training Loss: 0.849417487780253 Validation Loss: 1.1685714721679688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1471: Training Loss: 0.8488232493400574 Validation Loss: 1.1682339906692505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1472: Training Loss: 0.8493713736534119 Validation Loss: 1.1681115627288818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1473: Training Loss: 0.8480910062789917 Validation Loss: 1.1668857336044312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1474: Training Loss: 0.8473575711250305 Validation Loss: 1.1663577556610107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1475: Training Loss: 0.846977432568868 Validation Loss: 1.1654609441757202\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1476: Training Loss: 0.848826547463735 Validation Loss: 1.1652990579605103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1477: Training Loss: 0.8458857734998068 Validation Loss: 1.1644901037216187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1478: Training Loss: 0.846368412176768 Validation Loss: 1.1641265153884888\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1479: Training Loss: 0.8428586522738138 Validation Loss: 1.1635433435440063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1480: Training Loss: 0.8471458951632181 Validation Loss: 1.163038969039917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1481: Training Loss: 0.8435525298118591 Validation Loss: 1.162662148475647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1482: Training Loss: 0.8423909743626913 Validation Loss: 1.1620886325836182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1483: Training Loss: 0.8416635592778524 Validation Loss: 1.1617265939712524\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1484: Training Loss: 0.8409292499224345 Validation Loss: 1.1612763404846191\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1485: Training Loss: 0.8411389390627543 Validation Loss: 1.1603344678878784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1486: Training Loss: 0.8432261149088541 Validation Loss: 1.1601680517196655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1487: Training Loss: 0.8399772842725118 Validation Loss: 1.1596957445144653\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1488: Training Loss: 0.8391152620315552 Validation Loss: 1.1589642763137817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1489: Training Loss: 0.8390437761942545 Validation Loss: 1.1588164567947388\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1490: Training Loss: 0.838812013467153 Validation Loss: 1.1590200662612915\n",
      "Epoch 1491: Training Loss: 0.8368316491444906 Validation Loss: 1.158581256866455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1492: Training Loss: 0.8365177909533182 Validation Loss: 1.1581178903579712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1493: Training Loss: 0.8366194168726603 Validation Loss: 1.1574013233184814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1494: Training Loss: 0.8353310028711954 Validation Loss: 1.1560076475143433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1495: Training Loss: 0.8350140452384949 Validation Loss: 1.1554659605026245\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1496: Training Loss: 0.8344200849533081 Validation Loss: 1.1546947956085205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1497: Training Loss: 0.8343622088432312 Validation Loss: 1.1537082195281982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1498: Training Loss: 0.8331191142400106 Validation Loss: 1.154006838798523\n",
      "Epoch 1499: Training Loss: 0.8320429722468058 Validation Loss: 1.1541976928710938\n",
      "Epoch 1500: Training Loss: 0.8328091104825338 Validation Loss: 1.1541072130203247\n",
      "Epoch 1501: Training Loss: 0.8324429988861084 Validation Loss: 1.152956247329712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1502: Training Loss: 0.8308488329251608 Validation Loss: 1.152135968208313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1503: Training Loss: 0.8310901920000712 Validation Loss: 1.1518622636795044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1504: Training Loss: 0.8296189705530802 Validation Loss: 1.1509068012237549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1505: Training Loss: 0.8297368486722311 Validation Loss: 1.1501224040985107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1506: Training Loss: 0.8292625149091085 Validation Loss: 1.1501859426498413\n",
      "Epoch 1507: Training Loss: 0.8284396330515543 Validation Loss: 1.1499130725860596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1508: Training Loss: 0.8273870547612509 Validation Loss: 1.149430513381958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1509: Training Loss: 0.8267399668693542 Validation Loss: 1.1488350629806519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1510: Training Loss: 0.8269510666529337 Validation Loss: 1.1480226516723633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1511: Training Loss: 0.8257096608479818 Validation Loss: 1.1480779647827148\n",
      "Epoch 1512: Training Loss: 0.8250995477040609 Validation Loss: 1.1481819152832031\n",
      "Epoch 1513: Training Loss: 0.8240382870038351 Validation Loss: 1.1475276947021484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1514: Training Loss: 0.8243589003880819 Validation Loss: 1.1469333171844482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1515: Training Loss: 0.823302169640859 Validation Loss: 1.1460466384887695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1516: Training Loss: 0.8232744733492533 Validation Loss: 1.1451894044876099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1517: Training Loss: 0.8225767811139425 Validation Loss: 1.1450704336166382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1518: Training Loss: 0.821881095568339 Validation Loss: 1.144911289215088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1519: Training Loss: 0.8216597437858582 Validation Loss: 1.1443512439727783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1520: Training Loss: 0.8208921949068705 Validation Loss: 1.1442127227783203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1521: Training Loss: 0.819844921429952 Validation Loss: 1.1434437036514282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1522: Training Loss: 0.8203379909197489 Validation Loss: 1.1428325176239014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1523: Training Loss: 0.8180827299753824 Validation Loss: 1.1423282623291016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1524: Training Loss: 0.8186362981796265 Validation Loss: 1.1419398784637451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1525: Training Loss: 0.8185384273529053 Validation Loss: 1.1410634517669678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1526: Training Loss: 0.8189917008082072 Validation Loss: 1.1403814554214478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1527: Training Loss: 0.8186539808909098 Validation Loss: 1.1394221782684326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1528: Training Loss: 0.8157365520795187 Validation Loss: 1.1393555402755737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1529: Training Loss: 0.8152937491734823 Validation Loss: 1.1396404504776\n",
      "Epoch 1530: Training Loss: 0.8150188326835632 Validation Loss: 1.139388084411621\n",
      "Epoch 1531: Training Loss: 0.8163200616836548 Validation Loss: 1.138981819152832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1532: Training Loss: 0.8137741883595785 Validation Loss: 1.1376975774765015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1533: Training Loss: 0.8130476872126261 Validation Loss: 1.1365987062454224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1534: Training Loss: 0.8128728270530701 Validation Loss: 1.1362078189849854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1535: Training Loss: 0.8133132060368856 Validation Loss: 1.1363027095794678\n",
      "Epoch 1536: Training Loss: 0.8112886746724447 Validation Loss: 1.1363097429275513\n",
      "Epoch 1537: Training Loss: 0.8106783231099447 Validation Loss: 1.1360944509506226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1538: Training Loss: 0.8113087018330892 Validation Loss: 1.1355725526809692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1539: Training Loss: 0.8095038334528605 Validation Loss: 1.1348085403442383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1540: Training Loss: 0.8080576658248901 Validation Loss: 1.133955955505371\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1541: Training Loss: 0.8089290062586466 Validation Loss: 1.1340816020965576\n",
      "Epoch 1542: Training Loss: 0.8084497253100077 Validation Loss: 1.133690595626831\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1543: Training Loss: 0.8077526887257894 Validation Loss: 1.1329089403152466\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1544: Training Loss: 0.8071995973587036 Validation Loss: 1.132206678390503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1545: Training Loss: 0.807141383488973 Validation Loss: 1.132068157196045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1546: Training Loss: 0.8058976332346598 Validation Loss: 1.131394624710083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1547: Training Loss: 0.8055810133616129 Validation Loss: 1.1307897567749023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1548: Training Loss: 0.8045158584912618 Validation Loss: 1.1301573514938354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1549: Training Loss: 0.8086076577504476 Validation Loss: 1.1292139291763306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1550: Training Loss: 0.8067231774330139 Validation Loss: 1.1292476654052734\n",
      "Epoch 1551: Training Loss: 0.8035222291946411 Validation Loss: 1.1292177438735962\n",
      "Epoch 1552: Training Loss: 0.8034147620201111 Validation Loss: 1.1286112070083618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1553: Training Loss: 0.8014411330223083 Validation Loss: 1.1281644105911255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1554: Training Loss: 0.8008373181025187 Validation Loss: 1.1273269653320312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1555: Training Loss: 0.8012309670448303 Validation Loss: 1.1270548105239868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1556: Training Loss: 0.8023655017217001 Validation Loss: 1.1269639730453491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1557: Training Loss: 0.7991803884506226 Validation Loss: 1.1266436576843262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1558: Training Loss: 0.7985542416572571 Validation Loss: 1.126046061515808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1559: Training Loss: 0.7987588246663412 Validation Loss: 1.1250231266021729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1560: Training Loss: 0.8005868395169576 Validation Loss: 1.1244127750396729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1561: Training Loss: 0.7975921034812927 Validation Loss: 1.1236151456832886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1562: Training Loss: 0.7988722721735636 Validation Loss: 1.123154878616333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1563: Training Loss: 0.7963046630223592 Validation Loss: 1.1227459907531738\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1564: Training Loss: 0.7964298327763876 Validation Loss: 1.1229960918426514\n",
      "Epoch 1565: Training Loss: 0.7954363822937012 Validation Loss: 1.1228207349777222\n",
      "Epoch 1566: Training Loss: 0.7978291114171346 Validation Loss: 1.1224699020385742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1567: Training Loss: 0.7932485739390055 Validation Loss: 1.1218328475952148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1568: Training Loss: 0.7983576059341431 Validation Loss: 1.121484398841858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1569: Training Loss: 0.7934160629908243 Validation Loss: 1.1211341619491577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1570: Training Loss: 0.7899345556894938 Validation Loss: 1.1205734014511108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1571: Training Loss: 0.7923850019772848 Validation Loss: 1.119692087173462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1572: Training Loss: 0.7920591433842977 Validation Loss: 1.118958592414856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1573: Training Loss: 0.7909637093544006 Validation Loss: 1.1184296607971191\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1574: Training Loss: 0.7906111677487692 Validation Loss: 1.1183842420578003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1575: Training Loss: 0.7901778221130371 Validation Loss: 1.1179096698760986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1576: Training Loss: 0.7896008690198263 Validation Loss: 1.117453694343567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1577: Training Loss: 0.7878310481707255 Validation Loss: 1.1168228387832642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1578: Training Loss: 0.787521243095398 Validation Loss: 1.116515040397644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1579: Training Loss: 0.7892480492591858 Validation Loss: 1.1159114837646484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1580: Training Loss: 0.7876274188359579 Validation Loss: 1.1155928373336792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1581: Training Loss: 0.7855836749076843 Validation Loss: 1.1156874895095825\n",
      "Epoch 1582: Training Loss: 0.7871896425882975 Validation Loss: 1.115196704864502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1583: Training Loss: 0.7858957846959432 Validation Loss: 1.1143229007720947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1584: Training Loss: 0.7855895161628723 Validation Loss: 1.1144404411315918\n",
      "Epoch 1585: Training Loss: 0.7840944329897562 Validation Loss: 1.1138992309570312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1586: Training Loss: 0.783602257569631 Validation Loss: 1.113547444343567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1587: Training Loss: 0.7833593090375265 Validation Loss: 1.1129642724990845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1588: Training Loss: 0.7833725412686666 Validation Loss: 1.111645221710205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1589: Training Loss: 0.7832581798235575 Validation Loss: 1.1106919050216675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1590: Training Loss: 0.781828780968984 Validation Loss: 1.1104986667633057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1591: Training Loss: 0.7816114624341329 Validation Loss: 1.1101428270339966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1592: Training Loss: 0.7846829891204834 Validation Loss: 1.1099082231521606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1593: Training Loss: 0.78235391775767 Validation Loss: 1.1096681356430054\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1594: Training Loss: 0.7801359295845032 Validation Loss: 1.1091846227645874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1595: Training Loss: 0.7802402377128601 Validation Loss: 1.109009861946106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1596: Training Loss: 0.7797560493151346 Validation Loss: 1.109009027481079\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1597: Training Loss: 0.7783753077189127 Validation Loss: 1.1085928678512573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1598: Training Loss: 0.7784185608228048 Validation Loss: 1.1083446741104126\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1599: Training Loss: 0.7780302961667379 Validation Loss: 1.107210636138916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1600: Training Loss: 0.7758908867835999 Validation Loss: 1.1062555313110352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1601: Training Loss: 0.7756237586339315 Validation Loss: 1.1061958074569702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1602: Training Loss: 0.7758554021517435 Validation Loss: 1.1052923202514648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1603: Training Loss: 0.7754208842913309 Validation Loss: 1.1051883697509766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1604: Training Loss: 0.7743621468544006 Validation Loss: 1.104941725730896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1605: Training Loss: 0.7765412330627441 Validation Loss: 1.1048508882522583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1606: Training Loss: 0.7733046809832255 Validation Loss: 1.1040352582931519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1607: Training Loss: 0.774841825167338 Validation Loss: 1.1033294200897217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1608: Training Loss: 0.7731216549873352 Validation Loss: 1.1027967929840088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1609: Training Loss: 0.7718789974848429 Validation Loss: 1.102534532546997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1610: Training Loss: 0.772418479124705 Validation Loss: 1.1023868322372437\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1611: Training Loss: 0.7702285250027975 Validation Loss: 1.1020374298095703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1612: Training Loss: 0.770298977692922 Validation Loss: 1.1013689041137695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1613: Training Loss: 0.7700685461362203 Validation Loss: 1.1008234024047852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1614: Training Loss: 0.7716222206751505 Validation Loss: 1.100535273551941\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1615: Training Loss: 0.7683836420377096 Validation Loss: 1.0995482206344604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1616: Training Loss: 0.7683290839195251 Validation Loss: 1.099621057510376\n",
      "Epoch 1617: Training Loss: 0.7684293389320374 Validation Loss: 1.0995063781738281\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1618: Training Loss: 0.7665717005729675 Validation Loss: 1.0994511842727661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1619: Training Loss: 0.7664703329404196 Validation Loss: 1.0987733602523804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1620: Training Loss: 0.7658322254816691 Validation Loss: 1.0984231233596802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1621: Training Loss: 0.7664801875750223 Validation Loss: 1.097781777381897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1622: Training Loss: 0.7656358679135641 Validation Loss: 1.0970288515090942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1623: Training Loss: 0.7650012373924255 Validation Loss: 1.0961992740631104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1624: Training Loss: 0.7637278834978739 Validation Loss: 1.0957635641098022\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1625: Training Loss: 0.7651699384053549 Validation Loss: 1.0957705974578857\n",
      "Epoch 1626: Training Loss: 0.7645155787467957 Validation Loss: 1.0955994129180908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1627: Training Loss: 0.7628586292266846 Validation Loss: 1.0957274436950684\n",
      "Epoch 1628: Training Loss: 0.7639880975087484 Validation Loss: 1.0952091217041016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1629: Training Loss: 0.7613811095555624 Validation Loss: 1.0939600467681885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1630: Training Loss: 0.7618201772371928 Validation Loss: 1.0930718183517456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1631: Training Loss: 0.7611110210418701 Validation Loss: 1.092965841293335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1632: Training Loss: 0.7600300709406534 Validation Loss: 1.0928587913513184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1633: Training Loss: 0.7605109214782715 Validation Loss: 1.0921616554260254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1634: Training Loss: 0.7601653536160787 Validation Loss: 1.0920300483703613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1635: Training Loss: 0.7601775725682577 Validation Loss: 1.0914934873580933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1636: Training Loss: 0.759819229443868 Validation Loss: 1.0909597873687744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1637: Training Loss: 0.7573872407277426 Validation Loss: 1.0902833938598633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1638: Training Loss: 0.7570225397745768 Validation Loss: 1.089921474456787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1639: Training Loss: 0.7552944620450338 Validation Loss: 1.089402437210083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1640: Training Loss: 0.7563494245211283 Validation Loss: 1.0890477895736694\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1641: Training Loss: 0.7555066347122192 Validation Loss: 1.088884949684143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1642: Training Loss: 0.7543967763582865 Validation Loss: 1.08843994140625\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1643: Training Loss: 0.7540552218755087 Validation Loss: 1.0878206491470337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1644: Training Loss: 0.7546805739402771 Validation Loss: 1.08749258518219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1645: Training Loss: 0.7557863394419352 Validation Loss: 1.0873500108718872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1646: Training Loss: 0.7534671823183695 Validation Loss: 1.0875178575515747\n",
      "Epoch 1647: Training Loss: 0.7523440718650818 Validation Loss: 1.087364673614502\n",
      "Epoch 1648: Training Loss: 0.7519174218177795 Validation Loss: 1.0865527391433716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1649: Training Loss: 0.7509046792984009 Validation Loss: 1.0858246088027954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1650: Training Loss: 0.7504995465278625 Validation Loss: 1.0853652954101562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1651: Training Loss: 0.7503249049186707 Validation Loss: 1.0850913524627686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1652: Training Loss: 0.7490513523419698 Validation Loss: 1.0846624374389648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1653: Training Loss: 0.7489907542864481 Validation Loss: 1.0837483406066895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1654: Training Loss: 0.7492078145345052 Validation Loss: 1.083508849143982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1655: Training Loss: 0.7486583391825358 Validation Loss: 1.0827406644821167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1656: Training Loss: 0.7483590443929037 Validation Loss: 1.0819553136825562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1657: Training Loss: 0.7470788757006327 Validation Loss: 1.0817209482192993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1658: Training Loss: 0.7456972002983093 Validation Loss: 1.081527590751648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1659: Training Loss: 0.7469300031661987 Validation Loss: 1.0817981958389282\n",
      "Epoch 1660: Training Loss: 0.7458113630612692 Validation Loss: 1.0819623470306396\n",
      "Epoch 1661: Training Loss: 0.7470306356747946 Validation Loss: 1.0812960863113403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1662: Training Loss: 0.744692325592041 Validation Loss: 1.0808281898498535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1663: Training Loss: 0.7451932628949484 Validation Loss: 1.0801336765289307\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1664: Training Loss: 0.7436409791310629 Validation Loss: 1.079116702079773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1665: Training Loss: 0.7441319425900778 Validation Loss: 1.0786939859390259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1666: Training Loss: 0.7424985965092977 Validation Loss: 1.077813982963562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1667: Training Loss: 0.7424871325492859 Validation Loss: 1.0780161619186401\n",
      "Epoch 1668: Training Loss: 0.7420852382977804 Validation Loss: 1.0779308080673218\n",
      "Epoch 1669: Training Loss: 0.7422555088996887 Validation Loss: 1.0774801969528198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1670: Training Loss: 0.7439822753270467 Validation Loss: 1.0767364501953125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1671: Training Loss: 0.7395948568979899 Validation Loss: 1.0766419172286987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1672: Training Loss: 0.7375881870587667 Validation Loss: 1.0763221979141235\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1673: Training Loss: 0.7392404874165853 Validation Loss: 1.0762481689453125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1674: Training Loss: 0.7404735485712687 Validation Loss: 1.0750917196273804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1675: Training Loss: 0.7385043303171793 Validation Loss: 1.0746127367019653\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1676: Training Loss: 0.7381923596064249 Validation Loss: 1.0740121603012085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1677: Training Loss: 0.7357600331306458 Validation Loss: 1.0738996267318726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1678: Training Loss: 0.7387912074724833 Validation Loss: 1.073792576789856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1679: Training Loss: 0.7384585936864217 Validation Loss: 1.0735691785812378\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1680: Training Loss: 0.7375761667887369 Validation Loss: 1.0729243755340576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1681: Training Loss: 0.73533034324646 Validation Loss: 1.071771264076233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1682: Training Loss: 0.7365857561429342 Validation Loss: 1.071658968925476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1683: Training Loss: 0.7357991933822632 Validation Loss: 1.0717532634735107\n",
      "Epoch 1684: Training Loss: 0.7339755296707153 Validation Loss: 1.0714468955993652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1685: Training Loss: 0.733128547668457 Validation Loss: 1.0706802606582642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1686: Training Loss: 0.7339065074920654 Validation Loss: 1.0699328184127808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1687: Training Loss: 0.7327110767364502 Validation Loss: 1.0697308778762817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1688: Training Loss: 0.7327611843744913 Validation Loss: 1.0696066617965698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1689: Training Loss: 0.733290950457255 Validation Loss: 1.06905996799469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1690: Training Loss: 0.7321477135022482 Validation Loss: 1.0691206455230713\n",
      "Epoch 1691: Training Loss: 0.7297797004381815 Validation Loss: 1.0686393976211548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1692: Training Loss: 0.7292602062225342 Validation Loss: 1.067638635635376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1693: Training Loss: 0.7292449076970419 Validation Loss: 1.0674606561660767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1694: Training Loss: 0.730073610941569 Validation Loss: 1.0672246217727661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1695: Training Loss: 0.7298551996548971 Validation Loss: 1.066707730293274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1696: Training Loss: 0.7275250951449076 Validation Loss: 1.0663108825683594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1697: Training Loss: 0.7283391952514648 Validation Loss: 1.0661542415618896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1698: Training Loss: 0.7270126938819885 Validation Loss: 1.0653108358383179\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1699: Training Loss: 0.7269113262494405 Validation Loss: 1.0651618242263794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1700: Training Loss: 0.7260312438011169 Validation Loss: 1.064553141593933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1701: Training Loss: 0.7254940072695414 Validation Loss: 1.063887119293213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1702: Training Loss: 0.7263145248095194 Validation Loss: 1.0637083053588867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1703: Training Loss: 0.7276656230290731 Validation Loss: 1.0631837844848633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1704: Training Loss: 0.7245914737383524 Validation Loss: 1.0631290674209595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1705: Training Loss: 0.7250117659568787 Validation Loss: 1.0631299018859863\n",
      "Epoch 1706: Training Loss: 0.7237193385759989 Validation Loss: 1.062749981880188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1707: Training Loss: 0.7238518397013346 Validation Loss: 1.062725305557251\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1708: Training Loss: 0.7235628962516785 Validation Loss: 1.062062382698059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1709: Training Loss: 0.7228045264879862 Validation Loss: 1.0613270998001099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1710: Training Loss: 0.7214303414026896 Validation Loss: 1.0607106685638428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1711: Training Loss: 0.7212051749229431 Validation Loss: 1.0604379177093506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1712: Training Loss: 0.7203578551610311 Validation Loss: 1.0597596168518066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1713: Training Loss: 0.7208201686541239 Validation Loss: 1.0595996379852295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1714: Training Loss: 0.7199315627415975 Validation Loss: 1.0589573383331299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1715: Training Loss: 0.7193254431088766 Validation Loss: 1.05849027633667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1716: Training Loss: 0.722639262676239 Validation Loss: 1.0582866668701172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1717: Training Loss: 0.7175028324127197 Validation Loss: 1.058127760887146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1718: Training Loss: 0.7180028160413107 Validation Loss: 1.0580536127090454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1719: Training Loss: 0.7175102432568868 Validation Loss: 1.0570557117462158\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1720: Training Loss: 0.7183228135108948 Validation Loss: 1.0564696788787842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1721: Training Loss: 0.716559648513794 Validation Loss: 1.0563896894454956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1722: Training Loss: 0.7164873480796814 Validation Loss: 1.0559279918670654\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1723: Training Loss: 0.7164949774742126 Validation Loss: 1.0560998916625977\n",
      "Epoch 1724: Training Loss: 0.714531143506368 Validation Loss: 1.0560660362243652\n",
      "Epoch 1725: Training Loss: 0.7143622239430746 Validation Loss: 1.0553339719772339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1726: Training Loss: 0.7142500082651774 Validation Loss: 1.0551998615264893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1727: Training Loss: 0.7131600975990295 Validation Loss: 1.0546573400497437\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1728: Training Loss: 0.7128478686014811 Validation Loss: 1.0536686182022095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1729: Training Loss: 0.7137868801752726 Validation Loss: 1.0530602931976318\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1730: Training Loss: 0.7117562890052795 Validation Loss: 1.0530171394348145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1731: Training Loss: 0.7155750195185343 Validation Loss: 1.052504062652588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1732: Training Loss: 0.7111977140108744 Validation Loss: 1.052268385887146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1733: Training Loss: 0.7102404038111368 Validation Loss: 1.05160653591156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1734: Training Loss: 0.7097581227620443 Validation Loss: 1.0511925220489502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1735: Training Loss: 0.7100519140561422 Validation Loss: 1.0509395599365234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1736: Training Loss: 0.7084460059801737 Validation Loss: 1.0506902933120728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1737: Training Loss: 0.7115954955418905 Validation Loss: 1.0503742694854736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1738: Training Loss: 0.7086542050043741 Validation Loss: 1.0493673086166382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1739: Training Loss: 0.7093870441118876 Validation Loss: 1.0491821765899658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1740: Training Loss: 0.7104038993517557 Validation Loss: 1.0489673614501953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1741: Training Loss: 0.7067359487215678 Validation Loss: 1.0493266582489014\n",
      "Epoch 1742: Training Loss: 0.7076514959335327 Validation Loss: 1.0486572980880737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1743: Training Loss: 0.7070064345995585 Validation Loss: 1.0483225584030151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1744: Training Loss: 0.7066461046536764 Validation Loss: 1.0479381084442139\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1745: Training Loss: 0.7061762015024821 Validation Loss: 1.0475085973739624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1746: Training Loss: 0.705219586690267 Validation Loss: 1.046928882598877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1747: Training Loss: 0.7038217385609945 Validation Loss: 1.0469472408294678\n",
      "Epoch 1748: Training Loss: 0.7035629749298096 Validation Loss: 1.0459504127502441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1749: Training Loss: 0.7045202851295471 Validation Loss: 1.045436978340149\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1750: Training Loss: 0.7026791969935099 Validation Loss: 1.0453073978424072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1751: Training Loss: 0.7022342483202616 Validation Loss: 1.044817328453064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1752: Training Loss: 0.7011646032333374 Validation Loss: 1.0443710088729858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1753: Training Loss: 0.7007097800572714 Validation Loss: 1.044253945350647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1754: Training Loss: 0.7010565797487894 Validation Loss: 1.043960690498352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1755: Training Loss: 0.7004281878471375 Validation Loss: 1.0437304973602295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1756: Training Loss: 0.700173556804657 Validation Loss: 1.0433098077774048\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1757: Training Loss: 0.6992072065671285 Validation Loss: 1.042606234550476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1758: Training Loss: 0.6989692449569702 Validation Loss: 1.042235255241394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1759: Training Loss: 0.6991154551506042 Validation Loss: 1.0417802333831787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1760: Training Loss: 0.6981255610783895 Validation Loss: 1.0419853925704956\n",
      "Epoch 1761: Training Loss: 0.6971621910730997 Validation Loss: 1.0412280559539795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1762: Training Loss: 0.6970702807108561 Validation Loss: 1.0411202907562256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1763: Training Loss: 0.6969498594601949 Validation Loss: 1.04083251953125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1764: Training Loss: 0.6962357958157858 Validation Loss: 1.04057776927948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1765: Training Loss: 0.6974058747291565 Validation Loss: 1.0398569107055664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1766: Training Loss: 0.6961449186007181 Validation Loss: 1.039412260055542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1767: Training Loss: 0.6954938371976217 Validation Loss: 1.0386557579040527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1768: Training Loss: 0.695863405863444 Validation Loss: 1.0383024215698242\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1769: Training Loss: 0.6946159402529398 Validation Loss: 1.037886381149292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1770: Training Loss: 0.6916840473810831 Validation Loss: 1.0376088619232178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1771: Training Loss: 0.693927526473999 Validation Loss: 1.0373023748397827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1772: Training Loss: 0.6925822893778483 Validation Loss: 1.0368250608444214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1773: Training Loss: 0.6923267443974813 Validation Loss: 1.0364710092544556\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1774: Training Loss: 0.6932109594345093 Validation Loss: 1.036305546760559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1775: Training Loss: 0.6924558083216349 Validation Loss: 1.035957932472229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1776: Training Loss: 0.6936145027478536 Validation Loss: 1.0357595682144165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1777: Training Loss: 0.691826343536377 Validation Loss: 1.0356310606002808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1778: Training Loss: 0.6901939908663431 Validation Loss: 1.0350110530853271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1779: Training Loss: 0.6894983450571696 Validation Loss: 1.0343513488769531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1780: Training Loss: 0.6895129084587097 Validation Loss: 1.033630132675171\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1781: Training Loss: 0.6892959674199423 Validation Loss: 1.0339996814727783\n",
      "Epoch 1782: Training Loss: 0.6880544821421305 Validation Loss: 1.0337084531784058\n",
      "Epoch 1783: Training Loss: 0.6888422966003418 Validation Loss: 1.0334142446517944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1784: Training Loss: 0.6879450480143229 Validation Loss: 1.0328994989395142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1785: Training Loss: 0.6924710273742676 Validation Loss: 1.0326752662658691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1786: Training Loss: 0.6886011958122253 Validation Loss: 1.0323736667633057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1787: Training Loss: 0.6870911121368408 Validation Loss: 1.0315394401550293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1788: Training Loss: 0.686234692732493 Validation Loss: 1.0310368537902832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1789: Training Loss: 0.6857632795969645 Validation Loss: 1.0309031009674072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1790: Training Loss: 0.6837826371192932 Validation Loss: 1.0305027961730957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1791: Training Loss: 0.6851460933685303 Validation Loss: 1.0306012630462646\n",
      "Epoch 1792: Training Loss: 0.6841091314951578 Validation Loss: 1.0301504135131836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1793: Training Loss: 0.6833762923876444 Validation Loss: 1.0292863845825195\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1794: Training Loss: 0.68236110607783 Validation Loss: 1.028688669204712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1795: Training Loss: 0.6836728056271871 Validation Loss: 1.0284825563430786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1796: Training Loss: 0.682285209496816 Validation Loss: 1.0278981924057007\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1797: Training Loss: 0.6816425124804179 Validation Loss: 1.0275378227233887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1798: Training Loss: 0.6827332576115926 Validation Loss: 1.0277345180511475\n",
      "Epoch 1799: Training Loss: 0.6814095775286356 Validation Loss: 1.0273334980010986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1800: Training Loss: 0.6795777082443237 Validation Loss: 1.0268701314926147\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1801: Training Loss: 0.6795908013979594 Validation Loss: 1.0267629623413086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1802: Training Loss: 0.6806249022483826 Validation Loss: 1.0263811349868774\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1803: Training Loss: 0.678926686445872 Validation Loss: 1.0266180038452148\n",
      "Epoch 1804: Training Loss: 0.6798288226127625 Validation Loss: 1.026160717010498\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1805: Training Loss: 0.67825319369634 Validation Loss: 1.0256267786026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1806: Training Loss: 0.6790125767389933 Validation Loss: 1.0252188444137573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1807: Training Loss: 0.6776401003201803 Validation Loss: 1.0242209434509277\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1808: Training Loss: 0.6763013998667399 Validation Loss: 1.0240874290466309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1809: Training Loss: 0.6777231295903524 Validation Loss: 1.023546814918518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1810: Training Loss: 0.6771536072095236 Validation Loss: 1.0235445499420166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1811: Training Loss: 0.6753085851669312 Validation Loss: 1.0232874155044556\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1812: Training Loss: 0.6751873890558878 Validation Loss: 1.0230211019515991\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1813: Training Loss: 0.6765592098236084 Validation Loss: 1.02315092086792\n",
      "Epoch 1814: Training Loss: 0.6746145288149515 Validation Loss: 1.0224660634994507\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1815: Training Loss: 0.6753063003222147 Validation Loss: 1.0218225717544556\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1816: Training Loss: 0.6740381518999735 Validation Loss: 1.0216292142868042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1817: Training Loss: 0.6738998889923096 Validation Loss: 1.020840048789978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1818: Training Loss: 0.6739804546038309 Validation Loss: 1.0203166007995605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1819: Training Loss: 0.6745405991872152 Validation Loss: 1.0200761556625366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1820: Training Loss: 0.6714236338933309 Validation Loss: 1.0197545289993286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1821: Training Loss: 0.6714911063512167 Validation Loss: 1.019723653793335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1822: Training Loss: 0.6710892915725708 Validation Loss: 1.0196985006332397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1823: Training Loss: 0.6700859268506368 Validation Loss: 1.0189825296401978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1824: Training Loss: 0.6704405347506205 Validation Loss: 1.0184770822525024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1825: Training Loss: 0.6696265339851379 Validation Loss: 1.0176645517349243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1826: Training Loss: 0.6684150497118632 Validation Loss: 1.0171840190887451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1827: Training Loss: 0.6678744157155355 Validation Loss: 1.0171006917953491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1828: Training Loss: 0.66828719774882 Validation Loss: 1.0171407461166382\n",
      "Epoch 1829: Training Loss: 0.6681432127952576 Validation Loss: 1.0165292024612427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1830: Training Loss: 0.6675454378128052 Validation Loss: 1.0165385007858276\n",
      "Epoch 1831: Training Loss: 0.6669209202130636 Validation Loss: 1.0159332752227783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1832: Training Loss: 0.6671979427337646 Validation Loss: 1.0160744190216064\n",
      "Epoch 1833: Training Loss: 0.6662511428197225 Validation Loss: 1.0156000852584839\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1834: Training Loss: 0.6668110688527426 Validation Loss: 1.0148738622665405\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1835: Training Loss: 0.6652621825536092 Validation Loss: 1.014851450920105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1836: Training Loss: 0.6645552515983582 Validation Loss: 1.014721155166626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1837: Training Loss: 0.6641996701558431 Validation Loss: 1.0142699480056763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1838: Training Loss: 0.6650914351145426 Validation Loss: 1.0136890411376953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1839: Training Loss: 0.6637968818346659 Validation Loss: 1.0130637884140015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1840: Training Loss: 0.6620574394861857 Validation Loss: 1.012717604637146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1841: Training Loss: 0.6634190082550049 Validation Loss: 1.0129115581512451\n",
      "Epoch 1842: Training Loss: 0.662510613600413 Validation Loss: 1.0129446983337402\n",
      "Epoch 1843: Training Loss: 0.6623192230860392 Validation Loss: 1.0118999481201172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1844: Training Loss: 0.6624403993288676 Validation Loss: 1.0117706060409546\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1845: Training Loss: 0.6609447797139486 Validation Loss: 1.0110619068145752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1846: Training Loss: 0.6611471772193909 Validation Loss: 1.0112617015838623\n",
      "Epoch 1847: Training Loss: 0.6614782214164734 Validation Loss: 1.0108027458190918\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1848: Training Loss: 0.6602748036384583 Validation Loss: 1.0103765726089478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1849: Training Loss: 0.6598596374193827 Validation Loss: 1.0099457502365112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1850: Training Loss: 0.6593181093533834 Validation Loss: 1.009181261062622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1851: Training Loss: 0.6591106255849203 Validation Loss: 1.0090389251708984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1852: Training Loss: 0.6589164137840271 Validation Loss: 1.0092540979385376\n",
      "Epoch 1853: Training Loss: 0.657664438088735 Validation Loss: 1.0088646411895752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1854: Training Loss: 0.6570181846618652 Validation Loss: 1.0078802108764648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1855: Training Loss: 0.6581704219182333 Validation Loss: 1.0076524019241333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1856: Training Loss: 0.6563485662142435 Validation Loss: 1.0077388286590576\n",
      "Epoch 1857: Training Loss: 0.6553839842478434 Validation Loss: 1.0076302289962769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1858: Training Loss: 0.6564469536145529 Validation Loss: 1.007128119468689\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1859: Training Loss: 0.6555160681406657 Validation Loss: 1.0065304040908813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1860: Training Loss: 0.6544308463732401 Validation Loss: 1.0063798427581787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1861: Training Loss: 0.6545613606770834 Validation Loss: 1.0060290098190308\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1862: Training Loss: 0.6539925138155619 Validation Loss: 1.0053293704986572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1863: Training Loss: 0.655659019947052 Validation Loss: 1.0052522420883179\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1864: Training Loss: 0.6531946659088135 Validation Loss: 1.004881739616394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1865: Training Loss: 0.6529565254847208 Validation Loss: 1.0048093795776367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1866: Training Loss: 0.6535622874895731 Validation Loss: 1.0041030645370483\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1867: Training Loss: 0.6526300509770712 Validation Loss: 1.0036242008209229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1868: Training Loss: 0.6519938111305237 Validation Loss: 1.0038942098617554\n",
      "Epoch 1869: Training Loss: 0.6520024736722311 Validation Loss: 1.0037463903427124\n",
      "Epoch 1870: Training Loss: 0.6518151760101318 Validation Loss: 1.0031650066375732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1871: Training Loss: 0.6516508460044861 Validation Loss: 1.00254487991333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1872: Training Loss: 0.6497089862823486 Validation Loss: 1.0023680925369263\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1873: Training Loss: 0.6501020391782125 Validation Loss: 1.0012643337249756\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1874: Training Loss: 0.6492606997489929 Validation Loss: 1.0015226602554321\n",
      "Epoch 1875: Training Loss: 0.6506734689076742 Validation Loss: 1.000731348991394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1876: Training Loss: 0.6488528649012247 Validation Loss: 1.0006060600280762\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1877: Training Loss: 0.6479533314704895 Validation Loss: 1.0006842613220215\n",
      "Epoch 1878: Training Loss: 0.6484539111455282 Validation Loss: 1.000654935836792\n",
      "Epoch 1879: Training Loss: 0.6493716835975647 Validation Loss: 1.000243902206421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1880: Training Loss: 0.646036167939504 Validation Loss: 0.999796986579895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1881: Training Loss: 0.6466818849245707 Validation Loss: 0.9991388916969299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1882: Training Loss: 0.6466209093729655 Validation Loss: 0.999157190322876\n",
      "Epoch 1883: Training Loss: 0.6466512878735861 Validation Loss: 0.9989663362503052\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1884: Training Loss: 0.645324687163035 Validation Loss: 0.9981457591056824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1885: Training Loss: 0.6451658209164938 Validation Loss: 0.9977612495422363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1886: Training Loss: 0.6449652910232544 Validation Loss: 0.9972052574157715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1887: Training Loss: 0.6451003154118856 Validation Loss: 0.9973087906837463\n",
      "Epoch 1888: Training Loss: 0.644902765750885 Validation Loss: 0.9973674416542053\n",
      "Epoch 1889: Training Loss: 0.6431243618329366 Validation Loss: 0.9973169565200806\n",
      "Epoch 1890: Training Loss: 0.6431455016136169 Validation Loss: 0.99679034948349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1891: Training Loss: 0.6435125668843588 Validation Loss: 0.9961509704589844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1892: Training Loss: 0.641810139020284 Validation Loss: 0.9953713417053223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1893: Training Loss: 0.6417708198229471 Validation Loss: 0.9948528409004211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1894: Training Loss: 0.6419772903124491 Validation Loss: 0.994966447353363\n",
      "Epoch 1895: Training Loss: 0.64296555519104 Validation Loss: 0.9945477247238159\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1896: Training Loss: 0.6400360465049744 Validation Loss: 0.9940191507339478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1897: Training Loss: 0.6414962808291117 Validation Loss: 0.9946420788764954\n",
      "Epoch 1898: Training Loss: 0.6400271058082581 Validation Loss: 0.9948275685310364\n",
      "Epoch 1899: Training Loss: 0.6391591628392538 Validation Loss: 0.9943093657493591\n",
      "Epoch 1900: Training Loss: 0.6414421995480856 Validation Loss: 0.993789553642273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1901: Training Loss: 0.6381878058115641 Validation Loss: 0.9931415319442749\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1902: Training Loss: 0.6382433772087097 Validation Loss: 0.9927133321762085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1903: Training Loss: 0.6388700803120931 Validation Loss: 0.9921581149101257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1904: Training Loss: 0.6373671094576517 Validation Loss: 0.9912728667259216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1905: Training Loss: 0.6396976908047994 Validation Loss: 0.9910697340965271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1906: Training Loss: 0.6362373034159342 Validation Loss: 0.9910480976104736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1907: Training Loss: 0.6356999278068542 Validation Loss: 0.9909592866897583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1908: Training Loss: 0.635821521282196 Validation Loss: 0.990833580493927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1909: Training Loss: 0.6364684303601583 Validation Loss: 0.9908955097198486\n",
      "Epoch 1910: Training Loss: 0.6364855964978536 Validation Loss: 0.9899910092353821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1911: Training Loss: 0.6343313256899515 Validation Loss: 0.9893672466278076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1912: Training Loss: 0.6366132299105326 Validation Loss: 0.9891095757484436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1913: Training Loss: 0.6339649756749471 Validation Loss: 0.9889895915985107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1914: Training Loss: 0.6332930326461792 Validation Loss: 0.9885552525520325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1915: Training Loss: 0.6336900194485983 Validation Loss: 0.9882078766822815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1916: Training Loss: 0.6326708594957987 Validation Loss: 0.988446831703186\n",
      "Epoch 1917: Training Loss: 0.6320943236351013 Validation Loss: 0.9881448745727539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1918: Training Loss: 0.6320397655169169 Validation Loss: 0.987815797328949\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1919: Training Loss: 0.6332558790842692 Validation Loss: 0.987424373626709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1920: Training Loss: 0.63113800684611 Validation Loss: 0.9871433973312378\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1921: Training Loss: 0.631157398223877 Validation Loss: 0.9861828088760376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1922: Training Loss: 0.6303732991218567 Validation Loss: 0.9858527183532715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1923: Training Loss: 0.6297646760940552 Validation Loss: 0.9858365654945374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1924: Training Loss: 0.6297094623247782 Validation Loss: 0.9854058027267456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1925: Training Loss: 0.6289650400479635 Validation Loss: 0.9849885702133179\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1926: Training Loss: 0.6306800842285156 Validation Loss: 0.9848008751869202\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1927: Training Loss: 0.6291526754697164 Validation Loss: 0.9846099019050598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1928: Training Loss: 0.6273877819379171 Validation Loss: 0.984404444694519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1929: Training Loss: 0.6273820002873739 Validation Loss: 0.9843290448188782\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1930: Training Loss: 0.6274726192156473 Validation Loss: 0.9838277697563171\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1931: Training Loss: 0.6281211376190186 Validation Loss: 0.9835554957389832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1932: Training Loss: 0.6265225013097128 Validation Loss: 0.9835329651832581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1933: Training Loss: 0.6262132525444031 Validation Loss: 0.9824999570846558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1934: Training Loss: 0.6260101000467936 Validation Loss: 0.9818663001060486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1935: Training Loss: 0.6252573927243551 Validation Loss: 0.9815213680267334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1936: Training Loss: 0.6248976588249207 Validation Loss: 0.9815362095832825\n",
      "Epoch 1937: Training Loss: 0.6242875456809998 Validation Loss: 0.981614351272583\n",
      "Epoch 1938: Training Loss: 0.6254852414131165 Validation Loss: 0.9813587069511414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1939: Training Loss: 0.6269483168919882 Validation Loss: 0.9811384677886963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1940: Training Loss: 0.6250844796498617 Validation Loss: 0.9808574318885803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1941: Training Loss: 0.6220700542132059 Validation Loss: 0.9806315302848816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1942: Training Loss: 0.6237566272417704 Validation Loss: 0.980465292930603\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1943: Training Loss: 0.6239389379819235 Validation Loss: 0.9802149534225464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1944: Training Loss: 0.6217710773150126 Validation Loss: 0.9796125888824463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1945: Training Loss: 0.6209724346796671 Validation Loss: 0.979141891002655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1946: Training Loss: 0.6209288636843363 Validation Loss: 0.9783257246017456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1947: Training Loss: 0.6208340525627136 Validation Loss: 0.9779759049415588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1948: Training Loss: 0.6201352477073669 Validation Loss: 0.9781600832939148\n",
      "Epoch 1949: Training Loss: 0.6201579173405966 Validation Loss: 0.9781064391136169\n",
      "Epoch 1950: Training Loss: 0.6198094685872396 Validation Loss: 0.9781753420829773\n",
      "Epoch 1951: Training Loss: 0.6198527415593466 Validation Loss: 0.9779020547866821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1952: Training Loss: 0.6183772285779318 Validation Loss: 0.9772315621376038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1953: Training Loss: 0.6215629776318868 Validation Loss: 0.9768643975257874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1954: Training Loss: 0.6180819869041443 Validation Loss: 0.9762914776802063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1955: Training Loss: 0.617311954498291 Validation Loss: 0.975800096988678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1956: Training Loss: 0.6169581810633341 Validation Loss: 0.9754534959793091\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1957: Training Loss: 0.6170348127683004 Validation Loss: 0.9754376411437988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1958: Training Loss: 0.61682657400767 Validation Loss: 0.9751182794570923\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1959: Training Loss: 0.6165511210759481 Validation Loss: 0.9750967025756836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1960: Training Loss: 0.6155376633008321 Validation Loss: 0.9747921824455261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1961: Training Loss: 0.6154829661051432 Validation Loss: 0.9744699001312256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1962: Training Loss: 0.6159043908119202 Validation Loss: 0.9746782183647156\n",
      "Epoch 1963: Training Loss: 0.6148030757904053 Validation Loss: 0.97397780418396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1964: Training Loss: 0.6139844457308451 Validation Loss: 0.9736807942390442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1965: Training Loss: 0.6151326696077982 Validation Loss: 0.972858190536499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1966: Training Loss: 0.6135549942652384 Validation Loss: 0.9726752042770386\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1967: Training Loss: 0.6135761737823486 Validation Loss: 0.9723497033119202\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1968: Training Loss: 0.6135449806849161 Validation Loss: 0.972324013710022\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1969: Training Loss: 0.6133754054705302 Validation Loss: 0.9720250368118286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1970: Training Loss: 0.6115411520004272 Validation Loss: 0.9715006351470947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1971: Training Loss: 0.6117419799168905 Validation Loss: 0.9716169834136963\n",
      "Epoch 1972: Training Loss: 0.6107827226320902 Validation Loss: 0.9712714552879333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1973: Training Loss: 0.6104575395584106 Validation Loss: 0.9710492491722107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1974: Training Loss: 0.6115037401517233 Validation Loss: 0.9708759188652039\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1975: Training Loss: 0.6108133991559347 Validation Loss: 0.9704436659812927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1976: Training Loss: 0.6103771924972534 Validation Loss: 0.9701247215270996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1977: Training Loss: 0.6111227869987488 Validation Loss: 0.9695108532905579\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1978: Training Loss: 0.6101574897766113 Validation Loss: 0.9692226052284241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1979: Training Loss: 0.6089638272921244 Validation Loss: 0.9688608646392822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1980: Training Loss: 0.6097492575645447 Validation Loss: 0.9686811566352844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1981: Training Loss: 0.6088024775187174 Validation Loss: 0.9684351682662964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1982: Training Loss: 0.6077953775723776 Validation Loss: 0.9683830738067627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1983: Training Loss: 0.6071880459785461 Validation Loss: 0.9684018492698669\n",
      "Epoch 1984: Training Loss: 0.606723964214325 Validation Loss: 0.967923104763031\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1985: Training Loss: 0.6074845393498739 Validation Loss: 0.9672113060951233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1986: Training Loss: 0.6063866019248962 Validation Loss: 0.9666215777397156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1987: Training Loss: 0.6062476634979248 Validation Loss: 0.9662183523178101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1988: Training Loss: 0.6056801478068033 Validation Loss: 0.9664697051048279\n",
      "Epoch 1989: Training Loss: 0.6054116487503052 Validation Loss: 0.9662758708000183\n",
      "Epoch 1990: Training Loss: 0.6043704946835836 Validation Loss: 0.9661279916763306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1991: Training Loss: 0.6068776845932007 Validation Loss: 0.9657363295555115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1992: Training Loss: 0.6042187015215555 Validation Loss: 0.9652464389801025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1993: Training Loss: 0.6039074858029684 Validation Loss: 0.964792788028717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1994: Training Loss: 0.6028277079264323 Validation Loss: 0.9652180075645447\n",
      "Epoch 1995: Training Loss: 0.6043766736984253 Validation Loss: 0.964787483215332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1996: Training Loss: 0.6025970180829366 Validation Loss: 0.9641236662864685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1997: Training Loss: 0.6020435690879822 Validation Loss: 0.9638354182243347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1998: Training Loss: 0.6019868652025858 Validation Loss: 0.9632689952850342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1999: Training Loss: 0.6036705573399862 Validation Loss: 0.9627894163131714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2000: Training Loss: 0.6020239194234213 Validation Loss: 0.9622079730033875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2001: Training Loss: 0.6006130576133728 Validation Loss: 0.9621484875679016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2002: Training Loss: 0.600823720296224 Validation Loss: 0.9622468948364258\n",
      "Epoch 2003: Training Loss: 0.5997920831044515 Validation Loss: 0.9619847536087036\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2004: Training Loss: 0.599208414554596 Validation Loss: 0.96177738904953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2005: Training Loss: 0.6008685628573099 Validation Loss: 0.9620916247367859\n",
      "Epoch 2006: Training Loss: 0.5995886921882629 Validation Loss: 0.9618169069290161\n",
      "Epoch 2007: Training Loss: 0.6002646485964457 Validation Loss: 0.9612194895744324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2008: Training Loss: 0.5988539854685465 Validation Loss: 0.9605806469917297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2009: Training Loss: 0.599188764890035 Validation Loss: 0.9602685570716858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2010: Training Loss: 0.5979457894961039 Validation Loss: 0.9600815176963806\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2011: Training Loss: 0.6022006471951803 Validation Loss: 0.9596078395843506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2012: Training Loss: 0.5972417791684469 Validation Loss: 0.9592545032501221\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2013: Training Loss: 0.5968973437945048 Validation Loss: 0.9589937925338745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2014: Training Loss: 0.5964825749397278 Validation Loss: 0.9588752388954163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2015: Training Loss: 0.5953494707743326 Validation Loss: 0.9585275053977966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2016: Training Loss: 0.5970636606216431 Validation Loss: 0.9583058953285217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2017: Training Loss: 0.5961706240971884 Validation Loss: 0.9582982659339905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2018: Training Loss: 0.5946038365364075 Validation Loss: 0.9583953619003296\n",
      "Epoch 2019: Training Loss: 0.5941095550855001 Validation Loss: 0.9579687118530273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2020: Training Loss: 0.5939628680547079 Validation Loss: 0.9576454162597656\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2021: Training Loss: 0.5937634309132894 Validation Loss: 0.9573575258255005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2022: Training Loss: 0.5930895805358887 Validation Loss: 0.956956148147583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2023: Training Loss: 0.5923194686571757 Validation Loss: 0.9562484622001648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2024: Training Loss: 0.5930717984835306 Validation Loss: 0.9558654427528381\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2025: Training Loss: 0.5914821624755859 Validation Loss: 0.9559707045555115\n",
      "Epoch 2026: Training Loss: 0.5932267506917318 Validation Loss: 0.9558132290840149\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2027: Training Loss: 0.5928205251693726 Validation Loss: 0.9555886387825012\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2028: Training Loss: 0.5911626815795898 Validation Loss: 0.9547615647315979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2029: Training Loss: 0.5922971566518148 Validation Loss: 0.9549114108085632\n",
      "Epoch 2030: Training Loss: 0.5925113558769226 Validation Loss: 0.9542043209075928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2031: Training Loss: 0.5905477404594421 Validation Loss: 0.9546408653259277\n",
      "Epoch 2032: Training Loss: 0.5910319288571676 Validation Loss: 0.9545316696166992\n",
      "Epoch 2033: Training Loss: 0.5903804699579874 Validation Loss: 0.9540995359420776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2034: Training Loss: 0.5889100432395935 Validation Loss: 0.954233705997467\n",
      "Epoch 2035: Training Loss: 0.5889074206352234 Validation Loss: 0.9538655281066895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2036: Training Loss: 0.5881080230077108 Validation Loss: 0.9533685445785522\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2037: Training Loss: 0.588002065817515 Validation Loss: 0.9527086615562439\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2038: Training Loss: 0.5883357326189677 Validation Loss: 0.9524343013763428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2039: Training Loss: 0.5875603556632996 Validation Loss: 0.952322244644165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2040: Training Loss: 0.5867094000180563 Validation Loss: 0.9517349600791931\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2041: Training Loss: 0.5874879360198975 Validation Loss: 0.9511540532112122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2042: Training Loss: 0.5858698487281799 Validation Loss: 0.9511784911155701\n",
      "Epoch 2043: Training Loss: 0.5860838890075684 Validation Loss: 0.9507275819778442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2044: Training Loss: 0.5856902599334717 Validation Loss: 0.9510295987129211\n",
      "Epoch 2045: Training Loss: 0.5851203203201294 Validation Loss: 0.9505587816238403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2046: Training Loss: 0.5847089091936747 Validation Loss: 0.9497436285018921\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2047: Training Loss: 0.5846935510635376 Validation Loss: 0.9498212337493896\n",
      "Epoch 2048: Training Loss: 0.5847058892250061 Validation Loss: 0.9497775435447693\n",
      "Epoch 2049: Training Loss: 0.5847672820091248 Validation Loss: 0.9497953653335571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2050: Training Loss: 0.5846503376960754 Validation Loss: 0.9495400190353394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2051: Training Loss: 0.5850909749666849 Validation Loss: 0.9490927457809448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2052: Training Loss: 0.5828070640563965 Validation Loss: 0.9487709999084473\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2053: Training Loss: 0.5832654039065043 Validation Loss: 0.9484995007514954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2054: Training Loss: 0.5821812152862549 Validation Loss: 0.9480096101760864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2055: Training Loss: 0.5818681716918945 Validation Loss: 0.9474015831947327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2056: Training Loss: 0.5817526380221049 Validation Loss: 0.9471381902694702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2057: Training Loss: 0.5816704630851746 Validation Loss: 0.9468342661857605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2058: Training Loss: 0.5819509625434875 Validation Loss: 0.946499764919281\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2059: Training Loss: 0.5806716481844584 Validation Loss: 0.9463525414466858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2060: Training Loss: 0.5847093860308329 Validation Loss: 0.9464567303657532\n",
      "Epoch 2061: Training Loss: 0.5804440975189209 Validation Loss: 0.9468655586242676\n",
      "Epoch 2062: Training Loss: 0.5810746351877848 Validation Loss: 0.9468182921409607\n",
      "Epoch 2063: Training Loss: 0.5797030528386434 Validation Loss: 0.9461816549301147\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2064: Training Loss: 0.5783689419428507 Validation Loss: 0.9455932378768921\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2065: Training Loss: 0.5781829158465067 Validation Loss: 0.9450710415840149\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2066: Training Loss: 0.5791687568028768 Validation Loss: 0.9449268579483032\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2067: Training Loss: 0.5782378117243449 Validation Loss: 0.9447197914123535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2068: Training Loss: 0.5777386625607809 Validation Loss: 0.9440330266952515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2069: Training Loss: 0.5763307412465414 Validation Loss: 0.9435930252075195\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2070: Training Loss: 0.5764628847440084 Validation Loss: 0.9430459141731262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2071: Training Loss: 0.576213002204895 Validation Loss: 0.9428481459617615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2072: Training Loss: 0.5764603217442831 Validation Loss: 0.9427955150604248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2073: Training Loss: 0.5749351779619852 Validation Loss: 0.9429290890693665\n",
      "Epoch 2074: Training Loss: 0.5751534303029379 Validation Loss: 0.9427182674407959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2075: Training Loss: 0.5752092401186625 Validation Loss: 0.9427739977836609\n",
      "Epoch 2076: Training Loss: 0.5756027499834696 Validation Loss: 0.9431713819503784\n",
      "Epoch 2077: Training Loss: 0.5743582646052042 Validation Loss: 0.9426994919776917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2078: Training Loss: 0.5742660363515218 Validation Loss: 0.9424042701721191\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2079: Training Loss: 0.5734360218048096 Validation Loss: 0.9416913390159607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2080: Training Loss: 0.5722860296567281 Validation Loss: 0.9413074254989624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2081: Training Loss: 0.5737640460332235 Validation Loss: 0.9410459399223328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2082: Training Loss: 0.5744746327400208 Validation Loss: 0.9405214786529541\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2083: Training Loss: 0.5721295475959778 Validation Loss: 0.940422534942627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2084: Training Loss: 0.5726309617360433 Validation Loss: 0.9398143887519836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2085: Training Loss: 0.5724001328150431 Validation Loss: 0.9394645094871521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2086: Training Loss: 0.5703184604644775 Validation Loss: 0.9393178820610046\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2087: Training Loss: 0.5713432431221008 Validation Loss: 0.9390912652015686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2088: Training Loss: 0.5711800853411356 Validation Loss: 0.9389201402664185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2089: Training Loss: 0.5698516766230265 Validation Loss: 0.9389991164207458\n",
      "Epoch 2090: Training Loss: 0.5699887077013651 Validation Loss: 0.9384684562683105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2091: Training Loss: 0.5708624720573425 Validation Loss: 0.938437283039093\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2092: Training Loss: 0.5696444312731425 Validation Loss: 0.9385926127433777\n",
      "Epoch 2093: Training Loss: 0.5702956517537435 Validation Loss: 0.9380533695220947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2094: Training Loss: 0.5694941083590189 Validation Loss: 0.9378557801246643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2095: Training Loss: 0.5687667926152548 Validation Loss: 0.9379011988639832\n",
      "Epoch 2096: Training Loss: 0.5686064958572388 Validation Loss: 0.9375280141830444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2097: Training Loss: 0.5686330199241638 Validation Loss: 0.9368471503257751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2098: Training Loss: 0.5668975114822388 Validation Loss: 0.9364818334579468\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2099: Training Loss: 0.5670537749926249 Validation Loss: 0.9360474944114685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2100: Training Loss: 0.566646675268809 Validation Loss: 0.9355484843254089\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2101: Training Loss: 0.5667396585146586 Validation Loss: 0.9352365732192993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2102: Training Loss: 0.566343088944753 Validation Loss: 0.9352601766586304\n",
      "Epoch 2103: Training Loss: 0.5676125685373942 Validation Loss: 0.9352271556854248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2104: Training Loss: 0.5656368931134542 Validation Loss: 0.9350670576095581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2105: Training Loss: 0.566763679186503 Validation Loss: 0.934818685054779\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2106: Training Loss: 0.5653403600056967 Validation Loss: 0.9348353743553162\n",
      "Epoch 2107: Training Loss: 0.5645487705866495 Validation Loss: 0.9345280528068542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2108: Training Loss: 0.5645336707433065 Validation Loss: 0.9344267249107361\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2109: Training Loss: 0.5645791987578074 Validation Loss: 0.9339933395385742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2110: Training Loss: 0.5636148452758789 Validation Loss: 0.9339548945426941\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2111: Training Loss: 0.5627485513687134 Validation Loss: 0.9334529638290405\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2112: Training Loss: 0.5648673176765442 Validation Loss: 0.9331916570663452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2113: Training Loss: 0.5630058844884237 Validation Loss: 0.9329315423965454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2114: Training Loss: 0.5620871186256409 Validation Loss: 0.9324471950531006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2115: Training Loss: 0.5620959599812826 Validation Loss: 0.9321600794792175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2116: Training Loss: 0.5612342754999796 Validation Loss: 0.9321092367172241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2117: Training Loss: 0.5611603458722433 Validation Loss: 0.9319326281547546\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2118: Training Loss: 0.5610895454883575 Validation Loss: 0.9315186142921448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2119: Training Loss: 0.5611299872398376 Validation Loss: 0.9311957955360413\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2120: Training Loss: 0.5603562394777933 Validation Loss: 0.9310094118118286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2121: Training Loss: 0.560353418191274 Validation Loss: 0.9306187033653259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2122: Training Loss: 0.5595711270968119 Validation Loss: 0.9303930401802063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2123: Training Loss: 0.5595927039782206 Validation Loss: 0.9297156929969788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2124: Training Loss: 0.5590413808822632 Validation Loss: 0.9297848343849182\n",
      "Epoch 2125: Training Loss: 0.5589183767636617 Validation Loss: 0.9300724267959595\n",
      "Epoch 2126: Training Loss: 0.5588734348615011 Validation Loss: 0.9294760227203369\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2127: Training Loss: 0.5589471260706583 Validation Loss: 0.929693877696991\n",
      "Epoch 2128: Training Loss: 0.5581961671511332 Validation Loss: 0.9292603731155396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2129: Training Loss: 0.5573995113372803 Validation Loss: 0.9285634160041809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2130: Training Loss: 0.5571797887484232 Validation Loss: 0.928470253944397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2131: Training Loss: 0.5581416090329488 Validation Loss: 0.9281675815582275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2132: Training Loss: 0.5565309921900431 Validation Loss: 0.9282702207565308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2133: Training Loss: 0.5568155646324158 Validation Loss: 0.9278181791305542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2134: Training Loss: 0.5562488436698914 Validation Loss: 0.9273887276649475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2135: Training Loss: 0.5562672217686971 Validation Loss: 0.9269571900367737\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2136: Training Loss: 0.5547237396240234 Validation Loss: 0.9266085028648376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2137: Training Loss: 0.5547310511271158 Validation Loss: 0.9262446761131287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2138: Training Loss: 0.5542539358139038 Validation Loss: 0.9262732863426208\n",
      "Epoch 2139: Training Loss: 0.554880420366923 Validation Loss: 0.9264972805976868\n",
      "Epoch 2140: Training Loss: 0.5544659694035848 Validation Loss: 0.9256362915039062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2141: Training Loss: 0.5547125140825907 Validation Loss: 0.926234781742096\n",
      "Epoch 2142: Training Loss: 0.5535556674003601 Validation Loss: 0.9260653257369995\n",
      "Epoch 2143: Training Loss: 0.5527709325154623 Validation Loss: 0.925399661064148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2144: Training Loss: 0.5544523000717163 Validation Loss: 0.9253256916999817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2145: Training Loss: 0.5528656442960104 Validation Loss: 0.924879789352417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2146: Training Loss: 0.5527695814768473 Validation Loss: 0.9248789548873901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2147: Training Loss: 0.5538454651832581 Validation Loss: 0.924820065498352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2148: Training Loss: 0.5556788643201193 Validation Loss: 0.9246488809585571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2149: Training Loss: 0.5511376063028971 Validation Loss: 0.9240500330924988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2150: Training Loss: 0.5508512059847513 Validation Loss: 0.9231479167938232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2151: Training Loss: 0.5514795581499735 Validation Loss: 0.9230843186378479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2152: Training Loss: 0.5526659985383352 Validation Loss: 0.923083484172821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2153: Training Loss: 0.5505478779474894 Validation Loss: 0.9230114221572876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2154: Training Loss: 0.5495535731315613 Validation Loss: 0.9227461218833923\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2155: Training Loss: 0.5486546556154887 Validation Loss: 0.9221569895744324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2156: Training Loss: 0.5492140452067057 Validation Loss: 0.9222936630249023\n",
      "Epoch 2157: Training Loss: 0.5494560798009237 Validation Loss: 0.9218786358833313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2158: Training Loss: 0.5481607913970947 Validation Loss: 0.9218631386756897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2159: Training Loss: 0.5476490060488383 Validation Loss: 0.9221373200416565\n",
      "Epoch 2160: Training Loss: 0.5495951771736145 Validation Loss: 0.9222458004951477\n",
      "Epoch 2161: Training Loss: 0.5473035176595052 Validation Loss: 0.9217175245285034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2162: Training Loss: 0.547211209932963 Validation Loss: 0.9212945699691772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2163: Training Loss: 0.5464949210484823 Validation Loss: 0.9202936887741089\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2164: Training Loss: 0.546566923459371 Validation Loss: 0.9196935892105103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2165: Training Loss: 0.5465982755025228 Validation Loss: 0.9194048643112183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2166: Training Loss: 0.5452731251716614 Validation Loss: 0.9195206761360168\n",
      "Epoch 2167: Training Loss: 0.5462637344996134 Validation Loss: 0.9193394780158997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2168: Training Loss: 0.5449910362561544 Validation Loss: 0.9192755222320557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2169: Training Loss: 0.5450287063916525 Validation Loss: 0.9193615317344666\n",
      "Epoch 2170: Training Loss: 0.54611603418986 Validation Loss: 0.9195774793624878\n",
      "Epoch 2171: Training Loss: 0.5441463987032572 Validation Loss: 0.9188358783721924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2172: Training Loss: 0.5440385341644287 Validation Loss: 0.9183014035224915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2173: Training Loss: 0.5427904923756918 Validation Loss: 0.9177987575531006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2174: Training Loss: 0.5434431831041971 Validation Loss: 0.9175168871879578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2175: Training Loss: 0.5449726978937784 Validation Loss: 0.9176812767982483\n",
      "Epoch 2176: Training Loss: 0.5430302818616232 Validation Loss: 0.9175259470939636\n",
      "Epoch 2177: Training Loss: 0.542493095000585 Validation Loss: 0.917204737663269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2178: Training Loss: 0.5427504777908325 Validation Loss: 0.917219877243042\n",
      "Epoch 2179: Training Loss: 0.5419560472170512 Validation Loss: 0.9169453978538513\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2180: Training Loss: 0.5422357122103373 Validation Loss: 0.9166692495346069\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2181: Training Loss: 0.5415813525517782 Validation Loss: 0.9167051911354065\n",
      "Epoch 2182: Training Loss: 0.5408773024876913 Validation Loss: 0.9165330529212952\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2183: Training Loss: 0.5407275756200155 Validation Loss: 0.9160589575767517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2184: Training Loss: 0.5438133676846822 Validation Loss: 0.9158329963684082\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2185: Training Loss: 0.5400474071502686 Validation Loss: 0.9156090021133423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2186: Training Loss: 0.5398155649503072 Validation Loss: 0.9154663681983948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2187: Training Loss: 0.5398867328961691 Validation Loss: 0.9155257940292358\n",
      "Epoch 2188: Training Loss: 0.5386787255605062 Validation Loss: 0.9149607419967651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2189: Training Loss: 0.5388607978820801 Validation Loss: 0.9145914316177368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2190: Training Loss: 0.5383258064587911 Validation Loss: 0.9138166904449463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2191: Training Loss: 0.5383278727531433 Validation Loss: 0.913090169429779\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2192: Training Loss: 0.537892738978068 Validation Loss: 0.9132946133613586\n",
      "Epoch 2193: Training Loss: 0.5377594431241354 Validation Loss: 0.9132373929023743\n",
      "Epoch 2194: Training Loss: 0.5372234185536703 Validation Loss: 0.9130678176879883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2195: Training Loss: 0.5369867086410522 Validation Loss: 0.9131709933280945\n",
      "Epoch 2196: Training Loss: 0.5368034839630127 Validation Loss: 0.9130066633224487\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2197: Training Loss: 0.5380987326304117 Validation Loss: 0.9128857254981995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2198: Training Loss: 0.5362027088801066 Validation Loss: 0.9125471711158752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2199: Training Loss: 0.5361906091372172 Validation Loss: 0.9123249053955078\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2200: Training Loss: 0.5354911088943481 Validation Loss: 0.9121292233467102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2201: Training Loss: 0.5367770989735922 Validation Loss: 0.9119182825088501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2202: Training Loss: 0.5343756477038065 Validation Loss: 0.9116400480270386\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2203: Training Loss: 0.535228411356608 Validation Loss: 0.9112832546234131\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2204: Training Loss: 0.5346186757087708 Validation Loss: 0.9111855626106262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2205: Training Loss: 0.5349245766798655 Validation Loss: 0.9110920429229736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2206: Training Loss: 0.533610999584198 Validation Loss: 0.9105940461158752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2207: Training Loss: 0.533977230389913 Validation Loss: 0.910040557384491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2208: Training Loss: 0.534661740064621 Validation Loss: 0.9097113013267517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2209: Training Loss: 0.5334978004296621 Validation Loss: 0.909872829914093\n",
      "Epoch 2210: Training Loss: 0.5333073337872823 Validation Loss: 0.9096729755401611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2211: Training Loss: 0.5321691830952963 Validation Loss: 0.9097135066986084\n",
      "Epoch 2212: Training Loss: 0.537326991558075 Validation Loss: 0.9093894362449646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2213: Training Loss: 0.5331015984217325 Validation Loss: 0.9087058305740356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2214: Training Loss: 0.5324466228485107 Validation Loss: 0.9085850715637207\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2215: Training Loss: 0.53187229235967 Validation Loss: 0.9082621335983276\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2216: Training Loss: 0.5308039387067159 Validation Loss: 0.9086024165153503\n",
      "Epoch 2217: Training Loss: 0.5316233138243357 Validation Loss: 0.9087974429130554\n",
      "Epoch 2218: Training Loss: 0.5300740500291189 Validation Loss: 0.9086740612983704\n",
      "Epoch 2219: Training Loss: 0.530280609925588 Validation Loss: 0.9081938862800598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2220: Training Loss: 0.5299994746843973 Validation Loss: 0.9078431129455566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2221: Training Loss: 0.5300579170385996 Validation Loss: 0.9075720906257629\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2222: Training Loss: 0.5299071768919627 Validation Loss: 0.9065970182418823\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2223: Training Loss: 0.5292836328347524 Validation Loss: 0.9062575697898865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2224: Training Loss: 0.5288672049840292 Validation Loss: 0.9067711234092712\n",
      "Epoch 2225: Training Loss: 0.5284117261568705 Validation Loss: 0.9069032669067383\n",
      "Epoch 2226: Training Loss: 0.5288000702857971 Validation Loss: 0.9062623381614685\n",
      "Epoch 2227: Training Loss: 0.5291231671969095 Validation Loss: 0.906394362449646\n",
      "Epoch 2228: Training Loss: 0.526974747578303 Validation Loss: 0.9061262607574463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2229: Training Loss: 0.5269879500071207 Validation Loss: 0.9055976867675781\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2230: Training Loss: 0.5280132393042246 Validation Loss: 0.9051457047462463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2231: Training Loss: 0.5267391999562582 Validation Loss: 0.9055266976356506\n",
      "Epoch 2232: Training Loss: 0.5270525217056274 Validation Loss: 0.9051393866539001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2233: Training Loss: 0.5274301866690317 Validation Loss: 0.9050076007843018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2234: Training Loss: 0.5260218679904938 Validation Loss: 0.9043264389038086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2235: Training Loss: 0.5275061130523682 Validation Loss: 0.903931736946106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2236: Training Loss: 0.5256990194320679 Validation Loss: 0.9034969210624695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2237: Training Loss: 0.525478184223175 Validation Loss: 0.9038597345352173\n",
      "Epoch 2238: Training Loss: 0.5248803098996481 Validation Loss: 0.9035824537277222\n",
      "Epoch 2239: Training Loss: 0.5244476795196533 Validation Loss: 0.9029321074485779\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2240: Training Loss: 0.5248727301756541 Validation Loss: 0.903217613697052\n",
      "Epoch 2241: Training Loss: 0.5246726274490356 Validation Loss: 0.9032222628593445\n",
      "Epoch 2242: Training Loss: 0.5234240194161733 Validation Loss: 0.9030101895332336\n",
      "Epoch 2243: Training Loss: 0.5233358442783356 Validation Loss: 0.9029436707496643\n",
      "Epoch 2244: Training Loss: 0.5228452185789744 Validation Loss: 0.9024991989135742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2245: Training Loss: 0.5246492028236389 Validation Loss: 0.9021041989326477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2246: Training Loss: 0.5235176682472229 Validation Loss: 0.9015323519706726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2247: Training Loss: 0.5223153829574585 Validation Loss: 0.901110827922821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2248: Training Loss: 0.5220953921477 Validation Loss: 0.9012398719787598\n",
      "Epoch 2249: Training Loss: 0.5213739077250162 Validation Loss: 0.9010884761810303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2250: Training Loss: 0.5215218663215637 Validation Loss: 0.9011809229850769\n",
      "Epoch 2251: Training Loss: 0.5210634469985962 Validation Loss: 0.9009237885475159\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2252: Training Loss: 0.5210537711779276 Validation Loss: 0.9001522660255432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2253: Training Loss: 0.5215437610944113 Validation Loss: 0.8998884558677673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2254: Training Loss: 0.519183874130249 Validation Loss: 0.8997453451156616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2255: Training Loss: 0.5193612972895304 Validation Loss: 0.8998441696166992\n",
      "Epoch 2256: Training Loss: 0.5199385980765024 Validation Loss: 0.900179386138916\n",
      "Epoch 2257: Training Loss: 0.5189437667528788 Validation Loss: 0.8996261358261108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2258: Training Loss: 0.5202539165814718 Validation Loss: 0.8993737101554871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2259: Training Loss: 0.520101934671402 Validation Loss: 0.8993741869926453\n",
      "Epoch 2260: Training Loss: 0.5186398923397064 Validation Loss: 0.89936763048172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2261: Training Loss: 0.5176831682523092 Validation Loss: 0.8985719084739685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2262: Training Loss: 0.5176786184310913 Validation Loss: 0.8985069990158081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2263: Training Loss: 0.5174633761247 Validation Loss: 0.8979736566543579\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2264: Training Loss: 0.5189191202322642 Validation Loss: 0.8978066444396973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2265: Training Loss: 0.5173637568950653 Validation Loss: 0.897195041179657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2266: Training Loss: 0.5167820453643799 Validation Loss: 0.8974823355674744\n",
      "Epoch 2267: Training Loss: 0.5169954597949982 Validation Loss: 0.8973972797393799\n",
      "Epoch 2268: Training Loss: 0.5162575046221415 Validation Loss: 0.897421658039093\n",
      "Epoch 2269: Training Loss: 0.5156114002068838 Validation Loss: 0.8970257043838501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2270: Training Loss: 0.5153548121452332 Validation Loss: 0.896891176700592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2271: Training Loss: 0.51789191365242 Validation Loss: 0.8967346549034119\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2272: Training Loss: 0.5153283576170603 Validation Loss: 0.8964889049530029\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2273: Training Loss: 0.5156785647074381 Validation Loss: 0.895876944065094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2274: Training Loss: 0.5143296519915262 Validation Loss: 0.8960009813308716\n",
      "Epoch 2275: Training Loss: 0.5138724048932394 Validation Loss: 0.8956635594367981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2276: Training Loss: 0.514715164899826 Validation Loss: 0.8957569599151611\n",
      "Epoch 2277: Training Loss: 0.5125490923722585 Validation Loss: 0.8952996134757996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2278: Training Loss: 0.5131446321805319 Validation Loss: 0.8952303528785706\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2279: Training Loss: 0.5140833755334219 Validation Loss: 0.8946412205696106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2280: Training Loss: 0.5129461189111074 Validation Loss: 0.8943238854408264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2281: Training Loss: 0.5144180456797282 Validation Loss: 0.8940900564193726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2282: Training Loss: 0.5121881365776062 Validation Loss: 0.8944263458251953\n",
      "Epoch 2283: Training Loss: 0.512367715438207 Validation Loss: 0.8940723538398743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2284: Training Loss: 0.5134326120217642 Validation Loss: 0.894543468952179\n",
      "Epoch 2285: Training Loss: 0.5113191604614258 Validation Loss: 0.8943479657173157\n",
      "Epoch 2286: Training Loss: 0.5107850233713785 Validation Loss: 0.8940055966377258\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2287: Training Loss: 0.5108285943667094 Validation Loss: 0.8934665322303772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2288: Training Loss: 0.5107828179995219 Validation Loss: 0.8926004767417908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2289: Training Loss: 0.5098522504170736 Validation Loss: 0.8920703530311584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2290: Training Loss: 0.5106167197227478 Validation Loss: 0.8918412327766418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2291: Training Loss: 0.5098176896572113 Validation Loss: 0.8917819261550903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2292: Training Loss: 0.5092941323916117 Validation Loss: 0.8917275071144104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2293: Training Loss: 0.5096201399962107 Validation Loss: 0.8917973637580872\n",
      "Epoch 2294: Training Loss: 0.5095934371153513 Validation Loss: 0.8919475674629211\n",
      "Epoch 2295: Training Loss: 0.5091619888941447 Validation Loss: 0.8918536901473999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2296: Training Loss: 0.5088260571161906 Validation Loss: 0.8915649652481079\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2297: Training Loss: 0.5083167751630148 Validation Loss: 0.8913692831993103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2298: Training Loss: 0.508608321348826 Validation Loss: 0.8913065791130066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2299: Training Loss: 0.5073570211728414 Validation Loss: 0.8910139799118042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2300: Training Loss: 0.507445752620697 Validation Loss: 0.8907366394996643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2301: Training Loss: 0.509476234515508 Validation Loss: 0.8899573683738708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2302: Training Loss: 0.5074692865212759 Validation Loss: 0.8901525735855103\n",
      "Epoch 2303: Training Loss: 0.5067510306835175 Validation Loss: 0.890106737613678\n",
      "Epoch 2304: Training Loss: 0.5058653553326925 Validation Loss: 0.8900319337844849\n",
      "Epoch 2305: Training Loss: 0.5074687401453654 Validation Loss: 0.8897751569747925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2306: Training Loss: 0.5067395766576132 Validation Loss: 0.8896451592445374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2307: Training Loss: 0.5060691833496094 Validation Loss: 0.8894007205963135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2308: Training Loss: 0.5050678551197052 Validation Loss: 0.8891915678977966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2309: Training Loss: 0.5051660637060801 Validation Loss: 0.8892006874084473\n",
      "Epoch 2310: Training Loss: 0.5042886237303416 Validation Loss: 0.8885266184806824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2311: Training Loss: 0.5046658515930176 Validation Loss: 0.8882854580879211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2312: Training Loss: 0.5045531590779623 Validation Loss: 0.8883470296859741\n",
      "Epoch 2313: Training Loss: 0.5036299626032511 Validation Loss: 0.8879356980323792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2314: Training Loss: 0.5032887160778046 Validation Loss: 0.8877257108688354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2315: Training Loss: 0.5030501584211985 Validation Loss: 0.8874521255493164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2316: Training Loss: 0.5027439594268799 Validation Loss: 0.8876311779022217\n",
      "Epoch 2317: Training Loss: 0.5034075876077017 Validation Loss: 0.8872616291046143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2318: Training Loss: 0.5023312270641327 Validation Loss: 0.8868593573570251\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2319: Training Loss: 0.5023625393708547 Validation Loss: 0.8864914774894714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2320: Training Loss: 0.5019619564215342 Validation Loss: 0.8867143988609314\n",
      "Epoch 2321: Training Loss: 0.5013299087683359 Validation Loss: 0.8864858746528625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2322: Training Loss: 0.5035747190316519 Validation Loss: 0.8864358067512512\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2323: Training Loss: 0.5017547508080801 Validation Loss: 0.8862455487251282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2324: Training Loss: 0.5008728702863058 Validation Loss: 0.8861508369445801\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2325: Training Loss: 0.5012118319670359 Validation Loss: 0.8856838941574097\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2326: Training Loss: 0.5001554787158966 Validation Loss: 0.8850719928741455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2327: Training Loss: 0.5004559755325317 Validation Loss: 0.8845482468605042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2328: Training Loss: 0.500355452299118 Validation Loss: 0.8844186067581177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2329: Training Loss: 0.5000137885411581 Validation Loss: 0.8846165537834167\n",
      "Epoch 2330: Training Loss: 0.49918654561042786 Validation Loss: 0.8847259283065796\n",
      "Epoch 2331: Training Loss: 0.4993140796820323 Validation Loss: 0.8846285939216614\n",
      "Epoch 2332: Training Loss: 0.49954953789711 Validation Loss: 0.884083092212677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2333: Training Loss: 0.498123566309611 Validation Loss: 0.883936882019043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2334: Training Loss: 0.498281588157018 Validation Loss: 0.8838924169540405\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2335: Training Loss: 0.49868810176849365 Validation Loss: 0.883678674697876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2336: Training Loss: 0.49859315156936646 Validation Loss: 0.8831849098205566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2337: Training Loss: 0.4978743294874827 Validation Loss: 0.8827081918716431\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2338: Training Loss: 0.4969650109608968 Validation Loss: 0.8828479051589966\n",
      "Epoch 2339: Training Loss: 0.4968186418215434 Validation Loss: 0.8829279541969299\n",
      "Epoch 2340: Training Loss: 0.49906355142593384 Validation Loss: 0.8831185698509216\n",
      "Epoch 2341: Training Loss: 0.4967244466145833 Validation Loss: 0.882675051689148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2342: Training Loss: 0.49724188446998596 Validation Loss: 0.882745623588562\n",
      "Epoch 2343: Training Loss: 0.4960462749004364 Validation Loss: 0.8823378086090088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2344: Training Loss: 0.495204637447993 Validation Loss: 0.8819390535354614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2345: Training Loss: 0.49527593453725177 Validation Loss: 0.8818583488464355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2346: Training Loss: 0.49600835641225177 Validation Loss: 0.8816375136375427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2347: Training Loss: 0.49540358781814575 Validation Loss: 0.8811274766921997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2348: Training Loss: 0.4943561752637227 Validation Loss: 0.8806551098823547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2349: Training Loss: 0.495743195215861 Validation Loss: 0.8806953430175781\n",
      "Epoch 2350: Training Loss: 0.49427550037701923 Validation Loss: 0.8807315826416016\n",
      "Epoch 2351: Training Loss: 0.49526678522427875 Validation Loss: 0.8807022571563721\n",
      "Epoch 2352: Training Loss: 0.49369954069455463 Validation Loss: 0.8803285360336304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2353: Training Loss: 0.4926243523756663 Validation Loss: 0.8801202178001404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2354: Training Loss: 0.4944329261779785 Validation Loss: 0.8803068399429321\n",
      "Epoch 2355: Training Loss: 0.4922865331172943 Validation Loss: 0.8799616098403931\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2356: Training Loss: 0.49182947476704914 Validation Loss: 0.8797217607498169\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2357: Training Loss: 0.49414334694544476 Validation Loss: 0.8793679475784302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2358: Training Loss: 0.49201327562332153 Validation Loss: 0.8786900043487549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2359: Training Loss: 0.49234376351038617 Validation Loss: 0.8785850405693054\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2360: Training Loss: 0.4915013511975606 Validation Loss: 0.8786945343017578\n",
      "Epoch 2361: Training Loss: 0.4924139181772868 Validation Loss: 0.8783133625984192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2362: Training Loss: 0.4910111427307129 Validation Loss: 0.878419816493988\n",
      "Epoch 2363: Training Loss: 0.4903666277726491 Validation Loss: 0.8785701990127563\n",
      "Epoch 2364: Training Loss: 0.49046623706817627 Validation Loss: 0.8782423734664917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2365: Training Loss: 0.4891677300135295 Validation Loss: 0.8780077695846558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2366: Training Loss: 0.49059436718622845 Validation Loss: 0.8781235218048096\n",
      "Epoch 2367: Training Loss: 0.49095354477564496 Validation Loss: 0.8780604600906372\n",
      "Epoch 2368: Training Loss: 0.4916667938232422 Validation Loss: 0.8777840733528137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2369: Training Loss: 0.4892474909623464 Validation Loss: 0.8771100044250488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2370: Training Loss: 0.4907932976881663 Validation Loss: 0.8767676949501038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2371: Training Loss: 0.4880217909812927 Validation Loss: 0.8767610788345337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2372: Training Loss: 0.4886772930622101 Validation Loss: 0.8764024972915649\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2373: Training Loss: 0.48787664373715717 Validation Loss: 0.8756582736968994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2374: Training Loss: 0.48827863732973736 Validation Loss: 0.8757773041725159\n",
      "Epoch 2375: Training Loss: 0.48821836709976196 Validation Loss: 0.8760014772415161\n",
      "Epoch 2376: Training Loss: 0.4874177972475688 Validation Loss: 0.8759692311286926\n",
      "Epoch 2377: Training Loss: 0.4871753454208374 Validation Loss: 0.8758479952812195\n",
      "Epoch 2378: Training Loss: 0.4892345865567525 Validation Loss: 0.8755041360855103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2379: Training Loss: 0.48652775088946026 Validation Loss: 0.8749492168426514\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2380: Training Loss: 0.48696479201316833 Validation Loss: 0.8744561076164246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2381: Training Loss: 0.4857064684232076 Validation Loss: 0.8740795850753784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 2382: Training Loss: 0.4868324895699819 Validation Loss: 0.8742228746414185\n",
      "Epoch 2383: Training Loss: 0.48635411262512207 Validation Loss: 0.874181866645813\n",
      "Epoch 2384: Training Loss: 0.48461322983105976 Validation Loss: 0.8742674589157104\n",
      "Epoch 2385: Training Loss: 0.48512381315231323 Validation Loss: 0.8740277886390686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2386: Training Loss: 0.4844138026237488 Validation Loss: 0.8744377493858337\n",
      "Epoch 2387: Training Loss: 0.4861387113730113 Validation Loss: 0.8741443157196045\n",
      "Epoch 2388: Training Loss: 0.48514697949091595 Validation Loss: 0.8739219903945923\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2389: Training Loss: 0.4834741950035095 Validation Loss: 0.873923659324646\n",
      "Epoch 2390: Training Loss: 0.4839029212792714 Validation Loss: 0.8731728196144104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2391: Training Loss: 0.48508744438489276 Validation Loss: 0.8728827834129333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2392: Training Loss: 0.48320189118385315 Validation Loss: 0.8722139000892639\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2393: Training Loss: 0.4834934373696645 Validation Loss: 0.8720918893814087\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2394: Training Loss: 0.48229188720385235 Validation Loss: 0.8722830414772034\n",
      "Epoch 2395: Training Loss: 0.48543843626976013 Validation Loss: 0.8719820976257324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2396: Training Loss: 0.4820922116438548 Validation Loss: 0.8719977736473083\n",
      "Epoch 2397: Training Loss: 0.48201759656270343 Validation Loss: 0.8718926310539246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2398: Training Loss: 0.48262197772661847 Validation Loss: 0.8715062141418457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2399: Training Loss: 0.4842118521531423 Validation Loss: 0.8715512752532959\n",
      "Epoch 2400: Training Loss: 0.481678585211436 Validation Loss: 0.8715232014656067\n",
      "Epoch 2401: Training Loss: 0.48116253813107807 Validation Loss: 0.8716394901275635\n",
      "Epoch 2402: Training Loss: 0.4805515507857005 Validation Loss: 0.8706980347633362\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2403: Training Loss: 0.4803272287050883 Validation Loss: 0.8701290488243103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2404: Training Loss: 0.4808901349703471 Validation Loss: 0.8706165552139282\n",
      "Epoch 2405: Training Loss: 0.4805174072583516 Validation Loss: 0.870460033416748\n",
      "Epoch 2406: Training Loss: 0.4796201487382253 Validation Loss: 0.8709481954574585\n",
      "Epoch 2407: Training Loss: 0.48063069581985474 Validation Loss: 0.8708946704864502\n",
      "Epoch 2408: Training Loss: 0.4795573552449544 Validation Loss: 0.8703182935714722\n",
      "Epoch 2409: Training Loss: 0.4817521969477336 Validation Loss: 0.8696322441101074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2410: Training Loss: 0.478889803091685 Validation Loss: 0.8688782453536987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2411: Training Loss: 0.4793884754180908 Validation Loss: 0.8688340783119202\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2412: Training Loss: 0.4785963197549184 Validation Loss: 0.8688757419586182\n",
      "Epoch 2413: Training Loss: 0.47696979840596515 Validation Loss: 0.8685857057571411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2414: Training Loss: 0.47771838307380676 Validation Loss: 0.8685147166252136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2415: Training Loss: 0.4783448576927185 Validation Loss: 0.8683124780654907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2416: Training Loss: 0.4791853427886963 Validation Loss: 0.8682974576950073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2417: Training Loss: 0.47732457518577576 Validation Loss: 0.8685070276260376\n",
      "Epoch 2418: Training Loss: 0.47719013690948486 Validation Loss: 0.8684903383255005\n",
      "Epoch 2419: Training Loss: 0.4768465956052144 Validation Loss: 0.8682897090911865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2420: Training Loss: 0.47632018725077313 Validation Loss: 0.867705225944519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2421: Training Loss: 0.4761883119742076 Validation Loss: 0.8673670291900635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2422: Training Loss: 0.4759700397650401 Validation Loss: 0.8672480583190918\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2423: Training Loss: 0.4758607745170593 Validation Loss: 0.8669856190681458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2424: Training Loss: 0.4754090706507365 Validation Loss: 0.8668072819709778\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2425: Training Loss: 0.47535011172294617 Validation Loss: 0.8665761351585388\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2426: Training Loss: 0.4749399522940318 Validation Loss: 0.8669143319129944\n",
      "Epoch 2427: Training Loss: 0.47567935784657794 Validation Loss: 0.8669416904449463\n",
      "Epoch 2428: Training Loss: 0.47464292248090106 Validation Loss: 0.8665686845779419\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2429: Training Loss: 0.47463540236155194 Validation Loss: 0.8658204078674316\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2430: Training Loss: 0.47467754284540814 Validation Loss: 0.8657665848731995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2431: Training Loss: 0.47400058309237164 Validation Loss: 0.8655713200569153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2432: Training Loss: 0.4736178418000539 Validation Loss: 0.865398645401001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2433: Training Loss: 0.47382163008054096 Validation Loss: 0.8656929135322571\n",
      "Epoch 2434: Training Loss: 0.4734233319759369 Validation Loss: 0.8659437894821167\n",
      "Epoch 2435: Training Loss: 0.47246354818344116 Validation Loss: 0.8658005595207214\n",
      "Epoch 2436: Training Loss: 0.4728480478127797 Validation Loss: 0.8649100661277771\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2437: Training Loss: 0.4731622338294983 Validation Loss: 0.8639999032020569\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2438: Training Loss: 0.4722641110420227 Validation Loss: 0.863731861114502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2439: Training Loss: 0.47168534994125366 Validation Loss: 0.863868236541748\n",
      "Epoch 2440: Training Loss: 0.471155325571696 Validation Loss: 0.8639015555381775\n",
      "Epoch 2441: Training Loss: 0.47292427221934 Validation Loss: 0.864092230796814\n",
      "Epoch 2442: Training Loss: 0.47100602587064105 Validation Loss: 0.8638757467269897\n",
      "Epoch 2443: Training Loss: 0.4708249866962433 Validation Loss: 0.8637453317642212\n",
      "Epoch 2444: Training Loss: 0.47048016389211017 Validation Loss: 0.8635278940200806\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2445: Training Loss: 0.4705435534318288 Validation Loss: 0.8635826110839844\n",
      "Epoch 2446: Training Loss: 0.4716998239358266 Validation Loss: 0.8638107180595398\n",
      "Epoch 2447: Training Loss: 0.4708085854848226 Validation Loss: 0.8636061549186707\n",
      "Epoch 2448: Training Loss: 0.470895379781723 Validation Loss: 0.8628288507461548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2449: Training Loss: 0.46906279524167377 Validation Loss: 0.8623518943786621\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2450: Training Loss: 0.4694199860095978 Validation Loss: 0.8624335527420044\n",
      "Epoch 2451: Training Loss: 0.4684874216715495 Validation Loss: 0.8619731068611145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2452: Training Loss: 0.4687078694502513 Validation Loss: 0.8618208169937134\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2453: Training Loss: 0.4687364399433136 Validation Loss: 0.8621156215667725\n",
      "Epoch 2454: Training Loss: 0.4682493011156718 Validation Loss: 0.862064778804779\n",
      "Epoch 2455: Training Loss: 0.46904786427815753 Validation Loss: 0.8622018098831177\n",
      "Epoch 2456: Training Loss: 0.46784864862759906 Validation Loss: 0.8616487979888916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2457: Training Loss: 0.46746466557184857 Validation Loss: 0.8614986538887024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2458: Training Loss: 0.4675669272740682 Validation Loss: 0.8611143231391907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2459: Training Loss: 0.46778061985969543 Validation Loss: 0.860558032989502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2460: Training Loss: 0.4671267469724019 Validation Loss: 0.8603435754776001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2461: Training Loss: 0.4647681613763173 Validation Loss: 0.8603697419166565\n",
      "Epoch 2462: Training Loss: 0.4661269982655843 Validation Loss: 0.860398530960083\n",
      "Epoch 2463: Training Loss: 0.4678625166416168 Validation Loss: 0.8602162599563599\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2464: Training Loss: 0.46535587310791016 Validation Loss: 0.8600486516952515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2465: Training Loss: 0.4649096330006917 Validation Loss: 0.8601659536361694\n",
      "Epoch 2466: Training Loss: 0.4658893247445424 Validation Loss: 0.860089123249054\n",
      "Epoch 2467: Training Loss: 0.4646270275115967 Validation Loss: 0.8599174618721008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2468: Training Loss: 0.4656473199526469 Validation Loss: 0.8595557808876038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2469: Training Loss: 0.4674842556317647 Validation Loss: 0.8591880798339844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2470: Training Loss: 0.4649502734343211 Validation Loss: 0.8593482971191406\n",
      "Epoch 2471: Training Loss: 0.46468161543210346 Validation Loss: 0.8589840531349182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2472: Training Loss: 0.4641662836074829 Validation Loss: 0.8587408661842346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2473: Training Loss: 0.46473286549250287 Validation Loss: 0.8584913611412048\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2474: Training Loss: 0.46472833553949994 Validation Loss: 0.8585432767868042\n",
      "Epoch 2475: Training Loss: 0.46364789207776386 Validation Loss: 0.8583024144172668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2476: Training Loss: 0.46338798602422077 Validation Loss: 0.858095645904541\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2477: Training Loss: 0.4638476570447286 Validation Loss: 0.8577947616577148\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2478: Training Loss: 0.46276620030403137 Validation Loss: 0.8576153516769409\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2479: Training Loss: 0.4639686445395152 Validation Loss: 0.8574555516242981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2480: Training Loss: 0.4622445007165273 Validation Loss: 0.8571643233299255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2481: Training Loss: 0.46418126424153644 Validation Loss: 0.8567003011703491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2482: Training Loss: 0.46119679013888043 Validation Loss: 0.8569653630256653\n",
      "Epoch 2483: Training Loss: 0.4615372021993001 Validation Loss: 0.8571247458457947\n",
      "Epoch 2484: Training Loss: 0.46121161182721454 Validation Loss: 0.8568705916404724\n",
      "Epoch 2485: Training Loss: 0.46091632048288983 Validation Loss: 0.8565704822540283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2486: Training Loss: 0.460978239774704 Validation Loss: 0.8563511967658997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2487: Training Loss: 0.46227001150449115 Validation Loss: 0.8563591837882996\n",
      "Epoch 2488: Training Loss: 0.4602524936199188 Validation Loss: 0.8564940094947815\n",
      "Epoch 2489: Training Loss: 0.4596673647562663 Validation Loss: 0.8566541075706482\n",
      "Epoch 2490: Training Loss: 0.4603748619556427 Validation Loss: 0.856368899345398\n",
      "Epoch 2491: Training Loss: 0.45937274893124896 Validation Loss: 0.8562368750572205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2492: Training Loss: 0.45977232853571576 Validation Loss: 0.8555815815925598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2493: Training Loss: 0.4600422183672587 Validation Loss: 0.8550420999526978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2494: Training Loss: 0.458391805489858 Validation Loss: 0.855037271976471\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2495: Training Loss: 0.45858238140741986 Validation Loss: 0.8548214435577393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2496: Training Loss: 0.4588542381922404 Validation Loss: 0.8544115424156189\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2497: Training Loss: 0.4591805438200633 Validation Loss: 0.8539994359016418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2498: Training Loss: 0.45987815658251446 Validation Loss: 0.8539993762969971\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2499: Training Loss: 0.45832380652427673 Validation Loss: 0.854159414768219\n",
      "Epoch 2500: Training Loss: 0.4580622414747874 Validation Loss: 0.8544971942901611\n",
      "Epoch 2501: Training Loss: 0.45665029684702557 Validation Loss: 0.8544149398803711\n",
      "Epoch 2502: Training Loss: 0.4583393633365631 Validation Loss: 0.8543009757995605\n",
      "Epoch 2503: Training Loss: 0.45948825279871625 Validation Loss: 0.8536137342453003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2504: Training Loss: 0.45701200763384503 Validation Loss: 0.8533511161804199\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2505: Training Loss: 0.45579369862874347 Validation Loss: 0.8532025218009949\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2506: Training Loss: 0.4569069743156433 Validation Loss: 0.8534054160118103\n",
      "Epoch 2507: Training Loss: 0.45568005243937176 Validation Loss: 0.853177547454834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2508: Training Loss: 0.45552844802538556 Validation Loss: 0.8530390858650208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2509: Training Loss: 0.45571380853652954 Validation Loss: 0.852637767791748\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2510: Training Loss: 0.45611798763275146 Validation Loss: 0.852467954158783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2511: Training Loss: 0.4563681681950887 Validation Loss: 0.8522072434425354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2512: Training Loss: 0.4558471043904622 Validation Loss: 0.8520557880401611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2513: Training Loss: 0.4570116599400838 Validation Loss: 0.8522508144378662\n",
      "Epoch 2514: Training Loss: 0.4546689490477244 Validation Loss: 0.8517227172851562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2515: Training Loss: 0.45474714040756226 Validation Loss: 0.8518365621566772\n",
      "Epoch 2516: Training Loss: 0.4529884358247121 Validation Loss: 0.8514946103096008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2517: Training Loss: 0.4540157417456309 Validation Loss: 0.851418137550354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2518: Training Loss: 0.45421309272448224 Validation Loss: 0.8511224389076233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2519: Training Loss: 0.4534663458665212 Validation Loss: 0.8507489562034607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2520: Training Loss: 0.4542014499505361 Validation Loss: 0.8505135178565979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2521: Training Loss: 0.45408833026885986 Validation Loss: 0.8505850434303284\n",
      "Epoch 2522: Training Loss: 0.45255135496457416 Validation Loss: 0.85067218542099\n",
      "Epoch 2523: Training Loss: 0.4525860846042633 Validation Loss: 0.850123941898346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2524: Training Loss: 0.4562629163265228 Validation Loss: 0.8498384952545166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2525: Training Loss: 0.453250120083491 Validation Loss: 0.849841833114624\n",
      "Epoch 2526: Training Loss: 0.45208882292111713 Validation Loss: 0.8498830795288086\n",
      "Epoch 2527: Training Loss: 0.45476556817690533 Validation Loss: 0.8496355414390564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2528: Training Loss: 0.4517154296239217 Validation Loss: 0.8499874472618103\n",
      "Epoch 2529: Training Loss: 0.45257338881492615 Validation Loss: 0.8501008152961731\n",
      "Epoch 2530: Training Loss: 0.4511020878950755 Validation Loss: 0.849804699420929\n",
      "Epoch 2531: Training Loss: 0.4503933588663737 Validation Loss: 0.8496045470237732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2532: Training Loss: 0.4521900713443756 Validation Loss: 0.8489448428153992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2533: Training Loss: 0.45025260249773663 Validation Loss: 0.8484240770339966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2534: Training Loss: 0.4522050420443217 Validation Loss: 0.8483361005783081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2535: Training Loss: 0.4500807424386342 Validation Loss: 0.848173975944519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2536: Training Loss: 0.44936535755793255 Validation Loss: 0.8484671711921692\n",
      "Epoch 2537: Training Loss: 0.4500019351641337 Validation Loss: 0.8485008478164673\n",
      "Epoch 2538: Training Loss: 0.44864033659299213 Validation Loss: 0.8483555316925049\n",
      "Epoch 2539: Training Loss: 0.4487852652867635 Validation Loss: 0.8482455611228943\n",
      "Epoch 2540: Training Loss: 0.4499686161677043 Validation Loss: 0.8483145833015442\n",
      "Epoch 2541: Training Loss: 0.44835391640663147 Validation Loss: 0.8479653000831604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2542: Training Loss: 0.4475962022940318 Validation Loss: 0.8472186326980591\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2543: Training Loss: 0.44921472668647766 Validation Loss: 0.8469172716140747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2544: Training Loss: 0.4474589725335439 Validation Loss: 0.8468586206436157\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2545: Training Loss: 0.4473266402880351 Validation Loss: 0.8469549417495728\n",
      "Epoch 2546: Training Loss: 0.4479232132434845 Validation Loss: 0.8467184901237488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2547: Training Loss: 0.44784518082936603 Validation Loss: 0.8464758992195129\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2548: Training Loss: 0.4464217225710551 Validation Loss: 0.8465306758880615\n",
      "Epoch 2549: Training Loss: 0.4462626576423645 Validation Loss: 0.8464788794517517\n",
      "Epoch 2550: Training Loss: 0.44735946257909137 Validation Loss: 0.8463769555091858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2551: Training Loss: 0.44675412774086 Validation Loss: 0.8463798761367798\n",
      "Epoch 2552: Training Loss: 0.44619931777318317 Validation Loss: 0.8457147479057312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2553: Training Loss: 0.4464097321033478 Validation Loss: 0.8452065587043762\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2554: Training Loss: 0.4459005395571391 Validation Loss: 0.8456931710243225\n",
      "Epoch 2555: Training Loss: 0.4451127847035726 Validation Loss: 0.8451857566833496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2556: Training Loss: 0.4448833068211873 Validation Loss: 0.8452602624893188\n",
      "Epoch 2557: Training Loss: 0.44562849402427673 Validation Loss: 0.8451441526412964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2558: Training Loss: 0.4448908865451813 Validation Loss: 0.845261812210083\n",
      "Epoch 2559: Training Loss: 0.4446566700935364 Validation Loss: 0.844810426235199\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2560: Training Loss: 0.444457471370697 Validation Loss: 0.8444047570228577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2561: Training Loss: 0.44441163539886475 Validation Loss: 0.843925952911377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2562: Training Loss: 0.44397274653116864 Validation Loss: 0.8440513610839844\n",
      "Epoch 2563: Training Loss: 0.44402430454889935 Validation Loss: 0.843905508518219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2564: Training Loss: 0.44356854756673175 Validation Loss: 0.8438124656677246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2565: Training Loss: 0.443886399269104 Validation Loss: 0.8442749381065369\n",
      "Epoch 2566: Training Loss: 0.44369028011957806 Validation Loss: 0.8444704413414001\n",
      "Epoch 2567: Training Loss: 0.4428640405337016 Validation Loss: 0.8439947962760925\n",
      "Epoch 2568: Training Loss: 0.44373538096745807 Validation Loss: 0.843434751033783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2569: Training Loss: 0.4428887963294983 Validation Loss: 0.8437290787696838\n",
      "Epoch 2570: Training Loss: 0.4436752994855245 Validation Loss: 0.843284547328949\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2571: Training Loss: 0.44244513909022015 Validation Loss: 0.8431578874588013\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2572: Training Loss: 0.441867192586263 Validation Loss: 0.8425276279449463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2573: Training Loss: 0.4437854786713918 Validation Loss: 0.8422307372093201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2574: Training Loss: 0.4426039159297943 Validation Loss: 0.8421508073806763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2575: Training Loss: 0.4411630431811015 Validation Loss: 0.8426105380058289\n",
      "Epoch 2576: Training Loss: 0.44106339414914447 Validation Loss: 0.8423469662666321\n",
      "Epoch 2577: Training Loss: 0.44102852543195087 Validation Loss: 0.8423402309417725\n",
      "Epoch 2578: Training Loss: 0.4406937559445699 Validation Loss: 0.8417385816574097\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2579: Training Loss: 0.4409729838371277 Validation Loss: 0.8414881229400635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2580: Training Loss: 0.440514475107193 Validation Loss: 0.8410208821296692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2581: Training Loss: 0.4399356146653493 Validation Loss: 0.840923547744751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2582: Training Loss: 0.4405299524466197 Validation Loss: 0.8412510752677917\n",
      "Epoch 2583: Training Loss: 0.4403490424156189 Validation Loss: 0.8413617610931396\n",
      "Epoch 2584: Training Loss: 0.4424755970637004 Validation Loss: 0.8417218327522278\n",
      "Epoch 2585: Training Loss: 0.43923579653104144 Validation Loss: 0.8414087295532227\n",
      "Epoch 2586: Training Loss: 0.4404519299666087 Validation Loss: 0.8410229086875916\n",
      "Epoch 2587: Training Loss: 0.4389308492342631 Validation Loss: 0.840595006942749\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2588: Training Loss: 0.4403863350550334 Validation Loss: 0.8402836918830872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2589: Training Loss: 0.43852920333544415 Validation Loss: 0.8404828310012817\n",
      "Epoch 2590: Training Loss: 0.43864481647809345 Validation Loss: 0.8403892517089844\n",
      "Epoch 2591: Training Loss: 0.4378463824590047 Validation Loss: 0.8396971821784973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2592: Training Loss: 0.43807319800059 Validation Loss: 0.8397804498672485\n",
      "Epoch 2593: Training Loss: 0.4374457597732544 Validation Loss: 0.8399664163589478\n",
      "Epoch 2594: Training Loss: 0.4389368991057078 Validation Loss: 0.8398808836936951\n",
      "Epoch 2595: Training Loss: 0.4367166856924693 Validation Loss: 0.8399460911750793\n",
      "Epoch 2596: Training Loss: 0.4363108277320862 Validation Loss: 0.839714765548706\n",
      "Epoch 2597: Training Loss: 0.4360574285189311 Validation Loss: 0.8394843935966492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2598: Training Loss: 0.43655234575271606 Validation Loss: 0.8390401601791382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2599: Training Loss: 0.4369971851507823 Validation Loss: 0.8388285636901855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2600: Training Loss: 0.43526854117711383 Validation Loss: 0.8384610414505005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2601: Training Loss: 0.43543843428293866 Validation Loss: 0.8382014632225037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2602: Training Loss: 0.43776028354962665 Validation Loss: 0.8381993770599365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2603: Training Loss: 0.4352923234303792 Validation Loss: 0.8382445573806763\n",
      "Epoch 2604: Training Loss: 0.4346538484096527 Validation Loss: 0.8379397392272949\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2605: Training Loss: 0.43628527720769245 Validation Loss: 0.8380662798881531\n",
      "Epoch 2606: Training Loss: 0.43547526995340985 Validation Loss: 0.8377222418785095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2607: Training Loss: 0.4348745246728261 Validation Loss: 0.8377281427383423\n",
      "Epoch 2608: Training Loss: 0.434213787317276 Validation Loss: 0.8377212882041931\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2609: Training Loss: 0.4346434970696767 Validation Loss: 0.8372365832328796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2610: Training Loss: 0.43361876408259076 Validation Loss: 0.8373970985412598\n",
      "Epoch 2611: Training Loss: 0.4341117242972056 Validation Loss: 0.8370781540870667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2612: Training Loss: 0.4338800410429637 Validation Loss: 0.8362686634063721\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2613: Training Loss: 0.43479259808858234 Validation Loss: 0.8364278674125671\n",
      "Epoch 2614: Training Loss: 0.4336761732896169 Validation Loss: 0.8362423777580261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2615: Training Loss: 0.43310479323069256 Validation Loss: 0.836611270904541\n",
      "Epoch 2616: Training Loss: 0.43347009023030597 Validation Loss: 0.8364989757537842\n",
      "Epoch 2617: Training Loss: 0.4323226710160573 Validation Loss: 0.8365169763565063\n",
      "Epoch 2618: Training Loss: 0.4320359031359355 Validation Loss: 0.8362802267074585\n",
      "Epoch 2619: Training Loss: 0.4325433075428009 Validation Loss: 0.8361660242080688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2620: Training Loss: 0.4319025178750356 Validation Loss: 0.8363912105560303\n",
      "Epoch 2621: Training Loss: 0.4315994481245677 Validation Loss: 0.8364051580429077\n",
      "Epoch 2622: Training Loss: 0.4322390059630076 Validation Loss: 0.8360887169837952\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2623: Training Loss: 0.43190930287043255 Validation Loss: 0.8354715704917908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2624: Training Loss: 0.4320438504219055 Validation Loss: 0.834946870803833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2625: Training Loss: 0.4310547312100728 Validation Loss: 0.8346058130264282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2626: Training Loss: 0.43089431524276733 Validation Loss: 0.8341511487960815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2627: Training Loss: 0.4308462639649709 Validation Loss: 0.8341889977455139\n",
      "Epoch 2628: Training Loss: 0.4309436281522115 Validation Loss: 0.8342719078063965\n",
      "Epoch 2629: Training Loss: 0.42995216449101764 Validation Loss: 0.8340335488319397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2630: Training Loss: 0.42930007974306744 Validation Loss: 0.8342782855033875\n",
      "Epoch 2631: Training Loss: 0.4298434555530548 Validation Loss: 0.833962619304657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2632: Training Loss: 0.43072496851285297 Validation Loss: 0.8340702652931213\n",
      "Epoch 2633: Training Loss: 0.43013376990954083 Validation Loss: 0.8346080780029297\n",
      "Epoch 2634: Training Loss: 0.4299305280049642 Validation Loss: 0.8347132205963135\n",
      "Epoch 2635: Training Loss: 0.42985589305559796 Validation Loss: 0.8343419432640076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2636: Training Loss: 0.4287956853707631 Validation Loss: 0.8339064717292786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2637: Training Loss: 0.43029965957005817 Validation Loss: 0.8335883617401123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2638: Training Loss: 0.4290217161178589 Validation Loss: 0.8333054780960083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2639: Training Loss: 0.4292437831560771 Validation Loss: 0.8329510688781738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2640: Training Loss: 0.4274437924226125 Validation Loss: 0.8326356410980225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2641: Training Loss: 0.4285396337509155 Validation Loss: 0.8322250843048096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2642: Training Loss: 0.42789192994435626 Validation Loss: 0.8324271440505981\n",
      "Epoch 2643: Training Loss: 0.42723151048024494 Validation Loss: 0.832529604434967\n",
      "Epoch 2644: Training Loss: 0.4275392194588979 Validation Loss: 0.8325554132461548\n",
      "Epoch 2645: Training Loss: 0.42746493220329285 Validation Loss: 0.8318808674812317\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2646: Training Loss: 0.4271547595659892 Validation Loss: 0.8317484855651855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2647: Training Loss: 0.4256396194299062 Validation Loss: 0.8318279385566711\n",
      "Epoch 2648: Training Loss: 0.4283724327882131 Validation Loss: 0.831588864326477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2649: Training Loss: 0.426459143559138 Validation Loss: 0.831703782081604\n",
      "Epoch 2650: Training Loss: 0.4250262876351674 Validation Loss: 0.8317647576332092\n",
      "Epoch 2651: Training Loss: 0.42552193999290466 Validation Loss: 0.8318157196044922\n",
      "Epoch 2652: Training Loss: 0.4259548485279083 Validation Loss: 0.832000195980072\n",
      "Epoch 2653: Training Loss: 0.42546623945236206 Validation Loss: 0.8316237330436707\n",
      "Epoch 2654: Training Loss: 0.42442166805267334 Validation Loss: 0.8313314318656921\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2655: Training Loss: 0.42462246616681415 Validation Loss: 0.8306781053543091\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2656: Training Loss: 0.42425021529197693 Validation Loss: 0.8305084109306335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2657: Training Loss: 0.4256822466850281 Validation Loss: 0.8300142288208008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2658: Training Loss: 0.42397943139076233 Validation Loss: 0.8297109603881836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2659: Training Loss: 0.42420245210329693 Validation Loss: 0.8300687670707703\n",
      "Epoch 2660: Training Loss: 0.4241546591122945 Validation Loss: 0.830122709274292\n",
      "Epoch 2661: Training Loss: 0.4234618842601776 Validation Loss: 0.8306702971458435\n",
      "Epoch 2662: Training Loss: 0.4235953688621521 Validation Loss: 0.8301629424095154\n",
      "Epoch 2663: Training Loss: 0.42799071470896405 Validation Loss: 0.8295735716819763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2664: Training Loss: 0.42291266719500226 Validation Loss: 0.8295329213142395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2665: Training Loss: 0.42384769519170123 Validation Loss: 0.8292273283004761\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2666: Training Loss: 0.42250093817710876 Validation Loss: 0.8292610049247742\n",
      "Epoch 2667: Training Loss: 0.42230135202407837 Validation Loss: 0.8294717669487\n",
      "Epoch 2668: Training Loss: 0.4230353037516276 Validation Loss: 0.8300545811653137\n",
      "Epoch 2669: Training Loss: 0.4223286807537079 Validation Loss: 0.8297683596611023\n",
      "Epoch 2670: Training Loss: 0.4227181077003479 Validation Loss: 0.8300493955612183\n",
      "Epoch 2671: Training Loss: 0.4220816691716512 Validation Loss: 0.8291979432106018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2672: Training Loss: 0.42148839433987934 Validation Loss: 0.8285971879959106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2673: Training Loss: 0.4217549165089925 Validation Loss: 0.8282017111778259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2674: Training Loss: 0.421021709839503 Validation Loss: 0.8275904655456543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2675: Training Loss: 0.4209539592266083 Validation Loss: 0.8274138569831848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2676: Training Loss: 0.42031724254290265 Validation Loss: 0.8273763656616211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2677: Training Loss: 0.4203237295150757 Validation Loss: 0.8280251026153564\n",
      "Epoch 2678: Training Loss: 0.4206913510958354 Validation Loss: 0.8276159763336182\n",
      "Epoch 2679: Training Loss: 0.4203523298104604 Validation Loss: 0.827666163444519\n",
      "Epoch 2680: Training Loss: 0.4204856554667155 Validation Loss: 0.8276308178901672\n",
      "Epoch 2681: Training Loss: 0.42100146412849426 Validation Loss: 0.8274604678153992\n",
      "Epoch 2682: Training Loss: 0.4202147920926412 Validation Loss: 0.8274195194244385\n",
      "Epoch 2683: Training Loss: 0.4200507303078969 Validation Loss: 0.8271856904029846\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2684: Training Loss: 0.418977012236913 Validation Loss: 0.8269737362861633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2685: Training Loss: 0.4188608427842458 Validation Loss: 0.8267289996147156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2686: Training Loss: 0.4184456467628479 Validation Loss: 0.8268302083015442\n",
      "Epoch 2687: Training Loss: 0.41837536295255023 Validation Loss: 0.8264199495315552\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2688: Training Loss: 0.4182838002840678 Validation Loss: 0.8257748484611511\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2689: Training Loss: 0.4178936978181203 Validation Loss: 0.8257516622543335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2690: Training Loss: 0.4185396730899811 Validation Loss: 0.8261756300926208\n",
      "Epoch 2691: Training Loss: 0.41789154211680096 Validation Loss: 0.8257759213447571\n",
      "Epoch 2692: Training Loss: 0.4184466103712718 Validation Loss: 0.8255515098571777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2693: Training Loss: 0.417641947666804 Validation Loss: 0.8253680467605591\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2694: Training Loss: 0.41707226634025574 Validation Loss: 0.8254432678222656\n",
      "Epoch 2695: Training Loss: 0.4170024295647939 Validation Loss: 0.8260719776153564\n",
      "Epoch 2696: Training Loss: 0.4167335132757823 Validation Loss: 0.8257721662521362\n",
      "Epoch 2697: Training Loss: 0.41617444157600403 Validation Loss: 0.8257883787155151\n",
      "Epoch 2698: Training Loss: 0.4168582856655121 Validation Loss: 0.8254356980323792\n",
      "Epoch 2699: Training Loss: 0.41598236560821533 Validation Loss: 0.8247303366661072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2700: Training Loss: 0.41657960414886475 Validation Loss: 0.8245140910148621\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2701: Training Loss: 0.4165765345096588 Validation Loss: 0.8242018818855286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2702: Training Loss: 0.41522446274757385 Validation Loss: 0.8241721987724304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2703: Training Loss: 0.4151056508223216 Validation Loss: 0.8240742683410645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2704: Training Loss: 0.41558362046877545 Validation Loss: 0.8238992094993591\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2705: Training Loss: 0.41471168398857117 Validation Loss: 0.82417893409729\n",
      "Epoch 2706: Training Loss: 0.4148389895757039 Validation Loss: 0.8240911364555359\n",
      "Epoch 2707: Training Loss: 0.4140346149603526 Validation Loss: 0.8237161040306091\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2708: Training Loss: 0.41417107979456586 Validation Loss: 0.8235839605331421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2709: Training Loss: 0.4159019887447357 Validation Loss: 0.8235355019569397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2710: Training Loss: 0.41578052441279095 Validation Loss: 0.82328861951828\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2711: Training Loss: 0.41504953304926556 Validation Loss: 0.8230664134025574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2712: Training Loss: 0.41327977180480957 Validation Loss: 0.8233740925788879\n",
      "Epoch 2713: Training Loss: 0.4137694636980693 Validation Loss: 0.8235417604446411\n",
      "Epoch 2714: Training Loss: 0.4134816328684489 Validation Loss: 0.8231632113456726\n",
      "Epoch 2715: Training Loss: 0.413237988948822 Validation Loss: 0.8233007192611694\n",
      "Epoch 2716: Training Loss: 0.41293444236119586 Validation Loss: 0.8231354355812073\n",
      "Epoch 2717: Training Loss: 0.41232191522916156 Validation Loss: 0.8224987983703613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2718: Training Loss: 0.41242512067159015 Validation Loss: 0.8220493197441101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2719: Training Loss: 0.412583847840627 Validation Loss: 0.8218138217926025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2720: Training Loss: 0.41214234630266827 Validation Loss: 0.82254558801651\n",
      "Epoch 2721: Training Loss: 0.41236646970113117 Validation Loss: 0.822383463382721\n",
      "Epoch 2722: Training Loss: 0.4121667742729187 Validation Loss: 0.8226962685585022\n",
      "Epoch 2723: Training Loss: 0.4127652545770009 Validation Loss: 0.8223342895507812\n",
      "Epoch 2724: Training Loss: 0.41114450494448346 Validation Loss: 0.8220147490501404\n",
      "Epoch 2725: Training Loss: 0.41200260321299237 Validation Loss: 0.8215809464454651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2726: Training Loss: 0.41108547647794086 Validation Loss: 0.8210185170173645\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2727: Training Loss: 0.41201366980870563 Validation Loss: 0.8209500312805176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2728: Training Loss: 0.4102670152982076 Validation Loss: 0.8212820291519165\n",
      "Epoch 2729: Training Loss: 0.4129954973856608 Validation Loss: 0.8218786716461182\n",
      "Epoch 2730: Training Loss: 0.4110118051369985 Validation Loss: 0.8213302493095398\n",
      "Epoch 2731: Training Loss: 0.41010184089342755 Validation Loss: 0.8211697340011597\n",
      "Epoch 2732: Training Loss: 0.4100741446018219 Validation Loss: 0.8210091590881348\n",
      "Epoch 2733: Training Loss: 0.40987492601076764 Validation Loss: 0.8207670450210571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2734: Training Loss: 0.4121820131937663 Validation Loss: 0.8204914331436157\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2735: Training Loss: 0.4098784526189168 Validation Loss: 0.8201317191123962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2736: Training Loss: 0.40987735986709595 Validation Loss: 0.8200966715812683\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2737: Training Loss: 0.4086787899335225 Validation Loss: 0.8197293281555176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2738: Training Loss: 0.4087807734807332 Validation Loss: 0.819484531879425\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2739: Training Loss: 0.40876541535059613 Validation Loss: 0.8196007609367371\n",
      "Epoch 2740: Training Loss: 0.4079150954882304 Validation Loss: 0.8195357322692871\n",
      "Epoch 2741: Training Loss: 0.4082918067773183 Validation Loss: 0.8196243643760681\n",
      "Epoch 2742: Training Loss: 0.4079062342643738 Validation Loss: 0.8194562792778015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2743: Training Loss: 0.4079156319300334 Validation Loss: 0.8189041018486023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2744: Training Loss: 0.40787793199221295 Validation Loss: 0.818603515625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2745: Training Loss: 0.40759580334027606 Validation Loss: 0.8186497092247009\n",
      "Epoch 2746: Training Loss: 0.4102395872275035 Validation Loss: 0.8189070820808411\n",
      "Epoch 2747: Training Loss: 0.40693018833796185 Validation Loss: 0.8192585706710815\n",
      "Epoch 2748: Training Loss: 0.4090183476607005 Validation Loss: 0.8188880085945129\n",
      "Epoch 2749: Training Loss: 0.40714144706726074 Validation Loss: 0.8186834454536438\n",
      "Epoch 2750: Training Loss: 0.4067708949247996 Validation Loss: 0.8183391094207764\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2751: Training Loss: 0.40666558345158893 Validation Loss: 0.8180039525032043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2752: Training Loss: 0.40699822703997296 Validation Loss: 0.8179583549499512\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2753: Training Loss: 0.40816158056259155 Validation Loss: 0.8183708190917969\n",
      "Epoch 2754: Training Loss: 0.4060787657896678 Validation Loss: 0.8180724382400513\n",
      "Epoch 2755: Training Loss: 0.4055764973163605 Validation Loss: 0.8178880214691162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2756: Training Loss: 0.4055434763431549 Validation Loss: 0.8172951340675354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2757: Training Loss: 0.40534589687983197 Validation Loss: 0.8173194527626038\n",
      "Epoch 2758: Training Loss: 0.4057356019814809 Validation Loss: 0.8169745206832886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2759: Training Loss: 0.40501293540000916 Validation Loss: 0.8165581226348877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2760: Training Loss: 0.4060628612836202 Validation Loss: 0.8167784810066223\n",
      "Epoch 2761: Training Loss: 0.40491631627082825 Validation Loss: 0.8166544437408447\n",
      "Epoch 2762: Training Loss: 0.40411662062009174 Validation Loss: 0.8165010809898376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2763: Training Loss: 0.4049757818380992 Validation Loss: 0.8166895508766174\n",
      "Epoch 2764: Training Loss: 0.4042472541332245 Validation Loss: 0.8169264197349548\n",
      "Epoch 2765: Training Loss: 0.40389640132586163 Validation Loss: 0.8167408108711243\n",
      "Epoch 2766: Training Loss: 0.404072771469752 Validation Loss: 0.8167207837104797\n",
      "Epoch 2767: Training Loss: 0.40386911233266193 Validation Loss: 0.8165342211723328\n",
      "Epoch 2768: Training Loss: 0.4038858811060588 Validation Loss: 0.8159891366958618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2769: Training Loss: 0.4044877390066783 Validation Loss: 0.81596839427948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2770: Training Loss: 0.4057069818178813 Validation Loss: 0.8155657649040222\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2771: Training Loss: 0.4037234882513682 Validation Loss: 0.8153358697891235\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2772: Training Loss: 0.40249552329381305 Validation Loss: 0.8156081438064575\n",
      "Epoch 2773: Training Loss: 0.4019072751204173 Validation Loss: 0.8159267902374268\n",
      "Epoch 2774: Training Loss: 0.40202096104621887 Validation Loss: 0.8157966136932373\n",
      "Epoch 2775: Training Loss: 0.401924063762029 Validation Loss: 0.8151676058769226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2776: Training Loss: 0.4032438099384308 Validation Loss: 0.8149766325950623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2777: Training Loss: 0.4019046525160472 Validation Loss: 0.8144083023071289\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2778: Training Loss: 0.4019954403241475 Validation Loss: 0.8145264983177185\n",
      "Epoch 2779: Training Loss: 0.4004771908124288 Validation Loss: 0.8144060969352722\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2780: Training Loss: 0.4022023578484853 Validation Loss: 0.8143669962882996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2781: Training Loss: 0.40099190672238666 Validation Loss: 0.8148902654647827\n",
      "Epoch 2782: Training Loss: 0.40071391065915424 Validation Loss: 0.8147612810134888\n",
      "Epoch 2783: Training Loss: 0.4014599323272705 Validation Loss: 0.8149051666259766\n",
      "Epoch 2784: Training Loss: 0.402673880259196 Validation Loss: 0.8147953748703003\n",
      "Epoch 2785: Training Loss: 0.40015072623888653 Validation Loss: 0.8142709136009216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2786: Training Loss: 0.40143253405888873 Validation Loss: 0.8141956925392151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2787: Training Loss: 0.39971638719240826 Validation Loss: 0.8140323162078857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2788: Training Loss: 0.4001741210619609 Validation Loss: 0.8138411641120911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2789: Training Loss: 0.40077797571818036 Validation Loss: 0.8136662840843201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2790: Training Loss: 0.39956605434417725 Validation Loss: 0.8137015104293823\n",
      "Epoch 2791: Training Loss: 0.40071243047714233 Validation Loss: 0.8137227892875671\n",
      "Epoch 2792: Training Loss: 0.39938580989837646 Validation Loss: 0.813694953918457\n",
      "Epoch 2793: Training Loss: 0.3992348213990529 Validation Loss: 0.8134263753890991\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2794: Training Loss: 0.39938979347546893 Validation Loss: 0.8130050301551819\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2795: Training Loss: 0.3983507653077443 Validation Loss: 0.812903881072998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2796: Training Loss: 0.39803870519002277 Validation Loss: 0.812857985496521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2797: Training Loss: 0.3991568684577942 Validation Loss: 0.8123003244400024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2798: Training Loss: 0.3991684814294179 Validation Loss: 0.8121254444122314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2799: Training Loss: 0.3970215121905009 Validation Loss: 0.8121163845062256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2800: Training Loss: 0.39765246709187824 Validation Loss: 0.8125214576721191\n",
      "Epoch 2801: Training Loss: 0.39889100193977356 Validation Loss: 0.8123828768730164\n",
      "Epoch 2802: Training Loss: 0.3966176211833954 Validation Loss: 0.8125200271606445\n",
      "Epoch 2803: Training Loss: 0.39718466997146606 Validation Loss: 0.8125169277191162\n",
      "Epoch 2804: Training Loss: 0.3980911672115326 Validation Loss: 0.8123087286949158\n",
      "Epoch 2805: Training Loss: 0.39646559953689575 Validation Loss: 0.8120860457420349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2806: Training Loss: 0.39680049816767377 Validation Loss: 0.8114437460899353\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2807: Training Loss: 0.3968408902486165 Validation Loss: 0.8112167716026306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2808: Training Loss: 0.396205206712087 Validation Loss: 0.8111447095870972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2809: Training Loss: 0.3956117033958435 Validation Loss: 0.8109779357910156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2810: Training Loss: 0.39574896295865375 Validation Loss: 0.8108027577400208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2811: Training Loss: 0.3958212633927663 Validation Loss: 0.8107072114944458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2812: Training Loss: 0.39550426602363586 Validation Loss: 0.810827910900116\n",
      "Epoch 2813: Training Loss: 0.3970220585664113 Validation Loss: 0.8113409280776978\n",
      "Epoch 2814: Training Loss: 0.39637409647305805 Validation Loss: 0.8110138773918152\n",
      "Epoch 2815: Training Loss: 0.39563820759455365 Validation Loss: 0.8105762600898743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2816: Training Loss: 0.39478663603464764 Validation Loss: 0.8102149963378906\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2817: Training Loss: 0.3950022757053375 Validation Loss: 0.810026228427887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2818: Training Loss: 0.3944839636484782 Validation Loss: 0.8102278113365173\n",
      "Epoch 2819: Training Loss: 0.3942118485768636 Validation Loss: 0.8105289340019226\n",
      "Epoch 2820: Training Loss: 0.3959453801314036 Validation Loss: 0.8105058073997498\n",
      "Epoch 2821: Training Loss: 0.3937741021315257 Validation Loss: 0.8099395632743835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2822: Training Loss: 0.39323322971661884 Validation Loss: 0.8096176981925964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2823: Training Loss: 0.3942735294500987 Validation Loss: 0.8094455599784851\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2824: Training Loss: 0.3934711416562398 Validation Loss: 0.8092056512832642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2825: Training Loss: 0.3933848738670349 Validation Loss: 0.8090472221374512\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2826: Training Loss: 0.3928071955839793 Validation Loss: 0.8090311884880066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2827: Training Loss: 0.39274973670641583 Validation Loss: 0.8090224862098694\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2828: Training Loss: 0.392787237962087 Validation Loss: 0.8090720772743225\n",
      "Epoch 2829: Training Loss: 0.3926156957944234 Validation Loss: 0.8088424205780029\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2830: Training Loss: 0.3931560417016347 Validation Loss: 0.8088102340698242\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2831: Training Loss: 0.39240821202596027 Validation Loss: 0.808940052986145\n",
      "Epoch 2832: Training Loss: 0.39207587639490765 Validation Loss: 0.8089569807052612\n",
      "Epoch 2833: Training Loss: 0.39184650778770447 Validation Loss: 0.8085639476776123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2834: Training Loss: 0.39187777042388916 Validation Loss: 0.8082113862037659\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2835: Training Loss: 0.3915673593680064 Validation Loss: 0.8079769611358643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2836: Training Loss: 0.3910837570826213 Validation Loss: 0.8075728416442871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2837: Training Loss: 0.3924681544303894 Validation Loss: 0.8077011704444885\n",
      "Epoch 2838: Training Loss: 0.3910004297892253 Validation Loss: 0.8076517581939697\n",
      "Epoch 2839: Training Loss: 0.39082308610280353 Validation Loss: 0.807771623134613\n",
      "Epoch 2840: Training Loss: 0.390880823135376 Validation Loss: 0.8079323768615723\n",
      "Epoch 2841: Training Loss: 0.3904891808827718 Validation Loss: 0.8082942962646484\n",
      "Epoch 2842: Training Loss: 0.39099985361099243 Validation Loss: 0.8079317808151245\n",
      "Epoch 2843: Training Loss: 0.38972292343775433 Validation Loss: 0.8071982264518738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2844: Training Loss: 0.3905284603436788 Validation Loss: 0.8066173195838928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2845: Training Loss: 0.3905898332595825 Validation Loss: 0.8066082000732422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2846: Training Loss: 0.3898556927839915 Validation Loss: 0.8065606355667114\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2847: Training Loss: 0.3895191550254822 Validation Loss: 0.8069760203361511\n",
      "Epoch 2848: Training Loss: 0.38941041628519696 Validation Loss: 0.8069308996200562\n",
      "Epoch 2849: Training Loss: 0.3895315925280253 Validation Loss: 0.8069213032722473\n",
      "Epoch 2850: Training Loss: 0.3887348969777425 Validation Loss: 0.8060436248779297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2851: Training Loss: 0.3904927770296733 Validation Loss: 0.8062036037445068\n",
      "Epoch 2852: Training Loss: 0.38853077093760174 Validation Loss: 0.8062500953674316\n",
      "Epoch 2853: Training Loss: 0.3885057071844737 Validation Loss: 0.8062130212783813\n",
      "Epoch 2854: Training Loss: 0.3884757657845815 Validation Loss: 0.8061872124671936\n",
      "Epoch 2855: Training Loss: 0.38793371121088666 Validation Loss: 0.8058522343635559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2856: Training Loss: 0.3883817692597707 Validation Loss: 0.805566132068634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2857: Training Loss: 0.38811689615249634 Validation Loss: 0.8056119680404663\n",
      "Epoch 2858: Training Loss: 0.38757017254829407 Validation Loss: 0.8057387471199036\n",
      "Epoch 2859: Training Loss: 0.3875897129376729 Validation Loss: 0.8058898448944092\n",
      "Epoch 2860: Training Loss: 0.3878670434157054 Validation Loss: 0.8059236407279968\n",
      "Epoch 2861: Training Loss: 0.3882109522819519 Validation Loss: 0.8055993914604187\n",
      "Epoch 2862: Training Loss: 0.3876085082689921 Validation Loss: 0.8050537109375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2863: Training Loss: 0.3865169584751129 Validation Loss: 0.8048574924468994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2864: Training Loss: 0.3881719410419464 Validation Loss: 0.8047459721565247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2865: Training Loss: 0.3872860173384349 Validation Loss: 0.8049588203430176\n",
      "Epoch 2866: Training Loss: 0.3868364791075389 Validation Loss: 0.8048881888389587\n",
      "Epoch 2867: Training Loss: 0.387567937374115 Validation Loss: 0.8047165274620056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2868: Training Loss: 0.3867700397968292 Validation Loss: 0.8046170473098755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2869: Training Loss: 0.3850286106268565 Validation Loss: 0.8048010468482971\n",
      "Epoch 2870: Training Loss: 0.38540669282277423 Validation Loss: 0.8048402070999146\n",
      "Epoch 2871: Training Loss: 0.38621649146080017 Validation Loss: 0.8049148321151733\n",
      "Epoch 2872: Training Loss: 0.38601337869962055 Validation Loss: 0.8042935132980347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2873: Training Loss: 0.38434313734372455 Validation Loss: 0.8040468692779541\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2874: Training Loss: 0.3856752614180247 Validation Loss: 0.8040923476219177\n",
      "Epoch 2875: Training Loss: 0.3844986359278361 Validation Loss: 0.8035950064659119\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2876: Training Loss: 0.38508906960487366 Validation Loss: 0.8033894300460815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2877: Training Loss: 0.3847118417421977 Validation Loss: 0.8030888438224792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2878: Training Loss: 0.38424432277679443 Validation Loss: 0.8030301928520203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2879: Training Loss: 0.3843722641468048 Validation Loss: 0.8033340573310852\n",
      "Epoch 2880: Training Loss: 0.3848220507303874 Validation Loss: 0.8032088875770569\n",
      "Epoch 2881: Training Loss: 0.38398876786231995 Validation Loss: 0.8033891916275024\n",
      "Epoch 2882: Training Loss: 0.38336867094039917 Validation Loss: 0.8029567003250122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2883: Training Loss: 0.3839837312698364 Validation Loss: 0.8026713132858276\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2884: Training Loss: 0.3851791024208069 Validation Loss: 0.8025171160697937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2885: Training Loss: 0.383314977089564 Validation Loss: 0.8025607466697693\n",
      "Epoch 2886: Training Loss: 0.3833632965882619 Validation Loss: 0.8023511171340942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2887: Training Loss: 0.3827205300331116 Validation Loss: 0.8022802472114563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2888: Training Loss: 0.3826757272084554 Validation Loss: 0.8025044798851013\n",
      "Epoch 2889: Training Loss: 0.3829885522524516 Validation Loss: 0.8027349710464478\n",
      "Epoch 2890: Training Loss: 0.38171624143918353 Validation Loss: 0.8025559782981873\n",
      "Epoch 2891: Training Loss: 0.3828769127527873 Validation Loss: 0.8022418022155762\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2892: Training Loss: 0.38201819856961566 Validation Loss: 0.8021495938301086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2893: Training Loss: 0.3818738063176473 Validation Loss: 0.801866888999939\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2894: Training Loss: 0.3829810122648875 Validation Loss: 0.8021661639213562\n",
      "Epoch 2895: Training Loss: 0.3822376827398936 Validation Loss: 0.8015994429588318\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2896: Training Loss: 0.3815755546092987 Validation Loss: 0.8017600774765015\n",
      "Epoch 2897: Training Loss: 0.38114188114802044 Validation Loss: 0.8013355731964111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2898: Training Loss: 0.38088545203208923 Validation Loss: 0.8011108636856079\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2899: Training Loss: 0.3786907096703847 Validation Loss: 0.8009374141693115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2900: Training Loss: 0.38178058465321857 Validation Loss: 0.8005879521369934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2901: Training Loss: 0.38038138548533124 Validation Loss: 0.800519585609436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2902: Training Loss: 0.3805380662282308 Validation Loss: 0.800527036190033\n",
      "Epoch 2903: Training Loss: 0.38158251841862995 Validation Loss: 0.8007520437240601\n",
      "Epoch 2904: Training Loss: 0.3800041476885478 Validation Loss: 0.8010432124137878\n",
      "Epoch 2905: Training Loss: 0.3813433547814687 Validation Loss: 0.8009913563728333\n",
      "Epoch 2906: Training Loss: 0.3810690939426422 Validation Loss: 0.8010335564613342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2907: Training Loss: 0.37969762086868286 Validation Loss: 0.800696611404419\n",
      "Epoch 2908: Training Loss: 0.3792638381322225 Validation Loss: 0.8006003499031067\n",
      "Epoch 2909: Training Loss: 0.37949828306833905 Validation Loss: 0.8002272844314575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2910: Training Loss: 0.3804808457692464 Validation Loss: 0.8001583814620972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2911: Training Loss: 0.38104108969370526 Validation Loss: 0.7999268174171448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2912: Training Loss: 0.3787735402584076 Validation Loss: 0.7998203635215759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2913: Training Loss: 0.37979820370674133 Validation Loss: 0.7995798587799072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2914: Training Loss: 0.3783990641434987 Validation Loss: 0.7996472716331482\n",
      "Epoch 2915: Training Loss: 0.37832966446876526 Validation Loss: 0.7995507121086121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2916: Training Loss: 0.37784632047017414 Validation Loss: 0.7993745803833008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2917: Training Loss: 0.37856680154800415 Validation Loss: 0.798973023891449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2918: Training Loss: 0.37754107515017193 Validation Loss: 0.7988303899765015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2919: Training Loss: 0.3766058683395386 Validation Loss: 0.799241304397583\n",
      "Epoch 2920: Training Loss: 0.3778481682141622 Validation Loss: 0.7990448474884033\n",
      "Epoch 2921: Training Loss: 0.37705131371816 Validation Loss: 0.7991277575492859\n",
      "Epoch 2922: Training Loss: 0.3768830895423889 Validation Loss: 0.7989499568939209\n",
      "Epoch 2923: Training Loss: 0.376909871896108 Validation Loss: 0.798482358455658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2924: Training Loss: 0.3766358494758606 Validation Loss: 0.7984874844551086\n",
      "Epoch 2925: Training Loss: 0.3764928976694743 Validation Loss: 0.7981622219085693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2926: Training Loss: 0.37683213750521344 Validation Loss: 0.7980899810791016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2927: Training Loss: 0.3759847780068715 Validation Loss: 0.7982807159423828\n",
      "Epoch 2928: Training Loss: 0.37626684705416363 Validation Loss: 0.7981908321380615\n",
      "Epoch 2929: Training Loss: 0.37692458430926007 Validation Loss: 0.7983651757240295\n",
      "Epoch 2930: Training Loss: 0.3767124613126119 Validation Loss: 0.7980839014053345\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2931: Training Loss: 0.3754851321379344 Validation Loss: 0.7976800799369812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2932: Training Loss: 0.37644195556640625 Validation Loss: 0.7976953983306885\n",
      "Epoch 2933: Training Loss: 0.3749435444672902 Validation Loss: 0.7976357340812683\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2934: Training Loss: 0.37522634863853455 Validation Loss: 0.7971522212028503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2935: Training Loss: 0.37518811225891113 Validation Loss: 0.7970820665359497\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2936: Training Loss: 0.3750160833199819 Validation Loss: 0.797511100769043\n",
      "Epoch 2937: Training Loss: 0.3744211693604787 Validation Loss: 0.7975541949272156\n",
      "Epoch 2938: Training Loss: 0.37451279163360596 Validation Loss: 0.7975332140922546\n",
      "Epoch 2939: Training Loss: 0.37538859248161316 Validation Loss: 0.7972187399864197\n",
      "Epoch 2940: Training Loss: 0.3741304775079091 Validation Loss: 0.7970781922340393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2941: Training Loss: 0.37424400448799133 Validation Loss: 0.7966316938400269\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2942: Training Loss: 0.37411971886952716 Validation Loss: 0.796421468257904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2943: Training Loss: 0.3775284985701243 Validation Loss: 0.7962371706962585\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2944: Training Loss: 0.37369691332181293 Validation Loss: 0.7961177825927734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2945: Training Loss: 0.3752008080482483 Validation Loss: 0.7960769534111023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2946: Training Loss: 0.3735095461209615 Validation Loss: 0.7961543798446655\n",
      "Epoch 2947: Training Loss: 0.3742249310016632 Validation Loss: 0.7963224053382874\n",
      "Epoch 2948: Training Loss: 0.37330812215805054 Validation Loss: 0.7962750792503357\n",
      "Epoch 2949: Training Loss: 0.37349159518877667 Validation Loss: 0.7961055040359497\n",
      "Epoch 2950: Training Loss: 0.37393254041671753 Validation Loss: 0.7961479425430298\n",
      "Epoch 2951: Training Loss: 0.3726784586906433 Validation Loss: 0.7958076000213623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2952: Training Loss: 0.3730108141899109 Validation Loss: 0.7954140901565552\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2953: Training Loss: 0.37265772620836896 Validation Loss: 0.7957939505577087\n",
      "Epoch 2954: Training Loss: 0.37180131673812866 Validation Loss: 0.7953228950500488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2955: Training Loss: 0.3721892138322194 Validation Loss: 0.7954322099685669\n",
      "Epoch 2956: Training Loss: 0.3717685639858246 Validation Loss: 0.7954021096229553\n",
      "Epoch 2957: Training Loss: 0.37210382024447125 Validation Loss: 0.7954580783843994\n",
      "Epoch 2958: Training Loss: 0.3711522122224172 Validation Loss: 0.7954496741294861\n",
      "Epoch 2959: Training Loss: 0.37071216106414795 Validation Loss: 0.794906735420227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2960: Training Loss: 0.37108872334162396 Validation Loss: 0.7945688366889954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2961: Training Loss: 0.3711650272210439 Validation Loss: 0.7947652339935303\n",
      "Epoch 2962: Training Loss: 0.37325842181841534 Validation Loss: 0.7949925661087036\n",
      "Epoch 2963: Training Loss: 0.37063637375831604 Validation Loss: 0.7949054837226868\n",
      "Epoch 2964: Training Loss: 0.37103424469629925 Validation Loss: 0.7949644327163696\n",
      "Epoch 2965: Training Loss: 0.37115723888079327 Validation Loss: 0.7945727109909058\n",
      "Epoch 2966: Training Loss: 0.3713306784629822 Validation Loss: 0.7944291234016418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2967: Training Loss: 0.3701940377553304 Validation Loss: 0.7945166826248169\n",
      "Epoch 2968: Training Loss: 0.37018993496894836 Validation Loss: 0.7941501140594482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2969: Training Loss: 0.36929020285606384 Validation Loss: 0.7939343452453613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2970: Training Loss: 0.3692890803019206 Validation Loss: 0.793492317199707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2971: Training Loss: 0.3691258529822032 Validation Loss: 0.7930622696876526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2972: Training Loss: 0.36921854813893634 Validation Loss: 0.7934265732765198\n",
      "Epoch 2973: Training Loss: 0.36877769231796265 Validation Loss: 0.7935295104980469\n",
      "Epoch 2974: Training Loss: 0.3699260850747426 Validation Loss: 0.7936237454414368\n",
      "Epoch 2975: Training Loss: 0.37020203471183777 Validation Loss: 0.7934600114822388\n",
      "Epoch 2976: Training Loss: 0.36895901958147687 Validation Loss: 0.7933266758918762\n",
      "Epoch 2977: Training Loss: 0.36848687132199603 Validation Loss: 0.793471097946167\n",
      "Epoch 2978: Training Loss: 0.3690289556980133 Validation Loss: 0.7932791113853455\n",
      "Epoch 2979: Training Loss: 0.36841482917467755 Validation Loss: 0.7932387590408325\n",
      "Epoch 2980: Training Loss: 0.3685908019542694 Validation Loss: 0.793297290802002\n",
      "Epoch 2981: Training Loss: 0.3682629466056824 Validation Loss: 0.792975664138794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2982: Training Loss: 0.3677569528420766 Validation Loss: 0.792543888092041\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2983: Training Loss: 0.36743664741516113 Validation Loss: 0.792736828327179\n",
      "Epoch 2984: Training Loss: 0.3680281639099121 Validation Loss: 0.7924269437789917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2985: Training Loss: 0.3680553237597148 Validation Loss: 0.7921083569526672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2986: Training Loss: 0.36761752764383954 Validation Loss: 0.7919929027557373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2987: Training Loss: 0.36767040689786273 Validation Loss: 0.7921257019042969\n",
      "Epoch 2988: Training Loss: 0.3678075075149536 Validation Loss: 0.7924033403396606\n",
      "Epoch 2989: Training Loss: 0.3668999473253886 Validation Loss: 0.792184054851532\n",
      "Epoch 2990: Training Loss: 0.3664466639359792 Validation Loss: 0.7922354936599731\n",
      "Epoch 2991: Training Loss: 0.36622801423072815 Validation Loss: 0.7921065092086792\n",
      "Epoch 2992: Training Loss: 0.3660741051038106 Validation Loss: 0.7917262315750122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2993: Training Loss: 0.36591140429178876 Validation Loss: 0.7912706732749939\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2994: Training Loss: 0.3657932976881663 Validation Loss: 0.7911993265151978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2995: Training Loss: 0.3692717452843984 Validation Loss: 0.7913638353347778\n",
      "Epoch 2996: Training Loss: 0.365749587615331 Validation Loss: 0.7912887930870056\n",
      "Epoch 2997: Training Loss: 0.36540398995081586 Validation Loss: 0.79160475730896\n",
      "Epoch 2998: Training Loss: 0.365690678358078 Validation Loss: 0.7913417816162109\n",
      "Epoch 2999: Training Loss: 0.36488746603329975 Validation Loss: 0.7911020517349243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3000: Training Loss: 0.364639679590861 Validation Loss: 0.790808379650116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3001: Training Loss: 0.36457306146621704 Validation Loss: 0.7908650636672974\n",
      "Epoch 3002: Training Loss: 0.3643481532732646 Validation Loss: 0.7907410264015198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3003: Training Loss: 0.3643377621968587 Validation Loss: 0.7904062867164612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3004: Training Loss: 0.3642871081829071 Validation Loss: 0.7902655005455017\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3005: Training Loss: 0.364195853471756 Validation Loss: 0.7902827262878418\n",
      "Epoch 3006: Training Loss: 0.3641553620497386 Validation Loss: 0.7903010845184326\n",
      "Epoch 3007: Training Loss: 0.36534767349561054 Validation Loss: 0.7901804447174072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3008: Training Loss: 0.363869309425354 Validation Loss: 0.7901627421379089\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3009: Training Loss: 0.36587807536125183 Validation Loss: 0.7900804877281189\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3010: Training Loss: 0.3642631570498149 Validation Loss: 0.7902790307998657\n",
      "Epoch 3011: Training Loss: 0.3634355068206787 Validation Loss: 0.7899687886238098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3012: Training Loss: 0.3635816772778829 Validation Loss: 0.7897297143936157\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3013: Training Loss: 0.36270097891489667 Validation Loss: 0.7899187207221985\n",
      "Epoch 3014: Training Loss: 0.36378560463587445 Validation Loss: 0.78986656665802\n",
      "Epoch 3015: Training Loss: 0.3632570107777913 Validation Loss: 0.7898745536804199\n",
      "Epoch 3016: Training Loss: 0.3621309697628021 Validation Loss: 0.7893568277359009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3017: Training Loss: 0.3625916043917338 Validation Loss: 0.7891253232955933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3018: Training Loss: 0.36241474747657776 Validation Loss: 0.7889517545700073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3019: Training Loss: 0.36232099930445355 Validation Loss: 0.7892541885375977\n",
      "Epoch 3020: Training Loss: 0.36182335019111633 Validation Loss: 0.7888732552528381\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3021: Training Loss: 0.36206533511479694 Validation Loss: 0.7887309193611145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3022: Training Loss: 0.36161884665489197 Validation Loss: 0.7884695529937744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3023: Training Loss: 0.3612488806247711 Validation Loss: 0.7883845567703247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3024: Training Loss: 0.36055556933085126 Validation Loss: 0.7881090641021729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3025: Training Loss: 0.361240029335022 Validation Loss: 0.7883132100105286\n",
      "Epoch 3026: Training Loss: 0.36174291372299194 Validation Loss: 0.7882965803146362\n",
      "Epoch 3027: Training Loss: 0.36091646552085876 Validation Loss: 0.7887316942214966\n",
      "Epoch 3028: Training Loss: 0.3606557250022888 Validation Loss: 0.7892422676086426\n",
      "Epoch 3029: Training Loss: 0.3610652685165405 Validation Loss: 0.7890453934669495\n",
      "Epoch 3030: Training Loss: 0.3607221245765686 Validation Loss: 0.7889637351036072\n",
      "Epoch 3031: Training Loss: 0.3607944846153259 Validation Loss: 0.7885141372680664\n",
      "Epoch 3032: Training Loss: 0.3601259688536326 Validation Loss: 0.7881487607955933\n",
      "Epoch 3033: Training Loss: 0.3603210647900899 Validation Loss: 0.7877309918403625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3034: Training Loss: 0.3633691171805064 Validation Loss: 0.7875438332557678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3035: Training Loss: 0.35990296800931293 Validation Loss: 0.7876617312431335\n",
      "Epoch 3036: Training Loss: 0.3601284126440684 Validation Loss: 0.787433922290802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3037: Training Loss: 0.3597193757692973 Validation Loss: 0.7872013449668884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3038: Training Loss: 0.3594021201133728 Validation Loss: 0.7874014377593994\n",
      "Epoch 3039: Training Loss: 0.35879380504290265 Validation Loss: 0.7873585224151611\n",
      "Epoch 3040: Training Loss: 0.35907090703646344 Validation Loss: 0.7872529029846191\n",
      "Epoch 3041: Training Loss: 0.3587978680928548 Validation Loss: 0.7869439125061035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3042: Training Loss: 0.35992714762687683 Validation Loss: 0.7867220640182495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3043: Training Loss: 0.35885724425315857 Validation Loss: 0.7867765426635742\n",
      "Epoch 3044: Training Loss: 0.3583797216415405 Validation Loss: 0.7869314551353455\n",
      "Epoch 3045: Training Loss: 0.3584570785363515 Validation Loss: 0.7868216037750244\n",
      "Epoch 3046: Training Loss: 0.3588157892227173 Validation Loss: 0.7874423265457153\n",
      "Epoch 3047: Training Loss: 0.3580281337102254 Validation Loss: 0.7872770428657532\n",
      "Epoch 3048: Training Loss: 0.35913869738578796 Validation Loss: 0.7867163419723511\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3049: Training Loss: 0.35765329996744794 Validation Loss: 0.7864301800727844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3050: Training Loss: 0.35675113399823505 Validation Loss: 0.7860408425331116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3051: Training Loss: 0.3578019936879476 Validation Loss: 0.7858917713165283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3052: Training Loss: 0.3570243716239929 Validation Loss: 0.7858283519744873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3053: Training Loss: 0.35754962762196857 Validation Loss: 0.785893440246582\n",
      "Epoch 3054: Training Loss: 0.3570983012517293 Validation Loss: 0.7855907082557678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3055: Training Loss: 0.35707374413808185 Validation Loss: 0.7860044240951538\n",
      "Epoch 3056: Training Loss: 0.3571042517820994 Validation Loss: 0.7859383821487427\n",
      "Epoch 3057: Training Loss: 0.3570748269557953 Validation Loss: 0.7854169607162476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3058: Training Loss: 0.3557822108268738 Validation Loss: 0.7855620980262756\n",
      "Epoch 3059: Training Loss: 0.35726269086201984 Validation Loss: 0.7853700518608093\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3060: Training Loss: 0.3561039666334788 Validation Loss: 0.7854413986206055\n",
      "Epoch 3061: Training Loss: 0.3565232257048289 Validation Loss: 0.7851767539978027\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3062: Training Loss: 0.35598744948705036 Validation Loss: 0.7854868769645691\n",
      "Epoch 3063: Training Loss: 0.35544881224632263 Validation Loss: 0.7853203415870667\n",
      "Epoch 3064: Training Loss: 0.3556341628233592 Validation Loss: 0.7850364446640015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3065: Training Loss: 0.35893625020980835 Validation Loss: 0.7851710915565491\n",
      "Epoch 3066: Training Loss: 0.35532934466997784 Validation Loss: 0.7848948836326599\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3067: Training Loss: 0.3558398485183716 Validation Loss: 0.7848554253578186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3068: Training Loss: 0.356261005004247 Validation Loss: 0.7849863171577454\n",
      "Epoch 3069: Training Loss: 0.3544774254163106 Validation Loss: 0.7847108244895935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3070: Training Loss: 0.3547731737295787 Validation Loss: 0.7846741676330566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3071: Training Loss: 0.3546016712983449 Validation Loss: 0.7844274044036865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3072: Training Loss: 0.3545675377051036 Validation Loss: 0.7840816378593445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3073: Training Loss: 0.35543476541837055 Validation Loss: 0.7841055393218994\n",
      "Epoch 3074: Training Loss: 0.3545876940091451 Validation Loss: 0.7839641571044922\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3075: Training Loss: 0.35390060146649677 Validation Loss: 0.783836305141449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3076: Training Loss: 0.35392748316129047 Validation Loss: 0.7836959362030029\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3077: Training Loss: 0.3536616563796997 Validation Loss: 0.7837334871292114\n",
      "Epoch 3078: Training Loss: 0.35379938284556073 Validation Loss: 0.7839517593383789\n",
      "Epoch 3079: Training Loss: 0.3531558612982432 Validation Loss: 0.7841096520423889\n",
      "Epoch 3080: Training Loss: 0.3539879222710927 Validation Loss: 0.7840990424156189\n",
      "Epoch 3081: Training Loss: 0.3540571828683217 Validation Loss: 0.783708393573761\n",
      "Epoch 3082: Training Loss: 0.3531917333602905 Validation Loss: 0.7835642695426941\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3083: Training Loss: 0.3530120253562927 Validation Loss: 0.7831206321716309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3084: Training Loss: 0.35245366891225177 Validation Loss: 0.7828981280326843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3085: Training Loss: 0.3528928558031718 Validation Loss: 0.7826935052871704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3086: Training Loss: 0.35247334837913513 Validation Loss: 0.7828298211097717\n",
      "Epoch 3087: Training Loss: 0.35311787327130634 Validation Loss: 0.7827578186988831\n",
      "Epoch 3088: Training Loss: 0.3521297474702199 Validation Loss: 0.7826412916183472\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3089: Training Loss: 0.35190029939015705 Validation Loss: 0.7829912304878235\n",
      "Epoch 3090: Training Loss: 0.3534828821818034 Validation Loss: 0.782927393913269\n",
      "Epoch 3091: Training Loss: 0.3516578674316406 Validation Loss: 0.782692551612854\n",
      "Epoch 3092: Training Loss: 0.3524634043375651 Validation Loss: 0.7824406027793884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3093: Training Loss: 0.35347562034924823 Validation Loss: 0.782456636428833\n",
      "Epoch 3094: Training Loss: 0.3510965406894684 Validation Loss: 0.7827438712120056\n",
      "Epoch 3095: Training Loss: 0.35199864705403644 Validation Loss: 0.7827946543693542\n",
      "Epoch 3096: Training Loss: 0.35164641340573627 Validation Loss: 0.7824209332466125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3097: Training Loss: 0.3514154354731242 Validation Loss: 0.7824954390525818\n",
      "Epoch 3098: Training Loss: 0.3506743212540944 Validation Loss: 0.7824381589889526\n",
      "Epoch 3099: Training Loss: 0.3507700165112813 Validation Loss: 0.7820175290107727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3100: Training Loss: 0.3513658344745636 Validation Loss: 0.7818444967269897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3101: Training Loss: 0.35014261802037555 Validation Loss: 0.7817157506942749\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3102: Training Loss: 0.3504300018151601 Validation Loss: 0.7814860343933105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3103: Training Loss: 0.35013822714487713 Validation Loss: 0.7812311053276062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3104: Training Loss: 0.349720577398936 Validation Loss: 0.7811771035194397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3105: Training Loss: 0.3504193127155304 Validation Loss: 0.7811741828918457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3106: Training Loss: 0.350662628809611 Validation Loss: 0.7810338735580444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3107: Training Loss: 0.34998472531636554 Validation Loss: 0.7807623147964478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3108: Training Loss: 0.3498755395412445 Validation Loss: 0.7808361649513245\n",
      "Epoch 3109: Training Loss: 0.3486270308494568 Validation Loss: 0.7808858752250671\n",
      "Epoch 3110: Training Loss: 0.34952916701634723 Validation Loss: 0.7808063626289368\n",
      "Epoch 3111: Training Loss: 0.3498166700204213 Validation Loss: 0.7810189127922058\n",
      "Epoch 3112: Training Loss: 0.34912527600924176 Validation Loss: 0.7809526920318604\n",
      "Epoch 3113: Training Loss: 0.34952519337336224 Validation Loss: 0.7810375690460205\n",
      "Epoch 3114: Training Loss: 0.3494151731332143 Validation Loss: 0.7810614705085754\n",
      "Epoch 3115: Training Loss: 0.34852466980616253 Validation Loss: 0.7807937264442444\n",
      "Epoch 3116: Training Loss: 0.3497806191444397 Validation Loss: 0.7807587385177612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3117: Training Loss: 0.34877092639605206 Validation Loss: 0.7805392146110535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3118: Training Loss: 0.3479544321695964 Validation Loss: 0.7804018259048462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3119: Training Loss: 0.34887587030728656 Validation Loss: 0.7800904512405396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3120: Training Loss: 0.3475775917371114 Validation Loss: 0.7796086072921753\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3121: Training Loss: 0.3474469780921936 Validation Loss: 0.7799468636512756\n",
      "Epoch 3122: Training Loss: 0.3472830553849538 Validation Loss: 0.7799906134605408\n",
      "Epoch 3123: Training Loss: 0.3473820686340332 Validation Loss: 0.7800636291503906\n",
      "Epoch 3124: Training Loss: 0.347389151652654 Validation Loss: 0.7800097465515137\n",
      "Epoch 3125: Training Loss: 0.3471845289071401 Validation Loss: 0.7799546122550964\n",
      "Epoch 3126: Training Loss: 0.34876473744710285 Validation Loss: 0.7799042463302612\n",
      "Epoch 3127: Training Loss: 0.3461471696694692 Validation Loss: 0.7796245217323303\n",
      "Epoch 3128: Training Loss: 0.3470146656036377 Validation Loss: 0.7796144485473633\n",
      "Epoch 3129: Training Loss: 0.34635936220486957 Validation Loss: 0.7793994545936584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3130: Training Loss: 0.3474288880825043 Validation Loss: 0.7792003154754639\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3131: Training Loss: 0.3464791476726532 Validation Loss: 0.7793543934822083\n",
      "Epoch 3132: Training Loss: 0.34588711460431415 Validation Loss: 0.7792997360229492\n",
      "Epoch 3133: Training Loss: 0.3470146159331004 Validation Loss: 0.7790058851242065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3134: Training Loss: 0.3463144501050313 Validation Loss: 0.7787953019142151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3135: Training Loss: 0.3468916714191437 Validation Loss: 0.778831422328949\n",
      "Epoch 3136: Training Loss: 0.3458056052525838 Validation Loss: 0.7786624431610107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3137: Training Loss: 0.3459451695283254 Validation Loss: 0.7785086035728455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3138: Training Loss: 0.34538792570432025 Validation Loss: 0.7786510586738586\n",
      "Epoch 3139: Training Loss: 0.34527866045633954 Validation Loss: 0.7787580490112305\n",
      "Epoch 3140: Training Loss: 0.34640513857205707 Validation Loss: 0.7784529328346252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3141: Training Loss: 0.346810907125473 Validation Loss: 0.7782672047615051\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3142: Training Loss: 0.34450941284497577 Validation Loss: 0.7782764434814453\n",
      "Epoch 3143: Training Loss: 0.3450314402580261 Validation Loss: 0.7784761786460876\n",
      "Epoch 3144: Training Loss: 0.3450271387894948 Validation Loss: 0.778201162815094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3145: Training Loss: 0.34520918130874634 Validation Loss: 0.7779291272163391\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3146: Training Loss: 0.34537408749262494 Validation Loss: 0.7781310081481934\n",
      "Epoch 3147: Training Loss: 0.34624525904655457 Validation Loss: 0.7778813242912292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3148: Training Loss: 0.34395891427993774 Validation Loss: 0.7775560617446899\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3149: Training Loss: 0.34508825341860455 Validation Loss: 0.7780563831329346\n",
      "Epoch 3150: Training Loss: 0.34392911195755005 Validation Loss: 0.7780430912971497\n",
      "Epoch 3151: Training Loss: 0.3440020779768626 Validation Loss: 0.7773696184158325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3152: Training Loss: 0.34408944845199585 Validation Loss: 0.7772008180618286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3153: Training Loss: 0.34313668807347614 Validation Loss: 0.7773414850234985\n",
      "Epoch 3154: Training Loss: 0.3438862959543864 Validation Loss: 0.7774399518966675\n",
      "Epoch 3155: Training Loss: 0.343010942141215 Validation Loss: 0.7774436473846436\n",
      "Epoch 3156: Training Loss: 0.3436721861362457 Validation Loss: 0.777413010597229\n",
      "Epoch 3157: Training Loss: 0.3428840935230255 Validation Loss: 0.7773318290710449\n",
      "Epoch 3158: Training Loss: 0.3429355521996816 Validation Loss: 0.7774684429168701\n",
      "Epoch 3159: Training Loss: 0.34284894665082294 Validation Loss: 0.7773222923278809\n",
      "Epoch 3160: Training Loss: 0.34223516782124835 Validation Loss: 0.7773188352584839\n",
      "Epoch 3161: Training Loss: 0.34188475211461383 Validation Loss: 0.7773169875144958\n",
      "Epoch 3162: Training Loss: 0.34198768933614093 Validation Loss: 0.7770086526870728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3163: Training Loss: 0.3419131338596344 Validation Loss: 0.7765125632286072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3164: Training Loss: 0.34168875217437744 Validation Loss: 0.7762624025344849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3165: Training Loss: 0.34151272972424823 Validation Loss: 0.7763024568557739\n",
      "Epoch 3166: Training Loss: 0.3434290687243144 Validation Loss: 0.776305615901947\n",
      "Epoch 3167: Training Loss: 0.3411038815975189 Validation Loss: 0.7764210104942322\n",
      "Epoch 3168: Training Loss: 0.3439733187357585 Validation Loss: 0.7764615416526794\n",
      "Epoch 3169: Training Loss: 0.3409152428309123 Validation Loss: 0.7762699723243713\n",
      "Epoch 3170: Training Loss: 0.34100202719370526 Validation Loss: 0.775740921497345\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3171: Training Loss: 0.3407103916009267 Validation Loss: 0.775456428527832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3172: Training Loss: 0.34108104308446247 Validation Loss: 0.7754533886909485\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3173: Training Loss: 0.3408541679382324 Validation Loss: 0.7755406498908997\n",
      "Epoch 3174: Training Loss: 0.34038365880648297 Validation Loss: 0.7755002379417419\n",
      "Epoch 3175: Training Loss: 0.340211162964503 Validation Loss: 0.7755244970321655\n",
      "Epoch 3176: Training Loss: 0.3396100600560506 Validation Loss: 0.7754316926002502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3177: Training Loss: 0.3411130408445994 Validation Loss: 0.7754771113395691\n",
      "Epoch 3178: Training Loss: 0.33966654539108276 Validation Loss: 0.7756587862968445\n",
      "Epoch 3179: Training Loss: 0.34000636140505475 Validation Loss: 0.7761475443840027\n",
      "Epoch 3180: Training Loss: 0.3391663630803426 Validation Loss: 0.7761178016662598\n",
      "Epoch 3181: Training Loss: 0.3394283056259155 Validation Loss: 0.7756473422050476\n",
      "Epoch 3182: Training Loss: 0.340029497941335 Validation Loss: 0.7753317952156067\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3183: Training Loss: 0.33975522716840106 Validation Loss: 0.7747529745101929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3184: Training Loss: 0.3392617205778758 Validation Loss: 0.7746519446372986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3185: Training Loss: 0.33898869156837463 Validation Loss: 0.7745476961135864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3186: Training Loss: 0.33912648757298786 Validation Loss: 0.7747736573219299\n",
      "Epoch 3187: Training Loss: 0.33823952078819275 Validation Loss: 0.7748715877532959\n",
      "Epoch 3188: Training Loss: 0.33895166714986164 Validation Loss: 0.7744089365005493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3189: Training Loss: 0.33834269642829895 Validation Loss: 0.7745434641838074\n",
      "Epoch 3190: Training Loss: 0.33760973811149597 Validation Loss: 0.7747610807418823\n",
      "Epoch 3191: Training Loss: 0.33861836791038513 Validation Loss: 0.774534285068512\n",
      "Epoch 3192: Training Loss: 0.3376885652542114 Validation Loss: 0.7743777632713318\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3193: Training Loss: 0.3383304874102275 Validation Loss: 0.7746156454086304\n",
      "Epoch 3194: Training Loss: 0.339297483364741 Validation Loss: 0.7742742896080017\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3195: Training Loss: 0.3376012047131856 Validation Loss: 0.7743160128593445\n",
      "Epoch 3196: Training Loss: 0.33765119314193726 Validation Loss: 0.7739830017089844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3197: Training Loss: 0.3377200961112976 Validation Loss: 0.7739247679710388\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3198: Training Loss: 0.337492436170578 Validation Loss: 0.7741912603378296\n",
      "Epoch 3199: Training Loss: 0.3371100227038066 Validation Loss: 0.7739154100418091\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3200: Training Loss: 0.3371448715527852 Validation Loss: 0.7738010883331299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3201: Training Loss: 0.3363255163033803 Validation Loss: 0.7734775543212891\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3202: Training Loss: 0.3374602695306142 Validation Loss: 0.7736727595329285\n",
      "Epoch 3203: Training Loss: 0.33805249134699505 Validation Loss: 0.7734596729278564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3204: Training Loss: 0.337061067422231 Validation Loss: 0.7729960680007935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3205: Training Loss: 0.33626888195673627 Validation Loss: 0.7733198404312134\n",
      "Epoch 3206: Training Loss: 0.3363315562407176 Validation Loss: 0.7732662558555603\n",
      "Epoch 3207: Training Loss: 0.3363659183184306 Validation Loss: 0.7733220458030701\n",
      "Epoch 3208: Training Loss: 0.3358774979909261 Validation Loss: 0.7732802629470825\n",
      "Epoch 3209: Training Loss: 0.33642250299453735 Validation Loss: 0.7728224992752075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3210: Training Loss: 0.33639073371887207 Validation Loss: 0.7728345394134521\n",
      "Epoch 3211: Training Loss: 0.3354764183362325 Validation Loss: 0.7727729082107544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3212: Training Loss: 0.3356367846330007 Validation Loss: 0.7725201845169067\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3213: Training Loss: 0.3358233571052551 Validation Loss: 0.7727203369140625\n",
      "Epoch 3214: Training Loss: 0.33515725533167523 Validation Loss: 0.7724574208259583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3215: Training Loss: 0.33539511760075885 Validation Loss: 0.7722087502479553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3216: Training Loss: 0.33492742975552875 Validation Loss: 0.7723271250724792\n",
      "Epoch 3217: Training Loss: 0.3358093003431956 Validation Loss: 0.7722476124763489\n",
      "Epoch 3218: Training Loss: 0.3344716429710388 Validation Loss: 0.7722439169883728\n",
      "Epoch 3219: Training Loss: 0.3350072503089905 Validation Loss: 0.7722516655921936\n",
      "Epoch 3220: Training Loss: 0.33575522899627686 Validation Loss: 0.772347092628479\n",
      "Epoch 3221: Training Loss: 0.3342703580856323 Validation Loss: 0.7723124027252197\n",
      "Epoch 3222: Training Loss: 0.3359489440917969 Validation Loss: 0.7721143364906311\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3223: Training Loss: 0.3339149554570516 Validation Loss: 0.7719829678535461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3224: Training Loss: 0.33464063207308453 Validation Loss: 0.7716490626335144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3225: Training Loss: 0.33344216148058575 Validation Loss: 0.771647572517395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3226: Training Loss: 0.33457093437512714 Validation Loss: 0.7713766098022461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3227: Training Loss: 0.33412755529085797 Validation Loss: 0.7713311314582825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3228: Training Loss: 0.33369497458140057 Validation Loss: 0.7713417410850525\n",
      "Epoch 3229: Training Loss: 0.33327051997184753 Validation Loss: 0.7711144089698792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3230: Training Loss: 0.33301117022832233 Validation Loss: 0.7716193795204163\n",
      "Epoch 3231: Training Loss: 0.3330775499343872 Validation Loss: 0.7717073559761047\n",
      "Epoch 3232: Training Loss: 0.33252639571825665 Validation Loss: 0.7712447643280029\n",
      "Epoch 3233: Training Loss: 0.3335873484611511 Validation Loss: 0.770969808101654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3234: Training Loss: 0.33294354875882465 Validation Loss: 0.7707116007804871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3235: Training Loss: 0.3336737056573232 Validation Loss: 0.770842969417572\n",
      "Epoch 3236: Training Loss: 0.33244407176971436 Validation Loss: 0.7707820534706116\n",
      "Epoch 3237: Training Loss: 0.3323261837164561 Validation Loss: 0.7711354494094849\n",
      "Epoch 3238: Training Loss: 0.335973193248113 Validation Loss: 0.7710937261581421\n",
      "Epoch 3239: Training Loss: 0.33183524012565613 Validation Loss: 0.7711154222488403\n",
      "Epoch 3240: Training Loss: 0.3322189847628276 Validation Loss: 0.7710079550743103\n",
      "Epoch 3241: Training Loss: 0.3317644993464152 Validation Loss: 0.7709007859230042\n",
      "Epoch 3242: Training Loss: 0.3325328727563222 Validation Loss: 0.7702649235725403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3243: Training Loss: 0.3314927319685618 Validation Loss: 0.770388126373291\n",
      "Epoch 3244: Training Loss: 0.3320899208386739 Validation Loss: 0.7706567049026489\n",
      "Epoch 3245: Training Loss: 0.3323872188727061 Validation Loss: 0.7705271244049072\n",
      "Epoch 3246: Training Loss: 0.33393921454747516 Validation Loss: 0.7702363729476929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3247: Training Loss: 0.33143937587738037 Validation Loss: 0.7704761624336243\n",
      "Epoch 3248: Training Loss: 0.33106541633605957 Validation Loss: 0.7703853249549866\n",
      "Epoch 3249: Training Loss: 0.3308219313621521 Validation Loss: 0.7699082493782043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3250: Training Loss: 0.33147897322972614 Validation Loss: 0.7697526216506958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3251: Training Loss: 0.33091022570927936 Validation Loss: 0.7695170640945435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3252: Training Loss: 0.33269965648651123 Validation Loss: 0.7694299817085266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3253: Training Loss: 0.330582598845164 Validation Loss: 0.7698207497596741\n",
      "Epoch 3254: Training Loss: 0.33015137910842896 Validation Loss: 0.7700050473213196\n",
      "Epoch 3255: Training Loss: 0.3305992583433787 Validation Loss: 0.7696253061294556\n",
      "Epoch 3256: Training Loss: 0.32969191670417786 Validation Loss: 0.7693989872932434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3257: Training Loss: 0.33178771535555523 Validation Loss: 0.7691442966461182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3258: Training Loss: 0.3292346994082133 Validation Loss: 0.7692059278488159\n",
      "Epoch 3259: Training Loss: 0.3298519055048625 Validation Loss: 0.7697560787200928\n",
      "Epoch 3260: Training Loss: 0.3296521504720052 Validation Loss: 0.7696691751480103\n",
      "Epoch 3261: Training Loss: 0.3296995759010315 Validation Loss: 0.7696124911308289\n",
      "Epoch 3262: Training Loss: 0.33024879296620685 Validation Loss: 0.7690315842628479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3263: Training Loss: 0.32963432868321735 Validation Loss: 0.7689763307571411\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3264: Training Loss: 0.3288269142309825 Validation Loss: 0.7686774134635925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3265: Training Loss: 0.33022307356198627 Validation Loss: 0.7685254216194153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3266: Training Loss: 0.328972190618515 Validation Loss: 0.7683812975883484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3267: Training Loss: 0.32850133379300434 Validation Loss: 0.768361508846283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3268: Training Loss: 0.3284006218115489 Validation Loss: 0.7684890031814575\n",
      "Epoch 3269: Training Loss: 0.3299056490262349 Validation Loss: 0.768480658531189\n",
      "Epoch 3270: Training Loss: 0.3293084005514781 Validation Loss: 0.7687103152275085\n",
      "Epoch 3271: Training Loss: 0.32816975315411884 Validation Loss: 0.7688602209091187\n",
      "Epoch 3272: Training Loss: 0.3284302552541097 Validation Loss: 0.7688692808151245\n",
      "Epoch 3273: Training Loss: 0.3281959692637126 Validation Loss: 0.7684637308120728\n",
      "Epoch 3274: Training Loss: 0.3276441792647044 Validation Loss: 0.7686236500740051\n",
      "Epoch 3275: Training Loss: 0.3275248209635417 Validation Loss: 0.7688317894935608\n",
      "Epoch 3276: Training Loss: 0.32856622338294983 Validation Loss: 0.7686139345169067\n",
      "Epoch 3277: Training Loss: 0.3284351130326589 Validation Loss: 0.7686178088188171\n",
      "Epoch 3278: Training Loss: 0.3279637098312378 Validation Loss: 0.7687034606933594\n",
      "Epoch 3279: Training Loss: 0.3267507453759511 Validation Loss: 0.7682292461395264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3280: Training Loss: 0.32713500658671063 Validation Loss: 0.7677624225616455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3281: Training Loss: 0.32738526662190753 Validation Loss: 0.7672903537750244\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3282: Training Loss: 0.32657793164253235 Validation Loss: 0.7671619057655334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3283: Training Loss: 0.32741092642148334 Validation Loss: 0.7669702768325806\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3284: Training Loss: 0.3265083233515422 Validation Loss: 0.7671053409576416\n",
      "Epoch 3285: Training Loss: 0.32555245359738666 Validation Loss: 0.7668637633323669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3286: Training Loss: 0.32613324125607807 Validation Loss: 0.767255961894989\n",
      "Epoch 3287: Training Loss: 0.3261968692143758 Validation Loss: 0.7672016620635986\n",
      "Epoch 3288: Training Loss: 0.3259918689727783 Validation Loss: 0.767311692237854\n",
      "Epoch 3289: Training Loss: 0.32690614461898804 Validation Loss: 0.7673063278198242\n",
      "Epoch 3290: Training Loss: 0.3264991541703542 Validation Loss: 0.767006516456604\n",
      "Epoch 3291: Training Loss: 0.32454270124435425 Validation Loss: 0.7665069103240967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3292: Training Loss: 0.32562466462453205 Validation Loss: 0.7666966319084167\n",
      "Epoch 3293: Training Loss: 0.3255445857842763 Validation Loss: 0.7666944861412048\n",
      "Epoch 3294: Training Loss: 0.32547349731127423 Validation Loss: 0.767342746257782\n",
      "Epoch 3295: Training Loss: 0.32533466815948486 Validation Loss: 0.7673861384391785\n",
      "Epoch 3296: Training Loss: 0.32518141468365985 Validation Loss: 0.7667208313941956\n",
      "Epoch 3297: Training Loss: 0.3240468005339305 Validation Loss: 0.7659813761711121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3298: Training Loss: 0.32526567578315735 Validation Loss: 0.7664278745651245\n",
      "Epoch 3299: Training Loss: 0.3246177136898041 Validation Loss: 0.7662427425384521\n",
      "Epoch 3300: Training Loss: 0.3247482677300771 Validation Loss: 0.7664838433265686\n",
      "Epoch 3301: Training Loss: 0.32472192247708637 Validation Loss: 0.7663150429725647\n",
      "Epoch 3302: Training Loss: 0.32476287086804706 Validation Loss: 0.7660316228866577\n",
      "Epoch 3303: Training Loss: 0.3251255353291829 Validation Loss: 0.766235888004303\n",
      "Epoch 3304: Training Loss: 0.3252141773700714 Validation Loss: 0.7667188048362732\n",
      "Epoch 3305: Training Loss: 0.32486414909362793 Validation Loss: 0.7665777802467346\n",
      "Epoch 3306: Training Loss: 0.32376184066136676 Validation Loss: 0.7663436532020569\n",
      "Epoch 3307: Training Loss: 0.32393039266268414 Validation Loss: 0.7655445337295532\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3308: Training Loss: 0.3237266143163045 Validation Loss: 0.7655153870582581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3309: Training Loss: 0.3232472042242686 Validation Loss: 0.7654288411140442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3310: Training Loss: 0.3236878017584483 Validation Loss: 0.7656374573707581\n",
      "Epoch 3311: Training Loss: 0.3232647180557251 Validation Loss: 0.7655428051948547\n",
      "Epoch 3312: Training Loss: 0.3232367734114329 Validation Loss: 0.7651634216308594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3313: Training Loss: 0.3229497969150543 Validation Loss: 0.7653630971908569\n",
      "Epoch 3314: Training Loss: 0.3227955202261607 Validation Loss: 0.7653979063034058\n",
      "Epoch 3315: Training Loss: 0.322977473338445 Validation Loss: 0.7653520107269287\n",
      "Epoch 3316: Training Loss: 0.3231312731901805 Validation Loss: 0.7651232481002808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3317: Training Loss: 0.3220403989156087 Validation Loss: 0.7651680707931519\n",
      "Epoch 3318: Training Loss: 0.3225021958351135 Validation Loss: 0.765214741230011\n",
      "Epoch 3319: Training Loss: 0.3227890928586324 Validation Loss: 0.7651277184486389\n",
      "Epoch 3320: Training Loss: 0.3222096562385559 Validation Loss: 0.7653735280036926\n",
      "Epoch 3321: Training Loss: 0.3216826021671295 Validation Loss: 0.7651028633117676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3322: Training Loss: 0.32178254922231037 Validation Loss: 0.7646764516830444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3323: Training Loss: 0.32052069902420044 Validation Loss: 0.7647067904472351\n",
      "Epoch 3324: Training Loss: 0.3228913148244222 Validation Loss: 0.7645125985145569\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3325: Training Loss: 0.3237527310848236 Validation Loss: 0.7643265724182129\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3326: Training Loss: 0.32143041491508484 Validation Loss: 0.7643152475357056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3327: Training Loss: 0.3216785987218221 Validation Loss: 0.7643297910690308\n",
      "Epoch 3328: Training Loss: 0.32146644592285156 Validation Loss: 0.7642614841461182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3329: Training Loss: 0.32095526655515033 Validation Loss: 0.7643927931785583\n",
      "Epoch 3330: Training Loss: 0.32076042890548706 Validation Loss: 0.7644786834716797\n",
      "Epoch 3331: Training Loss: 0.32090772191683453 Validation Loss: 0.7646389603614807\n",
      "Epoch 3332: Training Loss: 0.32102714975674945 Validation Loss: 0.7643566131591797\n",
      "Epoch 3333: Training Loss: 0.32024141152699787 Validation Loss: 0.7639273405075073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3334: Training Loss: 0.32101237773895264 Validation Loss: 0.7634868621826172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3335: Training Loss: 0.320430467526118 Validation Loss: 0.7636737823486328\n",
      "Epoch 3336: Training Loss: 0.3190540373325348 Validation Loss: 0.764123797416687\n",
      "Epoch 3337: Training Loss: 0.32084975639979046 Validation Loss: 0.7644149661064148\n",
      "Epoch 3338: Training Loss: 0.321296493212382 Validation Loss: 0.7640058994293213\n",
      "Epoch 3339: Training Loss: 0.3203447659810384 Validation Loss: 0.7635176777839661\n",
      "Epoch 3340: Training Loss: 0.3191625773906708 Validation Loss: 0.7634947896003723\n",
      "Epoch 3341: Training Loss: 0.31950873136520386 Validation Loss: 0.7638119459152222\n",
      "Epoch 3342: Training Loss: 0.31954599420229596 Validation Loss: 0.7634058594703674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3343: Training Loss: 0.32045095165570575 Validation Loss: 0.763134777545929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3344: Training Loss: 0.3198978106180827 Validation Loss: 0.7630404829978943\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3345: Training Loss: 0.3196309606234233 Validation Loss: 0.7630125880241394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3346: Training Loss: 0.31912436087926227 Validation Loss: 0.7631018161773682\n",
      "Epoch 3347: Training Loss: 0.31896574298540753 Validation Loss: 0.7628024816513062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3348: Training Loss: 0.3185194730758667 Validation Loss: 0.762884795665741\n",
      "Epoch 3349: Training Loss: 0.319336861371994 Validation Loss: 0.7629633545875549\n",
      "Epoch 3350: Training Loss: 0.31884045402208966 Validation Loss: 0.762831449508667\n",
      "Epoch 3351: Training Loss: 0.31963016589482623 Validation Loss: 0.7628913521766663\n",
      "Epoch 3352: Training Loss: 0.31873422861099243 Validation Loss: 0.7626628875732422\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3353: Training Loss: 0.3185032705465953 Validation Loss: 0.7628118991851807\n",
      "Epoch 3354: Training Loss: 0.3183927635351817 Validation Loss: 0.7628020644187927\n",
      "Epoch 3355: Training Loss: 0.31782756249109906 Validation Loss: 0.7626506686210632\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3356: Training Loss: 0.3183075487613678 Validation Loss: 0.7629467248916626\n",
      "Epoch 3357: Training Loss: 0.3181885778903961 Validation Loss: 0.7628803253173828\n",
      "Epoch 3358: Training Loss: 0.3190903663635254 Validation Loss: 0.7622111439704895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3359: Training Loss: 0.31808440883954364 Validation Loss: 0.7618609070777893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3360: Training Loss: 0.31723305583000183 Validation Loss: 0.7614943385124207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3361: Training Loss: 0.3169131875038147 Validation Loss: 0.7618175148963928\n",
      "Epoch 3362: Training Loss: 0.3171110947926839 Validation Loss: 0.7621362805366516\n",
      "Epoch 3363: Training Loss: 0.3172521193822225 Validation Loss: 0.7620532512664795\n",
      "Epoch 3364: Training Loss: 0.317448357741038 Validation Loss: 0.7622354626655579\n",
      "Epoch 3365: Training Loss: 0.3169400195280711 Validation Loss: 0.7621236443519592\n",
      "Epoch 3366: Training Loss: 0.3173218369483948 Validation Loss: 0.7617244124412537\n",
      "Epoch 3367: Training Loss: 0.31668223937352497 Validation Loss: 0.7618745565414429\n",
      "Epoch 3368: Training Loss: 0.3165583511193593 Validation Loss: 0.7616603374481201\n",
      "Epoch 3369: Training Loss: 0.31675873200098675 Validation Loss: 0.7616442441940308\n",
      "Epoch 3370: Training Loss: 0.3161381185054779 Validation Loss: 0.7618420720100403\n",
      "Epoch 3371: Training Loss: 0.31718026598294574 Validation Loss: 0.7618909478187561\n",
      "Epoch 3372: Training Loss: 0.31644781430562335 Validation Loss: 0.7617348432540894\n",
      "Epoch 3373: Training Loss: 0.3157651921113332 Validation Loss: 0.7616784572601318\n",
      "Epoch 3374: Training Loss: 0.31694452961285907 Validation Loss: 0.7615823745727539\n",
      "Epoch 3375: Training Loss: 0.31550615032513935 Validation Loss: 0.7611749172210693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3376: Training Loss: 0.31751535336176556 Validation Loss: 0.7610234022140503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3377: Training Loss: 0.31572670737902325 Validation Loss: 0.7613072991371155\n",
      "Epoch 3378: Training Loss: 0.31899266441663104 Validation Loss: 0.7611595988273621\n",
      "Epoch 3379: Training Loss: 0.31561046838760376 Validation Loss: 0.7609809041023254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3380: Training Loss: 0.31493134299914044 Validation Loss: 0.7607795596122742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3381: Training Loss: 0.31699661413828534 Validation Loss: 0.7608368992805481\n",
      "Epoch 3382: Training Loss: 0.31574906905492145 Validation Loss: 0.7607683539390564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3383: Training Loss: 0.314275989929835 Validation Loss: 0.7607576251029968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3384: Training Loss: 0.3146218756834666 Validation Loss: 0.7608036994934082\n",
      "Epoch 3385: Training Loss: 0.31394187609354657 Validation Loss: 0.7606542110443115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3386: Training Loss: 0.3145587146282196 Validation Loss: 0.7608953714370728\n",
      "Epoch 3387: Training Loss: 0.31533703207969666 Validation Loss: 0.7606163620948792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3388: Training Loss: 0.3143696089585622 Validation Loss: 0.7605273723602295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3389: Training Loss: 0.31413499514261883 Validation Loss: 0.7602608799934387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3390: Training Loss: 0.3146149714787801 Validation Loss: 0.7597618103027344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3391: Training Loss: 0.3146256407101949 Validation Loss: 0.7596895694732666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3392: Training Loss: 0.31386516491572064 Validation Loss: 0.7599206566810608\n",
      "Epoch 3393: Training Loss: 0.3148578703403473 Validation Loss: 0.7600899934768677\n",
      "Epoch 3394: Training Loss: 0.31316906213760376 Validation Loss: 0.760287880897522\n",
      "Epoch 3395: Training Loss: 0.313605139652888 Validation Loss: 0.7605983018875122\n",
      "Epoch 3396: Training Loss: 0.3138710558414459 Validation Loss: 0.760500431060791\n",
      "Epoch 3397: Training Loss: 0.312905490398407 Validation Loss: 0.7602733969688416\n",
      "Epoch 3398: Training Loss: 0.31400953729947406 Validation Loss: 0.7601933479309082\n",
      "Epoch 3399: Training Loss: 0.3139795164267222 Validation Loss: 0.7598410248756409\n",
      "Epoch 3400: Training Loss: 0.31423378984133404 Validation Loss: 0.7595182061195374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3401: Training Loss: 0.3127674659093221 Validation Loss: 0.7596765756607056\n",
      "Epoch 3402: Training Loss: 0.3127252161502838 Validation Loss: 0.7597846388816833\n",
      "Epoch 3403: Training Loss: 0.3131277362505595 Validation Loss: 0.7596719861030579\n",
      "Epoch 3404: Training Loss: 0.3110971252123515 Validation Loss: 0.7592042088508606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3405: Training Loss: 0.312435120344162 Validation Loss: 0.7590288519859314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3406: Training Loss: 0.3129802644252777 Validation Loss: 0.7591690421104431\n",
      "Epoch 3407: Training Loss: 0.311922550201416 Validation Loss: 0.7593516707420349\n",
      "Epoch 3408: Training Loss: 0.31186042229334515 Validation Loss: 0.7591994404792786\n",
      "Epoch 3409: Training Loss: 0.31393012404441833 Validation Loss: 0.7591301202774048\n",
      "Epoch 3410: Training Loss: 0.3118687669436137 Validation Loss: 0.7591531872749329\n",
      "Epoch 3411: Training Loss: 0.31153390804926556 Validation Loss: 0.7592059373855591\n",
      "Epoch 3412: Training Loss: 0.31140053272247314 Validation Loss: 0.7593267560005188\n",
      "Epoch 3413: Training Loss: 0.3123740553855896 Validation Loss: 0.7592615485191345\n",
      "Epoch 3414: Training Loss: 0.31123678882916767 Validation Loss: 0.7589210867881775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3415: Training Loss: 0.3112509449323018 Validation Loss: 0.7586930394172668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3416: Training Loss: 0.3115205566088359 Validation Loss: 0.7590449452400208\n",
      "Epoch 3417: Training Loss: 0.31122859319051105 Validation Loss: 0.758609414100647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3418: Training Loss: 0.31096575657526654 Validation Loss: 0.7588211297988892\n",
      "Epoch 3419: Training Loss: 0.3108721673488617 Validation Loss: 0.7582868337631226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3420: Training Loss: 0.3111860652764638 Validation Loss: 0.7581140398979187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3421: Training Loss: 0.3110014895598094 Validation Loss: 0.7579451203346252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3422: Training Loss: 0.3128550847371419 Validation Loss: 0.7580869197845459\n",
      "Epoch 3423: Training Loss: 0.31061017513275146 Validation Loss: 0.7584335207939148\n",
      "Epoch 3424: Training Loss: 0.3097466826438904 Validation Loss: 0.7586029171943665\n",
      "Epoch 3425: Training Loss: 0.3100239634513855 Validation Loss: 0.7586825489997864\n",
      "Epoch 3426: Training Loss: 0.31011001269022626 Validation Loss: 0.758191704750061\n",
      "Epoch 3427: Training Loss: 0.31011895338694256 Validation Loss: 0.7577013373374939\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3428: Training Loss: 0.30956631898880005 Validation Loss: 0.7575936913490295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3429: Training Loss: 0.3096892436345418 Validation Loss: 0.7576696276664734\n",
      "Epoch 3430: Training Loss: 0.30966299772262573 Validation Loss: 0.7578015923500061\n",
      "Epoch 3431: Training Loss: 0.30969417095184326 Validation Loss: 0.7578779458999634\n",
      "Epoch 3432: Training Loss: 0.3095823625723521 Validation Loss: 0.757926881313324\n",
      "Epoch 3433: Training Loss: 0.30995115637779236 Validation Loss: 0.757798969745636\n",
      "Epoch 3434: Training Loss: 0.30902565519014996 Validation Loss: 0.7575485706329346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3435: Training Loss: 0.30978543559710187 Validation Loss: 0.7574276924133301\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3436: Training Loss: 0.30909451842308044 Validation Loss: 0.75777667760849\n",
      "Epoch 3437: Training Loss: 0.30870217084884644 Validation Loss: 0.757740318775177\n",
      "Epoch 3438: Training Loss: 0.30837753415107727 Validation Loss: 0.757748544216156\n",
      "Epoch 3439: Training Loss: 0.30912815531094867 Validation Loss: 0.7576702833175659\n",
      "Epoch 3440: Training Loss: 0.30786146720250446 Validation Loss: 0.7571993470191956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3441: Training Loss: 0.3090684910615285 Validation Loss: 0.757157027721405\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3442: Training Loss: 0.3090078830718994 Validation Loss: 0.7571274638175964\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3443: Training Loss: 0.30844002962112427 Validation Loss: 0.7570037841796875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3444: Training Loss: 0.30808260043462116 Validation Loss: 0.7570289373397827\n",
      "Epoch 3445: Training Loss: 0.30782867471377057 Validation Loss: 0.7565658688545227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3446: Training Loss: 0.3078225950400035 Validation Loss: 0.7564359307289124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3447: Training Loss: 0.3080388406912486 Validation Loss: 0.7564083337783813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3448: Training Loss: 0.3076813220977783 Validation Loss: 0.7568309307098389\n",
      "Epoch 3449: Training Loss: 0.30775462587674457 Validation Loss: 0.756828784942627\n",
      "Epoch 3450: Training Loss: 0.30862100919087726 Validation Loss: 0.7569268345832825\n",
      "Epoch 3451: Training Loss: 0.30691150824228924 Validation Loss: 0.756687581539154\n",
      "Epoch 3452: Training Loss: 0.30775758624076843 Validation Loss: 0.7565076947212219\n",
      "Epoch 3453: Training Loss: 0.3061860700448354 Validation Loss: 0.7563227415084839\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3454: Training Loss: 0.30725377798080444 Validation Loss: 0.7562570571899414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3455: Training Loss: 0.3074805935223897 Validation Loss: 0.7561123371124268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3456: Training Loss: 0.3067721426486969 Validation Loss: 0.7560827136039734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3457: Training Loss: 0.3066365122795105 Validation Loss: 0.7559400200843811\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3458: Training Loss: 0.30663735667864483 Validation Loss: 0.7559934258460999\n",
      "Epoch 3459: Training Loss: 0.3064030607541402 Validation Loss: 0.7564608454704285\n",
      "Epoch 3460: Training Loss: 0.3062565227349599 Validation Loss: 0.7564930319786072\n",
      "Epoch 3461: Training Loss: 0.3065353333950043 Validation Loss: 0.7565127015113831\n",
      "Epoch 3462: Training Loss: 0.30648888150850934 Validation Loss: 0.7560113668441772\n",
      "Epoch 3463: Training Loss: 0.3055872817834218 Validation Loss: 0.7557570934295654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3464: Training Loss: 0.3057152032852173 Validation Loss: 0.7555954456329346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3465: Training Loss: 0.3060148060321808 Validation Loss: 0.7557312846183777\n",
      "Epoch 3466: Training Loss: 0.3072485327720642 Validation Loss: 0.7557947635650635\n",
      "Epoch 3467: Training Loss: 0.30729928612709045 Validation Loss: 0.7555655837059021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3468: Training Loss: 0.30706076820691425 Validation Loss: 0.7559460997581482\n",
      "Epoch 3469: Training Loss: 0.305313378572464 Validation Loss: 0.7557275295257568\n",
      "Epoch 3470: Training Loss: 0.30527806282043457 Validation Loss: 0.7557889223098755\n",
      "Epoch 3471: Training Loss: 0.3054280976454417 Validation Loss: 0.7557475566864014\n",
      "Epoch 3472: Training Loss: 0.3060426910718282 Validation Loss: 0.7553891539573669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3473: Training Loss: 0.3055733342965444 Validation Loss: 0.7550455927848816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3474: Training Loss: 0.3045280973116557 Validation Loss: 0.7550644278526306\n",
      "Epoch 3475: Training Loss: 0.3050060172875722 Validation Loss: 0.7552921175956726\n",
      "Epoch 3476: Training Loss: 0.3045291503270467 Validation Loss: 0.7552614212036133\n",
      "Epoch 3477: Training Loss: 0.30450759331385296 Validation Loss: 0.75540691614151\n",
      "Epoch 3478: Training Loss: 0.3065265516440074 Validation Loss: 0.7554512023925781\n",
      "Epoch 3479: Training Loss: 0.3052980601787567 Validation Loss: 0.7547670006752014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3480: Training Loss: 0.30491485198338825 Validation Loss: 0.7545304298400879\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3481: Training Loss: 0.30526726444562274 Validation Loss: 0.7546958923339844\n",
      "Epoch 3482: Training Loss: 0.3037618100643158 Validation Loss: 0.7547912001609802\n",
      "Epoch 3483: Training Loss: 0.3036920726299286 Validation Loss: 0.7548253536224365\n",
      "Epoch 3484: Training Loss: 0.30509419242540997 Validation Loss: 0.7548725008964539\n",
      "Epoch 3485: Training Loss: 0.3037683566411336 Validation Loss: 0.7545787692070007\n",
      "Epoch 3486: Training Loss: 0.3036944369475047 Validation Loss: 0.7544834017753601\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3487: Training Loss: 0.3039620717366536 Validation Loss: 0.754268229007721\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3488: Training Loss: 0.30331175525983173 Validation Loss: 0.7539966702461243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3489: Training Loss: 0.3053575058778127 Validation Loss: 0.7545182704925537\n",
      "Epoch 3490: Training Loss: 0.3029613296190898 Validation Loss: 0.7545498609542847\n",
      "Epoch 3491: Training Loss: 0.3028407593568166 Validation Loss: 0.7546282410621643\n",
      "Epoch 3492: Training Loss: 0.3055746555328369 Validation Loss: 0.7542242407798767\n",
      "Epoch 3493: Training Loss: 0.3031911055246989 Validation Loss: 0.7541630268096924\n",
      "Epoch 3494: Training Loss: 0.30304141839345294 Validation Loss: 0.7541057467460632\n",
      "Epoch 3495: Training Loss: 0.30245456099510193 Validation Loss: 0.754218339920044\n",
      "Epoch 3496: Training Loss: 0.3033418854077657 Validation Loss: 0.7537671327590942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3497: Training Loss: 0.3023032446702321 Validation Loss: 0.7538348436355591\n",
      "Epoch 3498: Training Loss: 0.3024004598458608 Validation Loss: 0.7539771795272827\n",
      "Epoch 3499: Training Loss: 0.3020467758178711 Validation Loss: 0.7541863322257996\n",
      "Epoch 3500: Training Loss: 0.3037327627340953 Validation Loss: 0.7540584206581116\n",
      "Epoch 3501: Training Loss: 0.30196766058603924 Validation Loss: 0.7542676329612732\n",
      "Epoch 3502: Training Loss: 0.30160272121429443 Validation Loss: 0.7539811730384827\n",
      "Epoch 3503: Training Loss: 0.30132636427879333 Validation Loss: 0.753709077835083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3504: Training Loss: 0.3015422423680623 Validation Loss: 0.7535986304283142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3505: Training Loss: 0.3016193409760793 Validation Loss: 0.7536683678627014\n",
      "Epoch 3506: Training Loss: 0.3031059205532074 Validation Loss: 0.7535269856452942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3507: Training Loss: 0.3019103507200877 Validation Loss: 0.7536444067955017\n",
      "Epoch 3508: Training Loss: 0.30311477184295654 Validation Loss: 0.753578782081604\n",
      "Epoch 3509: Training Loss: 0.30094858010609943 Validation Loss: 0.7533178329467773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3510: Training Loss: 0.301941454410553 Validation Loss: 0.7529844641685486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3511: Training Loss: 0.30107519030570984 Validation Loss: 0.7526881098747253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3512: Training Loss: 0.30094874898592633 Validation Loss: 0.7530893087387085\n",
      "Epoch 3513: Training Loss: 0.3014746109644572 Validation Loss: 0.753210186958313\n",
      "Epoch 3514: Training Loss: 0.3001680374145508 Validation Loss: 0.7530519962310791\n",
      "Epoch 3515: Training Loss: 0.30107008417447406 Validation Loss: 0.7528504729270935\n",
      "Epoch 3516: Training Loss: 0.30016422271728516 Validation Loss: 0.7526358962059021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3517: Training Loss: 0.3004274070262909 Validation Loss: 0.7527284026145935\n",
      "Epoch 3518: Training Loss: 0.29981979727745056 Validation Loss: 0.7527163624763489\n",
      "Epoch 3519: Training Loss: 0.2999575138092041 Validation Loss: 0.7527598142623901\n",
      "Epoch 3520: Training Loss: 0.3001008729139964 Validation Loss: 0.7527482509613037\n",
      "Epoch 3521: Training Loss: 0.3004776934782664 Validation Loss: 0.7526451349258423\n",
      "Epoch 3522: Training Loss: 0.2995231548945109 Validation Loss: 0.7525668144226074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3523: Training Loss: 0.2995024522145589 Validation Loss: 0.752365231513977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3524: Training Loss: 0.29932523767153424 Validation Loss: 0.7523156404495239\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3525: Training Loss: 0.2993825475374858 Validation Loss: 0.7521215677261353\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3526: Training Loss: 0.29971914490063983 Validation Loss: 0.7521390914916992\n",
      "Epoch 3527: Training Loss: 0.29889734586079914 Validation Loss: 0.7519423365592957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3528: Training Loss: 0.2986819843451182 Validation Loss: 0.7517847418785095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3529: Training Loss: 0.29871800541877747 Validation Loss: 0.7521154284477234\n",
      "Epoch 3530: Training Loss: 0.2989034156004588 Validation Loss: 0.752265214920044\n",
      "Epoch 3531: Training Loss: 0.29899002114931744 Validation Loss: 0.752306342124939\n",
      "Epoch 3532: Training Loss: 0.29903939366340637 Validation Loss: 0.7521119117736816\n",
      "Epoch 3533: Training Loss: 0.29816200335820514 Validation Loss: 0.7522451877593994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3534: Training Loss: 0.29841817418734234 Validation Loss: 0.7519968152046204\n",
      "Epoch 3535: Training Loss: 0.29907886187235516 Validation Loss: 0.7517634034156799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3536: Training Loss: 0.29819950461387634 Validation Loss: 0.7517132759094238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3537: Training Loss: 0.29895153641700745 Validation Loss: 0.7514001131057739\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3538: Training Loss: 0.29853949944178265 Validation Loss: 0.7513723969459534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3539: Training Loss: 0.29956353704134625 Validation Loss: 0.7514044642448425\n",
      "Epoch 3540: Training Loss: 0.297799547513326 Validation Loss: 0.7513560652732849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3541: Training Loss: 0.2991427530845006 Validation Loss: 0.7512972950935364\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3542: Training Loss: 0.29792433977127075 Validation Loss: 0.7512732148170471\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3543: Training Loss: 0.2980349063873291 Validation Loss: 0.751236617565155\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3544: Training Loss: 0.2977546751499176 Validation Loss: 0.7513230443000793\n",
      "Epoch 3545: Training Loss: 0.2974763611952464 Validation Loss: 0.7515998482704163\n",
      "Epoch 3546: Training Loss: 0.2970421314239502 Validation Loss: 0.7519758939743042\n",
      "Epoch 3547: Training Loss: 0.2969187796115875 Validation Loss: 0.7517337203025818\n",
      "Epoch 3548: Training Loss: 0.29747384786605835 Validation Loss: 0.7515723705291748\n",
      "Epoch 3549: Training Loss: 0.2978740135828654 Validation Loss: 0.7515507340431213\n",
      "Epoch 3550: Training Loss: 0.29797248045603436 Validation Loss: 0.7513009309768677\n",
      "Epoch 3551: Training Loss: 0.2966790596644084 Validation Loss: 0.7508338093757629\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3552: Training Loss: 0.2967722217241923 Validation Loss: 0.7504127621650696\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3553: Training Loss: 0.29762670397758484 Validation Loss: 0.7503383755683899\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3554: Training Loss: 0.29900171359380084 Validation Loss: 0.750348687171936\n",
      "Epoch 3555: Training Loss: 0.29609933495521545 Validation Loss: 0.7503722310066223\n",
      "Epoch 3556: Training Loss: 0.29606138666470844 Validation Loss: 0.7506417036056519\n",
      "Epoch 3557: Training Loss: 0.2979795386393865 Validation Loss: 0.7506391406059265\n",
      "Epoch 3558: Training Loss: 0.2960842748483022 Validation Loss: 0.750700831413269\n",
      "Epoch 3559: Training Loss: 0.29554808139801025 Validation Loss: 0.7508065104484558\n",
      "Epoch 3560: Training Loss: 0.29551459352175397 Validation Loss: 0.7506448030471802\n",
      "Epoch 3561: Training Loss: 0.29555147886276245 Validation Loss: 0.7504032850265503\n",
      "Epoch 3562: Training Loss: 0.2956608533859253 Validation Loss: 0.7504594326019287\n",
      "Epoch 3563: Training Loss: 0.2968280017375946 Validation Loss: 0.7508702278137207\n",
      "Epoch 3564: Training Loss: 0.2961355745792389 Validation Loss: 0.7505982518196106\n",
      "Epoch 3565: Training Loss: 0.29506536324818927 Validation Loss: 0.7502719163894653\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3566: Training Loss: 0.29524317383766174 Validation Loss: 0.7499540448188782\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3567: Training Loss: 0.29619984825452167 Validation Loss: 0.749830424785614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3568: Training Loss: 0.2976603905359904 Validation Loss: 0.7494056820869446\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3569: Training Loss: 0.29468711217244464 Validation Loss: 0.7495893836021423\n",
      "Epoch 3570: Training Loss: 0.29504350821177167 Validation Loss: 0.7498978972434998\n",
      "Epoch 3571: Training Loss: 0.29452915986378986 Validation Loss: 0.7497473359107971\n",
      "Epoch 3572: Training Loss: 0.29484012722969055 Validation Loss: 0.7498259544372559\n",
      "Epoch 3573: Training Loss: 0.29448293646176654 Validation Loss: 0.7497519254684448\n",
      "Epoch 3574: Training Loss: 0.2981211245059967 Validation Loss: 0.7498651146888733\n",
      "Epoch 3575: Training Loss: 0.2946530282497406 Validation Loss: 0.7498635649681091\n",
      "Epoch 3576: Training Loss: 0.29366671045621234 Validation Loss: 0.7495267391204834\n",
      "Epoch 3577: Training Loss: 0.2940860092639923 Validation Loss: 0.7496764659881592\n",
      "Epoch 3578: Training Loss: 0.294402539730072 Validation Loss: 0.7496033310890198\n",
      "Epoch 3579: Training Loss: 0.29348985354105633 Validation Loss: 0.7498654723167419\n",
      "Epoch 3580: Training Loss: 0.29414469997088116 Validation Loss: 0.7497321963310242\n",
      "Epoch 3581: Training Loss: 0.2940300206343333 Validation Loss: 0.7496610879898071\n",
      "Epoch 3582: Training Loss: 0.29469481110572815 Validation Loss: 0.7497525811195374\n",
      "Epoch 3583: Training Loss: 0.2932222882906596 Validation Loss: 0.7499220371246338\n",
      "Epoch 3584: Training Loss: 0.2930551568667094 Validation Loss: 0.7495360374450684\n",
      "Epoch 3585: Training Loss: 0.2932841380437215 Validation Loss: 0.749339759349823\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3586: Training Loss: 0.29340145985285443 Validation Loss: 0.7492783069610596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3587: Training Loss: 0.2930659254391988 Validation Loss: 0.749104380607605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3588: Training Loss: 0.2930177052815755 Validation Loss: 0.7492275834083557\n",
      "Epoch 3589: Training Loss: 0.29302850365638733 Validation Loss: 0.7494558095932007\n",
      "Epoch 3590: Training Loss: 0.293127844731013 Validation Loss: 0.7490898966789246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3591: Training Loss: 0.2931225697199504 Validation Loss: 0.7486825585365295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3592: Training Loss: 0.2927345037460327 Validation Loss: 0.7483647465705872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3593: Training Loss: 0.2923171619574229 Validation Loss: 0.7484679818153381\n",
      "Epoch 3594: Training Loss: 0.2930520276228587 Validation Loss: 0.7485458850860596\n",
      "Epoch 3595: Training Loss: 0.29405222336451214 Validation Loss: 0.7485939860343933\n",
      "Epoch 3596: Training Loss: 0.2917302946249644 Validation Loss: 0.7488015294075012\n",
      "Epoch 3597: Training Loss: 0.2921871344248454 Validation Loss: 0.7487999200820923\n",
      "Epoch 3598: Training Loss: 0.2935163080692291 Validation Loss: 0.7485721111297607\n",
      "Epoch 3599: Training Loss: 0.29238247871398926 Validation Loss: 0.7487497329711914\n",
      "Epoch 3600: Training Loss: 0.29169047872225445 Validation Loss: 0.7487092018127441\n",
      "Epoch 3601: Training Loss: 0.2931961864233017 Validation Loss: 0.7489787936210632\n",
      "Epoch 3602: Training Loss: 0.2915387650330861 Validation Loss: 0.7490155696868896\n",
      "Epoch 3603: Training Loss: 0.29135135809580487 Validation Loss: 0.7488579750061035\n",
      "Epoch 3604: Training Loss: 0.29066986838976544 Validation Loss: 0.7486224174499512\n",
      "Epoch 3605: Training Loss: 0.2912222345670064 Validation Loss: 0.7480500936508179\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3606: Training Loss: 0.29213885466257733 Validation Loss: 0.7477648258209229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3607: Training Loss: 0.2908882200717926 Validation Loss: 0.7478505373001099\n",
      "Epoch 3608: Training Loss: 0.29092614849408466 Validation Loss: 0.7476957440376282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3609: Training Loss: 0.2910909156004588 Validation Loss: 0.7477734684944153\n",
      "Epoch 3610: Training Loss: 0.2906932532787323 Validation Loss: 0.7479703426361084\n",
      "Epoch 3611: Training Loss: 0.290970782438914 Validation Loss: 0.7478108406066895\n",
      "Epoch 3612: Training Loss: 0.2916645606358846 Validation Loss: 0.7478087544441223\n",
      "Epoch 3613: Training Loss: 0.2904820342858632 Validation Loss: 0.7476889491081238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3614: Training Loss: 0.29078683257102966 Validation Loss: 0.747666597366333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3615: Training Loss: 0.2908948063850403 Validation Loss: 0.7476846575737\n",
      "Epoch 3616: Training Loss: 0.2905399004618327 Validation Loss: 0.7477992177009583\n",
      "Epoch 3617: Training Loss: 0.28979069987932843 Validation Loss: 0.7479962706565857\n",
      "Epoch 3618: Training Loss: 0.2899496754010518 Validation Loss: 0.7476977109909058\n",
      "Epoch 3619: Training Loss: 0.2897828320662181 Validation Loss: 0.7474491596221924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3620: Training Loss: 0.2899594207604726 Validation Loss: 0.747223436832428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3621: Training Loss: 0.2896180550257365 Validation Loss: 0.7471866607666016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3622: Training Loss: 0.2898411750793457 Validation Loss: 0.7473835349082947\n",
      "Epoch 3623: Training Loss: 0.28890615701675415 Validation Loss: 0.7474234700202942\n",
      "Epoch 3624: Training Loss: 0.28947291771570843 Validation Loss: 0.7475575804710388\n",
      "Epoch 3625: Training Loss: 0.288655420144399 Validation Loss: 0.7470707297325134\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3626: Training Loss: 0.2895225981871287 Validation Loss: 0.7471135258674622\n",
      "Epoch 3627: Training Loss: 0.28899626930554706 Validation Loss: 0.7471836805343628\n",
      "Epoch 3628: Training Loss: 0.28877973556518555 Validation Loss: 0.7468335032463074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3629: Training Loss: 0.28916727503140766 Validation Loss: 0.7466470003128052\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3630: Training Loss: 0.2888096074263255 Validation Loss: 0.7468376755714417\n",
      "Epoch 3631: Training Loss: 0.2898911237716675 Validation Loss: 0.7470167875289917\n",
      "Epoch 3632: Training Loss: 0.28935712575912476 Validation Loss: 0.7472072243690491\n",
      "Epoch 3633: Training Loss: 0.2891365587711334 Validation Loss: 0.747029721736908\n",
      "Epoch 3634: Training Loss: 0.2886366844177246 Validation Loss: 0.7469302415847778\n",
      "Epoch 3635: Training Loss: 0.2887202203273773 Validation Loss: 0.7467752695083618\n",
      "Epoch 3636: Training Loss: 0.2894319345553716 Validation Loss: 0.7468443512916565\n",
      "Epoch 3637: Training Loss: 0.2880196472009023 Validation Loss: 0.7469266653060913\n",
      "Epoch 3638: Training Loss: 0.28874553243319195 Validation Loss: 0.7469143867492676\n",
      "Epoch 3639: Training Loss: 0.2888047794500987 Validation Loss: 0.7471628785133362\n",
      "Epoch 3640: Training Loss: 0.288014551003774 Validation Loss: 0.7469496726989746\n",
      "Epoch 3641: Training Loss: 0.287576824426651 Validation Loss: 0.7467384934425354\n",
      "Epoch 3642: Training Loss: 0.2888478934764862 Validation Loss: 0.7465367317199707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3643: Training Loss: 0.2886870304743449 Validation Loss: 0.7465736269950867\n",
      "Epoch 3644: Training Loss: 0.28747130433718365 Validation Loss: 0.7463542222976685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3645: Training Loss: 0.2882462243239085 Validation Loss: 0.74617999792099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3646: Training Loss: 0.28716952602068585 Validation Loss: 0.7458458542823792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3647: Training Loss: 0.2896168529987335 Validation Loss: 0.7458708882331848\n",
      "Epoch 3648: Training Loss: 0.28822749853134155 Validation Loss: 0.7458130717277527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3649: Training Loss: 0.286925087372462 Validation Loss: 0.745872974395752\n",
      "Epoch 3650: Training Loss: 0.2868031859397888 Validation Loss: 0.7460269927978516\n",
      "Epoch 3651: Training Loss: 0.2873796025911967 Validation Loss: 0.7463746666908264\n",
      "Epoch 3652: Training Loss: 0.2873287598292033 Validation Loss: 0.7461524605751038\n",
      "Epoch 3653: Training Loss: 0.2860376238822937 Validation Loss: 0.7459760904312134\n",
      "Epoch 3654: Training Loss: 0.2876153190930684 Validation Loss: 0.745768666267395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3655: Training Loss: 0.28625959157943726 Validation Loss: 0.7457773685455322\n",
      "Epoch 3656: Training Loss: 0.2866334915161133 Validation Loss: 0.745642900466919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3657: Training Loss: 0.2863895098368327 Validation Loss: 0.7456228733062744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3658: Training Loss: 0.2857128282388051 Validation Loss: 0.7457112669944763\n",
      "Epoch 3659: Training Loss: 0.2869066496690114 Validation Loss: 0.7456570863723755\n",
      "Epoch 3660: Training Loss: 0.2855990727742513 Validation Loss: 0.745747983455658\n",
      "Epoch 3661: Training Loss: 0.2857720951239268 Validation Loss: 0.7455238103866577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3662: Training Loss: 0.2862333456675212 Validation Loss: 0.7456833720207214\n",
      "Epoch 3663: Training Loss: 0.28582147757212323 Validation Loss: 0.7455154061317444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3664: Training Loss: 0.28638704617818195 Validation Loss: 0.7452723383903503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3665: Training Loss: 0.284972886244456 Validation Loss: 0.7450481057167053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3666: Training Loss: 0.2853422661622365 Validation Loss: 0.7448360919952393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3667: Training Loss: 0.2854414681593577 Validation Loss: 0.7448855638504028\n",
      "Epoch 3668: Training Loss: 0.285261203845342 Validation Loss: 0.7448660135269165\n",
      "Epoch 3669: Training Loss: 0.2851439416408539 Validation Loss: 0.7447530031204224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3670: Training Loss: 0.2860884765783946 Validation Loss: 0.7449957132339478\n",
      "Epoch 3671: Training Loss: 0.28480876485506695 Validation Loss: 0.7450369596481323\n",
      "Epoch 3672: Training Loss: 0.2856249411900838 Validation Loss: 0.7447822690010071\n",
      "Epoch 3673: Training Loss: 0.285676305492719 Validation Loss: 0.7446863651275635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3674: Training Loss: 0.2856000065803528 Validation Loss: 0.7447065711021423\n",
      "Epoch 3675: Training Loss: 0.28467853864034015 Validation Loss: 0.744572103023529\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3676: Training Loss: 0.28699590762456256 Validation Loss: 0.7447282671928406\n",
      "Epoch 3677: Training Loss: 0.2862813472747803 Validation Loss: 0.7450642585754395\n",
      "Epoch 3678: Training Loss: 0.2844560543696086 Validation Loss: 0.7454817891120911\n",
      "Epoch 3679: Training Loss: 0.2857263386249542 Validation Loss: 0.7457764744758606\n",
      "Epoch 3680: Training Loss: 0.2839182913303375 Validation Loss: 0.7451997995376587\n",
      "Epoch 3681: Training Loss: 0.28390801946322125 Validation Loss: 0.7447627186775208\n",
      "Epoch 3682: Training Loss: 0.28472229838371277 Validation Loss: 0.7444103360176086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3683: Training Loss: 0.28530608117580414 Validation Loss: 0.7442612648010254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3684: Training Loss: 0.28395891189575195 Validation Loss: 0.7444450855255127\n",
      "Epoch 3685: Training Loss: 0.28387917081514996 Validation Loss: 0.7447100877761841\n",
      "Epoch 3686: Training Loss: 0.28325216968854267 Validation Loss: 0.744584858417511\n",
      "Epoch 3687: Training Loss: 0.2830433249473572 Validation Loss: 0.7447088360786438\n",
      "Epoch 3688: Training Loss: 0.2835395137468974 Validation Loss: 0.7442333698272705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3689: Training Loss: 0.2826214035352071 Validation Loss: 0.7437576055526733\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3690: Training Loss: 0.28307435909907025 Validation Loss: 0.743992269039154\n",
      "Epoch 3691: Training Loss: 0.2830262879530589 Validation Loss: 0.744162380695343\n",
      "Epoch 3692: Training Loss: 0.28334198395411175 Validation Loss: 0.7443830370903015\n",
      "Epoch 3693: Training Loss: 0.28286896149317425 Validation Loss: 0.7439621090888977\n",
      "Epoch 3694: Training Loss: 0.2830362816651662 Validation Loss: 0.7438169717788696\n",
      "Epoch 3695: Training Loss: 0.28194454809029895 Validation Loss: 0.7438808679580688\n",
      "Epoch 3696: Training Loss: 0.28289371728897095 Validation Loss: 0.7436791062355042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3697: Training Loss: 0.2821315824985504 Validation Loss: 0.7437493801116943\n",
      "Epoch 3698: Training Loss: 0.28241387009620667 Validation Loss: 0.744040310382843\n",
      "Epoch 3699: Training Loss: 0.28223050634066266 Validation Loss: 0.7440013289451599\n",
      "Epoch 3700: Training Loss: 0.28209614753723145 Validation Loss: 0.7436107397079468\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3701: Training Loss: 0.28339925905068714 Validation Loss: 0.7436440587043762\n",
      "Epoch 3702: Training Loss: 0.2821110188961029 Validation Loss: 0.7435389757156372\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3703: Training Loss: 0.28300023078918457 Validation Loss: 0.7439455389976501\n",
      "Epoch 3704: Training Loss: 0.28178004423777264 Validation Loss: 0.7440274357795715\n",
      "Epoch 3705: Training Loss: 0.2818284034729004 Validation Loss: 0.7438315749168396\n",
      "Epoch 3706: Training Loss: 0.2813745637734731 Validation Loss: 0.7436848878860474\n",
      "Epoch 3707: Training Loss: 0.28258732954661053 Validation Loss: 0.7431607842445374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3708: Training Loss: 0.281130313873291 Validation Loss: 0.7432119250297546\n",
      "Epoch 3709: Training Loss: 0.2820339898268382 Validation Loss: 0.7431665658950806\n",
      "Epoch 3710: Training Loss: 0.28147125244140625 Validation Loss: 0.7427680492401123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3711: Training Loss: 0.2813103397687276 Validation Loss: 0.7429710626602173\n",
      "Epoch 3712: Training Loss: 0.28177009026209515 Validation Loss: 0.7427477240562439\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3713: Training Loss: 0.282680203517278 Validation Loss: 0.7427477240562439\n",
      "Epoch 3714: Training Loss: 0.28062619765599567 Validation Loss: 0.7426491379737854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3715: Training Loss: 0.28160033623377484 Validation Loss: 0.7428266406059265\n",
      "Epoch 3716: Training Loss: 0.2803321083386739 Validation Loss: 0.7431929111480713\n",
      "Epoch 3717: Training Loss: 0.2806934018929799 Validation Loss: 0.7431777715682983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3718: Training Loss: 0.28053290645281476 Validation Loss: 0.7430146336555481\n",
      "Epoch 3719: Training Loss: 0.2813611527283986 Validation Loss: 0.742695152759552\n",
      "Epoch 3720: Training Loss: 0.28045259912808734 Validation Loss: 0.7425990104675293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3721: Training Loss: 0.280298372109731 Validation Loss: 0.7425969243049622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3722: Training Loss: 0.28173524141311646 Validation Loss: 0.7424742579460144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3723: Training Loss: 0.28105703989664715 Validation Loss: 0.7428216338157654\n",
      "Epoch 3724: Training Loss: 0.28010039528210956 Validation Loss: 0.742952823638916\n",
      "Epoch 3725: Training Loss: 0.28001242876052856 Validation Loss: 0.7430293560028076\n",
      "Epoch 3726: Training Loss: 0.2796507974465688 Validation Loss: 0.7427673935890198\n",
      "Epoch 3727: Training Loss: 0.2802907923857371 Validation Loss: 0.742417573928833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3728: Training Loss: 0.28223665555318195 Validation Loss: 0.7423833608627319\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3729: Training Loss: 0.279841144879659 Validation Loss: 0.7419306039810181\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3730: Training Loss: 0.2796988288561503 Validation Loss: 0.7421411275863647\n",
      "Epoch 3731: Training Loss: 0.27928707003593445 Validation Loss: 0.7418988943099976\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3732: Training Loss: 0.27936115860939026 Validation Loss: 0.7418481111526489\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3733: Training Loss: 0.27928535143534344 Validation Loss: 0.7418686747550964\n",
      "Epoch 3734: Training Loss: 0.27959489822387695 Validation Loss: 0.7421203255653381\n",
      "Epoch 3735: Training Loss: 0.27944276730219525 Validation Loss: 0.7424049973487854\n",
      "Epoch 3736: Training Loss: 0.2794567247231801 Validation Loss: 0.7426294684410095\n",
      "Epoch 3737: Training Loss: 0.280317356189092 Validation Loss: 0.7425581812858582\n",
      "Epoch 3738: Training Loss: 0.27876031398773193 Validation Loss: 0.7423296570777893\n",
      "Epoch 3739: Training Loss: 0.2801294724146525 Validation Loss: 0.7422671914100647\n",
      "Epoch 3740: Training Loss: 0.2786712944507599 Validation Loss: 0.7421829104423523\n",
      "Epoch 3741: Training Loss: 0.2793219983577728 Validation Loss: 0.7418274879455566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3742: Training Loss: 0.2786579628785451 Validation Loss: 0.741537868976593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3743: Training Loss: 0.2786569992701213 Validation Loss: 0.7415481805801392\n",
      "Epoch 3744: Training Loss: 0.27820180853207904 Validation Loss: 0.7414936423301697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3745: Training Loss: 0.2782619496186574 Validation Loss: 0.7416123747825623\n",
      "Epoch 3746: Training Loss: 0.2786010007063548 Validation Loss: 0.7418678402900696\n",
      "Epoch 3747: Training Loss: 0.2783466378847758 Validation Loss: 0.7420194745063782\n",
      "Epoch 3748: Training Loss: 0.2781857947508494 Validation Loss: 0.7421476244926453\n",
      "Epoch 3749: Training Loss: 0.2782320976257324 Validation Loss: 0.7421911358833313\n",
      "Epoch 3750: Training Loss: 0.27757083376248676 Validation Loss: 0.741734504699707\n",
      "Epoch 3751: Training Loss: 0.27747511863708496 Validation Loss: 0.7412379384040833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3752: Training Loss: 0.27764371037483215 Validation Loss: 0.7409795522689819\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3753: Training Loss: 0.2777336786190669 Validation Loss: 0.7409427762031555\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3754: Training Loss: 0.27755483984947205 Validation Loss: 0.7411605715751648\n",
      "Epoch 3755: Training Loss: 0.2772010763486226 Validation Loss: 0.7412164211273193\n",
      "Epoch 3756: Training Loss: 0.27771035333474475 Validation Loss: 0.741608202457428\n",
      "Epoch 3757: Training Loss: 0.27761757373809814 Validation Loss: 0.7417638301849365\n",
      "Epoch 3758: Training Loss: 0.27724499503771466 Validation Loss: 0.7412065863609314\n",
      "Epoch 3759: Training Loss: 0.276917427778244 Validation Loss: 0.7408431172370911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3760: Training Loss: 0.27727051079273224 Validation Loss: 0.740641176700592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3761: Training Loss: 0.2775154908498128 Validation Loss: 0.7406341433525085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3762: Training Loss: 0.27724597851435345 Validation Loss: 0.7408344745635986\n",
      "Epoch 3763: Training Loss: 0.276665061712265 Validation Loss: 0.7410773634910583\n",
      "Epoch 3764: Training Loss: 0.2761676212151845 Validation Loss: 0.7407707571983337\n",
      "Epoch 3765: Training Loss: 0.27731669942537945 Validation Loss: 0.7404709458351135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3766: Training Loss: 0.2766408522923787 Validation Loss: 0.7405429482460022\n",
      "Epoch 3767: Training Loss: 0.27621997396151227 Validation Loss: 0.7406893968582153\n",
      "Epoch 3768: Training Loss: 0.27607856194178265 Validation Loss: 0.7413434982299805\n",
      "Epoch 3769: Training Loss: 0.27567283312479657 Validation Loss: 0.7412828803062439\n",
      "Epoch 3770: Training Loss: 0.27569685379664105 Validation Loss: 0.7410717010498047\n",
      "Epoch 3771: Training Loss: 0.27638670802116394 Validation Loss: 0.7407461404800415\n",
      "Epoch 3772: Training Loss: 0.27653148770332336 Validation Loss: 0.7406922578811646\n",
      "Epoch 3773: Training Loss: 0.27629772822062176 Validation Loss: 0.7405335903167725\n",
      "Epoch 3774: Training Loss: 0.27576743563016254 Validation Loss: 0.7403765320777893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3775: Training Loss: 0.27495155731836957 Validation Loss: 0.7402362823486328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3776: Training Loss: 0.27534780899683636 Validation Loss: 0.7401316165924072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3777: Training Loss: 0.27657141784826916 Validation Loss: 0.7404159903526306\n",
      "Epoch 3778: Training Loss: 0.2762973556915919 Validation Loss: 0.7406976222991943\n",
      "Epoch 3779: Training Loss: 0.27582304179668427 Validation Loss: 0.7404935956001282\n",
      "Epoch 3780: Training Loss: 0.27593467632929486 Validation Loss: 0.7403123378753662\n",
      "Epoch 3781: Training Loss: 0.2748311956723531 Validation Loss: 0.7402036786079407\n",
      "Epoch 3782: Training Loss: 0.27511996030807495 Validation Loss: 0.7400669455528259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3783: Training Loss: 0.27486252784729004 Validation Loss: 0.7401027083396912\n",
      "Epoch 3784: Training Loss: 0.27483702699343365 Validation Loss: 0.7400743961334229\n",
      "Epoch 3785: Training Loss: 0.27377039194107056 Validation Loss: 0.7403080463409424\n",
      "Epoch 3786: Training Loss: 0.27490479747454327 Validation Loss: 0.7402544021606445\n",
      "Epoch 3787: Training Loss: 0.2751968502998352 Validation Loss: 0.7400954365730286\n",
      "Epoch 3788: Training Loss: 0.2742309073607127 Validation Loss: 0.7401744723320007\n",
      "Epoch 3789: Training Loss: 0.2743383049964905 Validation Loss: 0.7399008870124817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3790: Training Loss: 0.2741365333398183 Validation Loss: 0.7398831844329834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3791: Training Loss: 0.2742159614960353 Validation Loss: 0.7397592067718506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3792: Training Loss: 0.2743609348932902 Validation Loss: 0.7396766543388367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3793: Training Loss: 0.2737261752287547 Validation Loss: 0.7396831512451172\n",
      "Epoch 3794: Training Loss: 0.27395209670066833 Validation Loss: 0.7397342324256897\n",
      "Epoch 3795: Training Loss: 0.27366069952646893 Validation Loss: 0.7395601868629456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3796: Training Loss: 0.2735421458880107 Validation Loss: 0.7394964098930359\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3797: Training Loss: 0.27361005544662476 Validation Loss: 0.7396224141120911\n",
      "Epoch 3798: Training Loss: 0.27330972254276276 Validation Loss: 0.7395815253257751\n",
      "Epoch 3799: Training Loss: 0.2740533749262492 Validation Loss: 0.7397276759147644\n",
      "Epoch 3800: Training Loss: 0.273176372051239 Validation Loss: 0.7396241426467896\n",
      "Epoch 3801: Training Loss: 0.275090495745341 Validation Loss: 0.7397869229316711\n",
      "Epoch 3802: Training Loss: 0.2730148732662201 Validation Loss: 0.7393375635147095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3803: Training Loss: 0.27380821108818054 Validation Loss: 0.738959789276123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3804: Training Loss: 0.2733779847621918 Validation Loss: 0.7388228178024292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3805: Training Loss: 0.2743067443370819 Validation Loss: 0.7387141585350037\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3806: Training Loss: 0.2724231382211049 Validation Loss: 0.7387712597846985\n",
      "Epoch 3807: Training Loss: 0.2725826899210612 Validation Loss: 0.7387948036193848\n",
      "Epoch 3808: Training Loss: 0.27265849709510803 Validation Loss: 0.7390395402908325\n",
      "Epoch 3809: Training Loss: 0.27265047033627826 Validation Loss: 0.7390702962875366\n",
      "Epoch 3810: Training Loss: 0.27351607879002887 Validation Loss: 0.7393564581871033\n",
      "Epoch 3811: Training Loss: 0.2735212246576945 Validation Loss: 0.7390636205673218\n",
      "Epoch 3812: Training Loss: 0.27397193511327106 Validation Loss: 0.7393814325332642\n",
      "Epoch 3813: Training Loss: 0.2733825395504634 Validation Loss: 0.7394257187843323\n",
      "Epoch 3814: Training Loss: 0.2727366586526235 Validation Loss: 0.7393019199371338\n",
      "Epoch 3815: Training Loss: 0.27232324083646137 Validation Loss: 0.7392081618309021\n",
      "Epoch 3816: Training Loss: 0.2728557040294011 Validation Loss: 0.7389116883277893\n",
      "Epoch 3817: Training Loss: 0.2726446787516276 Validation Loss: 0.7386606335639954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3818: Training Loss: 0.2718561291694641 Validation Loss: 0.738433837890625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3819: Training Loss: 0.271923432747523 Validation Loss: 0.7383678555488586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3820: Training Loss: 0.2711513737837474 Validation Loss: 0.7385345101356506\n",
      "Epoch 3821: Training Loss: 0.2717004418373108 Validation Loss: 0.7388492822647095\n",
      "Epoch 3822: Training Loss: 0.27156933148701984 Validation Loss: 0.738471269607544\n",
      "Epoch 3823: Training Loss: 0.27126092712084454 Validation Loss: 0.7384746670722961\n",
      "Epoch 3824: Training Loss: 0.27114490667978924 Validation Loss: 0.7381812334060669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3825: Training Loss: 0.27292686700820923 Validation Loss: 0.7381194233894348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3826: Training Loss: 0.2709740201632182 Validation Loss: 0.7383090257644653\n",
      "Epoch 3827: Training Loss: 0.27063341935475665 Validation Loss: 0.7381125688552856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3828: Training Loss: 0.2708128094673157 Validation Loss: 0.7385531663894653\n",
      "Epoch 3829: Training Loss: 0.27069814999898273 Validation Loss: 0.7386430501937866\n",
      "Epoch 3830: Training Loss: 0.2702683011690776 Validation Loss: 0.7384644746780396\n",
      "Epoch 3831: Training Loss: 0.271589994430542 Validation Loss: 0.7380908727645874\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3832: Training Loss: 0.270849068959554 Validation Loss: 0.7379294037818909\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3833: Training Loss: 0.27072449525197345 Validation Loss: 0.7379425168037415\n",
      "Epoch 3834: Training Loss: 0.27044156193733215 Validation Loss: 0.7379105091094971\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3835: Training Loss: 0.27179448306560516 Validation Loss: 0.7380868196487427\n",
      "Epoch 3836: Training Loss: 0.2707412342230479 Validation Loss: 0.7381348609924316\n",
      "Epoch 3837: Training Loss: 0.27006396651268005 Validation Loss: 0.7379887104034424\n",
      "Epoch 3838: Training Loss: 0.2702532907327016 Validation Loss: 0.7381599545478821\n",
      "Epoch 3839: Training Loss: 0.2702041616042455 Validation Loss: 0.7381089925765991\n",
      "Epoch 3840: Training Loss: 0.2697285811106364 Validation Loss: 0.7382410764694214\n",
      "Epoch 3841: Training Loss: 0.26958395540714264 Validation Loss: 0.7379933595657349\n",
      "Epoch 3842: Training Loss: 0.270002822081248 Validation Loss: 0.7372878193855286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3843: Training Loss: 0.270210603872935 Validation Loss: 0.7372241616249084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3844: Training Loss: 0.2694619695345561 Validation Loss: 0.7373615503311157\n",
      "Epoch 3845: Training Loss: 0.2686673700809479 Validation Loss: 0.737270176410675\n",
      "Epoch 3846: Training Loss: 0.27100858589013416 Validation Loss: 0.7374250888824463\n",
      "Epoch 3847: Training Loss: 0.2693343659241994 Validation Loss: 0.7375898361206055\n",
      "Epoch 3848: Training Loss: 0.2695041398207347 Validation Loss: 0.7374610900878906\n",
      "Epoch 3849: Training Loss: 0.2694844603538513 Validation Loss: 0.7377365827560425\n",
      "Epoch 3850: Training Loss: 0.26810909310976666 Validation Loss: 0.7378916144371033\n",
      "Epoch 3851: Training Loss: 0.2696749021609624 Validation Loss: 0.7377546429634094\n",
      "Epoch 3852: Training Loss: 0.2685355842113495 Validation Loss: 0.7375847101211548\n",
      "Epoch 3853: Training Loss: 0.2688769996166229 Validation Loss: 0.7370955348014832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3854: Training Loss: 0.2685157507658005 Validation Loss: 0.7371295690536499\n",
      "Epoch 3855: Training Loss: 0.2689097970724106 Validation Loss: 0.7373562455177307\n",
      "Epoch 3856: Training Loss: 0.2689846605062485 Validation Loss: 0.7374346852302551\n",
      "Epoch 3857: Training Loss: 0.2688157856464386 Validation Loss: 0.7373332381248474\n",
      "Epoch 3858: Training Loss: 0.2676607022682826 Validation Loss: 0.7371891736984253\n",
      "Epoch 3859: Training Loss: 0.2681436191002528 Validation Loss: 0.7369254231452942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3860: Training Loss: 0.2681763172149658 Validation Loss: 0.737009584903717\n",
      "Epoch 3861: Training Loss: 0.26772861182689667 Validation Loss: 0.7367651462554932\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3862: Training Loss: 0.26823092500368756 Validation Loss: 0.7368707060813904\n",
      "Epoch 3863: Training Loss: 0.26866554220517475 Validation Loss: 0.7369863390922546\n",
      "Epoch 3864: Training Loss: 0.268435259660085 Validation Loss: 0.737060010433197\n",
      "Epoch 3865: Training Loss: 0.26768747965494794 Validation Loss: 0.7370434403419495\n",
      "Epoch 3866: Training Loss: 0.2674751778443654 Validation Loss: 0.7370704412460327\n",
      "Epoch 3867: Training Loss: 0.2676731050014496 Validation Loss: 0.736941933631897\n",
      "Epoch 3868: Training Loss: 0.26837970813115436 Validation Loss: 0.7366306781768799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3869: Training Loss: 0.26836878557999927 Validation Loss: 0.7361950874328613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3870: Training Loss: 0.2696002572774887 Validation Loss: 0.736057460308075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3871: Training Loss: 0.26714373628298443 Validation Loss: 0.7365437746047974\n",
      "Epoch 3872: Training Loss: 0.2685181647539139 Validation Loss: 0.7367537617683411\n",
      "Epoch 3873: Training Loss: 0.267936830719312 Validation Loss: 0.7366868853569031\n",
      "Epoch 3874: Training Loss: 0.2666531304518382 Validation Loss: 0.7366933226585388\n",
      "Epoch 3875: Training Loss: 0.26695720354715985 Validation Loss: 0.7366443872451782\n",
      "Epoch 3876: Training Loss: 0.2666768729686737 Validation Loss: 0.7367905974388123\n",
      "Epoch 3877: Training Loss: 0.2665512164433797 Validation Loss: 0.7366118431091309\n",
      "Epoch 3878: Training Loss: 0.2670012265443802 Validation Loss: 0.7361847162246704\n",
      "Epoch 3879: Training Loss: 0.26703430712223053 Validation Loss: 0.7361714243888855\n",
      "Epoch 3880: Training Loss: 0.2661394377549489 Validation Loss: 0.7362058162689209\n",
      "Epoch 3881: Training Loss: 0.26742712656656903 Validation Loss: 0.7365729212760925\n",
      "Epoch 3882: Training Loss: 0.26687057316303253 Validation Loss: 0.7363962531089783\n",
      "Epoch 3883: Training Loss: 0.2669925292332967 Validation Loss: 0.7362577319145203\n",
      "Epoch 3884: Training Loss: 0.2661225398381551 Validation Loss: 0.7362537384033203\n",
      "Epoch 3885: Training Loss: 0.2682763487100601 Validation Loss: 0.736029863357544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3886: Training Loss: 0.26732611656188965 Validation Loss: 0.7360119819641113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3887: Training Loss: 0.26492003599802655 Validation Loss: 0.7359170317649841\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3888: Training Loss: 0.26603323221206665 Validation Loss: 0.7362566590309143\n",
      "Epoch 3889: Training Loss: 0.26548189918200177 Validation Loss: 0.7361495494842529\n",
      "Epoch 3890: Training Loss: 0.26601878305276233 Validation Loss: 0.7360437512397766\n",
      "Epoch 3891: Training Loss: 0.2653012325366338 Validation Loss: 0.7355653643608093\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3892: Training Loss: 0.2669701725244522 Validation Loss: 0.7356285452842712\n",
      "Epoch 3893: Training Loss: 0.2660619666179021 Validation Loss: 0.7356973886489868\n",
      "Epoch 3894: Training Loss: 0.2651512821515401 Validation Loss: 0.7357131838798523\n",
      "Epoch 3895: Training Loss: 0.26618583500385284 Validation Loss: 0.7358464002609253\n",
      "Epoch 3896: Training Loss: 0.2655034412940343 Validation Loss: 0.7359166741371155\n",
      "Epoch 3897: Training Loss: 0.2659117778142293 Validation Loss: 0.7358150482177734\n",
      "Epoch 3898: Training Loss: 0.26551567018032074 Validation Loss: 0.7358012199401855\n",
      "Epoch 3899: Training Loss: 0.26493192215760547 Validation Loss: 0.7353823184967041\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3900: Training Loss: 0.26547003785769147 Validation Loss: 0.7354381680488586\n",
      "Epoch 3901: Training Loss: 0.2666933089494705 Validation Loss: 0.7353458404541016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3902: Training Loss: 0.26455310980478924 Validation Loss: 0.7352632284164429\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3903: Training Loss: 0.2646501710017522 Validation Loss: 0.7351512908935547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3904: Training Loss: 0.26503620048364 Validation Loss: 0.7351950407028198\n",
      "Epoch 3905: Training Loss: 0.2647893925507863 Validation Loss: 0.7350695729255676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3906: Training Loss: 0.2640920082728068 Validation Loss: 0.7353259921073914\n",
      "Epoch 3907: Training Loss: 0.26395246386528015 Validation Loss: 0.735386073589325\n",
      "Epoch 3908: Training Loss: 0.2643335560957591 Validation Loss: 0.735268235206604\n",
      "Epoch 3909: Training Loss: 0.2648149331410726 Validation Loss: 0.735450029373169\n",
      "Epoch 3910: Training Loss: 0.26422395805517834 Validation Loss: 0.7351347804069519\n",
      "Epoch 3911: Training Loss: 0.2643986294666926 Validation Loss: 0.7350417375564575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3912: Training Loss: 0.2642122358083725 Validation Loss: 0.7352229356765747\n",
      "Epoch 3913: Training Loss: 0.2638489504655202 Validation Loss: 0.73521488904953\n",
      "Epoch 3914: Training Loss: 0.2634078512589137 Validation Loss: 0.7349492311477661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3915: Training Loss: 0.264042466878891 Validation Loss: 0.734933614730835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3916: Training Loss: 0.2643366903066635 Validation Loss: 0.7350170016288757\n",
      "Epoch 3917: Training Loss: 0.26388516028722125 Validation Loss: 0.7350764274597168\n",
      "Epoch 3918: Training Loss: 0.26355848213036853 Validation Loss: 0.7351841330528259\n",
      "Epoch 3919: Training Loss: 0.2629225552082062 Validation Loss: 0.7352904677391052\n",
      "Epoch 3920: Training Loss: 0.2631981124480565 Validation Loss: 0.7353076934814453\n",
      "Epoch 3921: Training Loss: 0.26445090770721436 Validation Loss: 0.735255777835846\n",
      "Epoch 3922: Training Loss: 0.2628665665785472 Validation Loss: 0.7352049350738525\n",
      "Epoch 3923: Training Loss: 0.2646694133679072 Validation Loss: 0.7348240613937378\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3924: Training Loss: 0.2633998642365138 Validation Loss: 0.7350122332572937\n",
      "Epoch 3925: Training Loss: 0.26296810309092206 Validation Loss: 0.7346451878547668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3926: Training Loss: 0.2623339146375656 Validation Loss: 0.734673261642456\n",
      "Epoch 3927: Training Loss: 0.26266886790593463 Validation Loss: 0.734125018119812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3928: Training Loss: 0.2625507314999898 Validation Loss: 0.7341693639755249\n",
      "Epoch 3929: Training Loss: 0.26237039764722186 Validation Loss: 0.7343857288360596\n",
      "Epoch 3930: Training Loss: 0.26271427671114606 Validation Loss: 0.734208881855011\n",
      "Epoch 3931: Training Loss: 0.26287779211997986 Validation Loss: 0.7339381575584412\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3932: Training Loss: 0.26215457916259766 Validation Loss: 0.7342028021812439\n",
      "Epoch 3933: Training Loss: 0.26267487307389575 Validation Loss: 0.7343488931655884\n",
      "Epoch 3934: Training Loss: 0.26230754951636 Validation Loss: 0.7345085144042969\n",
      "Epoch 3935: Training Loss: 0.26191119849681854 Validation Loss: 0.7343860268592834\n",
      "Epoch 3936: Training Loss: 0.2618655761082967 Validation Loss: 0.734361469745636\n",
      "Epoch 3937: Training Loss: 0.26247119903564453 Validation Loss: 0.734429121017456\n",
      "Epoch 3938: Training Loss: 0.261740247408549 Validation Loss: 0.7341616749763489\n",
      "Epoch 3939: Training Loss: 0.2617604037125905 Validation Loss: 0.7340614199638367\n",
      "Epoch 3940: Training Loss: 0.261999174952507 Validation Loss: 0.733998715877533\n",
      "Epoch 3941: Training Loss: 0.26134543617566425 Validation Loss: 0.7340022921562195\n",
      "Epoch 3942: Training Loss: 0.26142605145772296 Validation Loss: 0.7338913679122925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3943: Training Loss: 0.2614925106366475 Validation Loss: 0.7337348461151123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3944: Training Loss: 0.2613495389620463 Validation Loss: 0.7337900996208191\n",
      "Epoch 3945: Training Loss: 0.2613402207692464 Validation Loss: 0.7336207628250122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3946: Training Loss: 0.26126301288604736 Validation Loss: 0.7336522936820984\n",
      "Epoch 3947: Training Loss: 0.2613816261291504 Validation Loss: 0.7337123155593872\n",
      "Epoch 3948: Training Loss: 0.26097987095514935 Validation Loss: 0.7337534427642822\n",
      "Epoch 3949: Training Loss: 0.26124998927116394 Validation Loss: 0.7342159152030945\n",
      "Epoch 3950: Training Loss: 0.2613965968290965 Validation Loss: 0.7341238260269165\n",
      "Epoch 3951: Training Loss: 0.2608041912317276 Validation Loss: 0.7341052293777466\n",
      "Epoch 3952: Training Loss: 0.2608238259951274 Validation Loss: 0.7337034940719604\n",
      "Epoch 3953: Training Loss: 0.2605951229731242 Validation Loss: 0.733666181564331\n",
      "Epoch 3954: Training Loss: 0.2601146896680196 Validation Loss: 0.7336073517799377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3955: Training Loss: 0.2601946045955022 Validation Loss: 0.7341242432594299\n",
      "Epoch 3956: Training Loss: 0.2607848246892293 Validation Loss: 0.7341259717941284\n",
      "Epoch 3957: Training Loss: 0.26049570242563885 Validation Loss: 0.7339075207710266\n",
      "Epoch 3958: Training Loss: 0.2601891408363978 Validation Loss: 0.7334986925125122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3959: Training Loss: 0.2600754002730052 Validation Loss: 0.7333136200904846\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3960: Training Loss: 0.26013655463854474 Validation Loss: 0.7331557273864746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3961: Training Loss: 0.25869108736515045 Validation Loss: 0.7329905033111572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3962: Training Loss: 0.25994376341501874 Validation Loss: 0.7331215739250183\n",
      "Epoch 3963: Training Loss: 0.26057226955890656 Validation Loss: 0.7330663204193115\n",
      "Epoch 3964: Training Loss: 0.2597579111655553 Validation Loss: 0.7333143353462219\n",
      "Epoch 3965: Training Loss: 0.2596176862716675 Validation Loss: 0.7334160804748535\n",
      "Epoch 3966: Training Loss: 0.25994441906611127 Validation Loss: 0.7331665754318237\n",
      "Epoch 3967: Training Loss: 0.25988397002220154 Validation Loss: 0.7330374121665955\n",
      "Epoch 3968: Training Loss: 0.25945377846558887 Validation Loss: 0.7330228090286255\n",
      "Epoch 3969: Training Loss: 0.25944692889849347 Validation Loss: 0.73285311460495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3970: Training Loss: 0.26097825666268665 Validation Loss: 0.7331204414367676\n",
      "Epoch 3971: Training Loss: 0.2592281997203827 Validation Loss: 0.7331749796867371\n",
      "Epoch 3972: Training Loss: 0.2590770373741786 Validation Loss: 0.7330570220947266\n",
      "Epoch 3973: Training Loss: 0.2598379005988439 Validation Loss: 0.7329700589179993\n",
      "Epoch 3974: Training Loss: 0.25897156198819477 Validation Loss: 0.7328194975852966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3975: Training Loss: 0.2595799465974172 Validation Loss: 0.7330209016799927\n",
      "Epoch 3976: Training Loss: 0.2590775638818741 Validation Loss: 0.7329677939414978\n",
      "Epoch 3977: Training Loss: 0.25904246668020886 Validation Loss: 0.7333687543869019\n",
      "Epoch 3978: Training Loss: 0.25865031282107037 Validation Loss: 0.7331094145774841\n",
      "Epoch 3979: Training Loss: 0.2585199574629466 Validation Loss: 0.7328687906265259\n",
      "Epoch 3980: Training Loss: 0.2589662621418635 Validation Loss: 0.7328239679336548\n",
      "Epoch 3981: Training Loss: 0.2593365013599396 Validation Loss: 0.7326710224151611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3982: Training Loss: 0.2584421883026759 Validation Loss: 0.7328166961669922\n",
      "Epoch 3983: Training Loss: 0.258172740538915 Validation Loss: 0.7328111529350281\n",
      "Epoch 3984: Training Loss: 0.25849168996016186 Validation Loss: 0.7326465845108032\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3985: Training Loss: 0.2581196228663127 Validation Loss: 0.7325096726417542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3986: Training Loss: 0.25777100523312885 Validation Loss: 0.7323740124702454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3987: Training Loss: 0.25821280976136524 Validation Loss: 0.7324236631393433\n",
      "Epoch 3988: Training Loss: 0.2581218381722768 Validation Loss: 0.7326058745384216\n",
      "Epoch 3989: Training Loss: 0.2585157553354899 Validation Loss: 0.7325999736785889\n",
      "Epoch 3990: Training Loss: 0.2579652617375056 Validation Loss: 0.7326778769493103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3991: Training Loss: 0.25755979120731354 Validation Loss: 0.7323428988456726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3992: Training Loss: 0.2576797852913539 Validation Loss: 0.7322598695755005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3993: Training Loss: 0.2571649104356766 Validation Loss: 0.7321280837059021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3994: Training Loss: 0.2581981122493744 Validation Loss: 0.732200026512146\n",
      "Epoch 3995: Training Loss: 0.25717779000600177 Validation Loss: 0.7322039604187012\n",
      "Epoch 3996: Training Loss: 0.25730961561203003 Validation Loss: 0.7324256896972656\n",
      "Epoch 3997: Training Loss: 0.25749899943669635 Validation Loss: 0.7326347827911377\n",
      "Epoch 3998: Training Loss: 0.2576514234145482 Validation Loss: 0.7326173782348633\n",
      "Epoch 3999: Training Loss: 0.257493923107783 Validation Loss: 0.7324123382568359\n",
      "Epoch 4000: Training Loss: 0.2572295516729355 Validation Loss: 0.731924295425415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4001: Training Loss: 0.2568575342496236 Validation Loss: 0.7316977977752686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4002: Training Loss: 0.2570956001679103 Validation Loss: 0.7318036556243896\n",
      "Epoch 4003: Training Loss: 0.25686221818129223 Validation Loss: 0.731505811214447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4004: Training Loss: 0.25593669215838116 Validation Loss: 0.7317031025886536\n",
      "Epoch 4005: Training Loss: 0.2566146453221639 Validation Loss: 0.7318131327629089\n",
      "Epoch 4006: Training Loss: 0.25643040239810944 Validation Loss: 0.7316616177558899\n",
      "Epoch 4007: Training Loss: 0.25613953669865924 Validation Loss: 0.7317633628845215\n",
      "Epoch 4008: Training Loss: 0.25645693639914197 Validation Loss: 0.7320128679275513\n",
      "Epoch 4009: Training Loss: 0.2563224236170451 Validation Loss: 0.7321837544441223\n",
      "Epoch 4010: Training Loss: 0.25637811919053394 Validation Loss: 0.7321438193321228\n",
      "Epoch 4011: Training Loss: 0.25637295842170715 Validation Loss: 0.7321151494979858\n",
      "Epoch 4012: Training Loss: 0.25726473331451416 Validation Loss: 0.7318418025970459\n",
      "Epoch 4013: Training Loss: 0.25652169187863666 Validation Loss: 0.7323095798492432\n",
      "Epoch 4014: Training Loss: 0.2562793244918187 Validation Loss: 0.7320376634597778\n",
      "Epoch 4015: Training Loss: 0.25578958292802173 Validation Loss: 0.7318305969238281\n",
      "Epoch 4016: Training Loss: 0.25607312222321826 Validation Loss: 0.7316566109657288\n",
      "Epoch 4017: Training Loss: 0.2556786835193634 Validation Loss: 0.7316287755966187\n",
      "Epoch 4018: Training Loss: 0.2553880612055461 Validation Loss: 0.7313920855522156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4019: Training Loss: 0.25585808356602985 Validation Loss: 0.7312349677085876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4020: Training Loss: 0.2560056447982788 Validation Loss: 0.7311015725135803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4021: Training Loss: 0.2558228274186452 Validation Loss: 0.7308646440505981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4022: Training Loss: 0.25532478590806323 Validation Loss: 0.7309986352920532\n",
      "Epoch 4023: Training Loss: 0.2569570044676463 Validation Loss: 0.7311668395996094\n",
      "Epoch 4024: Training Loss: 0.2552129775285721 Validation Loss: 0.7316566109657288\n",
      "Epoch 4025: Training Loss: 0.2552389055490494 Validation Loss: 0.7315833568572998\n",
      "Epoch 4026: Training Loss: 0.2545612653096517 Validation Loss: 0.7320116758346558\n",
      "Epoch 4027: Training Loss: 0.2545251150925954 Validation Loss: 0.7318525314331055\n",
      "Epoch 4028: Training Loss: 0.2550892581542333 Validation Loss: 0.7316560745239258\n",
      "Epoch 4029: Training Loss: 0.2549202690521876 Validation Loss: 0.7315029501914978\n",
      "Epoch 4030: Training Loss: 0.25476699074109393 Validation Loss: 0.7311217188835144\n",
      "Epoch 4031: Training Loss: 0.2542918572823207 Validation Loss: 0.7311177253723145\n",
      "Epoch 4032: Training Loss: 0.25481635828812915 Validation Loss: 0.7311708331108093\n",
      "Epoch 4033: Training Loss: 0.2541907876729965 Validation Loss: 0.731182336807251\n",
      "Epoch 4034: Training Loss: 0.25407590468724567 Validation Loss: 0.7311415076255798\n",
      "Epoch 4035: Training Loss: 0.2545194774866104 Validation Loss: 0.7310255169868469\n",
      "Epoch 4036: Training Loss: 0.2545359432697296 Validation Loss: 0.7308842539787292\n",
      "Epoch 4037: Training Loss: 0.2543090482552846 Validation Loss: 0.7308688163757324\n",
      "Epoch 4038: Training Loss: 0.25485097865263623 Validation Loss: 0.7309116721153259\n",
      "Epoch 4039: Training Loss: 0.25473496317863464 Validation Loss: 0.730998694896698\n",
      "Epoch 4040: Training Loss: 0.25394676625728607 Validation Loss: 0.7308017015457153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4041: Training Loss: 0.2537480692068736 Validation Loss: 0.7309331297874451\n",
      "Epoch 4042: Training Loss: 0.2545599987109502 Validation Loss: 0.7314109802246094\n",
      "Epoch 4043: Training Loss: 0.2539074718952179 Validation Loss: 0.7309788465499878\n",
      "Epoch 4044: Training Loss: 0.25342602531115216 Validation Loss: 0.7306555509567261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4045: Training Loss: 0.2529726525147756 Validation Loss: 0.7304918766021729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4046: Training Loss: 0.2539414366086324 Validation Loss: 0.7303474545478821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4047: Training Loss: 0.2534575015306473 Validation Loss: 0.7305812835693359\n",
      "Epoch 4048: Training Loss: 0.25379203756650287 Validation Loss: 0.7303979396820068\n",
      "Epoch 4049: Training Loss: 0.25346654156843823 Validation Loss: 0.7303053140640259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4050: Training Loss: 0.25354648133118945 Validation Loss: 0.730544924736023\n",
      "Epoch 4051: Training Loss: 0.25287579496701557 Validation Loss: 0.7307735085487366\n",
      "Epoch 4052: Training Loss: 0.2533298482497533 Validation Loss: 0.7305313944816589\n",
      "Epoch 4053: Training Loss: 0.2521170526742935 Validation Loss: 0.7303977608680725\n",
      "Epoch 4054: Training Loss: 0.25270865857601166 Validation Loss: 0.7305445671081543\n",
      "Epoch 4055: Training Loss: 0.25333961844444275 Validation Loss: 0.7305387258529663\n",
      "Epoch 4056: Training Loss: 0.2534402012825012 Validation Loss: 0.7307462692260742\n",
      "Epoch 4057: Training Loss: 0.2547566344340642 Validation Loss: 0.7309219241142273\n",
      "Epoch 4058: Training Loss: 0.2519891361395518 Validation Loss: 0.730968177318573\n",
      "Epoch 4059: Training Loss: 0.25235240161418915 Validation Loss: 0.7304018139839172\n",
      "Epoch 4060: Training Loss: 0.2545848737160365 Validation Loss: 0.7302699089050293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4061: Training Loss: 0.2523324290911357 Validation Loss: 0.7301221489906311\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4062: Training Loss: 0.25228553513685864 Validation Loss: 0.7299786806106567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4063: Training Loss: 0.2524523138999939 Validation Loss: 0.73001629114151\n",
      "Epoch 4064: Training Loss: 0.25219212969144184 Validation Loss: 0.7301663756370544\n",
      "Epoch 4065: Training Loss: 0.251990407705307 Validation Loss: 0.7301211953163147\n",
      "Epoch 4066: Training Loss: 0.2535010725259781 Validation Loss: 0.7298305034637451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4067: Training Loss: 0.2520741472641627 Validation Loss: 0.7298914194107056\n",
      "Epoch 4068: Training Loss: 0.2539975295464198 Validation Loss: 0.7299726605415344\n",
      "Epoch 4069: Training Loss: 0.2517374058564504 Validation Loss: 0.7297775745391846\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4070: Training Loss: 0.2524099051952362 Validation Loss: 0.7300217151641846\n",
      "Epoch 4071: Training Loss: 0.25154853860537213 Validation Loss: 0.7302529215812683\n",
      "Epoch 4072: Training Loss: 0.2516067276398341 Validation Loss: 0.7300025224685669\n",
      "Epoch 4073: Training Loss: 0.2515144993861516 Validation Loss: 0.7299118638038635\n",
      "Epoch 4074: Training Loss: 0.2517729103565216 Validation Loss: 0.7299591898918152\n",
      "Epoch 4075: Training Loss: 0.2514893015225728 Validation Loss: 0.7297964096069336\n",
      "Epoch 4076: Training Loss: 0.25258737802505493 Validation Loss: 0.7297460436820984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4077: Training Loss: 0.25110892951488495 Validation Loss: 0.7296218872070312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4078: Training Loss: 0.25023992359638214 Validation Loss: 0.7295250296592712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4079: Training Loss: 0.25142072637875873 Validation Loss: 0.7296905517578125\n",
      "Epoch 4080: Training Loss: 0.2509382913510005 Validation Loss: 0.7299736142158508\n",
      "Epoch 4081: Training Loss: 0.25147707263628644 Validation Loss: 0.7301414012908936\n",
      "Epoch 4082: Training Loss: 0.2510173171758652 Validation Loss: 0.7299332618713379\n",
      "Epoch 4083: Training Loss: 0.2508535434802373 Validation Loss: 0.7295470237731934\n",
      "Epoch 4084: Training Loss: 0.250815545519193 Validation Loss: 0.7292100191116333\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4085: Training Loss: 0.25031142433484393 Validation Loss: 0.7290634512901306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4086: Training Loss: 0.25061218440532684 Validation Loss: 0.7292355298995972\n",
      "Epoch 4087: Training Loss: 0.2509502023458481 Validation Loss: 0.7296299934387207\n",
      "Epoch 4088: Training Loss: 0.25036489963531494 Validation Loss: 0.7299814820289612\n",
      "Epoch 4089: Training Loss: 0.25012949109077454 Validation Loss: 0.7296467423439026\n",
      "Epoch 4090: Training Loss: 0.24992921451727548 Validation Loss: 0.729357898235321\n",
      "Epoch 4091: Training Loss: 0.25049738585948944 Validation Loss: 0.7292730808258057\n",
      "Epoch 4092: Training Loss: 0.25003287692864734 Validation Loss: 0.7292934060096741\n",
      "Epoch 4093: Training Loss: 0.2504199842611949 Validation Loss: 0.7291772365570068\n",
      "Epoch 4094: Training Loss: 0.25016315281391144 Validation Loss: 0.7292131781578064\n",
      "Epoch 4095: Training Loss: 0.24978727598985037 Validation Loss: 0.7293505668640137\n",
      "Epoch 4096: Training Loss: 0.2503003577391307 Validation Loss: 0.7292509078979492\n",
      "Epoch 4097: Training Loss: 0.2497393637895584 Validation Loss: 0.7291229963302612\n",
      "Epoch 4098: Training Loss: 0.24961272875467935 Validation Loss: 0.7292133569717407\n",
      "Epoch 4099: Training Loss: 0.2506488064924876 Validation Loss: 0.7292965054512024\n",
      "Epoch 4100: Training Loss: 0.24995066225528717 Validation Loss: 0.7291287183761597\n",
      "Epoch 4101: Training Loss: 0.24960954984029135 Validation Loss: 0.7291852235794067\n",
      "Epoch 4102: Training Loss: 0.25050976872444153 Validation Loss: 0.7288460731506348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4103: Training Loss: 0.24891308943430582 Validation Loss: 0.7289842367172241\n",
      "Epoch 4104: Training Loss: 0.24951252341270447 Validation Loss: 0.7290231585502625\n",
      "Epoch 4105: Training Loss: 0.24908377726872763 Validation Loss: 0.7290571928024292\n",
      "Epoch 4106: Training Loss: 0.24921170870463052 Validation Loss: 0.7290980219841003\n",
      "Epoch 4107: Training Loss: 0.24934658904870352 Validation Loss: 0.7292737364768982\n",
      "Epoch 4108: Training Loss: 0.24943314492702484 Validation Loss: 0.7289970517158508\n",
      "Epoch 4109: Training Loss: 0.2510019739468892 Validation Loss: 0.7287419438362122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4110: Training Loss: 0.24953946967919668 Validation Loss: 0.7285224199295044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4111: Training Loss: 0.2486033389965693 Validation Loss: 0.7282030582427979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4112: Training Loss: 0.2489869644244512 Validation Loss: 0.7284687757492065\n",
      "Epoch 4113: Training Loss: 0.24992267290751138 Validation Loss: 0.7284085154533386\n",
      "Epoch 4114: Training Loss: 0.24916849037011465 Validation Loss: 0.7287329435348511\n",
      "Epoch 4115: Training Loss: 0.24814829230308533 Validation Loss: 0.7288810014724731\n",
      "Epoch 4116: Training Loss: 0.24873265624046326 Validation Loss: 0.7290651202201843\n",
      "Epoch 4117: Training Loss: 0.24847383300463358 Validation Loss: 0.729207456111908\n",
      "Epoch 4118: Training Loss: 0.24834343294302622 Validation Loss: 0.729053258895874\n",
      "Epoch 4119: Training Loss: 0.24806312223275503 Validation Loss: 0.7287464737892151\n",
      "Epoch 4120: Training Loss: 0.2483801543712616 Validation Loss: 0.7285454273223877\n",
      "Epoch 4121: Training Loss: 0.24807353814442953 Validation Loss: 0.7283295392990112\n",
      "Epoch 4122: Training Loss: 0.24848811328411102 Validation Loss: 0.728517472743988\n",
      "Epoch 4123: Training Loss: 0.24789913495381674 Validation Loss: 0.7286714911460876\n",
      "Epoch 4124: Training Loss: 0.2482629418373108 Validation Loss: 0.728476881980896\n",
      "Epoch 4125: Training Loss: 0.24803553521633148 Validation Loss: 0.7286211252212524\n",
      "Epoch 4126: Training Loss: 0.24768447379271188 Validation Loss: 0.7284321188926697\n",
      "Epoch 4127: Training Loss: 0.2475155939658483 Validation Loss: 0.7280656695365906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4128: Training Loss: 0.24795990685621896 Validation Loss: 0.7281600832939148\n",
      "Epoch 4129: Training Loss: 0.24772645036379495 Validation Loss: 0.7284666299819946\n",
      "Epoch 4130: Training Loss: 0.24733840922514597 Validation Loss: 0.7286888360977173\n",
      "Epoch 4131: Training Loss: 0.24729876716931662 Validation Loss: 0.7286483645439148\n",
      "Epoch 4132: Training Loss: 0.2478205462296804 Validation Loss: 0.7287688255310059\n",
      "Epoch 4133: Training Loss: 0.24796060721079508 Validation Loss: 0.7281842827796936\n",
      "Epoch 4134: Training Loss: 0.24706365168094635 Validation Loss: 0.7281447052955627\n",
      "Epoch 4135: Training Loss: 0.2490805188814799 Validation Loss: 0.7280006408691406\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4136: Training Loss: 0.24686061342557272 Validation Loss: 0.7278189063072205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4137: Training Loss: 0.24805845816930136 Validation Loss: 0.7280632853507996\n",
      "Epoch 4138: Training Loss: 0.2467075785001119 Validation Loss: 0.7280997633934021\n",
      "Epoch 4139: Training Loss: 0.24734661479791006 Validation Loss: 0.7281988859176636\n",
      "Epoch 4140: Training Loss: 0.24740426739056906 Validation Loss: 0.7281872630119324\n",
      "Epoch 4141: Training Loss: 0.24596049884955087 Validation Loss: 0.7280898690223694\n",
      "Epoch 4142: Training Loss: 0.24580392241477966 Validation Loss: 0.7277952432632446\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4143: Training Loss: 0.24671984712282816 Validation Loss: 0.7277539372444153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4144: Training Loss: 0.24635104835033417 Validation Loss: 0.7277335524559021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4145: Training Loss: 0.24658775826295218 Validation Loss: 0.7279003262519836\n",
      "Epoch 4146: Training Loss: 0.24632554749647775 Validation Loss: 0.7280055284500122\n",
      "Epoch 4147: Training Loss: 0.24647930264472961 Validation Loss: 0.7281261086463928\n",
      "Epoch 4148: Training Loss: 0.24667741854985556 Validation Loss: 0.7279134392738342\n",
      "Epoch 4149: Training Loss: 0.24614566067854562 Validation Loss: 0.7275997996330261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4150: Training Loss: 0.24633307258288065 Validation Loss: 0.7277038097381592\n",
      "Epoch 4151: Training Loss: 0.2458990216255188 Validation Loss: 0.7277780175209045\n",
      "Epoch 4152: Training Loss: 0.24652870496114096 Validation Loss: 0.7278650999069214\n",
      "Epoch 4153: Training Loss: 0.24567698438962302 Validation Loss: 0.7276320457458496\n",
      "Epoch 4154: Training Loss: 0.24578253428141275 Validation Loss: 0.7275443077087402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4155: Training Loss: 0.24560371041297913 Validation Loss: 0.7272878289222717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4156: Training Loss: 0.24613488217194876 Validation Loss: 0.7270981669425964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4157: Training Loss: 0.24633221824963888 Validation Loss: 0.727051317691803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4158: Training Loss: 0.24623671670754751 Validation Loss: 0.7271705865859985\n",
      "Epoch 4159: Training Loss: 0.24581837157408395 Validation Loss: 0.727348804473877\n",
      "Epoch 4160: Training Loss: 0.24534167846043906 Validation Loss: 0.7276979684829712\n",
      "Epoch 4161: Training Loss: 0.24499601622422537 Validation Loss: 0.7280122637748718\n",
      "Epoch 4162: Training Loss: 0.24491373201211294 Validation Loss: 0.7279911041259766\n",
      "Epoch 4163: Training Loss: 0.24450659255186716 Validation Loss: 0.7279251217842102\n",
      "Epoch 4164: Training Loss: 0.24573471148808798 Validation Loss: 0.7272588014602661\n",
      "Epoch 4165: Training Loss: 0.2447950392961502 Validation Loss: 0.72725909948349\n",
      "Epoch 4166: Training Loss: 0.24497802058855692 Validation Loss: 0.7273336052894592\n",
      "Epoch 4167: Training Loss: 0.24539529780546823 Validation Loss: 0.7272059321403503\n",
      "Epoch 4168: Training Loss: 0.24515032271544138 Validation Loss: 0.7275475859642029\n",
      "Epoch 4169: Training Loss: 0.24467664460341135 Validation Loss: 0.7276468276977539\n",
      "Epoch 4170: Training Loss: 0.24456173181533813 Validation Loss: 0.7276110053062439\n",
      "Epoch 4171: Training Loss: 0.2448258896668752 Validation Loss: 0.7276925444602966\n",
      "Epoch 4172: Training Loss: 0.24517885347207388 Validation Loss: 0.7273528575897217\n",
      "Epoch 4173: Training Loss: 0.2442613492409388 Validation Loss: 0.7270309925079346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4174: Training Loss: 0.24453305701414743 Validation Loss: 0.7268341779708862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4175: Training Loss: 0.24448014299074808 Validation Loss: 0.7266126275062561\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4176: Training Loss: 0.24440602958202362 Validation Loss: 0.7267454266548157\n",
      "Epoch 4177: Training Loss: 0.24399476746718088 Validation Loss: 0.7269107699394226\n",
      "Epoch 4178: Training Loss: 0.24430154263973236 Validation Loss: 0.7268116474151611\n",
      "Epoch 4179: Training Loss: 0.2442216475804647 Validation Loss: 0.7269339561462402\n",
      "Epoch 4180: Training Loss: 0.24379780888557434 Validation Loss: 0.7268065214157104\n",
      "Epoch 4181: Training Loss: 0.2441546767950058 Validation Loss: 0.7269031405448914\n",
      "Epoch 4182: Training Loss: 0.2437470704317093 Validation Loss: 0.7269427180290222\n",
      "Epoch 4183: Training Loss: 0.24537168939908346 Validation Loss: 0.7271921634674072\n",
      "Epoch 4184: Training Loss: 0.24382385114828745 Validation Loss: 0.7267627716064453\n",
      "Epoch 4185: Training Loss: 0.24356678624947867 Validation Loss: 0.7266625165939331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4186: Training Loss: 0.24340493977069855 Validation Loss: 0.7264125943183899\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4187: Training Loss: 0.24409346282482147 Validation Loss: 0.7264493107795715\n",
      "Epoch 4188: Training Loss: 0.24340232213338217 Validation Loss: 0.7265469431877136\n",
      "Epoch 4189: Training Loss: 0.2430564413468043 Validation Loss: 0.7266388535499573\n",
      "Epoch 4190: Training Loss: 0.2431216537952423 Validation Loss: 0.7267811894416809\n",
      "Epoch 4191: Training Loss: 0.24264267086982727 Validation Loss: 0.7270024418830872\n",
      "Epoch 4192: Training Loss: 0.24389019111792246 Validation Loss: 0.7267738580703735\n",
      "Epoch 4193: Training Loss: 0.24323194722334543 Validation Loss: 0.7267306447029114\n",
      "Epoch 4194: Training Loss: 0.24311319986979166 Validation Loss: 0.7263205051422119\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4195: Training Loss: 0.24328691263993582 Validation Loss: 0.7263783812522888\n",
      "Epoch 4196: Training Loss: 0.24257704615592957 Validation Loss: 0.7266528606414795\n",
      "Epoch 4197: Training Loss: 0.2426869124174118 Validation Loss: 0.7266964912414551\n",
      "Epoch 4198: Training Loss: 0.2429330050945282 Validation Loss: 0.726454496383667\n",
      "Epoch 4199: Training Loss: 0.24258092045783997 Validation Loss: 0.726365327835083\n",
      "Epoch 4200: Training Loss: 0.24242016673088074 Validation Loss: 0.7261707782745361\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4201: Training Loss: 0.2423823078473409 Validation Loss: 0.7262479066848755\n",
      "Epoch 4202: Training Loss: 0.24223741392294565 Validation Loss: 0.7260830998420715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4203: Training Loss: 0.2428315927584966 Validation Loss: 0.7259082794189453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4204: Training Loss: 0.2424809734026591 Validation Loss: 0.7259743809700012\n",
      "Epoch 4205: Training Loss: 0.24268901348114014 Validation Loss: 0.7258809804916382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4206: Training Loss: 0.24259584645430246 Validation Loss: 0.726147472858429\n",
      "Epoch 4207: Training Loss: 0.24203540881474814 Validation Loss: 0.7262998223304749\n",
      "Epoch 4208: Training Loss: 0.24192963043848673 Validation Loss: 0.7264419198036194\n",
      "Epoch 4209: Training Loss: 0.2419267843166987 Validation Loss: 0.7259898781776428\n",
      "Epoch 4210: Training Loss: 0.24205035964647928 Validation Loss: 0.7259197235107422\n",
      "Epoch 4211: Training Loss: 0.24224109450976053 Validation Loss: 0.7260077595710754\n",
      "Epoch 4212: Training Loss: 0.24211231370766959 Validation Loss: 0.7260407209396362\n",
      "Epoch 4213: Training Loss: 0.24156290292739868 Validation Loss: 0.7261146306991577\n",
      "Epoch 4214: Training Loss: 0.24158494174480438 Validation Loss: 0.7261812090873718\n",
      "Epoch 4215: Training Loss: 0.2416884551445643 Validation Loss: 0.7258480787277222\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4216: Training Loss: 0.24192271133263907 Validation Loss: 0.7256280183792114\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4217: Training Loss: 0.24246095617612204 Validation Loss: 0.7255053520202637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4218: Training Loss: 0.2410915046930313 Validation Loss: 0.7256371378898621\n",
      "Epoch 4219: Training Loss: 0.24088428417841592 Validation Loss: 0.725878119468689\n",
      "Epoch 4220: Training Loss: 0.24129879971345267 Validation Loss: 0.7260114550590515\n",
      "Epoch 4221: Training Loss: 0.24122086664040884 Validation Loss: 0.725989043712616\n",
      "Epoch 4222: Training Loss: 0.24091439445813498 Validation Loss: 0.7261819839477539\n",
      "Epoch 4223: Training Loss: 0.24207010368506113 Validation Loss: 0.7259724140167236\n",
      "Epoch 4224: Training Loss: 0.24063560863335928 Validation Loss: 0.7257068753242493\n",
      "Epoch 4225: Training Loss: 0.24089585741360983 Validation Loss: 0.7255232334136963\n",
      "Epoch 4226: Training Loss: 0.24094941218694052 Validation Loss: 0.7256718873977661\n",
      "Epoch 4227: Training Loss: 0.2409899781147639 Validation Loss: 0.7257288098335266\n",
      "Epoch 4228: Training Loss: 0.2407753566900889 Validation Loss: 0.7259308695793152\n",
      "Epoch 4229: Training Loss: 0.240234836935997 Validation Loss: 0.7259165644645691\n",
      "Epoch 4230: Training Loss: 0.24104913572470346 Validation Loss: 0.7257165908813477\n",
      "Epoch 4231: Training Loss: 0.24159515897432962 Validation Loss: 0.7253033518791199\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4232: Training Loss: 0.24064200619856516 Validation Loss: 0.7255957126617432\n",
      "Epoch 4233: Training Loss: 0.24054880440235138 Validation Loss: 0.7253488302230835\n",
      "Epoch 4234: Training Loss: 0.24014280239741007 Validation Loss: 0.7252890467643738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4235: Training Loss: 0.2400601158539454 Validation Loss: 0.7253273725509644\n",
      "Epoch 4236: Training Loss: 0.24021979173024496 Validation Loss: 0.7256491184234619\n",
      "Epoch 4237: Training Loss: 0.24106026192506155 Validation Loss: 0.7256941199302673\n",
      "Epoch 4238: Training Loss: 0.241373673081398 Validation Loss: 0.7255863547325134\n",
      "Epoch 4239: Training Loss: 0.2398432493209839 Validation Loss: 0.7255985736846924\n",
      "Epoch 4240: Training Loss: 0.2405069669087728 Validation Loss: 0.7255011796951294\n",
      "Epoch 4241: Training Loss: 0.24041987458864847 Validation Loss: 0.7253903746604919\n",
      "Epoch 4242: Training Loss: 0.24007840951283774 Validation Loss: 0.7250208854675293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4243: Training Loss: 0.23986787100632986 Validation Loss: 0.7252399325370789\n",
      "Epoch 4244: Training Loss: 0.23997144401073456 Validation Loss: 0.7254908084869385\n",
      "Epoch 4245: Training Loss: 0.23957527677218118 Validation Loss: 0.7253022789955139\n",
      "Epoch 4246: Training Loss: 0.239279439051946 Validation Loss: 0.7252024412155151\n",
      "Epoch 4247: Training Loss: 0.23891833424568176 Validation Loss: 0.7251666784286499\n",
      "Epoch 4248: Training Loss: 0.23944814999898276 Validation Loss: 0.7249922156333923\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4249: Training Loss: 0.23948903381824493 Validation Loss: 0.7249405980110168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4250: Training Loss: 0.23927979667981467 Validation Loss: 0.7248548865318298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4251: Training Loss: 0.23910152912139893 Validation Loss: 0.7249463796615601\n",
      "Epoch 4252: Training Loss: 0.23867960274219513 Validation Loss: 0.7249770164489746\n",
      "Epoch 4253: Training Loss: 0.23884278039137521 Validation Loss: 0.7247214317321777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4254: Training Loss: 0.2395992229382197 Validation Loss: 0.724738359451294\n",
      "Epoch 4255: Training Loss: 0.23878533144791922 Validation Loss: 0.7247296571731567\n",
      "Epoch 4256: Training Loss: 0.23904946446418762 Validation Loss: 0.7245534062385559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4257: Training Loss: 0.2391936480998993 Validation Loss: 0.7245720028877258\n",
      "Epoch 4258: Training Loss: 0.2388319124778112 Validation Loss: 0.7248668670654297\n",
      "Epoch 4259: Training Loss: 0.238368089000384 Validation Loss: 0.7248130440711975\n",
      "Epoch 4260: Training Loss: 0.23851781586805978 Validation Loss: 0.7250140309333801\n",
      "Epoch 4261: Training Loss: 0.2382033516963323 Validation Loss: 0.7249643206596375\n",
      "Epoch 4262: Training Loss: 0.23850642641385397 Validation Loss: 0.7249608039855957\n",
      "Epoch 4263: Training Loss: 0.23869119584560394 Validation Loss: 0.7245602607727051\n",
      "Epoch 4264: Training Loss: 0.2381987472375234 Validation Loss: 0.724619448184967\n",
      "Epoch 4265: Training Loss: 0.23805059492588043 Validation Loss: 0.7246026992797852\n",
      "Epoch 4266: Training Loss: 0.23944215973218283 Validation Loss: 0.7246261835098267\n",
      "Epoch 4267: Training Loss: 0.2395015408595403 Validation Loss: 0.7249009609222412\n",
      "Epoch 4268: Training Loss: 0.23678777615229288 Validation Loss: 0.7249553203582764\n",
      "Epoch 4269: Training Loss: 0.2386878033479055 Validation Loss: 0.7249577045440674\n",
      "Epoch 4270: Training Loss: 0.23826934893925986 Validation Loss: 0.7246550917625427\n",
      "Epoch 4271: Training Loss: 0.23781624933083853 Validation Loss: 0.724510669708252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4272: Training Loss: 0.23846090336640677 Validation Loss: 0.7246105670928955\n",
      "Epoch 4273: Training Loss: 0.23802412549654642 Validation Loss: 0.7245877981185913\n",
      "Epoch 4274: Training Loss: 0.2374577671289444 Validation Loss: 0.72454833984375\n",
      "Epoch 4275: Training Loss: 0.2390208194653193 Validation Loss: 0.7242929935455322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4276: Training Loss: 0.23782663544019064 Validation Loss: 0.7245702743530273\n",
      "Epoch 4277: Training Loss: 0.23793360590934753 Validation Loss: 0.724414587020874\n",
      "Epoch 4278: Training Loss: 0.237298846244812 Validation Loss: 0.7245604991912842\n",
      "Epoch 4279: Training Loss: 0.23749462266763052 Validation Loss: 0.7243595123291016\n",
      "Epoch 4280: Training Loss: 0.23710118730862936 Validation Loss: 0.7243859171867371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4281: Training Loss: 0.23702386021614075 Validation Loss: 0.7240313291549683\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4282: Training Loss: 0.23692041138807932 Validation Loss: 0.7238744497299194\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4283: Training Loss: 0.2373580833276113 Validation Loss: 0.7244540452957153\n",
      "Epoch 4284: Training Loss: 0.2368025779724121 Validation Loss: 0.7248054146766663\n",
      "Epoch 4285: Training Loss: 0.23706771433353424 Validation Loss: 0.7247902154922485\n",
      "Epoch 4286: Training Loss: 0.23690535624821982 Validation Loss: 0.7243565320968628\n",
      "Epoch 4287: Training Loss: 0.23701390127340952 Validation Loss: 0.7243133187294006\n",
      "Epoch 4288: Training Loss: 0.2367478907108307 Validation Loss: 0.7244575619697571\n",
      "Epoch 4289: Training Loss: 0.23620178798834482 Validation Loss: 0.724136233329773\n",
      "Epoch 4290: Training Loss: 0.23608879248301187 Validation Loss: 0.7240893244743347\n",
      "Epoch 4291: Training Loss: 0.23774046699206033 Validation Loss: 0.7244129776954651\n",
      "Epoch 4292: Training Loss: 0.23647063970565796 Validation Loss: 0.7241727113723755\n",
      "Epoch 4293: Training Loss: 0.2369805077711741 Validation Loss: 0.7242854833602905\n",
      "Epoch 4294: Training Loss: 0.23726984361807504 Validation Loss: 0.7240558862686157\n",
      "Epoch 4295: Training Loss: 0.23653937876224518 Validation Loss: 0.7244179844856262\n",
      "Epoch 4296: Training Loss: 0.23666006326675415 Validation Loss: 0.7245106101036072\n",
      "Epoch 4297: Training Loss: 0.23620604475339255 Validation Loss: 0.7242422103881836\n",
      "Epoch 4298: Training Loss: 0.23610680798689523 Validation Loss: 0.7243217825889587\n",
      "Epoch 4299: Training Loss: 0.23617926239967346 Validation Loss: 0.7238047122955322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4300: Training Loss: 0.23668400943279266 Validation Loss: 0.723580539226532\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4301: Training Loss: 0.2356959581375122 Validation Loss: 0.7236144542694092\n",
      "Epoch 4302: Training Loss: 0.23606739938259125 Validation Loss: 0.7235800623893738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4303: Training Loss: 0.23736422260602316 Validation Loss: 0.7235895991325378\n",
      "Epoch 4304: Training Loss: 0.23743327955404916 Validation Loss: 0.7236419916152954\n",
      "Epoch 4305: Training Loss: 0.23614782591660818 Validation Loss: 0.7236853837966919\n",
      "Epoch 4306: Training Loss: 0.23550703128178915 Validation Loss: 0.7236777544021606\n",
      "Epoch 4307: Training Loss: 0.23527447879314423 Validation Loss: 0.7239331007003784\n",
      "Epoch 4308: Training Loss: 0.23706490794817606 Validation Loss: 0.7239686846733093\n",
      "Epoch 4309: Training Loss: 0.23502070208390555 Validation Loss: 0.7239061594009399\n",
      "Epoch 4310: Training Loss: 0.23517212768395743 Validation Loss: 0.7239705920219421\n",
      "Epoch 4311: Training Loss: 0.23643661538759866 Validation Loss: 0.7238925099372864\n",
      "Epoch 4312: Training Loss: 0.2352040857076645 Validation Loss: 0.7236684560775757\n",
      "Epoch 4313: Training Loss: 0.23508396248022714 Validation Loss: 0.723260760307312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4314: Training Loss: 0.2354227602481842 Validation Loss: 0.723431408405304\n",
      "Epoch 4315: Training Loss: 0.23496535420417786 Validation Loss: 0.7232726216316223\n",
      "Epoch 4316: Training Loss: 0.2354680746793747 Validation Loss: 0.7231595516204834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4317: Training Loss: 0.23466625809669495 Validation Loss: 0.7232338786125183\n",
      "Epoch 4318: Training Loss: 0.23471941550572714 Validation Loss: 0.7231605052947998\n",
      "Epoch 4319: Training Loss: 0.23525633911291757 Validation Loss: 0.723329484462738\n",
      "Epoch 4320: Training Loss: 0.23477114737033844 Validation Loss: 0.7232124209403992\n",
      "Epoch 4321: Training Loss: 0.23473064104715982 Validation Loss: 0.7231960892677307\n",
      "Epoch 4322: Training Loss: 0.23424111803372702 Validation Loss: 0.7233213186264038\n",
      "Epoch 4323: Training Loss: 0.23493948578834534 Validation Loss: 0.7233092188835144\n",
      "Epoch 4324: Training Loss: 0.23438174029191336 Validation Loss: 0.7232744097709656\n",
      "Epoch 4325: Training Loss: 0.23438498874505362 Validation Loss: 0.7234635949134827\n",
      "Epoch 4326: Training Loss: 0.2342671056588491 Validation Loss: 0.723423957824707\n",
      "Epoch 4327: Training Loss: 0.2341076781352361 Validation Loss: 0.723425030708313\n",
      "Epoch 4328: Training Loss: 0.23498980700969696 Validation Loss: 0.723734438419342\n",
      "Epoch 4329: Training Loss: 0.233646959066391 Validation Loss: 0.7240955233573914\n",
      "Epoch 4330: Training Loss: 0.23474684357643127 Validation Loss: 0.7239631414413452\n",
      "Epoch 4331: Training Loss: 0.23365747928619385 Validation Loss: 0.7236074805259705\n",
      "Epoch 4332: Training Loss: 0.2342370549837748 Validation Loss: 0.7235139608383179\n",
      "Epoch 4333: Training Loss: 0.2328761766354243 Validation Loss: 0.7233134508132935\n",
      "Epoch 4334: Training Loss: 0.23366059362888336 Validation Loss: 0.7230665683746338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4335: Training Loss: 0.23358853658040366 Validation Loss: 0.7229382395744324\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4336: Training Loss: 0.23341765999794006 Validation Loss: 0.7229184508323669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4337: Training Loss: 0.233573650320371 Validation Loss: 0.7230148911476135\n",
      "Epoch 4338: Training Loss: 0.23312815527121225 Validation Loss: 0.7229965925216675\n",
      "Epoch 4339: Training Loss: 0.23397023479143778 Validation Loss: 0.7226821780204773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4340: Training Loss: 0.233237624168396 Validation Loss: 0.7227205634117126\n",
      "Epoch 4341: Training Loss: 0.2332900414864222 Validation Loss: 0.7225515842437744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4342: Training Loss: 0.2331272562344869 Validation Loss: 0.7225487232208252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4343: Training Loss: 0.2334467718998591 Validation Loss: 0.7228090167045593\n",
      "Epoch 4344: Training Loss: 0.23301343619823456 Validation Loss: 0.7230116724967957\n",
      "Epoch 4345: Training Loss: 0.23454463481903076 Validation Loss: 0.7229450941085815\n",
      "Epoch 4346: Training Loss: 0.2331796536842982 Validation Loss: 0.722934365272522\n",
      "Epoch 4347: Training Loss: 0.235033318400383 Validation Loss: 0.7228730320930481\n",
      "Epoch 4348: Training Loss: 0.23340986172358194 Validation Loss: 0.7227808237075806\n",
      "Epoch 4349: Training Loss: 0.23353957633177438 Validation Loss: 0.7225937247276306\n",
      "Epoch 4350: Training Loss: 0.23229711751143137 Validation Loss: 0.7227794528007507\n",
      "Epoch 4351: Training Loss: 0.2339335779349009 Validation Loss: 0.7233604788780212\n",
      "Epoch 4352: Training Loss: 0.23369653026262918 Validation Loss: 0.7231425046920776\n",
      "Epoch 4353: Training Loss: 0.23258088529109955 Validation Loss: 0.7228720784187317\n",
      "Epoch 4354: Training Loss: 0.23272178570429483 Validation Loss: 0.7228171825408936\n",
      "Epoch 4355: Training Loss: 0.23257391154766083 Validation Loss: 0.7228918671607971\n",
      "Epoch 4356: Training Loss: 0.23289267718791962 Validation Loss: 0.7228266596794128\n",
      "Epoch 4357: Training Loss: 0.23210913439591727 Validation Loss: 0.7223474979400635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4358: Training Loss: 0.23272203902403513 Validation Loss: 0.7222925424575806\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4359: Training Loss: 0.23208252588907877 Validation Loss: 0.722327470779419\n",
      "Epoch 4360: Training Loss: 0.2323476274808248 Validation Loss: 0.7228155732154846\n",
      "Epoch 4361: Training Loss: 0.23222607870896658 Validation Loss: 0.7228353023529053\n",
      "Epoch 4362: Training Loss: 0.2315728614727656 Validation Loss: 0.7226751446723938\n",
      "Epoch 4363: Training Loss: 0.2318775107463201 Validation Loss: 0.7226355671882629\n",
      "Epoch 4364: Training Loss: 0.2316457579533259 Validation Loss: 0.722449541091919\n",
      "Epoch 4365: Training Loss: 0.23272674779097238 Validation Loss: 0.7226861715316772\n",
      "Epoch 4366: Training Loss: 0.23224754134813944 Validation Loss: 0.7223867774009705\n",
      "Epoch 4367: Training Loss: 0.23222993314266205 Validation Loss: 0.7223536372184753\n",
      "Epoch 4368: Training Loss: 0.2316128263870875 Validation Loss: 0.7225555777549744\n",
      "Epoch 4369: Training Loss: 0.23161651194095612 Validation Loss: 0.7224090099334717\n",
      "Epoch 4370: Training Loss: 0.23156465590000153 Validation Loss: 0.7225805521011353\n",
      "Epoch 4371: Training Loss: 0.23198534548282623 Validation Loss: 0.7223844528198242\n",
      "Epoch 4372: Training Loss: 0.23082411289215088 Validation Loss: 0.7221602201461792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4373: Training Loss: 0.2311281164487203 Validation Loss: 0.7221105098724365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4374: Training Loss: 0.23182405034701029 Validation Loss: 0.7221696972846985\n",
      "Epoch 4375: Training Loss: 0.23112418750921884 Validation Loss: 0.7225605845451355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4376: Training Loss: 0.23106572031974792 Validation Loss: 0.722557008266449\n",
      "Epoch 4377: Training Loss: 0.23120703796545664 Validation Loss: 0.7225306630134583\n",
      "Epoch 4378: Training Loss: 0.23081640402475992 Validation Loss: 0.7226986885070801\n",
      "Epoch 4379: Training Loss: 0.23122537632783255 Validation Loss: 0.7228047847747803\n",
      "Epoch 4380: Training Loss: 0.23042774200439453 Validation Loss: 0.7226532101631165\n",
      "Epoch 4381: Training Loss: 0.23218786219755808 Validation Loss: 0.7222028374671936\n",
      "Epoch 4382: Training Loss: 0.23050263027350107 Validation Loss: 0.7222325801849365\n",
      "Epoch 4383: Training Loss: 0.23051017026106516 Validation Loss: 0.7219389081001282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4384: Training Loss: 0.23053670426209769 Validation Loss: 0.7219612002372742\n",
      "Epoch 4385: Training Loss: 0.23253882924715677 Validation Loss: 0.7222643494606018\n",
      "Epoch 4386: Training Loss: 0.231089914838473 Validation Loss: 0.7221201658248901\n",
      "Epoch 4387: Training Loss: 0.23085442185401917 Validation Loss: 0.722191333770752\n",
      "Epoch 4388: Training Loss: 0.23137907683849335 Validation Loss: 0.7220094203948975\n",
      "Epoch 4389: Training Loss: 0.22981817026933035 Validation Loss: 0.7220138311386108\n",
      "Epoch 4390: Training Loss: 0.2304904063542684 Validation Loss: 0.7220463156700134\n",
      "Epoch 4391: Training Loss: 0.23004269103209177 Validation Loss: 0.7219263315200806\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4392: Training Loss: 0.23044486343860626 Validation Loss: 0.721796989440918\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4393: Training Loss: 0.23018285632133484 Validation Loss: 0.7220652103424072\n",
      "Epoch 4394: Training Loss: 0.23079872131347656 Validation Loss: 0.7221924066543579\n",
      "Epoch 4395: Training Loss: 0.2299994577964147 Validation Loss: 0.7221506834030151\n",
      "Epoch 4396: Training Loss: 0.22968039413293204 Validation Loss: 0.7218648195266724\n",
      "Epoch 4397: Training Loss: 0.23031496504942575 Validation Loss: 0.721579909324646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4398: Training Loss: 0.22988357146581015 Validation Loss: 0.7217207551002502\n",
      "Epoch 4399: Training Loss: 0.22984038790067038 Validation Loss: 0.7218623757362366\n",
      "Epoch 4400: Training Loss: 0.22990740835666656 Validation Loss: 0.7217971682548523\n",
      "Epoch 4401: Training Loss: 0.23003249863783518 Validation Loss: 0.7216518521308899\n",
      "Epoch 4402: Training Loss: 0.22994242111841837 Validation Loss: 0.7219012379646301\n",
      "Epoch 4403: Training Loss: 0.22933171192804971 Validation Loss: 0.7218603491783142\n",
      "Epoch 4404: Training Loss: 0.22917995850245157 Validation Loss: 0.721518874168396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4405: Training Loss: 0.22962099810441336 Validation Loss: 0.7212585806846619\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4406: Training Loss: 0.23016089697678885 Validation Loss: 0.7210581302642822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4407: Training Loss: 0.22940521438916525 Validation Loss: 0.7212340235710144\n",
      "Epoch 4408: Training Loss: 0.22918387750784555 Validation Loss: 0.7215400338172913\n",
      "Epoch 4409: Training Loss: 0.2286867549022039 Validation Loss: 0.7219634652137756\n",
      "Epoch 4410: Training Loss: 0.2291769136985143 Validation Loss: 0.7218074202537537\n",
      "Epoch 4411: Training Loss: 0.23039613167444864 Validation Loss: 0.7218112945556641\n",
      "Epoch 4412: Training Loss: 0.22920545935630798 Validation Loss: 0.7216771245002747\n",
      "Epoch 4413: Training Loss: 0.22873408099015555 Validation Loss: 0.721753716468811\n",
      "Epoch 4414: Training Loss: 0.22880259156227112 Validation Loss: 0.7213332056999207\n",
      "Epoch 4415: Training Loss: 0.2287072390317917 Validation Loss: 0.7211493849754333\n",
      "Epoch 4416: Training Loss: 0.22847465674082437 Validation Loss: 0.7216159105300903\n",
      "Epoch 4417: Training Loss: 0.22978290418783823 Validation Loss: 0.7217070460319519\n",
      "Epoch 4418: Training Loss: 0.22806685169537863 Validation Loss: 0.7215067744255066\n",
      "Epoch 4419: Training Loss: 0.22888692220052084 Validation Loss: 0.7214739322662354\n",
      "Epoch 4420: Training Loss: 0.2283371537923813 Validation Loss: 0.7210957407951355\n",
      "Epoch 4421: Training Loss: 0.2287157823642095 Validation Loss: 0.7209291458129883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4422: Training Loss: 0.22815641264120737 Validation Loss: 0.7210221290588379\n",
      "Epoch 4423: Training Loss: 0.2283788671096166 Validation Loss: 0.721197783946991\n",
      "Epoch 4424: Training Loss: 0.2277701497077942 Validation Loss: 0.7212734222412109\n",
      "Epoch 4425: Training Loss: 0.22816097239653269 Validation Loss: 0.7213318347930908\n",
      "Epoch 4426: Training Loss: 0.2297470917304357 Validation Loss: 0.7216544151306152\n",
      "Epoch 4427: Training Loss: 0.2289944738149643 Validation Loss: 0.7214744091033936\n",
      "Epoch 4428: Training Loss: 0.22821474075317383 Validation Loss: 0.72149658203125\n",
      "Epoch 4429: Training Loss: 0.2279310723145803 Validation Loss: 0.721373438835144\n",
      "Epoch 4430: Training Loss: 0.2279269446929296 Validation Loss: 0.7213783264160156\n",
      "Epoch 4431: Training Loss: 0.22798976302146912 Validation Loss: 0.7211254835128784\n",
      "Epoch 4432: Training Loss: 0.22842263182004294 Validation Loss: 0.7209070324897766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4433: Training Loss: 0.22776357332865396 Validation Loss: 0.7210755944252014\n",
      "Epoch 4434: Training Loss: 0.22757765154043832 Validation Loss: 0.7212515473365784\n",
      "Epoch 4435: Training Loss: 0.227670818567276 Validation Loss: 0.7214255332946777\n",
      "Epoch 4436: Training Loss: 0.22933693726857504 Validation Loss: 0.7213377952575684\n",
      "Epoch 4437: Training Loss: 0.22787758708000183 Validation Loss: 0.7211076617240906\n",
      "Epoch 4438: Training Loss: 0.227312833070755 Validation Loss: 0.7208912372589111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4439: Training Loss: 0.22755996882915497 Validation Loss: 0.7207346558570862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4440: Training Loss: 0.22809875011444092 Validation Loss: 0.7207599878311157\n",
      "Epoch 4441: Training Loss: 0.2270186891158422 Validation Loss: 0.7205849885940552\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4442: Training Loss: 0.2282584011554718 Validation Loss: 0.7206136584281921\n",
      "Epoch 4443: Training Loss: 0.2279889682928721 Validation Loss: 0.7207363247871399\n",
      "Epoch 4444: Training Loss: 0.22756613294283548 Validation Loss: 0.7212264537811279\n",
      "Epoch 4445: Training Loss: 0.2269746313492457 Validation Loss: 0.7211705446243286\n",
      "Epoch 4446: Training Loss: 0.22664629916350046 Validation Loss: 0.7209694981575012\n",
      "Epoch 4447: Training Loss: 0.22673313319683075 Validation Loss: 0.7208850979804993\n",
      "Epoch 4448: Training Loss: 0.22831551233927408 Validation Loss: 0.7207651138305664\n",
      "Epoch 4449: Training Loss: 0.22677484651406607 Validation Loss: 0.7210449576377869\n",
      "Epoch 4450: Training Loss: 0.22659454743067423 Validation Loss: 0.7205909490585327\n",
      "Epoch 4451: Training Loss: 0.22642632822195688 Validation Loss: 0.7206993103027344\n",
      "Epoch 4452: Training Loss: 0.22657444079717 Validation Loss: 0.7206449508666992\n",
      "Epoch 4453: Training Loss: 0.2266997049252192 Validation Loss: 0.7202808260917664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4454: Training Loss: 0.2269525428613027 Validation Loss: 0.7203619480133057\n",
      "Epoch 4455: Training Loss: 0.22587847709655762 Validation Loss: 0.7207568883895874\n",
      "Epoch 4456: Training Loss: 0.2261819193760554 Validation Loss: 0.7209721803665161\n",
      "Epoch 4457: Training Loss: 0.22583970427513123 Validation Loss: 0.7210375666618347\n",
      "Epoch 4458: Training Loss: 0.22724148631095886 Validation Loss: 0.7208651900291443\n",
      "Epoch 4459: Training Loss: 0.22570160031318665 Validation Loss: 0.7207587361335754\n",
      "Epoch 4460: Training Loss: 0.2260910371939341 Validation Loss: 0.7204822897911072\n",
      "Epoch 4461: Training Loss: 0.2272618760665258 Validation Loss: 0.7201564311981201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4462: Training Loss: 0.22560633222262064 Validation Loss: 0.7202180624008179\n",
      "Epoch 4463: Training Loss: 0.22551939388116202 Validation Loss: 0.7202237248420715\n",
      "Epoch 4464: Training Loss: 0.22656426330407461 Validation Loss: 0.7202277183532715\n",
      "Epoch 4465: Training Loss: 0.2263945738474528 Validation Loss: 0.7204977869987488\n",
      "Epoch 4466: Training Loss: 0.2259525309006373 Validation Loss: 0.7204353213310242\n",
      "Epoch 4467: Training Loss: 0.22559435665607452 Validation Loss: 0.7207615375518799\n",
      "Epoch 4468: Training Loss: 0.22625807424386343 Validation Loss: 0.7209551930427551\n",
      "Epoch 4469: Training Loss: 0.2246524691581726 Validation Loss: 0.7206602096557617\n",
      "Epoch 4470: Training Loss: 0.22570276260375977 Validation Loss: 0.7205016613006592\n",
      "Epoch 4471: Training Loss: 0.2261731227238973 Validation Loss: 0.7203731536865234\n",
      "Epoch 4472: Training Loss: 0.22574918965498605 Validation Loss: 0.7204250693321228\n",
      "Epoch 4473: Training Loss: 0.22541714211304983 Validation Loss: 0.7204764485359192\n",
      "Epoch 4474: Training Loss: 0.22524442772070566 Validation Loss: 0.7204669713973999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4475: Training Loss: 0.22497967878977457 Validation Loss: 0.7205636501312256\n",
      "Epoch 4476: Training Loss: 0.22449997067451477 Validation Loss: 0.7205661535263062\n",
      "Epoch 4477: Training Loss: 0.22490081191062927 Validation Loss: 0.7206081748008728\n",
      "Epoch 4478: Training Loss: 0.2250592658917109 Validation Loss: 0.7202660441398621\n",
      "Epoch 4479: Training Loss: 0.2251143455505371 Validation Loss: 0.72011798620224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4480: Training Loss: 0.22469029327233633 Validation Loss: 0.719925045967102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4481: Training Loss: 0.22462434569994608 Validation Loss: 0.7199991345405579\n",
      "Epoch 4482: Training Loss: 0.22469466924667358 Validation Loss: 0.7197373509407043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4483: Training Loss: 0.22479548553625742 Validation Loss: 0.7199453711509705\n",
      "Epoch 4484: Training Loss: 0.22553874552249908 Validation Loss: 0.7200466394424438\n",
      "Epoch 4485: Training Loss: 0.22492115199565887 Validation Loss: 0.7202185988426208\n",
      "Epoch 4486: Training Loss: 0.22438098986943564 Validation Loss: 0.7204656004905701\n",
      "Epoch 4487: Training Loss: 0.22509545584519705 Validation Loss: 0.7206915616989136\n",
      "Epoch 4488: Training Loss: 0.22530039151509604 Validation Loss: 0.7205772995948792\n",
      "Epoch 4489: Training Loss: 0.2241720954577128 Validation Loss: 0.720217764377594\n",
      "Epoch 4490: Training Loss: 0.2243164380391439 Validation Loss: 0.7198890447616577\n",
      "Epoch 4491: Training Loss: 0.22411835193634033 Validation Loss: 0.7200717329978943\n",
      "Epoch 4492: Training Loss: 0.22473275661468506 Validation Loss: 0.7199906706809998\n",
      "Epoch 4493: Training Loss: 0.2251515587170919 Validation Loss: 0.7198911905288696\n",
      "Epoch 4494: Training Loss: 0.22438948849836984 Validation Loss: 0.7199156284332275\n",
      "Epoch 4495: Training Loss: 0.22395904858907065 Validation Loss: 0.720089852809906\n",
      "Epoch 4496: Training Loss: 0.22477146486441293 Validation Loss: 0.7200614213943481\n",
      "Epoch 4497: Training Loss: 0.22328852117061615 Validation Loss: 0.7201464176177979\n",
      "Epoch 4498: Training Loss: 0.22384628653526306 Validation Loss: 0.7199925780296326\n",
      "Epoch 4499: Training Loss: 0.22426425417264303 Validation Loss: 0.7197923064231873\n",
      "Epoch 4500: Training Loss: 0.22393934428691864 Validation Loss: 0.7198494076728821\n",
      "Epoch 4501: Training Loss: 0.22350179652372995 Validation Loss: 0.7196410894393921\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4502: Training Loss: 0.22327184677124023 Validation Loss: 0.7196428775787354\n",
      "Epoch 4503: Training Loss: 0.223300039768219 Validation Loss: 0.7198559045791626\n",
      "Epoch 4504: Training Loss: 0.2237455944220225 Validation Loss: 0.7197793126106262\n",
      "Epoch 4505: Training Loss: 0.22368206083774567 Validation Loss: 0.7198504209518433\n",
      "Epoch 4506: Training Loss: 0.22366237143675485 Validation Loss: 0.7200943827629089\n",
      "Epoch 4507: Training Loss: 0.22386289139588675 Validation Loss: 0.7198488116264343\n",
      "Epoch 4508: Training Loss: 0.22324961920579275 Validation Loss: 0.7199541330337524\n",
      "Epoch 4509: Training Loss: 0.22278647124767303 Validation Loss: 0.7199456691741943\n",
      "Epoch 4510: Training Loss: 0.22357754906018576 Validation Loss: 0.7198949456214905\n",
      "Epoch 4511: Training Loss: 0.22303720812002817 Validation Loss: 0.7194595336914062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4512: Training Loss: 0.2233913242816925 Validation Loss: 0.7193525433540344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4513: Training Loss: 0.22370213766892752 Validation Loss: 0.719626247882843\n",
      "Epoch 4514: Training Loss: 0.22283650437990823 Validation Loss: 0.7197893261909485\n",
      "Epoch 4515: Training Loss: 0.22325396041075388 Validation Loss: 0.719982385635376\n",
      "Epoch 4516: Training Loss: 0.22263504068056741 Validation Loss: 0.719940185546875\n",
      "Epoch 4517: Training Loss: 0.22221946716308594 Validation Loss: 0.719781219959259\n",
      "Epoch 4518: Training Loss: 0.22269650300343832 Validation Loss: 0.7192201018333435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4519: Training Loss: 0.22308901945749918 Validation Loss: 0.7191177010536194\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4520: Training Loss: 0.2236356089512507 Validation Loss: 0.7193504571914673\n",
      "Epoch 4521: Training Loss: 0.22264291842778525 Validation Loss: 0.7195916771888733\n",
      "Epoch 4522: Training Loss: 0.22205126285552979 Validation Loss: 0.7196206450462341\n",
      "Epoch 4523: Training Loss: 0.22270851830641428 Validation Loss: 0.7193171381950378\n",
      "Epoch 4524: Training Loss: 0.2223055362701416 Validation Loss: 0.7195137143135071\n",
      "Epoch 4525: Training Loss: 0.2243732362985611 Validation Loss: 0.7193836569786072\n",
      "Epoch 4526: Training Loss: 0.22249274949232736 Validation Loss: 0.7195547223091125\n",
      "Epoch 4527: Training Loss: 0.22215811411539713 Validation Loss: 0.7194693088531494\n",
      "Epoch 4528: Training Loss: 0.2220488041639328 Validation Loss: 0.7196653485298157\n",
      "Epoch 4529: Training Loss: 0.22189895808696747 Validation Loss: 0.7198101878166199\n",
      "Epoch 4530: Training Loss: 0.22258989016215006 Validation Loss: 0.7196416854858398\n",
      "Epoch 4531: Training Loss: 0.22168989976247153 Validation Loss: 0.7196230292320251\n",
      "Epoch 4532: Training Loss: 0.22217337290445963 Validation Loss: 0.7193643450737\n",
      "Epoch 4533: Training Loss: 0.22220651308695474 Validation Loss: 0.7189854979515076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4534: Training Loss: 0.22172607481479645 Validation Loss: 0.7191147208213806\n",
      "Epoch 4535: Training Loss: 0.2218321959177653 Validation Loss: 0.7192911505699158\n",
      "Epoch 4536: Training Loss: 0.2218935340642929 Validation Loss: 0.7191237807273865\n",
      "Epoch 4537: Training Loss: 0.22152867913246155 Validation Loss: 0.7191066741943359\n",
      "Epoch 4538: Training Loss: 0.22100852926572165 Validation Loss: 0.718900203704834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4539: Training Loss: 0.22161472837130228 Validation Loss: 0.7189224362373352\n",
      "Epoch 4540: Training Loss: 0.22285776336987814 Validation Loss: 0.7191251516342163\n",
      "Epoch 4541: Training Loss: 0.22093096872170767 Validation Loss: 0.7193636894226074\n",
      "Epoch 4542: Training Loss: 0.22104095419247946 Validation Loss: 0.7193930745124817\n",
      "Epoch 4543: Training Loss: 0.22142761449019113 Validation Loss: 0.7195354700088501\n",
      "Epoch 4544: Training Loss: 0.2221264441808065 Validation Loss: 0.7196440696716309\n",
      "Epoch 4545: Training Loss: 0.22170566519101462 Validation Loss: 0.7195336222648621\n",
      "Epoch 4546: Training Loss: 0.22161516547203064 Validation Loss: 0.7195456027984619\n",
      "Epoch 4547: Training Loss: 0.22169344127178192 Validation Loss: 0.7192511558532715\n",
      "Epoch 4548: Training Loss: 0.22095843156178793 Validation Loss: 0.7187396883964539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4549: Training Loss: 0.22171547015508017 Validation Loss: 0.7187687158584595\n",
      "Epoch 4550: Training Loss: 0.22061323126157126 Validation Loss: 0.7187512516975403\n",
      "Epoch 4551: Training Loss: 0.22072569032510123 Validation Loss: 0.7188547253608704\n",
      "Epoch 4552: Training Loss: 0.22173505028088888 Validation Loss: 0.7188487648963928\n",
      "Epoch 4553: Training Loss: 0.220435564716657 Validation Loss: 0.7190354466438293\n",
      "Epoch 4554: Training Loss: 0.2205233226219813 Validation Loss: 0.7190644145011902\n",
      "Epoch 4555: Training Loss: 0.22059056162834167 Validation Loss: 0.7189481854438782\n",
      "Epoch 4556: Training Loss: 0.22123638788859049 Validation Loss: 0.7187878489494324\n",
      "Epoch 4557: Training Loss: 0.2205187330643336 Validation Loss: 0.718842625617981\n",
      "Epoch 4558: Training Loss: 0.22103976706663767 Validation Loss: 0.7187469005584717\n",
      "Epoch 4559: Training Loss: 0.2203496446212133 Validation Loss: 0.7190788984298706\n",
      "Epoch 4560: Training Loss: 0.21999143064022064 Validation Loss: 0.7189933657646179\n",
      "Epoch 4561: Training Loss: 0.2207495371500651 Validation Loss: 0.7188512682914734\n",
      "Epoch 4562: Training Loss: 0.22139802078406015 Validation Loss: 0.7190030217170715\n",
      "Epoch 4563: Training Loss: 0.2200692743062973 Validation Loss: 0.7189623117446899\n",
      "Epoch 4564: Training Loss: 0.2204626500606537 Validation Loss: 0.7186700701713562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4565: Training Loss: 0.2202805777390798 Validation Loss: 0.7186805605888367\n",
      "Epoch 4566: Training Loss: 0.22006202240784964 Validation Loss: 0.7187434434890747\n",
      "Epoch 4567: Training Loss: 0.21961409350236258 Validation Loss: 0.7187602519989014\n",
      "Epoch 4568: Training Loss: 0.21927683552106222 Validation Loss: 0.7190219759941101\n",
      "Epoch 4569: Training Loss: 0.220395694176356 Validation Loss: 0.7189481258392334\n",
      "Epoch 4570: Training Loss: 0.21996977925300598 Validation Loss: 0.7186250686645508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4571: Training Loss: 0.21956146756807962 Validation Loss: 0.71842360496521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4572: Training Loss: 0.21991070111592612 Validation Loss: 0.7182210087776184\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4573: Training Loss: 0.22063391904036203 Validation Loss: 0.7181669473648071\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4574: Training Loss: 0.219718798995018 Validation Loss: 0.7184620499610901\n",
      "Epoch 4575: Training Loss: 0.21932806074619293 Validation Loss: 0.7187861204147339\n",
      "Epoch 4576: Training Loss: 0.21996413667996725 Validation Loss: 0.7189092040061951\n",
      "Epoch 4577: Training Loss: 0.21947667996088663 Validation Loss: 0.7187725305557251\n",
      "Epoch 4578: Training Loss: 0.21971438328425089 Validation Loss: 0.7186464667320251\n",
      "Epoch 4579: Training Loss: 0.21920358637968698 Validation Loss: 0.7184017896652222\n",
      "Epoch 4580: Training Loss: 0.21954338749249777 Validation Loss: 0.7186028361320496\n",
      "Epoch 4581: Training Loss: 0.2193252295255661 Validation Loss: 0.718462347984314\n",
      "Epoch 4582: Training Loss: 0.21918525298436484 Validation Loss: 0.7185333371162415\n",
      "Epoch 4583: Training Loss: 0.21890073517958322 Validation Loss: 0.7186018228530884\n",
      "Epoch 4584: Training Loss: 0.2188378075758616 Validation Loss: 0.7186416983604431\n",
      "Epoch 4585: Training Loss: 0.21975229183832803 Validation Loss: 0.7186683416366577\n",
      "Epoch 4586: Training Loss: 0.21844751139481863 Validation Loss: 0.7187047004699707\n",
      "Epoch 4587: Training Loss: 0.21864511569341025 Validation Loss: 0.7186709046363831\n",
      "Epoch 4588: Training Loss: 0.21884443859259287 Validation Loss: 0.7186319231987\n",
      "Epoch 4589: Training Loss: 0.21875016391277313 Validation Loss: 0.7183940410614014\n",
      "Epoch 4590: Training Loss: 0.21832242111365 Validation Loss: 0.7183179259300232\n",
      "Epoch 4591: Training Loss: 0.22137698531150818 Validation Loss: 0.7182151675224304\n",
      "Epoch 4592: Training Loss: 0.21879415214061737 Validation Loss: 0.7179192900657654\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4593: Training Loss: 0.21873652438322702 Validation Loss: 0.717943549156189\n",
      "Epoch 4594: Training Loss: 0.2190314084291458 Validation Loss: 0.717883825302124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4595: Training Loss: 0.21849706768989563 Validation Loss: 0.7181302309036255\n",
      "Epoch 4596: Training Loss: 0.2184875855843226 Validation Loss: 0.7185192108154297\n",
      "Epoch 4597: Training Loss: 0.21847063799699148 Validation Loss: 0.7186449766159058\n",
      "Epoch 4598: Training Loss: 0.21813888351122537 Validation Loss: 0.7186445593833923\n",
      "Epoch 4599: Training Loss: 0.21792585154374441 Validation Loss: 0.7187227010726929\n",
      "Epoch 4600: Training Loss: 0.2177828997373581 Validation Loss: 0.7185155749320984\n",
      "Epoch 4601: Training Loss: 0.2177404115597407 Validation Loss: 0.7183854579925537\n",
      "Epoch 4602: Training Loss: 0.21818422277768454 Validation Loss: 0.7183259129524231\n",
      "Epoch 4603: Training Loss: 0.21827965478102365 Validation Loss: 0.718216061592102\n",
      "Epoch 4604: Training Loss: 0.21784385045369467 Validation Loss: 0.7181248068809509\n",
      "Epoch 4605: Training Loss: 0.21900807321071625 Validation Loss: 0.7182128429412842\n",
      "Epoch 4606: Training Loss: 0.2177532563606898 Validation Loss: 0.7182350754737854\n",
      "Epoch 4607: Training Loss: 0.21759429077307382 Validation Loss: 0.7182697653770447\n",
      "Epoch 4608: Training Loss: 0.21821929017702738 Validation Loss: 0.7181414365768433\n",
      "Epoch 4609: Training Loss: 0.21743919452031454 Validation Loss: 0.7180304527282715\n",
      "Epoch 4610: Training Loss: 0.21721710761388144 Validation Loss: 0.7181642651557922\n",
      "Epoch 4611: Training Loss: 0.21755482256412506 Validation Loss: 0.718162477016449\n",
      "Epoch 4612: Training Loss: 0.21796453495820364 Validation Loss: 0.7179690599441528\n",
      "Epoch 4613: Training Loss: 0.21735904614130655 Validation Loss: 0.7179582118988037\n",
      "Epoch 4614: Training Loss: 0.21727717916170755 Validation Loss: 0.7178990840911865\n",
      "Epoch 4615: Training Loss: 0.2174483686685562 Validation Loss: 0.7181103229522705\n",
      "Epoch 4616: Training Loss: 0.21709628403186798 Validation Loss: 0.7179974913597107\n",
      "Epoch 4617: Training Loss: 0.2172626256942749 Validation Loss: 0.717764139175415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4618: Training Loss: 0.2171736309925715 Validation Loss: 0.7177385687828064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4619: Training Loss: 0.21894988417625427 Validation Loss: 0.717728853225708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4620: Training Loss: 0.2169300615787506 Validation Loss: 0.7176560163497925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4621: Training Loss: 0.21692524353663126 Validation Loss: 0.7179105281829834\n",
      "Epoch 4622: Training Loss: 0.21686404943466187 Validation Loss: 0.7180827856063843\n",
      "Epoch 4623: Training Loss: 0.21680390338102976 Validation Loss: 0.718227744102478\n",
      "Epoch 4624: Training Loss: 0.21673740446567535 Validation Loss: 0.7183228135108948\n",
      "Epoch 4625: Training Loss: 0.217391237616539 Validation Loss: 0.718053936958313\n",
      "Epoch 4626: Training Loss: 0.21647575497627258 Validation Loss: 0.7181388139724731\n",
      "Epoch 4627: Training Loss: 0.21638109783331552 Validation Loss: 0.7179636359214783\n",
      "Epoch 4628: Training Loss: 0.2164284735918045 Validation Loss: 0.7179278135299683\n",
      "Epoch 4629: Training Loss: 0.21679763992627463 Validation Loss: 0.7174893617630005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4630: Training Loss: 0.21678508818149567 Validation Loss: 0.7176177501678467\n",
      "Epoch 4631: Training Loss: 0.21659161150455475 Validation Loss: 0.7178815603256226\n",
      "Epoch 4632: Training Loss: 0.21633385121822357 Validation Loss: 0.7180466055870056\n",
      "Epoch 4633: Training Loss: 0.2162450303634008 Validation Loss: 0.7176998257637024\n",
      "Epoch 4634: Training Loss: 0.21625280380249023 Validation Loss: 0.7177017331123352\n",
      "Epoch 4635: Training Loss: 0.2170313000679016 Validation Loss: 0.7178919911384583\n",
      "Epoch 4636: Training Loss: 0.2167875518401464 Validation Loss: 0.7181065678596497\n",
      "Epoch 4637: Training Loss: 0.21606503427028656 Validation Loss: 0.7179432511329651\n",
      "Epoch 4638: Training Loss: 0.2158979574839274 Validation Loss: 0.7178047299385071\n",
      "Epoch 4639: Training Loss: 0.2161852022012075 Validation Loss: 0.7176327109336853\n",
      "Epoch 4640: Training Loss: 0.21603926022847494 Validation Loss: 0.717369794845581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4641: Training Loss: 0.21622351308663687 Validation Loss: 0.7171846032142639\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4642: Training Loss: 0.2160098453362783 Validation Loss: 0.7171708345413208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4643: Training Loss: 0.21561922132968903 Validation Loss: 0.7174380421638489\n",
      "Epoch 4644: Training Loss: 0.21650318801403046 Validation Loss: 0.717368483543396\n",
      "Epoch 4645: Training Loss: 0.21581417322158813 Validation Loss: 0.7171935439109802\n",
      "Epoch 4646: Training Loss: 0.2155504028002421 Validation Loss: 0.7172566056251526\n",
      "Epoch 4647: Training Loss: 0.21586173276106516 Validation Loss: 0.717494010925293\n",
      "Epoch 4648: Training Loss: 0.2155740112066269 Validation Loss: 0.717538058757782\n",
      "Epoch 4649: Training Loss: 0.21555010974407196 Validation Loss: 0.7175742387771606\n",
      "Epoch 4650: Training Loss: 0.2157713621854782 Validation Loss: 0.7176371812820435\n",
      "Epoch 4651: Training Loss: 0.21557540198167166 Validation Loss: 0.7176931500434875\n",
      "Epoch 4652: Training Loss: 0.2155400017897288 Validation Loss: 0.7175147533416748\n",
      "Epoch 4653: Training Loss: 0.21536053717136383 Validation Loss: 0.717303991317749\n",
      "Epoch 4654: Training Loss: 0.2151855727036794 Validation Loss: 0.7173694968223572\n",
      "Epoch 4655: Training Loss: 0.21681349476178488 Validation Loss: 0.717415988445282\n",
      "Epoch 4656: Training Loss: 0.21501707037289938 Validation Loss: 0.7175836563110352\n",
      "Epoch 4657: Training Loss: 0.21496854225794473 Validation Loss: 0.7173988223075867\n",
      "Epoch 4658: Training Loss: 0.2147821436325709 Validation Loss: 0.7173187136650085\n",
      "Epoch 4659: Training Loss: 0.21478104094664255 Validation Loss: 0.7168444395065308\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4660: Training Loss: 0.21479139228661856 Validation Loss: 0.7169036269187927\n",
      "Epoch 4661: Training Loss: 0.21494328478972116 Validation Loss: 0.7173580527305603\n",
      "Epoch 4662: Training Loss: 0.21564813951651254 Validation Loss: 0.7175775766372681\n",
      "Epoch 4663: Training Loss: 0.2154323160648346 Validation Loss: 0.7181572914123535\n",
      "Epoch 4664: Training Loss: 0.21509829660256705 Validation Loss: 0.718024492263794\n",
      "Epoch 4665: Training Loss: 0.21536585688591003 Validation Loss: 0.717428982257843\n",
      "Epoch 4666: Training Loss: 0.2146283984184265 Validation Loss: 0.7172693014144897\n",
      "Epoch 4667: Training Loss: 0.21442932883898416 Validation Loss: 0.7172425389289856\n",
      "Epoch 4668: Training Loss: 0.21428101261456808 Validation Loss: 0.7170619964599609\n",
      "Epoch 4669: Training Loss: 0.21448509891827902 Validation Loss: 0.7168981432914734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4670: Training Loss: 0.21454653143882751 Validation Loss: 0.7168563008308411\n",
      "Epoch 4671: Training Loss: 0.21535355846087137 Validation Loss: 0.7173505425453186\n",
      "Epoch 4672: Training Loss: 0.21400121847788492 Validation Loss: 0.7174667119979858\n",
      "Epoch 4673: Training Loss: 0.21404277284940085 Validation Loss: 0.7174738645553589\n",
      "Epoch 4674: Training Loss: 0.21470044056574503 Validation Loss: 0.71760094165802\n",
      "Epoch 4675: Training Loss: 0.21429820855458578 Validation Loss: 0.7174334526062012\n",
      "Epoch 4676: Training Loss: 0.21451629201571146 Validation Loss: 0.7173148989677429\n",
      "Epoch 4677: Training Loss: 0.21396937469641367 Validation Loss: 0.7173740863800049\n",
      "Epoch 4678: Training Loss: 0.21385379135608673 Validation Loss: 0.7172787189483643\n",
      "Epoch 4679: Training Loss: 0.21406799058119455 Validation Loss: 0.7172431349754333\n",
      "Epoch 4680: Training Loss: 0.21394500136375427 Validation Loss: 0.7171822786331177\n",
      "Epoch 4681: Training Loss: 0.21420167883237204 Validation Loss: 0.7171013355255127\n",
      "Epoch 4682: Training Loss: 0.21414440870285034 Validation Loss: 0.7171916365623474\n",
      "Epoch 4683: Training Loss: 0.21428429087003073 Validation Loss: 0.7171992063522339\n",
      "Epoch 4684: Training Loss: 0.21329830586910248 Validation Loss: 0.7172336578369141\n",
      "Epoch 4685: Training Loss: 0.2134685218334198 Validation Loss: 0.7168341279029846\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4686: Training Loss: 0.21486224234104156 Validation Loss: 0.7168814539909363\n",
      "Epoch 4687: Training Loss: 0.21323005358378092 Validation Loss: 0.7167515158653259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4688: Training Loss: 0.21335835258165994 Validation Loss: 0.7166010737419128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4689: Training Loss: 0.21405190726121268 Validation Loss: 0.7165173888206482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4690: Training Loss: 0.21347553034623465 Validation Loss: 0.7168564200401306\n",
      "Epoch 4691: Training Loss: 0.21279379725456238 Validation Loss: 0.7171087265014648\n",
      "Epoch 4692: Training Loss: 0.21296420693397522 Validation Loss: 0.7173734307289124\n",
      "Epoch 4693: Training Loss: 0.21300838390986124 Validation Loss: 0.7174287438392639\n",
      "Epoch 4694: Training Loss: 0.2136648247639338 Validation Loss: 0.7173025608062744\n",
      "Epoch 4695: Training Loss: 0.21294304231802622 Validation Loss: 0.7172946929931641\n",
      "Epoch 4696: Training Loss: 0.21277526021003723 Validation Loss: 0.7169660329818726\n",
      "Epoch 4697: Training Loss: 0.21287170549233755 Validation Loss: 0.7167351841926575\n",
      "Epoch 4698: Training Loss: 0.21307851374149323 Validation Loss: 0.7166457772254944\n",
      "Epoch 4699: Training Loss: 0.21325901647408804 Validation Loss: 0.7164725065231323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4700: Training Loss: 0.21253393590450287 Validation Loss: 0.7165113687515259\n",
      "Epoch 4701: Training Loss: 0.21264696617921194 Validation Loss: 0.7165258526802063\n",
      "Epoch 4702: Training Loss: 0.21260821322600046 Validation Loss: 0.716978132724762\n",
      "Epoch 4703: Training Loss: 0.2127628376086553 Validation Loss: 0.7168692946434021\n",
      "Epoch 4704: Training Loss: 0.21238878866036734 Validation Loss: 0.716727077960968\n",
      "Epoch 4705: Training Loss: 0.2124218593041102 Validation Loss: 0.7165960669517517\n",
      "Epoch 4706: Training Loss: 0.21258770922819772 Validation Loss: 0.7167007923126221\n",
      "Epoch 4707: Training Loss: 0.2121529926856359 Validation Loss: 0.7166575193405151\n",
      "Epoch 4708: Training Loss: 0.21343032022317251 Validation Loss: 0.716945469379425\n",
      "Epoch 4709: Training Loss: 0.2132717470328013 Validation Loss: 0.7168575525283813\n",
      "Epoch 4710: Training Loss: 0.21227169036865234 Validation Loss: 0.7168802618980408\n",
      "Epoch 4711: Training Loss: 0.21196377277374268 Validation Loss: 0.7169971466064453\n",
      "Epoch 4712: Training Loss: 0.21360454459985098 Validation Loss: 0.7166364192962646\n",
      "Epoch 4713: Training Loss: 0.21192454795042673 Validation Loss: 0.7166972160339355\n",
      "Epoch 4714: Training Loss: 0.2116218606630961 Validation Loss: 0.7171237468719482\n",
      "Epoch 4715: Training Loss: 0.21185089647769928 Validation Loss: 0.717217743396759\n",
      "Epoch 4716: Training Loss: 0.21206626296043396 Validation Loss: 0.7172563672065735\n",
      "Epoch 4717: Training Loss: 0.21195930739243826 Validation Loss: 0.7171056866645813\n",
      "Epoch 4718: Training Loss: 0.2118794322013855 Validation Loss: 0.716839075088501\n",
      "Epoch 4719: Training Loss: 0.21183586617310843 Validation Loss: 0.7166692614555359\n",
      "Epoch 4720: Training Loss: 0.21271316210428873 Validation Loss: 0.7164949774742126\n",
      "Epoch 4721: Training Loss: 0.21156123777230582 Validation Loss: 0.7164545059204102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4722: Training Loss: 0.21176389853159586 Validation Loss: 0.7161905765533447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4723: Training Loss: 0.21232658624649048 Validation Loss: 0.7164592146873474\n",
      "Epoch 4724: Training Loss: 0.2114951511224111 Validation Loss: 0.7165766954421997\n",
      "Epoch 4725: Training Loss: 0.211092342933019 Validation Loss: 0.7163146138191223\n",
      "Epoch 4726: Training Loss: 0.21169116099675497 Validation Loss: 0.7161155343055725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4727: Training Loss: 0.2120046764612198 Validation Loss: 0.7162688970565796\n",
      "Epoch 4728: Training Loss: 0.21090074876944223 Validation Loss: 0.7164808511734009\n",
      "Epoch 4729: Training Loss: 0.21099843084812164 Validation Loss: 0.7164754867553711\n",
      "Epoch 4730: Training Loss: 0.21163912614186606 Validation Loss: 0.7167419195175171\n",
      "Epoch 4731: Training Loss: 0.21199456850687662 Validation Loss: 0.7163897752761841\n",
      "Epoch 4732: Training Loss: 0.21090819935003915 Validation Loss: 0.7163838148117065\n",
      "Epoch 4733: Training Loss: 0.21117502450942993 Validation Loss: 0.7161487936973572\n",
      "Epoch 4734: Training Loss: 0.21004214386145273 Validation Loss: 0.7164020538330078\n",
      "Epoch 4735: Training Loss: 0.2107878029346466 Validation Loss: 0.7166476249694824\n",
      "Epoch 4736: Training Loss: 0.21108244359493256 Validation Loss: 0.7167447209358215\n",
      "Epoch 4737: Training Loss: 0.2121694286664327 Validation Loss: 0.7165297865867615\n",
      "Epoch 4738: Training Loss: 0.21161726117134094 Validation Loss: 0.7164645791053772\n",
      "Epoch 4739: Training Loss: 0.21117260058720908 Validation Loss: 0.7163704633712769\n",
      "Epoch 4740: Training Loss: 0.21057692170143127 Validation Loss: 0.7163492441177368\n",
      "Epoch 4741: Training Loss: 0.2116342435280482 Validation Loss: 0.7163254022598267\n",
      "Epoch 4742: Training Loss: 0.21059979498386383 Validation Loss: 0.7164782285690308\n",
      "Epoch 4743: Training Loss: 0.21096086502075195 Validation Loss: 0.7165505886077881\n",
      "Epoch 4744: Training Loss: 0.21048185726006827 Validation Loss: 0.7165160179138184\n",
      "Epoch 4745: Training Loss: 0.21068852146466574 Validation Loss: 0.7165383100509644\n",
      "Epoch 4746: Training Loss: 0.2107040931781133 Validation Loss: 0.7164556980133057\n",
      "Epoch 4747: Training Loss: 0.21112805108229318 Validation Loss: 0.7161881327629089\n",
      "Epoch 4748: Training Loss: 0.2096992383400599 Validation Loss: 0.716093122959137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4749: Training Loss: 0.21028949320316315 Validation Loss: 0.7157239317893982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4750: Training Loss: 0.21008945008118948 Validation Loss: 0.7159378528594971\n",
      "Epoch 4751: Training Loss: 0.20978707075119019 Validation Loss: 0.7160093188285828\n",
      "Epoch 4752: Training Loss: 0.21048459907372793 Validation Loss: 0.716428279876709\n",
      "Epoch 4753: Training Loss: 0.21042261024316153 Validation Loss: 0.7166213393211365\n",
      "Epoch 4754: Training Loss: 0.21109781662623087 Validation Loss: 0.7165367603302002\n",
      "Epoch 4755: Training Loss: 0.20982909699281058 Validation Loss: 0.7165014743804932\n",
      "Epoch 4756: Training Loss: 0.20940729478995004 Validation Loss: 0.7161591649055481\n",
      "Epoch 4757: Training Loss: 0.21309361358483633 Validation Loss: 0.7159647941589355\n",
      "Epoch 4758: Training Loss: 0.20978671312332153 Validation Loss: 0.715973973274231\n",
      "Epoch 4759: Training Loss: 0.20959108074506125 Validation Loss: 0.7159767746925354\n",
      "Epoch 4760: Training Loss: 0.20982970794041952 Validation Loss: 0.7157737016677856\n",
      "Epoch 4761: Training Loss: 0.2096175899108251 Validation Loss: 0.7161455154418945\n",
      "Epoch 4762: Training Loss: 0.20981265107790628 Validation Loss: 0.7164223790168762\n",
      "Epoch 4763: Training Loss: 0.20940201977888742 Validation Loss: 0.7164437174797058\n",
      "Epoch 4764: Training Loss: 0.20950105289618173 Validation Loss: 0.7163781523704529\n",
      "Epoch 4765: Training Loss: 0.2094342758258184 Validation Loss: 0.7161293625831604\n",
      "Epoch 4766: Training Loss: 0.20958037177721658 Validation Loss: 0.7159581184387207\n",
      "Epoch 4767: Training Loss: 0.20931736131509146 Validation Loss: 0.7160505652427673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4768: Training Loss: 0.20966167747974396 Validation Loss: 0.7160754799842834\n",
      "Epoch 4769: Training Loss: 0.2090270072221756 Validation Loss: 0.7160218954086304\n",
      "Epoch 4770: Training Loss: 0.21108085910479227 Validation Loss: 0.715903639793396\n",
      "Epoch 4771: Training Loss: 0.21021446585655212 Validation Loss: 0.7161100506782532\n",
      "Epoch 4772: Training Loss: 0.20928364992141724 Validation Loss: 0.7163174152374268\n",
      "Epoch 4773: Training Loss: 0.20911244054635367 Validation Loss: 0.7162807583808899\n",
      "Epoch 4774: Training Loss: 0.20891471207141876 Validation Loss: 0.715991735458374\n",
      "Epoch 4775: Training Loss: 0.20973756412665048 Validation Loss: 0.7161035537719727\n",
      "Epoch 4776: Training Loss: 0.20923401912053427 Validation Loss: 0.7158553004264832\n",
      "Epoch 4777: Training Loss: 0.20892687638600668 Validation Loss: 0.7159487009048462\n",
      "Epoch 4778: Training Loss: 0.20876272022724152 Validation Loss: 0.7160632610321045\n",
      "Epoch 4779: Training Loss: 0.20866874853769937 Validation Loss: 0.7160832285881042\n",
      "Epoch 4780: Training Loss: 0.2088758647441864 Validation Loss: 0.715857744216919\n",
      "Epoch 4781: Training Loss: 0.20907541612784067 Validation Loss: 0.7156614065170288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4782: Training Loss: 0.2085490624109904 Validation Loss: 0.7157197594642639\n",
      "Epoch 4783: Training Loss: 0.20828923086325327 Validation Loss: 0.7156296372413635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4784: Training Loss: 0.20839864512284598 Validation Loss: 0.7155877351760864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4785: Training Loss: 0.2089706907669703 Validation Loss: 0.7158055901527405\n",
      "Epoch 4786: Training Loss: 0.2084222932656606 Validation Loss: 0.7160903215408325\n",
      "Epoch 4787: Training Loss: 0.20856273670991263 Validation Loss: 0.7163258194923401\n",
      "Epoch 4788: Training Loss: 0.2086994449297587 Validation Loss: 0.7158952951431274\n",
      "Epoch 4789: Training Loss: 0.2082778513431549 Validation Loss: 0.7156069874763489\n",
      "Epoch 4790: Training Loss: 0.20798354347546896 Validation Loss: 0.7153661251068115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4791: Training Loss: 0.2081925223271052 Validation Loss: 0.7154437899589539\n",
      "Epoch 4792: Training Loss: 0.2081833779811859 Validation Loss: 0.7153181433677673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4793: Training Loss: 0.2078531632820765 Validation Loss: 0.7154233455657959\n",
      "Epoch 4794: Training Loss: 0.20815921823183695 Validation Loss: 0.7156992554664612\n",
      "Epoch 4795: Training Loss: 0.20757170021533966 Validation Loss: 0.7158793210983276\n",
      "Epoch 4796: Training Loss: 0.20763270060221353 Validation Loss: 0.7158587574958801\n",
      "Epoch 4797: Training Loss: 0.2083107978105545 Validation Loss: 0.7158769369125366\n",
      "Epoch 4798: Training Loss: 0.20831110576788583 Validation Loss: 0.7160513997077942\n",
      "Epoch 4799: Training Loss: 0.20773819088935852 Validation Loss: 0.7157642245292664\n",
      "Epoch 4800: Training Loss: 0.2080275366703669 Validation Loss: 0.7157058119773865\n",
      "Epoch 4801: Training Loss: 0.20746750632921854 Validation Loss: 0.7156811952590942\n",
      "Epoch 4802: Training Loss: 0.20754340787728628 Validation Loss: 0.7157062292098999\n",
      "Epoch 4803: Training Loss: 0.2073885848124822 Validation Loss: 0.7156307697296143\n",
      "Epoch 4804: Training Loss: 0.20752145846684775 Validation Loss: 0.7155430912971497\n",
      "Epoch 4805: Training Loss: 0.2075088918209076 Validation Loss: 0.7153835296630859\n",
      "Epoch 4806: Training Loss: 0.20819718639055887 Validation Loss: 0.7156363129615784\n",
      "Epoch 4807: Training Loss: 0.20786639551321665 Validation Loss: 0.7159473896026611\n",
      "Epoch 4808: Training Loss: 0.2083874593178431 Validation Loss: 0.7157329320907593\n",
      "Epoch 4809: Training Loss: 0.2070639282464981 Validation Loss: 0.7154083847999573\n",
      "Epoch 4810: Training Loss: 0.20742644369602203 Validation Loss: 0.715465247631073\n",
      "Epoch 4811: Training Loss: 0.20697715878486633 Validation Loss: 0.7155811786651611\n",
      "Epoch 4812: Training Loss: 0.20789884527524313 Validation Loss: 0.715455949306488\n",
      "Epoch 4813: Training Loss: 0.20661766330401102 Validation Loss: 0.7157538533210754\n",
      "Epoch 4814: Training Loss: 0.20819608370463052 Validation Loss: 0.7155665755271912\n",
      "Epoch 4815: Training Loss: 0.20715138812859854 Validation Loss: 0.7155683636665344\n",
      "Epoch 4816: Training Loss: 0.20653359591960907 Validation Loss: 0.7156749367713928\n",
      "Epoch 4817: Training Loss: 0.20756782591342926 Validation Loss: 0.7158907055854797\n",
      "Epoch 4818: Training Loss: 0.2067209134499232 Validation Loss: 0.7156201004981995\n",
      "Epoch 4819: Training Loss: 0.2070724368095398 Validation Loss: 0.7157034873962402\n",
      "Epoch 4820: Training Loss: 0.20651803414026895 Validation Loss: 0.7153445482254028\n",
      "Epoch 4821: Training Loss: 0.20706793665885925 Validation Loss: 0.7152895331382751\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4822: Training Loss: 0.20527897775173187 Validation Loss: 0.7153956890106201\n",
      "Epoch 4823: Training Loss: 0.2063299467166265 Validation Loss: 0.7155769467353821\n",
      "Epoch 4824: Training Loss: 0.2065222511688868 Validation Loss: 0.7156181931495667\n",
      "Epoch 4825: Training Loss: 0.20647030572096506 Validation Loss: 0.7157400250434875\n",
      "Epoch 4826: Training Loss: 0.2067946990331014 Validation Loss: 0.7158342599868774\n",
      "Epoch 4827: Training Loss: 0.2065530071655909 Validation Loss: 0.7157829403877258\n",
      "Epoch 4828: Training Loss: 0.20622570315996805 Validation Loss: 0.7156081795692444\n",
      "Epoch 4829: Training Loss: 0.20732529958089194 Validation Loss: 0.7154108285903931\n",
      "Epoch 4830: Training Loss: 0.20612948636213937 Validation Loss: 0.7153459191322327\n",
      "Epoch 4831: Training Loss: 0.20635117093722025 Validation Loss: 0.7151809334754944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4832: Training Loss: 0.20598829785982767 Validation Loss: 0.7153288722038269\n",
      "Epoch 4833: Training Loss: 0.20643902818361917 Validation Loss: 0.7157989144325256\n",
      "Epoch 4834: Training Loss: 0.2060829351345698 Validation Loss: 0.7156167030334473\n",
      "Epoch 4835: Training Loss: 0.20586107671260834 Validation Loss: 0.7151121497154236\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4836: Training Loss: 0.20606867969036102 Validation Loss: 0.7148491144180298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4837: Training Loss: 0.2064655919869741 Validation Loss: 0.714911162853241\n",
      "Epoch 4838: Training Loss: 0.20555523037910461 Validation Loss: 0.7149013876914978\n",
      "Epoch 4839: Training Loss: 0.2062687873840332 Validation Loss: 0.7149611115455627\n",
      "Epoch 4840: Training Loss: 0.2055323968331019 Validation Loss: 0.7154732942581177\n",
      "Epoch 4841: Training Loss: 0.2047183314959208 Validation Loss: 0.7151637077331543\n",
      "Epoch 4842: Training Loss: 0.20609704653422037 Validation Loss: 0.7153297662734985\n",
      "Epoch 4843: Training Loss: 0.20552064975102743 Validation Loss: 0.7156575918197632\n",
      "Epoch 4844: Training Loss: 0.20577536026636759 Validation Loss: 0.7155774235725403\n",
      "Epoch 4845: Training Loss: 0.2054583082596461 Validation Loss: 0.7152828574180603\n",
      "Epoch 4846: Training Loss: 0.20601529876391092 Validation Loss: 0.7151448130607605\n",
      "Epoch 4847: Training Loss: 0.2057633101940155 Validation Loss: 0.7150923609733582\n",
      "Epoch 4848: Training Loss: 0.20683185259501138 Validation Loss: 0.715100884437561\n",
      "Epoch 4849: Training Loss: 0.20645167430241904 Validation Loss: 0.7150396108627319\n",
      "Epoch 4850: Training Loss: 0.205183078845342 Validation Loss: 0.7153252363204956\n",
      "Epoch 4851: Training Loss: 0.205083966255188 Validation Loss: 0.7154222726821899\n",
      "Epoch 4852: Training Loss: 0.20628571013609567 Validation Loss: 0.7152868509292603\n",
      "Epoch 4853: Training Loss: 0.2051402380069097 Validation Loss: 0.7151220440864563\n",
      "Epoch 4854: Training Loss: 0.20481998225053152 Validation Loss: 0.7149981260299683\n",
      "Epoch 4855: Training Loss: 0.20457184314727783 Validation Loss: 0.7151956558227539\n",
      "Epoch 4856: Training Loss: 0.2056400179862976 Validation Loss: 0.7152234315872192\n",
      "Epoch 4857: Training Loss: 0.20525256295998892 Validation Loss: 0.7149242758750916\n",
      "Epoch 4858: Training Loss: 0.20465205113093057 Validation Loss: 0.7151031494140625\n",
      "Epoch 4859: Training Loss: 0.2047917346159617 Validation Loss: 0.7153257131576538\n",
      "Epoch 4860: Training Loss: 0.20569228132565817 Validation Loss: 0.7153962850570679\n",
      "Epoch 4861: Training Loss: 0.20510279635588327 Validation Loss: 0.7151575684547424\n",
      "Epoch 4862: Training Loss: 0.2046478440364202 Validation Loss: 0.7149540185928345\n",
      "Epoch 4863: Training Loss: 0.20494181414445242 Validation Loss: 0.7149728536605835\n",
      "Epoch 4864: Training Loss: 0.20520968238512674 Validation Loss: 0.7153663635253906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4865: Training Loss: 0.20436089734236398 Validation Loss: 0.7153257131576538\n",
      "Epoch 4866: Training Loss: 0.2048019071420034 Validation Loss: 0.7151203155517578\n",
      "Epoch 4867: Training Loss: 0.2046906699736913 Validation Loss: 0.7148294448852539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4868: Training Loss: 0.2041806677977244 Validation Loss: 0.7149832248687744\n",
      "Epoch 4869: Training Loss: 0.20418566962083182 Validation Loss: 0.7145278453826904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4870: Training Loss: 0.2049139440059662 Validation Loss: 0.7145233750343323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4871: Training Loss: 0.20417865117390951 Validation Loss: 0.7146925926208496\n",
      "Epoch 4872: Training Loss: 0.2041913866996765 Validation Loss: 0.7146390080451965\n",
      "Epoch 4873: Training Loss: 0.20451492071151733 Validation Loss: 0.7148802876472473\n",
      "Epoch 4874: Training Loss: 0.20533196131388345 Validation Loss: 0.7149404883384705\n",
      "Epoch 4875: Training Loss: 0.2039212385813395 Validation Loss: 0.7150629758834839\n",
      "Epoch 4876: Training Loss: 0.20417973895867667 Validation Loss: 0.7151436805725098\n",
      "Epoch 4877: Training Loss: 0.20397586127122244 Validation Loss: 0.7150341868400574\n",
      "Epoch 4878: Training Loss: 0.2041298747062683 Validation Loss: 0.715137779712677\n",
      "Epoch 4879: Training Loss: 0.2035899410645167 Validation Loss: 0.7153117060661316\n",
      "Epoch 4880: Training Loss: 0.20383753875891367 Validation Loss: 0.7149989008903503\n",
      "Epoch 4881: Training Loss: 0.20482686658700308 Validation Loss: 0.7147853374481201\n",
      "Epoch 4882: Training Loss: 0.2035189469655355 Validation Loss: 0.7147493362426758\n",
      "Epoch 4883: Training Loss: 0.20364471276601157 Validation Loss: 0.7146040797233582\n",
      "Epoch 4884: Training Loss: 0.20292818546295166 Validation Loss: 0.7146700024604797\n",
      "Epoch 4885: Training Loss: 0.20328671236832938 Validation Loss: 0.714945912361145\n",
      "Epoch 4886: Training Loss: 0.20335257053375244 Validation Loss: 0.7148942947387695\n",
      "Epoch 4887: Training Loss: 0.20375798145929971 Validation Loss: 0.7151959538459778\n",
      "Epoch 4888: Training Loss: 0.20338252186775208 Validation Loss: 0.7150900363922119\n",
      "Epoch 4889: Training Loss: 0.20336047808329263 Validation Loss: 0.7148844599723816\n",
      "Epoch 4890: Training Loss: 0.20375957091649374 Validation Loss: 0.7147745490074158\n",
      "Epoch 4891: Training Loss: 0.20367036759853363 Validation Loss: 0.7146860957145691\n",
      "Epoch 4892: Training Loss: 0.20471091071764627 Validation Loss: 0.7147913575172424\n",
      "Epoch 4893: Training Loss: 0.20322518547375998 Validation Loss: 0.714871883392334\n",
      "Epoch 4894: Training Loss: 0.20421970387299856 Validation Loss: 0.7145659923553467\n",
      "Epoch 4895: Training Loss: 0.20400698482990265 Validation Loss: 0.714418888092041\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4896: Training Loss: 0.2044734905163447 Validation Loss: 0.7143018245697021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4897: Training Loss: 0.2032027244567871 Validation Loss: 0.7146199941635132\n",
      "Epoch 4898: Training Loss: 0.20298685133457184 Validation Loss: 0.7148123979568481\n",
      "Epoch 4899: Training Loss: 0.2028715858856837 Validation Loss: 0.7150683999061584\n",
      "Epoch 4900: Training Loss: 0.2032047063112259 Validation Loss: 0.7154911756515503\n",
      "Epoch 4901: Training Loss: 0.20289944112300873 Validation Loss: 0.7153939604759216\n",
      "Epoch 4902: Training Loss: 0.20278404653072357 Validation Loss: 0.7154561877250671\n",
      "Epoch 4903: Training Loss: 0.2024174084266027 Validation Loss: 0.7152087688446045\n",
      "Epoch 4904: Training Loss: 0.20515582958857217 Validation Loss: 0.7147517204284668\n",
      "Epoch 4905: Training Loss: 0.20288008948167166 Validation Loss: 0.7144917845726013\n",
      "Epoch 4906: Training Loss: 0.20415867368380228 Validation Loss: 0.7145877480506897\n",
      "Epoch 4907: Training Loss: 0.20301630596319833 Validation Loss: 0.7146964073181152\n",
      "Epoch 4908: Training Loss: 0.20264450709025064 Validation Loss: 0.714697539806366\n",
      "Epoch 4909: Training Loss: 0.20257274309794107 Validation Loss: 0.7149632573127747\n",
      "Epoch 4910: Training Loss: 0.20208285748958588 Validation Loss: 0.7152618169784546\n",
      "Epoch 4911: Training Loss: 0.20230624079704285 Validation Loss: 0.7149811387062073\n",
      "Epoch 4912: Training Loss: 0.20235049724578857 Validation Loss: 0.7147157192230225\n",
      "Epoch 4913: Training Loss: 0.202073206504186 Validation Loss: 0.7146963477134705\n",
      "Epoch 4914: Training Loss: 0.20214941104253134 Validation Loss: 0.7145032286643982\n",
      "Epoch 4915: Training Loss: 0.2020780990521113 Validation Loss: 0.7144870758056641\n",
      "Epoch 4916: Training Loss: 0.2020158072312673 Validation Loss: 0.7142242193222046\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4917: Training Loss: 0.20190181334813437 Validation Loss: 0.7141745090484619\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4918: Training Loss: 0.20261757572491965 Validation Loss: 0.714119017124176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4919: Training Loss: 0.20222237706184387 Validation Loss: 0.7144224047660828\n",
      "Epoch 4920: Training Loss: 0.20170705517133078 Validation Loss: 0.7144359946250916\n",
      "Epoch 4921: Training Loss: 0.20217218498388925 Validation Loss: 0.7143869996070862\n",
      "Epoch 4922: Training Loss: 0.20215795934200287 Validation Loss: 0.7146120667457581\n",
      "Epoch 4923: Training Loss: 0.20228877166906992 Validation Loss: 0.7145205140113831\n",
      "Epoch 4924: Training Loss: 0.20182112356026968 Validation Loss: 0.7146214246749878\n",
      "Epoch 4925: Training Loss: 0.20244050522645315 Validation Loss: 0.7144068479537964\n",
      "Epoch 4926: Training Loss: 0.20175967117150626 Validation Loss: 0.714504063129425\n",
      "Epoch 4927: Training Loss: 0.20190548400084177 Validation Loss: 0.7149615287780762\n",
      "Epoch 4928: Training Loss: 0.2017222742239634 Validation Loss: 0.7148829102516174\n",
      "Epoch 4929: Training Loss: 0.20284420251846313 Validation Loss: 0.7146497368812561\n",
      "Epoch 4930: Training Loss: 0.2015716383854548 Validation Loss: 0.7147696614265442\n",
      "Epoch 4931: Training Loss: 0.20172427594661713 Validation Loss: 0.714766800403595\n",
      "Epoch 4932: Training Loss: 0.20168305933475494 Validation Loss: 0.7145459055900574\n",
      "Epoch 4933: Training Loss: 0.20139144857724509 Validation Loss: 0.7147618532180786\n",
      "Epoch 4934: Training Loss: 0.20276313026746115 Validation Loss: 0.7144198417663574\n",
      "Epoch 4935: Training Loss: 0.20236080388228098 Validation Loss: 0.7143665552139282\n",
      "Epoch 4936: Training Loss: 0.20129833122094473 Validation Loss: 0.7143176198005676\n",
      "Epoch 4937: Training Loss: 0.20127134025096893 Validation Loss: 0.7146663665771484\n",
      "Epoch 4938: Training Loss: 0.20092376073201498 Validation Loss: 0.7144203782081604\n",
      "Epoch 4939: Training Loss: 0.2014644593000412 Validation Loss: 0.7141143679618835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4940: Training Loss: 0.20089265207449594 Validation Loss: 0.7145668268203735\n",
      "Epoch 4941: Training Loss: 0.2027105689048767 Validation Loss: 0.7144116759300232\n",
      "Epoch 4942: Training Loss: 0.20160135130087534 Validation Loss: 0.7146424651145935\n",
      "Epoch 4943: Training Loss: 0.20065446694691977 Validation Loss: 0.7144540548324585\n",
      "Epoch 4944: Training Loss: 0.2006864051024119 Validation Loss: 0.7142336964607239\n",
      "Epoch 4945: Training Loss: 0.20066331326961517 Validation Loss: 0.7141110301017761\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4946: Training Loss: 0.2005330721537272 Validation Loss: 0.7143704295158386\n",
      "Epoch 4947: Training Loss: 0.19968402882417044 Validation Loss: 0.714599609375\n",
      "Epoch 4948: Training Loss: 0.20080804824829102 Validation Loss: 0.7147020101547241\n",
      "Epoch 4949: Training Loss: 0.2002002497514089 Validation Loss: 0.7144541144371033\n",
      "Epoch 4950: Training Loss: 0.20090576012929282 Validation Loss: 0.7143892645835876\n",
      "Epoch 4951: Training Loss: 0.20074195166428885 Validation Loss: 0.7144874334335327\n",
      "Epoch 4952: Training Loss: 0.201874112089475 Validation Loss: 0.7145581245422363\n",
      "Epoch 4953: Training Loss: 0.2006238798300425 Validation Loss: 0.7146991491317749\n",
      "Epoch 4954: Training Loss: 0.20067964990933737 Validation Loss: 0.7143071889877319\n",
      "Epoch 4955: Training Loss: 0.20023216307163239 Validation Loss: 0.7142429947853088\n",
      "Epoch 4956: Training Loss: 0.20017904539903006 Validation Loss: 0.714287281036377\n",
      "Epoch 4957: Training Loss: 0.20049182077248892 Validation Loss: 0.7139990329742432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4958: Training Loss: 0.2002410590648651 Validation Loss: 0.7138409614562988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4959: Training Loss: 0.1998788317044576 Validation Loss: 0.7139612436294556\n",
      "Epoch 4960: Training Loss: 0.2002317508061727 Validation Loss: 0.7141926884651184\n",
      "Epoch 4961: Training Loss: 0.20034565528233847 Validation Loss: 0.7144915461540222\n",
      "Epoch 4962: Training Loss: 0.200437992811203 Validation Loss: 0.7145339250564575\n",
      "Epoch 4963: Training Loss: 0.20066031316916147 Validation Loss: 0.7146687507629395\n",
      "Epoch 4964: Training Loss: 0.19969474275906882 Validation Loss: 0.7145899534225464\n",
      "Epoch 4965: Training Loss: 0.19974568982919058 Validation Loss: 0.7145060896873474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4966: Training Loss: 0.20046652853488922 Validation Loss: 0.7144948244094849\n",
      "Epoch 4967: Training Loss: 0.1998747189839681 Validation Loss: 0.7141817212104797\n",
      "Epoch 4968: Training Loss: 0.2001519501209259 Validation Loss: 0.7139350175857544\n",
      "Epoch 4969: Training Loss: 0.19971108933289847 Validation Loss: 0.7135854363441467\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4970: Training Loss: 0.2006352891524633 Validation Loss: 0.7135721445083618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4971: Training Loss: 0.1997941533724467 Validation Loss: 0.7137812376022339\n",
      "Epoch 4972: Training Loss: 0.2000137766202291 Validation Loss: 0.7137174010276794\n",
      "Epoch 4973: Training Loss: 0.19985419511795044 Validation Loss: 0.7143617272377014\n",
      "Epoch 4974: Training Loss: 0.19967354834079742 Validation Loss: 0.7144194841384888\n",
      "Epoch 4975: Training Loss: 0.19956906139850616 Validation Loss: 0.7145076394081116\n",
      "Epoch 4976: Training Loss: 0.19994105398654938 Validation Loss: 0.7145682573318481\n",
      "Epoch 4977: Training Loss: 0.19973886013031006 Validation Loss: 0.7140589356422424\n",
      "Epoch 4978: Training Loss: 0.19939177731672922 Validation Loss: 0.7142191529273987\n",
      "Epoch 4979: Training Loss: 0.2000753084818522 Validation Loss: 0.7143402695655823\n",
      "Epoch 4980: Training Loss: 0.19902323683102927 Validation Loss: 0.7142756581306458\n",
      "Epoch 4981: Training Loss: 0.19924222429593405 Validation Loss: 0.7139909863471985\n",
      "Epoch 4982: Training Loss: 0.19879630704720816 Validation Loss: 0.714271068572998\n",
      "Epoch 4983: Training Loss: 0.19881698489189148 Validation Loss: 0.7141935229301453\n",
      "Epoch 4984: Training Loss: 0.198891152938207 Validation Loss: 0.7145564556121826\n",
      "Epoch 4985: Training Loss: 0.1989684800306956 Validation Loss: 0.7141812443733215\n",
      "Epoch 4986: Training Loss: 0.1989573339621226 Validation Loss: 0.7140644192695618\n",
      "Epoch 4987: Training Loss: 0.19856863220532736 Validation Loss: 0.7140436768531799\n",
      "Epoch 4988: Training Loss: 0.19904719789822897 Validation Loss: 0.7139478325843811\n",
      "Epoch 4989: Training Loss: 0.19841555754343668 Validation Loss: 0.7137691378593445\n",
      "Epoch 4990: Training Loss: 0.1986687034368515 Validation Loss: 0.7134759426116943\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4991: Training Loss: 0.1989669899145762 Validation Loss: 0.7135974168777466\n",
      "Epoch 4992: Training Loss: 0.19909568627675375 Validation Loss: 0.7140052914619446\n",
      "Epoch 4993: Training Loss: 0.19862843553225198 Validation Loss: 0.7139350175857544\n",
      "Epoch 4994: Training Loss: 0.1985486845175425 Validation Loss: 0.7141298055648804\n",
      "Epoch 4995: Training Loss: 0.19915246466795603 Validation Loss: 0.7146539688110352\n",
      "Epoch 4996: Training Loss: 0.1990630328655243 Validation Loss: 0.7146464586257935\n",
      "Epoch 4997: Training Loss: 0.1982711503903071 Validation Loss: 0.7141616940498352\n",
      "Epoch 4998: Training Loss: 0.19841905434926352 Validation Loss: 0.7138649225234985\n",
      "Epoch 4999: Training Loss: 0.19870997965335846 Validation Loss: 0.7138018608093262\n",
      "Epoch 5000: Training Loss: 0.19813174506028494 Validation Loss: 0.7140776515007019\n",
      "Epoch 5001: Training Loss: 0.19895727932453156 Validation Loss: 0.7137758731842041\n",
      "Epoch 5002: Training Loss: 0.19864642123381296 Validation Loss: 0.7137588858604431\n",
      "Epoch 5003: Training Loss: 0.19822675486405691 Validation Loss: 0.7136971950531006\n",
      "Epoch 5004: Training Loss: 0.1984575887521108 Validation Loss: 0.7134791612625122\n",
      "Epoch 5005: Training Loss: 0.1984718789656957 Validation Loss: 0.7134951949119568\n",
      "Epoch 5006: Training Loss: 0.1978804220755895 Validation Loss: 0.7136877775192261\n",
      "Epoch 5007: Training Loss: 0.19786745309829712 Validation Loss: 0.7137603759765625\n",
      "Epoch 5008: Training Loss: 0.19760101536909738 Validation Loss: 0.7138556241989136\n",
      "Epoch 5009: Training Loss: 0.19757433732350668 Validation Loss: 0.7139478325843811\n",
      "Epoch 5010: Training Loss: 0.19830525914827982 Validation Loss: 0.7140284776687622\n",
      "Epoch 5011: Training Loss: 0.19792131582895914 Validation Loss: 0.7140112519264221\n",
      "Epoch 5012: Training Loss: 0.1973298192024231 Validation Loss: 0.7140067219734192\n",
      "Epoch 5013: Training Loss: 0.1982294817765554 Validation Loss: 0.7138487100601196\n",
      "Epoch 5014: Training Loss: 0.1975600024064382 Validation Loss: 0.7135956883430481\n",
      "Epoch 5015: Training Loss: 0.1982848048210144 Validation Loss: 0.7136573195457458\n",
      "Epoch 5016: Training Loss: 0.1977050950129827 Validation Loss: 0.7137817144393921\n",
      "Epoch 5017: Training Loss: 0.19773588081200918 Validation Loss: 0.7138330340385437\n",
      "Epoch 5018: Training Loss: 0.19753427803516388 Validation Loss: 0.7138341069221497\n",
      "Epoch 5019: Training Loss: 0.1972653071085612 Validation Loss: 0.7137827277183533\n",
      "Epoch 5020: Training Loss: 0.19728091855843863 Validation Loss: 0.7137975692749023\n",
      "Epoch 5021: Training Loss: 0.19710453848044077 Validation Loss: 0.7139220833778381\n",
      "Epoch 5022: Training Loss: 0.19697558879852295 Validation Loss: 0.7138924598693848\n",
      "Epoch 5023: Training Loss: 0.19716618458429971 Validation Loss: 0.7137324213981628\n",
      "Epoch 5024: Training Loss: 0.19799500703811646 Validation Loss: 0.7135192155838013\n",
      "Epoch 5025: Training Loss: 0.19695928196112314 Validation Loss: 0.7136592268943787\n",
      "Epoch 5026: Training Loss: 0.19677663346131644 Validation Loss: 0.7139421701431274\n",
      "Epoch 5027: Training Loss: 0.19752506911754608 Validation Loss: 0.7137861847877502\n",
      "Epoch 5028: Training Loss: 0.1972301403681437 Validation Loss: 0.7135294675827026\n",
      "Epoch 5029: Training Loss: 0.19708319505055746 Validation Loss: 0.7138050198554993\n",
      "Epoch 5030: Training Loss: 0.19738907118638357 Validation Loss: 0.7136500477790833\n",
      "Epoch 5031: Training Loss: 0.19702587028344473 Validation Loss: 0.7137649655342102\n",
      "Epoch 5032: Training Loss: 0.19671596586704254 Validation Loss: 0.7135714888572693\n",
      "Epoch 5033: Training Loss: 0.19640158116817474 Validation Loss: 0.7136143445968628\n",
      "Epoch 5034: Training Loss: 0.19663136204083762 Validation Loss: 0.7136117219924927\n",
      "Epoch 5035: Training Loss: 0.19652557373046875 Validation Loss: 0.713888943195343\n",
      "Epoch 5036: Training Loss: 0.1966513693332672 Validation Loss: 0.7139481902122498\n",
      "Epoch 5037: Training Loss: 0.1969646861155828 Validation Loss: 0.7137467861175537\n",
      "Epoch 5038: Training Loss: 0.19637665649255118 Validation Loss: 0.7136847972869873\n",
      "Epoch 5039: Training Loss: 0.19651695589224497 Validation Loss: 0.7137715816497803\n",
      "Epoch 5040: Training Loss: 0.1966679791609446 Validation Loss: 0.7138514518737793\n",
      "Epoch 5041: Training Loss: 0.19642317791779837 Validation Loss: 0.7139714360237122\n",
      "Epoch 5042: Training Loss: 0.19642565151055655 Validation Loss: 0.7140933871269226\n",
      "Epoch 5043: Training Loss: 0.19648893674214682 Validation Loss: 0.7138782739639282\n",
      "Epoch 5044: Training Loss: 0.1966327726840973 Validation Loss: 0.7134021520614624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5045: Training Loss: 0.19643024106820425 Validation Loss: 0.7132367491722107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5046: Training Loss: 0.19600005944569907 Validation Loss: 0.7131167650222778\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5047: Training Loss: 0.19726581871509552 Validation Loss: 0.713215708732605\n",
      "Epoch 5048: Training Loss: 0.1961982101202011 Validation Loss: 0.7134014368057251\n",
      "Epoch 5049: Training Loss: 0.19587895274162292 Validation Loss: 0.7132036685943604\n",
      "Epoch 5050: Training Loss: 0.19619618356227875 Validation Loss: 0.7133175134658813\n",
      "Epoch 5051: Training Loss: 0.19576042890548706 Validation Loss: 0.7134284973144531\n",
      "Epoch 5052: Training Loss: 0.19642425576845804 Validation Loss: 0.7133463621139526\n",
      "Epoch 5053: Training Loss: 0.1967170536518097 Validation Loss: 0.7134034633636475\n",
      "Epoch 5054: Training Loss: 0.19615693390369415 Validation Loss: 0.7134841680526733\n",
      "Epoch 5055: Training Loss: 0.19588475922743478 Validation Loss: 0.7136234045028687\n",
      "Epoch 5056: Training Loss: 0.19597738981246948 Validation Loss: 0.7136638760566711\n",
      "Epoch 5057: Training Loss: 0.19586756825447083 Validation Loss: 0.7135846614837646\n",
      "Epoch 5058: Training Loss: 0.19579453766345978 Validation Loss: 0.7135212421417236\n",
      "Epoch 5059: Training Loss: 0.1957883983850479 Validation Loss: 0.7135822176933289\n",
      "Epoch 5060: Training Loss: 0.19592356185118356 Validation Loss: 0.7135620713233948\n",
      "Epoch 5061: Training Loss: 0.19640431801478067 Validation Loss: 0.7136308550834656\n",
      "Epoch 5062: Training Loss: 0.19548233350118002 Validation Loss: 0.7137067317962646\n",
      "Epoch 5063: Training Loss: 0.19552058974901834 Validation Loss: 0.7138893008232117\n",
      "Epoch 5064: Training Loss: 0.19522015750408173 Validation Loss: 0.7138816714286804\n",
      "Epoch 5065: Training Loss: 0.19559087852636972 Validation Loss: 0.7136130928993225\n",
      "Epoch 5066: Training Loss: 0.1950456698735555 Validation Loss: 0.7132999300956726\n",
      "Epoch 5067: Training Loss: 0.1961657851934433 Validation Loss: 0.7131384611129761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5068: Training Loss: 0.19581210116545358 Validation Loss: 0.7133011221885681\n",
      "Epoch 5069: Training Loss: 0.19469058513641357 Validation Loss: 0.7135578393936157\n",
      "Epoch 5070: Training Loss: 0.19557970265547434 Validation Loss: 0.713284432888031\n",
      "Epoch 5071: Training Loss: 0.19574465850989023 Validation Loss: 0.7134292721748352\n",
      "Epoch 5072: Training Loss: 0.19571478168169656 Validation Loss: 0.7136524319648743\n",
      "Epoch 5073: Training Loss: 0.19536180794239044 Validation Loss: 0.713417649269104\n",
      "Epoch 5074: Training Loss: 0.1948469877243042 Validation Loss: 0.7135211229324341\n",
      "Epoch 5075: Training Loss: 0.1950685679912567 Validation Loss: 0.7135372757911682\n",
      "Epoch 5076: Training Loss: 0.1959920475880305 Validation Loss: 0.7133159637451172\n",
      "Epoch 5077: Training Loss: 0.19494891166687012 Validation Loss: 0.7133457064628601\n",
      "Epoch 5078: Training Loss: 0.19496108094851175 Validation Loss: 0.7135679721832275\n",
      "Epoch 5079: Training Loss: 0.19467722872893015 Validation Loss: 0.714034914970398\n",
      "Epoch 5080: Training Loss: 0.19464945296446481 Validation Loss: 0.7138966917991638\n",
      "Epoch 5081: Training Loss: 0.1953907310962677 Validation Loss: 0.7137268781661987\n",
      "Epoch 5082: Training Loss: 0.1951799194018046 Validation Loss: 0.7134438753128052\n",
      "Epoch 5083: Training Loss: 0.19578356544176737 Validation Loss: 0.7138415575027466\n",
      "Epoch 5084: Training Loss: 0.19532548387845358 Validation Loss: 0.7138410806655884\n",
      "Epoch 5085: Training Loss: 0.1953421284755071 Validation Loss: 0.7135849595069885\n",
      "Epoch 5086: Training Loss: 0.19468347231547037 Validation Loss: 0.7134867906570435\n",
      "Epoch 5087: Training Loss: 0.19483586152394614 Validation Loss: 0.7132783532142639\n",
      "Epoch 5088: Training Loss: 0.19437070190906525 Validation Loss: 0.7132428288459778\n",
      "Epoch 5089: Training Loss: 0.19445339838663736 Validation Loss: 0.7133491039276123\n",
      "Epoch 5090: Training Loss: 0.1938980221748352 Validation Loss: 0.7133070230484009\n",
      "Epoch 5091: Training Loss: 0.19431485732396445 Validation Loss: 0.7131313681602478\n",
      "Epoch 5092: Training Loss: 0.19527802368005118 Validation Loss: 0.7127857804298401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5093: Training Loss: 0.19433253010114035 Validation Loss: 0.7131229043006897\n",
      "Epoch 5094: Training Loss: 0.1947737584511439 Validation Loss: 0.7131766080856323\n",
      "Epoch 5095: Training Loss: 0.1940548668305079 Validation Loss: 0.7134234309196472\n",
      "Epoch 5096: Training Loss: 0.1941435138384501 Validation Loss: 0.7135732769966125\n",
      "Epoch 5097: Training Loss: 0.1946140925089518 Validation Loss: 0.7136864066123962\n",
      "Epoch 5098: Training Loss: 0.1941143274307251 Validation Loss: 0.7133106589317322\n",
      "Epoch 5099: Training Loss: 0.19390866657098135 Validation Loss: 0.7133547067642212\n",
      "Epoch 5100: Training Loss: 0.19409018754959106 Validation Loss: 0.7133064270019531\n",
      "Epoch 5101: Training Loss: 0.19354480504989624 Validation Loss: 0.7134731411933899\n",
      "Epoch 5102: Training Loss: 0.19392029444376627 Validation Loss: 0.7137036919593811\n",
      "Epoch 5103: Training Loss: 0.19373520215352377 Validation Loss: 0.7135627269744873\n",
      "Epoch 5104: Training Loss: 0.19360644618670145 Validation Loss: 0.7135680913925171\n",
      "Epoch 5105: Training Loss: 0.1936373511950175 Validation Loss: 0.7137050032615662\n",
      "Epoch 5106: Training Loss: 0.1939283808072408 Validation Loss: 0.7134090065956116\n",
      "Epoch 5107: Training Loss: 0.1942912886540095 Validation Loss: 0.7132762670516968\n",
      "Epoch 5108: Training Loss: 0.1948414444923401 Validation Loss: 0.7133675813674927\n",
      "Epoch 5109: Training Loss: 0.19379565119743347 Validation Loss: 0.7131957411766052\n",
      "Epoch 5110: Training Loss: 0.19343133767445883 Validation Loss: 0.7132829427719116\n",
      "Epoch 5111: Training Loss: 0.19345778226852417 Validation Loss: 0.7132801413536072\n",
      "Epoch 5112: Training Loss: 0.19412421683470407 Validation Loss: 0.7131883502006531\n",
      "Epoch 5113: Training Loss: 0.19333600997924805 Validation Loss: 0.7132521867752075\n",
      "Epoch 5114: Training Loss: 0.19369452695051828 Validation Loss: 0.7131862640380859\n",
      "Epoch 5115: Training Loss: 0.19335565467675528 Validation Loss: 0.7131291031837463\n",
      "Epoch 5116: Training Loss: 0.1945707897345225 Validation Loss: 0.7132242321968079\n",
      "Epoch 5117: Training Loss: 0.19310500224431357 Validation Loss: 0.7131543159484863\n",
      "Epoch 5118: Training Loss: 0.1932162642478943 Validation Loss: 0.7131944894790649\n",
      "Epoch 5119: Training Loss: 0.19310169915358225 Validation Loss: 0.7131820917129517\n",
      "Epoch 5120: Training Loss: 0.1930738091468811 Validation Loss: 0.7130998969078064\n",
      "Epoch 5121: Training Loss: 0.19296806553999582 Validation Loss: 0.7128883004188538\n",
      "Epoch 5122: Training Loss: 0.19290372232596079 Validation Loss: 0.7130587697029114\n",
      "Epoch 5123: Training Loss: 0.19312130411465964 Validation Loss: 0.7131121158599854\n",
      "Epoch 5124: Training Loss: 0.19373230139414468 Validation Loss: 0.7131273746490479\n",
      "Epoch 5125: Training Loss: 0.1930164247751236 Validation Loss: 0.713095486164093\n",
      "Epoch 5126: Training Loss: 0.19270948072274527 Validation Loss: 0.7132617235183716\n",
      "Epoch 5127: Training Loss: 0.1929365595181783 Validation Loss: 0.7132838368415833\n",
      "Epoch 5128: Training Loss: 0.19272345304489136 Validation Loss: 0.7131728529930115\n",
      "Epoch 5129: Training Loss: 0.19263731439908346 Validation Loss: 0.7130230665206909\n",
      "Epoch 5130: Training Loss: 0.19259339570999146 Validation Loss: 0.7128408551216125\n",
      "Epoch 5131: Training Loss: 0.1925795873006185 Validation Loss: 0.7129528522491455\n",
      "Epoch 5132: Training Loss: 0.19298498829205832 Validation Loss: 0.7132683396339417\n",
      "Epoch 5133: Training Loss: 0.19248593846956888 Validation Loss: 0.7130306363105774\n",
      "Epoch 5134: Training Loss: 0.1920505811770757 Validation Loss: 0.7130457758903503\n",
      "Epoch 5135: Training Loss: 0.19256681700547537 Validation Loss: 0.713451623916626\n",
      "Epoch 5136: Training Loss: 0.19222100575764975 Validation Loss: 0.713311493396759\n",
      "Epoch 5137: Training Loss: 0.19273337721824646 Validation Loss: 0.7131814360618591\n",
      "Epoch 5138: Training Loss: 0.19263042509555817 Validation Loss: 0.7131902575492859\n",
      "Epoch 5139: Training Loss: 0.19195950527985892 Validation Loss: 0.7131074070930481\n",
      "Epoch 5140: Training Loss: 0.19211352864901224 Validation Loss: 0.7130224108695984\n",
      "Epoch 5141: Training Loss: 0.1922341138124466 Validation Loss: 0.7130274176597595\n",
      "Epoch 5142: Training Loss: 0.19235050678253174 Validation Loss: 0.7128477692604065\n",
      "Epoch 5143: Training Loss: 0.1922072966893514 Validation Loss: 0.7126883268356323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5144: Training Loss: 0.19218477110068002 Validation Loss: 0.7127594351768494\n",
      "Epoch 5145: Training Loss: 0.19240749378999075 Validation Loss: 0.7129096388816833\n",
      "Epoch 5146: Training Loss: 0.19191206494967142 Validation Loss: 0.7131202220916748\n",
      "Epoch 5147: Training Loss: 0.19192506869633993 Validation Loss: 0.7129604816436768\n",
      "Epoch 5148: Training Loss: 0.19191884994506836 Validation Loss: 0.7131648063659668\n",
      "Epoch 5149: Training Loss: 0.1930376241604487 Validation Loss: 0.7131497859954834\n",
      "Epoch 5150: Training Loss: 0.19249760111172995 Validation Loss: 0.712661862373352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5151: Training Loss: 0.19255250692367554 Validation Loss: 0.7125394940376282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5152: Training Loss: 0.19231208662192026 Validation Loss: 0.7126113176345825\n",
      "Epoch 5153: Training Loss: 0.19185187419255575 Validation Loss: 0.7130044102668762\n",
      "Epoch 5154: Training Loss: 0.1918651262919108 Validation Loss: 0.7135906219482422\n",
      "Epoch 5155: Training Loss: 0.1913846880197525 Validation Loss: 0.7137383222579956\n",
      "Epoch 5156: Training Loss: 0.19363553325335184 Validation Loss: 0.7136011123657227\n",
      "Epoch 5157: Training Loss: 0.19139559070269266 Validation Loss: 0.7134606838226318\n",
      "Epoch 5158: Training Loss: 0.1915126840273539 Validation Loss: 0.713269829750061\n",
      "Epoch 5159: Training Loss: 0.19151902695496878 Validation Loss: 0.7133194208145142\n",
      "Epoch 5160: Training Loss: 0.1918367544809977 Validation Loss: 0.7131872773170471\n",
      "Epoch 5161: Training Loss: 0.19167290131251016 Validation Loss: 0.712985634803772\n",
      "Epoch 5162: Training Loss: 0.19148766497770944 Validation Loss: 0.7128746509552002\n",
      "Epoch 5163: Training Loss: 0.19121586283047995 Validation Loss: 0.7128776907920837\n",
      "Epoch 5164: Training Loss: 0.19117291271686554 Validation Loss: 0.7129130959510803\n",
      "Epoch 5165: Training Loss: 0.19119732081890106 Validation Loss: 0.7130167484283447\n",
      "Epoch 5166: Training Loss: 0.19125518202781677 Validation Loss: 0.7128324508666992\n",
      "Epoch 5167: Training Loss: 0.1908919761578242 Validation Loss: 0.7126390337944031\n",
      "Epoch 5168: Training Loss: 0.19131923218568167 Validation Loss: 0.712600827217102\n",
      "Epoch 5169: Training Loss: 0.19102737804253897 Validation Loss: 0.7127222418785095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5170: Training Loss: 0.19077843924363455 Validation Loss: 0.7127661108970642\n",
      "Epoch 5171: Training Loss: 0.19175698856512705 Validation Loss: 0.7131303548812866\n",
      "Epoch 5172: Training Loss: 0.1909336050351461 Validation Loss: 0.7130098938941956\n",
      "Epoch 5173: Training Loss: 0.1909667799870173 Validation Loss: 0.7126339077949524\n",
      "Epoch 5174: Training Loss: 0.19092264274756113 Validation Loss: 0.7128089070320129\n",
      "Epoch 5175: Training Loss: 0.19088859359423319 Validation Loss: 0.7128102779388428\n",
      "Epoch 5176: Training Loss: 0.19176354507605234 Validation Loss: 0.7131019830703735\n",
      "Epoch 5177: Training Loss: 0.19037993252277374 Validation Loss: 0.7129859328269958\n",
      "Epoch 5178: Training Loss: 0.19063710669676462 Validation Loss: 0.7129623889923096\n",
      "Epoch 5179: Training Loss: 0.19071395695209503 Validation Loss: 0.712801992893219\n",
      "Epoch 5180: Training Loss: 0.19032732645670572 Validation Loss: 0.7128161787986755\n",
      "Epoch 5181: Training Loss: 0.1908819576104482 Validation Loss: 0.7130893468856812\n",
      "Epoch 5182: Training Loss: 0.1905063440402349 Validation Loss: 0.7128116488456726\n",
      "Epoch 5183: Training Loss: 0.190967986981074 Validation Loss: 0.7127540111541748\n",
      "Epoch 5184: Training Loss: 0.19047888120015463 Validation Loss: 0.712794303894043\n",
      "Epoch 5185: Training Loss: 0.1905437856912613 Validation Loss: 0.7130045890808105\n",
      "Epoch 5186: Training Loss: 0.1906409611304601 Validation Loss: 0.7130987048149109\n",
      "Epoch 5187: Training Loss: 0.19027878840764365 Validation Loss: 0.7130701541900635\n",
      "Epoch 5188: Training Loss: 0.19004018604755402 Validation Loss: 0.712967038154602\n",
      "Epoch 5189: Training Loss: 0.19036699831485748 Validation Loss: 0.7127594947814941\n",
      "Epoch 5190: Training Loss: 0.1903745780388514 Validation Loss: 0.712872326374054\n",
      "Epoch 5191: Training Loss: 0.19163579245408377 Validation Loss: 0.7125628590583801\n",
      "Epoch 5192: Training Loss: 0.19030408561229706 Validation Loss: 0.7126467823982239\n",
      "Epoch 5193: Training Loss: 0.19060009717941284 Validation Loss: 0.7128917574882507\n",
      "Epoch 5194: Training Loss: 0.19010422130425772 Validation Loss: 0.7127848267555237\n",
      "Epoch 5195: Training Loss: 0.18989881873130798 Validation Loss: 0.7130836248397827\n",
      "Epoch 5196: Training Loss: 0.18994317452112833 Validation Loss: 0.7131504416465759\n",
      "Epoch 5197: Training Loss: 0.19026090701421103 Validation Loss: 0.7127173542976379\n",
      "Epoch 5198: Training Loss: 0.18976553777853647 Validation Loss: 0.7126200199127197\n",
      "Epoch 5199: Training Loss: 0.18951295812924704 Validation Loss: 0.7130240201950073\n",
      "Epoch 5200: Training Loss: 0.19026874005794525 Validation Loss: 0.7132592797279358\n",
      "Epoch 5201: Training Loss: 0.18996464212735495 Validation Loss: 0.7134045362472534\n",
      "Epoch 5202: Training Loss: 0.18956435720125833 Validation Loss: 0.7130084037780762\n",
      "Epoch 5203: Training Loss: 0.1901263395945231 Validation Loss: 0.7128934860229492\n",
      "Epoch 5204: Training Loss: 0.18971662720044455 Validation Loss: 0.7128033638000488\n",
      "Epoch 5205: Training Loss: 0.1920583744843801 Validation Loss: 0.7125693559646606\n",
      "Epoch 5206: Training Loss: 0.1898158391316732 Validation Loss: 0.712419867515564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5207: Training Loss: 0.18918125331401825 Validation Loss: 0.712595522403717\n",
      "Epoch 5208: Training Loss: 0.18948974708716074 Validation Loss: 0.7126257419586182\n",
      "Epoch 5209: Training Loss: 0.18989761670430502 Validation Loss: 0.7128708362579346\n",
      "Epoch 5210: Training Loss: 0.18939265608787537 Validation Loss: 0.7127622961997986\n",
      "Epoch 5211: Training Loss: 0.1895214468240738 Validation Loss: 0.7131843566894531\n",
      "Epoch 5212: Training Loss: 0.1895343859990438 Validation Loss: 0.7129630446434021\n",
      "Epoch 5213: Training Loss: 0.18984735508759817 Validation Loss: 0.7130104899406433\n",
      "Epoch 5214: Training Loss: 0.1893368512392044 Validation Loss: 0.7126206159591675\n",
      "Epoch 5215: Training Loss: 0.18917875985304514 Validation Loss: 0.712509036064148\n",
      "Epoch 5216: Training Loss: 0.18931558231512705 Validation Loss: 0.7124364972114563\n",
      "Epoch 5217: Training Loss: 0.19001485407352448 Validation Loss: 0.7125428915023804\n",
      "Epoch 5218: Training Loss: 0.18937086562315622 Validation Loss: 0.7125792503356934\n",
      "Epoch 5219: Training Loss: 0.18912013868490854 Validation Loss: 0.7125884294509888\n",
      "Epoch 5220: Training Loss: 0.1893881012996038 Validation Loss: 0.7127071022987366\n",
      "Epoch 5221: Training Loss: 0.18896357218424478 Validation Loss: 0.7128497362136841\n",
      "Epoch 5222: Training Loss: 0.18902239700158438 Validation Loss: 0.7128360867500305\n",
      "Epoch 5223: Training Loss: 0.1889339635769526 Validation Loss: 0.7126396894454956\n",
      "Epoch 5224: Training Loss: 0.1881986310084661 Validation Loss: 0.7125191688537598\n",
      "Epoch 5225: Training Loss: 0.18888482948144278 Validation Loss: 0.7121888399124146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5226: Training Loss: 0.1885573317607244 Validation Loss: 0.7122859954833984\n",
      "Epoch 5227: Training Loss: 0.1886208156744639 Validation Loss: 0.7121741771697998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5228: Training Loss: 0.18865326543649039 Validation Loss: 0.712473452091217\n",
      "Epoch 5229: Training Loss: 0.18835369745890299 Validation Loss: 0.7129612565040588\n",
      "Epoch 5230: Training Loss: 0.18846628566582999 Validation Loss: 0.7130036950111389\n",
      "Epoch 5231: Training Loss: 0.18883900344371796 Validation Loss: 0.7127704620361328\n",
      "Epoch 5232: Training Loss: 0.18863732119401297 Validation Loss: 0.7125425934791565\n",
      "Epoch 5233: Training Loss: 0.18969219426314035 Validation Loss: 0.712400496006012\n",
      "Epoch 5234: Training Loss: 0.18864228824774423 Validation Loss: 0.7127164006233215\n",
      "Epoch 5235: Training Loss: 0.18852351605892181 Validation Loss: 0.7128676176071167\n",
      "Epoch 5236: Training Loss: 0.18890860180060068 Validation Loss: 0.7127170562744141\n",
      "Epoch 5237: Training Loss: 0.1884849617878596 Validation Loss: 0.7125455737113953\n",
      "Epoch 5238: Training Loss: 0.18854395051797232 Validation Loss: 0.7123633027076721\n",
      "Epoch 5239: Training Loss: 0.18799730141957602 Validation Loss: 0.7125376462936401\n",
      "Epoch 5240: Training Loss: 0.18849746882915497 Validation Loss: 0.7126452326774597\n",
      "Epoch 5241: Training Loss: 0.18962055444717407 Validation Loss: 0.7128176689147949\n",
      "Epoch 5242: Training Loss: 0.18835156162579855 Validation Loss: 0.7124971747398376\n",
      "Epoch 5243: Training Loss: 0.18816105524698892 Validation Loss: 0.7122913599014282\n",
      "Epoch 5244: Training Loss: 0.1883233884970347 Validation Loss: 0.712247908115387\n",
      "Epoch 5245: Training Loss: 0.18834348022937775 Validation Loss: 0.7123833298683167\n",
      "Epoch 5246: Training Loss: 0.188713272412618 Validation Loss: 0.712744951248169\n",
      "Epoch 5247: Training Loss: 0.1880408674478531 Validation Loss: 0.7130651473999023\n",
      "Epoch 5248: Training Loss: 0.18807602425416312 Validation Loss: 0.7127910852432251\n",
      "Epoch 5249: Training Loss: 0.18826689819494882 Validation Loss: 0.7125152349472046\n",
      "Epoch 5250: Training Loss: 0.18814112742741904 Validation Loss: 0.7122747302055359\n",
      "Epoch 5251: Training Loss: 0.18817144632339478 Validation Loss: 0.7123803496360779\n",
      "Epoch 5252: Training Loss: 0.18811970949172974 Validation Loss: 0.7129272818565369\n",
      "Epoch 5253: Training Loss: 0.18770043551921844 Validation Loss: 0.7129693031311035\n",
      "Epoch 5254: Training Loss: 0.18736052513122559 Validation Loss: 0.7126429677009583\n",
      "Epoch 5255: Training Loss: 0.1875013361374537 Validation Loss: 0.7125228643417358\n",
      "Epoch 5256: Training Loss: 0.18774652481079102 Validation Loss: 0.7125768661499023\n",
      "Epoch 5257: Training Loss: 0.1885639876127243 Validation Loss: 0.7125085592269897\n",
      "Epoch 5258: Training Loss: 0.187445600827535 Validation Loss: 0.7125341892242432\n",
      "Epoch 5259: Training Loss: 0.18786205351352692 Validation Loss: 0.712353527545929\n",
      "Epoch 5260: Training Loss: 0.18710768222808838 Validation Loss: 0.7126095294952393\n",
      "Epoch 5261: Training Loss: 0.18772743145624796 Validation Loss: 0.7127923369407654\n",
      "Epoch 5262: Training Loss: 0.18763812879721323 Validation Loss: 0.7125853300094604\n",
      "Epoch 5263: Training Loss: 0.1872879515091578 Validation Loss: 0.7128694653511047\n",
      "Epoch 5264: Training Loss: 0.18768303592999777 Validation Loss: 0.7127971649169922\n",
      "Epoch 5265: Training Loss: 0.1873138944307963 Validation Loss: 0.7124250531196594\n",
      "Epoch 5266: Training Loss: 0.187168151140213 Validation Loss: 0.7122651934623718\n",
      "Epoch 5267: Training Loss: 0.18700852990150452 Validation Loss: 0.7122570872306824\n",
      "Epoch 5268: Training Loss: 0.18724956611792246 Validation Loss: 0.7123374342918396\n",
      "Epoch 5269: Training Loss: 0.1887697031100591 Validation Loss: 0.7124853730201721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5270: Training Loss: 0.1879958212375641 Validation Loss: 0.7126953601837158\n",
      "Epoch 5271: Training Loss: 0.1876278817653656 Validation Loss: 0.712672770023346\n",
      "Epoch 5272: Training Loss: 0.1869292159875234 Validation Loss: 0.7126724720001221\n",
      "Epoch 5273: Training Loss: 0.18661008775234222 Validation Loss: 0.7124870419502258\n",
      "Epoch 5274: Training Loss: 0.1874450296163559 Validation Loss: 0.7123146653175354\n",
      "Epoch 5275: Training Loss: 0.18754475315411887 Validation Loss: 0.712683916091919\n",
      "Epoch 5276: Training Loss: 0.18663418789704642 Validation Loss: 0.7127075791358948\n",
      "Epoch 5277: Training Loss: 0.1869978259007136 Validation Loss: 0.7126924991607666\n",
      "Epoch 5278: Training Loss: 0.18802312513192496 Validation Loss: 0.7124040126800537\n",
      "Epoch 5279: Training Loss: 0.18698481221993765 Validation Loss: 0.7127195000648499\n",
      "Epoch 5280: Training Loss: 0.1865936517715454 Validation Loss: 0.7125679850578308\n",
      "Epoch 5281: Training Loss: 0.1875007003545761 Validation Loss: 0.7126531004905701\n",
      "Epoch 5282: Training Loss: 0.1862220565478007 Validation Loss: 0.7126649618148804\n",
      "Epoch 5283: Training Loss: 0.1863100230693817 Validation Loss: 0.7124943733215332\n",
      "Epoch 5284: Training Loss: 0.18648207684357962 Validation Loss: 0.7123777866363525\n",
      "Epoch 5285: Training Loss: 0.18645707766215006 Validation Loss: 0.7122543454170227\n",
      "Epoch 5286: Training Loss: 0.1868378221988678 Validation Loss: 0.7121198177337646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5287: Training Loss: 0.18710977335770926 Validation Loss: 0.7123039960861206\n",
      "Epoch 5288: Training Loss: 0.1860956350962321 Validation Loss: 0.7126887440681458\n",
      "Epoch 5289: Training Loss: 0.18631062904993692 Validation Loss: 0.7123008370399475\n",
      "Epoch 5290: Training Loss: 0.18612173696359 Validation Loss: 0.7122282385826111\n",
      "Epoch 5291: Training Loss: 0.18632775048414865 Validation Loss: 0.7124089002609253\n",
      "Epoch 5292: Training Loss: 0.1869152138630549 Validation Loss: 0.7123474478721619\n",
      "Epoch 5293: Training Loss: 0.18627043068408966 Validation Loss: 0.7121770977973938\n",
      "Epoch 5294: Training Loss: 0.18634059031804404 Validation Loss: 0.7121263742446899\n",
      "Epoch 5295: Training Loss: 0.18621604144573212 Validation Loss: 0.7122839093208313\n",
      "Epoch 5296: Training Loss: 0.1867530345916748 Validation Loss: 0.7124181389808655\n",
      "Epoch 5297: Training Loss: 0.18592452506224313 Validation Loss: 0.712705135345459\n",
      "Epoch 5298: Training Loss: 0.18627575039863586 Validation Loss: 0.7126055955886841\n",
      "Epoch 5299: Training Loss: 0.18629617989063263 Validation Loss: 0.712636411190033\n",
      "Epoch 5300: Training Loss: 0.18586649497350058 Validation Loss: 0.712376058101654\n",
      "Epoch 5301: Training Loss: 0.1857573688030243 Validation Loss: 0.712266206741333\n",
      "Epoch 5302: Training Loss: 0.1858566552400589 Validation Loss: 0.7123739719390869\n",
      "Epoch 5303: Training Loss: 0.18593734999497732 Validation Loss: 0.7122035622596741\n",
      "Epoch 5304: Training Loss: 0.1858845849831899 Validation Loss: 0.7122666835784912\n",
      "Epoch 5305: Training Loss: 0.1858680695295334 Validation Loss: 0.7123637199401855\n",
      "Epoch 5306: Training Loss: 0.18508559465408325 Validation Loss: 0.7123108506202698\n",
      "Epoch 5307: Training Loss: 0.18554342786471048 Validation Loss: 0.7126981019973755\n",
      "Epoch 5308: Training Loss: 0.18598850071430206 Validation Loss: 0.712614893913269\n",
      "Epoch 5309: Training Loss: 0.18526448806126913 Validation Loss: 0.7123807668685913\n",
      "Epoch 5310: Training Loss: 0.185431698958079 Validation Loss: 0.7122657299041748\n",
      "Epoch 5311: Training Loss: 0.1854102611541748 Validation Loss: 0.712245523929596\n",
      "Epoch 5312: Training Loss: 0.18654105563958487 Validation Loss: 0.7121941447257996\n",
      "Epoch 5313: Training Loss: 0.1870023012161255 Validation Loss: 0.7122706770896912\n",
      "Epoch 5314: Training Loss: 0.1851817468802134 Validation Loss: 0.7125892043113708\n",
      "Epoch 5315: Training Loss: 0.18506601949532828 Validation Loss: 0.7126008868217468\n",
      "Epoch 5316: Training Loss: 0.18741014103094736 Validation Loss: 0.7122830152511597\n",
      "Epoch 5317: Training Loss: 0.18521789213021597 Validation Loss: 0.7120381593704224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5318: Training Loss: 0.18522306780020395 Validation Loss: 0.7119237780570984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5319: Training Loss: 0.185477152466774 Validation Loss: 0.7120480537414551\n",
      "Epoch 5320: Training Loss: 0.18504886825879416 Validation Loss: 0.7123988270759583\n",
      "Epoch 5321: Training Loss: 0.18499547243118286 Validation Loss: 0.7126785516738892\n",
      "Epoch 5322: Training Loss: 0.18497089048226675 Validation Loss: 0.7127730250358582\n",
      "Epoch 5323: Training Loss: 0.18504661321640015 Validation Loss: 0.7125130295753479\n",
      "Epoch 5324: Training Loss: 0.18486965199311575 Validation Loss: 0.712336540222168\n",
      "Epoch 5325: Training Loss: 0.18403425812721252 Validation Loss: 0.7121384739875793\n",
      "Epoch 5326: Training Loss: 0.1846890797217687 Validation Loss: 0.7122103571891785\n",
      "Epoch 5327: Training Loss: 0.18529152870178223 Validation Loss: 0.7127424478530884\n",
      "Epoch 5328: Training Loss: 0.18474842111269632 Validation Loss: 0.7129080891609192\n",
      "Epoch 5329: Training Loss: 0.1848678489526113 Validation Loss: 0.7127434611320496\n",
      "Epoch 5330: Training Loss: 0.18493878344694772 Validation Loss: 0.7130144834518433\n",
      "Epoch 5331: Training Loss: 0.18379742900530496 Validation Loss: 0.7127840518951416\n",
      "Epoch 5332: Training Loss: 0.18437667191028595 Validation Loss: 0.7126273512840271\n",
      "Epoch 5333: Training Loss: 0.18540247281392416 Validation Loss: 0.7124066352844238\n",
      "Epoch 5334: Training Loss: 0.1850948085387548 Validation Loss: 0.7125319838523865\n",
      "Epoch 5335: Training Loss: 0.18421452244122824 Validation Loss: 0.7127658128738403\n",
      "Epoch 5336: Training Loss: 0.18578265607357025 Validation Loss: 0.7122994661331177\n",
      "Epoch 5337: Training Loss: 0.18503112097581229 Validation Loss: 0.7120882272720337\n",
      "Epoch 5338: Training Loss: 0.1843639463186264 Validation Loss: 0.7119219303131104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5339: Training Loss: 0.18435425559679666 Validation Loss: 0.7118933200836182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5340: Training Loss: 0.18488346536954245 Validation Loss: 0.712029755115509\n",
      "Epoch 5341: Training Loss: 0.18478664755821228 Validation Loss: 0.7120992541313171\n",
      "Epoch 5342: Training Loss: 0.18461021780967712 Validation Loss: 0.7124106884002686\n",
      "Epoch 5343: Training Loss: 0.18409939110279083 Validation Loss: 0.7126181721687317\n",
      "Epoch 5344: Training Loss: 0.18489483495553335 Validation Loss: 0.7128234505653381\n",
      "Epoch 5345: Training Loss: 0.1842091033856074 Validation Loss: 0.7128741145133972\n",
      "Epoch 5346: Training Loss: 0.18418537576993307 Validation Loss: 0.712471067905426\n",
      "Epoch 5347: Training Loss: 0.18565168480078378 Validation Loss: 0.7126527428627014\n",
      "Epoch 5348: Training Loss: 0.18412192165851593 Validation Loss: 0.7123532295227051\n",
      "Epoch 5349: Training Loss: 0.18395612140496573 Validation Loss: 0.7125260829925537\n",
      "Epoch 5350: Training Loss: 0.18414429326852164 Validation Loss: 0.7126510739326477\n",
      "Epoch 5351: Training Loss: 0.18503854175408682 Validation Loss: 0.7125048041343689\n",
      "Epoch 5352: Training Loss: 0.1850960204998652 Validation Loss: 0.7124271392822266\n",
      "Epoch 5353: Training Loss: 0.18388599654038748 Validation Loss: 0.71254962682724\n",
      "Epoch 5354: Training Loss: 0.18459692100683847 Validation Loss: 0.7128404378890991\n",
      "Epoch 5355: Training Loss: 0.1838158369064331 Validation Loss: 0.7129673361778259\n",
      "Epoch 5356: Training Loss: 0.18421639502048492 Validation Loss: 0.7125560641288757\n",
      "Epoch 5357: Training Loss: 0.18487275143464407 Validation Loss: 0.7121886610984802\n",
      "Epoch 5358: Training Loss: 0.18407506744066873 Validation Loss: 0.712026834487915\n",
      "Epoch 5359: Training Loss: 0.1836650719245275 Validation Loss: 0.7121918797492981\n",
      "Epoch 5360: Training Loss: 0.18455956876277924 Validation Loss: 0.7125405669212341\n",
      "Epoch 5361: Training Loss: 0.18359694381554922 Validation Loss: 0.7124994397163391\n",
      "Epoch 5362: Training Loss: 0.18366934855779013 Validation Loss: 0.7126058340072632\n",
      "Epoch 5363: Training Loss: 0.18367277085781097 Validation Loss: 0.7121440172195435\n",
      "Epoch 5364: Training Loss: 0.18325198690096536 Validation Loss: 0.7121791839599609\n",
      "Epoch 5365: Training Loss: 0.18337552746136984 Validation Loss: 0.7119707465171814\n",
      "Epoch 5366: Training Loss: 0.18383689721425375 Validation Loss: 0.7119768261909485\n",
      "Epoch 5367: Training Loss: 0.18313529590765634 Validation Loss: 0.7122148275375366\n",
      "Epoch 5368: Training Loss: 0.1831553429365158 Validation Loss: 0.7124311327934265\n",
      "Epoch 5369: Training Loss: 0.18306017418702444 Validation Loss: 0.7121633291244507\n",
      "Epoch 5370: Training Loss: 0.1835721880197525 Validation Loss: 0.7121634483337402\n",
      "Epoch 5371: Training Loss: 0.18342716991901398 Validation Loss: 0.7122281789779663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5372: Training Loss: 0.18299764891465506 Validation Loss: 0.7120416164398193\n",
      "Epoch 5373: Training Loss: 0.18313672145207724 Validation Loss: 0.7121736407279968\n",
      "Epoch 5374: Training Loss: 0.18367544809977213 Validation Loss: 0.7121400833129883\n",
      "Epoch 5375: Training Loss: 0.1838991791009903 Validation Loss: 0.712204098701477\n",
      "Epoch 5376: Training Loss: 0.1830898920694987 Validation Loss: 0.7120983004570007\n",
      "Epoch 5377: Training Loss: 0.18307362496852875 Validation Loss: 0.7121095657348633\n",
      "Epoch 5378: Training Loss: 0.1831772724787394 Validation Loss: 0.7123938202857971\n",
      "Epoch 5379: Training Loss: 0.1841026246547699 Validation Loss: 0.7124847769737244\n",
      "Epoch 5380: Training Loss: 0.1828464468320211 Validation Loss: 0.7125831842422485\n",
      "Epoch 5381: Training Loss: 0.1833911587794622 Validation Loss: 0.7123838067054749\n",
      "Epoch 5382: Training Loss: 0.18294736742973328 Validation Loss: 0.7122873067855835\n",
      "Epoch 5383: Training Loss: 0.18247444927692413 Validation Loss: 0.7124457359313965\n",
      "Epoch 5384: Training Loss: 0.18266993264357248 Validation Loss: 0.7124073505401611\n",
      "Epoch 5385: Training Loss: 0.182403102517128 Validation Loss: 0.7124612927436829\n",
      "Epoch 5386: Training Loss: 0.18306629856427512 Validation Loss: 0.7125075459480286\n",
      "Epoch 5387: Training Loss: 0.1832525779803594 Validation Loss: 0.7123411297798157\n",
      "Epoch 5388: Training Loss: 0.18209781249364218 Validation Loss: 0.71211838722229\n",
      "Epoch 5389: Training Loss: 0.18298972149689993 Validation Loss: 0.7122604250907898\n",
      "Epoch 5390: Training Loss: 0.18210248152414957 Validation Loss: 0.7126262187957764\n",
      "Epoch 5391: Training Loss: 0.1827502946058909 Validation Loss: 0.7124139666557312\n",
      "Epoch 5392: Training Loss: 0.1824124107758204 Validation Loss: 0.7122029066085815\n",
      "Epoch 5393: Training Loss: 0.18160871167977652 Validation Loss: 0.7122941017150879\n",
      "Epoch 5394: Training Loss: 0.18246095379193625 Validation Loss: 0.7122318148612976\n",
      "Epoch 5395: Training Loss: 0.18223693470160165 Validation Loss: 0.7118600606918335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5396: Training Loss: 0.18312235176563263 Validation Loss: 0.7118971347808838\n",
      "Epoch 5397: Training Loss: 0.18268478413422903 Validation Loss: 0.7117542624473572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5398: Training Loss: 0.18201007942358652 Validation Loss: 0.7120277881622314\n",
      "Epoch 5399: Training Loss: 0.1824276695648829 Validation Loss: 0.7123342752456665\n",
      "Epoch 5400: Training Loss: 0.18302333851655325 Validation Loss: 0.7122191190719604\n",
      "Epoch 5401: Training Loss: 0.18188835680484772 Validation Loss: 0.7122697234153748\n",
      "Epoch 5402: Training Loss: 0.1819220781326294 Validation Loss: 0.7122024297714233\n",
      "Epoch 5403: Training Loss: 0.1825592964887619 Validation Loss: 0.7122724056243896\n",
      "Epoch 5404: Training Loss: 0.1823627750078837 Validation Loss: 0.71230149269104\n",
      "Epoch 5405: Training Loss: 0.18226934472719827 Validation Loss: 0.7123966217041016\n",
      "Epoch 5406: Training Loss: 0.18143736322720846 Validation Loss: 0.7125179767608643\n",
      "Epoch 5407: Training Loss: 0.18236844738324484 Validation Loss: 0.7122386693954468\n",
      "Epoch 5408: Training Loss: 0.18201678494612375 Validation Loss: 0.7120715975761414\n",
      "Epoch 5409: Training Loss: 0.18297405540943146 Validation Loss: 0.7118463516235352\n",
      "Epoch 5410: Training Loss: 0.1819082349538803 Validation Loss: 0.7120034098625183\n",
      "Epoch 5411: Training Loss: 0.1812622845172882 Validation Loss: 0.7117047905921936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5412: Training Loss: 0.1816268116235733 Validation Loss: 0.7119675278663635\n",
      "Epoch 5413: Training Loss: 0.1822366068760554 Validation Loss: 0.7119569182395935\n",
      "Epoch 5414: Training Loss: 0.1818853219350179 Validation Loss: 0.7123700380325317\n",
      "Epoch 5415: Training Loss: 0.1810618738333384 Validation Loss: 0.712457001209259\n",
      "Epoch 5416: Training Loss: 0.1814072479804357 Validation Loss: 0.7122007608413696\n",
      "Epoch 5417: Training Loss: 0.18169510861237845 Validation Loss: 0.7120864987373352\n",
      "Epoch 5418: Training Loss: 0.1817813515663147 Validation Loss: 0.7119426727294922\n",
      "Epoch 5419: Training Loss: 0.18173465132713318 Validation Loss: 0.7119433879852295\n",
      "Epoch 5420: Training Loss: 0.18134066462516785 Validation Loss: 0.711872935295105\n",
      "Epoch 5421: Training Loss: 0.1814015954732895 Validation Loss: 0.712118923664093\n",
      "Epoch 5422: Training Loss: 0.1820703794558843 Validation Loss: 0.712174654006958\n",
      "Epoch 5423: Training Loss: 0.18147062261899313 Validation Loss: 0.7122721076011658\n",
      "Epoch 5424: Training Loss: 0.18153464297453561 Validation Loss: 0.7123122811317444\n",
      "Epoch 5425: Training Loss: 0.18115081389745077 Validation Loss: 0.7122073173522949\n",
      "Epoch 5426: Training Loss: 0.18141132593154907 Validation Loss: 0.712166428565979\n",
      "Epoch 5427: Training Loss: 0.18102059761683145 Validation Loss: 0.7121520042419434\n",
      "Epoch 5428: Training Loss: 0.18125595152378082 Validation Loss: 0.7122273445129395\n",
      "Epoch 5429: Training Loss: 0.18116234242916107 Validation Loss: 0.712093710899353\n",
      "Epoch 5430: Training Loss: 0.18218829731146494 Validation Loss: 0.712251603603363\n",
      "Epoch 5431: Training Loss: 0.1820253680149714 Validation Loss: 0.7120422124862671\n",
      "Epoch 5432: Training Loss: 0.18092048664887747 Validation Loss: 0.711931049823761\n",
      "Epoch 5433: Training Loss: 0.1810875286658605 Validation Loss: 0.7118820548057556\n",
      "Epoch 5434: Training Loss: 0.1808342585961024 Validation Loss: 0.7118350863456726\n",
      "Epoch 5435: Training Loss: 0.1810442457596461 Validation Loss: 0.7118608951568604\n",
      "Epoch 5436: Training Loss: 0.18102300663789114 Validation Loss: 0.7121318578720093\n",
      "Epoch 5437: Training Loss: 0.18082896371682486 Validation Loss: 0.7122148275375366\n",
      "Epoch 5438: Training Loss: 0.18170459071795145 Validation Loss: 0.7124933004379272\n",
      "Epoch 5439: Training Loss: 0.1803184151649475 Validation Loss: 0.7122130393981934\n",
      "Epoch 5440: Training Loss: 0.18119812508424124 Validation Loss: 0.7122994661331177\n",
      "Epoch 5441: Training Loss: 0.18138361473878226 Validation Loss: 0.7122398018836975\n",
      "Epoch 5442: Training Loss: 0.1817092796166738 Validation Loss: 0.7121278643608093\n",
      "Epoch 5443: Training Loss: 0.1805910517772039 Validation Loss: 0.7119565010070801\n",
      "Epoch 5444: Training Loss: 0.18154135843118033 Validation Loss: 0.7122013568878174\n",
      "Epoch 5445: Training Loss: 0.18046735723813376 Validation Loss: 0.7119678854942322\n",
      "Epoch 5446: Training Loss: 0.18035452564557394 Validation Loss: 0.7116709351539612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5447: Training Loss: 0.18002326786518097 Validation Loss: 0.7118741869926453\n",
      "Epoch 5448: Training Loss: 0.18041077752908072 Validation Loss: 0.7120195627212524\n",
      "Epoch 5449: Training Loss: 0.18037120501200357 Validation Loss: 0.7124334573745728\n",
      "Epoch 5450: Training Loss: 0.1804730991522471 Validation Loss: 0.7124964594841003\n",
      "Epoch 5451: Training Loss: 0.18096805612246195 Validation Loss: 0.712320864200592\n",
      "Epoch 5452: Training Loss: 0.18010514974594116 Validation Loss: 0.7121768593788147\n",
      "Epoch 5453: Training Loss: 0.18012963235378265 Validation Loss: 0.7121887803077698\n",
      "Epoch 5454: Training Loss: 0.18027155101299286 Validation Loss: 0.7120104432106018\n",
      "Epoch 5455: Training Loss: 0.1803551266590754 Validation Loss: 0.7119601368904114\n",
      "Epoch 5456: Training Loss: 0.18038460115591684 Validation Loss: 0.7121127843856812\n",
      "Epoch 5457: Training Loss: 0.18029659986495972 Validation Loss: 0.7123081684112549\n",
      "Epoch 5458: Training Loss: 0.17989668746789297 Validation Loss: 0.7127622961997986\n",
      "Epoch 5459: Training Loss: 0.18022580444812775 Validation Loss: 0.7125923037528992\n",
      "Epoch 5460: Training Loss: 0.18080068131287894 Validation Loss: 0.7123873233795166\n",
      "Epoch 5461: Training Loss: 0.17989791929721832 Validation Loss: 0.7119489312171936\n",
      "Epoch 5462: Training Loss: 0.1799113800128301 Validation Loss: 0.7118786573410034\n",
      "Epoch 5463: Training Loss: 0.1797413875659307 Validation Loss: 0.7119222283363342\n",
      "Epoch 5464: Training Loss: 0.18062434097131094 Validation Loss: 0.7118812203407288\n",
      "Epoch 5465: Training Loss: 0.17966006696224213 Validation Loss: 0.7118951082229614\n",
      "Epoch 5466: Training Loss: 0.17958901325861612 Validation Loss: 0.7120805382728577\n",
      "Epoch 5467: Training Loss: 0.1795967072248459 Validation Loss: 0.7122265696525574\n",
      "Epoch 5468: Training Loss: 0.18030576407909393 Validation Loss: 0.7121361494064331\n",
      "Epoch 5469: Training Loss: 0.1799540470043818 Validation Loss: 0.7122089862823486\n",
      "Epoch 5470: Training Loss: 0.17976996302604675 Validation Loss: 0.7122429013252258\n",
      "Epoch 5471: Training Loss: 0.1793991575638453 Validation Loss: 0.7118706703186035\n",
      "Epoch 5472: Training Loss: 0.17981326083342233 Validation Loss: 0.711644172668457\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5473: Training Loss: 0.17939226826032004 Validation Loss: 0.7117513418197632\n",
      "Epoch 5474: Training Loss: 0.17915006478627524 Validation Loss: 0.7118616700172424\n",
      "Epoch 5475: Training Loss: 0.17959558963775635 Validation Loss: 0.711884081363678\n",
      "Epoch 5476: Training Loss: 0.17938214043776193 Validation Loss: 0.7117999196052551\n",
      "Epoch 5477: Training Loss: 0.17934222022692362 Validation Loss: 0.7118251323699951\n",
      "Epoch 5478: Training Loss: 0.17938470343748728 Validation Loss: 0.7121584415435791\n",
      "Epoch 5479: Training Loss: 0.17917748788992563 Validation Loss: 0.7121686935424805\n",
      "Epoch 5480: Training Loss: 0.17947501937548319 Validation Loss: 0.7121790051460266\n",
      "Epoch 5481: Training Loss: 0.1790054589509964 Validation Loss: 0.7120238542556763\n",
      "Epoch 5482: Training Loss: 0.17923599978288016 Validation Loss: 0.7121887803077698\n",
      "Epoch 5483: Training Loss: 0.17985080679257712 Validation Loss: 0.7123034000396729\n",
      "Epoch 5484: Training Loss: 0.17902051409085593 Validation Loss: 0.7126525044441223\n",
      "Epoch 5485: Training Loss: 0.17869209746519724 Validation Loss: 0.7123730182647705\n",
      "Epoch 5486: Training Loss: 0.17960106333096823 Validation Loss: 0.7122945189476013\n",
      "Epoch 5487: Training Loss: 0.18034318586190543 Validation Loss: 0.7117100358009338\n",
      "Epoch 5488: Training Loss: 0.17929117878278097 Validation Loss: 0.7116166353225708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5489: Training Loss: 0.17924915750821432 Validation Loss: 0.7116268277168274\n",
      "Epoch 5490: Training Loss: 0.1791940579811732 Validation Loss: 0.7116021513938904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5491: Training Loss: 0.18195407589276633 Validation Loss: 0.7120271921157837\n",
      "Epoch 5492: Training Loss: 0.17941637337207794 Validation Loss: 0.7121834754943848\n",
      "Epoch 5493: Training Loss: 0.17910516758759817 Validation Loss: 0.7121722102165222\n",
      "Epoch 5494: Training Loss: 0.17910729348659515 Validation Loss: 0.7124645709991455\n",
      "Epoch 5495: Training Loss: 0.17928813894589743 Validation Loss: 0.7128015160560608\n",
      "Epoch 5496: Training Loss: 0.17878478268782297 Validation Loss: 0.7128719091415405\n",
      "Epoch 5497: Training Loss: 0.17877804239590964 Validation Loss: 0.7128044366836548\n",
      "Epoch 5498: Training Loss: 0.17907348771890005 Validation Loss: 0.7126240134239197\n",
      "Epoch 5499: Training Loss: 0.1805624117453893 Validation Loss: 0.7121849656105042\n",
      "Epoch 5500: Training Loss: 0.17942330241203308 Validation Loss: 0.7118510007858276\n",
      "Epoch 5501: Training Loss: 0.17853247125943503 Validation Loss: 0.7117828726768494\n",
      "Epoch 5502: Training Loss: 0.17834802468617758 Validation Loss: 0.7121638655662537\n",
      "Epoch 5503: Training Loss: 0.17835700511932373 Validation Loss: 0.7123624682426453\n",
      "Epoch 5504: Training Loss: 0.1780609985192617 Validation Loss: 0.7123389840126038\n",
      "Epoch 5505: Training Loss: 0.17886993288993835 Validation Loss: 0.7121740579605103\n",
      "Epoch 5506: Training Loss: 0.17839010059833527 Validation Loss: 0.7121967077255249\n",
      "Epoch 5507: Training Loss: 0.17863435546557108 Validation Loss: 0.7123762369155884\n",
      "Epoch 5508: Training Loss: 0.17823679745197296 Validation Loss: 0.7121645212173462\n",
      "Epoch 5509: Training Loss: 0.1781847874323527 Validation Loss: 0.7121790051460266\n",
      "Epoch 5510: Training Loss: 0.17793601751327515 Validation Loss: 0.7121044397354126\n",
      "Epoch 5511: Training Loss: 0.1781369000673294 Validation Loss: 0.7119458913803101\n",
      "Epoch 5512: Training Loss: 0.17825190722942352 Validation Loss: 0.7119175791740417\n",
      "Epoch 5513: Training Loss: 0.17877964675426483 Validation Loss: 0.7117486596107483\n",
      "Epoch 5514: Training Loss: 0.1779896765947342 Validation Loss: 0.711496889591217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5515: Training Loss: 0.17852008839448294 Validation Loss: 0.711642861366272\n",
      "Epoch 5516: Training Loss: 0.17809154093265533 Validation Loss: 0.7120001316070557\n",
      "Epoch 5517: Training Loss: 0.17818543314933777 Validation Loss: 0.7120981216430664\n",
      "Epoch 5518: Training Loss: 0.17786132792631784 Validation Loss: 0.7120712399482727\n",
      "Epoch 5519: Training Loss: 0.17787697911262512 Validation Loss: 0.7119666934013367\n",
      "Epoch 5520: Training Loss: 0.1794120967388153 Validation Loss: 0.7120620608329773\n",
      "Epoch 5521: Training Loss: 0.1777824511130651 Validation Loss: 0.7121247053146362\n",
      "Epoch 5522: Training Loss: 0.17812797923882803 Validation Loss: 0.712184727191925\n",
      "Epoch 5523: Training Loss: 0.1776683876911799 Validation Loss: 0.7119261026382446\n",
      "Epoch 5524: Training Loss: 0.17789041996002197 Validation Loss: 0.7118812203407288\n",
      "Epoch 5525: Training Loss: 0.17962596813837686 Validation Loss: 0.7117049694061279\n",
      "Epoch 5526: Training Loss: 0.17797045906384787 Validation Loss: 0.7119947671890259\n",
      "Epoch 5527: Training Loss: 0.17742439111073813 Validation Loss: 0.7119916081428528\n",
      "Epoch 5528: Training Loss: 0.17725017666816711 Validation Loss: 0.7120220065116882\n",
      "Epoch 5529: Training Loss: 0.17732184628645578 Validation Loss: 0.7119702100753784\n",
      "Epoch 5530: Training Loss: 0.1776211808125178 Validation Loss: 0.7120214700698853\n",
      "Epoch 5531: Training Loss: 0.17748664319515228 Validation Loss: 0.7119145393371582\n",
      "Epoch 5532: Training Loss: 0.17762387792269388 Validation Loss: 0.7119700908660889\n",
      "Epoch 5533: Training Loss: 0.1773470938205719 Validation Loss: 0.7118907570838928\n",
      "Epoch 5534: Training Loss: 0.177641491095225 Validation Loss: 0.7117169499397278\n",
      "Epoch 5535: Training Loss: 0.17758490641911825 Validation Loss: 0.7117270827293396\n",
      "Epoch 5536: Training Loss: 0.17795486251513162 Validation Loss: 0.7120930552482605\n",
      "Epoch 5537: Training Loss: 0.1775814096132914 Validation Loss: 0.7123875617980957\n",
      "Epoch 5538: Training Loss: 0.17816107471783957 Validation Loss: 0.7125308513641357\n",
      "Epoch 5539: Training Loss: 0.1774383286635081 Validation Loss: 0.7121959924697876\n",
      "Epoch 5540: Training Loss: 0.1774860918521881 Validation Loss: 0.7121151685714722\n",
      "Epoch 5541: Training Loss: 0.1765949676434199 Validation Loss: 0.7123639583587646\n",
      "Epoch 5542: Training Loss: 0.1767666737238566 Validation Loss: 0.7121299505233765\n",
      "Epoch 5543: Training Loss: 0.17693361143271127 Validation Loss: 0.711831271648407\n",
      "Epoch 5544: Training Loss: 0.1769281675418218 Validation Loss: 0.7119019031524658\n",
      "Epoch 5545: Training Loss: 0.17668328185876211 Validation Loss: 0.7119386196136475\n",
      "Epoch 5546: Training Loss: 0.17731388409932455 Validation Loss: 0.7119240164756775\n",
      "Epoch 5547: Training Loss: 0.1774096836646398 Validation Loss: 0.7118112444877625\n",
      "Epoch 5548: Training Loss: 0.17702403167883554 Validation Loss: 0.7117972373962402\n",
      "Epoch 5549: Training Loss: 0.1769226094086965 Validation Loss: 0.7119017839431763\n",
      "Epoch 5550: Training Loss: 0.17672098676363626 Validation Loss: 0.7119172215461731\n",
      "Epoch 5551: Training Loss: 0.17745257914066315 Validation Loss: 0.711929202079773\n",
      "Epoch 5552: Training Loss: 0.1766244868437449 Validation Loss: 0.7118113040924072\n",
      "Epoch 5553: Training Loss: 0.1778341829776764 Validation Loss: 0.7121149897575378\n",
      "Epoch 5554: Training Loss: 0.17677289247512817 Validation Loss: 0.712074339389801\n",
      "Epoch 5555: Training Loss: 0.17650450766086578 Validation Loss: 0.7119784951210022\n",
      "Epoch 5556: Training Loss: 0.1770048439502716 Validation Loss: 0.7120055556297302\n",
      "Epoch 5557: Training Loss: 0.17659941812356314 Validation Loss: 0.7117548584938049\n",
      "Epoch 5558: Training Loss: 0.1769851098457972 Validation Loss: 0.711736261844635\n",
      "Epoch 5559: Training Loss: 0.1763466795285543 Validation Loss: 0.7122220993041992\n",
      "Epoch 5560: Training Loss: 0.1766729156176249 Validation Loss: 0.7121306657791138\n",
      "Epoch 5561: Training Loss: 0.17647424340248108 Validation Loss: 0.7123464345932007\n",
      "Epoch 5562: Training Loss: 0.17616912722587585 Validation Loss: 0.7121168375015259\n",
      "Epoch 5563: Training Loss: 0.17793246606985727 Validation Loss: 0.711769163608551\n",
      "Epoch 5564: Training Loss: 0.1769040028254191 Validation Loss: 0.7119075655937195\n",
      "Epoch 5565: Training Loss: 0.1762277732292811 Validation Loss: 0.7119889855384827\n",
      "Epoch 5566: Training Loss: 0.1768010606368383 Validation Loss: 0.7119427919387817\n",
      "Epoch 5567: Training Loss: 0.17627168198426565 Validation Loss: 0.712196409702301\n",
      "Epoch 5568: Training Loss: 0.17625504235426584 Validation Loss: 0.7123739719390869\n",
      "Epoch 5569: Training Loss: 0.17640100419521332 Validation Loss: 0.7119020819664001\n",
      "Epoch 5570: Training Loss: 0.1773541917403539 Validation Loss: 0.7121002674102783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5571: Training Loss: 0.17634464303652445 Validation Loss: 0.7120579481124878\n",
      "Epoch 5572: Training Loss: 0.17621946334838867 Validation Loss: 0.7117348313331604\n",
      "Epoch 5573: Training Loss: 0.17633783320585886 Validation Loss: 0.7119616866111755\n",
      "Epoch 5574: Training Loss: 0.1763816624879837 Validation Loss: 0.711895763874054\n",
      "Epoch 5575: Training Loss: 0.17536453902721405 Validation Loss: 0.7121258974075317\n",
      "Epoch 5576: Training Loss: 0.17844687898953757 Validation Loss: 0.7118815183639526\n",
      "Epoch 5577: Training Loss: 0.17566912869612375 Validation Loss: 0.711738109588623\n",
      "Epoch 5578: Training Loss: 0.17607036729653677 Validation Loss: 0.7117630243301392\n",
      "Epoch 5579: Training Loss: 0.1765832006931305 Validation Loss: 0.7119333744049072\n",
      "Epoch 5580: Training Loss: 0.17564463118712106 Validation Loss: 0.7122048735618591\n",
      "Epoch 5581: Training Loss: 0.17571081221103668 Validation Loss: 0.7120606303215027\n",
      "Epoch 5582: Training Loss: 0.17611191173394522 Validation Loss: 0.7122414112091064\n",
      "Epoch 5583: Training Loss: 0.17551052073637644 Validation Loss: 0.7123690247535706\n",
      "Epoch 5584: Training Loss: 0.17567587892214456 Validation Loss: 0.7126263976097107\n",
      "Epoch 5585: Training Loss: 0.17601586878299713 Validation Loss: 0.7125343084335327\n",
      "Epoch 5586: Training Loss: 0.17544940610726675 Validation Loss: 0.7120311260223389\n",
      "Epoch 5587: Training Loss: 0.1754420151313146 Validation Loss: 0.7118038535118103\n",
      "Epoch 5588: Training Loss: 0.17526010672251383 Validation Loss: 0.711517870426178\n",
      "Epoch 5589: Training Loss: 0.17568988601366678 Validation Loss: 0.7115098834037781\n",
      "Epoch 5590: Training Loss: 0.17573960622151694 Validation Loss: 0.7115129828453064\n",
      "Epoch 5591: Training Loss: 0.1758578618367513 Validation Loss: 0.7116701602935791\n",
      "Epoch 5592: Training Loss: 0.17559177180131277 Validation Loss: 0.7118240594863892\n",
      "Epoch 5593: Training Loss: 0.17463266849517822 Validation Loss: 0.7121590375900269\n",
      "Epoch 5594: Training Loss: 0.17490127185980478 Validation Loss: 0.7119864821434021\n",
      "Epoch 5595: Training Loss: 0.17539813121159872 Validation Loss: 0.7120088934898376\n",
      "Epoch 5596: Training Loss: 0.17554516593615213 Validation Loss: 0.7120645642280579\n",
      "Epoch 5597: Training Loss: 0.17531786362330118 Validation Loss: 0.7120248079299927\n",
      "Epoch 5598: Training Loss: 0.17590244114398956 Validation Loss: 0.7122064828872681\n",
      "Epoch 5599: Training Loss: 0.17535778880119324 Validation Loss: 0.7119414806365967\n",
      "Epoch 5600: Training Loss: 0.17574403683344522 Validation Loss: 0.7122263312339783\n",
      "Epoch 5601: Training Loss: 0.1754507670799891 Validation Loss: 0.7123852968215942\n",
      "Epoch 5602: Training Loss: 0.17494492729504904 Validation Loss: 0.7121526598930359\n",
      "Epoch 5603: Training Loss: 0.17479720215002695 Validation Loss: 0.7120669484138489\n",
      "Epoch 5604: Training Loss: 0.17498035232226053 Validation Loss: 0.7120954990386963\n",
      "Epoch 5605: Training Loss: 0.1753786156574885 Validation Loss: 0.7119405269622803\n",
      "Epoch 5606: Training Loss: 0.1748833159605662 Validation Loss: 0.7115747928619385\n",
      "Epoch 5607: Training Loss: 0.17511009176572165 Validation Loss: 0.7113900184631348\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5608: Training Loss: 0.17586172620455423 Validation Loss: 0.7115499377250671\n",
      "Epoch 5609: Training Loss: 0.17530915141105652 Validation Loss: 0.7116892337799072\n",
      "Epoch 5610: Training Loss: 0.17484456797440848 Validation Loss: 0.7115809321403503\n",
      "Epoch 5611: Training Loss: 0.17517513036727905 Validation Loss: 0.7119361758232117\n",
      "Epoch 5612: Training Loss: 0.17503014703591666 Validation Loss: 0.7118213176727295\n",
      "Epoch 5613: Training Loss: 0.17501051723957062 Validation Loss: 0.711895227432251\n",
      "Epoch 5614: Training Loss: 0.17468944688638052 Validation Loss: 0.7123279571533203\n",
      "Epoch 5615: Training Loss: 0.17573727170626322 Validation Loss: 0.7126016020774841\n",
      "Epoch 5616: Training Loss: 0.17462790509064993 Validation Loss: 0.7125101685523987\n",
      "Epoch 5617: Training Loss: 0.17503896355628967 Validation Loss: 0.7120789885520935\n",
      "Epoch 5618: Training Loss: 0.17457320292790732 Validation Loss: 0.7117888331413269\n",
      "Epoch 5619: Training Loss: 0.1749140868584315 Validation Loss: 0.7116152048110962\n",
      "Epoch 5620: Training Loss: 0.17459616561730704 Validation Loss: 0.7116155028343201\n",
      "Epoch 5621: Training Loss: 0.174607182542483 Validation Loss: 0.7118857502937317\n",
      "Epoch 5622: Training Loss: 0.17427108685175577 Validation Loss: 0.7120561003684998\n",
      "Epoch 5623: Training Loss: 0.17454109092553458 Validation Loss: 0.7119390964508057\n",
      "Epoch 5624: Training Loss: 0.17432449261347452 Validation Loss: 0.7121292352676392\n",
      "Epoch 5625: Training Loss: 0.1744731366634369 Validation Loss: 0.7118438482284546\n",
      "Epoch 5626: Training Loss: 0.17390520870685577 Validation Loss: 0.7118584513664246\n",
      "Epoch 5627: Training Loss: 0.1741806964079539 Validation Loss: 0.71202152967453\n",
      "Epoch 5628: Training Loss: 0.17457583049933115 Validation Loss: 0.7120233178138733\n",
      "Epoch 5629: Training Loss: 0.1739437977472941 Validation Loss: 0.7123561501502991\n",
      "Epoch 5630: Training Loss: 0.17502826948960623 Validation Loss: 0.712375819683075\n",
      "Epoch 5631: Training Loss: 0.1735693464676539 Validation Loss: 0.7118562459945679\n",
      "Epoch 5632: Training Loss: 0.17450772722562155 Validation Loss: 0.7117816805839539\n",
      "Epoch 5633: Training Loss: 0.17459515730539957 Validation Loss: 0.7116427421569824\n",
      "Epoch 5634: Training Loss: 0.17394969860712686 Validation Loss: 0.7115786075592041\n",
      "Epoch 5635: Training Loss: 0.17375553150971731 Validation Loss: 0.711614727973938\n",
      "Epoch 5636: Training Loss: 0.17408366004625955 Validation Loss: 0.7119422554969788\n",
      "Epoch 5637: Training Loss: 0.1737932413816452 Validation Loss: 0.7121554017066956\n",
      "Epoch 5638: Training Loss: 0.17390135924021402 Validation Loss: 0.7122699618339539\n",
      "Epoch 5639: Training Loss: 0.1736075927813848 Validation Loss: 0.7122361063957214\n",
      "Epoch 5640: Training Loss: 0.17363926768302917 Validation Loss: 0.7119277119636536\n",
      "Epoch 5641: Training Loss: 0.17386174698670706 Validation Loss: 0.7118051052093506\n",
      "Epoch 5642: Training Loss: 0.17458599309126535 Validation Loss: 0.7118837833404541\n",
      "Epoch 5643: Training Loss: 0.1736263632774353 Validation Loss: 0.7115955352783203\n",
      "Epoch 5644: Training Loss: 0.17366748054822287 Validation Loss: 0.7116135954856873\n",
      "Epoch 5645: Training Loss: 0.17329874138037363 Validation Loss: 0.7116850018501282\n",
      "Epoch 5646: Training Loss: 0.1741689791282018 Validation Loss: 0.7118844389915466\n",
      "Epoch 5647: Training Loss: 0.17405030131340027 Validation Loss: 0.7121117115020752\n",
      "Epoch 5648: Training Loss: 0.17355847358703613 Validation Loss: 0.7118726968765259\n",
      "Epoch 5649: Training Loss: 0.17409429450829825 Validation Loss: 0.7117915153503418\n",
      "Epoch 5650: Training Loss: 0.17368709544340769 Validation Loss: 0.7118475437164307\n",
      "Epoch 5651: Training Loss: 0.17320929964383444 Validation Loss: 0.7119179368019104\n",
      "Epoch 5652: Training Loss: 0.17329404751459757 Validation Loss: 0.712058424949646\n",
      "Epoch 5653: Training Loss: 0.17334002256393433 Validation Loss: 0.7124766111373901\n",
      "Epoch 5654: Training Loss: 0.17412509520848593 Validation Loss: 0.7126680016517639\n",
      "Epoch 5655: Training Loss: 0.17288120090961456 Validation Loss: 0.7124620079994202\n",
      "Epoch 5656: Training Loss: 0.17373248438040415 Validation Loss: 0.7122154235839844\n",
      "Epoch 5657: Training Loss: 0.17289969325065613 Validation Loss: 0.7118767499923706\n",
      "Epoch 5658: Training Loss: 0.1738619158665339 Validation Loss: 0.7115358710289001\n",
      "Epoch 5659: Training Loss: 0.1733036239941915 Validation Loss: 0.7116451263427734\n",
      "Epoch 5660: Training Loss: 0.17425274352232614 Validation Loss: 0.7119215130805969\n",
      "Epoch 5661: Training Loss: 0.17344471315542856 Validation Loss: 0.7120869159698486\n",
      "Epoch 5662: Training Loss: 0.17300312717755637 Validation Loss: 0.7121489644050598\n",
      "Epoch 5663: Training Loss: 0.17309461534023285 Validation Loss: 0.7122722268104553\n",
      "Epoch 5664: Training Loss: 0.17367706696192423 Validation Loss: 0.7120246291160583\n",
      "Epoch 5665: Training Loss: 0.17297962804635367 Validation Loss: 0.7117860317230225\n",
      "Epoch 5666: Training Loss: 0.17296642065048218 Validation Loss: 0.7117626070976257\n",
      "Epoch 5667: Training Loss: 0.17306718230247498 Validation Loss: 0.7117759585380554\n",
      "Epoch 5668: Training Loss: 0.1733707735935847 Validation Loss: 0.7118879556655884\n",
      "Epoch 5669: Training Loss: 0.17275643845399222 Validation Loss: 0.7120233774185181\n",
      "Epoch 5670: Training Loss: 0.17268731693426767 Validation Loss: 0.7118327617645264\n",
      "Epoch 5671: Training Loss: 0.1729138990243276 Validation Loss: 0.712134599685669\n",
      "Epoch 5672: Training Loss: 0.17292868594328561 Validation Loss: 0.7120205163955688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5673: Training Loss: 0.17306675016880035 Validation Loss: 0.7119348645210266\n",
      "Epoch 5674: Training Loss: 0.1727626472711563 Validation Loss: 0.7118747234344482\n",
      "Epoch 5675: Training Loss: 0.1735164870818456 Validation Loss: 0.7123316526412964\n",
      "Epoch 5676: Training Loss: 0.17321579158306122 Validation Loss: 0.7125635743141174\n",
      "Epoch 5677: Training Loss: 0.1727444330851237 Validation Loss: 0.7124807834625244\n",
      "Epoch 5678: Training Loss: 0.17250393827756247 Validation Loss: 0.7119160294532776\n",
      "Epoch 5679: Training Loss: 0.1723700910806656 Validation Loss: 0.7117109894752502\n",
      "Epoch 5680: Training Loss: 0.1730637550354004 Validation Loss: 0.711661159992218\n",
      "Epoch 5681: Training Loss: 0.17408423125743866 Validation Loss: 0.7117670774459839\n",
      "Epoch 5682: Training Loss: 0.17278555035591125 Validation Loss: 0.7119606137275696\n",
      "Epoch 5683: Training Loss: 0.17223606010278067 Validation Loss: 0.7120341062545776\n",
      "Epoch 5684: Training Loss: 0.17261666556199393 Validation Loss: 0.7120526432991028\n",
      "Epoch 5685: Training Loss: 0.1720979611078898 Validation Loss: 0.7119629383087158\n",
      "Epoch 5686: Training Loss: 0.17181665698687235 Validation Loss: 0.7118784189224243\n",
      "Epoch 5687: Training Loss: 0.1721209535996119 Validation Loss: 0.7115578651428223\n",
      "Epoch 5688: Training Loss: 0.17178210616111755 Validation Loss: 0.7117438316345215\n",
      "Epoch 5689: Training Loss: 0.17234181861082712 Validation Loss: 0.7119267582893372\n",
      "Epoch 5690: Training Loss: 0.17239279548327127 Validation Loss: 0.7122199535369873\n",
      "Epoch 5691: Training Loss: 0.17196829120318094 Validation Loss: 0.7124385237693787\n",
      "Epoch 5692: Training Loss: 0.17331704994042715 Validation Loss: 0.712465763092041\n",
      "Epoch 5693: Training Loss: 0.1722196638584137 Validation Loss: 0.7122142314910889\n",
      "Epoch 5694: Training Loss: 0.17245461543401083 Validation Loss: 0.7121191620826721\n",
      "Epoch 5695: Training Loss: 0.17230096956094107 Validation Loss: 0.7118728160858154\n",
      "Epoch 5696: Training Loss: 0.1717867652575175 Validation Loss: 0.7118737697601318\n",
      "Epoch 5697: Training Loss: 0.17195422450701395 Validation Loss: 0.71186763048172\n",
      "Epoch 5698: Training Loss: 0.1718688358863195 Validation Loss: 0.7120140790939331\n",
      "Epoch 5699: Training Loss: 0.17174010972181955 Validation Loss: 0.712279736995697\n",
      "Epoch 5700: Training Loss: 0.17174377044041952 Validation Loss: 0.7122193574905396\n",
      "Epoch 5701: Training Loss: 0.17173032462596893 Validation Loss: 0.7119178175926208\n",
      "Epoch 5702: Training Loss: 0.17177019019921622 Validation Loss: 0.7117103338241577\n",
      "Epoch 5703: Training Loss: 0.17203938961029053 Validation Loss: 0.7116773128509521\n",
      "Epoch 5704: Training Loss: 0.17172764241695404 Validation Loss: 0.7117650508880615\n",
      "Epoch 5705: Training Loss: 0.17244264483451843 Validation Loss: 0.7116535902023315\n",
      "Epoch 5706: Training Loss: 0.17207190891106924 Validation Loss: 0.7121610045433044\n",
      "Epoch 5707: Training Loss: 0.17220413188139597 Validation Loss: 0.7122847437858582\n",
      "Epoch 5708: Training Loss: 0.17146162192026773 Validation Loss: 0.7124645113945007\n",
      "Epoch 5709: Training Loss: 0.17148665090401968 Validation Loss: 0.7122488021850586\n",
      "Epoch 5710: Training Loss: 0.1721997708082199 Validation Loss: 0.7122460603713989\n",
      "Epoch 5711: Training Loss: 0.1709884305795034 Validation Loss: 0.7118819355964661\n",
      "Epoch 5712: Training Loss: 0.1713771472374598 Validation Loss: 0.7116500735282898\n",
      "Epoch 5713: Training Loss: 0.1713545819123586 Validation Loss: 0.7118474841117859\n",
      "Epoch 5714: Training Loss: 0.17166839043299356 Validation Loss: 0.7118688821792603\n",
      "Epoch 5715: Training Loss: 0.17127243181069693 Validation Loss: 0.7122179269790649\n",
      "Epoch 5716: Training Loss: 0.17157711585362753 Validation Loss: 0.7123625874519348\n",
      "Epoch 5717: Training Loss: 0.17143457134564719 Validation Loss: 0.7125064730644226\n",
      "Epoch 5718: Training Loss: 0.17132453620433807 Validation Loss: 0.7121712565422058\n",
      "Epoch 5719: Training Loss: 0.171369269490242 Validation Loss: 0.7121961116790771\n",
      "Epoch 5720: Training Loss: 0.17212512095769247 Validation Loss: 0.7122577428817749\n",
      "Epoch 5721: Training Loss: 0.17091124753157297 Validation Loss: 0.7120859622955322\n",
      "Epoch 5722: Training Loss: 0.17194427053133646 Validation Loss: 0.7120082974433899\n",
      "Epoch 5723: Training Loss: 0.17172256608804068 Validation Loss: 0.7120030522346497\n",
      "Epoch 5724: Training Loss: 0.17110691467920938 Validation Loss: 0.7119174003601074\n",
      "Epoch 5725: Training Loss: 0.17137746016184488 Validation Loss: 0.7118916511535645\n",
      "Epoch 5726: Training Loss: 0.171063502629598 Validation Loss: 0.711741030216217\n",
      "Epoch 5727: Training Loss: 0.17125062147776285 Validation Loss: 0.7119209170341492\n",
      "Epoch 5728: Training Loss: 0.17122551302115122 Validation Loss: 0.7118842005729675\n",
      "Epoch 5729: Training Loss: 0.1730512777964274 Validation Loss: 0.7120704650878906\n",
      "Epoch 5730: Training Loss: 0.17131340503692627 Validation Loss: 0.7121047973632812\n",
      "Epoch 5731: Training Loss: 0.17123655478159586 Validation Loss: 0.7123712301254272\n",
      "Epoch 5732: Training Loss: 0.1710421641667684 Validation Loss: 0.7121361494064331\n",
      "Epoch 5733: Training Loss: 0.1709014674027761 Validation Loss: 0.7120094895362854\n",
      "Epoch 5734: Training Loss: 0.1717853993177414 Validation Loss: 0.7118661403656006\n",
      "Epoch 5735: Training Loss: 0.17061848441759744 Validation Loss: 0.7121001482009888\n",
      "Epoch 5736: Training Loss: 0.17075463632742563 Validation Loss: 0.712216317653656\n",
      "Epoch 5737: Training Loss: 0.17261233429114023 Validation Loss: 0.7123129367828369\n",
      "Epoch 5738: Training Loss: 0.17158984144528708 Validation Loss: 0.7126673460006714\n",
      "Epoch 5739: Training Loss: 0.17284929007291794 Validation Loss: 0.7126626372337341\n",
      "Epoch 5740: Training Loss: 0.1701798290014267 Validation Loss: 0.7127602100372314\n",
      "Epoch 5741: Training Loss: 0.17063904305299124 Validation Loss: 0.7124230265617371\n",
      "Epoch 5742: Training Loss: 0.17045308152834573 Validation Loss: 0.7121545076370239\n",
      "Epoch 5743: Training Loss: 0.1702257295449575 Validation Loss: 0.7124316692352295\n",
      "Epoch 5744: Training Loss: 0.17076080044110617 Validation Loss: 0.7121022939682007\n",
      "Epoch 5745: Training Loss: 0.17044329146544138 Validation Loss: 0.712196409702301\n",
      "Epoch 5746: Training Loss: 0.17014922698338827 Validation Loss: 0.7120717763900757\n",
      "Epoch 5747: Training Loss: 0.1711802581946055 Validation Loss: 0.7121419906616211\n",
      "Epoch 5748: Training Loss: 0.1701731930176417 Validation Loss: 0.7120246291160583\n",
      "Epoch 5749: Training Loss: 0.17027788857618967 Validation Loss: 0.7121742367744446\n",
      "Epoch 5750: Training Loss: 0.1702315260966619 Validation Loss: 0.7122703194618225\n",
      "Epoch 5751: Training Loss: 0.17146945496400198 Validation Loss: 0.7123043537139893\n",
      "Epoch 5752: Training Loss: 0.17033842206001282 Validation Loss: 0.7122604250907898\n",
      "Epoch 5753: Training Loss: 0.17028493682543436 Validation Loss: 0.7122538685798645\n",
      "Epoch 5754: Training Loss: 0.17001696427663168 Validation Loss: 0.7120263576507568\n",
      "Epoch 5755: Training Loss: 0.17029734452565512 Validation Loss: 0.7120382785797119\n",
      "Epoch 5756: Training Loss: 0.1702278107404709 Validation Loss: 0.7121065258979797\n",
      "Epoch 5757: Training Loss: 0.1700031558672587 Validation Loss: 0.7117184400558472\n",
      "Epoch 5758: Training Loss: 0.1698843389749527 Validation Loss: 0.7116849422454834\n",
      "Epoch 5759: Training Loss: 0.16956281661987305 Validation Loss: 0.7120050191879272\n",
      "Epoch 5760: Training Loss: 0.1707992653052012 Validation Loss: 0.7121493816375732\n",
      "Epoch 5761: Training Loss: 0.1702654759089152 Validation Loss: 0.7119848728179932\n",
      "Epoch 5762: Training Loss: 0.16986357172330221 Validation Loss: 0.7120921611785889\n",
      "Epoch 5763: Training Loss: 0.16978163520495096 Validation Loss: 0.7121034860610962\n",
      "Epoch 5764: Training Loss: 0.1698612074057261 Validation Loss: 0.7121624946594238\n",
      "Epoch 5765: Training Loss: 0.16961697737375894 Validation Loss: 0.7117425799369812\n",
      "Epoch 5766: Training Loss: 0.16984318693478903 Validation Loss: 0.7114390730857849\n",
      "Epoch 5767: Training Loss: 0.16977029045422873 Validation Loss: 0.7118436694145203\n",
      "Epoch 5768: Training Loss: 0.16979054609934488 Validation Loss: 0.7120228409767151\n",
      "Epoch 5769: Training Loss: 0.16964695354302725 Validation Loss: 0.7120664715766907\n",
      "Epoch 5770: Training Loss: 0.16972880065441132 Validation Loss: 0.7121538519859314\n",
      "Epoch 5771: Training Loss: 0.1698301782210668 Validation Loss: 0.7120792865753174\n",
      "Epoch 5772: Training Loss: 0.1697091112534205 Validation Loss: 0.7119655609130859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5773: Training Loss: 0.1693616509437561 Validation Loss: 0.7120622396469116\n",
      "Epoch 5774: Training Loss: 0.1696378489335378 Validation Loss: 0.7117168307304382\n",
      "Epoch 5775: Training Loss: 0.17094522714614868 Validation Loss: 0.7118581533432007\n",
      "Epoch 5776: Training Loss: 0.1691200832525889 Validation Loss: 0.7121220827102661\n",
      "Epoch 5777: Training Loss: 0.16939819355805716 Validation Loss: 0.7119898796081543\n",
      "Epoch 5778: Training Loss: 0.16956964631875357 Validation Loss: 0.7122930288314819\n",
      "Epoch 5779: Training Loss: 0.16910873850186667 Validation Loss: 0.7123512029647827\n",
      "Epoch 5780: Training Loss: 0.16948539515336355 Validation Loss: 0.7121557593345642\n",
      "Epoch 5781: Training Loss: 0.16919901967048645 Validation Loss: 0.7122436761856079\n",
      "Epoch 5782: Training Loss: 0.1698787659406662 Validation Loss: 0.7118958830833435\n",
      "Epoch 5783: Training Loss: 0.1690913289785385 Validation Loss: 0.7119228839874268\n",
      "Epoch 5784: Training Loss: 0.168865496913592 Validation Loss: 0.7120437026023865\n",
      "Epoch 5785: Training Loss: 0.1689021736383438 Validation Loss: 0.7121158838272095\n",
      "Epoch 5786: Training Loss: 0.1693924218416214 Validation Loss: 0.712104320526123\n",
      "Epoch 5787: Training Loss: 0.16989759107430777 Validation Loss: 0.7124118208885193\n",
      "Epoch 5788: Training Loss: 0.1689903289079666 Validation Loss: 0.712682843208313\n",
      "Epoch 5789: Training Loss: 0.16893870135148367 Validation Loss: 0.7126708030700684\n",
      "Epoch 5790: Training Loss: 0.16936073700586954 Validation Loss: 0.7125844359397888\n",
      "Epoch 5791: Training Loss: 0.1694647620121638 Validation Loss: 0.7125765085220337\n",
      "Epoch 5792: Training Loss: 0.1688067615032196 Validation Loss: 0.7125012278556824\n",
      "Epoch 5793: Training Loss: 0.16914819677670798 Validation Loss: 0.7124670147895813\n",
      "Epoch 5794: Training Loss: 0.16932465136051178 Validation Loss: 0.7123204469680786\n",
      "Epoch 5795: Training Loss: 0.16851244866847992 Validation Loss: 0.7122962474822998\n",
      "Epoch 5796: Training Loss: 0.16960627337296805 Validation Loss: 0.7120741605758667\n",
      "Epoch 5797: Training Loss: 0.1688454399506251 Validation Loss: 0.7122548222541809\n",
      "Epoch 5798: Training Loss: 0.17116432636976242 Validation Loss: 0.7119244337081909\n",
      "Epoch 5799: Training Loss: 0.16878676414489746 Validation Loss: 0.7118432521820068\n",
      "Epoch 5800: Training Loss: 0.16873126725355783 Validation Loss: 0.7119722366333008\n",
      "Epoch 5801: Training Loss: 0.16869372626145682 Validation Loss: 0.7121531963348389\n",
      "Epoch 5802: Training Loss: 0.1696167935927709 Validation Loss: 0.7122172713279724\n",
      "Epoch 5803: Training Loss: 0.16878697276115417 Validation Loss: 0.7121360898017883\n",
      "Epoch 5804: Training Loss: 0.16888837019602457 Validation Loss: 0.7119624018669128\n",
      "Epoch 5805: Training Loss: 0.16883228719234467 Validation Loss: 0.7121295928955078\n",
      "Epoch 5806: Training Loss: 0.16863842805226645 Validation Loss: 0.712168276309967\n",
      "Epoch 5807: Training Loss: 0.16858923435211182 Validation Loss: 0.7120072245597839\n",
      "Epoch 5808: Training Loss: 0.16860721011956534 Validation Loss: 0.7119196653366089\n",
      "Epoch 5809: Training Loss: 0.16862628360589346 Validation Loss: 0.7122877836227417\n",
      "Epoch 5810: Training Loss: 0.16849102079868317 Validation Loss: 0.7124668955802917\n",
      "Epoch 5811: Training Loss: 0.16850289702415466 Validation Loss: 0.7124895453453064\n",
      "Epoch 5812: Training Loss: 0.16861072182655334 Validation Loss: 0.7121949791908264\n",
      "Epoch 5813: Training Loss: 0.16974092026551565 Validation Loss: 0.7121424674987793\n",
      "Epoch 5814: Training Loss: 0.1683481534322103 Validation Loss: 0.7120854258537292\n",
      "Epoch 5815: Training Loss: 0.16809012492497763 Validation Loss: 0.7121672034263611\n",
      "Epoch 5816: Training Loss: 0.16875767211119333 Validation Loss: 0.7122821807861328\n",
      "Epoch 5817: Training Loss: 0.16891043384869894 Validation Loss: 0.7123020887374878\n",
      "Epoch 5818: Training Loss: 0.16818645099798837 Validation Loss: 0.7123197317123413\n",
      "Epoch 5819: Training Loss: 0.16872786978880563 Validation Loss: 0.7120834589004517\n",
      "Epoch 5820: Training Loss: 0.16814546783765158 Validation Loss: 0.7118751406669617\n",
      "Epoch 5821: Training Loss: 0.16829184194405875 Validation Loss: 0.7121859788894653\n",
      "Epoch 5822: Training Loss: 0.1680362323919932 Validation Loss: 0.7122203707695007\n",
      "Epoch 5823: Training Loss: 0.16828388969103494 Validation Loss: 0.712273120880127\n",
      "Epoch 5824: Training Loss: 0.16841301818688711 Validation Loss: 0.712440550327301\n",
      "Epoch 5825: Training Loss: 0.1680196871360143 Validation Loss: 0.7121262550354004\n",
      "Epoch 5826: Training Loss: 0.16797352830568948 Validation Loss: 0.7122842073440552\n",
      "Epoch 5827: Training Loss: 0.1688422461350759 Validation Loss: 0.7124347686767578\n",
      "Epoch 5828: Training Loss: 0.1678087761004766 Validation Loss: 0.7123425006866455\n",
      "Epoch 5829: Training Loss: 0.16813371082146963 Validation Loss: 0.7119182348251343\n",
      "Epoch 5830: Training Loss: 0.16791638235251108 Validation Loss: 0.7116159200668335\n",
      "Epoch 5831: Training Loss: 0.16801839073499045 Validation Loss: 0.7118561267852783\n",
      "Epoch 5832: Training Loss: 0.16772193213303885 Validation Loss: 0.7121953964233398\n",
      "Epoch 5833: Training Loss: 0.16802063584327698 Validation Loss: 0.7123246192932129\n",
      "Epoch 5834: Training Loss: 0.16919590532779694 Validation Loss: 0.7123308777809143\n",
      "Epoch 5835: Training Loss: 0.1679197351137797 Validation Loss: 0.7122890949249268\n",
      "Epoch 5836: Training Loss: 0.16814072926839194 Validation Loss: 0.7123042941093445\n",
      "Epoch 5837: Training Loss: 0.16748301684856415 Validation Loss: 0.712044894695282\n",
      "Epoch 5838: Training Loss: 0.16720203558603922 Validation Loss: 0.7117984890937805\n",
      "Epoch 5839: Training Loss: 0.1678276906410853 Validation Loss: 0.711532711982727\n",
      "Epoch 5840: Training Loss: 0.16773454348246256 Validation Loss: 0.7117457985877991\n",
      "Epoch 5841: Training Loss: 0.16752093036969504 Validation Loss: 0.7116974592208862\n",
      "Epoch 5842: Training Loss: 0.1679934412240982 Validation Loss: 0.7122090458869934\n",
      "Epoch 5843: Training Loss: 0.1673668771982193 Validation Loss: 0.7121111750602722\n",
      "Epoch 5844: Training Loss: 0.1674781491359075 Validation Loss: 0.7121924757957458\n",
      "Epoch 5845: Training Loss: 0.1675513039032618 Validation Loss: 0.7120224237442017\n",
      "Epoch 5846: Training Loss: 0.16740461190541586 Validation Loss: 0.7122546434402466\n",
      "Epoch 5847: Training Loss: 0.16748420397440592 Validation Loss: 0.7126287817955017\n",
      "Epoch 5848: Training Loss: 0.1677082230647405 Validation Loss: 0.7124071717262268\n",
      "Epoch 5849: Training Loss: 0.16701138516267142 Validation Loss: 0.7123620510101318\n",
      "Epoch 5850: Training Loss: 0.1670894573132197 Validation Loss: 0.7121120095252991\n",
      "Epoch 5851: Training Loss: 0.1673277666171392 Validation Loss: 0.7122352719306946\n",
      "Epoch 5852: Training Loss: 0.16719197730223337 Validation Loss: 0.7120494246482849\n",
      "Epoch 5853: Training Loss: 0.1673782765865326 Validation Loss: 0.712248682975769\n",
      "Epoch 5854: Training Loss: 0.16847020387649536 Validation Loss: 0.7124354839324951\n",
      "Epoch 5855: Training Loss: 0.16814147929350534 Validation Loss: 0.7122799754142761\n",
      "Epoch 5856: Training Loss: 0.1666656235853831 Validation Loss: 0.7122915387153625\n",
      "Epoch 5857: Training Loss: 0.16678430636723837 Validation Loss: 0.7120129466056824\n",
      "Epoch 5858: Training Loss: 0.16708614925543466 Validation Loss: 0.7120265960693359\n",
      "Epoch 5859: Training Loss: 0.1669511099656423 Validation Loss: 0.7117894291877747\n",
      "Epoch 5860: Training Loss: 0.1666538566350937 Validation Loss: 0.7116683721542358\n",
      "Epoch 5861: Training Loss: 0.16660955548286438 Validation Loss: 0.7118796110153198\n",
      "Epoch 5862: Training Loss: 0.1667345017194748 Validation Loss: 0.7120651006698608\n",
      "Epoch 5863: Training Loss: 0.16779696444670358 Validation Loss: 0.7122182250022888\n",
      "Epoch 5864: Training Loss: 0.16713046034177145 Validation Loss: 0.7122683525085449\n",
      "Epoch 5865: Training Loss: 0.16716600954532623 Validation Loss: 0.7122392654418945\n",
      "Epoch 5866: Training Loss: 0.1664741039276123 Validation Loss: 0.7120923399925232\n",
      "Epoch 5867: Training Loss: 0.16653857131799063 Validation Loss: 0.7122032046318054\n",
      "Epoch 5868: Training Loss: 0.166571706533432 Validation Loss: 0.7121343612670898\n",
      "Epoch 5869: Training Loss: 0.1660464107990265 Validation Loss: 0.7121443152427673\n",
      "Epoch 5870: Training Loss: 0.16660299400488535 Validation Loss: 0.7119520902633667\n",
      "Epoch 5871: Training Loss: 0.1663250724474589 Validation Loss: 0.7120571732521057\n",
      "Epoch 5872: Training Loss: 0.16635258495807648 Validation Loss: 0.712243378162384\n",
      "Epoch 5873: Training Loss: 0.16703482965628305 Validation Loss: 0.712287962436676\n",
      "Epoch 5874: Training Loss: 0.16674768924713135 Validation Loss: 0.7121838927268982\n",
      "Epoch 5875: Training Loss: 0.16651165982087454 Validation Loss: 0.7124564051628113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5876: Training Loss: 0.16660315295060477 Validation Loss: 0.7122761011123657\n",
      "Epoch 5877: Training Loss: 0.16669836143652597 Validation Loss: 0.7122111916542053\n",
      "Epoch 5878: Training Loss: 0.16734819114208221 Validation Loss: 0.7122326493263245\n",
      "Epoch 5879: Training Loss: 0.16681801279385886 Validation Loss: 0.7121161818504333\n",
      "Epoch 5880: Training Loss: 0.1666004260381063 Validation Loss: 0.7123167514801025\n",
      "Epoch 5881: Training Loss: 0.16628476480642954 Validation Loss: 0.711999773979187\n",
      "Epoch 5882: Training Loss: 0.1666217396656672 Validation Loss: 0.7123627066612244\n",
      "Epoch 5883: Training Loss: 0.16608536740144095 Validation Loss: 0.7126796841621399\n",
      "Epoch 5884: Training Loss: 0.16661839187145233 Validation Loss: 0.7128223180770874\n",
      "Epoch 5885: Training Loss: 0.1674812783797582 Validation Loss: 0.7127339243888855\n",
      "Epoch 5886: Training Loss: 0.16595667600631714 Validation Loss: 0.7122988104820251\n",
      "Epoch 5887: Training Loss: 0.16750961542129517 Validation Loss: 0.7120171785354614\n",
      "Epoch 5888: Training Loss: 0.16610486805438995 Validation Loss: 0.7120750546455383\n",
      "Epoch 5889: Training Loss: 0.16690039137999216 Validation Loss: 0.7121361494064331\n",
      "Epoch 5890: Training Loss: 0.1660420298576355 Validation Loss: 0.7124041318893433\n",
      "Epoch 5891: Training Loss: 0.1661992222070694 Validation Loss: 0.7124853134155273\n",
      "Epoch 5892: Training Loss: 0.1664768805106481 Validation Loss: 0.7124332189559937\n",
      "Epoch 5893: Training Loss: 0.1661031891902288 Validation Loss: 0.7123714685440063\n",
      "Epoch 5894: Training Loss: 0.16596070925394693 Validation Loss: 0.7124414443969727\n",
      "Epoch 5895: Training Loss: 0.16685518622398376 Validation Loss: 0.712795078754425\n",
      "Epoch 5896: Training Loss: 0.16590109964211783 Validation Loss: 0.7124414443969727\n",
      "Epoch 5897: Training Loss: 0.16635172069072723 Validation Loss: 0.7120599150657654\n",
      "Epoch 5898: Training Loss: 0.16588289539019266 Validation Loss: 0.7121503949165344\n",
      "Epoch 5899: Training Loss: 0.16615788638591766 Validation Loss: 0.7120833396911621\n",
      "Epoch 5900: Training Loss: 0.16591525077819824 Validation Loss: 0.7122111320495605\n",
      "Epoch 5901: Training Loss: 0.1655639261007309 Validation Loss: 0.7122673392295837\n",
      "Epoch 5902: Training Loss: 0.1660845379034678 Validation Loss: 0.7121765613555908\n",
      "Epoch 5903: Training Loss: 0.16569355626900992 Validation Loss: 0.7124341726303101\n",
      "Epoch 5904: Training Loss: 0.16558961073557535 Validation Loss: 0.7126388549804688\n",
      "Epoch 5905: Training Loss: 0.1656816601753235 Validation Loss: 0.7125870585441589\n",
      "Epoch 5906: Training Loss: 0.16623572508494058 Validation Loss: 0.7125425338745117\n",
      "Epoch 5907: Training Loss: 0.1658960630496343 Validation Loss: 0.7125765085220337\n",
      "Epoch 5908: Training Loss: 0.16616875429948172 Validation Loss: 0.7122014760971069\n",
      "Epoch 5909: Training Loss: 0.16561313470204672 Validation Loss: 0.7121462225914001\n",
      "Epoch 5910: Training Loss: 0.16544549663861594 Validation Loss: 0.7122134566307068\n",
      "Epoch 5911: Training Loss: 0.16570670902729034 Validation Loss: 0.7122578024864197\n",
      "Epoch 5912: Training Loss: 0.1654928425947825 Validation Loss: 0.7124035358428955\n",
      "Epoch 5913: Training Loss: 0.1656442036231359 Validation Loss: 0.7124505639076233\n",
      "Epoch 5914: Training Loss: 0.165200208624204 Validation Loss: 0.712346613407135\n",
      "Epoch 5915: Training Loss: 0.16542773942152658 Validation Loss: 0.7121913433074951\n",
      "Epoch 5916: Training Loss: 0.16542190313339233 Validation Loss: 0.7123674154281616\n",
      "Epoch 5917: Training Loss: 0.16529267529646555 Validation Loss: 0.7124121189117432\n",
      "Epoch 5918: Training Loss: 0.16510849197705588 Validation Loss: 0.7123987078666687\n",
      "Epoch 5919: Training Loss: 0.16509387890497842 Validation Loss: 0.7125391364097595\n",
      "Epoch 5920: Training Loss: 0.16559375325838724 Validation Loss: 0.7123963832855225\n",
      "Epoch 5921: Training Loss: 0.1647115300099055 Validation Loss: 0.7123554944992065\n",
      "Epoch 5922: Training Loss: 0.1648230403661728 Validation Loss: 0.7127293348312378\n",
      "Epoch 5923: Training Loss: 0.1652695337931315 Validation Loss: 0.7125484347343445\n",
      "Epoch 5924: Training Loss: 0.16545771062374115 Validation Loss: 0.7125541567802429\n",
      "Epoch 5925: Training Loss: 0.16544606784979501 Validation Loss: 0.7124478816986084\n",
      "Epoch 5926: Training Loss: 0.164894238114357 Validation Loss: 0.7125008702278137\n",
      "Epoch 5927: Training Loss: 0.16499988734722137 Validation Loss: 0.71262127161026\n",
      "Epoch 5928: Training Loss: 0.16496572395165762 Validation Loss: 0.7126793265342712\n",
      "Epoch 5929: Training Loss: 0.16511115928490958 Validation Loss: 0.7128399014472961\n",
      "Epoch 5930: Training Loss: 0.16484151283899942 Validation Loss: 0.712536096572876\n",
      "Epoch 5931: Training Loss: 0.1648427148660024 Validation Loss: 0.7123078107833862\n",
      "Epoch 5932: Training Loss: 0.16506429016590118 Validation Loss: 0.7122917771339417\n",
      "Epoch 5933: Training Loss: 0.16435568034648895 Validation Loss: 0.7123158574104309\n",
      "Epoch 5934: Training Loss: 0.16443713009357452 Validation Loss: 0.7120935916900635\n",
      "Epoch 5935: Training Loss: 0.1654473841190338 Validation Loss: 0.711991548538208\n",
      "Epoch 5936: Training Loss: 0.16502506534258524 Validation Loss: 0.7122820019721985\n",
      "Epoch 5937: Training Loss: 0.16491963466008505 Validation Loss: 0.7122347950935364\n",
      "Epoch 5938: Training Loss: 0.16530673702557883 Validation Loss: 0.7120704054832458\n",
      "Epoch 5939: Training Loss: 0.164791410168012 Validation Loss: 0.7121137380599976\n",
      "Epoch 5940: Training Loss: 0.16475569705168405 Validation Loss: 0.71211838722229\n",
      "Epoch 5941: Training Loss: 0.1654329945643743 Validation Loss: 0.7122862339019775\n",
      "Epoch 5942: Training Loss: 0.16429502268632254 Validation Loss: 0.7121641635894775\n",
      "Epoch 5943: Training Loss: 0.16569746534029642 Validation Loss: 0.7119947671890259\n",
      "Epoch 5944: Training Loss: 0.16447566946347555 Validation Loss: 0.7122059464454651\n",
      "Epoch 5945: Training Loss: 0.1648183912038803 Validation Loss: 0.7121586203575134\n",
      "Epoch 5946: Training Loss: 0.16543288032213846 Validation Loss: 0.7122648358345032\n",
      "Epoch 5947: Training Loss: 0.16431788603464761 Validation Loss: 0.7124329209327698\n",
      "Epoch 5948: Training Loss: 0.1655049870411555 Validation Loss: 0.7125449776649475\n",
      "Epoch 5949: Training Loss: 0.1643043508132299 Validation Loss: 0.7128261923789978\n",
      "Epoch 5950: Training Loss: 0.1647005726893743 Validation Loss: 0.7128521203994751\n",
      "Epoch 5951: Training Loss: 0.16433254877726236 Validation Loss: 0.7131603360176086\n",
      "Epoch 5952: Training Loss: 0.16438543299833933 Validation Loss: 0.7128890752792358\n",
      "Epoch 5953: Training Loss: 0.16420539220174155 Validation Loss: 0.7126256823539734\n",
      "Epoch 5954: Training Loss: 0.16395401457945505 Validation Loss: 0.7123879790306091\n",
      "Epoch 5955: Training Loss: 0.16462690631548563 Validation Loss: 0.7123327255249023\n",
      "Epoch 5956: Training Loss: 0.1641161193450292 Validation Loss: 0.7124009132385254\n",
      "Epoch 5957: Training Loss: 0.1640485922495524 Validation Loss: 0.7124913930892944\n",
      "Epoch 5958: Training Loss: 0.16413799424966177 Validation Loss: 0.7125138640403748\n",
      "Epoch 5959: Training Loss: 0.16408255199591318 Validation Loss: 0.712602436542511\n",
      "Epoch 5960: Training Loss: 0.16454691191514334 Validation Loss: 0.7125781178474426\n",
      "Epoch 5961: Training Loss: 0.16398530701796213 Validation Loss: 0.7121742367744446\n",
      "Epoch 5962: Training Loss: 0.16423712174097696 Validation Loss: 0.7119317650794983\n",
      "Epoch 5963: Training Loss: 0.16387239595254263 Validation Loss: 0.7121707201004028\n",
      "Epoch 5964: Training Loss: 0.16458461185296377 Validation Loss: 0.7124192714691162\n",
      "Epoch 5965: Training Loss: 0.16404749949773154 Validation Loss: 0.7122913002967834\n",
      "Epoch 5966: Training Loss: 0.16398835182189941 Validation Loss: 0.7122437357902527\n",
      "Epoch 5967: Training Loss: 0.16391324003537497 Validation Loss: 0.7121601104736328\n",
      "Epoch 5968: Training Loss: 0.16368423899014792 Validation Loss: 0.7125040888786316\n",
      "Epoch 5969: Training Loss: 0.16367390751838684 Validation Loss: 0.7127061486244202\n",
      "Epoch 5970: Training Loss: 0.1640309343735377 Validation Loss: 0.7127583622932434\n",
      "Epoch 5971: Training Loss: 0.16426973541577658 Validation Loss: 0.7127704620361328\n",
      "Epoch 5972: Training Loss: 0.16441291570663452 Validation Loss: 0.7124755382537842\n",
      "Epoch 5973: Training Loss: 0.16458534200986227 Validation Loss: 0.7124291658401489\n",
      "Epoch 5974: Training Loss: 0.1636293133099874 Validation Loss: 0.7121888399124146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5975: Training Loss: 0.1639775981505712 Validation Loss: 0.7121133208274841\n",
      "Epoch 5976: Training Loss: 0.16447182993094125 Validation Loss: 0.7126069664955139\n",
      "Epoch 5977: Training Loss: 0.16329399744669595 Validation Loss: 0.7127108573913574\n",
      "Epoch 5978: Training Loss: 0.16353639960289001 Validation Loss: 0.7128274440765381\n",
      "Epoch 5979: Training Loss: 0.16366014877955118 Validation Loss: 0.7128093838691711\n",
      "Epoch 5980: Training Loss: 0.16390005747477213 Validation Loss: 0.7129996418952942\n",
      "Epoch 5981: Training Loss: 0.1631083538134893 Validation Loss: 0.7127357125282288\n",
      "Epoch 5982: Training Loss: 0.16351171831289926 Validation Loss: 0.712524950504303\n",
      "Epoch 5983: Training Loss: 0.16345534225304922 Validation Loss: 0.7125629186630249\n",
      "Epoch 5984: Training Loss: 0.1634932110706965 Validation Loss: 0.7125145196914673\n",
      "Epoch 5985: Training Loss: 0.16344578564167023 Validation Loss: 0.7123709321022034\n",
      "Epoch 5986: Training Loss: 0.16400047143300375 Validation Loss: 0.7127841114997864\n",
      "Epoch 5987: Training Loss: 0.16348478694756827 Validation Loss: 0.7126907706260681\n",
      "Epoch 5988: Training Loss: 0.16301337877909342 Validation Loss: 0.7126787900924683\n",
      "Epoch 5989: Training Loss: 0.1632222682237625 Validation Loss: 0.7125361561775208\n",
      "Epoch 5990: Training Loss: 0.1634016583363215 Validation Loss: 0.712297797203064\n",
      "Epoch 5991: Training Loss: 0.16393210490544638 Validation Loss: 0.712339460849762\n",
      "Epoch 5992: Training Loss: 0.16337421039740244 Validation Loss: 0.7123372554779053\n",
      "Epoch 5993: Training Loss: 0.16330918669700623 Validation Loss: 0.7125887870788574\n",
      "Epoch 5994: Training Loss: 0.1631475289662679 Validation Loss: 0.7127430438995361\n",
      "Epoch 5995: Training Loss: 0.16307364404201508 Validation Loss: 0.7127296924591064\n",
      "Epoch 5996: Training Loss: 0.16350281735261282 Validation Loss: 0.7125517725944519\n",
      "Epoch 5997: Training Loss: 0.16288904349009195 Validation Loss: 0.7125434279441833\n",
      "Epoch 5998: Training Loss: 0.16311535239219666 Validation Loss: 0.712438702583313\n",
      "Epoch 5999: Training Loss: 0.162568469842275 Validation Loss: 0.7121703028678894\n",
      "Epoch 6000: Training Loss: 0.1630840798219045 Validation Loss: 0.7122719287872314\n",
      "Epoch 6001: Training Loss: 0.16290645798047385 Validation Loss: 0.712425708770752\n",
      "Epoch 6002: Training Loss: 0.16255302727222443 Validation Loss: 0.7124805450439453\n",
      "Epoch 6003: Training Loss: 0.16334550082683563 Validation Loss: 0.7126049399375916\n",
      "Epoch 6004: Training Loss: 0.16306171814600626 Validation Loss: 0.7125881314277649\n",
      "Epoch 6005: Training Loss: 0.1627414027849833 Validation Loss: 0.712573766708374\n",
      "Epoch 6006: Training Loss: 0.1633970687786738 Validation Loss: 0.712429404258728\n",
      "Epoch 6007: Training Loss: 0.1626912256081899 Validation Loss: 0.71239173412323\n",
      "Epoch 6008: Training Loss: 0.16329451402028403 Validation Loss: 0.7121978402137756\n",
      "Epoch 6009: Training Loss: 0.16298053661982217 Validation Loss: 0.7123953104019165\n",
      "Epoch 6010: Training Loss: 0.16217751304308572 Validation Loss: 0.7128824591636658\n",
      "Epoch 6011: Training Loss: 0.16321364541848501 Validation Loss: 0.7129720449447632\n",
      "Epoch 6012: Training Loss: 0.16236857076485953 Validation Loss: 0.7130367159843445\n",
      "Epoch 6013: Training Loss: 0.16240732371807098 Validation Loss: 0.7128086686134338\n",
      "Epoch 6014: Training Loss: 0.164461816350619 Validation Loss: 0.7123693823814392\n",
      "Epoch 6015: Training Loss: 0.16241261859734854 Validation Loss: 0.7123765349388123\n",
      "Epoch 6016: Training Loss: 0.162581334511439 Validation Loss: 0.7124225497245789\n",
      "Epoch 6017: Training Loss: 0.1624667445818583 Validation Loss: 0.7124055624008179\n",
      "Epoch 6018: Training Loss: 0.1623842567205429 Validation Loss: 0.71245276927948\n",
      "Epoch 6019: Training Loss: 0.1625547856092453 Validation Loss: 0.7123689651489258\n",
      "Epoch 6020: Training Loss: 0.16234205663204193 Validation Loss: 0.7126179337501526\n",
      "Epoch 6021: Training Loss: 0.16255525251229605 Validation Loss: 0.7127310037612915\n",
      "Epoch 6022: Training Loss: 0.1624534328778585 Validation Loss: 0.7124310731887817\n",
      "Epoch 6023: Training Loss: 0.16315357883771262 Validation Loss: 0.7126359939575195\n",
      "Epoch 6024: Training Loss: 0.16230601569016775 Validation Loss: 0.7128031253814697\n",
      "Epoch 6025: Training Loss: 0.16232632597287497 Validation Loss: 0.7128230929374695\n",
      "Epoch 6026: Training Loss: 0.16222872336705527 Validation Loss: 0.712637186050415\n",
      "Epoch 6027: Training Loss: 0.16208852330843607 Validation Loss: 0.7127307057380676\n",
      "Epoch 6028: Training Loss: 0.16211815178394318 Validation Loss: 0.7130210995674133\n",
      "Epoch 6029: Training Loss: 0.16239302357037863 Validation Loss: 0.7130259871482849\n",
      "Epoch 6030: Training Loss: 0.16224561631679535 Validation Loss: 0.7124394774436951\n",
      "Epoch 6031: Training Loss: 0.1619649330774943 Validation Loss: 0.7123352885246277\n",
      "Epoch 6032: Training Loss: 0.16140852868556976 Validation Loss: 0.7125622034072876\n",
      "Epoch 6033: Training Loss: 0.16192168494065604 Validation Loss: 0.7125144600868225\n",
      "Epoch 6034: Training Loss: 0.1630458782116572 Validation Loss: 0.7127237319946289\n",
      "Epoch 6035: Training Loss: 0.1616396258274714 Validation Loss: 0.7127478718757629\n",
      "Epoch 6036: Training Loss: 0.16195575892925262 Validation Loss: 0.7129039168357849\n",
      "Epoch 6037: Training Loss: 0.16166836520036063 Validation Loss: 0.713103711605072\n",
      "Epoch 6038: Training Loss: 0.16188219686349234 Validation Loss: 0.7129749059677124\n",
      "Epoch 6039: Training Loss: 0.16180848081906637 Validation Loss: 0.712660014629364\n",
      "Epoch 6040: Training Loss: 0.16223224997520447 Validation Loss: 0.712213933467865\n",
      "Epoch 6041: Training Loss: 0.16190492113431296 Validation Loss: 0.7119693160057068\n",
      "Epoch 6042: Training Loss: 0.16165554026762644 Validation Loss: 0.7119870185852051\n",
      "Epoch 6043: Training Loss: 0.1616493413845698 Validation Loss: 0.7124666571617126\n",
      "Epoch 6044: Training Loss: 0.16200129191080728 Validation Loss: 0.7123892307281494\n",
      "Epoch 6045: Training Loss: 0.16171795129776 Validation Loss: 0.7124273777008057\n",
      "Epoch 6046: Training Loss: 0.1618625819683075 Validation Loss: 0.7129833698272705\n",
      "Epoch 6047: Training Loss: 0.16153478622436523 Validation Loss: 0.7129903435707092\n",
      "Epoch 6048: Training Loss: 0.1623296191294988 Validation Loss: 0.712693989276886\n",
      "Epoch 6049: Training Loss: 0.16178472836812338 Validation Loss: 0.7125853300094604\n",
      "Epoch 6050: Training Loss: 0.16152548293272653 Validation Loss: 0.7124893665313721\n",
      "Epoch 6051: Training Loss: 0.16156719624996185 Validation Loss: 0.7122684717178345\n",
      "Epoch 6052: Training Loss: 0.16136635839939117 Validation Loss: 0.7121074199676514\n",
      "Epoch 6053: Training Loss: 0.16131131847699484 Validation Loss: 0.7121401429176331\n",
      "Epoch 6054: Training Loss: 0.16154280801614126 Validation Loss: 0.7122677564620972\n",
      "Epoch 6055: Training Loss: 0.16096805036067963 Validation Loss: 0.7124813795089722\n",
      "Epoch 6056: Training Loss: 0.16148296495278677 Validation Loss: 0.7123892307281494\n",
      "Epoch 6057: Training Loss: 0.16194047033786774 Validation Loss: 0.7125008702278137\n",
      "Epoch 6058: Training Loss: 0.1611699958642324 Validation Loss: 0.7127072811126709\n",
      "Epoch 6059: Training Loss: 0.16131762166817984 Validation Loss: 0.7128385305404663\n",
      "Epoch 6060: Training Loss: 0.16137678921222687 Validation Loss: 0.7128963470458984\n",
      "Epoch 6061: Training Loss: 0.16125093897183737 Validation Loss: 0.7126500010490417\n",
      "Epoch 6062: Training Loss: 0.1614310642083486 Validation Loss: 0.7128620743751526\n",
      "Epoch 6063: Training Loss: 0.16138542195161185 Validation Loss: 0.7128258943557739\n",
      "Epoch 6064: Training Loss: 0.16117340823014578 Validation Loss: 0.7125154137611389\n",
      "Epoch 6065: Training Loss: 0.16151881714661917 Validation Loss: 0.7123148441314697\n",
      "Epoch 6066: Training Loss: 0.16149336596330008 Validation Loss: 0.7123655080795288\n",
      "Epoch 6067: Training Loss: 0.1623720700542132 Validation Loss: 0.712949275970459\n",
      "Epoch 6068: Training Loss: 0.16131768623987833 Validation Loss: 0.712878942489624\n",
      "Epoch 6069: Training Loss: 0.16096606850624084 Validation Loss: 0.7125535011291504\n",
      "Epoch 6070: Training Loss: 0.16148953636487326 Validation Loss: 0.7125710248947144\n",
      "Epoch 6071: Training Loss: 0.16106506188710532 Validation Loss: 0.7125421166419983\n",
      "Epoch 6072: Training Loss: 0.1610961010058721 Validation Loss: 0.712628185749054\n",
      "Epoch 6073: Training Loss: 0.1613278736670812 Validation Loss: 0.712597131729126\n",
      "Epoch 6074: Training Loss: 0.16100895901521048 Validation Loss: 0.7127267718315125\n",
      "Epoch 6075: Training Loss: 0.16053064167499542 Validation Loss: 0.712921679019928\n",
      "Epoch 6076: Training Loss: 0.16146029035250345 Validation Loss: 0.7128781080245972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6077: Training Loss: 0.16098767022291818 Validation Loss: 0.7131368517875671\n",
      "Epoch 6078: Training Loss: 0.1609812080860138 Validation Loss: 0.7129007577896118\n",
      "Epoch 6079: Training Loss: 0.1613173484802246 Validation Loss: 0.7125932574272156\n",
      "Epoch 6080: Training Loss: 0.1608746349811554 Validation Loss: 0.7126479744911194\n",
      "Epoch 6081: Training Loss: 0.1604993095000585 Validation Loss: 0.712345540523529\n",
      "Epoch 6082: Training Loss: 0.1606137901544571 Validation Loss: 0.712282657623291\n",
      "Epoch 6083: Training Loss: 0.16128906110922495 Validation Loss: 0.7124805450439453\n",
      "Epoch 6084: Training Loss: 0.16130198538303375 Validation Loss: 0.7125604748725891\n",
      "Epoch 6085: Training Loss: 0.16046914954980215 Validation Loss: 0.7129123210906982\n",
      "Epoch 6086: Training Loss: 0.160384530822436 Validation Loss: 0.7130298018455505\n",
      "Epoch 6087: Training Loss: 0.16034180919329324 Validation Loss: 0.7130172252655029\n",
      "Epoch 6088: Training Loss: 0.16086076696713766 Validation Loss: 0.7130833864212036\n",
      "Epoch 6089: Training Loss: 0.16086287796497345 Validation Loss: 0.7129782438278198\n",
      "Epoch 6090: Training Loss: 0.16019409894943237 Validation Loss: 0.7128276228904724\n",
      "Epoch 6091: Training Loss: 0.16022279361883798 Validation Loss: 0.7125639319419861\n",
      "Epoch 6092: Training Loss: 0.16015227635701498 Validation Loss: 0.7126786708831787\n",
      "Epoch 6093: Training Loss: 0.16036167740821838 Validation Loss: 0.7125554084777832\n",
      "Epoch 6094: Training Loss: 0.15993447105089822 Validation Loss: 0.712627112865448\n",
      "Epoch 6095: Training Loss: 0.1604426254828771 Validation Loss: 0.7130568027496338\n",
      "Epoch 6096: Training Loss: 0.1606878141562144 Validation Loss: 0.7130706310272217\n",
      "Epoch 6097: Training Loss: 0.16009495655695596 Validation Loss: 0.7131737470626831\n",
      "Epoch 6098: Training Loss: 0.16013438999652863 Validation Loss: 0.7128663659095764\n",
      "Epoch 6099: Training Loss: 0.16041879852612814 Validation Loss: 0.7125951647758484\n",
      "Epoch 6100: Training Loss: 0.1604363073905309 Validation Loss: 0.7122949361801147\n",
      "Epoch 6101: Training Loss: 0.16068031887213388 Validation Loss: 0.712627649307251\n",
      "Epoch 6102: Training Loss: 0.16057165960470834 Validation Loss: 0.7127656936645508\n",
      "Epoch 6103: Training Loss: 0.16003488500912985 Validation Loss: 0.71256422996521\n",
      "Epoch 6104: Training Loss: 0.16010956466197968 Validation Loss: 0.7127764225006104\n",
      "Epoch 6105: Training Loss: 0.1598947842915853 Validation Loss: 0.7127013802528381\n",
      "Epoch 6106: Training Loss: 0.16005079944928488 Validation Loss: 0.7127293348312378\n",
      "Epoch 6107: Training Loss: 0.16096680363019308 Validation Loss: 0.7128560543060303\n",
      "Epoch 6108: Training Loss: 0.16005970537662506 Validation Loss: 0.7130369544029236\n",
      "Epoch 6109: Training Loss: 0.15994685888290405 Validation Loss: 0.7128638029098511\n",
      "Epoch 6110: Training Loss: 0.15978715817133585 Validation Loss: 0.7127774953842163\n",
      "Epoch 6111: Training Loss: 0.16005262732505798 Validation Loss: 0.712641179561615\n",
      "Epoch 6112: Training Loss: 0.15946313738822937 Validation Loss: 0.7127167582511902\n",
      "Epoch 6113: Training Loss: 0.1608247607946396 Validation Loss: 0.7129112482070923\n",
      "Epoch 6114: Training Loss: 0.15976295868555704 Validation Loss: 0.7128705382347107\n",
      "Epoch 6115: Training Loss: 0.15995573997497559 Validation Loss: 0.7128327488899231\n",
      "Epoch 6116: Training Loss: 0.15967446565628052 Validation Loss: 0.7126451134681702\n",
      "Epoch 6117: Training Loss: 0.1600728233655294 Validation Loss: 0.712810218334198\n",
      "Epoch 6118: Training Loss: 0.1594868799050649 Validation Loss: 0.712969958782196\n",
      "Epoch 6119: Training Loss: 0.159670760234197 Validation Loss: 0.7129999995231628\n",
      "Epoch 6120: Training Loss: 0.1588470290104548 Validation Loss: 0.7130618095397949\n",
      "Epoch 6121: Training Loss: 0.15948856870333353 Validation Loss: 0.7131743431091309\n",
      "Epoch 6122: Training Loss: 0.15969277918338776 Validation Loss: 0.713093638420105\n",
      "Epoch 6123: Training Loss: 0.15945144991079965 Validation Loss: 0.7133182883262634\n",
      "Epoch 6124: Training Loss: 0.15968773265679678 Validation Loss: 0.7129290103912354\n",
      "Epoch 6125: Training Loss: 0.15979457894961038 Validation Loss: 0.7129654288291931\n",
      "Epoch 6126: Training Loss: 0.15956388413906097 Validation Loss: 0.7128559947013855\n",
      "Epoch 6127: Training Loss: 0.1597585380077362 Validation Loss: 0.7129568457603455\n",
      "Epoch 6128: Training Loss: 0.15997539460659027 Validation Loss: 0.7133365273475647\n",
      "Epoch 6129: Training Loss: 0.15943669279416403 Validation Loss: 0.7131185531616211\n",
      "Epoch 6130: Training Loss: 0.16152274111906686 Validation Loss: 0.7129827737808228\n",
      "Epoch 6131: Training Loss: 0.1599234143892924 Validation Loss: 0.7128289937973022\n",
      "Epoch 6132: Training Loss: 0.15997818112373352 Validation Loss: 0.7124770283699036\n",
      "Epoch 6133: Training Loss: 0.15965810418128967 Validation Loss: 0.7128124237060547\n",
      "Epoch 6134: Training Loss: 0.15946581959724426 Validation Loss: 0.7129293084144592\n",
      "Epoch 6135: Training Loss: 0.16161289314428964 Validation Loss: 0.7124766111373901\n",
      "Epoch 6136: Training Loss: 0.15998601913452148 Validation Loss: 0.7125271558761597\n",
      "Epoch 6137: Training Loss: 0.15941599011421204 Validation Loss: 0.7125309109687805\n",
      "Epoch 6138: Training Loss: 0.15912854174772897 Validation Loss: 0.7127259373664856\n",
      "Epoch 6139: Training Loss: 0.15927299857139587 Validation Loss: 0.7128345966339111\n",
      "Epoch 6140: Training Loss: 0.16015770534674326 Validation Loss: 0.7130985260009766\n",
      "Epoch 6141: Training Loss: 0.15996590505043665 Validation Loss: 0.7130208015441895\n",
      "Epoch 6142: Training Loss: 0.15909616649150848 Validation Loss: 0.7131378054618835\n",
      "Epoch 6143: Training Loss: 0.15950491031010947 Validation Loss: 0.7129728198051453\n",
      "Epoch 6144: Training Loss: 0.16018302738666534 Validation Loss: 0.7126055359840393\n",
      "Epoch 6145: Training Loss: 0.1594690332810084 Validation Loss: 0.7125290036201477\n",
      "Epoch 6146: Training Loss: 0.15910226106643677 Validation Loss: 0.7128278017044067\n",
      "Epoch 6147: Training Loss: 0.15895911554495493 Validation Loss: 0.7130141854286194\n",
      "Epoch 6148: Training Loss: 0.15884331862131754 Validation Loss: 0.7132222056388855\n",
      "Epoch 6149: Training Loss: 0.15910208225250244 Validation Loss: 0.7130159139633179\n",
      "Epoch 6150: Training Loss: 0.1583178589741389 Validation Loss: 0.7130383849143982\n",
      "Epoch 6151: Training Loss: 0.15896319349606833 Validation Loss: 0.7133483290672302\n",
      "Epoch 6152: Training Loss: 0.15874043107032776 Validation Loss: 0.7131859064102173\n",
      "Epoch 6153: Training Loss: 0.15961413582166037 Validation Loss: 0.713268518447876\n",
      "Epoch 6154: Training Loss: 0.15949944158395132 Validation Loss: 0.713081419467926\n",
      "Epoch 6155: Training Loss: 0.15883215765158334 Validation Loss: 0.7130470275878906\n",
      "Epoch 6156: Training Loss: 0.15933851897716522 Validation Loss: 0.7130257487297058\n",
      "Epoch 6157: Training Loss: 0.15804424385229746 Validation Loss: 0.7129883766174316\n",
      "Epoch 6158: Training Loss: 0.15980797509352365 Validation Loss: 0.712975263595581\n",
      "Epoch 6159: Training Loss: 0.15837935109933218 Validation Loss: 0.7129876017570496\n",
      "Epoch 6160: Training Loss: 0.1585033635298411 Validation Loss: 0.7131009697914124\n",
      "Epoch 6161: Training Loss: 0.1589526186386744 Validation Loss: 0.7128053903579712\n",
      "Epoch 6162: Training Loss: 0.15895076592763266 Validation Loss: 0.7127560973167419\n",
      "Epoch 6163: Training Loss: 0.1597287654876709 Validation Loss: 0.7128985524177551\n",
      "Epoch 6164: Training Loss: 0.15842912594477335 Validation Loss: 0.713023841381073\n",
      "Epoch 6165: Training Loss: 0.15886117766300836 Validation Loss: 0.7130826115608215\n",
      "Epoch 6166: Training Loss: 0.1588525598247846 Validation Loss: 0.7129921317100525\n",
      "Epoch 6167: Training Loss: 0.15833637615044913 Validation Loss: 0.7129619717597961\n",
      "Epoch 6168: Training Loss: 0.15860975782076517 Validation Loss: 0.7127389311790466\n",
      "Epoch 6169: Training Loss: 0.15822761754194895 Validation Loss: 0.7127991318702698\n",
      "Epoch 6170: Training Loss: 0.15846451620260874 Validation Loss: 0.7130090594291687\n",
      "Epoch 6171: Training Loss: 0.15799051523208618 Validation Loss: 0.713015615940094\n",
      "Epoch 6172: Training Loss: 0.1589916075269381 Validation Loss: 0.7130745053291321\n",
      "Epoch 6173: Training Loss: 0.1586244503657023 Validation Loss: 0.7131336331367493\n",
      "Epoch 6174: Training Loss: 0.15864060819149017 Validation Loss: 0.7134284973144531\n",
      "Epoch 6175: Training Loss: 0.15814108153184256 Validation Loss: 0.7134395837783813\n",
      "Epoch 6176: Training Loss: 0.15793420374393463 Validation Loss: 0.7135405540466309\n",
      "Epoch 6177: Training Loss: 0.15791939198970795 Validation Loss: 0.7133582830429077\n",
      "Epoch 6178: Training Loss: 0.1583904673655828 Validation Loss: 0.7131971716880798\n",
      "Epoch 6179: Training Loss: 0.15789898236592612 Validation Loss: 0.7131546139717102\n",
      "Epoch 6180: Training Loss: 0.15810038646062216 Validation Loss: 0.7131919264793396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6181: Training Loss: 0.15799642105897269 Validation Loss: 0.7132003307342529\n",
      "Epoch 6182: Training Loss: 0.15841630597909293 Validation Loss: 0.7131051421165466\n",
      "Epoch 6183: Training Loss: 0.15817136565844217 Validation Loss: 0.7131169438362122\n",
      "Epoch 6184: Training Loss: 0.15872673193613687 Validation Loss: 0.7129217982292175\n",
      "Epoch 6185: Training Loss: 0.1581322799126307 Validation Loss: 0.712988018989563\n",
      "Epoch 6186: Training Loss: 0.15797643860181174 Validation Loss: 0.7131388783454895\n",
      "Epoch 6187: Training Loss: 0.15829850236574808 Validation Loss: 0.7130158543586731\n",
      "Epoch 6188: Training Loss: 0.15806202590465546 Validation Loss: 0.7132967710494995\n",
      "Epoch 6189: Training Loss: 0.15799475212891897 Validation Loss: 0.7133212685585022\n",
      "Epoch 6190: Training Loss: 0.15895999471346536 Validation Loss: 0.7133475542068481\n",
      "Epoch 6191: Training Loss: 0.1578338493903478 Validation Loss: 0.7132390141487122\n",
      "Epoch 6192: Training Loss: 0.1582647462685903 Validation Loss: 0.7131938338279724\n",
      "Epoch 6193: Training Loss: 0.15797429780165353 Validation Loss: 0.712834358215332\n",
      "Epoch 6194: Training Loss: 0.15783527990182242 Validation Loss: 0.7125660181045532\n",
      "Epoch 6195: Training Loss: 0.15790811677773794 Validation Loss: 0.7125592827796936\n",
      "Epoch 6196: Training Loss: 0.15764747063318887 Validation Loss: 0.7131550312042236\n",
      "Epoch 6197: Training Loss: 0.15784100691477457 Validation Loss: 0.7135447859764099\n",
      "Epoch 6198: Training Loss: 0.15876021484533945 Validation Loss: 0.7138145565986633\n",
      "Epoch 6199: Training Loss: 0.15786095956961313 Validation Loss: 0.7141250371932983\n",
      "Epoch 6200: Training Loss: 0.1575005998214086 Validation Loss: 0.7140324115753174\n",
      "Epoch 6201: Training Loss: 0.15765167772769928 Validation Loss: 0.713259220123291\n",
      "Epoch 6202: Training Loss: 0.15749585131804147 Validation Loss: 0.713165819644928\n",
      "Epoch 6203: Training Loss: 0.15823699533939362 Validation Loss: 0.713188886642456\n",
      "Epoch 6204: Training Loss: 0.15736297269662222 Validation Loss: 0.7129279375076294\n",
      "Epoch 6205: Training Loss: 0.15737034877141318 Validation Loss: 0.7130826711654663\n",
      "Epoch 6206: Training Loss: 0.15800867974758148 Validation Loss: 0.7130035161972046\n",
      "Epoch 6207: Training Loss: 0.15690062443415323 Validation Loss: 0.7130988836288452\n",
      "Epoch 6208: Training Loss: 0.15740015606085458 Validation Loss: 0.713283360004425\n",
      "Epoch 6209: Training Loss: 0.15726669132709503 Validation Loss: 0.7134908437728882\n",
      "Epoch 6210: Training Loss: 0.1578241934378942 Validation Loss: 0.7132460474967957\n",
      "Epoch 6211: Training Loss: 0.15700494746367136 Validation Loss: 0.7132042050361633\n",
      "Epoch 6212: Training Loss: 0.15722957253456116 Validation Loss: 0.7131907343864441\n",
      "Epoch 6213: Training Loss: 0.15771387020746866 Validation Loss: 0.7130244374275208\n",
      "Epoch 6214: Training Loss: 0.1573225806156794 Validation Loss: 0.7132502794265747\n",
      "Epoch 6215: Training Loss: 0.15704612930615744 Validation Loss: 0.713079035282135\n",
      "Epoch 6216: Training Loss: 0.15713036557038626 Validation Loss: 0.7132269144058228\n",
      "Epoch 6217: Training Loss: 0.15706026057402292 Validation Loss: 0.7133182287216187\n",
      "Epoch 6218: Training Loss: 0.156962921222051 Validation Loss: 0.713505208492279\n",
      "Epoch 6219: Training Loss: 0.15727482736110687 Validation Loss: 0.7135025858879089\n",
      "Epoch 6220: Training Loss: 0.1567794382572174 Validation Loss: 0.7133791446685791\n",
      "Epoch 6221: Training Loss: 0.15687230229377747 Validation Loss: 0.7132849097251892\n",
      "Epoch 6222: Training Loss: 0.15715650717417398 Validation Loss: 0.7132523655891418\n",
      "Epoch 6223: Training Loss: 0.15703595181306204 Validation Loss: 0.7134596705436707\n",
      "Epoch 6224: Training Loss: 0.15680374205112457 Validation Loss: 0.7136521339416504\n",
      "Epoch 6225: Training Loss: 0.1570175588130951 Validation Loss: 0.7134676575660706\n",
      "Epoch 6226: Training Loss: 0.1572350114583969 Validation Loss: 0.7133278250694275\n",
      "Epoch 6227: Training Loss: 0.15763893475135168 Validation Loss: 0.7135620713233948\n",
      "Epoch 6228: Training Loss: 0.157301664352417 Validation Loss: 0.7134474515914917\n",
      "Epoch 6229: Training Loss: 0.1567188153664271 Validation Loss: 0.7134436964988708\n",
      "Epoch 6230: Training Loss: 0.15686099231243134 Validation Loss: 0.713402509689331\n",
      "Epoch 6231: Training Loss: 0.15669666230678558 Validation Loss: 0.7132971882820129\n",
      "Epoch 6232: Training Loss: 0.156204491853714 Validation Loss: 0.7129287719726562\n",
      "Epoch 6233: Training Loss: 0.15706381698449454 Validation Loss: 0.7132632732391357\n",
      "Epoch 6234: Training Loss: 0.15735491613547006 Validation Loss: 0.7132042050361633\n",
      "Epoch 6235: Training Loss: 0.15656882027784982 Validation Loss: 0.7133060693740845\n",
      "Epoch 6236: Training Loss: 0.15628457069396973 Validation Loss: 0.7136071920394897\n",
      "Epoch 6237: Training Loss: 0.15698463718096414 Validation Loss: 0.7136924862861633\n",
      "Epoch 6238: Training Loss: 0.15706940988699594 Validation Loss: 0.713566780090332\n",
      "Epoch 6239: Training Loss: 0.15647001067797342 Validation Loss: 0.7136499881744385\n",
      "Epoch 6240: Training Loss: 0.1565824250380198 Validation Loss: 0.7138009667396545\n",
      "Epoch 6241: Training Loss: 0.15668333570162454 Validation Loss: 0.7138857841491699\n",
      "Epoch 6242: Training Loss: 0.15595552821954092 Validation Loss: 0.7138168811798096\n",
      "Epoch 6243: Training Loss: 0.1565458426872889 Validation Loss: 0.7134380340576172\n",
      "Epoch 6244: Training Loss: 0.1564845343430837 Validation Loss: 0.7133890986442566\n",
      "Epoch 6245: Training Loss: 0.1563155005375544 Validation Loss: 0.7132978439331055\n",
      "Epoch 6246: Training Loss: 0.15609326461950937 Validation Loss: 0.7131593227386475\n",
      "Epoch 6247: Training Loss: 0.15644111235936484 Validation Loss: 0.7129920125007629\n",
      "Epoch 6248: Training Loss: 0.15625259776910147 Validation Loss: 0.7129340171813965\n",
      "Epoch 6249: Training Loss: 0.15636354684829712 Validation Loss: 0.713171660900116\n",
      "Epoch 6250: Training Loss: 0.15616604685783386 Validation Loss: 0.7132488489151001\n",
      "Epoch 6251: Training Loss: 0.156111349662145 Validation Loss: 0.7132843732833862\n",
      "Epoch 6252: Training Loss: 0.1556674689054489 Validation Loss: 0.7135600447654724\n",
      "Epoch 6253: Training Loss: 0.15603591005007425 Validation Loss: 0.7137888073921204\n",
      "Epoch 6254: Training Loss: 0.15797792623440424 Validation Loss: 0.7134934663772583\n",
      "Epoch 6255: Training Loss: 0.15688060224056244 Validation Loss: 0.7134348154067993\n",
      "Epoch 6256: Training Loss: 0.15652225414911905 Validation Loss: 0.7135078310966492\n",
      "Epoch 6257: Training Loss: 0.15599665542443594 Validation Loss: 0.7136795520782471\n",
      "Epoch 6258: Training Loss: 0.1562425990899404 Validation Loss: 0.7136871814727783\n",
      "Epoch 6259: Training Loss: 0.15632816652456918 Validation Loss: 0.7135931849479675\n",
      "Epoch 6260: Training Loss: 0.15639890233675638 Validation Loss: 0.7136867046356201\n",
      "Epoch 6261: Training Loss: 0.15572471916675568 Validation Loss: 0.7136987447738647\n",
      "Epoch 6262: Training Loss: 0.15597190459569296 Validation Loss: 0.7135430574417114\n",
      "Epoch 6263: Training Loss: 0.15621207654476166 Validation Loss: 0.7137471437454224\n",
      "Epoch 6264: Training Loss: 0.15586984157562256 Validation Loss: 0.7136622667312622\n",
      "Epoch 6265: Training Loss: 0.1554711510737737 Validation Loss: 0.7136014103889465\n",
      "Epoch 6266: Training Loss: 0.1563300540049871 Validation Loss: 0.7133628129959106\n",
      "Epoch 6267: Training Loss: 0.15695754190286001 Validation Loss: 0.7133373618125916\n",
      "Epoch 6268: Training Loss: 0.15604078769683838 Validation Loss: 0.7134678959846497\n",
      "Epoch 6269: Training Loss: 0.15559818844000498 Validation Loss: 0.7133835554122925\n",
      "Epoch 6270: Training Loss: 0.15554497639338175 Validation Loss: 0.7132008671760559\n",
      "Epoch 6271: Training Loss: 0.15673215687274933 Validation Loss: 0.7134549617767334\n",
      "Epoch 6272: Training Loss: 0.1558294097582499 Validation Loss: 0.7135311961174011\n",
      "Epoch 6273: Training Loss: 0.15581498543421426 Validation Loss: 0.7134485244750977\n",
      "Epoch 6274: Training Loss: 0.15528562913338342 Validation Loss: 0.7137128114700317\n",
      "Epoch 6275: Training Loss: 0.1558839480082194 Validation Loss: 0.7135598659515381\n",
      "Epoch 6276: Training Loss: 0.15555287897586823 Validation Loss: 0.713517963886261\n",
      "Epoch 6277: Training Loss: 0.15500599642594656 Validation Loss: 0.7135651111602783\n",
      "Epoch 6278: Training Loss: 0.15540477633476257 Validation Loss: 0.7136117219924927\n",
      "Epoch 6279: Training Loss: 0.15565572679042816 Validation Loss: 0.7136721611022949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6280: Training Loss: 0.15529748797416687 Validation Loss: 0.7138800024986267\n",
      "Epoch 6281: Training Loss: 0.15539530913035074 Validation Loss: 0.7139893770217896\n",
      "Epoch 6282: Training Loss: 0.15579839050769806 Validation Loss: 0.7136484980583191\n",
      "Epoch 6283: Training Loss: 0.1553455044825872 Validation Loss: 0.71341472864151\n",
      "Epoch 6284: Training Loss: 0.15607894460360208 Validation Loss: 0.7132485508918762\n",
      "Epoch 6285: Training Loss: 0.1552779177824656 Validation Loss: 0.7135477662086487\n",
      "Epoch 6286: Training Loss: 0.15522168080012003 Validation Loss: 0.7133269906044006\n",
      "Epoch 6287: Training Loss: 0.15533343454202017 Validation Loss: 0.7132886648178101\n",
      "Epoch 6288: Training Loss: 0.15511495371659598 Validation Loss: 0.7134157419204712\n",
      "Epoch 6289: Training Loss: 0.15550881624221802 Validation Loss: 0.7134767174720764\n",
      "Epoch 6290: Training Loss: 0.15614858269691467 Validation Loss: 0.7133861780166626\n",
      "Epoch 6291: Training Loss: 0.15521892408529916 Validation Loss: 0.7137158513069153\n",
      "Epoch 6292: Training Loss: 0.15570122003555298 Validation Loss: 0.714222252368927\n",
      "Epoch 6293: Training Loss: 0.15547150373458862 Validation Loss: 0.7140340209007263\n",
      "Epoch 6294: Training Loss: 0.1550541172424952 Validation Loss: 0.7136942744255066\n",
      "Epoch 6295: Training Loss: 0.15543635189533234 Validation Loss: 0.7135178446769714\n",
      "Epoch 6296: Training Loss: 0.15644114464521408 Validation Loss: 0.7133277654647827\n",
      "Epoch 6297: Training Loss: 0.1550352027018865 Validation Loss: 0.7131490111351013\n",
      "Epoch 6298: Training Loss: 0.15426004429658255 Validation Loss: 0.713329553604126\n",
      "Epoch 6299: Training Loss: 0.1557219127813975 Validation Loss: 0.713765561580658\n",
      "Epoch 6300: Training Loss: 0.15528504053751627 Validation Loss: 0.7137508392333984\n",
      "Epoch 6301: Training Loss: 0.15491012732187906 Validation Loss: 0.713668942451477\n",
      "Epoch 6302: Training Loss: 0.1550657997528712 Validation Loss: 0.7134158611297607\n",
      "Epoch 6303: Training Loss: 0.15535223484039307 Validation Loss: 0.7139543890953064\n",
      "Epoch 6304: Training Loss: 0.15499220291773477 Validation Loss: 0.7140828967094421\n",
      "Epoch 6305: Training Loss: 0.15519299109776816 Validation Loss: 0.7137974500656128\n",
      "Epoch 6306: Training Loss: 0.15478218098481497 Validation Loss: 0.714213490486145\n",
      "Epoch 6307: Training Loss: 0.15544853111108145 Validation Loss: 0.7141815423965454\n",
      "Epoch 6308: Training Loss: 0.1547461450099945 Validation Loss: 0.7137793898582458\n",
      "Epoch 6309: Training Loss: 0.15495840708414713 Validation Loss: 0.7137215733528137\n",
      "Epoch 6310: Training Loss: 0.15478634337584177 Validation Loss: 0.7135522365570068\n",
      "Epoch 6311: Training Loss: 0.154902512828509 Validation Loss: 0.7137146592140198\n",
      "Epoch 6312: Training Loss: 0.15455661714076996 Validation Loss: 0.7138984799385071\n",
      "Epoch 6313: Training Loss: 0.1550799310207367 Validation Loss: 0.713854193687439\n",
      "Epoch 6314: Training Loss: 0.15467068056265512 Validation Loss: 0.7138928174972534\n",
      "Epoch 6315: Training Loss: 0.15447273353735605 Validation Loss: 0.7138127684593201\n",
      "Epoch 6316: Training Loss: 0.1545309474070867 Validation Loss: 0.7138340473175049\n",
      "Epoch 6317: Training Loss: 0.15502742926279703 Validation Loss: 0.7137550115585327\n",
      "Epoch 6318: Training Loss: 0.1547331909338633 Validation Loss: 0.7142215967178345\n",
      "Epoch 6319: Training Loss: 0.1548342059055964 Validation Loss: 0.7138580083847046\n",
      "Epoch 6320: Training Loss: 0.1552768275141716 Validation Loss: 0.7136717438697815\n",
      "Epoch 6321: Training Loss: 0.15434377392133078 Validation Loss: 0.7137195467948914\n",
      "Epoch 6322: Training Loss: 0.15472799042860666 Validation Loss: 0.7134363055229187\n",
      "Epoch 6323: Training Loss: 0.1545248180627823 Validation Loss: 0.7135071754455566\n",
      "Epoch 6324: Training Loss: 0.15423559149106345 Validation Loss: 0.713437020778656\n",
      "Epoch 6325: Training Loss: 0.1542285035053889 Validation Loss: 0.7134953737258911\n",
      "Epoch 6326: Training Loss: 0.15421264866987863 Validation Loss: 0.7136380672454834\n",
      "Epoch 6327: Training Loss: 0.15497931838035583 Validation Loss: 0.7135868072509766\n",
      "Epoch 6328: Training Loss: 0.1543145477771759 Validation Loss: 0.7137281894683838\n",
      "Epoch 6329: Training Loss: 0.1539745976527532 Validation Loss: 0.7138901352882385\n",
      "Epoch 6330: Training Loss: 0.15404454370339712 Validation Loss: 0.7138320803642273\n",
      "Epoch 6331: Training Loss: 0.1545624484618505 Validation Loss: 0.7136971354484558\n",
      "Epoch 6332: Training Loss: 0.15449325740337372 Validation Loss: 0.7135507464408875\n",
      "Epoch 6333: Training Loss: 0.15413624544938406 Validation Loss: 0.7135618925094604\n",
      "Epoch 6334: Training Loss: 0.15403450032075247 Validation Loss: 0.7137254476547241\n",
      "Epoch 6335: Training Loss: 0.15428954362869263 Validation Loss: 0.7140927314758301\n",
      "Epoch 6336: Training Loss: 0.1541030357281367 Validation Loss: 0.7141808867454529\n",
      "Epoch 6337: Training Loss: 0.15517655511697134 Validation Loss: 0.7139744758605957\n",
      "Epoch 6338: Training Loss: 0.15404009819030762 Validation Loss: 0.7144047617912292\n",
      "Epoch 6339: Training Loss: 0.15378178656101227 Validation Loss: 0.7140638828277588\n",
      "Epoch 6340: Training Loss: 0.15407517552375793 Validation Loss: 0.7140100598335266\n",
      "Epoch 6341: Training Loss: 0.15375395119190216 Validation Loss: 0.7138175368309021\n",
      "Epoch 6342: Training Loss: 0.154116024573644 Validation Loss: 0.713922381401062\n",
      "Epoch 6343: Training Loss: 0.15477545062700906 Validation Loss: 0.7141391634941101\n",
      "Epoch 6344: Training Loss: 0.15364804863929749 Validation Loss: 0.7142486572265625\n",
      "Epoch 6345: Training Loss: 0.15347352623939514 Validation Loss: 0.7138574123382568\n",
      "Epoch 6346: Training Loss: 0.15361768503983816 Validation Loss: 0.7135728597640991\n",
      "Epoch 6347: Training Loss: 0.1538911759853363 Validation Loss: 0.7135422229766846\n",
      "Epoch 6348: Training Loss: 0.15480372806390127 Validation Loss: 0.713763415813446\n",
      "Epoch 6349: Training Loss: 0.1538265993197759 Validation Loss: 0.7141473889350891\n",
      "Epoch 6350: Training Loss: 0.15384543438752493 Validation Loss: 0.7138807773590088\n",
      "Epoch 6351: Training Loss: 0.15391852458318075 Validation Loss: 0.713665246963501\n",
      "Epoch 6352: Training Loss: 0.15393263598283133 Validation Loss: 0.7137684226036072\n",
      "Epoch 6353: Training Loss: 0.1535316159327825 Validation Loss: 0.7139931321144104\n",
      "Epoch 6354: Training Loss: 0.15394441286722818 Validation Loss: 0.7140668034553528\n",
      "Epoch 6355: Training Loss: 0.15441995859146118 Validation Loss: 0.7141067385673523\n",
      "Epoch 6356: Training Loss: 0.1533715029557546 Validation Loss: 0.7140779495239258\n",
      "Epoch 6357: Training Loss: 0.15362727642059326 Validation Loss: 0.7141987085342407\n",
      "Epoch 6358: Training Loss: 0.15408264100551605 Validation Loss: 0.7141073942184448\n",
      "Epoch 6359: Training Loss: 0.1536072095235189 Validation Loss: 0.713550865650177\n",
      "Epoch 6360: Training Loss: 0.15390383700529733 Validation Loss: 0.7134226560592651\n",
      "Epoch 6361: Training Loss: 0.15329372882843018 Validation Loss: 0.7136365175247192\n",
      "Epoch 6362: Training Loss: 0.15415272613366446 Validation Loss: 0.7139506340026855\n",
      "Epoch 6363: Training Loss: 0.15454742312431335 Validation Loss: 0.7142153382301331\n",
      "Epoch 6364: Training Loss: 0.15328912436962128 Validation Loss: 0.7142308950424194\n",
      "Epoch 6365: Training Loss: 0.1536859174569448 Validation Loss: 0.714141309261322\n",
      "Epoch 6366: Training Loss: 0.15331428746382395 Validation Loss: 0.7141202092170715\n",
      "Epoch 6367: Training Loss: 0.153590127825737 Validation Loss: 0.7139882445335388\n",
      "Epoch 6368: Training Loss: 0.15351478258768717 Validation Loss: 0.7135507464408875\n",
      "Epoch 6369: Training Loss: 0.15327603618303934 Validation Loss: 0.7140117287635803\n",
      "Epoch 6370: Training Loss: 0.15267379581928253 Validation Loss: 0.7141686081886292\n",
      "Epoch 6371: Training Loss: 0.1535711189111074 Validation Loss: 0.7144542336463928\n",
      "Epoch 6372: Training Loss: 0.1530955582857132 Validation Loss: 0.7144619226455688\n",
      "Epoch 6373: Training Loss: 0.15367217361927032 Validation Loss: 0.7142763733863831\n",
      "Epoch 6374: Training Loss: 0.1532187138994535 Validation Loss: 0.7141242623329163\n",
      "Epoch 6375: Training Loss: 0.153132826089859 Validation Loss: 0.7142835259437561\n",
      "Epoch 6376: Training Loss: 0.1531661500533422 Validation Loss: 0.7139912843704224\n",
      "Epoch 6377: Training Loss: 0.15291506548722586 Validation Loss: 0.7139018774032593\n",
      "Epoch 6378: Training Loss: 0.15294436117013296 Validation Loss: 0.7137925624847412\n",
      "Epoch 6379: Training Loss: 0.15287588040033975 Validation Loss: 0.7138994932174683\n",
      "Epoch 6380: Training Loss: 0.15369791289170584 Validation Loss: 0.7141028046607971\n",
      "Epoch 6381: Training Loss: 0.153058722615242 Validation Loss: 0.7139407396316528\n",
      "Epoch 6382: Training Loss: 0.15383879840373993 Validation Loss: 0.7139296531677246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6383: Training Loss: 0.15242458879947662 Validation Loss: 0.7139963507652283\n",
      "Epoch 6384: Training Loss: 0.1533753921588262 Validation Loss: 0.7142063975334167\n",
      "Epoch 6385: Training Loss: 0.15286591152350107 Validation Loss: 0.7140514850616455\n",
      "Epoch 6386: Training Loss: 0.15270517766475677 Validation Loss: 0.7141360640525818\n",
      "Epoch 6387: Training Loss: 0.1530392716328303 Validation Loss: 0.7140551805496216\n",
      "Epoch 6388: Training Loss: 0.15275393923123678 Validation Loss: 0.7136371731758118\n",
      "Epoch 6389: Training Loss: 0.15265774230162302 Validation Loss: 0.7138997316360474\n",
      "Epoch 6390: Training Loss: 0.15267971654733023 Validation Loss: 0.7141663432121277\n",
      "Epoch 6391: Training Loss: 0.15296107033888498 Validation Loss: 0.7141157388687134\n",
      "Epoch 6392: Training Loss: 0.15324236949284872 Validation Loss: 0.7145251035690308\n",
      "Epoch 6393: Training Loss: 0.15270756681760153 Validation Loss: 0.714468777179718\n",
      "Epoch 6394: Training Loss: 0.15282770494620004 Validation Loss: 0.7136257886886597\n",
      "Epoch 6395: Training Loss: 0.15259779493014017 Validation Loss: 0.7134323120117188\n",
      "Epoch 6396: Training Loss: 0.15271143118540445 Validation Loss: 0.7131600975990295\n",
      "Epoch 6397: Training Loss: 0.1529129296541214 Validation Loss: 0.7135262489318848\n",
      "Epoch 6398: Training Loss: 0.15269633134206137 Validation Loss: 0.7135869264602661\n",
      "Epoch 6399: Training Loss: 0.1526023248831431 Validation Loss: 0.7138789892196655\n",
      "Epoch 6400: Training Loss: 0.15264969567457834 Validation Loss: 0.7144038081169128\n",
      "Epoch 6401: Training Loss: 0.15325424571832022 Validation Loss: 0.714307963848114\n",
      "Epoch 6402: Training Loss: 0.1525186449289322 Validation Loss: 0.7145763635635376\n",
      "Epoch 6403: Training Loss: 0.1531816820303599 Validation Loss: 0.7143406867980957\n",
      "Epoch 6404: Training Loss: 0.1533660093943278 Validation Loss: 0.7140057682991028\n",
      "Epoch 6405: Training Loss: 0.1523696780204773 Validation Loss: 0.7141771912574768\n",
      "Epoch 6406: Training Loss: 0.15264162421226501 Validation Loss: 0.7146415710449219\n",
      "Epoch 6407: Training Loss: 0.1522935082515081 Validation Loss: 0.714694082736969\n",
      "Epoch 6408: Training Loss: 0.1522344301144282 Validation Loss: 0.7144201397895813\n",
      "Epoch 6409: Training Loss: 0.15245757500330606 Validation Loss: 0.713970422744751\n",
      "Epoch 6410: Training Loss: 0.15225845078627268 Validation Loss: 0.7139521241188049\n",
      "Epoch 6411: Training Loss: 0.1527292231718699 Validation Loss: 0.7140777111053467\n",
      "Epoch 6412: Training Loss: 0.15223480761051178 Validation Loss: 0.7140469551086426\n",
      "Epoch 6413: Training Loss: 0.15255103508631387 Validation Loss: 0.7140424847602844\n",
      "Epoch 6414: Training Loss: 0.15262128909428915 Validation Loss: 0.7142037749290466\n",
      "Epoch 6415: Training Loss: 0.1522448013226191 Validation Loss: 0.7144344449043274\n",
      "Epoch 6416: Training Loss: 0.15244315067927042 Validation Loss: 0.7144103050231934\n",
      "Epoch 6417: Training Loss: 0.15221489469210306 Validation Loss: 0.7145004868507385\n",
      "Epoch 6418: Training Loss: 0.15291906396547952 Validation Loss: 0.7139495015144348\n",
      "Epoch 6419: Training Loss: 0.15271109839280447 Validation Loss: 0.7138685584068298\n",
      "Epoch 6420: Training Loss: 0.15280031661192575 Validation Loss: 0.7137844562530518\n",
      "Epoch 6421: Training Loss: 0.1517022947470347 Validation Loss: 0.7142162919044495\n",
      "Epoch 6422: Training Loss: 0.1528134991725286 Validation Loss: 0.7144141793251038\n",
      "Epoch 6423: Training Loss: 0.1521900991598765 Validation Loss: 0.7146141529083252\n",
      "Epoch 6424: Training Loss: 0.15196112294991812 Validation Loss: 0.7146010994911194\n",
      "Epoch 6425: Training Loss: 0.15241573750972748 Validation Loss: 0.7144995331764221\n",
      "Epoch 6426: Training Loss: 0.1521356850862503 Validation Loss: 0.7143412828445435\n",
      "Epoch 6427: Training Loss: 0.15257377922534943 Validation Loss: 0.7143757343292236\n",
      "Epoch 6428: Training Loss: 0.15215763449668884 Validation Loss: 0.7142624855041504\n",
      "Epoch 6429: Training Loss: 0.15248837570349374 Validation Loss: 0.7144051194190979\n",
      "Epoch 6430: Training Loss: 0.153381717701753 Validation Loss: 0.7142970561981201\n",
      "Epoch 6431: Training Loss: 0.15185234447320303 Validation Loss: 0.7143833041191101\n",
      "Epoch 6432: Training Loss: 0.15192008515199026 Validation Loss: 0.7141842246055603\n",
      "Epoch 6433: Training Loss: 0.15220321218172708 Validation Loss: 0.7140014171600342\n",
      "Epoch 6434: Training Loss: 0.15191219747066498 Validation Loss: 0.7142283916473389\n",
      "Epoch 6435: Training Loss: 0.1519568314154943 Validation Loss: 0.7142921090126038\n",
      "Epoch 6436: Training Loss: 0.1518919070561727 Validation Loss: 0.7142255902290344\n",
      "Epoch 6437: Training Loss: 0.1514013260602951 Validation Loss: 0.714410662651062\n",
      "Epoch 6438: Training Loss: 0.15177593131860098 Validation Loss: 0.7144224047660828\n",
      "Epoch 6439: Training Loss: 0.15173121790091196 Validation Loss: 0.7141242027282715\n",
      "Epoch 6440: Training Loss: 0.15174841384092966 Validation Loss: 0.7140164971351624\n",
      "Epoch 6441: Training Loss: 0.15120315551757812 Validation Loss: 0.7140815854072571\n",
      "Epoch 6442: Training Loss: 0.15165596206982931 Validation Loss: 0.714385449886322\n",
      "Epoch 6443: Training Loss: 0.15128426253795624 Validation Loss: 0.7143562436103821\n",
      "Epoch 6444: Training Loss: 0.15152209997177124 Validation Loss: 0.7142020463943481\n",
      "Epoch 6445: Training Loss: 0.1512395590543747 Validation Loss: 0.7145293354988098\n",
      "Epoch 6446: Training Loss: 0.153887910147508 Validation Loss: 0.7147835493087769\n",
      "Epoch 6447: Training Loss: 0.15153796474138895 Validation Loss: 0.7147583961486816\n",
      "Epoch 6448: Training Loss: 0.15122123559316 Validation Loss: 0.7145256996154785\n",
      "Epoch 6449: Training Loss: 0.15234339237213135 Validation Loss: 0.714418888092041\n",
      "Epoch 6450: Training Loss: 0.15233702957630157 Validation Loss: 0.7143198251724243\n",
      "Epoch 6451: Training Loss: 0.1513760636250178 Validation Loss: 0.7147576212882996\n",
      "Epoch 6452: Training Loss: 0.15141302843888602 Validation Loss: 0.714689314365387\n",
      "Epoch 6453: Training Loss: 0.15140342712402344 Validation Loss: 0.7142089009284973\n",
      "Epoch 6454: Training Loss: 0.15153380235036215 Validation Loss: 0.7138755917549133\n",
      "Epoch 6455: Training Loss: 0.15152608354886374 Validation Loss: 0.7142583727836609\n",
      "Epoch 6456: Training Loss: 0.1512676477432251 Validation Loss: 0.71448814868927\n",
      "Epoch 6457: Training Loss: 0.15135952830314636 Validation Loss: 0.7140811085700989\n",
      "Epoch 6458: Training Loss: 0.15163115411996841 Validation Loss: 0.7140908241271973\n",
      "Epoch 6459: Training Loss: 0.15125665565331778 Validation Loss: 0.7143692374229431\n",
      "Epoch 6460: Training Loss: 0.15119580427805582 Validation Loss: 0.7144886255264282\n",
      "Epoch 6461: Training Loss: 0.15059435864289603 Validation Loss: 0.7145927548408508\n",
      "Epoch 6462: Training Loss: 0.15141205986340842 Validation Loss: 0.7145401239395142\n",
      "Epoch 6463: Training Loss: 0.15058655043443045 Validation Loss: 0.7146949172019958\n",
      "Epoch 6464: Training Loss: 0.1513931800921758 Validation Loss: 0.7143798470497131\n",
      "Epoch 6465: Training Loss: 0.15097654859224954 Validation Loss: 0.714280366897583\n",
      "Epoch 6466: Training Loss: 0.15120730797449747 Validation Loss: 0.7140876650810242\n",
      "Epoch 6467: Training Loss: 0.1506190299987793 Validation Loss: 0.7147079110145569\n",
      "Epoch 6468: Training Loss: 0.15267236530780792 Validation Loss: 0.7146648168563843\n",
      "Epoch 6469: Training Loss: 0.15132338305314383 Validation Loss: 0.7145297527313232\n",
      "Epoch 6470: Training Loss: 0.15029556304216385 Validation Loss: 0.714333176612854\n",
      "Epoch 6471: Training Loss: 0.1511414796113968 Validation Loss: 0.7144511342048645\n",
      "Epoch 6472: Training Loss: 0.15089776118596396 Validation Loss: 0.7144577503204346\n",
      "Epoch 6473: Training Loss: 0.1508652021487554 Validation Loss: 0.7146661877632141\n",
      "Epoch 6474: Training Loss: 0.15109479427337646 Validation Loss: 0.714760959148407\n",
      "Epoch 6475: Training Loss: 0.1509783367315928 Validation Loss: 0.7148647904396057\n",
      "Epoch 6476: Training Loss: 0.150897150238355 Validation Loss: 0.7145547866821289\n",
      "Epoch 6477: Training Loss: 0.15156344075997671 Validation Loss: 0.7143301963806152\n",
      "Epoch 6478: Training Loss: 0.15202582875887552 Validation Loss: 0.714383602142334\n",
      "Epoch 6479: Training Loss: 0.15071466068426767 Validation Loss: 0.7142929434776306\n",
      "Epoch 6480: Training Loss: 0.15053662657737732 Validation Loss: 0.7142967581748962\n",
      "Epoch 6481: Training Loss: 0.15102537473042807 Validation Loss: 0.7146441340446472\n",
      "Epoch 6482: Training Loss: 0.15080794195334116 Validation Loss: 0.7145138382911682\n",
      "Epoch 6483: Training Loss: 0.15055466194947562 Validation Loss: 0.7146952152252197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6484: Training Loss: 0.1504414975643158 Validation Loss: 0.7147513628005981\n",
      "Epoch 6485: Training Loss: 0.1506382723649343 Validation Loss: 0.7148460745811462\n",
      "Epoch 6486: Training Loss: 0.1504665513833364 Validation Loss: 0.7146618366241455\n",
      "Epoch 6487: Training Loss: 0.15028080840905508 Validation Loss: 0.7145654559135437\n",
      "Epoch 6488: Training Loss: 0.15028666456540427 Validation Loss: 0.7147489786148071\n",
      "Epoch 6489: Training Loss: 0.15008516112963358 Validation Loss: 0.7148352265357971\n",
      "Epoch 6490: Training Loss: 0.15040257573127747 Validation Loss: 0.7145915627479553\n",
      "Epoch 6491: Training Loss: 0.15165840834379196 Validation Loss: 0.7143920660018921\n",
      "Epoch 6492: Training Loss: 0.15080887079238892 Validation Loss: 0.714528501033783\n",
      "Epoch 6493: Training Loss: 0.1506433586279551 Validation Loss: 0.7144364714622498\n",
      "Epoch 6494: Training Loss: 0.15052954852581024 Validation Loss: 0.7141237258911133\n",
      "Epoch 6495: Training Loss: 0.15039156874020895 Validation Loss: 0.7138816714286804\n",
      "Epoch 6496: Training Loss: 0.15103931725025177 Validation Loss: 0.714246392250061\n",
      "Epoch 6497: Training Loss: 0.14999126891295114 Validation Loss: 0.7145655155181885\n",
      "Epoch 6498: Training Loss: 0.15022411942481995 Validation Loss: 0.7147563695907593\n",
      "Epoch 6499: Training Loss: 0.15069646636644998 Validation Loss: 0.7145735025405884\n",
      "Epoch 6500: Training Loss: 0.15002001325289407 Validation Loss: 0.714495837688446\n",
      "Epoch 6501: Training Loss: 0.15134423971176147 Validation Loss: 0.714240550994873\n",
      "Epoch 6502: Training Loss: 0.15046614905198416 Validation Loss: 0.7146105170249939\n",
      "Epoch 6503: Training Loss: 0.149523397286733 Validation Loss: 0.7147915363311768\n",
      "Epoch 6504: Training Loss: 0.15185568233331045 Validation Loss: 0.7147589921951294\n",
      "Epoch 6505: Training Loss: 0.15002891421318054 Validation Loss: 0.714692234992981\n",
      "Epoch 6506: Training Loss: 0.1503971815109253 Validation Loss: 0.7147197127342224\n",
      "Epoch 6507: Training Loss: 0.15006573498249054 Validation Loss: 0.7146932482719421\n",
      "Epoch 6508: Training Loss: 0.14982015391190848 Validation Loss: 0.714960515499115\n",
      "Epoch 6509: Training Loss: 0.14998677372932434 Validation Loss: 0.7149598598480225\n",
      "Epoch 6510: Training Loss: 0.15011493861675262 Validation Loss: 0.7148913741111755\n",
      "Epoch 6511: Training Loss: 0.14965698619683585 Validation Loss: 0.7148078083992004\n",
      "Epoch 6512: Training Loss: 0.15113581965366998 Validation Loss: 0.7146627902984619\n",
      "Epoch 6513: Training Loss: 0.15010652442773184 Validation Loss: 0.7145408391952515\n",
      "Epoch 6514: Training Loss: 0.15037869910399118 Validation Loss: 0.7148061990737915\n",
      "Epoch 6515: Training Loss: 0.14984374741713205 Validation Loss: 0.7152755260467529\n",
      "Epoch 6516: Training Loss: 0.15125625332196554 Validation Loss: 0.7152021527290344\n",
      "Epoch 6517: Training Loss: 0.14939943452676138 Validation Loss: 0.7151642441749573\n",
      "Epoch 6518: Training Loss: 0.14987264076868692 Validation Loss: 0.7153390645980835\n",
      "Epoch 6519: Training Loss: 0.1503227377931277 Validation Loss: 0.7153459191322327\n",
      "Epoch 6520: Training Loss: 0.1501816213130951 Validation Loss: 0.71534663438797\n",
      "Epoch 6521: Training Loss: 0.14978214104970297 Validation Loss: 0.7153225541114807\n",
      "Epoch 6522: Training Loss: 0.14985093971093497 Validation Loss: 0.7150548100471497\n",
      "Epoch 6523: Training Loss: 0.1507121572891871 Validation Loss: 0.7146575450897217\n",
      "Epoch 6524: Training Loss: 0.14976618687311807 Validation Loss: 0.7145621180534363\n",
      "Epoch 6525: Training Loss: 0.14954808354377747 Validation Loss: 0.7147637009620667\n",
      "Epoch 6526: Training Loss: 0.14999313404162726 Validation Loss: 0.7146437168121338\n",
      "Epoch 6527: Training Loss: 0.14955761035283408 Validation Loss: 0.7145906686782837\n",
      "Epoch 6528: Training Loss: 0.1501092662413915 Validation Loss: 0.7148324251174927\n",
      "Epoch 6529: Training Loss: 0.14955766995747885 Validation Loss: 0.7148927450180054\n",
      "Epoch 6530: Training Loss: 0.1498428781827291 Validation Loss: 0.7150670886039734\n",
      "Epoch 6531: Training Loss: 0.14988947908083597 Validation Loss: 0.7151418328285217\n",
      "Epoch 6532: Training Loss: 0.1493197331825892 Validation Loss: 0.7145704627037048\n",
      "Epoch 6533: Training Loss: 0.15025622646013895 Validation Loss: 0.7148697972297668\n",
      "Epoch 6534: Training Loss: 0.14974849919478098 Validation Loss: 0.7149171829223633\n",
      "Epoch 6535: Training Loss: 0.149245152870814 Validation Loss: 0.7146715521812439\n",
      "Epoch 6536: Training Loss: 0.14997924864292145 Validation Loss: 0.714582085609436\n",
      "Epoch 6537: Training Loss: 0.14956911404927573 Validation Loss: 0.7148839235305786\n",
      "Epoch 6538: Training Loss: 0.1509587491552035 Validation Loss: 0.7148427367210388\n",
      "Epoch 6539: Training Loss: 0.14931748062372208 Validation Loss: 0.7148729562759399\n",
      "Epoch 6540: Training Loss: 0.14923955500125885 Validation Loss: 0.7147629857063293\n",
      "Epoch 6541: Training Loss: 0.1501240779956182 Validation Loss: 0.7149346470832825\n",
      "Epoch 6542: Training Loss: 0.14917847514152527 Validation Loss: 0.7150192856788635\n",
      "Epoch 6543: Training Loss: 0.15056530634562174 Validation Loss: 0.7147559523582458\n",
      "Epoch 6544: Training Loss: 0.1495360533396403 Validation Loss: 0.7145580053329468\n",
      "Epoch 6545: Training Loss: 0.14930747946103415 Validation Loss: 0.7146832346916199\n",
      "Epoch 6546: Training Loss: 0.14930076897144318 Validation Loss: 0.7147043943405151\n",
      "Epoch 6547: Training Loss: 0.149603803952535 Validation Loss: 0.7151206731796265\n",
      "Epoch 6548: Training Loss: 0.14934134980042776 Validation Loss: 0.7148390412330627\n",
      "Epoch 6549: Training Loss: 0.14928308129310608 Validation Loss: 0.7146055698394775\n",
      "Epoch 6550: Training Loss: 0.1490064263343811 Validation Loss: 0.7152420878410339\n",
      "Epoch 6551: Training Loss: 0.14939061800638834 Validation Loss: 0.7149873375892639\n",
      "Epoch 6552: Training Loss: 0.14984301229317984 Validation Loss: 0.7149727940559387\n",
      "Epoch 6553: Training Loss: 0.14912229776382446 Validation Loss: 0.7148575186729431\n",
      "Epoch 6554: Training Loss: 0.1489089826742808 Validation Loss: 0.7148915529251099\n",
      "Epoch 6555: Training Loss: 0.1490319768587748 Validation Loss: 0.714788019657135\n",
      "Epoch 6556: Training Loss: 0.14896945655345917 Validation Loss: 0.7147151827812195\n",
      "Epoch 6557: Training Loss: 0.14873428146044412 Validation Loss: 0.7148609161376953\n",
      "Epoch 6558: Training Loss: 0.14856800436973572 Validation Loss: 0.7151944637298584\n",
      "Epoch 6559: Training Loss: 0.14878535270690918 Validation Loss: 0.7151558995246887\n",
      "Epoch 6560: Training Loss: 0.14869503676891327 Validation Loss: 0.7154137492179871\n",
      "Epoch 6561: Training Loss: 0.1490869919459025 Validation Loss: 0.7151926755905151\n",
      "Epoch 6562: Training Loss: 0.14885765314102173 Validation Loss: 0.7148867845535278\n",
      "Epoch 6563: Training Loss: 0.1506076082587242 Validation Loss: 0.7150468826293945\n",
      "Epoch 6564: Training Loss: 0.14860441784063974 Validation Loss: 0.715042233467102\n",
      "Epoch 6565: Training Loss: 0.14926821986834207 Validation Loss: 0.7153766751289368\n",
      "Epoch 6566: Training Loss: 0.14864849795897803 Validation Loss: 0.7153964042663574\n",
      "Epoch 6567: Training Loss: 0.14877900977929434 Validation Loss: 0.7152244448661804\n",
      "Epoch 6568: Training Loss: 0.14885109663009644 Validation Loss: 0.7153304815292358\n",
      "Epoch 6569: Training Loss: 0.14887441198031107 Validation Loss: 0.7152917981147766\n",
      "Epoch 6570: Training Loss: 0.14860683182875314 Validation Loss: 0.7151365876197815\n",
      "Epoch 6571: Training Loss: 0.14866223434607187 Validation Loss: 0.714915931224823\n",
      "Epoch 6572: Training Loss: 0.14869241416454315 Validation Loss: 0.7147355675697327\n",
      "Epoch 6573: Training Loss: 0.1485830694437027 Validation Loss: 0.7146949768066406\n",
      "Epoch 6574: Training Loss: 0.14861746629079184 Validation Loss: 0.7149794697761536\n",
      "Epoch 6575: Training Loss: 0.14943712204694748 Validation Loss: 0.7150269746780396\n",
      "Epoch 6576: Training Loss: 0.14849147697289786 Validation Loss: 0.7147434949874878\n",
      "Epoch 6577: Training Loss: 0.1485236535469691 Validation Loss: 0.7147963643074036\n",
      "Epoch 6578: Training Loss: 0.1484403262535731 Validation Loss: 0.7153469324111938\n",
      "Epoch 6579: Training Loss: 0.1484509309132894 Validation Loss: 0.7156083583831787\n",
      "Epoch 6580: Training Loss: 0.14853440721829733 Validation Loss: 0.7154690623283386\n",
      "Epoch 6581: Training Loss: 0.1483060916264852 Validation Loss: 0.7152919173240662\n",
      "Epoch 6582: Training Loss: 0.14831106861432394 Validation Loss: 0.7152637839317322\n",
      "Epoch 6583: Training Loss: 0.148451566696167 Validation Loss: 0.7149795889854431\n",
      "Epoch 6584: Training Loss: 0.14829076826572418 Validation Loss: 0.7151362299919128\n",
      "Epoch 6585: Training Loss: 0.14914612968762717 Validation Loss: 0.7148920297622681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6586: Training Loss: 0.14824892083803812 Validation Loss: 0.7150347828865051\n",
      "Epoch 6587: Training Loss: 0.14851498107115427 Validation Loss: 0.7152859568595886\n",
      "Epoch 6588: Training Loss: 0.1484034856160482 Validation Loss: 0.7155656218528748\n",
      "Epoch 6589: Training Loss: 0.1490770330031713 Validation Loss: 0.715246856212616\n",
      "Epoch 6590: Training Loss: 0.14878898362318674 Validation Loss: 0.7149547934532166\n",
      "Epoch 6591: Training Loss: 0.14799732466538748 Validation Loss: 0.7150771617889404\n",
      "Epoch 6592: Training Loss: 0.14847067495187125 Validation Loss: 0.7153470516204834\n",
      "Epoch 6593: Training Loss: 0.14823190867900848 Validation Loss: 0.7152895331382751\n",
      "Epoch 6594: Training Loss: 0.1479326287905375 Validation Loss: 0.715560793876648\n",
      "Epoch 6595: Training Loss: 0.14806641141573587 Validation Loss: 0.7154483795166016\n",
      "Epoch 6596: Training Loss: 0.14824798703193665 Validation Loss: 0.7152257561683655\n",
      "Epoch 6597: Training Loss: 0.14871352910995483 Validation Loss: 0.7151629328727722\n",
      "Epoch 6598: Training Loss: 0.14799145857493082 Validation Loss: 0.7152074575424194\n",
      "Epoch 6599: Training Loss: 0.1479350874821345 Validation Loss: 0.7151345610618591\n",
      "Epoch 6600: Training Loss: 0.14784332116444907 Validation Loss: 0.7149783968925476\n",
      "Epoch 6601: Training Loss: 0.14768370985984802 Validation Loss: 0.7149094343185425\n",
      "Epoch 6602: Training Loss: 0.14812514185905457 Validation Loss: 0.7147053480148315\n",
      "Epoch 6603: Training Loss: 0.14798040191332498 Validation Loss: 0.7149617671966553\n",
      "Epoch 6604: Training Loss: 0.14806654552618662 Validation Loss: 0.7153184413909912\n",
      "Epoch 6605: Training Loss: 0.1479550004005432 Validation Loss: 0.715259850025177\n",
      "Epoch 6606: Training Loss: 0.1473281035820643 Validation Loss: 0.7154936790466309\n",
      "Epoch 6607: Training Loss: 0.14794779320557913 Validation Loss: 0.7155146598815918\n",
      "Epoch 6608: Training Loss: 0.14804892738660178 Validation Loss: 0.7153652310371399\n",
      "Epoch 6609: Training Loss: 0.1477909634510676 Validation Loss: 0.7154094576835632\n",
      "Epoch 6610: Training Loss: 0.147661825021108 Validation Loss: 0.7154926657676697\n",
      "Epoch 6611: Training Loss: 0.14843524992465973 Validation Loss: 0.7155685424804688\n",
      "Epoch 6612: Training Loss: 0.1477594549457232 Validation Loss: 0.7156446576118469\n",
      "Epoch 6613: Training Loss: 0.14873166382312775 Validation Loss: 0.7151079177856445\n",
      "Epoch 6614: Training Loss: 0.14754319687684378 Validation Loss: 0.7151015996932983\n",
      "Epoch 6615: Training Loss: 0.14865069588025412 Validation Loss: 0.7154645919799805\n",
      "Epoch 6616: Training Loss: 0.14726389447848 Validation Loss: 0.7159212827682495\n",
      "Epoch 6617: Training Loss: 0.14765965938568115 Validation Loss: 0.7158575654029846\n",
      "Epoch 6618: Training Loss: 0.14731546739737192 Validation Loss: 0.715568482875824\n",
      "Epoch 6619: Training Loss: 0.1477200984954834 Validation Loss: 0.7153027057647705\n",
      "Epoch 6620: Training Loss: 0.14764916400114694 Validation Loss: 0.7148600220680237\n",
      "Epoch 6621: Training Loss: 0.14757317304611206 Validation Loss: 0.714875340461731\n",
      "Epoch 6622: Training Loss: 0.14790762960910797 Validation Loss: 0.7151952981948853\n",
      "Epoch 6623: Training Loss: 0.14915392299493155 Validation Loss: 0.7157209515571594\n",
      "Epoch 6624: Training Loss: 0.14740854501724243 Validation Loss: 0.7156671285629272\n",
      "Epoch 6625: Training Loss: 0.14788610736529031 Validation Loss: 0.7152271866798401\n",
      "Epoch 6626: Training Loss: 0.14929049213727316 Validation Loss: 0.7149254679679871\n",
      "Epoch 6627: Training Loss: 0.14771357675393423 Validation Loss: 0.7150784134864807\n",
      "Epoch 6628: Training Loss: 0.14726301034291586 Validation Loss: 0.7151663899421692\n",
      "Epoch 6629: Training Loss: 0.14774200320243835 Validation Loss: 0.7154684662818909\n",
      "Epoch 6630: Training Loss: 0.14729446669419607 Validation Loss: 0.7154029607772827\n",
      "Epoch 6631: Training Loss: 0.14761683841546377 Validation Loss: 0.7153642773628235\n",
      "Epoch 6632: Training Loss: 0.14749292532602945 Validation Loss: 0.7153961062431335\n",
      "Epoch 6633: Training Loss: 0.14742538084586462 Validation Loss: 0.7153285145759583\n",
      "Epoch 6634: Training Loss: 0.14650733023881912 Validation Loss: 0.7150174975395203\n",
      "Epoch 6635: Training Loss: 0.1472438375155131 Validation Loss: 0.7151628136634827\n",
      "Epoch 6636: Training Loss: 0.14734052121639252 Validation Loss: 0.7157741785049438\n",
      "Epoch 6637: Training Loss: 0.14735732475916544 Validation Loss: 0.7156054973602295\n",
      "Epoch 6638: Training Loss: 0.14778147637844086 Validation Loss: 0.7156617045402527\n",
      "Epoch 6639: Training Loss: 0.14754978567361832 Validation Loss: 0.715607225894928\n",
      "Epoch 6640: Training Loss: 0.14727960030237833 Validation Loss: 0.7153844237327576\n",
      "Epoch 6641: Training Loss: 0.14679803947607675 Validation Loss: 0.7150822877883911\n",
      "Epoch 6642: Training Loss: 0.14731167753537497 Validation Loss: 0.7150399684906006\n",
      "Epoch 6643: Training Loss: 0.14705235262711844 Validation Loss: 0.7152450084686279\n",
      "Epoch 6644: Training Loss: 0.14719903965791067 Validation Loss: 0.7157343029975891\n",
      "Epoch 6645: Training Loss: 0.14742773274580637 Validation Loss: 0.7158343195915222\n",
      "Epoch 6646: Training Loss: 0.14736693600813547 Validation Loss: 0.7156617641448975\n",
      "Epoch 6647: Training Loss: 0.14718736708164215 Validation Loss: 0.7154391407966614\n",
      "Epoch 6648: Training Loss: 0.14769629637400308 Validation Loss: 0.7152671813964844\n",
      "Epoch 6649: Training Loss: 0.14726415276527405 Validation Loss: 0.7152295112609863\n",
      "Epoch 6650: Training Loss: 0.14735145370165506 Validation Loss: 0.7151352167129517\n",
      "Epoch 6651: Training Loss: 0.147393008073171 Validation Loss: 0.7152984142303467\n",
      "Epoch 6652: Training Loss: 0.14663240810235342 Validation Loss: 0.7155207395553589\n",
      "Epoch 6653: Training Loss: 0.1468806117773056 Validation Loss: 0.715695858001709\n",
      "Epoch 6654: Training Loss: 0.14664401610692343 Validation Loss: 0.7156440019607544\n",
      "Epoch 6655: Training Loss: 0.1467833866675695 Validation Loss: 0.7158455848693848\n",
      "Epoch 6656: Training Loss: 0.14711770912011465 Validation Loss: 0.715950608253479\n",
      "Epoch 6657: Training Loss: 0.14900555709997812 Validation Loss: 0.7153382897377014\n",
      "Epoch 6658: Training Loss: 0.14684893687566122 Validation Loss: 0.7152073979377747\n",
      "Epoch 6659: Training Loss: 0.14770697305599848 Validation Loss: 0.7151808142662048\n",
      "Epoch 6660: Training Loss: 0.1465484251578649 Validation Loss: 0.7153100967407227\n",
      "Epoch 6661: Training Loss: 0.14749797185262045 Validation Loss: 0.715593695640564\n",
      "Epoch 6662: Training Loss: 0.1468403438727061 Validation Loss: 0.7157048583030701\n",
      "Epoch 6663: Training Loss: 0.14650954802831015 Validation Loss: 0.7155598402023315\n",
      "Epoch 6664: Training Loss: 0.14666702349980673 Validation Loss: 0.7157591581344604\n",
      "Epoch 6665: Training Loss: 0.1465069204568863 Validation Loss: 0.7157425880432129\n",
      "Epoch 6666: Training Loss: 0.14631467560927072 Validation Loss: 0.7157275676727295\n",
      "Epoch 6667: Training Loss: 0.14751380185286203 Validation Loss: 0.7153838872909546\n",
      "Epoch 6668: Training Loss: 0.14643209179242453 Validation Loss: 0.7155739068984985\n",
      "Epoch 6669: Training Loss: 0.14660040537516275 Validation Loss: 0.715445339679718\n",
      "Epoch 6670: Training Loss: 0.1464690069357554 Validation Loss: 0.7153128981590271\n",
      "Epoch 6671: Training Loss: 0.14629149933656058 Validation Loss: 0.7156773805618286\n",
      "Epoch 6672: Training Loss: 0.1463900407155355 Validation Loss: 0.7156314849853516\n",
      "Epoch 6673: Training Loss: 0.14629546304543814 Validation Loss: 0.7155060172080994\n",
      "Epoch 6674: Training Loss: 0.14637174705664316 Validation Loss: 0.7155353426933289\n",
      "Epoch 6675: Training Loss: 0.1459923138221105 Validation Loss: 0.715458333492279\n",
      "Epoch 6676: Training Loss: 0.14642647902170816 Validation Loss: 0.7155250310897827\n",
      "Epoch 6677: Training Loss: 0.14881324271361032 Validation Loss: 0.7155157327651978\n",
      "Epoch 6678: Training Loss: 0.14634080231189728 Validation Loss: 0.7155918478965759\n",
      "Epoch 6679: Training Loss: 0.14615184565385184 Validation Loss: 0.715597927570343\n",
      "Epoch 6680: Training Loss: 0.146617462237676 Validation Loss: 0.7161270380020142\n",
      "Epoch 6681: Training Loss: 0.14716568837563196 Validation Loss: 0.7164346575737\n",
      "Epoch 6682: Training Loss: 0.14636759956677756 Validation Loss: 0.7160173058509827\n",
      "Epoch 6683: Training Loss: 0.14613076547781625 Validation Loss: 0.7154581546783447\n",
      "Epoch 6684: Training Loss: 0.14663507541020712 Validation Loss: 0.7154936790466309\n",
      "Epoch 6685: Training Loss: 0.14623616635799408 Validation Loss: 0.7155992388725281\n",
      "Epoch 6686: Training Loss: 0.14608288804690042 Validation Loss: 0.7158719897270203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6687: Training Loss: 0.14675805469353995 Validation Loss: 0.7158005237579346\n",
      "Epoch 6688: Training Loss: 0.14589128891626993 Validation Loss: 0.7156545519828796\n",
      "Epoch 6689: Training Loss: 0.1461977412303289 Validation Loss: 0.7160384058952332\n",
      "Epoch 6690: Training Loss: 0.1460143874088923 Validation Loss: 0.7160581350326538\n",
      "Epoch 6691: Training Loss: 0.14622579018274942 Validation Loss: 0.7156165242195129\n",
      "Epoch 6692: Training Loss: 0.14605864385763803 Validation Loss: 0.7153543829917908\n",
      "Epoch 6693: Training Loss: 0.1468905732035637 Validation Loss: 0.7154235243797302\n",
      "Epoch 6694: Training Loss: 0.14629057049751282 Validation Loss: 0.715565025806427\n",
      "Epoch 6695: Training Loss: 0.14592556158701578 Validation Loss: 0.7156729102134705\n",
      "Epoch 6696: Training Loss: 0.1458689421415329 Validation Loss: 0.7160112857818604\n",
      "Epoch 6697: Training Loss: 0.14631350338459015 Validation Loss: 0.7162913084030151\n",
      "Epoch 6698: Training Loss: 0.14591450989246368 Validation Loss: 0.7165167331695557\n",
      "Epoch 6699: Training Loss: 0.14608034491539001 Validation Loss: 0.7163507342338562\n",
      "Epoch 6700: Training Loss: 0.14569410681724548 Validation Loss: 0.7161445617675781\n",
      "Epoch 6701: Training Loss: 0.14613299071788788 Validation Loss: 0.7157333493232727\n",
      "Epoch 6702: Training Loss: 0.14613264799118042 Validation Loss: 0.7154322862625122\n",
      "Epoch 6703: Training Loss: 0.14585310220718384 Validation Loss: 0.7153846621513367\n",
      "Epoch 6704: Training Loss: 0.1466513698299726 Validation Loss: 0.7157883048057556\n",
      "Epoch 6705: Training Loss: 0.14595998326937357 Validation Loss: 0.7158384919166565\n",
      "Epoch 6706: Training Loss: 0.14606616894404092 Validation Loss: 0.7158938646316528\n",
      "Epoch 6707: Training Loss: 0.14637223382790884 Validation Loss: 0.7161758542060852\n",
      "Epoch 6708: Training Loss: 0.14605741202831268 Validation Loss: 0.7166177034378052\n",
      "Epoch 6709: Training Loss: 0.145914355913798 Validation Loss: 0.7168306708335876\n",
      "Epoch 6710: Training Loss: 0.1457730233669281 Validation Loss: 0.7168043851852417\n",
      "Epoch 6711: Training Loss: 0.14562331636746725 Validation Loss: 0.7162530422210693\n",
      "Epoch 6712: Training Loss: 0.14574344952901205 Validation Loss: 0.7160961031913757\n",
      "Epoch 6713: Training Loss: 0.14563585817813873 Validation Loss: 0.7159878611564636\n",
      "Epoch 6714: Training Loss: 0.14578198889891306 Validation Loss: 0.7157549262046814\n",
      "Epoch 6715: Training Loss: 0.14559435844421387 Validation Loss: 0.7156516313552856\n",
      "Epoch 6716: Training Loss: 0.1455619732538859 Validation Loss: 0.7160370945930481\n",
      "Epoch 6717: Training Loss: 0.14589602251847586 Validation Loss: 0.7163106799125671\n",
      "Epoch 6718: Training Loss: 0.1452477624018987 Validation Loss: 0.7161709070205688\n",
      "Epoch 6719: Training Loss: 0.14639327426751456 Validation Loss: 0.7158989310264587\n",
      "Epoch 6720: Training Loss: 0.14555710554122925 Validation Loss: 0.7157278060913086\n",
      "Epoch 6721: Training Loss: 0.14534221589565277 Validation Loss: 0.7157595753669739\n",
      "Epoch 6722: Training Loss: 0.14564610520998636 Validation Loss: 0.7157781720161438\n",
      "Epoch 6723: Training Loss: 0.14548164109388986 Validation Loss: 0.7158066630363464\n",
      "Epoch 6724: Training Loss: 0.14528709650039673 Validation Loss: 0.7158750295639038\n",
      "Epoch 6725: Training Loss: 0.14528272052605948 Validation Loss: 0.7160911560058594\n",
      "Epoch 6726: Training Loss: 0.1467432603240013 Validation Loss: 0.715849757194519\n",
      "Epoch 6727: Training Loss: 0.14527585109074911 Validation Loss: 0.7156585454940796\n",
      "Epoch 6728: Training Loss: 0.14561008413632712 Validation Loss: 0.7161223888397217\n",
      "Epoch 6729: Training Loss: 0.146175483862559 Validation Loss: 0.7160431146621704\n",
      "Epoch 6730: Training Loss: 0.14521482586860657 Validation Loss: 0.7158899307250977\n",
      "Epoch 6731: Training Loss: 0.145210862159729 Validation Loss: 0.7161909341812134\n",
      "Epoch 6732: Training Loss: 0.14562482138474783 Validation Loss: 0.7161137461662292\n",
      "Epoch 6733: Training Loss: 0.145519549647967 Validation Loss: 0.7160494327545166\n",
      "Epoch 6734: Training Loss: 0.14522848526636759 Validation Loss: 0.7160191535949707\n",
      "Epoch 6735: Training Loss: 0.14524505039056143 Validation Loss: 0.7163609862327576\n",
      "Epoch 6736: Training Loss: 0.14587325851122537 Validation Loss: 0.7163727283477783\n",
      "Epoch 6737: Training Loss: 0.1451074630022049 Validation Loss: 0.7162055373191833\n",
      "Epoch 6738: Training Loss: 0.1449048419793447 Validation Loss: 0.715971052646637\n",
      "Epoch 6739: Training Loss: 0.14506820340951285 Validation Loss: 0.7160398960113525\n",
      "Epoch 6740: Training Loss: 0.1450688143571218 Validation Loss: 0.7158488631248474\n",
      "Epoch 6741: Training Loss: 0.14508227507273355 Validation Loss: 0.7158252596855164\n",
      "Epoch 6742: Training Loss: 0.14542526006698608 Validation Loss: 0.7158594131469727\n",
      "Epoch 6743: Training Loss: 0.14521908263365427 Validation Loss: 0.7159746885299683\n",
      "Epoch 6744: Training Loss: 0.14500164488951364 Validation Loss: 0.7159170508384705\n",
      "Epoch 6745: Training Loss: 0.14486591021219888 Validation Loss: 0.7161880135536194\n",
      "Epoch 6746: Training Loss: 0.1450423151254654 Validation Loss: 0.7160043120384216\n",
      "Epoch 6747: Training Loss: 0.1450707366069158 Validation Loss: 0.7157140374183655\n",
      "Epoch 6748: Training Loss: 0.14536317686239877 Validation Loss: 0.7160818576812744\n",
      "Epoch 6749: Training Loss: 0.14492715895175934 Validation Loss: 0.7160478234291077\n",
      "Epoch 6750: Training Loss: 0.14499912659327188 Validation Loss: 0.7161583304405212\n",
      "Epoch 6751: Training Loss: 0.14527114729086557 Validation Loss: 0.7159823775291443\n",
      "Epoch 6752: Training Loss: 0.14420382181803384 Validation Loss: 0.7161686420440674\n",
      "Epoch 6753: Training Loss: 0.1448864663640658 Validation Loss: 0.7162579298019409\n",
      "Epoch 6754: Training Loss: 0.1458717187245687 Validation Loss: 0.7161989808082581\n",
      "Epoch 6755: Training Loss: 0.14461987217267355 Validation Loss: 0.7162772417068481\n",
      "Epoch 6756: Training Loss: 0.14490743974844614 Validation Loss: 0.7166600227355957\n",
      "Epoch 6757: Training Loss: 0.14451221625010172 Validation Loss: 0.716904878616333\n",
      "Epoch 6758: Training Loss: 0.14510230968395868 Validation Loss: 0.7167737483978271\n",
      "Epoch 6759: Training Loss: 0.14498154322306314 Validation Loss: 0.7166539430618286\n",
      "Epoch 6760: Training Loss: 0.14506807178258896 Validation Loss: 0.7163976430892944\n",
      "Epoch 6761: Training Loss: 0.14468476176261902 Validation Loss: 0.7160892486572266\n",
      "Epoch 6762: Training Loss: 0.14452985922495523 Validation Loss: 0.7158752679824829\n",
      "Epoch 6763: Training Loss: 0.14476625124613443 Validation Loss: 0.7156879901885986\n",
      "Epoch 6764: Training Loss: 0.14603748420874277 Validation Loss: 0.7154694199562073\n",
      "Epoch 6765: Training Loss: 0.14449151357014975 Validation Loss: 0.7158704400062561\n",
      "Epoch 6766: Training Loss: 0.1447485238313675 Validation Loss: 0.7160998582839966\n",
      "Epoch 6767: Training Loss: 0.14475436011950174 Validation Loss: 0.7161975502967834\n",
      "Epoch 6768: Training Loss: 0.1444494922955831 Validation Loss: 0.7159285545349121\n",
      "Epoch 6769: Training Loss: 0.1445326308409373 Validation Loss: 0.7159278392791748\n",
      "Epoch 6770: Training Loss: 0.14442764719327292 Validation Loss: 0.7162665128707886\n",
      "Epoch 6771: Training Loss: 0.14613883942365646 Validation Loss: 0.7164181470870972\n",
      "Epoch 6772: Training Loss: 0.1446210741996765 Validation Loss: 0.7163313627243042\n",
      "Epoch 6773: Training Loss: 0.1446981281042099 Validation Loss: 0.7164871692657471\n",
      "Epoch 6774: Training Loss: 0.14408471683661142 Validation Loss: 0.7169923782348633\n",
      "Epoch 6775: Training Loss: 0.14489736904700598 Validation Loss: 0.7165210843086243\n",
      "Epoch 6776: Training Loss: 0.14439799388249716 Validation Loss: 0.7164983153343201\n",
      "Epoch 6777: Training Loss: 0.1446481098731359 Validation Loss: 0.716579020023346\n",
      "Epoch 6778: Training Loss: 0.14429013431072235 Validation Loss: 0.7163565158843994\n",
      "Epoch 6779: Training Loss: 0.1438734084367752 Validation Loss: 0.716303825378418\n",
      "Epoch 6780: Training Loss: 0.1441827267408371 Validation Loss: 0.7160701751708984\n",
      "Epoch 6781: Training Loss: 0.14398021002610525 Validation Loss: 0.7162391543388367\n",
      "Epoch 6782: Training Loss: 0.1437625288963318 Validation Loss: 0.7164148688316345\n",
      "Epoch 6783: Training Loss: 0.14479112873474756 Validation Loss: 0.7166241407394409\n",
      "Epoch 6784: Training Loss: 0.14372409135103226 Validation Loss: 0.7165769338607788\n",
      "Epoch 6785: Training Loss: 0.14420646677414575 Validation Loss: 0.7166361808776855\n",
      "Epoch 6786: Training Loss: 0.14398279786109924 Validation Loss: 0.7163553237915039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6787: Training Loss: 0.144268532594045 Validation Loss: 0.716318666934967\n",
      "Epoch 6788: Training Loss: 0.14449933171272278 Validation Loss: 0.7160882353782654\n",
      "Epoch 6789: Training Loss: 0.14413464069366455 Validation Loss: 0.7162769436836243\n",
      "Epoch 6790: Training Loss: 0.14470546940962473 Validation Loss: 0.7161710262298584\n",
      "Epoch 6791: Training Loss: 0.14435412486394247 Validation Loss: 0.7165589332580566\n",
      "Epoch 6792: Training Loss: 0.1446418215831121 Validation Loss: 0.7169291973114014\n",
      "Epoch 6793: Training Loss: 0.1455040698250135 Validation Loss: 0.7165697813034058\n",
      "Epoch 6794: Training Loss: 0.14391090969244638 Validation Loss: 0.7163143754005432\n",
      "Epoch 6795: Training Loss: 0.1435954769452413 Validation Loss: 0.7162371277809143\n",
      "Epoch 6796: Training Loss: 0.14444938550392786 Validation Loss: 0.7163637280464172\n",
      "Epoch 6797: Training Loss: 0.144630196193854 Validation Loss: 0.7162718176841736\n",
      "Epoch 6798: Training Loss: 0.14405823250611624 Validation Loss: 0.7163416147232056\n",
      "Epoch 6799: Training Loss: 0.14482800165812174 Validation Loss: 0.7163128852844238\n",
      "Epoch 6800: Training Loss: 0.14404255151748657 Validation Loss: 0.7164977192878723\n",
      "Epoch 6801: Training Loss: 0.14470490316549936 Validation Loss: 0.7168840765953064\n",
      "Epoch 6802: Training Loss: 0.1442758118112882 Validation Loss: 0.7166610360145569\n",
      "Epoch 6803: Training Loss: 0.14385483165582022 Validation Loss: 0.716565728187561\n",
      "Epoch 6804: Training Loss: 0.14352237433195114 Validation Loss: 0.7161872386932373\n",
      "Epoch 6805: Training Loss: 0.1442431559165319 Validation Loss: 0.7160845994949341\n",
      "Epoch 6806: Training Loss: 0.1435856024424235 Validation Loss: 0.7162007689476013\n",
      "Epoch 6807: Training Loss: 0.14381187160809836 Validation Loss: 0.7162793874740601\n",
      "Epoch 6808: Training Loss: 0.14321929961442947 Validation Loss: 0.7163918018341064\n",
      "Epoch 6809: Training Loss: 0.14366890490055084 Validation Loss: 0.7166085839271545\n",
      "Epoch 6810: Training Loss: 0.14417287210623422 Validation Loss: 0.716829776763916\n",
      "Epoch 6811: Training Loss: 0.14412694424390793 Validation Loss: 0.7171003222465515\n",
      "Epoch 6812: Training Loss: 0.1433441365758578 Validation Loss: 0.7171741724014282\n",
      "Epoch 6813: Training Loss: 0.14363835255304971 Validation Loss: 0.7170366644859314\n",
      "Epoch 6814: Training Loss: 0.14350726207097372 Validation Loss: 0.7169061303138733\n",
      "Epoch 6815: Training Loss: 0.14349170525868735 Validation Loss: 0.7166460752487183\n",
      "Epoch 6816: Training Loss: 0.14339225987593332 Validation Loss: 0.7164784669876099\n",
      "Epoch 6817: Training Loss: 0.1436062455177307 Validation Loss: 0.7166051864624023\n",
      "Epoch 6818: Training Loss: 0.14389054973920187 Validation Loss: 0.7167810797691345\n",
      "Epoch 6819: Training Loss: 0.1435625503460566 Validation Loss: 0.7167402505874634\n",
      "Epoch 6820: Training Loss: 0.14313553273677826 Validation Loss: 0.7166244983673096\n",
      "Epoch 6821: Training Loss: 0.14299274484316507 Validation Loss: 0.71648108959198\n",
      "Epoch 6822: Training Loss: 0.14335676530996957 Validation Loss: 0.7166792154312134\n",
      "Epoch 6823: Training Loss: 0.1434240241845449 Validation Loss: 0.7168295979499817\n",
      "Epoch 6824: Training Loss: 0.14330929021040598 Validation Loss: 0.716610848903656\n",
      "Epoch 6825: Training Loss: 0.14356112480163574 Validation Loss: 0.716506838798523\n",
      "Epoch 6826: Training Loss: 0.14346847931543985 Validation Loss: 0.7165183424949646\n",
      "Epoch 6827: Training Loss: 0.14265247186024985 Validation Loss: 0.7165200710296631\n",
      "Epoch 6828: Training Loss: 0.14319478472073874 Validation Loss: 0.7165554761886597\n",
      "Epoch 6829: Training Loss: 0.14284170667330423 Validation Loss: 0.7167953252792358\n",
      "Epoch 6830: Training Loss: 0.14315217236677805 Validation Loss: 0.7167776823043823\n",
      "Epoch 6831: Training Loss: 0.14350056648254395 Validation Loss: 0.7170228958129883\n",
      "Epoch 6832: Training Loss: 0.1432205786307653 Validation Loss: 0.7168378829956055\n",
      "Epoch 6833: Training Loss: 0.14306670924027762 Validation Loss: 0.7167345881462097\n",
      "Epoch 6834: Training Loss: 0.14366011569897333 Validation Loss: 0.7162845134735107\n",
      "Epoch 6835: Training Loss: 0.14328692853450775 Validation Loss: 0.7163247466087341\n",
      "Epoch 6836: Training Loss: 0.14284144838651022 Validation Loss: 0.7167105674743652\n",
      "Epoch 6837: Training Loss: 0.14314748843510947 Validation Loss: 0.7166473269462585\n",
      "Epoch 6838: Training Loss: 0.14324188232421875 Validation Loss: 0.716546893119812\n",
      "Epoch 6839: Training Loss: 0.14300093054771423 Validation Loss: 0.7164998054504395\n",
      "Epoch 6840: Training Loss: 0.14478358626365662 Validation Loss: 0.7167194485664368\n",
      "Epoch 6841: Training Loss: 0.14214311043421426 Validation Loss: 0.7167873382568359\n",
      "Epoch 6842: Training Loss: 0.1434885933995247 Validation Loss: 0.7169926166534424\n",
      "Epoch 6843: Training Loss: 0.14313951631387076 Validation Loss: 0.716815173625946\n",
      "Epoch 6844: Training Loss: 0.1429774910211563 Validation Loss: 0.7164046168327332\n",
      "Epoch 6845: Training Loss: 0.1429856469233831 Validation Loss: 0.7166683673858643\n",
      "Epoch 6846: Training Loss: 0.14302249252796173 Validation Loss: 0.7166293859481812\n",
      "Epoch 6847: Training Loss: 0.14284002780914307 Validation Loss: 0.716255247592926\n",
      "Epoch 6848: Training Loss: 0.14293869336446127 Validation Loss: 0.7166227698326111\n",
      "Epoch 6849: Training Loss: 0.14269923170407614 Validation Loss: 0.7166727781295776\n",
      "Epoch 6850: Training Loss: 0.14275331795215607 Validation Loss: 0.7168578505516052\n",
      "Epoch 6851: Training Loss: 0.14249629775683084 Validation Loss: 0.7167284488677979\n",
      "Epoch 6852: Training Loss: 0.14272832870483398 Validation Loss: 0.7168247103691101\n",
      "Epoch 6853: Training Loss: 0.1424390027920405 Validation Loss: 0.7170886993408203\n",
      "Epoch 6854: Training Loss: 0.14269554118315378 Validation Loss: 0.716989278793335\n",
      "Epoch 6855: Training Loss: 0.14268479744593301 Validation Loss: 0.7166407704353333\n",
      "Epoch 6856: Training Loss: 0.14271860321362814 Validation Loss: 0.7169235944747925\n",
      "Epoch 6857: Training Loss: 0.1425463060537974 Validation Loss: 0.7170565128326416\n",
      "Epoch 6858: Training Loss: 0.14234119156996408 Validation Loss: 0.7172433733940125\n",
      "Epoch 6859: Training Loss: 0.14286820342143378 Validation Loss: 0.7172890901565552\n",
      "Epoch 6860: Training Loss: 0.14449138442675272 Validation Loss: 0.7169676423072815\n",
      "Epoch 6861: Training Loss: 0.14267252882321677 Validation Loss: 0.7164583206176758\n",
      "Epoch 6862: Training Loss: 0.1434061030546824 Validation Loss: 0.7167647480964661\n",
      "Epoch 6863: Training Loss: 0.142783522605896 Validation Loss: 0.7169238328933716\n",
      "Epoch 6864: Training Loss: 0.14286044736703238 Validation Loss: 0.7168347835540771\n",
      "Epoch 6865: Training Loss: 0.14266304175059 Validation Loss: 0.7170324921607971\n",
      "Epoch 6866: Training Loss: 0.14267234255870184 Validation Loss: 0.716938853263855\n",
      "Epoch 6867: Training Loss: 0.1429645319779714 Validation Loss: 0.7169278860092163\n",
      "Epoch 6868: Training Loss: 0.1426477332909902 Validation Loss: 0.7166258096694946\n",
      "Epoch 6869: Training Loss: 0.1424946387608846 Validation Loss: 0.7168993949890137\n",
      "Epoch 6870: Training Loss: 0.1419523830215136 Validation Loss: 0.7167943120002747\n",
      "Epoch 6871: Training Loss: 0.1424106533328692 Validation Loss: 0.7169076204299927\n",
      "Epoch 6872: Training Loss: 0.14330202341079712 Validation Loss: 0.7170059084892273\n",
      "Epoch 6873: Training Loss: 0.14229384561379751 Validation Loss: 0.7165871858596802\n",
      "Epoch 6874: Training Loss: 0.14270544052124023 Validation Loss: 0.7165063619613647\n",
      "Epoch 6875: Training Loss: 0.14239293336868286 Validation Loss: 0.7166950702667236\n",
      "Epoch 6876: Training Loss: 0.14272875835498175 Validation Loss: 0.7167853116989136\n",
      "Epoch 6877: Training Loss: 0.1421598196029663 Validation Loss: 0.7168938517570496\n",
      "Epoch 6878: Training Loss: 0.14163664480050406 Validation Loss: 0.7170142531394958\n",
      "Epoch 6879: Training Loss: 0.1425549586613973 Validation Loss: 0.7168439626693726\n",
      "Epoch 6880: Training Loss: 0.14228128641843796 Validation Loss: 0.7169026136398315\n",
      "Epoch 6881: Training Loss: 0.142166535059611 Validation Loss: 0.7170199155807495\n",
      "Epoch 6882: Training Loss: 0.14361711839834848 Validation Loss: 0.7164779901504517\n",
      "Epoch 6883: Training Loss: 0.14210892220338187 Validation Loss: 0.7166799902915955\n",
      "Epoch 6884: Training Loss: 0.14237005511919656 Validation Loss: 0.7168893218040466\n",
      "Epoch 6885: Training Loss: 0.1421307772397995 Validation Loss: 0.7172803282737732\n",
      "Epoch 6886: Training Loss: 0.142229696114858 Validation Loss: 0.7172353267669678\n",
      "Epoch 6887: Training Loss: 0.14222311476866403 Validation Loss: 0.717179536819458\n",
      "Epoch 6888: Training Loss: 0.14192349712053934 Validation Loss: 0.7168990969657898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6889: Training Loss: 0.1420921583970388 Validation Loss: 0.7167983651161194\n",
      "Epoch 6890: Training Loss: 0.1422537863254547 Validation Loss: 0.7172971963882446\n",
      "Epoch 6891: Training Loss: 0.14171664913495383 Validation Loss: 0.717379629611969\n",
      "Epoch 6892: Training Loss: 0.14213239153226218 Validation Loss: 0.7170878052711487\n",
      "Epoch 6893: Training Loss: 0.14191061755021414 Validation Loss: 0.7168979644775391\n",
      "Epoch 6894: Training Loss: 0.14206605901320776 Validation Loss: 0.7170348167419434\n",
      "Epoch 6895: Training Loss: 0.1430444990595182 Validation Loss: 0.7172453999519348\n",
      "Epoch 6896: Training Loss: 0.1416612615187963 Validation Loss: 0.7173738479614258\n",
      "Epoch 6897: Training Loss: 0.14175914724667868 Validation Loss: 0.7174075245857239\n",
      "Epoch 6898: Training Loss: 0.1418385704358419 Validation Loss: 0.7174924612045288\n",
      "Epoch 6899: Training Loss: 0.14177997907002768 Validation Loss: 0.7171213030815125\n",
      "Epoch 6900: Training Loss: 0.14171802004178366 Validation Loss: 0.7170712947845459\n",
      "Epoch 6901: Training Loss: 0.14265082528193793 Validation Loss: 0.7167140245437622\n",
      "Epoch 6902: Training Loss: 0.14210609098275503 Validation Loss: 0.7165154814720154\n",
      "Epoch 6903: Training Loss: 0.14183207352956137 Validation Loss: 0.716693639755249\n",
      "Epoch 6904: Training Loss: 0.14235659937063852 Validation Loss: 0.7168997526168823\n",
      "Epoch 6905: Training Loss: 0.14186341563860574 Validation Loss: 0.7172784805297852\n",
      "Epoch 6906: Training Loss: 0.1416631539662679 Validation Loss: 0.7175628542900085\n",
      "Epoch 6907: Training Loss: 0.14224064350128174 Validation Loss: 0.7179610133171082\n",
      "Epoch 6908: Training Loss: 0.14174719154834747 Validation Loss: 0.7176618576049805\n",
      "Epoch 6909: Training Loss: 0.14168955385684967 Validation Loss: 0.7173224687576294\n",
      "Epoch 6910: Training Loss: 0.14198225736618042 Validation Loss: 0.7173528671264648\n",
      "Epoch 6911: Training Loss: 0.14161824186642966 Validation Loss: 0.7171915769577026\n",
      "Epoch 6912: Training Loss: 0.14152259131272635 Validation Loss: 0.7170124053955078\n",
      "Epoch 6913: Training Loss: 0.14156710108121237 Validation Loss: 0.7168753147125244\n",
      "Epoch 6914: Training Loss: 0.14201970398426056 Validation Loss: 0.7171698212623596\n",
      "Epoch 6915: Training Loss: 0.14164514342943826 Validation Loss: 0.7171218991279602\n",
      "Epoch 6916: Training Loss: 0.14196096857388815 Validation Loss: 0.717258870601654\n",
      "Epoch 6917: Training Loss: 0.14249591529369354 Validation Loss: 0.7174750566482544\n",
      "Epoch 6918: Training Loss: 0.14216425766547522 Validation Loss: 0.7176937460899353\n",
      "Epoch 6919: Training Loss: 0.14236115912596384 Validation Loss: 0.7176507711410522\n",
      "Epoch 6920: Training Loss: 0.14136585096518198 Validation Loss: 0.7175672650337219\n",
      "Epoch 6921: Training Loss: 0.1424369936188062 Validation Loss: 0.7174001932144165\n",
      "Epoch 6922: Training Loss: 0.14119951923688254 Validation Loss: 0.7175700068473816\n",
      "Epoch 6923: Training Loss: 0.14192313452561697 Validation Loss: 0.7174053192138672\n",
      "Epoch 6924: Training Loss: 0.1416712055603663 Validation Loss: 0.7170279026031494\n",
      "Epoch 6925: Training Loss: 0.14168774584929147 Validation Loss: 0.7169862985610962\n",
      "Epoch 6926: Training Loss: 0.14130893598000208 Validation Loss: 0.7172855734825134\n",
      "Epoch 6927: Training Loss: 0.14128793279329935 Validation Loss: 0.7177215218544006\n",
      "Epoch 6928: Training Loss: 0.14153127123912176 Validation Loss: 0.7174545526504517\n",
      "Epoch 6929: Training Loss: 0.14160772413015366 Validation Loss: 0.7172237038612366\n",
      "Epoch 6930: Training Loss: 0.14106271167596182 Validation Loss: 0.7172874808311462\n",
      "Epoch 6931: Training Loss: 0.14134550591309866 Validation Loss: 0.7173287272453308\n",
      "Epoch 6932: Training Loss: 0.14153763403495154 Validation Loss: 0.7174752950668335\n",
      "Epoch 6933: Training Loss: 0.14136826495329538 Validation Loss: 0.7179754376411438\n",
      "Epoch 6934: Training Loss: 0.1412370204925537 Validation Loss: 0.7178914546966553\n",
      "Epoch 6935: Training Loss: 0.14137431979179382 Validation Loss: 0.7175041437149048\n",
      "Epoch 6936: Training Loss: 0.14172444740931192 Validation Loss: 0.717564046382904\n",
      "Epoch 6937: Training Loss: 0.14126108090082803 Validation Loss: 0.7170702815055847\n",
      "Epoch 6938: Training Loss: 0.14149386435747147 Validation Loss: 0.7166872620582581\n",
      "Epoch 6939: Training Loss: 0.14112258950869241 Validation Loss: 0.71663898229599\n",
      "Epoch 6940: Training Loss: 0.1416346604625384 Validation Loss: 0.7172994613647461\n",
      "Epoch 6941: Training Loss: 0.14178582032521567 Validation Loss: 0.7176433801651001\n",
      "Epoch 6942: Training Loss: 0.14084268112977347 Validation Loss: 0.7177112698554993\n",
      "Epoch 6943: Training Loss: 0.14099294443925223 Validation Loss: 0.7173203825950623\n",
      "Epoch 6944: Training Loss: 0.14103425790866217 Validation Loss: 0.7173010110855103\n",
      "Epoch 6945: Training Loss: 0.14099394281705221 Validation Loss: 0.7173806428909302\n",
      "Epoch 6946: Training Loss: 0.14063886801401773 Validation Loss: 0.7176185846328735\n",
      "Epoch 6947: Training Loss: 0.1407297452290853 Validation Loss: 0.717528760433197\n",
      "Epoch 6948: Training Loss: 0.1408239205678304 Validation Loss: 0.7174731492996216\n",
      "Epoch 6949: Training Loss: 0.14139385521411896 Validation Loss: 0.717545211315155\n",
      "Epoch 6950: Training Loss: 0.14263909061749777 Validation Loss: 0.7175952196121216\n",
      "Epoch 6951: Training Loss: 0.1406450221935908 Validation Loss: 0.7177102565765381\n",
      "Epoch 6952: Training Loss: 0.14080733060836792 Validation Loss: 0.7176377773284912\n",
      "Epoch 6953: Training Loss: 0.14076975484689078 Validation Loss: 0.7176588773727417\n",
      "Epoch 6954: Training Loss: 0.14081238706906637 Validation Loss: 0.717715859413147\n",
      "Epoch 6955: Training Loss: 0.14139739672342935 Validation Loss: 0.7176405787467957\n",
      "Epoch 6956: Training Loss: 0.14147278914848962 Validation Loss: 0.7176949381828308\n",
      "Epoch 6957: Training Loss: 0.14060786366462708 Validation Loss: 0.7174474596977234\n",
      "Epoch 6958: Training Loss: 0.1407827983299891 Validation Loss: 0.7176119685173035\n",
      "Epoch 6959: Training Loss: 0.1407927324374517 Validation Loss: 0.7174345850944519\n",
      "Epoch 6960: Training Loss: 0.14027845114469528 Validation Loss: 0.7175420522689819\n",
      "Epoch 6961: Training Loss: 0.1406135062376658 Validation Loss: 0.717615008354187\n",
      "Epoch 6962: Training Loss: 0.14094450573126474 Validation Loss: 0.7177871465682983\n",
      "Epoch 6963: Training Loss: 0.14082627495129904 Validation Loss: 0.7176863551139832\n",
      "Epoch 6964: Training Loss: 0.1405821293592453 Validation Loss: 0.7177602052688599\n",
      "Epoch 6965: Training Loss: 0.1398757522304853 Validation Loss: 0.7177167534828186\n",
      "Epoch 6966: Training Loss: 0.14099402725696564 Validation Loss: 0.7177382707595825\n",
      "Epoch 6967: Training Loss: 0.1405509114265442 Validation Loss: 0.7176159024238586\n",
      "Epoch 6968: Training Loss: 0.14314745118220648 Validation Loss: 0.7175391912460327\n",
      "Epoch 6969: Training Loss: 0.1404898762702942 Validation Loss: 0.7177348732948303\n",
      "Epoch 6970: Training Loss: 0.14043707152207693 Validation Loss: 0.7175666093826294\n",
      "Epoch 6971: Training Loss: 0.14073372880617777 Validation Loss: 0.7177382707595825\n",
      "Epoch 6972: Training Loss: 0.1402745395898819 Validation Loss: 0.717677891254425\n",
      "Epoch 6973: Training Loss: 0.14029071728388467 Validation Loss: 0.7175956964492798\n",
      "Epoch 6974: Training Loss: 0.14064809679985046 Validation Loss: 0.7174462080001831\n",
      "Epoch 6975: Training Loss: 0.14037702977657318 Validation Loss: 0.7173365950584412\n",
      "Epoch 6976: Training Loss: 0.14060362676779428 Validation Loss: 0.7178154587745667\n",
      "Epoch 6977: Training Loss: 0.14022954305013022 Validation Loss: 0.717897355556488\n",
      "Epoch 6978: Training Loss: 0.14020209511121115 Validation Loss: 0.7180962562561035\n",
      "Epoch 6979: Training Loss: 0.14031650374333063 Validation Loss: 0.7180263996124268\n",
      "Epoch 6980: Training Loss: 0.1405294140179952 Validation Loss: 0.718159019947052\n",
      "Epoch 6981: Training Loss: 0.14059208581844965 Validation Loss: 0.7179340124130249\n",
      "Epoch 6982: Training Loss: 0.14058667421340942 Validation Loss: 0.7177649736404419\n",
      "Epoch 6983: Training Loss: 0.14040933549404144 Validation Loss: 0.7179677486419678\n",
      "Epoch 6984: Training Loss: 0.14007723331451416 Validation Loss: 0.7180193662643433\n",
      "Epoch 6985: Training Loss: 0.14031199365854263 Validation Loss: 0.7179754972457886\n",
      "Epoch 6986: Training Loss: 0.1405877023935318 Validation Loss: 0.7176710367202759\n",
      "Epoch 6987: Training Loss: 0.13985150555769602 Validation Loss: 0.717613697052002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6988: Training Loss: 0.14011655251185098 Validation Loss: 0.7173309922218323\n",
      "Epoch 6989: Training Loss: 0.14010030527909598 Validation Loss: 0.7175425291061401\n",
      "Epoch 6990: Training Loss: 0.1398573766152064 Validation Loss: 0.7176152467727661\n",
      "Epoch 6991: Training Loss: 0.14062288651863733 Validation Loss: 0.7176256775856018\n",
      "Epoch 6992: Training Loss: 0.14056758085886636 Validation Loss: 0.7174713015556335\n",
      "Epoch 6993: Training Loss: 0.14026421308517456 Validation Loss: 0.7175789475440979\n",
      "Epoch 6994: Training Loss: 0.14020828902721405 Validation Loss: 0.7175944447517395\n",
      "Epoch 6995: Training Loss: 0.14141317456960678 Validation Loss: 0.7180118560791016\n",
      "Epoch 6996: Training Loss: 0.14022221167882284 Validation Loss: 0.7180142998695374\n",
      "Epoch 6997: Training Loss: 0.14018919070561728 Validation Loss: 0.7181150317192078\n",
      "Epoch 6998: Training Loss: 0.14019874731699625 Validation Loss: 0.7180407643318176\n",
      "Epoch 6999: Training Loss: 0.1401912420988083 Validation Loss: 0.7176953554153442\n",
      "Epoch 7000: Training Loss: 0.13995079696178436 Validation Loss: 0.7179309129714966\n",
      "Epoch 7001: Training Loss: 0.1401827707886696 Validation Loss: 0.7175549268722534\n",
      "Epoch 7002: Training Loss: 0.1395863617459933 Validation Loss: 0.7176929712295532\n",
      "Epoch 7003: Training Loss: 0.13994150857130686 Validation Loss: 0.7181409597396851\n",
      "Epoch 7004: Training Loss: 0.13982811570167542 Validation Loss: 0.7183195352554321\n",
      "Epoch 7005: Training Loss: 0.1400428811709086 Validation Loss: 0.7183822989463806\n",
      "Epoch 7006: Training Loss: 0.13916692386070886 Validation Loss: 0.7180145382881165\n",
      "Epoch 7007: Training Loss: 0.13991339753071466 Validation Loss: 0.717708945274353\n",
      "Epoch 7008: Training Loss: 0.1400211602449417 Validation Loss: 0.7177700996398926\n",
      "Epoch 7009: Training Loss: 0.14003346612056097 Validation Loss: 0.717789888381958\n",
      "Epoch 7010: Training Loss: 0.139725461602211 Validation Loss: 0.718280017375946\n",
      "Epoch 7011: Training Loss: 0.1393370454510053 Validation Loss: 0.7179369926452637\n",
      "Epoch 7012: Training Loss: 0.14035011331240335 Validation Loss: 0.7179813385009766\n",
      "Epoch 7013: Training Loss: 0.13994600375493368 Validation Loss: 0.7178959250450134\n",
      "Epoch 7014: Training Loss: 0.13963469862937927 Validation Loss: 0.7178113460540771\n",
      "Epoch 7015: Training Loss: 0.13989627361297607 Validation Loss: 0.7177660465240479\n",
      "Epoch 7016: Training Loss: 0.13938387235005698 Validation Loss: 0.7180389165878296\n",
      "Epoch 7017: Training Loss: 0.13954564929008484 Validation Loss: 0.7182560563087463\n",
      "Epoch 7018: Training Loss: 0.13940583169460297 Validation Loss: 0.7183752655982971\n",
      "Epoch 7019: Training Loss: 0.13953501979509988 Validation Loss: 0.7179962992668152\n",
      "Epoch 7020: Training Loss: 0.13963235914707184 Validation Loss: 0.717888593673706\n",
      "Epoch 7021: Training Loss: 0.13957295815149942 Validation Loss: 0.7178376913070679\n",
      "Epoch 7022: Training Loss: 0.1400412917137146 Validation Loss: 0.7181411385536194\n",
      "Epoch 7023: Training Loss: 0.1393056015173594 Validation Loss: 0.7180752158164978\n",
      "Epoch 7024: Training Loss: 0.1394535352786382 Validation Loss: 0.7181674838066101\n",
      "Epoch 7025: Training Loss: 0.13928154110908508 Validation Loss: 0.7180920839309692\n",
      "Epoch 7026: Training Loss: 0.13950119415918985 Validation Loss: 0.7181851863861084\n",
      "Epoch 7027: Training Loss: 0.13944821059703827 Validation Loss: 0.7181405425071716\n",
      "Epoch 7028: Training Loss: 0.13910325368245444 Validation Loss: 0.7183293700218201\n",
      "Epoch 7029: Training Loss: 0.13927369813124338 Validation Loss: 0.7182483673095703\n",
      "Epoch 7030: Training Loss: 0.1396726742386818 Validation Loss: 0.7181997299194336\n",
      "Epoch 7031: Training Loss: 0.13960285484790802 Validation Loss: 0.7180178761482239\n",
      "Epoch 7032: Training Loss: 0.13996568322181702 Validation Loss: 0.7178939580917358\n",
      "Epoch 7033: Training Loss: 0.13944043467442194 Validation Loss: 0.7176769971847534\n",
      "Epoch 7034: Training Loss: 0.1391443187991778 Validation Loss: 0.7176287770271301\n",
      "Epoch 7035: Training Loss: 0.13969741761684418 Validation Loss: 0.7179903388023376\n",
      "Epoch 7036: Training Loss: 0.13924607634544373 Validation Loss: 0.717799723148346\n",
      "Epoch 7037: Training Loss: 0.13919186095396677 Validation Loss: 0.7182466387748718\n",
      "Epoch 7038: Training Loss: 0.1394342283407847 Validation Loss: 0.7182571887969971\n",
      "Epoch 7039: Training Loss: 0.13936304301023483 Validation Loss: 0.7181816697120667\n",
      "Epoch 7040: Training Loss: 0.13902599116166434 Validation Loss: 0.7179486155509949\n",
      "Epoch 7041: Training Loss: 0.13915934165318808 Validation Loss: 0.7180982828140259\n",
      "Epoch 7042: Training Loss: 0.13908759752909342 Validation Loss: 0.7180410027503967\n",
      "Epoch 7043: Training Loss: 0.13895972073078156 Validation Loss: 0.717778205871582\n",
      "Epoch 7044: Training Loss: 0.13908136387666067 Validation Loss: 0.7179827690124512\n",
      "Epoch 7045: Training Loss: 0.13882862279812494 Validation Loss: 0.7181259989738464\n",
      "Epoch 7046: Training Loss: 0.13963737587134042 Validation Loss: 0.7181853652000427\n",
      "Epoch 7047: Training Loss: 0.13894387086232504 Validation Loss: 0.7183101773262024\n",
      "Epoch 7048: Training Loss: 0.1393204132715861 Validation Loss: 0.7182050943374634\n",
      "Epoch 7049: Training Loss: 0.13916629056135812 Validation Loss: 0.7185579538345337\n",
      "Epoch 7050: Training Loss: 0.13910754521687826 Validation Loss: 0.7183667421340942\n",
      "Epoch 7051: Training Loss: 0.13957165429989496 Validation Loss: 0.7181580066680908\n",
      "Epoch 7052: Training Loss: 0.13894168535868326 Validation Loss: 0.7182074785232544\n",
      "Epoch 7053: Training Loss: 0.13977839797735214 Validation Loss: 0.7183532118797302\n",
      "Epoch 7054: Training Loss: 0.13917693495750427 Validation Loss: 0.7181103825569153\n",
      "Epoch 7055: Training Loss: 0.13945015768210092 Validation Loss: 0.7185111045837402\n",
      "Epoch 7056: Training Loss: 0.1390982319911321 Validation Loss: 0.7185216546058655\n",
      "Epoch 7057: Training Loss: 0.13899585604667664 Validation Loss: 0.7182283401489258\n",
      "Epoch 7058: Training Loss: 0.13880064090092978 Validation Loss: 0.7182451486587524\n",
      "Epoch 7059: Training Loss: 0.13871492445468903 Validation Loss: 0.7184341549873352\n",
      "Epoch 7060: Training Loss: 0.13878035048643747 Validation Loss: 0.718757688999176\n",
      "Epoch 7061: Training Loss: 0.13872545957565308 Validation Loss: 0.7186369299888611\n",
      "Epoch 7062: Training Loss: 0.13873200118541718 Validation Loss: 0.7184563279151917\n",
      "Epoch 7063: Training Loss: 0.13894324004650116 Validation Loss: 0.7182297110557556\n",
      "Epoch 7064: Training Loss: 0.13852535684903464 Validation Loss: 0.7183703184127808\n",
      "Epoch 7065: Training Loss: 0.1387731432914734 Validation Loss: 0.7184496521949768\n",
      "Epoch 7066: Training Loss: 0.13892983396848044 Validation Loss: 0.7183690071105957\n",
      "Epoch 7067: Training Loss: 0.1386479139328003 Validation Loss: 0.7183301448822021\n",
      "Epoch 7068: Training Loss: 0.1399268185098966 Validation Loss: 0.718209445476532\n",
      "Epoch 7069: Training Loss: 0.1387504587570826 Validation Loss: 0.7185051441192627\n",
      "Epoch 7070: Training Loss: 0.1395596837004026 Validation Loss: 0.7182988524436951\n",
      "Epoch 7071: Training Loss: 0.13859331607818604 Validation Loss: 0.7180938124656677\n",
      "Epoch 7072: Training Loss: 0.1385988195737203 Validation Loss: 0.7181404829025269\n",
      "Epoch 7073: Training Loss: 0.1383829563856125 Validation Loss: 0.7182122468948364\n",
      "Epoch 7074: Training Loss: 0.13899021595716476 Validation Loss: 0.7185336947441101\n",
      "Epoch 7075: Training Loss: 0.13848402599493662 Validation Loss: 0.7184193730354309\n",
      "Epoch 7076: Training Loss: 0.14063099523385367 Validation Loss: 0.7186205387115479\n",
      "Epoch 7077: Training Loss: 0.13875122865041098 Validation Loss: 0.7184846997261047\n",
      "Epoch 7078: Training Loss: 0.13875221957763037 Validation Loss: 0.7181105613708496\n",
      "Epoch 7079: Training Loss: 0.13856152693430582 Validation Loss: 0.7185056805610657\n",
      "Epoch 7080: Training Loss: 0.13819499810536703 Validation Loss: 0.7184615731239319\n",
      "Epoch 7081: Training Loss: 0.13851920266946158 Validation Loss: 0.718252420425415\n",
      "Epoch 7082: Training Loss: 0.13948926577965418 Validation Loss: 0.7184579968452454\n",
      "Epoch 7083: Training Loss: 0.13859148820241293 Validation Loss: 0.7189843654632568\n",
      "Epoch 7084: Training Loss: 0.1382126584649086 Validation Loss: 0.7189350724220276\n",
      "Epoch 7085: Training Loss: 0.13810702661673227 Validation Loss: 0.718541145324707\n",
      "Epoch 7086: Training Loss: 0.13870488107204437 Validation Loss: 0.7186934947967529\n",
      "Epoch 7087: Training Loss: 0.13808328409989676 Validation Loss: 0.7188292145729065\n",
      "Epoch 7088: Training Loss: 0.13826864461104074 Validation Loss: 0.7186999917030334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7089: Training Loss: 0.1386952425042788 Validation Loss: 0.718381404876709\n",
      "Epoch 7090: Training Loss: 0.13914995392163595 Validation Loss: 0.7183377146720886\n",
      "Epoch 7091: Training Loss: 0.1381929169098536 Validation Loss: 0.7186329364776611\n",
      "Epoch 7092: Training Loss: 0.13829611738522848 Validation Loss: 0.7186037302017212\n",
      "Epoch 7093: Training Loss: 0.1382619614402453 Validation Loss: 0.7185699939727783\n",
      "Epoch 7094: Training Loss: 0.13829381267229715 Validation Loss: 0.7186552286148071\n",
      "Epoch 7095: Training Loss: 0.1388868490854899 Validation Loss: 0.7190385460853577\n",
      "Epoch 7096: Training Loss: 0.13868105908234915 Validation Loss: 0.7186349630355835\n",
      "Epoch 7097: Training Loss: 0.13796962797641754 Validation Loss: 0.7185877561569214\n",
      "Epoch 7098: Training Loss: 0.13786178330580393 Validation Loss: 0.7185832858085632\n",
      "Epoch 7099: Training Loss: 0.1381658265988032 Validation Loss: 0.7183393836021423\n",
      "Epoch 7100: Training Loss: 0.13864395022392273 Validation Loss: 0.7182742357254028\n",
      "Epoch 7101: Training Loss: 0.1376616656780243 Validation Loss: 0.7182275056838989\n",
      "Epoch 7102: Training Loss: 0.13809478282928467 Validation Loss: 0.718310534954071\n",
      "Epoch 7103: Training Loss: 0.1382248823841413 Validation Loss: 0.7186697721481323\n",
      "Epoch 7104: Training Loss: 0.13786221047242483 Validation Loss: 0.71879643201828\n",
      "Epoch 7105: Training Loss: 0.1384430651863416 Validation Loss: 0.7188788056373596\n",
      "Epoch 7106: Training Loss: 0.13804547985394797 Validation Loss: 0.7189310193061829\n",
      "Epoch 7107: Training Loss: 0.1381596475839615 Validation Loss: 0.7186846733093262\n",
      "Epoch 7108: Training Loss: 0.13790557285149893 Validation Loss: 0.7183340191841125\n",
      "Epoch 7109: Training Loss: 0.1380833238363266 Validation Loss: 0.718132734298706\n",
      "Epoch 7110: Training Loss: 0.13802786668141684 Validation Loss: 0.7185988426208496\n",
      "Epoch 7111: Training Loss: 0.13792627056439719 Validation Loss: 0.7189139127731323\n",
      "Epoch 7112: Training Loss: 0.13808079063892365 Validation Loss: 0.7191232442855835\n",
      "Epoch 7113: Training Loss: 0.13792128364245096 Validation Loss: 0.7190998196601868\n",
      "Epoch 7114: Training Loss: 0.13812948763370514 Validation Loss: 0.7189227342605591\n",
      "Epoch 7115: Training Loss: 0.13790913422902426 Validation Loss: 0.7189444899559021\n",
      "Epoch 7116: Training Loss: 0.13753310342629751 Validation Loss: 0.7188288569450378\n",
      "Epoch 7117: Training Loss: 0.13764175275961557 Validation Loss: 0.7186208963394165\n",
      "Epoch 7118: Training Loss: 0.13769972821076712 Validation Loss: 0.7185829281806946\n",
      "Epoch 7119: Training Loss: 0.13774986068407694 Validation Loss: 0.7185395359992981\n",
      "Epoch 7120: Training Loss: 0.1377363751331965 Validation Loss: 0.7187236547470093\n",
      "Epoch 7121: Training Loss: 0.13808455069859824 Validation Loss: 0.7188514471054077\n",
      "Epoch 7122: Training Loss: 0.13781127333641052 Validation Loss: 0.7191805243492126\n",
      "Epoch 7123: Training Loss: 0.13769210875034332 Validation Loss: 0.7188798189163208\n",
      "Epoch 7124: Training Loss: 0.13789583245913187 Validation Loss: 0.7186017632484436\n",
      "Epoch 7125: Training Loss: 0.1377776563167572 Validation Loss: 0.7184761166572571\n",
      "Epoch 7126: Training Loss: 0.13842765738566717 Validation Loss: 0.7181208729743958\n",
      "Epoch 7127: Training Loss: 0.13760298490524292 Validation Loss: 0.7180247902870178\n",
      "Epoch 7128: Training Loss: 0.13754144807656607 Validation Loss: 0.718573808670044\n",
      "Epoch 7129: Training Loss: 0.13815672198931375 Validation Loss: 0.7189005613327026\n",
      "Epoch 7130: Training Loss: 0.13760855297247568 Validation Loss: 0.7189597487449646\n",
      "Epoch 7131: Training Loss: 0.13758158683776855 Validation Loss: 0.71869957447052\n",
      "Epoch 7132: Training Loss: 0.13752837727467218 Validation Loss: 0.718451201915741\n",
      "Epoch 7133: Training Loss: 0.13744098444779715 Validation Loss: 0.7185665965080261\n",
      "Epoch 7134: Training Loss: 0.13750369350115457 Validation Loss: 0.7181556224822998\n",
      "Epoch 7135: Training Loss: 0.13786974797646204 Validation Loss: 0.7184125781059265\n",
      "Epoch 7136: Training Loss: 0.13738483687241873 Validation Loss: 0.7186349034309387\n",
      "Epoch 7137: Training Loss: 0.13767319172620773 Validation Loss: 0.7188056111335754\n",
      "Epoch 7138: Training Loss: 0.13731098175048828 Validation Loss: 0.7187639474868774\n",
      "Epoch 7139: Training Loss: 0.13719312846660614 Validation Loss: 0.718668520450592\n",
      "Epoch 7140: Training Loss: 0.13752739628156027 Validation Loss: 0.7184206247329712\n",
      "Epoch 7141: Training Loss: 0.13725323478380838 Validation Loss: 0.7184085249900818\n",
      "Epoch 7142: Training Loss: 0.1372232735157013 Validation Loss: 0.719071626663208\n",
      "Epoch 7143: Training Loss: 0.13785228629906973 Validation Loss: 0.718989372253418\n",
      "Epoch 7144: Training Loss: 0.1381757656733195 Validation Loss: 0.7193455696105957\n",
      "Epoch 7145: Training Loss: 0.13684477160374323 Validation Loss: 0.7193533778190613\n",
      "Epoch 7146: Training Loss: 0.1380793352921804 Validation Loss: 0.7191370129585266\n",
      "Epoch 7147: Training Loss: 0.13812939325968424 Validation Loss: 0.7191254496574402\n",
      "Epoch 7148: Training Loss: 0.13718606531620026 Validation Loss: 0.719062328338623\n",
      "Epoch 7149: Training Loss: 0.1375868891676267 Validation Loss: 0.7191497683525085\n",
      "Epoch 7150: Training Loss: 0.13725622495015463 Validation Loss: 0.7190794348716736\n",
      "Epoch 7151: Training Loss: 0.13698329528172812 Validation Loss: 0.719283401966095\n",
      "Epoch 7152: Training Loss: 0.137196014324824 Validation Loss: 0.7191135287284851\n",
      "Epoch 7153: Training Loss: 0.13707553346951803 Validation Loss: 0.7189342975616455\n",
      "Epoch 7154: Training Loss: 0.13767176618178686 Validation Loss: 0.7189846038818359\n",
      "Epoch 7155: Training Loss: 0.1371499573191007 Validation Loss: 0.7189618349075317\n",
      "Epoch 7156: Training Loss: 0.1369607150554657 Validation Loss: 0.7190462350845337\n",
      "Epoch 7157: Training Loss: 0.13706794877847037 Validation Loss: 0.7192753553390503\n",
      "Epoch 7158: Training Loss: 0.13706558446089426 Validation Loss: 0.7189511060714722\n",
      "Epoch 7159: Training Loss: 0.1371521477897962 Validation Loss: 0.7193882465362549\n",
      "Epoch 7160: Training Loss: 0.13706588745117188 Validation Loss: 0.7192618250846863\n",
      "Epoch 7161: Training Loss: 0.1370370015501976 Validation Loss: 0.7190478444099426\n",
      "Epoch 7162: Training Loss: 0.13729050010442734 Validation Loss: 0.7189515829086304\n",
      "Epoch 7163: Training Loss: 0.1394701600074768 Validation Loss: 0.7192357778549194\n",
      "Epoch 7164: Training Loss: 0.13730556517839432 Validation Loss: 0.7191567420959473\n",
      "Epoch 7165: Training Loss: 0.13744162023067474 Validation Loss: 0.7191155552864075\n",
      "Epoch 7166: Training Loss: 0.13719246784845987 Validation Loss: 0.7190586924552917\n",
      "Epoch 7167: Training Loss: 0.13697517911593118 Validation Loss: 0.7190313339233398\n",
      "Epoch 7168: Training Loss: 0.13749611874421439 Validation Loss: 0.7190005779266357\n",
      "Epoch 7169: Training Loss: 0.13749903440475464 Validation Loss: 0.7193279266357422\n",
      "Epoch 7170: Training Loss: 0.13693621009588242 Validation Loss: 0.7189971208572388\n",
      "Epoch 7171: Training Loss: 0.1370567704240481 Validation Loss: 0.719154953956604\n",
      "Epoch 7172: Training Loss: 0.13689749936262766 Validation Loss: 0.7192093729972839\n",
      "Epoch 7173: Training Loss: 0.1369867374499639 Validation Loss: 0.7193590402603149\n",
      "Epoch 7174: Training Loss: 0.1376583973566691 Validation Loss: 0.7192562222480774\n",
      "Epoch 7175: Training Loss: 0.13686452309290567 Validation Loss: 0.7192370891571045\n",
      "Epoch 7176: Training Loss: 0.13678446412086487 Validation Loss: 0.718993604183197\n",
      "Epoch 7177: Training Loss: 0.1364157646894455 Validation Loss: 0.7191833257675171\n",
      "Epoch 7178: Training Loss: 0.13678759088118872 Validation Loss: 0.719505250453949\n",
      "Epoch 7179: Training Loss: 0.13640441497166952 Validation Loss: 0.7193720936775208\n",
      "Epoch 7180: Training Loss: 0.13706129292647043 Validation Loss: 0.7194130420684814\n",
      "Epoch 7181: Training Loss: 0.1368105709552765 Validation Loss: 0.719398021697998\n",
      "Epoch 7182: Training Loss: 0.13681335002183914 Validation Loss: 0.7193205952644348\n",
      "Epoch 7183: Training Loss: 0.13648917277654013 Validation Loss: 0.7195584774017334\n",
      "Epoch 7184: Training Loss: 0.1367088407278061 Validation Loss: 0.7193430662155151\n",
      "Epoch 7185: Training Loss: 0.13697488605976105 Validation Loss: 0.7196098566055298\n",
      "Epoch 7186: Training Loss: 0.13658840954303741 Validation Loss: 0.7194200158119202\n",
      "Epoch 7187: Training Loss: 0.13653347392876944 Validation Loss: 0.7194200158119202\n",
      "Epoch 7188: Training Loss: 0.1373133361339569 Validation Loss: 0.7195118069648743\n",
      "Epoch 7189: Training Loss: 0.1368290583292643 Validation Loss: 0.7195536494255066\n",
      "Epoch 7190: Training Loss: 0.13653642932573953 Validation Loss: 0.71954745054245\n",
      "Epoch 7191: Training Loss: 0.13622646033763885 Validation Loss: 0.719731330871582\n",
      "Epoch 7192: Training Loss: 0.13648628691832224 Validation Loss: 0.7198290228843689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7193: Training Loss: 0.1379943092664083 Validation Loss: 0.7200356125831604\n",
      "Epoch 7194: Training Loss: 0.13676037142674127 Validation Loss: 0.7196561694145203\n",
      "Epoch 7195: Training Loss: 0.13630122194687525 Validation Loss: 0.7193899154663086\n",
      "Epoch 7196: Training Loss: 0.13628766437371573 Validation Loss: 0.7193477749824524\n",
      "Epoch 7197: Training Loss: 0.1362529620528221 Validation Loss: 0.7192053198814392\n",
      "Epoch 7198: Training Loss: 0.13706031441688538 Validation Loss: 0.7192198038101196\n",
      "Epoch 7199: Training Loss: 0.1362540473540624 Validation Loss: 0.7192408442497253\n",
      "Epoch 7200: Training Loss: 0.1366965596874555 Validation Loss: 0.7195879220962524\n",
      "Epoch 7201: Training Loss: 0.13620403409004211 Validation Loss: 0.7197089791297913\n",
      "Epoch 7202: Training Loss: 0.13661731282869974 Validation Loss: 0.7195993661880493\n",
      "Epoch 7203: Training Loss: 0.13636837402979532 Validation Loss: 0.7193833589553833\n",
      "Epoch 7204: Training Loss: 0.1365353912115097 Validation Loss: 0.7191640734672546\n",
      "Epoch 7205: Training Loss: 0.1365333447853724 Validation Loss: 0.719025194644928\n",
      "Epoch 7206: Training Loss: 0.13612220187981924 Validation Loss: 0.7193788290023804\n",
      "Epoch 7207: Training Loss: 0.13625266154607138 Validation Loss: 0.7194478511810303\n",
      "Epoch 7208: Training Loss: 0.13636988898118338 Validation Loss: 0.7195703387260437\n",
      "Epoch 7209: Training Loss: 0.13638214270273843 Validation Loss: 0.7195912003517151\n",
      "Epoch 7210: Training Loss: 0.13650901118914285 Validation Loss: 0.7193472385406494\n",
      "Epoch 7211: Training Loss: 0.13624021410942078 Validation Loss: 0.7193059325218201\n",
      "Epoch 7212: Training Loss: 0.1365340749422709 Validation Loss: 0.7196448445320129\n",
      "Epoch 7213: Training Loss: 0.1359575092792511 Validation Loss: 0.7200821042060852\n",
      "Epoch 7214: Training Loss: 0.1373161052664121 Validation Loss: 0.7199066281318665\n",
      "Epoch 7215: Training Loss: 0.13600937525431314 Validation Loss: 0.7195149660110474\n",
      "Epoch 7216: Training Loss: 0.1370043953259786 Validation Loss: 0.719313383102417\n",
      "Epoch 7217: Training Loss: 0.1364947035908699 Validation Loss: 0.7197962403297424\n",
      "Epoch 7218: Training Loss: 0.1366944139202436 Validation Loss: 0.7196548581123352\n",
      "Epoch 7219: Training Loss: 0.13599474479754767 Validation Loss: 0.7191773653030396\n",
      "Epoch 7220: Training Loss: 0.13571216662724814 Validation Loss: 0.7193915843963623\n",
      "Epoch 7221: Training Loss: 0.13606087118387222 Validation Loss: 0.7196143269538879\n",
      "Epoch 7222: Training Loss: 0.13640016814072928 Validation Loss: 0.7196428179740906\n",
      "Epoch 7223: Training Loss: 0.13585125903288522 Validation Loss: 0.71951824426651\n",
      "Epoch 7224: Training Loss: 0.13590245942274728 Validation Loss: 0.7194584012031555\n",
      "Epoch 7225: Training Loss: 0.13612537334362665 Validation Loss: 0.7195431590080261\n",
      "Epoch 7226: Training Loss: 0.1363420064250628 Validation Loss: 0.7193994522094727\n",
      "Epoch 7227: Training Loss: 0.13592799256245294 Validation Loss: 0.719355046749115\n",
      "Epoch 7228: Training Loss: 0.1358364075422287 Validation Loss: 0.7194991111755371\n",
      "Epoch 7229: Training Loss: 0.13546856244405112 Validation Loss: 0.7194796204566956\n",
      "Epoch 7230: Training Loss: 0.13628443578879038 Validation Loss: 0.7197898626327515\n",
      "Epoch 7231: Training Loss: 0.13621299217144647 Validation Loss: 0.7196712493896484\n",
      "Epoch 7232: Training Loss: 0.13551532725493112 Validation Loss: 0.7199822664260864\n",
      "Epoch 7233: Training Loss: 0.13600162913401923 Validation Loss: 0.719569981098175\n",
      "Epoch 7234: Training Loss: 0.1356926163037618 Validation Loss: 0.7196224927902222\n",
      "Epoch 7235: Training Loss: 0.13574457168579102 Validation Loss: 0.7195613980293274\n",
      "Epoch 7236: Training Loss: 0.13576176017522812 Validation Loss: 0.7194216847419739\n",
      "Epoch 7237: Training Loss: 0.13546455651521683 Validation Loss: 0.7194575667381287\n",
      "Epoch 7238: Training Loss: 0.1357923075556755 Validation Loss: 0.7194402813911438\n",
      "Epoch 7239: Training Loss: 0.13580597937107086 Validation Loss: 0.7196844220161438\n",
      "Epoch 7240: Training Loss: 0.13585581630468369 Validation Loss: 0.7196434140205383\n",
      "Epoch 7241: Training Loss: 0.13563421120246252 Validation Loss: 0.719657301902771\n",
      "Epoch 7242: Training Loss: 0.13537239034970602 Validation Loss: 0.7196390628814697\n",
      "Epoch 7243: Training Loss: 0.135394756992658 Validation Loss: 0.7196853756904602\n",
      "Epoch 7244: Training Loss: 0.13597158839305243 Validation Loss: 0.7199154496192932\n",
      "Epoch 7245: Training Loss: 0.13546863943338394 Validation Loss: 0.7194145917892456\n",
      "Epoch 7246: Training Loss: 0.1358023782571157 Validation Loss: 0.7196395397186279\n",
      "Epoch 7247: Training Loss: 0.13690267503261566 Validation Loss: 0.7197626233100891\n",
      "Epoch 7248: Training Loss: 0.13582844038804373 Validation Loss: 0.7198300957679749\n",
      "Epoch 7249: Training Loss: 0.1356518715620041 Validation Loss: 0.7198700904846191\n",
      "Epoch 7250: Training Loss: 0.13568712770938873 Validation Loss: 0.7199448943138123\n",
      "Epoch 7251: Training Loss: 0.13706570118665695 Validation Loss: 0.7196630239486694\n",
      "Epoch 7252: Training Loss: 0.13532766699790955 Validation Loss: 0.7195197939872742\n",
      "Epoch 7253: Training Loss: 0.13582713653643927 Validation Loss: 0.7196506261825562\n",
      "Epoch 7254: Training Loss: 0.13526071111361185 Validation Loss: 0.7193528413772583\n",
      "Epoch 7255: Training Loss: 0.13499619563420615 Validation Loss: 0.7192782759666443\n",
      "Epoch 7256: Training Loss: 0.1363901694615682 Validation Loss: 0.7196887135505676\n",
      "Epoch 7257: Training Loss: 0.1352732703089714 Validation Loss: 0.7201350927352905\n",
      "Epoch 7258: Training Loss: 0.1349952220916748 Validation Loss: 0.7203634977340698\n",
      "Epoch 7259: Training Loss: 0.13556786874930063 Validation Loss: 0.7205689549446106\n",
      "Epoch 7260: Training Loss: 0.13532453775405884 Validation Loss: 0.7201816439628601\n",
      "Epoch 7261: Training Loss: 0.1352080156405767 Validation Loss: 0.7199204564094543\n",
      "Epoch 7262: Training Loss: 0.13528357446193695 Validation Loss: 0.7197252511978149\n",
      "Epoch 7263: Training Loss: 0.1360167587796847 Validation Loss: 0.7195216417312622\n",
      "Epoch 7264: Training Loss: 0.13535874088605246 Validation Loss: 0.7196047902107239\n",
      "Epoch 7265: Training Loss: 0.13544261952241263 Validation Loss: 0.7196967601776123\n",
      "Epoch 7266: Training Loss: 0.1355109065771103 Validation Loss: 0.7200686931610107\n",
      "Epoch 7267: Training Loss: 0.1351887583732605 Validation Loss: 0.7202486991882324\n",
      "Epoch 7268: Training Loss: 0.13516865670681 Validation Loss: 0.7200013995170593\n",
      "Epoch 7269: Training Loss: 0.13551822304725647 Validation Loss: 0.7199376225471497\n",
      "Epoch 7270: Training Loss: 0.13607539484898248 Validation Loss: 0.7196424007415771\n",
      "Epoch 7271: Training Loss: 0.13511278480291367 Validation Loss: 0.7200285196304321\n",
      "Epoch 7272: Training Loss: 0.1352620075146357 Validation Loss: 0.720070481300354\n",
      "Epoch 7273: Training Loss: 0.1348168303569158 Validation Loss: 0.7204022407531738\n",
      "Epoch 7274: Training Loss: 0.13551634550094604 Validation Loss: 0.720285952091217\n",
      "Epoch 7275: Training Loss: 0.13573566327492395 Validation Loss: 0.72013258934021\n",
      "Epoch 7276: Training Loss: 0.13489014903704324 Validation Loss: 0.7202152013778687\n",
      "Epoch 7277: Training Loss: 0.13586102177699408 Validation Loss: 0.7200120091438293\n",
      "Epoch 7278: Training Loss: 0.13582788407802582 Validation Loss: 0.7199193835258484\n",
      "Epoch 7279: Training Loss: 0.13497957090536752 Validation Loss: 0.7198012471199036\n",
      "Epoch 7280: Training Loss: 0.13469812522331873 Validation Loss: 0.7201698422431946\n",
      "Epoch 7281: Training Loss: 0.13499325513839722 Validation Loss: 0.7203625440597534\n",
      "Epoch 7282: Training Loss: 0.13484952847162882 Validation Loss: 0.7202768325805664\n",
      "Epoch 7283: Training Loss: 0.13500281423330307 Validation Loss: 0.7201833724975586\n",
      "Epoch 7284: Training Loss: 0.13487554093201956 Validation Loss: 0.720187246799469\n",
      "Epoch 7285: Training Loss: 0.13448588053385416 Validation Loss: 0.7199224233627319\n",
      "Epoch 7286: Training Loss: 0.13526416818300882 Validation Loss: 0.7197846174240112\n",
      "Epoch 7287: Training Loss: 0.13518114387989044 Validation Loss: 0.719744086265564\n",
      "Epoch 7288: Training Loss: 0.1358823925256729 Validation Loss: 0.719391942024231\n",
      "Epoch 7289: Training Loss: 0.13474426170190176 Validation Loss: 0.7195878028869629\n",
      "Epoch 7290: Training Loss: 0.1348667691151301 Validation Loss: 0.7198383808135986\n",
      "Epoch 7291: Training Loss: 0.13446337481339773 Validation Loss: 0.720061719417572\n",
      "Epoch 7292: Training Loss: 0.1349871481458346 Validation Loss: 0.7200462818145752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7293: Training Loss: 0.13472053905328116 Validation Loss: 0.7199737429618835\n",
      "Epoch 7294: Training Loss: 0.1345940257112185 Validation Loss: 0.7202085256576538\n",
      "Epoch 7295: Training Loss: 0.13416633754968643 Validation Loss: 0.7203190922737122\n",
      "Epoch 7296: Training Loss: 0.13445433974266052 Validation Loss: 0.720306396484375\n",
      "Epoch 7297: Training Loss: 0.13523746033509573 Validation Loss: 0.720275342464447\n",
      "Epoch 7298: Training Loss: 0.13478037963310877 Validation Loss: 0.7200025320053101\n",
      "Epoch 7299: Training Loss: 0.13430566092332205 Validation Loss: 0.7201015949249268\n",
      "Epoch 7300: Training Loss: 0.13543760776519775 Validation Loss: 0.7203594446182251\n",
      "Epoch 7301: Training Loss: 0.13439867148796716 Validation Loss: 0.7201560139656067\n",
      "Epoch 7302: Training Loss: 0.1341868539651235 Validation Loss: 0.7202914357185364\n",
      "Epoch 7303: Training Loss: 0.13503563404083252 Validation Loss: 0.7199930548667908\n",
      "Epoch 7304: Training Loss: 0.134521484375 Validation Loss: 0.7202980518341064\n",
      "Epoch 7305: Training Loss: 0.13464590658744177 Validation Loss: 0.7207368016242981\n",
      "Epoch 7306: Training Loss: 0.13463132083415985 Validation Loss: 0.7205274105072021\n",
      "Epoch 7307: Training Loss: 0.13435470312833786 Validation Loss: 0.7202772498130798\n",
      "Epoch 7308: Training Loss: 0.13496906061967215 Validation Loss: 0.7202920317649841\n",
      "Epoch 7309: Training Loss: 0.13432549933592478 Validation Loss: 0.7203254699707031\n",
      "Epoch 7310: Training Loss: 0.13474341730276743 Validation Loss: 0.7199692726135254\n",
      "Epoch 7311: Training Loss: 0.134506123761336 Validation Loss: 0.7202110886573792\n",
      "Epoch 7312: Training Loss: 0.13461778064568838 Validation Loss: 0.7204126715660095\n",
      "Epoch 7313: Training Loss: 0.1344466656446457 Validation Loss: 0.7205216288566589\n",
      "Epoch 7314: Training Loss: 0.1346108317375183 Validation Loss: 0.7202940583229065\n",
      "Epoch 7315: Training Loss: 0.13409380366404852 Validation Loss: 0.7203272581100464\n",
      "Epoch 7316: Training Loss: 0.13412620623906454 Validation Loss: 0.7204448580741882\n",
      "Epoch 7317: Training Loss: 0.13428852458794913 Validation Loss: 0.7202073335647583\n",
      "Epoch 7318: Training Loss: 0.1347538356979688 Validation Loss: 0.7204182744026184\n",
      "Epoch 7319: Training Loss: 0.1340833380818367 Validation Loss: 0.7205238938331604\n",
      "Epoch 7320: Training Loss: 0.134540726741155 Validation Loss: 0.720478892326355\n",
      "Epoch 7321: Training Loss: 0.13518705467383066 Validation Loss: 0.7201630473136902\n",
      "Epoch 7322: Training Loss: 0.1338769495487213 Validation Loss: 0.7198678255081177\n",
      "Epoch 7323: Training Loss: 0.1341062436501185 Validation Loss: 0.7198736667633057\n",
      "Epoch 7324: Training Loss: 0.13415499528249106 Validation Loss: 0.7196869850158691\n",
      "Epoch 7325: Training Loss: 0.1344422698020935 Validation Loss: 0.720169723033905\n",
      "Epoch 7326: Training Loss: 0.13393615434567133 Validation Loss: 0.7206597328186035\n",
      "Epoch 7327: Training Loss: 0.1342895949880282 Validation Loss: 0.7206472158432007\n",
      "Epoch 7328: Training Loss: 0.13406547904014587 Validation Loss: 0.7204052209854126\n",
      "Epoch 7329: Training Loss: 0.13426133493582407 Validation Loss: 0.7201422452926636\n",
      "Epoch 7330: Training Loss: 0.134794091184934 Validation Loss: 0.720268189907074\n",
      "Epoch 7331: Training Loss: 0.13387267291545868 Validation Loss: 0.7201080322265625\n",
      "Epoch 7332: Training Loss: 0.13444472352663675 Validation Loss: 0.7203401327133179\n",
      "Epoch 7333: Training Loss: 0.1338628108302752 Validation Loss: 0.72038334608078\n",
      "Epoch 7334: Training Loss: 0.1340657373269399 Validation Loss: 0.7203177213668823\n",
      "Epoch 7335: Training Loss: 0.13439392298460007 Validation Loss: 0.720517098903656\n",
      "Epoch 7336: Training Loss: 0.1339484006166458 Validation Loss: 0.7210140824317932\n",
      "Epoch 7337: Training Loss: 0.13489028066396713 Validation Loss: 0.7208250761032104\n",
      "Epoch 7338: Training Loss: 0.13377274572849274 Validation Loss: 0.7204567193984985\n",
      "Epoch 7339: Training Loss: 0.13425355404615402 Validation Loss: 0.7199912667274475\n",
      "Epoch 7340: Training Loss: 0.1341144467393557 Validation Loss: 0.7203355431556702\n",
      "Epoch 7341: Training Loss: 0.13394637405872345 Validation Loss: 0.7205579876899719\n",
      "Epoch 7342: Training Loss: 0.13406875481208166 Validation Loss: 0.7208728194236755\n",
      "Epoch 7343: Training Loss: 0.13378702104091644 Validation Loss: 0.7208888530731201\n",
      "Epoch 7344: Training Loss: 0.13342941055695215 Validation Loss: 0.7206540107727051\n",
      "Epoch 7345: Training Loss: 0.1338357130686442 Validation Loss: 0.7207067012786865\n",
      "Epoch 7346: Training Loss: 0.13406772166490555 Validation Loss: 0.7207551002502441\n",
      "Epoch 7347: Training Loss: 0.13390404979387918 Validation Loss: 0.7208655476570129\n",
      "Epoch 7348: Training Loss: 0.13354896754026413 Validation Loss: 0.7208527326583862\n",
      "Epoch 7349: Training Loss: 0.13423039515813193 Validation Loss: 0.720645010471344\n",
      "Epoch 7350: Training Loss: 0.13395497699578604 Validation Loss: 0.7207474708557129\n",
      "Epoch 7351: Training Loss: 0.13365181783835092 Validation Loss: 0.7206512689590454\n",
      "Epoch 7352: Training Loss: 0.13371956845124564 Validation Loss: 0.7206059694290161\n",
      "Epoch 7353: Training Loss: 0.13370091219743094 Validation Loss: 0.7201640605926514\n",
      "Epoch 7354: Training Loss: 0.13395027071237564 Validation Loss: 0.7203502058982849\n",
      "Epoch 7355: Training Loss: 0.1339727739493052 Validation Loss: 0.720227837562561\n",
      "Epoch 7356: Training Loss: 0.1341125617424647 Validation Loss: 0.7203122973442078\n",
      "Epoch 7357: Training Loss: 0.13401790708303452 Validation Loss: 0.7207522392272949\n",
      "Epoch 7358: Training Loss: 0.13416743278503418 Validation Loss: 0.7208486795425415\n",
      "Epoch 7359: Training Loss: 0.13352855543295541 Validation Loss: 0.7208006381988525\n",
      "Epoch 7360: Training Loss: 0.1341987227400144 Validation Loss: 0.7206363081932068\n",
      "Epoch 7361: Training Loss: 0.1335247258345286 Validation Loss: 0.7206916213035583\n",
      "Epoch 7362: Training Loss: 0.13345139722029367 Validation Loss: 0.7204201221466064\n",
      "Epoch 7363: Training Loss: 0.13439973443746567 Validation Loss: 0.7205652594566345\n",
      "Epoch 7364: Training Loss: 0.1337827816605568 Validation Loss: 0.7208379507064819\n",
      "Epoch 7365: Training Loss: 0.13335494697093964 Validation Loss: 0.7209183573722839\n",
      "Epoch 7366: Training Loss: 0.1333127866188685 Validation Loss: 0.7210267782211304\n",
      "Epoch 7367: Training Loss: 0.1336936503648758 Validation Loss: 0.7209650278091431\n",
      "Epoch 7368: Training Loss: 0.13349063942829767 Validation Loss: 0.7208483815193176\n",
      "Epoch 7369: Training Loss: 0.13351349532604218 Validation Loss: 0.7206934094429016\n",
      "Epoch 7370: Training Loss: 0.13374519844849905 Validation Loss: 0.7208520174026489\n",
      "Epoch 7371: Training Loss: 0.13418467342853546 Validation Loss: 0.7207862138748169\n",
      "Epoch 7372: Training Loss: 0.13374006003141403 Validation Loss: 0.7204724550247192\n",
      "Epoch 7373: Training Loss: 0.1333960791428884 Validation Loss: 0.7202688455581665\n",
      "Epoch 7374: Training Loss: 0.13365954160690308 Validation Loss: 0.7206653356552124\n",
      "Epoch 7375: Training Loss: 0.13346386949221292 Validation Loss: 0.7204767465591431\n",
      "Epoch 7376: Training Loss: 0.1329676459232966 Validation Loss: 0.7205842733383179\n",
      "Epoch 7377: Training Loss: 0.13318769137064615 Validation Loss: 0.7208176851272583\n",
      "Epoch 7378: Training Loss: 0.13362165788809457 Validation Loss: 0.7208563685417175\n",
      "Epoch 7379: Training Loss: 0.13387772689263025 Validation Loss: 0.7211809754371643\n",
      "Epoch 7380: Training Loss: 0.13318490982055664 Validation Loss: 0.7210232019424438\n",
      "Epoch 7381: Training Loss: 0.13319354007641473 Validation Loss: 0.7211911678314209\n",
      "Epoch 7382: Training Loss: 0.13407644629478455 Validation Loss: 0.7212715148925781\n",
      "Epoch 7383: Training Loss: 0.13313296933968863 Validation Loss: 0.7212957143783569\n",
      "Epoch 7384: Training Loss: 0.13329220563173294 Validation Loss: 0.7211663126945496\n",
      "Epoch 7385: Training Loss: 0.1333511471748352 Validation Loss: 0.7207673192024231\n",
      "Epoch 7386: Training Loss: 0.13285310566425323 Validation Loss: 0.7209581136703491\n",
      "Epoch 7387: Training Loss: 0.13362510005633035 Validation Loss: 0.7208267450332642\n",
      "Epoch 7388: Training Loss: 0.13316873709360758 Validation Loss: 0.7211893796920776\n",
      "Epoch 7389: Training Loss: 0.13404221584399542 Validation Loss: 0.7213414907455444\n",
      "Epoch 7390: Training Loss: 0.13312222808599472 Validation Loss: 0.7216575145721436\n",
      "Epoch 7391: Training Loss: 0.1329692304134369 Validation Loss: 0.7214577794075012\n",
      "Epoch 7392: Training Loss: 0.1332082450389862 Validation Loss: 0.7210594415664673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7393: Training Loss: 0.13317781190077463 Validation Loss: 0.7208281755447388\n",
      "Epoch 7394: Training Loss: 0.1333605001370112 Validation Loss: 0.7208705544471741\n",
      "Epoch 7395: Training Loss: 0.1331800396243731 Validation Loss: 0.7207291722297668\n",
      "Epoch 7396: Training Loss: 0.13309961060682932 Validation Loss: 0.7207235097885132\n",
      "Epoch 7397: Training Loss: 0.13296291728814444 Validation Loss: 0.7207410931587219\n",
      "Epoch 7398: Training Loss: 0.13370591898759207 Validation Loss: 0.7209666967391968\n",
      "Epoch 7399: Training Loss: 0.13288500408331552 Validation Loss: 0.7210210561752319\n",
      "Epoch 7400: Training Loss: 0.1334253897269567 Validation Loss: 0.721015989780426\n",
      "Epoch 7401: Training Loss: 0.13303792725006738 Validation Loss: 0.7207610011100769\n",
      "Epoch 7402: Training Loss: 0.13304688533147177 Validation Loss: 0.7210728526115417\n",
      "Epoch 7403: Training Loss: 0.132796640197436 Validation Loss: 0.7209306359291077\n",
      "Epoch 7404: Training Loss: 0.13319471975167593 Validation Loss: 0.7210254669189453\n",
      "Epoch 7405: Training Loss: 0.13450990368922552 Validation Loss: 0.7215498685836792\n",
      "Epoch 7406: Training Loss: 0.13311525930960974 Validation Loss: 0.7210776209831238\n",
      "Epoch 7407: Training Loss: 0.13286199420690536 Validation Loss: 0.7209315299987793\n",
      "Epoch 7408: Training Loss: 0.1327547530333201 Validation Loss: 0.7208744883537292\n",
      "Epoch 7409: Training Loss: 0.13281042873859406 Validation Loss: 0.720905065536499\n",
      "Epoch 7410: Training Loss: 0.13273881624142328 Validation Loss: 0.7209015488624573\n",
      "Epoch 7411: Training Loss: 0.1327920804421107 Validation Loss: 0.7209928631782532\n",
      "Epoch 7412: Training Loss: 0.13295961916446686 Validation Loss: 0.7209538221359253\n",
      "Epoch 7413: Training Loss: 0.13266444702943167 Validation Loss: 0.7208823561668396\n",
      "Epoch 7414: Training Loss: 0.1329242760936419 Validation Loss: 0.7212780117988586\n",
      "Epoch 7415: Training Loss: 0.13264975448449454 Validation Loss: 0.7215545177459717\n",
      "Epoch 7416: Training Loss: 0.13325131684541702 Validation Loss: 0.7214725017547607\n",
      "Epoch 7417: Training Loss: 0.13241331527630487 Validation Loss: 0.7215657234191895\n",
      "Epoch 7418: Training Loss: 0.1326296031475067 Validation Loss: 0.7213857769966125\n",
      "Epoch 7419: Training Loss: 0.13286791741847992 Validation Loss: 0.7211523056030273\n",
      "Epoch 7420: Training Loss: 0.13262700537840524 Validation Loss: 0.7211984395980835\n",
      "Epoch 7421: Training Loss: 0.13272118816773096 Validation Loss: 0.7209658622741699\n",
      "Epoch 7422: Training Loss: 0.13259131709734598 Validation Loss: 0.7213018536567688\n",
      "Epoch 7423: Training Loss: 0.13243753214677176 Validation Loss: 0.7213826179504395\n",
      "Epoch 7424: Training Loss: 0.13247584799925485 Validation Loss: 0.7212656140327454\n",
      "Epoch 7425: Training Loss: 0.13289771477381387 Validation Loss: 0.7212523221969604\n",
      "Epoch 7426: Training Loss: 0.13257469236850739 Validation Loss: 0.7216755747795105\n",
      "Epoch 7427: Training Loss: 0.13208417097727457 Validation Loss: 0.721572756767273\n",
      "Epoch 7428: Training Loss: 0.13282904277245203 Validation Loss: 0.7214074730873108\n",
      "Epoch 7429: Training Loss: 0.13444902002811432 Validation Loss: 0.7211360931396484\n",
      "Epoch 7430: Training Loss: 0.13244892160097757 Validation Loss: 0.7209755778312683\n",
      "Epoch 7431: Training Loss: 0.1324494183063507 Validation Loss: 0.7210106253623962\n",
      "Epoch 7432: Training Loss: 0.13215124607086182 Validation Loss: 0.7212703824043274\n",
      "Epoch 7433: Training Loss: 0.13206516951322556 Validation Loss: 0.7214714288711548\n",
      "Epoch 7434: Training Loss: 0.13238097230593363 Validation Loss: 0.7214924097061157\n",
      "Epoch 7435: Training Loss: 0.1325631986061732 Validation Loss: 0.7217154502868652\n",
      "Epoch 7436: Training Loss: 0.13263334333896637 Validation Loss: 0.7218314409255981\n",
      "Epoch 7437: Training Loss: 0.1323030342658361 Validation Loss: 0.7216050028800964\n",
      "Epoch 7438: Training Loss: 0.13224647690852484 Validation Loss: 0.7214615941047668\n",
      "Epoch 7439: Training Loss: 0.132408877213796 Validation Loss: 0.721286416053772\n",
      "Epoch 7440: Training Loss: 0.1323351338505745 Validation Loss: 0.7212423086166382\n",
      "Epoch 7441: Training Loss: 0.13286034762859344 Validation Loss: 0.720869243144989\n",
      "Epoch 7442: Training Loss: 0.13273325562477112 Validation Loss: 0.7210567593574524\n",
      "Epoch 7443: Training Loss: 0.13211117933193842 Validation Loss: 0.7211833596229553\n",
      "Epoch 7444: Training Loss: 0.13221029688914618 Validation Loss: 0.7212277054786682\n",
      "Epoch 7445: Training Loss: 0.13256882379452387 Validation Loss: 0.7215059995651245\n",
      "Epoch 7446: Training Loss: 0.13217420130968094 Validation Loss: 0.7215372920036316\n",
      "Epoch 7447: Training Loss: 0.1320803090929985 Validation Loss: 0.7214570045471191\n",
      "Epoch 7448: Training Loss: 0.13283097992340723 Validation Loss: 0.7210811972618103\n",
      "Epoch 7449: Training Loss: 0.13211881120999655 Validation Loss: 0.721189022064209\n",
      "Epoch 7450: Training Loss: 0.13271202643712363 Validation Loss: 0.7215301394462585\n",
      "Epoch 7451: Training Loss: 0.13186909506718317 Validation Loss: 0.7214966416358948\n",
      "Epoch 7452: Training Loss: 0.1320519099632899 Validation Loss: 0.7215242385864258\n",
      "Epoch 7453: Training Loss: 0.13290186723073324 Validation Loss: 0.7217158079147339\n",
      "Epoch 7454: Training Loss: 0.13227547953526178 Validation Loss: 0.721195638179779\n",
      "Epoch 7455: Training Loss: 0.13224685688813528 Validation Loss: 0.7211212515830994\n",
      "Epoch 7456: Training Loss: 0.13202300171057382 Validation Loss: 0.7209856510162354\n",
      "Epoch 7457: Training Loss: 0.1319553256034851 Validation Loss: 0.7210296988487244\n",
      "Epoch 7458: Training Loss: 0.13327274471521378 Validation Loss: 0.7212668061256409\n",
      "Epoch 7459: Training Loss: 0.1322072148323059 Validation Loss: 0.721442461013794\n",
      "Epoch 7460: Training Loss: 0.13222873707612356 Validation Loss: 0.7217333316802979\n",
      "Epoch 7461: Training Loss: 0.13199681043624878 Validation Loss: 0.7216889262199402\n",
      "Epoch 7462: Training Loss: 0.13193675875663757 Validation Loss: 0.721707820892334\n",
      "Epoch 7463: Training Loss: 0.13322973002990088 Validation Loss: 0.7219710350036621\n",
      "Epoch 7464: Training Loss: 0.13202062994241714 Validation Loss: 0.7217139005661011\n",
      "Epoch 7465: Training Loss: 0.13187703986962637 Validation Loss: 0.7217801809310913\n",
      "Epoch 7466: Training Loss: 0.1319991648197174 Validation Loss: 0.7214486598968506\n",
      "Epoch 7467: Training Loss: 0.1317622239391009 Validation Loss: 0.721233606338501\n",
      "Epoch 7468: Training Loss: 0.1322919229666392 Validation Loss: 0.7214078903198242\n",
      "Epoch 7469: Training Loss: 0.13201370338598886 Validation Loss: 0.7215840816497803\n",
      "Epoch 7470: Training Loss: 0.13213753203550974 Validation Loss: 0.7220025062561035\n",
      "Epoch 7471: Training Loss: 0.1321272278825442 Validation Loss: 0.7217830419540405\n",
      "Epoch 7472: Training Loss: 0.13175994902849197 Validation Loss: 0.7214227318763733\n",
      "Epoch 7473: Training Loss: 0.1312962770462036 Validation Loss: 0.7212960720062256\n",
      "Epoch 7474: Training Loss: 0.13196383913358053 Validation Loss: 0.721330463886261\n",
      "Epoch 7475: Training Loss: 0.13178139428297678 Validation Loss: 0.7213106751441956\n",
      "Epoch 7476: Training Loss: 0.1321262369553248 Validation Loss: 0.7213407158851624\n",
      "Epoch 7477: Training Loss: 0.1315826972325643 Validation Loss: 0.721384584903717\n",
      "Epoch 7478: Training Loss: 0.13191610078016916 Validation Loss: 0.721569299697876\n",
      "Epoch 7479: Training Loss: 0.1315919285019239 Validation Loss: 0.7216225266456604\n",
      "Epoch 7480: Training Loss: 0.13204161077737808 Validation Loss: 0.7215983867645264\n",
      "Epoch 7481: Training Loss: 0.13184776405493417 Validation Loss: 0.7216907739639282\n",
      "Epoch 7482: Training Loss: 0.131553016602993 Validation Loss: 0.7213232517242432\n",
      "Epoch 7483: Training Loss: 0.13153490920861563 Validation Loss: 0.7214394807815552\n",
      "Epoch 7484: Training Loss: 0.13209580381711325 Validation Loss: 0.7213149666786194\n",
      "Epoch 7485: Training Loss: 0.13115240633487701 Validation Loss: 0.7213634252548218\n",
      "Epoch 7486: Training Loss: 0.1319598282376925 Validation Loss: 0.7214844226837158\n",
      "Epoch 7487: Training Loss: 0.13139177858829498 Validation Loss: 0.7219467163085938\n",
      "Epoch 7488: Training Loss: 0.1316666677594185 Validation Loss: 0.7223461866378784\n",
      "Epoch 7489: Training Loss: 0.13178415844837824 Validation Loss: 0.7222017049789429\n",
      "Epoch 7490: Training Loss: 0.13299775123596191 Validation Loss: 0.7217144966125488\n",
      "Epoch 7491: Training Loss: 0.13156633079051971 Validation Loss: 0.7218090891838074\n",
      "Epoch 7492: Training Loss: 0.1321249157190323 Validation Loss: 0.7217699289321899\n",
      "Epoch 7493: Training Loss: 0.13208091259002686 Validation Loss: 0.7219945788383484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7494: Training Loss: 0.1316851650675138 Validation Loss: 0.7220537662506104\n",
      "Epoch 7495: Training Loss: 0.13221216450134912 Validation Loss: 0.7219664454460144\n",
      "Epoch 7496: Training Loss: 0.13155647615591684 Validation Loss: 0.722061812877655\n",
      "Epoch 7497: Training Loss: 0.13139310479164124 Validation Loss: 0.7219104170799255\n",
      "Epoch 7498: Training Loss: 0.13130341470241547 Validation Loss: 0.7219852209091187\n",
      "Epoch 7499: Training Loss: 0.13143161435921988 Validation Loss: 0.7221940159797668\n",
      "Epoch 7500: Training Loss: 0.13101105888684592 Validation Loss: 0.7215418219566345\n",
      "Epoch 7501: Training Loss: 0.13138295461734137 Validation Loss: 0.7213237881660461\n",
      "Epoch 7502: Training Loss: 0.13113727420568466 Validation Loss: 0.7215440273284912\n",
      "Epoch 7503: Training Loss: 0.13183804849783579 Validation Loss: 0.7215511202812195\n",
      "Epoch 7504: Training Loss: 0.1313839703798294 Validation Loss: 0.7219052314758301\n",
      "Epoch 7505: Training Loss: 0.13120276729265848 Validation Loss: 0.721950113773346\n",
      "Epoch 7506: Training Loss: 0.1309723233183225 Validation Loss: 0.7219604253768921\n",
      "Epoch 7507: Training Loss: 0.13165135184923807 Validation Loss: 0.7219110131263733\n",
      "Epoch 7508: Training Loss: 0.13105095674594244 Validation Loss: 0.7218230962753296\n",
      "Epoch 7509: Training Loss: 0.13133079061905542 Validation Loss: 0.7218775749206543\n",
      "Epoch 7510: Training Loss: 0.13105822106202444 Validation Loss: 0.721924364566803\n",
      "Epoch 7511: Training Loss: 0.1311071738600731 Validation Loss: 0.7219404578208923\n",
      "Epoch 7512: Training Loss: 0.13133179396390915 Validation Loss: 0.721916675567627\n",
      "Epoch 7513: Training Loss: 0.13123332460721335 Validation Loss: 0.721940279006958\n",
      "Epoch 7514: Training Loss: 0.13154866049687067 Validation Loss: 0.721886932849884\n",
      "Epoch 7515: Training Loss: 0.1313791275024414 Validation Loss: 0.7219371795654297\n",
      "Epoch 7516: Training Loss: 0.1320775349934896 Validation Loss: 0.7218210697174072\n",
      "Epoch 7517: Training Loss: 0.13140225410461426 Validation Loss: 0.7220979332923889\n",
      "Epoch 7518: Training Loss: 0.1310440053542455 Validation Loss: 0.7223414182662964\n",
      "Epoch 7519: Training Loss: 0.1314592808485031 Validation Loss: 0.722433865070343\n",
      "Epoch 7520: Training Loss: 0.13115128129720688 Validation Loss: 0.7219388484954834\n",
      "Epoch 7521: Training Loss: 0.13093803822994232 Validation Loss: 0.7216054797172546\n",
      "Epoch 7522: Training Loss: 0.13100322087605795 Validation Loss: 0.7219027876853943\n",
      "Epoch 7523: Training Loss: 0.13164184987545013 Validation Loss: 0.7215620279312134\n",
      "Epoch 7524: Training Loss: 0.13142184168100357 Validation Loss: 0.7220757007598877\n",
      "Epoch 7525: Training Loss: 0.13092472900946936 Validation Loss: 0.7221176028251648\n",
      "Epoch 7526: Training Loss: 0.13135810444752374 Validation Loss: 0.72226881980896\n",
      "Epoch 7527: Training Loss: 0.13147447258234024 Validation Loss: 0.7221975326538086\n",
      "Epoch 7528: Training Loss: 0.13138354818026224 Validation Loss: 0.7223517298698425\n",
      "Epoch 7529: Training Loss: 0.13092525800069174 Validation Loss: 0.7223938703536987\n",
      "Epoch 7530: Training Loss: 0.13093169530232748 Validation Loss: 0.7218816876411438\n",
      "Epoch 7531: Training Loss: 0.13097268839677176 Validation Loss: 0.7214613556861877\n",
      "Epoch 7532: Training Loss: 0.13107178111871085 Validation Loss: 0.7214664816856384\n",
      "Epoch 7533: Training Loss: 0.13091863691806793 Validation Loss: 0.721683919429779\n",
      "Epoch 7534: Training Loss: 0.13295155515273413 Validation Loss: 0.7223717570304871\n",
      "Epoch 7535: Training Loss: 0.13097239037354788 Validation Loss: 0.7226359248161316\n",
      "Epoch 7536: Training Loss: 0.13060404856999716 Validation Loss: 0.722942054271698\n",
      "Epoch 7537: Training Loss: 0.13112854212522507 Validation Loss: 0.7222708463668823\n",
      "Epoch 7538: Training Loss: 0.13105637828509012 Validation Loss: 0.721966028213501\n",
      "Epoch 7539: Training Loss: 0.13079034040371576 Validation Loss: 0.7221609354019165\n",
      "Epoch 7540: Training Loss: 0.13059941679239273 Validation Loss: 0.7221065163612366\n",
      "Epoch 7541: Training Loss: 0.13074749459822974 Validation Loss: 0.7221805453300476\n",
      "Epoch 7542: Training Loss: 0.1306206782658895 Validation Loss: 0.7222105860710144\n",
      "Epoch 7543: Training Loss: 0.1308577706416448 Validation Loss: 0.7223657965660095\n",
      "Epoch 7544: Training Loss: 0.13078265637159348 Validation Loss: 0.7224507331848145\n",
      "Epoch 7545: Training Loss: 0.13080031424760818 Validation Loss: 0.7222608923912048\n",
      "Epoch 7546: Training Loss: 0.13056729237238565 Validation Loss: 0.7222577333450317\n",
      "Epoch 7547: Training Loss: 0.1321628838777542 Validation Loss: 0.7227973341941833\n",
      "Epoch 7548: Training Loss: 0.1308800553282102 Validation Loss: 0.7228121161460876\n",
      "Epoch 7549: Training Loss: 0.1307582507530848 Validation Loss: 0.7220168113708496\n",
      "Epoch 7550: Training Loss: 0.13025829941034317 Validation Loss: 0.7216234803199768\n",
      "Epoch 7551: Training Loss: 0.1305921028057734 Validation Loss: 0.7216705083847046\n",
      "Epoch 7552: Training Loss: 0.13064895321925482 Validation Loss: 0.7219386696815491\n",
      "Epoch 7553: Training Loss: 0.13200747221708298 Validation Loss: 0.7224273085594177\n",
      "Epoch 7554: Training Loss: 0.13090471674998602 Validation Loss: 0.7224242687225342\n",
      "Epoch 7555: Training Loss: 0.13063556204239526 Validation Loss: 0.7224965691566467\n",
      "Epoch 7556: Training Loss: 0.13003757844368616 Validation Loss: 0.7220252752304077\n",
      "Epoch 7557: Training Loss: 0.13074392825365067 Validation Loss: 0.7218250632286072\n",
      "Epoch 7558: Training Loss: 0.13113951434691748 Validation Loss: 0.7219020128250122\n",
      "Epoch 7559: Training Loss: 0.1307533010840416 Validation Loss: 0.722104549407959\n",
      "Epoch 7560: Training Loss: 0.13093807051579157 Validation Loss: 0.7224918007850647\n",
      "Epoch 7561: Training Loss: 0.13126279910405478 Validation Loss: 0.723040759563446\n",
      "Epoch 7562: Training Loss: 0.13042867680390677 Validation Loss: 0.7227506637573242\n",
      "Epoch 7563: Training Loss: 0.1304504598180453 Validation Loss: 0.7225337624549866\n",
      "Epoch 7564: Training Loss: 0.1306755617260933 Validation Loss: 0.7222862839698792\n",
      "Epoch 7565: Training Loss: 0.13081195702155432 Validation Loss: 0.7218405604362488\n",
      "Epoch 7566: Training Loss: 0.13022549698750177 Validation Loss: 0.7218179106712341\n",
      "Epoch 7567: Training Loss: 0.13025803864002228 Validation Loss: 0.7218696475028992\n",
      "Epoch 7568: Training Loss: 0.13027103741963705 Validation Loss: 0.7219929695129395\n",
      "Epoch 7569: Training Loss: 0.13034793982903162 Validation Loss: 0.7223780751228333\n",
      "Epoch 7570: Training Loss: 0.1303969845175743 Validation Loss: 0.7225953936576843\n",
      "Epoch 7571: Training Loss: 0.1304007669289907 Validation Loss: 0.7224636077880859\n",
      "Epoch 7572: Training Loss: 0.13003854950269064 Validation Loss: 0.7222030162811279\n",
      "Epoch 7573: Training Loss: 0.13063373665014902 Validation Loss: 0.721905529499054\n",
      "Epoch 7574: Training Loss: 0.13070803632338843 Validation Loss: 0.7223749756813049\n",
      "Epoch 7575: Training Loss: 0.12990993012984595 Validation Loss: 0.722571074962616\n",
      "Epoch 7576: Training Loss: 0.13028375804424286 Validation Loss: 0.7225229144096375\n",
      "Epoch 7577: Training Loss: 0.13007833560307822 Validation Loss: 0.722641110420227\n",
      "Epoch 7578: Training Loss: 0.13038446505864462 Validation Loss: 0.7227033376693726\n",
      "Epoch 7579: Training Loss: 0.1301277975241343 Validation Loss: 0.7224777340888977\n",
      "Epoch 7580: Training Loss: 0.13029309610525766 Validation Loss: 0.7220467925071716\n",
      "Epoch 7581: Training Loss: 0.13011989494164786 Validation Loss: 0.7218765020370483\n",
      "Epoch 7582: Training Loss: 0.1311351160208384 Validation Loss: 0.7223355174064636\n",
      "Epoch 7583: Training Loss: 0.13015548388163248 Validation Loss: 0.7225536704063416\n",
      "Epoch 7584: Training Loss: 0.13021483272314072 Validation Loss: 0.7229008674621582\n",
      "Epoch 7585: Training Loss: 0.13059662530819574 Validation Loss: 0.7229709029197693\n",
      "Epoch 7586: Training Loss: 0.12979916234811148 Validation Loss: 0.722519040107727\n",
      "Epoch 7587: Training Loss: 0.1299223725994428 Validation Loss: 0.7225404381752014\n",
      "Epoch 7588: Training Loss: 0.13044671714305878 Validation Loss: 0.7221203446388245\n",
      "Epoch 7589: Training Loss: 0.12999224662780762 Validation Loss: 0.72235506772995\n",
      "Epoch 7590: Training Loss: 0.13004431128501892 Validation Loss: 0.7224845290184021\n",
      "Epoch 7591: Training Loss: 0.13023201624552408 Validation Loss: 0.7227095365524292\n",
      "Epoch 7592: Training Loss: 0.13007594148317972 Validation Loss: 0.7228002548217773\n",
      "Epoch 7593: Training Loss: 0.13027251760164896 Validation Loss: 0.7230709791183472\n",
      "Epoch 7594: Training Loss: 0.12978778779506683 Validation Loss: 0.7228025197982788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7595: Training Loss: 0.1306267331043879 Validation Loss: 0.7230187654495239\n",
      "Epoch 7596: Training Loss: 0.13009759535392126 Validation Loss: 0.7227399349212646\n",
      "Epoch 7597: Training Loss: 0.1298097645243009 Validation Loss: 0.7224065065383911\n",
      "Epoch 7598: Training Loss: 0.1292151709397634 Validation Loss: 0.7224041223526001\n",
      "Epoch 7599: Training Loss: 0.1300402134656906 Validation Loss: 0.7223479151725769\n",
      "Epoch 7600: Training Loss: 0.13039277493953705 Validation Loss: 0.7222421765327454\n",
      "Epoch 7601: Training Loss: 0.12978249291578928 Validation Loss: 0.7221403121948242\n",
      "Epoch 7602: Training Loss: 0.12993478775024414 Validation Loss: 0.7220844030380249\n",
      "Epoch 7603: Training Loss: 0.1299134741226832 Validation Loss: 0.722221851348877\n",
      "Epoch 7604: Training Loss: 0.13052320728699365 Validation Loss: 0.7227349281311035\n",
      "Epoch 7605: Training Loss: 0.12996685256560644 Validation Loss: 0.7228377461433411\n",
      "Epoch 7606: Training Loss: 0.13016091038783392 Validation Loss: 0.7231868505477905\n",
      "Epoch 7607: Training Loss: 0.13046093781789145 Validation Loss: 0.7228413820266724\n",
      "Epoch 7608: Training Loss: 0.12986544768015543 Validation Loss: 0.7225281596183777\n",
      "Epoch 7609: Training Loss: 0.12970422953367233 Validation Loss: 0.7225592136383057\n",
      "Epoch 7610: Training Loss: 0.12971599896748862 Validation Loss: 0.7229519486427307\n",
      "Epoch 7611: Training Loss: 0.1299209569891294 Validation Loss: 0.7232457995414734\n",
      "Epoch 7612: Training Loss: 0.12974235912164053 Validation Loss: 0.723781168460846\n",
      "Epoch 7613: Training Loss: 0.12966458251078924 Validation Loss: 0.7233514785766602\n",
      "Epoch 7614: Training Loss: 0.12971192101637521 Validation Loss: 0.7231481075286865\n",
      "Epoch 7615: Training Loss: 0.12962057193120322 Validation Loss: 0.7226951718330383\n",
      "Epoch 7616: Training Loss: 0.1292171503106753 Validation Loss: 0.7227240800857544\n",
      "Epoch 7617: Training Loss: 0.12948005398114523 Validation Loss: 0.7227630019187927\n",
      "Epoch 7618: Training Loss: 0.12957302232583365 Validation Loss: 0.7228546142578125\n",
      "Epoch 7619: Training Loss: 0.12961003929376602 Validation Loss: 0.7225624918937683\n",
      "Epoch 7620: Training Loss: 0.13042497634887695 Validation Loss: 0.7228643298149109\n",
      "Epoch 7621: Training Loss: 0.12999003380537033 Validation Loss: 0.7231740951538086\n",
      "Epoch 7622: Training Loss: 0.13035858670870462 Validation Loss: 0.7229219675064087\n",
      "Epoch 7623: Training Loss: 0.12992453823486963 Validation Loss: 0.7226591110229492\n",
      "Epoch 7624: Training Loss: 0.1300534208615621 Validation Loss: 0.7225825190544128\n",
      "Epoch 7625: Training Loss: 0.13029739509026209 Validation Loss: 0.7225620150566101\n",
      "Epoch 7626: Training Loss: 0.12960838278134665 Validation Loss: 0.7224007248878479\n",
      "Epoch 7627: Training Loss: 0.12951884915431341 Validation Loss: 0.7226543426513672\n",
      "Epoch 7628: Training Loss: 0.12973346809546152 Validation Loss: 0.7229466438293457\n",
      "Epoch 7629: Training Loss: 0.12908431390921274 Validation Loss: 0.72357577085495\n",
      "Epoch 7630: Training Loss: 0.1301368623971939 Validation Loss: 0.7233020067214966\n",
      "Epoch 7631: Training Loss: 0.13001137475172678 Validation Loss: 0.7231700420379639\n",
      "Epoch 7632: Training Loss: 0.12962170938650766 Validation Loss: 0.7230817675590515\n",
      "Epoch 7633: Training Loss: 0.13014240562915802 Validation Loss: 0.7226487994194031\n",
      "Epoch 7634: Training Loss: 0.1300200472275416 Validation Loss: 0.7224885821342468\n",
      "Epoch 7635: Training Loss: 0.1298357198635737 Validation Loss: 0.7227528095245361\n",
      "Epoch 7636: Training Loss: 0.13019314408302307 Validation Loss: 0.7228860259056091\n",
      "Epoch 7637: Training Loss: 0.12924838811159134 Validation Loss: 0.7230827808380127\n",
      "Epoch 7638: Training Loss: 0.12996916472911835 Validation Loss: 0.7230061292648315\n",
      "Epoch 7639: Training Loss: 0.12958189596732458 Validation Loss: 0.7231498956680298\n",
      "Epoch 7640: Training Loss: 0.12903335193792978 Validation Loss: 0.7228803038597107\n",
      "Epoch 7641: Training Loss: 0.12948912878831229 Validation Loss: 0.7228755950927734\n",
      "Epoch 7642: Training Loss: 0.12920981148878732 Validation Loss: 0.7230255007743835\n",
      "Epoch 7643: Training Loss: 0.12950611859560013 Validation Loss: 0.7227866053581238\n",
      "Epoch 7644: Training Loss: 0.1298134004076322 Validation Loss: 0.7232083678245544\n",
      "Epoch 7645: Training Loss: 0.13020716607570648 Validation Loss: 0.7237352132797241\n",
      "Epoch 7646: Training Loss: 0.12914475550254187 Validation Loss: 0.7236565947532654\n",
      "Epoch 7647: Training Loss: 0.12919569512208304 Validation Loss: 0.7233109474182129\n",
      "Epoch 7648: Training Loss: 0.12913407882054648 Validation Loss: 0.7227201461791992\n",
      "Epoch 7649: Training Loss: 0.12897430111964545 Validation Loss: 0.7225881814956665\n",
      "Epoch 7650: Training Loss: 0.1291939690709114 Validation Loss: 0.7228127121925354\n",
      "Epoch 7651: Training Loss: 0.12944626559813818 Validation Loss: 0.7234159111976624\n",
      "Epoch 7652: Training Loss: 0.12902684261401495 Validation Loss: 0.7236610651016235\n",
      "Epoch 7653: Training Loss: 0.12969715148210526 Validation Loss: 0.7234653234481812\n",
      "Epoch 7654: Training Loss: 0.12961149712403616 Validation Loss: 0.7234494090080261\n",
      "Epoch 7655: Training Loss: 0.1290232539176941 Validation Loss: 0.7233136892318726\n",
      "Epoch 7656: Training Loss: 0.1290315662821134 Validation Loss: 0.7232204079627991\n",
      "Epoch 7657: Training Loss: 0.12885338068008423 Validation Loss: 0.7231624722480774\n",
      "Epoch 7658: Training Loss: 0.1289854496717453 Validation Loss: 0.7235713601112366\n",
      "Epoch 7659: Training Loss: 0.13015940537055334 Validation Loss: 0.7234625816345215\n",
      "Epoch 7660: Training Loss: 0.12894665946563086 Validation Loss: 0.7234876751899719\n",
      "Epoch 7661: Training Loss: 0.1288974235455195 Validation Loss: 0.7233890295028687\n",
      "Epoch 7662: Training Loss: 0.12894083807865778 Validation Loss: 0.7234770059585571\n",
      "Epoch 7663: Training Loss: 0.12934381266434988 Validation Loss: 0.7232261300086975\n",
      "Epoch 7664: Training Loss: 0.12885992725690207 Validation Loss: 0.7232102155685425\n",
      "Epoch 7665: Training Loss: 0.1288516347606977 Validation Loss: 0.7232749462127686\n",
      "Epoch 7666: Training Loss: 0.13008158405621847 Validation Loss: 0.7230878472328186\n",
      "Epoch 7667: Training Loss: 0.12901406983534494 Validation Loss: 0.7233849167823792\n",
      "Epoch 7668: Training Loss: 0.12877907852331796 Validation Loss: 0.7235346436500549\n",
      "Epoch 7669: Training Loss: 0.1294229452808698 Validation Loss: 0.7235087752342224\n",
      "Epoch 7670: Training Loss: 0.12875318030516306 Validation Loss: 0.7230114340782166\n",
      "Epoch 7671: Training Loss: 0.12890922526518503 Validation Loss: 0.722913920879364\n",
      "Epoch 7672: Training Loss: 0.1291015992561976 Validation Loss: 0.7229973673820496\n",
      "Epoch 7673: Training Loss: 0.12863194942474365 Validation Loss: 0.723097562789917\n",
      "Epoch 7674: Training Loss: 0.12946642438570657 Validation Loss: 0.7230733036994934\n",
      "Epoch 7675: Training Loss: 0.12855982532103857 Validation Loss: 0.7230189442634583\n",
      "Epoch 7676: Training Loss: 0.12963489691416422 Validation Loss: 0.7230792045593262\n",
      "Epoch 7677: Training Loss: 0.12881459792455038 Validation Loss: 0.7231584787368774\n",
      "Epoch 7678: Training Loss: 0.12967969725529352 Validation Loss: 0.72365403175354\n",
      "Epoch 7679: Training Loss: 0.1289099156856537 Validation Loss: 0.7234709858894348\n",
      "Epoch 7680: Training Loss: 0.12853866318861643 Validation Loss: 0.7236276865005493\n",
      "Epoch 7681: Training Loss: 0.12791427224874496 Validation Loss: 0.7234829068183899\n",
      "Epoch 7682: Training Loss: 0.12852870176235834 Validation Loss: 0.7236371636390686\n",
      "Epoch 7683: Training Loss: 0.12859476854403815 Validation Loss: 0.7236628532409668\n",
      "Epoch 7684: Training Loss: 0.12863033016522726 Validation Loss: 0.7239733338356018\n",
      "Epoch 7685: Training Loss: 0.12965658555428186 Validation Loss: 0.7236748337745667\n",
      "Epoch 7686: Training Loss: 0.1297376255194346 Validation Loss: 0.7238339185714722\n",
      "Epoch 7687: Training Loss: 0.1282836322983106 Validation Loss: 0.7239043116569519\n",
      "Epoch 7688: Training Loss: 0.128476952513059 Validation Loss: 0.7237565517425537\n",
      "Epoch 7689: Training Loss: 0.1284176508585612 Validation Loss: 0.7235099077224731\n",
      "Epoch 7690: Training Loss: 0.1294973318775495 Validation Loss: 0.7233781814575195\n",
      "Epoch 7691: Training Loss: 0.1283324882388115 Validation Loss: 0.7236153483390808\n",
      "Epoch 7692: Training Loss: 0.12868621945381165 Validation Loss: 0.7234167456626892\n",
      "Epoch 7693: Training Loss: 0.12836877504984537 Validation Loss: 0.7236313223838806\n",
      "Epoch 7694: Training Loss: 0.12879562129577002 Validation Loss: 0.7232688665390015\n",
      "Epoch 7695: Training Loss: 0.12920468300580978 Validation Loss: 0.7234867215156555\n",
      "Epoch 7696: Training Loss: 0.12892343600591025 Validation Loss: 0.7237192988395691\n",
      "Epoch 7697: Training Loss: 0.12857726961374283 Validation Loss: 0.7234554290771484\n",
      "Epoch 7698: Training Loss: 0.12908311188220978 Validation Loss: 0.7235555052757263\n",
      "Epoch 7699: Training Loss: 0.12858829398949942 Validation Loss: 0.7234089970588684\n",
      "Epoch 7700: Training Loss: 0.12851609041293463 Validation Loss: 0.7235704660415649\n",
      "Epoch 7701: Training Loss: 0.12865774830182394 Validation Loss: 0.7235743999481201\n",
      "Epoch 7702: Training Loss: 0.12846432129542032 Validation Loss: 0.7234054803848267\n",
      "Epoch 7703: Training Loss: 0.12822861472765604 Validation Loss: 0.7233444452285767\n",
      "Epoch 7704: Training Loss: 0.1290802831451098 Validation Loss: 0.7234142422676086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7705: Training Loss: 0.1284388725956281 Validation Loss: 0.7236686944961548\n",
      "Epoch 7706: Training Loss: 0.1282473752895991 Validation Loss: 0.723579466342926\n",
      "Epoch 7707: Training Loss: 0.1283055916428566 Validation Loss: 0.7236298322677612\n",
      "Epoch 7708: Training Loss: 0.12860341866811117 Validation Loss: 0.7237887978553772\n",
      "Epoch 7709: Training Loss: 0.12882989893356958 Validation Loss: 0.7238137125968933\n",
      "Epoch 7710: Training Loss: 0.12825969606637955 Validation Loss: 0.7238706350326538\n",
      "Epoch 7711: Training Loss: 0.12710842986901602 Validation Loss: 0.7234276533126831\n",
      "Epoch 7712: Training Loss: 0.12818373243014017 Validation Loss: 0.7234004139900208\n",
      "Epoch 7713: Training Loss: 0.12879494080940881 Validation Loss: 0.7236102223396301\n",
      "Epoch 7714: Training Loss: 0.12858378638823828 Validation Loss: 0.7236872911453247\n",
      "Epoch 7715: Training Loss: 0.12845411151647568 Validation Loss: 0.723849356174469\n",
      "Epoch 7716: Training Loss: 0.12814244627952576 Validation Loss: 0.7238761782646179\n",
      "Epoch 7717: Training Loss: 0.12801451484362283 Validation Loss: 0.7237548828125\n",
      "Epoch 7718: Training Loss: 0.128120223681132 Validation Loss: 0.723609983921051\n",
      "Epoch 7719: Training Loss: 0.12838434427976608 Validation Loss: 0.723671019077301\n",
      "Epoch 7720: Training Loss: 0.1282170663277308 Validation Loss: 0.7238740921020508\n",
      "Epoch 7721: Training Loss: 0.12781173487504324 Validation Loss: 0.723761260509491\n",
      "Epoch 7722: Training Loss: 0.12801853070656458 Validation Loss: 0.7234318852424622\n",
      "Epoch 7723: Training Loss: 0.1280715341369311 Validation Loss: 0.7236919403076172\n",
      "Epoch 7724: Training Loss: 0.12826208025217056 Validation Loss: 0.7237849831581116\n",
      "Epoch 7725: Training Loss: 0.12825728952884674 Validation Loss: 0.723793089389801\n",
      "Epoch 7726: Training Loss: 0.1283863733212153 Validation Loss: 0.723963737487793\n",
      "Epoch 7727: Training Loss: 0.1280040442943573 Validation Loss: 0.7242283821105957\n",
      "Epoch 7728: Training Loss: 0.12804009268681207 Validation Loss: 0.723861038684845\n",
      "Epoch 7729: Training Loss: 0.12815206001202264 Validation Loss: 0.7236654758453369\n",
      "Epoch 7730: Training Loss: 0.12794673442840576 Validation Loss: 0.7237709760665894\n",
      "Epoch 7731: Training Loss: 0.12817818174759546 Validation Loss: 0.7238106727600098\n",
      "Epoch 7732: Training Loss: 0.1283674662311872 Validation Loss: 0.724015474319458\n",
      "Epoch 7733: Training Loss: 0.12734454373518625 Validation Loss: 0.724057674407959\n",
      "Epoch 7734: Training Loss: 0.12808701395988464 Validation Loss: 0.7243629693984985\n",
      "Epoch 7735: Training Loss: 0.12799722204605737 Validation Loss: 0.7240079641342163\n",
      "Epoch 7736: Training Loss: 0.1277667631705602 Validation Loss: 0.7237336039543152\n",
      "Epoch 7737: Training Loss: 0.12777618815501532 Validation Loss: 0.7234582304954529\n",
      "Epoch 7738: Training Loss: 0.12795106818278631 Validation Loss: 0.7236467003822327\n",
      "Epoch 7739: Training Loss: 0.12869306902090707 Validation Loss: 0.7239903807640076\n",
      "Epoch 7740: Training Loss: 0.12793021400769553 Validation Loss: 0.7236876487731934\n",
      "Epoch 7741: Training Loss: 0.12922491878271103 Validation Loss: 0.7236303091049194\n",
      "Epoch 7742: Training Loss: 0.12762772043546042 Validation Loss: 0.7242316603660583\n",
      "Epoch 7743: Training Loss: 0.12779074162244797 Validation Loss: 0.7241004109382629\n",
      "Epoch 7744: Training Loss: 0.12753464529911676 Validation Loss: 0.7241386771202087\n",
      "Epoch 7745: Training Loss: 0.12794036666552225 Validation Loss: 0.7239285707473755\n",
      "Epoch 7746: Training Loss: 0.1277730961640676 Validation Loss: 0.7239324450492859\n",
      "Epoch 7747: Training Loss: 0.12882931530475616 Validation Loss: 0.7241421937942505\n",
      "Epoch 7748: Training Loss: 0.1277169237534205 Validation Loss: 0.7244870662689209\n",
      "Epoch 7749: Training Loss: 0.12766755123933157 Validation Loss: 0.7242240905761719\n",
      "Epoch 7750: Training Loss: 0.12784764667352042 Validation Loss: 0.7242541909217834\n",
      "Epoch 7751: Training Loss: 0.1277429461479187 Validation Loss: 0.7242471575737\n",
      "Epoch 7752: Training Loss: 0.12818257013956705 Validation Loss: 0.7243092060089111\n",
      "Epoch 7753: Training Loss: 0.12769105782111487 Validation Loss: 0.7241547107696533\n",
      "Epoch 7754: Training Loss: 0.12787613769372305 Validation Loss: 0.7239420413970947\n",
      "Epoch 7755: Training Loss: 0.12755430738131204 Validation Loss: 0.7240480780601501\n",
      "Epoch 7756: Training Loss: 0.12776844700177512 Validation Loss: 0.7243481278419495\n",
      "Epoch 7757: Training Loss: 0.12793557594219843 Validation Loss: 0.72401362657547\n",
      "Epoch 7758: Training Loss: 0.12743379175662994 Validation Loss: 0.7239680886268616\n",
      "Epoch 7759: Training Loss: 0.12756266196568808 Validation Loss: 0.7237284779548645\n",
      "Epoch 7760: Training Loss: 0.12741228441397348 Validation Loss: 0.7239390015602112\n",
      "Epoch 7761: Training Loss: 0.12743433813254038 Validation Loss: 0.724143385887146\n",
      "Epoch 7762: Training Loss: 0.12742103884617487 Validation Loss: 0.7242115139961243\n",
      "Epoch 7763: Training Loss: 0.12758538872003555 Validation Loss: 0.7240689992904663\n",
      "Epoch 7764: Training Loss: 0.127434770266215 Validation Loss: 0.7240304350852966\n",
      "Epoch 7765: Training Loss: 0.1275788719455401 Validation Loss: 0.7240123152732849\n",
      "Epoch 7766: Training Loss: 0.12746396412452063 Validation Loss: 0.7240594029426575\n",
      "Epoch 7767: Training Loss: 0.12753018736839294 Validation Loss: 0.7244548201560974\n",
      "Epoch 7768: Training Loss: 0.12714646756649017 Validation Loss: 0.7244717478752136\n",
      "Epoch 7769: Training Loss: 0.12726822247107825 Validation Loss: 0.724441409111023\n",
      "Epoch 7770: Training Loss: 0.1273330102364222 Validation Loss: 0.7242081761360168\n",
      "Epoch 7771: Training Loss: 0.12727210422356924 Validation Loss: 0.7242612242698669\n",
      "Epoch 7772: Training Loss: 0.1281999573111534 Validation Loss: 0.724391520023346\n",
      "Epoch 7773: Training Loss: 0.12732422103484473 Validation Loss: 0.7243954539299011\n",
      "Epoch 7774: Training Loss: 0.128945991396904 Validation Loss: 0.7237457036972046\n",
      "Epoch 7775: Training Loss: 0.12735178818305334 Validation Loss: 0.7235638499259949\n",
      "Epoch 7776: Training Loss: 0.12741264204184213 Validation Loss: 0.7239192724227905\n",
      "Epoch 7777: Training Loss: 0.1274303843577703 Validation Loss: 0.7240216732025146\n",
      "Epoch 7778: Training Loss: 0.12788612892230353 Validation Loss: 0.7246074080467224\n",
      "Epoch 7779: Training Loss: 0.12721799314022064 Validation Loss: 0.7244580984115601\n",
      "Epoch 7780: Training Loss: 0.12765581905841827 Validation Loss: 0.7243620157241821\n",
      "Epoch 7781: Training Loss: 0.12767688433329263 Validation Loss: 0.7241530418395996\n",
      "Epoch 7782: Training Loss: 0.12726387629906336 Validation Loss: 0.7242029309272766\n",
      "Epoch 7783: Training Loss: 0.12723369151353836 Validation Loss: 0.7238948345184326\n",
      "Epoch 7784: Training Loss: 0.12738990286986032 Validation Loss: 0.7242323756217957\n",
      "Epoch 7785: Training Loss: 0.12769166131814322 Validation Loss: 0.7247828245162964\n",
      "Epoch 7786: Training Loss: 0.12746365120013556 Validation Loss: 0.7249141931533813\n",
      "Epoch 7787: Training Loss: 0.1270236223936081 Validation Loss: 0.7244642376899719\n",
      "Epoch 7788: Training Loss: 0.12758423884709677 Validation Loss: 0.7241822481155396\n",
      "Epoch 7789: Training Loss: 0.1269962266087532 Validation Loss: 0.7241685390472412\n",
      "Epoch 7790: Training Loss: 0.12762935211261114 Validation Loss: 0.7245985269546509\n",
      "Epoch 7791: Training Loss: 0.12737749268611273 Validation Loss: 0.7243174314498901\n",
      "Epoch 7792: Training Loss: 0.12705431878566742 Validation Loss: 0.7244798541069031\n",
      "Epoch 7793: Training Loss: 0.12708344558874765 Validation Loss: 0.7241696119308472\n",
      "Epoch 7794: Training Loss: 0.1273976688583692 Validation Loss: 0.7243479490280151\n",
      "Epoch 7795: Training Loss: 0.12756663809219995 Validation Loss: 0.7245639562606812\n",
      "Epoch 7796: Training Loss: 0.12725080301364264 Validation Loss: 0.7246597409248352\n",
      "Epoch 7797: Training Loss: 0.12764040380716324 Validation Loss: 0.7247371077537537\n",
      "Epoch 7798: Training Loss: 0.12686125934123993 Validation Loss: 0.7246743440628052\n",
      "Epoch 7799: Training Loss: 0.12687633434931436 Validation Loss: 0.7242759466171265\n",
      "Epoch 7800: Training Loss: 0.1270906999707222 Validation Loss: 0.7240164279937744\n",
      "Epoch 7801: Training Loss: 0.12682135899861655 Validation Loss: 0.7240383625030518\n",
      "Epoch 7802: Training Loss: 0.12742194533348083 Validation Loss: 0.7241808772087097\n",
      "Epoch 7803: Training Loss: 0.1268178472916285 Validation Loss: 0.7240334749221802\n",
      "Epoch 7804: Training Loss: 0.1271620119611422 Validation Loss: 0.724375307559967\n",
      "Epoch 7805: Training Loss: 0.12723776201407114 Validation Loss: 0.7244457602500916\n",
      "Epoch 7806: Training Loss: 0.12695273756980896 Validation Loss: 0.7242501974105835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7807: Training Loss: 0.12673245618740717 Validation Loss: 0.7243666052818298\n",
      "Epoch 7808: Training Loss: 0.12710399429003397 Validation Loss: 0.7244173288345337\n",
      "Epoch 7809: Training Loss: 0.12654023120800653 Validation Loss: 0.7246448397636414\n",
      "Epoch 7810: Training Loss: 0.12698819488286972 Validation Loss: 0.7244678735733032\n",
      "Epoch 7811: Training Loss: 0.12743687381347021 Validation Loss: 0.7245460748672485\n",
      "Epoch 7812: Training Loss: 0.126660684744517 Validation Loss: 0.724507212638855\n",
      "Epoch 7813: Training Loss: 0.12621679653724036 Validation Loss: 0.7248367667198181\n",
      "Epoch 7814: Training Loss: 0.12668659538030624 Validation Loss: 0.7245380878448486\n",
      "Epoch 7815: Training Loss: 0.1274755597114563 Validation Loss: 0.7243853211402893\n",
      "Epoch 7816: Training Loss: 0.12746470173199972 Validation Loss: 0.7242591381072998\n",
      "Epoch 7817: Training Loss: 0.12698538353045782 Validation Loss: 0.7243020534515381\n",
      "Epoch 7818: Training Loss: 0.1267287110288938 Validation Loss: 0.7242578268051147\n",
      "Epoch 7819: Training Loss: 0.1269658754269282 Validation Loss: 0.7245703935623169\n",
      "Epoch 7820: Training Loss: 0.12710636109113693 Validation Loss: 0.7250186800956726\n",
      "Epoch 7821: Training Loss: 0.12721794346968332 Validation Loss: 0.7251663208007812\n",
      "Epoch 7822: Training Loss: 0.12655413647492728 Validation Loss: 0.7250449657440186\n",
      "Epoch 7823: Training Loss: 0.1264430359005928 Validation Loss: 0.7250837087631226\n",
      "Epoch 7824: Training Loss: 0.12738765279452005 Validation Loss: 0.7249549627304077\n",
      "Epoch 7825: Training Loss: 0.1265870655576388 Validation Loss: 0.7245873212814331\n",
      "Epoch 7826: Training Loss: 0.12650249650081 Validation Loss: 0.7248833179473877\n",
      "Epoch 7827: Training Loss: 0.12637547155221304 Validation Loss: 0.7249229550361633\n",
      "Epoch 7828: Training Loss: 0.12688170870145163 Validation Loss: 0.7251315116882324\n",
      "Epoch 7829: Training Loss: 0.12646419803301492 Validation Loss: 0.7249919772148132\n",
      "Epoch 7830: Training Loss: 0.12750548869371414 Validation Loss: 0.7249117493629456\n",
      "Epoch 7831: Training Loss: 0.1265887270371119 Validation Loss: 0.7247552871704102\n",
      "Epoch 7832: Training Loss: 0.12701549132665 Validation Loss: 0.7246075868606567\n",
      "Epoch 7833: Training Loss: 0.12658709039290747 Validation Loss: 0.7244930863380432\n",
      "Epoch 7834: Training Loss: 0.12777815262476602 Validation Loss: 0.7246968150138855\n",
      "Epoch 7835: Training Loss: 0.12670199821392694 Validation Loss: 0.7249531745910645\n",
      "Epoch 7836: Training Loss: 0.12636431058247885 Validation Loss: 0.7250344753265381\n",
      "Epoch 7837: Training Loss: 0.1265238250295321 Validation Loss: 0.7248315215110779\n",
      "Epoch 7838: Training Loss: 0.12664839377005896 Validation Loss: 0.7247458100318909\n",
      "Epoch 7839: Training Loss: 0.127396859228611 Validation Loss: 0.7252350449562073\n",
      "Epoch 7840: Training Loss: 0.1261983091632525 Validation Loss: 0.7249521017074585\n",
      "Epoch 7841: Training Loss: 0.12638763089974722 Validation Loss: 0.7250308990478516\n",
      "Epoch 7842: Training Loss: 0.12612740198771158 Validation Loss: 0.724886417388916\n",
      "Epoch 7843: Training Loss: 0.12650408099095026 Validation Loss: 0.7246713042259216\n",
      "Epoch 7844: Training Loss: 0.126154991487662 Validation Loss: 0.7246955633163452\n",
      "Epoch 7845: Training Loss: 0.1261803830663363 Validation Loss: 0.7248630523681641\n",
      "Epoch 7846: Training Loss: 0.12629186362028122 Validation Loss: 0.7248810529708862\n",
      "Epoch 7847: Training Loss: 0.12658225744962692 Validation Loss: 0.7249535322189331\n",
      "Epoch 7848: Training Loss: 0.12717398504416147 Validation Loss: 0.7249278426170349\n",
      "Epoch 7849: Training Loss: 0.1260747288664182 Validation Loss: 0.7254087328910828\n",
      "Epoch 7850: Training Loss: 0.12652351210514703 Validation Loss: 0.725803792476654\n",
      "Epoch 7851: Training Loss: 0.12638514985640845 Validation Loss: 0.725386917591095\n",
      "Epoch 7852: Training Loss: 0.12626361350218454 Validation Loss: 0.7252422571182251\n",
      "Epoch 7853: Training Loss: 0.12656570225954056 Validation Loss: 0.724994421005249\n",
      "Epoch 7854: Training Loss: 0.1262845496336619 Validation Loss: 0.7253445386886597\n",
      "Epoch 7855: Training Loss: 0.12617596238851547 Validation Loss: 0.7250231504440308\n",
      "Epoch 7856: Training Loss: 0.12624665846427283 Validation Loss: 0.7250186800956726\n",
      "Epoch 7857: Training Loss: 0.12608753144741058 Validation Loss: 0.725080132484436\n",
      "Epoch 7858: Training Loss: 0.1262474606434504 Validation Loss: 0.7250074148178101\n",
      "Epoch 7859: Training Loss: 0.1260741874575615 Validation Loss: 0.7249286770820618\n",
      "Epoch 7860: Training Loss: 0.12605176120996475 Validation Loss: 0.7250551581382751\n",
      "Epoch 7861: Training Loss: 0.12620063871145248 Validation Loss: 0.7250670790672302\n",
      "Epoch 7862: Training Loss: 0.12652969360351562 Validation Loss: 0.7249223589897156\n",
      "Epoch 7863: Training Loss: 0.12573626389106116 Validation Loss: 0.7250670194625854\n",
      "Epoch 7864: Training Loss: 0.1255799929300944 Validation Loss: 0.7250345349311829\n",
      "Epoch 7865: Training Loss: 0.12646393974622092 Validation Loss: 0.7249186635017395\n",
      "Epoch 7866: Training Loss: 0.1260389437278112 Validation Loss: 0.7246712446212769\n",
      "Epoch 7867: Training Loss: 0.12641209115584692 Validation Loss: 0.7247929573059082\n",
      "Epoch 7868: Training Loss: 0.125857246418794 Validation Loss: 0.7249533534049988\n",
      "Epoch 7869: Training Loss: 0.12642771999041238 Validation Loss: 0.7248936295509338\n",
      "Epoch 7870: Training Loss: 0.12619846810897192 Validation Loss: 0.7254881858825684\n",
      "Epoch 7871: Training Loss: 0.12618932873010635 Validation Loss: 0.7255505323410034\n",
      "Epoch 7872: Training Loss: 0.12613878895839056 Validation Loss: 0.72504061460495\n",
      "Epoch 7873: Training Loss: 0.1254464661081632 Validation Loss: 0.7246825695037842\n",
      "Epoch 7874: Training Loss: 0.1259288564324379 Validation Loss: 0.7248594760894775\n",
      "Epoch 7875: Training Loss: 0.12602337698141733 Validation Loss: 0.7247487902641296\n",
      "Epoch 7876: Training Loss: 0.12625317027171454 Validation Loss: 0.7249326109886169\n",
      "Epoch 7877: Training Loss: 0.1261579990386963 Validation Loss: 0.7251480221748352\n",
      "Epoch 7878: Training Loss: 0.12631819397211075 Validation Loss: 0.7254936695098877\n",
      "Epoch 7879: Training Loss: 0.1258705680569013 Validation Loss: 0.7258206605911255\n",
      "Epoch 7880: Training Loss: 0.12555912137031555 Validation Loss: 0.725752592086792\n",
      "Epoch 7881: Training Loss: 0.12659267832835516 Validation Loss: 0.7252510786056519\n",
      "Epoch 7882: Training Loss: 0.12661568572123846 Validation Loss: 0.7250098586082458\n",
      "Epoch 7883: Training Loss: 0.12563433249791464 Validation Loss: 0.7250475287437439\n",
      "Epoch 7884: Training Loss: 0.1256545732418696 Validation Loss: 0.7252194881439209\n",
      "Epoch 7885: Training Loss: 0.12547883888085684 Validation Loss: 0.7252697944641113\n",
      "Epoch 7886: Training Loss: 0.1257372349500656 Validation Loss: 0.7251690626144409\n",
      "Epoch 7887: Training Loss: 0.12599210441112518 Validation Loss: 0.7249133586883545\n",
      "Epoch 7888: Training Loss: 0.12570933252573013 Validation Loss: 0.7249144315719604\n",
      "Epoch 7889: Training Loss: 0.12565946330626807 Validation Loss: 0.7251283526420593\n",
      "Epoch 7890: Training Loss: 0.12565209716558456 Validation Loss: 0.7250552773475647\n",
      "Epoch 7891: Training Loss: 0.12568125625451407 Validation Loss: 0.7255362868309021\n",
      "Epoch 7892: Training Loss: 0.12559819718201956 Validation Loss: 0.7255901098251343\n",
      "Epoch 7893: Training Loss: 0.1254972368478775 Validation Loss: 0.7252530455589294\n",
      "Epoch 7894: Training Loss: 0.12580138196547827 Validation Loss: 0.7250642776489258\n",
      "Epoch 7895: Training Loss: 0.12583967298269272 Validation Loss: 0.7250125408172607\n",
      "Epoch 7896: Training Loss: 0.12544721364974976 Validation Loss: 0.7249332070350647\n",
      "Epoch 7897: Training Loss: 0.12581782042980194 Validation Loss: 0.7254024147987366\n",
      "Epoch 7898: Training Loss: 0.12585372974475226 Validation Loss: 0.7252160310745239\n",
      "Epoch 7899: Training Loss: 0.12605131417512894 Validation Loss: 0.7253689169883728\n",
      "Epoch 7900: Training Loss: 0.12570703277985254 Validation Loss: 0.7254395484924316\n",
      "Epoch 7901: Training Loss: 0.12566876411437988 Validation Loss: 0.725195050239563\n",
      "Epoch 7902: Training Loss: 0.12593643367290497 Validation Loss: 0.724807620048523\n",
      "Epoch 7903: Training Loss: 0.1257713884115219 Validation Loss: 0.7246505618095398\n",
      "Epoch 7904: Training Loss: 0.12544899682203928 Validation Loss: 0.7251162528991699\n",
      "Epoch 7905: Training Loss: 0.12579937279224396 Validation Loss: 0.7257058620452881\n",
      "Epoch 7906: Training Loss: 0.12575038025776544 Validation Loss: 0.7257975935935974\n",
      "Epoch 7907: Training Loss: 0.12568839887777963 Validation Loss: 0.7256278395652771\n",
      "Epoch 7908: Training Loss: 0.12549871702988943 Validation Loss: 0.7257086038589478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7909: Training Loss: 0.1255327487985293 Validation Loss: 0.7253473401069641\n",
      "Epoch 7910: Training Loss: 0.12573559333880743 Validation Loss: 0.7251628041267395\n",
      "Epoch 7911: Training Loss: 0.12538195898135504 Validation Loss: 0.7252851724624634\n",
      "Epoch 7912: Training Loss: 0.1262980674703916 Validation Loss: 0.7253545522689819\n",
      "Epoch 7913: Training Loss: 0.12546058744192123 Validation Loss: 0.7253264784812927\n",
      "Epoch 7914: Training Loss: 0.12573452790578207 Validation Loss: 0.7252599000930786\n",
      "Epoch 7915: Training Loss: 0.12595601628224054 Validation Loss: 0.7250939607620239\n",
      "Epoch 7916: Training Loss: 0.1253167117635409 Validation Loss: 0.72508305311203\n",
      "Epoch 7917: Training Loss: 0.12543601046005884 Validation Loss: 0.7253152132034302\n",
      "Epoch 7918: Training Loss: 0.12522872537374496 Validation Loss: 0.7256398797035217\n",
      "Epoch 7919: Training Loss: 0.12540197869141897 Validation Loss: 0.7257781028747559\n",
      "Epoch 7920: Training Loss: 0.12540863205989203 Validation Loss: 0.7254618406295776\n",
      "Epoch 7921: Training Loss: 0.12486884494622548 Validation Loss: 0.7252023816108704\n",
      "Epoch 7922: Training Loss: 0.12560619413852692 Validation Loss: 0.7251498103141785\n",
      "Epoch 7923: Training Loss: 0.12531665215889612 Validation Loss: 0.7250999808311462\n",
      "Epoch 7924: Training Loss: 0.12550874799489975 Validation Loss: 0.7254658341407776\n",
      "Epoch 7925: Training Loss: 0.12490434447924297 Validation Loss: 0.7257986068725586\n",
      "Epoch 7926: Training Loss: 0.12495548278093338 Validation Loss: 0.7259994149208069\n",
      "Epoch 7927: Training Loss: 0.12570548554261526 Validation Loss: 0.7256338000297546\n",
      "Epoch 7928: Training Loss: 0.1251075640320778 Validation Loss: 0.7258080244064331\n",
      "Epoch 7929: Training Loss: 0.1253655950228373 Validation Loss: 0.725784957408905\n",
      "Epoch 7930: Training Loss: 0.12491429597139359 Validation Loss: 0.7257771492004395\n",
      "Epoch 7931: Training Loss: 0.12544580300649008 Validation Loss: 0.7259608507156372\n",
      "Epoch 7932: Training Loss: 0.12507051974534988 Validation Loss: 0.7259731888771057\n",
      "Epoch 7933: Training Loss: 0.1250623588760694 Validation Loss: 0.7259467244148254\n",
      "Epoch 7934: Training Loss: 0.12534213811159134 Validation Loss: 0.725683867931366\n",
      "Epoch 7935: Training Loss: 0.12484318266312282 Validation Loss: 0.7257538437843323\n",
      "Epoch 7936: Training Loss: 0.12497591227293015 Validation Loss: 0.7257623076438904\n",
      "Epoch 7937: Training Loss: 0.12513119479020438 Validation Loss: 0.7255846858024597\n",
      "Epoch 7938: Training Loss: 0.12572149435679117 Validation Loss: 0.7255971431732178\n",
      "Epoch 7939: Training Loss: 0.12515288839737573 Validation Loss: 0.725612461566925\n",
      "Epoch 7940: Training Loss: 0.12573067595561346 Validation Loss: 0.7258023619651794\n",
      "Epoch 7941: Training Loss: 0.1251116469502449 Validation Loss: 0.7260220646858215\n",
      "Epoch 7942: Training Loss: 0.12494786580403645 Validation Loss: 0.7262741327285767\n",
      "Epoch 7943: Training Loss: 0.1251664236187935 Validation Loss: 0.7263302803039551\n",
      "Epoch 7944: Training Loss: 0.12509286652008691 Validation Loss: 0.7261110544204712\n",
      "Epoch 7945: Training Loss: 0.12472209582726161 Validation Loss: 0.7261061668395996\n",
      "Epoch 7946: Training Loss: 0.12493871649106343 Validation Loss: 0.7257624268531799\n",
      "Epoch 7947: Training Loss: 0.12500158697366714 Validation Loss: 0.725830614566803\n",
      "Epoch 7948: Training Loss: 0.12495471785465877 Validation Loss: 0.7256240844726562\n",
      "Epoch 7949: Training Loss: 0.12559214979410172 Validation Loss: 0.7258951663970947\n",
      "Epoch 7950: Training Loss: 0.1249008799592654 Validation Loss: 0.7263935208320618\n",
      "Epoch 7951: Training Loss: 0.1249520406126976 Validation Loss: 0.7262630462646484\n",
      "Epoch 7952: Training Loss: 0.125056063135465 Validation Loss: 0.7263080477714539\n",
      "Epoch 7953: Training Loss: 0.12434647728999455 Validation Loss: 0.7258172035217285\n",
      "Epoch 7954: Training Loss: 0.12462816139062245 Validation Loss: 0.725631058216095\n",
      "Epoch 7955: Training Loss: 0.12485893815755844 Validation Loss: 0.7257809042930603\n",
      "Epoch 7956: Training Loss: 0.125654103855292 Validation Loss: 0.7258744239807129\n",
      "Epoch 7957: Training Loss: 0.12525784224271774 Validation Loss: 0.7263793349266052\n",
      "Epoch 7958: Training Loss: 0.12473868330319722 Validation Loss: 0.7262353897094727\n",
      "Epoch 7959: Training Loss: 0.12474468102057774 Validation Loss: 0.7261393666267395\n",
      "Epoch 7960: Training Loss: 0.1248294860124588 Validation Loss: 0.7258957624435425\n",
      "Epoch 7961: Training Loss: 0.12474613885084788 Validation Loss: 0.7260856628417969\n",
      "Epoch 7962: Training Loss: 0.12479463219642639 Validation Loss: 0.7261390089988708\n",
      "Epoch 7963: Training Loss: 0.12449229011933009 Validation Loss: 0.726036548614502\n",
      "Epoch 7964: Training Loss: 0.1246144746740659 Validation Loss: 0.7258972525596619\n",
      "Epoch 7965: Training Loss: 0.12476061284542084 Validation Loss: 0.7259882092475891\n",
      "Epoch 7966: Training Loss: 0.12463156878948212 Validation Loss: 0.7260754108428955\n",
      "Epoch 7967: Training Loss: 0.12453971306482951 Validation Loss: 0.7259857654571533\n",
      "Epoch 7968: Training Loss: 0.12481641272703807 Validation Loss: 0.725695013999939\n",
      "Epoch 7969: Training Loss: 0.12447195996840794 Validation Loss: 0.72574782371521\n",
      "Epoch 7970: Training Loss: 0.124661090473334 Validation Loss: 0.7259281873703003\n",
      "Epoch 7971: Training Loss: 0.12494335075219472 Validation Loss: 0.7257466316223145\n",
      "Epoch 7972: Training Loss: 0.12466200192769368 Validation Loss: 0.7260226011276245\n",
      "Epoch 7973: Training Loss: 0.12406159689029057 Validation Loss: 0.7259182929992676\n",
      "Epoch 7974: Training Loss: 0.1251371055841446 Validation Loss: 0.7261932492256165\n",
      "Epoch 7975: Training Loss: 0.12492045263449351 Validation Loss: 0.7257475852966309\n",
      "Epoch 7976: Training Loss: 0.12466586381196976 Validation Loss: 0.725591242313385\n",
      "Epoch 7977: Training Loss: 0.12477609515190125 Validation Loss: 0.7260620594024658\n",
      "Epoch 7978: Training Loss: 0.12445194025834401 Validation Loss: 0.7263426780700684\n",
      "Epoch 7979: Training Loss: 0.12516426791747412 Validation Loss: 0.7265437841415405\n",
      "Epoch 7980: Training Loss: 0.12418481955925624 Validation Loss: 0.7263282537460327\n",
      "Epoch 7981: Training Loss: 0.12513390680154166 Validation Loss: 0.7261852622032166\n",
      "Epoch 7982: Training Loss: 0.12453446040550868 Validation Loss: 0.7264413833618164\n",
      "Epoch 7983: Training Loss: 0.1240260899066925 Validation Loss: 0.726372241973877\n",
      "Epoch 7984: Training Loss: 0.12460388739903767 Validation Loss: 0.7262874841690063\n",
      "Epoch 7985: Training Loss: 0.12442280848821004 Validation Loss: 0.7261658906936646\n",
      "Epoch 7986: Training Loss: 0.12480305135250092 Validation Loss: 0.7260734438896179\n",
      "Epoch 7987: Training Loss: 0.12434235960245132 Validation Loss: 0.7260071039199829\n",
      "Epoch 7988: Training Loss: 0.12446929762760799 Validation Loss: 0.7265974283218384\n",
      "Epoch 7989: Training Loss: 0.12497399250666301 Validation Loss: 0.7264381647109985\n",
      "Epoch 7990: Training Loss: 0.12438107530275981 Validation Loss: 0.7262582778930664\n",
      "Epoch 7991: Training Loss: 0.12409244726101558 Validation Loss: 0.7261651754379272\n",
      "Epoch 7992: Training Loss: 0.1239553913474083 Validation Loss: 0.7260695695877075\n",
      "Epoch 7993: Training Loss: 0.12438338994979858 Validation Loss: 0.7259521484375\n",
      "Epoch 7994: Training Loss: 0.1240177055199941 Validation Loss: 0.7258391976356506\n",
      "Epoch 7995: Training Loss: 0.12495889763037364 Validation Loss: 0.726168692111969\n",
      "Epoch 7996: Training Loss: 0.12427583088477452 Validation Loss: 0.7260313630104065\n",
      "Epoch 7997: Training Loss: 0.12454390029112498 Validation Loss: 0.7260987162590027\n",
      "Epoch 7998: Training Loss: 0.12487876663605373 Validation Loss: 0.7263258695602417\n",
      "Epoch 7999: Training Loss: 0.12412004669507344 Validation Loss: 0.7263532876968384\n",
      "Epoch 8000: Training Loss: 0.12440311163663864 Validation Loss: 0.7261508703231812\n",
      "Epoch 8001: Training Loss: 0.12425246089696884 Validation Loss: 0.7260154485702515\n",
      "Epoch 8002: Training Loss: 0.12429315100113551 Validation Loss: 0.7258535623550415\n",
      "Epoch 8003: Training Loss: 0.1244503582517306 Validation Loss: 0.7261402606964111\n",
      "Epoch 8004: Training Loss: 0.12442235151926677 Validation Loss: 0.7263978719711304\n",
      "Epoch 8005: Training Loss: 0.12410704543193181 Validation Loss: 0.7265436053276062\n",
      "Epoch 8006: Training Loss: 0.12397405256827672 Validation Loss: 0.7265428304672241\n",
      "Epoch 8007: Training Loss: 0.12419893840948741 Validation Loss: 0.7263041734695435\n",
      "Epoch 8008: Training Loss: 0.12497766564289729 Validation Loss: 0.7262535691261292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8009: Training Loss: 0.12417494008938472 Validation Loss: 0.7263957262039185\n",
      "Epoch 8010: Training Loss: 0.12393314143021901 Validation Loss: 0.7263197302818298\n",
      "Epoch 8011: Training Loss: 0.12436520556608836 Validation Loss: 0.7264500856399536\n",
      "Epoch 8012: Training Loss: 0.1242964615424474 Validation Loss: 0.7264554500579834\n",
      "Epoch 8013: Training Loss: 0.12404454002777736 Validation Loss: 0.7263994812965393\n",
      "Epoch 8014: Training Loss: 0.12453872213761012 Validation Loss: 0.726661741733551\n",
      "Epoch 8015: Training Loss: 0.12398096422354381 Validation Loss: 0.726651668548584\n",
      "Epoch 8016: Training Loss: 0.12422792613506317 Validation Loss: 0.7262811660766602\n",
      "Epoch 8017: Training Loss: 0.12406910459200542 Validation Loss: 0.7260620594024658\n",
      "Epoch 8018: Training Loss: 0.12404934068520863 Validation Loss: 0.7258192300796509\n",
      "Epoch 8019: Training Loss: 0.12386638174454372 Validation Loss: 0.7258627414703369\n",
      "Epoch 8020: Training Loss: 0.12384217480818431 Validation Loss: 0.7261983752250671\n",
      "Epoch 8021: Training Loss: 0.1242243821422259 Validation Loss: 0.7260040044784546\n",
      "Epoch 8022: Training Loss: 0.1238648494084676 Validation Loss: 0.7262113094329834\n",
      "Epoch 8023: Training Loss: 0.12391640990972519 Validation Loss: 0.7269843220710754\n",
      "Epoch 8024: Training Loss: 0.12378655870755513 Validation Loss: 0.7271994352340698\n",
      "Epoch 8025: Training Loss: 0.12387339025735855 Validation Loss: 0.7269341945648193\n",
      "Epoch 8026: Training Loss: 0.1240225260456403 Validation Loss: 0.7268584966659546\n",
      "Epoch 8027: Training Loss: 0.12380099296569824 Validation Loss: 0.726200520992279\n",
      "Epoch 8028: Training Loss: 0.1243101954460144 Validation Loss: 0.7260549068450928\n",
      "Epoch 8029: Training Loss: 0.12365865459044774 Validation Loss: 0.7265254855155945\n",
      "Epoch 8030: Training Loss: 0.12384346375862758 Validation Loss: 0.726483166217804\n",
      "Epoch 8031: Training Loss: 0.12377710888783137 Validation Loss: 0.7267186641693115\n",
      "Epoch 8032: Training Loss: 0.12387419988711675 Validation Loss: 0.7269136309623718\n",
      "Epoch 8033: Training Loss: 0.1231765349706014 Validation Loss: 0.7268490195274353\n",
      "Epoch 8034: Training Loss: 0.12401704986890157 Validation Loss: 0.7268072366714478\n",
      "Epoch 8035: Training Loss: 0.12382022788127263 Validation Loss: 0.7263439893722534\n",
      "Epoch 8036: Training Loss: 0.12445687750975291 Validation Loss: 0.7258859872817993\n",
      "Epoch 8037: Training Loss: 0.12390245497226715 Validation Loss: 0.7262220978736877\n",
      "Epoch 8038: Training Loss: 0.12367358307043712 Validation Loss: 0.7263813018798828\n",
      "Epoch 8039: Training Loss: 0.12443832804759343 Validation Loss: 0.7266197204589844\n",
      "Epoch 8040: Training Loss: 0.12371481209993362 Validation Loss: 0.7267100214958191\n",
      "Epoch 8041: Training Loss: 0.1237363616625468 Validation Loss: 0.7270535826683044\n",
      "Epoch 8042: Training Loss: 0.12395824491977692 Validation Loss: 0.7269189357757568\n",
      "Epoch 8043: Training Loss: 0.12376584857702255 Validation Loss: 0.7264729142189026\n",
      "Epoch 8044: Training Loss: 0.12504543860753378 Validation Loss: 0.7265414595603943\n",
      "Epoch 8045: Training Loss: 0.12356542547543843 Validation Loss: 0.7263687252998352\n",
      "Epoch 8046: Training Loss: 0.1252206489443779 Validation Loss: 0.7267565727233887\n",
      "Epoch 8047: Training Loss: 0.12360911319653194 Validation Loss: 0.7270413041114807\n",
      "Epoch 8048: Training Loss: 0.12447054187456767 Validation Loss: 0.7268832921981812\n",
      "Epoch 8049: Training Loss: 0.12348062296708424 Validation Loss: 0.726653516292572\n",
      "Epoch 8050: Training Loss: 0.12406500925620396 Validation Loss: 0.7265125513076782\n",
      "Epoch 8051: Training Loss: 0.12380647659301758 Validation Loss: 0.7269068956375122\n",
      "Epoch 8052: Training Loss: 0.12330744415521622 Validation Loss: 0.727145791053772\n",
      "Epoch 8053: Training Loss: 0.12344518303871155 Validation Loss: 0.7270714044570923\n",
      "Epoch 8054: Training Loss: 0.12392512212196986 Validation Loss: 0.7270976901054382\n",
      "Epoch 8055: Training Loss: 0.12330174694458644 Validation Loss: 0.7270538210868835\n",
      "Epoch 8056: Training Loss: 0.12346313397089641 Validation Loss: 0.7270287275314331\n",
      "Epoch 8057: Training Loss: 0.12465664247671764 Validation Loss: 0.7270798683166504\n",
      "Epoch 8058: Training Loss: 0.12369548777739207 Validation Loss: 0.72713702917099\n",
      "Epoch 8059: Training Loss: 0.12347443401813507 Validation Loss: 0.727381706237793\n",
      "Epoch 8060: Training Loss: 0.12334440151850383 Validation Loss: 0.7269200086593628\n",
      "Epoch 8061: Training Loss: 0.1235031709074974 Validation Loss: 0.7267048358917236\n",
      "Epoch 8062: Training Loss: 0.12342674036820729 Validation Loss: 0.7263469099998474\n",
      "Epoch 8063: Training Loss: 0.12332346538702647 Validation Loss: 0.7261610627174377\n",
      "Epoch 8064: Training Loss: 0.12346121420462926 Validation Loss: 0.7266510725021362\n",
      "Epoch 8065: Training Loss: 0.12376087158918381 Validation Loss: 0.7270634770393372\n",
      "Epoch 8066: Training Loss: 0.1233569656809171 Validation Loss: 0.7270272970199585\n",
      "Epoch 8067: Training Loss: 0.12353173643350601 Validation Loss: 0.7271763682365417\n",
      "Epoch 8068: Training Loss: 0.12407733003298442 Validation Loss: 0.7267051339149475\n",
      "Epoch 8069: Training Loss: 0.12356752405563991 Validation Loss: 0.7269187569618225\n",
      "Epoch 8070: Training Loss: 0.12366961936155955 Validation Loss: 0.7268635034561157\n",
      "Epoch 8071: Training Loss: 0.12326089044411977 Validation Loss: 0.7270480394363403\n",
      "Epoch 8072: Training Loss: 0.12315673381090164 Validation Loss: 0.7270262241363525\n",
      "Epoch 8073: Training Loss: 0.12322621544202168 Validation Loss: 0.727220356464386\n",
      "Epoch 8074: Training Loss: 0.12344598273436229 Validation Loss: 0.7269629240036011\n",
      "Epoch 8075: Training Loss: 0.12379742165406545 Validation Loss: 0.7271665930747986\n",
      "Epoch 8076: Training Loss: 0.1233914444843928 Validation Loss: 0.7272714376449585\n",
      "Epoch 8077: Training Loss: 0.12428370614846547 Validation Loss: 0.7265987396240234\n",
      "Epoch 8078: Training Loss: 0.12333875894546509 Validation Loss: 0.7267830967903137\n",
      "Epoch 8079: Training Loss: 0.12361719459295273 Validation Loss: 0.7274150252342224\n",
      "Epoch 8080: Training Loss: 0.12315332392851512 Validation Loss: 0.7276800870895386\n",
      "Epoch 8081: Training Loss: 0.12326940645774205 Validation Loss: 0.7273271679878235\n",
      "Epoch 8082: Training Loss: 0.12319610516230266 Validation Loss: 0.7272471785545349\n",
      "Epoch 8083: Training Loss: 0.12328087786833446 Validation Loss: 0.7269421219825745\n",
      "Epoch 8084: Training Loss: 0.12352095295985539 Validation Loss: 0.726383626461029\n",
      "Epoch 8085: Training Loss: 0.12349368631839752 Validation Loss: 0.7265552282333374\n",
      "Epoch 8086: Training Loss: 0.1232260490457217 Validation Loss: 0.7266294956207275\n",
      "Epoch 8087: Training Loss: 0.12329816073179245 Validation Loss: 0.7269968390464783\n",
      "Epoch 8088: Training Loss: 0.12294438978036244 Validation Loss: 0.7270244359970093\n",
      "Epoch 8089: Training Loss: 0.12303528189659119 Validation Loss: 0.7270687222480774\n",
      "Epoch 8090: Training Loss: 0.12326546510060628 Validation Loss: 0.7270395755767822\n",
      "Epoch 8091: Training Loss: 0.12313757836818695 Validation Loss: 0.7270534634590149\n",
      "Epoch 8092: Training Loss: 0.1229185884197553 Validation Loss: 0.7271747589111328\n",
      "Epoch 8093: Training Loss: 0.12364816168944041 Validation Loss: 0.7276374101638794\n",
      "Epoch 8094: Training Loss: 0.1236045037706693 Validation Loss: 0.7275764346122742\n",
      "Epoch 8095: Training Loss: 0.12272026886542638 Validation Loss: 0.7275751233100891\n",
      "Epoch 8096: Training Loss: 0.12301681687434514 Validation Loss: 0.727313220500946\n",
      "Epoch 8097: Training Loss: 0.12307032446066539 Validation Loss: 0.7273793816566467\n",
      "Epoch 8098: Training Loss: 0.1230483849843343 Validation Loss: 0.7271396517753601\n",
      "Epoch 8099: Training Loss: 0.12301234404246013 Validation Loss: 0.7269103527069092\n",
      "Epoch 8100: Training Loss: 0.12300864607095718 Validation Loss: 0.727037787437439\n",
      "Epoch 8101: Training Loss: 0.12312719225883484 Validation Loss: 0.7273135781288147\n",
      "Epoch 8102: Training Loss: 0.12329448759555817 Validation Loss: 0.7270988821983337\n",
      "Epoch 8103: Training Loss: 0.12284839153289795 Validation Loss: 0.727101743221283\n",
      "Epoch 8104: Training Loss: 0.12302836279074351 Validation Loss: 0.727249801158905\n",
      "Epoch 8105: Training Loss: 0.12354601174592972 Validation Loss: 0.727096438407898\n",
      "Epoch 8106: Training Loss: 0.12296567608912785 Validation Loss: 0.7270784378051758\n",
      "Epoch 8107: Training Loss: 0.12345061202843984 Validation Loss: 0.7270567417144775\n",
      "Epoch 8108: Training Loss: 0.12306067099173863 Validation Loss: 0.7273857593536377\n",
      "Epoch 8109: Training Loss: 0.12275037417809169 Validation Loss: 0.727752685546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8110: Training Loss: 0.1228681206703186 Validation Loss: 0.7279453873634338\n",
      "Epoch 8111: Training Loss: 0.12301021814346313 Validation Loss: 0.7275440096855164\n",
      "Epoch 8112: Training Loss: 0.12305658310651779 Validation Loss: 0.7274399995803833\n",
      "Epoch 8113: Training Loss: 0.1231791799267133 Validation Loss: 0.7275263667106628\n",
      "Epoch 8114: Training Loss: 0.12257647265990575 Validation Loss: 0.7273459434509277\n",
      "Epoch 8115: Training Loss: 0.12329812596241634 Validation Loss: 0.7271881699562073\n",
      "Epoch 8116: Training Loss: 0.12331599245468776 Validation Loss: 0.727157711982727\n",
      "Epoch 8117: Training Loss: 0.12307146191596985 Validation Loss: 0.7272711396217346\n",
      "Epoch 8118: Training Loss: 0.12278192738691966 Validation Loss: 0.7272442579269409\n",
      "Epoch 8119: Training Loss: 0.12395338962475459 Validation Loss: 0.727301836013794\n",
      "Epoch 8120: Training Loss: 0.12362565845251083 Validation Loss: 0.7273841500282288\n",
      "Epoch 8121: Training Loss: 0.12264336148897807 Validation Loss: 0.7274346351623535\n",
      "Epoch 8122: Training Loss: 0.1228666181365649 Validation Loss: 0.7271456122398376\n",
      "Epoch 8123: Training Loss: 0.12289675821860631 Validation Loss: 0.7271355390548706\n",
      "Epoch 8124: Training Loss: 0.12255701422691345 Validation Loss: 0.7271703481674194\n",
      "Epoch 8125: Training Loss: 0.12352907905975978 Validation Loss: 0.7273357510566711\n",
      "Epoch 8126: Training Loss: 0.12276229013999303 Validation Loss: 0.7279301285743713\n",
      "Epoch 8127: Training Loss: 0.12245396773020427 Validation Loss: 0.7278617024421692\n",
      "Epoch 8128: Training Loss: 0.12245787680149078 Validation Loss: 0.7276128530502319\n",
      "Epoch 8129: Training Loss: 0.12248178819815318 Validation Loss: 0.7277407050132751\n",
      "Epoch 8130: Training Loss: 0.1226058229804039 Validation Loss: 0.7277491092681885\n",
      "Epoch 8131: Training Loss: 0.1227385476231575 Validation Loss: 0.7276673316955566\n",
      "Epoch 8132: Training Loss: 0.12262647598981857 Validation Loss: 0.727575421333313\n",
      "Epoch 8133: Training Loss: 0.12256066749493282 Validation Loss: 0.7277175188064575\n",
      "Epoch 8134: Training Loss: 0.12232573578755061 Validation Loss: 0.7277581691741943\n",
      "Epoch 8135: Training Loss: 0.12300667663415273 Validation Loss: 0.7275350689888\n",
      "Epoch 8136: Training Loss: 0.12301080425580342 Validation Loss: 0.7273038625717163\n",
      "Epoch 8137: Training Loss: 0.12263812373081844 Validation Loss: 0.7275077700614929\n",
      "Epoch 8138: Training Loss: 0.1225426197052002 Validation Loss: 0.7270088195800781\n",
      "Epoch 8139: Training Loss: 0.12243388096491496 Validation Loss: 0.726893961429596\n",
      "Epoch 8140: Training Loss: 0.12246592094500859 Validation Loss: 0.727209210395813\n",
      "Epoch 8141: Training Loss: 0.12244982520739238 Validation Loss: 0.7276460528373718\n",
      "Epoch 8142: Training Loss: 0.12295052905877431 Validation Loss: 0.7279757261276245\n",
      "Epoch 8143: Training Loss: 0.12230448673168819 Validation Loss: 0.7281637191772461\n",
      "Epoch 8144: Training Loss: 0.1223337451616923 Validation Loss: 0.728061318397522\n",
      "Epoch 8145: Training Loss: 0.12291213124990463 Validation Loss: 0.7281202673912048\n",
      "Epoch 8146: Training Loss: 0.12217823912700017 Validation Loss: 0.7277749180793762\n",
      "Epoch 8147: Training Loss: 0.12241296966870625 Validation Loss: 0.7275108695030212\n",
      "Epoch 8148: Training Loss: 0.12209325283765793 Validation Loss: 0.7273754477500916\n",
      "Epoch 8149: Training Loss: 0.12277598927418391 Validation Loss: 0.7271666526794434\n",
      "Epoch 8150: Training Loss: 0.12227148562669754 Validation Loss: 0.7275595664978027\n",
      "Epoch 8151: Training Loss: 0.12234791119893391 Validation Loss: 0.7275550961494446\n",
      "Epoch 8152: Training Loss: 0.12211364507675171 Validation Loss: 0.7278507947921753\n",
      "Epoch 8153: Training Loss: 0.12241733570893605 Validation Loss: 0.7278664112091064\n",
      "Epoch 8154: Training Loss: 0.12229173382123311 Validation Loss: 0.7279207110404968\n",
      "Epoch 8155: Training Loss: 0.12249873081843059 Validation Loss: 0.7277337908744812\n",
      "Epoch 8156: Training Loss: 0.12218660612901051 Validation Loss: 0.7275975942611694\n",
      "Epoch 8157: Training Loss: 0.12216556817293167 Validation Loss: 0.7272888422012329\n",
      "Epoch 8158: Training Loss: 0.12178737173477809 Validation Loss: 0.7274519801139832\n",
      "Epoch 8159: Training Loss: 0.12216504414876302 Validation Loss: 0.7275242209434509\n",
      "Epoch 8160: Training Loss: 0.12224142005046208 Validation Loss: 0.7279075980186462\n",
      "Epoch 8161: Training Loss: 0.12191818157831828 Validation Loss: 0.7281725406646729\n",
      "Epoch 8162: Training Loss: 0.1224607601761818 Validation Loss: 0.7280429005622864\n",
      "Epoch 8163: Training Loss: 0.12206248939037323 Validation Loss: 0.7279560565948486\n",
      "Epoch 8164: Training Loss: 0.12261348962783813 Validation Loss: 0.7279950976371765\n",
      "Epoch 8165: Training Loss: 0.12222652385632198 Validation Loss: 0.7280981540679932\n",
      "Epoch 8166: Training Loss: 0.1220526893933614 Validation Loss: 0.7281085252761841\n",
      "Epoch 8167: Training Loss: 0.12237537403901418 Validation Loss: 0.7276017069816589\n",
      "Epoch 8168: Training Loss: 0.12223317970832188 Validation Loss: 0.7270875573158264\n",
      "Epoch 8169: Training Loss: 0.12220876167217891 Validation Loss: 0.7274877429008484\n",
      "Epoch 8170: Training Loss: 0.12289322167634964 Validation Loss: 0.7276076078414917\n",
      "Epoch 8171: Training Loss: 0.12210039794445038 Validation Loss: 0.7272822260856628\n",
      "Epoch 8172: Training Loss: 0.12197608004013698 Validation Loss: 0.7275072336196899\n",
      "Epoch 8173: Training Loss: 0.121856689453125 Validation Loss: 0.7276538610458374\n",
      "Epoch 8174: Training Loss: 0.12277789910634358 Validation Loss: 0.7277423739433289\n",
      "Epoch 8175: Training Loss: 0.12203887104988098 Validation Loss: 0.7277442216873169\n",
      "Epoch 8176: Training Loss: 0.12213916331529617 Validation Loss: 0.7278216481208801\n",
      "Epoch 8177: Training Loss: 0.12194559474786122 Validation Loss: 0.7276721000671387\n",
      "Epoch 8178: Training Loss: 0.12191473444302876 Validation Loss: 0.7279345989227295\n",
      "Epoch 8179: Training Loss: 0.12217614303032558 Validation Loss: 0.7277852296829224\n",
      "Epoch 8180: Training Loss: 0.12262712170680364 Validation Loss: 0.7280034422874451\n",
      "Epoch 8181: Training Loss: 0.1220777357618014 Validation Loss: 0.7281473278999329\n",
      "Epoch 8182: Training Loss: 0.12202796091636021 Validation Loss: 0.7288370728492737\n",
      "Epoch 8183: Training Loss: 0.12205923348665237 Validation Loss: 0.7284579277038574\n",
      "Epoch 8184: Training Loss: 0.12216440836588542 Validation Loss: 0.7281309962272644\n",
      "Epoch 8185: Training Loss: 0.12178803980350494 Validation Loss: 0.7278565168380737\n",
      "Epoch 8186: Training Loss: 0.12229795753955841 Validation Loss: 0.7275132536888123\n",
      "Epoch 8187: Training Loss: 0.12213923285404842 Validation Loss: 0.7274530529975891\n",
      "Epoch 8188: Training Loss: 0.12147142241398494 Validation Loss: 0.7274636626243591\n",
      "Epoch 8189: Training Loss: 0.12183352559804916 Validation Loss: 0.7277365922927856\n",
      "Epoch 8190: Training Loss: 0.12185560166835785 Validation Loss: 0.7281751036643982\n",
      "Epoch 8191: Training Loss: 0.12188348174095154 Validation Loss: 0.7281839847564697\n",
      "Epoch 8192: Training Loss: 0.12222787489493687 Validation Loss: 0.7283438444137573\n",
      "Epoch 8193: Training Loss: 0.12195667376120885 Validation Loss: 0.7283972501754761\n",
      "Epoch 8194: Training Loss: 0.12132780750592549 Validation Loss: 0.7286680340766907\n",
      "Epoch 8195: Training Loss: 0.12158956875403722 Validation Loss: 0.7283737659454346\n",
      "Epoch 8196: Training Loss: 0.12165281424919765 Validation Loss: 0.7280972599983215\n",
      "Epoch 8197: Training Loss: 0.12161373347043991 Validation Loss: 0.7277994751930237\n",
      "Epoch 8198: Training Loss: 0.12195594360431035 Validation Loss: 0.7278326749801636\n",
      "Epoch 8199: Training Loss: 0.12146943310896556 Validation Loss: 0.727959394454956\n",
      "Epoch 8200: Training Loss: 0.1215684711933136 Validation Loss: 0.7278217673301697\n",
      "Epoch 8201: Training Loss: 0.12208377321561177 Validation Loss: 0.7280036211013794\n",
      "Epoch 8202: Training Loss: 0.12243571132421494 Validation Loss: 0.7280420660972595\n",
      "Epoch 8203: Training Loss: 0.1221785048643748 Validation Loss: 0.7280289530754089\n",
      "Epoch 8204: Training Loss: 0.12231677770614624 Validation Loss: 0.7278367877006531\n",
      "Epoch 8205: Training Loss: 0.12173351148764293 Validation Loss: 0.728153645992279\n",
      "Epoch 8206: Training Loss: 0.12172207484642665 Validation Loss: 0.7283270955085754\n",
      "Epoch 8207: Training Loss: 0.12190267443656921 Validation Loss: 0.7281498908996582\n",
      "Epoch 8208: Training Loss: 0.12336210658152898 Validation Loss: 0.728252649307251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8209: Training Loss: 0.12183073163032532 Validation Loss: 0.728246808052063\n",
      "Epoch 8210: Training Loss: 0.12210095922152202 Validation Loss: 0.7283177375793457\n",
      "Epoch 8211: Training Loss: 0.12297498434782028 Validation Loss: 0.7279643416404724\n",
      "Epoch 8212: Training Loss: 0.12162075191736221 Validation Loss: 0.7283779382705688\n",
      "Epoch 8213: Training Loss: 0.12162096798419952 Validation Loss: 0.7284599542617798\n",
      "Epoch 8214: Training Loss: 0.12158272167046864 Validation Loss: 0.7285241484642029\n",
      "Epoch 8215: Training Loss: 0.12137473622957866 Validation Loss: 0.7284830808639526\n",
      "Epoch 8216: Training Loss: 0.12133457760016124 Validation Loss: 0.7282466888427734\n",
      "Epoch 8217: Training Loss: 0.12143719444672267 Validation Loss: 0.7281131744384766\n",
      "Epoch 8218: Training Loss: 0.12183850506941478 Validation Loss: 0.7284128665924072\n",
      "Epoch 8219: Training Loss: 0.12128303200006485 Validation Loss: 0.7281975150108337\n",
      "Epoch 8220: Training Loss: 0.12201140075922012 Validation Loss: 0.7280381321907043\n",
      "Epoch 8221: Training Loss: 0.121216615041097 Validation Loss: 0.7282431125640869\n",
      "Epoch 8222: Training Loss: 0.12152130156755447 Validation Loss: 0.7284277081489563\n",
      "Epoch 8223: Training Loss: 0.12135269741217296 Validation Loss: 0.7286568284034729\n",
      "Epoch 8224: Training Loss: 0.12174115826686223 Validation Loss: 0.7285863161087036\n",
      "Epoch 8225: Training Loss: 0.12150528778632481 Validation Loss: 0.7284486889839172\n",
      "Epoch 8226: Training Loss: 0.12237921108802159 Validation Loss: 0.7284463047981262\n",
      "Epoch 8227: Training Loss: 0.12156197428703308 Validation Loss: 0.7281290292739868\n",
      "Epoch 8228: Training Loss: 0.1215765376885732 Validation Loss: 0.7284480929374695\n",
      "Epoch 8229: Training Loss: 0.12165528535842896 Validation Loss: 0.7281838655471802\n",
      "Epoch 8230: Training Loss: 0.12128765136003494 Validation Loss: 0.7279984951019287\n",
      "Epoch 8231: Training Loss: 0.12114382286866505 Validation Loss: 0.7280957102775574\n",
      "Epoch 8232: Training Loss: 0.12138647834459941 Validation Loss: 0.7277661561965942\n",
      "Epoch 8233: Training Loss: 0.12150528281927109 Validation Loss: 0.727859616279602\n",
      "Epoch 8234: Training Loss: 0.12176843235890071 Validation Loss: 0.7280504107475281\n",
      "Epoch 8235: Training Loss: 0.12102164328098297 Validation Loss: 0.7284800410270691\n",
      "Epoch 8236: Training Loss: 0.12087346116701762 Validation Loss: 0.7288941740989685\n",
      "Epoch 8237: Training Loss: 0.12185241530338924 Validation Loss: 0.7286961078643799\n",
      "Epoch 8238: Training Loss: 0.12099327643712361 Validation Loss: 0.7281796932220459\n",
      "Epoch 8239: Training Loss: 0.12125122298796971 Validation Loss: 0.7283812761306763\n",
      "Epoch 8240: Training Loss: 0.12168041616678238 Validation Loss: 0.7280200719833374\n",
      "Epoch 8241: Training Loss: 0.12108174959818523 Validation Loss: 0.7282688617706299\n",
      "Epoch 8242: Training Loss: 0.12131792306900024 Validation Loss: 0.7284309267997742\n",
      "Epoch 8243: Training Loss: 0.12124540408452351 Validation Loss: 0.728488564491272\n",
      "Epoch 8244: Training Loss: 0.12132864445447922 Validation Loss: 0.728162407875061\n",
      "Epoch 8245: Training Loss: 0.12166557709376018 Validation Loss: 0.7284499406814575\n",
      "Epoch 8246: Training Loss: 0.12218596786260605 Validation Loss: 0.7284709215164185\n",
      "Epoch 8247: Training Loss: 0.1211423526207606 Validation Loss: 0.7286257147789001\n",
      "Epoch 8248: Training Loss: 0.12166442722082138 Validation Loss: 0.7283253073692322\n",
      "Epoch 8249: Training Loss: 0.1209819143017133 Validation Loss: 0.7283486127853394\n",
      "Epoch 8250: Training Loss: 0.12126387655735016 Validation Loss: 0.7284201383590698\n",
      "Epoch 8251: Training Loss: 0.12108797828356425 Validation Loss: 0.7285631895065308\n",
      "Epoch 8252: Training Loss: 0.1212981070081393 Validation Loss: 0.728698194026947\n",
      "Epoch 8253: Training Loss: 0.12104865908622742 Validation Loss: 0.7288158535957336\n",
      "Epoch 8254: Training Loss: 0.12136191378037135 Validation Loss: 0.7290520071983337\n",
      "Epoch 8255: Training Loss: 0.12141654640436172 Validation Loss: 0.7288931012153625\n",
      "Epoch 8256: Training Loss: 0.12166715413331985 Validation Loss: 0.72870934009552\n",
      "Epoch 8257: Training Loss: 0.12167166918516159 Validation Loss: 0.728618860244751\n",
      "Epoch 8258: Training Loss: 0.1208812693754832 Validation Loss: 0.7288694381713867\n",
      "Epoch 8259: Training Loss: 0.12111455202102661 Validation Loss: 0.7287775278091431\n",
      "Epoch 8260: Training Loss: 0.12053445726633072 Validation Loss: 0.7286143898963928\n",
      "Epoch 8261: Training Loss: 0.12119653075933456 Validation Loss: 0.7284091114997864\n",
      "Epoch 8262: Training Loss: 0.12079028536876042 Validation Loss: 0.7287276983261108\n",
      "Epoch 8263: Training Loss: 0.12101781616608302 Validation Loss: 0.7289021015167236\n",
      "Epoch 8264: Training Loss: 0.12109352648258209 Validation Loss: 0.7287920117378235\n",
      "Epoch 8265: Training Loss: 0.12120465437571208 Validation Loss: 0.7288246750831604\n",
      "Epoch 8266: Training Loss: 0.12111750741799672 Validation Loss: 0.7289947867393494\n",
      "Epoch 8267: Training Loss: 0.12124718477328618 Validation Loss: 0.7292476296424866\n",
      "Epoch 8268: Training Loss: 0.12074473748604457 Validation Loss: 0.7285834550857544\n",
      "Epoch 8269: Training Loss: 0.12084487080574036 Validation Loss: 0.7285838723182678\n",
      "Epoch 8270: Training Loss: 0.12041603525479634 Validation Loss: 0.7284326553344727\n",
      "Epoch 8271: Training Loss: 0.12084519863128662 Validation Loss: 0.7285625338554382\n",
      "Epoch 8272: Training Loss: 0.12115879605213802 Validation Loss: 0.7285066246986389\n",
      "Epoch 8273: Training Loss: 0.12073744585116704 Validation Loss: 0.7286238670349121\n",
      "Epoch 8274: Training Loss: 0.12182273467381795 Validation Loss: 0.7286238670349121\n",
      "Epoch 8275: Training Loss: 0.12058164179325104 Validation Loss: 0.7288874387741089\n",
      "Epoch 8276: Training Loss: 0.12093450874090195 Validation Loss: 0.728999137878418\n",
      "Epoch 8277: Training Loss: 0.12087948620319366 Validation Loss: 0.7288457155227661\n",
      "Epoch 8278: Training Loss: 0.1213584691286087 Validation Loss: 0.7287306785583496\n",
      "Epoch 8279: Training Loss: 0.12079875667889912 Validation Loss: 0.7286136746406555\n",
      "Epoch 8280: Training Loss: 0.12080644071102142 Validation Loss: 0.7287442684173584\n",
      "Epoch 8281: Training Loss: 0.12197334816058476 Validation Loss: 0.7288770079612732\n",
      "Epoch 8282: Training Loss: 0.12054885923862457 Validation Loss: 0.729315996170044\n",
      "Epoch 8283: Training Loss: 0.12081451465686162 Validation Loss: 0.7292729020118713\n",
      "Epoch 8284: Training Loss: 0.12074536085128784 Validation Loss: 0.7291343212127686\n",
      "Epoch 8285: Training Loss: 0.12022933860619862 Validation Loss: 0.7289772629737854\n",
      "Epoch 8286: Training Loss: 0.12063230325778325 Validation Loss: 0.7289941310882568\n",
      "Epoch 8287: Training Loss: 0.12072603156169255 Validation Loss: 0.7291429042816162\n",
      "Epoch 8288: Training Loss: 0.12099639326334 Validation Loss: 0.7291818857192993\n",
      "Epoch 8289: Training Loss: 0.12056005001068115 Validation Loss: 0.7290041446685791\n",
      "Epoch 8290: Training Loss: 0.12048641095558803 Validation Loss: 0.728935956954956\n",
      "Epoch 8291: Training Loss: 0.12088334063688914 Validation Loss: 0.7289537787437439\n",
      "Epoch 8292: Training Loss: 0.12197339286406834 Validation Loss: 0.7290568947792053\n",
      "Epoch 8293: Training Loss: 0.12037663658459981 Validation Loss: 0.7294131517410278\n",
      "Epoch 8294: Training Loss: 0.12059326469898224 Validation Loss: 0.7296364307403564\n",
      "Epoch 8295: Training Loss: 0.12049794942140579 Validation Loss: 0.7296119928359985\n",
      "Epoch 8296: Training Loss: 0.1205269272128741 Validation Loss: 0.7295339703559875\n",
      "Epoch 8297: Training Loss: 0.12067018697659175 Validation Loss: 0.7289190292358398\n",
      "Epoch 8298: Training Loss: 0.12057351072629292 Validation Loss: 0.7287643551826477\n",
      "Epoch 8299: Training Loss: 0.12049703548351924 Validation Loss: 0.7284414172172546\n",
      "Epoch 8300: Training Loss: 0.12009335805972417 Validation Loss: 0.7287570238113403\n",
      "Epoch 8301: Training Loss: 0.120954064031442 Validation Loss: 0.7290377020835876\n",
      "Epoch 8302: Training Loss: 0.1204623356461525 Validation Loss: 0.7290915250778198\n",
      "Epoch 8303: Training Loss: 0.12101377050081889 Validation Loss: 0.7297069430351257\n",
      "Epoch 8304: Training Loss: 0.12048718333244324 Validation Loss: 0.7292386293411255\n",
      "Epoch 8305: Training Loss: 0.12037785351276398 Validation Loss: 0.7289184927940369\n",
      "Epoch 8306: Training Loss: 0.12071377287308376 Validation Loss: 0.7289522886276245\n",
      "Epoch 8307: Training Loss: 0.12065118302901585 Validation Loss: 0.7288822531700134\n",
      "Epoch 8308: Training Loss: 0.12000936766465505 Validation Loss: 0.7291173338890076\n",
      "Epoch 8309: Training Loss: 0.12062373012304306 Validation Loss: 0.729449450969696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8310: Training Loss: 0.12076407670974731 Validation Loss: 0.7293635606765747\n",
      "Epoch 8311: Training Loss: 0.12035262584686279 Validation Loss: 0.7292351126670837\n",
      "Epoch 8312: Training Loss: 0.12019277612368266 Validation Loss: 0.729054868221283\n",
      "Epoch 8313: Training Loss: 0.12029627958933513 Validation Loss: 0.7292259335517883\n",
      "Epoch 8314: Training Loss: 0.12007095168034236 Validation Loss: 0.7296009659767151\n",
      "Epoch 8315: Training Loss: 0.12029394507408142 Validation Loss: 0.7292163968086243\n",
      "Epoch 8316: Training Loss: 0.1207105815410614 Validation Loss: 0.7289565801620483\n",
      "Epoch 8317: Training Loss: 0.12132049600283305 Validation Loss: 0.7289173007011414\n",
      "Epoch 8318: Training Loss: 0.12026839206616084 Validation Loss: 0.7290407419204712\n",
      "Epoch 8319: Training Loss: 0.1202257474263509 Validation Loss: 0.7291873693466187\n",
      "Epoch 8320: Training Loss: 0.12015983710686366 Validation Loss: 0.7289197444915771\n",
      "Epoch 8321: Training Loss: 0.12038061022758484 Validation Loss: 0.7294057011604309\n",
      "Epoch 8322: Training Loss: 0.12010548015435536 Validation Loss: 0.7293359041213989\n",
      "Epoch 8323: Training Loss: 0.120212122797966 Validation Loss: 0.7293789982795715\n",
      "Epoch 8324: Training Loss: 0.1202470933397611 Validation Loss: 0.7296111583709717\n",
      "Epoch 8325: Training Loss: 0.12013123432795207 Validation Loss: 0.7295416593551636\n",
      "Epoch 8326: Training Loss: 0.12026939292748769 Validation Loss: 0.729830265045166\n",
      "Epoch 8327: Training Loss: 0.11973932137091954 Validation Loss: 0.7294730544090271\n",
      "Epoch 8328: Training Loss: 0.12093426287174225 Validation Loss: 0.7290738821029663\n",
      "Epoch 8329: Training Loss: 0.12026911228895187 Validation Loss: 0.7290661334991455\n",
      "Epoch 8330: Training Loss: 0.12000861763954163 Validation Loss: 0.7294400334358215\n",
      "Epoch 8331: Training Loss: 0.11996744324763615 Validation Loss: 0.7294459342956543\n",
      "Epoch 8332: Training Loss: 0.12079984694719315 Validation Loss: 0.7299528121948242\n",
      "Epoch 8333: Training Loss: 0.11978249500195186 Validation Loss: 0.7299726605415344\n",
      "Epoch 8334: Training Loss: 0.11981885135173798 Validation Loss: 0.729911208152771\n",
      "Epoch 8335: Training Loss: 0.12019946177800496 Validation Loss: 0.7292992472648621\n",
      "Epoch 8336: Training Loss: 0.12012132008870442 Validation Loss: 0.7290897965431213\n",
      "Epoch 8337: Training Loss: 0.12003469467163086 Validation Loss: 0.729105532169342\n",
      "Epoch 8338: Training Loss: 0.12011577188968658 Validation Loss: 0.7290767431259155\n",
      "Epoch 8339: Training Loss: 0.12087001899878184 Validation Loss: 0.729174792766571\n",
      "Epoch 8340: Training Loss: 0.11991557478904724 Validation Loss: 0.7292745113372803\n",
      "Epoch 8341: Training Loss: 0.12007277955611546 Validation Loss: 0.7297462224960327\n",
      "Epoch 8342: Training Loss: 0.1201138769586881 Validation Loss: 0.7297323346138\n",
      "Epoch 8343: Training Loss: 0.12004652867714564 Validation Loss: 0.7298232913017273\n",
      "Epoch 8344: Training Loss: 0.11999304095904033 Validation Loss: 0.7296349406242371\n",
      "Epoch 8345: Training Loss: 0.11992979794740677 Validation Loss: 0.7295206785202026\n",
      "Epoch 8346: Training Loss: 0.11972243835528691 Validation Loss: 0.7294247150421143\n",
      "Epoch 8347: Training Loss: 0.1199721246957779 Validation Loss: 0.7296346426010132\n",
      "Epoch 8348: Training Loss: 0.11996409793694814 Validation Loss: 0.7298000454902649\n",
      "Epoch 8349: Training Loss: 0.11984595656394958 Validation Loss: 0.7299490571022034\n",
      "Epoch 8350: Training Loss: 0.12008740256230037 Validation Loss: 0.7299295663833618\n",
      "Epoch 8351: Training Loss: 0.11994805683692296 Validation Loss: 0.7292461395263672\n",
      "Epoch 8352: Training Loss: 0.11985695610443751 Validation Loss: 0.7294495701789856\n",
      "Epoch 8353: Training Loss: 0.11971916009982426 Validation Loss: 0.7295137643814087\n",
      "Epoch 8354: Training Loss: 0.11910590281089146 Validation Loss: 0.7296238541603088\n",
      "Epoch 8355: Training Loss: 0.12031117578347524 Validation Loss: 0.7296565175056458\n",
      "Epoch 8356: Training Loss: 0.12011201679706573 Validation Loss: 0.7296959161758423\n",
      "Epoch 8357: Training Loss: 0.11999350289503734 Validation Loss: 0.7295843958854675\n",
      "Epoch 8358: Training Loss: 0.11969731748104095 Validation Loss: 0.7292046546936035\n",
      "Epoch 8359: Training Loss: 0.12003104637066524 Validation Loss: 0.7291480302810669\n",
      "Epoch 8360: Training Loss: 0.12037226557731628 Validation Loss: 0.729465126991272\n",
      "Epoch 8361: Training Loss: 0.11959594488143921 Validation Loss: 0.7296268343925476\n",
      "Epoch 8362: Training Loss: 0.120355191330115 Validation Loss: 0.7294765114784241\n",
      "Epoch 8363: Training Loss: 0.11976359287897746 Validation Loss: 0.7295140027999878\n",
      "Epoch 8364: Training Loss: 0.1207033966978391 Validation Loss: 0.7299944758415222\n",
      "Epoch 8365: Training Loss: 0.11964440594116847 Validation Loss: 0.7300689220428467\n",
      "Epoch 8366: Training Loss: 0.11985181520382564 Validation Loss: 0.73003089427948\n",
      "Epoch 8367: Training Loss: 0.11953009416659673 Validation Loss: 0.729969322681427\n",
      "Epoch 8368: Training Loss: 0.11973132689793904 Validation Loss: 0.7300382256507874\n",
      "Epoch 8369: Training Loss: 0.12042512993017833 Validation Loss: 0.7299768328666687\n",
      "Epoch 8370: Training Loss: 0.12015776087840398 Validation Loss: 0.7294908761978149\n",
      "Epoch 8371: Training Loss: 0.11974110454320908 Validation Loss: 0.7293460369110107\n",
      "Epoch 8372: Training Loss: 0.11960442860921223 Validation Loss: 0.7295124530792236\n",
      "Epoch 8373: Training Loss: 0.11977694928646088 Validation Loss: 0.7294121980667114\n",
      "Epoch 8374: Training Loss: 0.11918764561414719 Validation Loss: 0.7297987341880798\n",
      "Epoch 8375: Training Loss: 0.11957269161939621 Validation Loss: 0.7298983335494995\n",
      "Epoch 8376: Training Loss: 0.12057984868685405 Validation Loss: 0.730303943157196\n",
      "Epoch 8377: Training Loss: 0.11961714426676433 Validation Loss: 0.7299634218215942\n",
      "Epoch 8378: Training Loss: 0.11933529873689015 Validation Loss: 0.7294926643371582\n",
      "Epoch 8379: Training Loss: 0.11946692069371541 Validation Loss: 0.7296983003616333\n",
      "Epoch 8380: Training Loss: 0.11955826232830684 Validation Loss: 0.7297065258026123\n",
      "Epoch 8381: Training Loss: 0.11939530074596405 Validation Loss: 0.7295827865600586\n",
      "Epoch 8382: Training Loss: 0.11970582852760951 Validation Loss: 0.7295457720756531\n",
      "Epoch 8383: Training Loss: 0.11905630429585774 Validation Loss: 0.7297290563583374\n",
      "Epoch 8384: Training Loss: 0.11974440515041351 Validation Loss: 0.7294880151748657\n",
      "Epoch 8385: Training Loss: 0.11949186523755391 Validation Loss: 0.7302417159080505\n",
      "Epoch 8386: Training Loss: 0.11971265822649002 Validation Loss: 0.7300040125846863\n",
      "Epoch 8387: Training Loss: 0.11926049242417018 Validation Loss: 0.7299118041992188\n",
      "Epoch 8388: Training Loss: 0.12181144704421361 Validation Loss: 0.7298347353935242\n",
      "Epoch 8389: Training Loss: 0.11919445047775905 Validation Loss: 0.7295303344726562\n",
      "Epoch 8390: Training Loss: 0.11924538016319275 Validation Loss: 0.7294731140136719\n",
      "Epoch 8391: Training Loss: 0.11957034220298131 Validation Loss: 0.7300185561180115\n",
      "Epoch 8392: Training Loss: 0.11934276918570201 Validation Loss: 0.730210542678833\n",
      "Epoch 8393: Training Loss: 0.12012978146473567 Validation Loss: 0.7300520539283752\n",
      "Epoch 8394: Training Loss: 0.11954380323489507 Validation Loss: 0.7304628491401672\n",
      "Epoch 8395: Training Loss: 0.119466632604599 Validation Loss: 0.7300394773483276\n",
      "Epoch 8396: Training Loss: 0.11955158412456512 Validation Loss: 0.72990483045578\n",
      "Epoch 8397: Training Loss: 0.12057097007830937 Validation Loss: 0.729650616645813\n",
      "Epoch 8398: Training Loss: 0.11941294620434444 Validation Loss: 0.729583203792572\n",
      "Epoch 8399: Training Loss: 0.11976106216510136 Validation Loss: 0.7295963168144226\n",
      "Epoch 8400: Training Loss: 0.11948817471663158 Validation Loss: 0.7295641899108887\n",
      "Epoch 8401: Training Loss: 0.11925867448250453 Validation Loss: 0.7299090027809143\n",
      "Epoch 8402: Training Loss: 0.11948758363723755 Validation Loss: 0.7298373579978943\n",
      "Epoch 8403: Training Loss: 0.11946974198023479 Validation Loss: 0.7301464080810547\n",
      "Epoch 8404: Training Loss: 0.11920057982206345 Validation Loss: 0.7304112315177917\n",
      "Epoch 8405: Training Loss: 0.11938682695229848 Validation Loss: 0.7304818034172058\n",
      "Epoch 8406: Training Loss: 0.11966806650161743 Validation Loss: 0.7303491234779358\n",
      "Epoch 8407: Training Loss: 0.1198137750228246 Validation Loss: 0.7301312685012817\n",
      "Epoch 8408: Training Loss: 0.11931628982226054 Validation Loss: 0.7299652099609375\n",
      "Epoch 8409: Training Loss: 0.119233933587869 Validation Loss: 0.7298819422721863\n",
      "Epoch 8410: Training Loss: 0.11892204731702805 Validation Loss: 0.7298168540000916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8411: Training Loss: 0.11929633468389511 Validation Loss: 0.7295671701431274\n",
      "Epoch 8412: Training Loss: 0.11915531257788341 Validation Loss: 0.730102002620697\n",
      "Epoch 8413: Training Loss: 0.11895943681399028 Validation Loss: 0.7302272915840149\n",
      "Epoch 8414: Training Loss: 0.11960079769293468 Validation Loss: 0.7304359078407288\n",
      "Epoch 8415: Training Loss: 0.11990729967753093 Validation Loss: 0.7305486798286438\n",
      "Epoch 8416: Training Loss: 0.12003264824549358 Validation Loss: 0.7304571866989136\n",
      "Epoch 8417: Training Loss: 0.11906116455793381 Validation Loss: 0.7303881049156189\n",
      "Epoch 8418: Training Loss: 0.11941224584976833 Validation Loss: 0.7304080128669739\n",
      "Epoch 8419: Training Loss: 0.11904539167881012 Validation Loss: 0.7302056550979614\n",
      "Epoch 8420: Training Loss: 0.11887819071610768 Validation Loss: 0.7301616668701172\n",
      "Epoch 8421: Training Loss: 0.11937837302684784 Validation Loss: 0.7301329374313354\n",
      "Epoch 8422: Training Loss: 0.11897047112385432 Validation Loss: 0.7302853465080261\n",
      "Epoch 8423: Training Loss: 0.11903591950734456 Validation Loss: 0.7305001616477966\n",
      "Epoch 8424: Training Loss: 0.11916986107826233 Validation Loss: 0.7305179834365845\n",
      "Epoch 8425: Training Loss: 0.11900470902522405 Validation Loss: 0.7306467294692993\n",
      "Epoch 8426: Training Loss: 0.11880260705947876 Validation Loss: 0.7306106090545654\n",
      "Epoch 8427: Training Loss: 0.11886560420195262 Validation Loss: 0.7304553985595703\n",
      "Epoch 8428: Training Loss: 0.11984997242689133 Validation Loss: 0.7300829887390137\n",
      "Epoch 8429: Training Loss: 0.11945391446352005 Validation Loss: 0.7297608852386475\n",
      "Epoch 8430: Training Loss: 0.11895221223433812 Validation Loss: 0.7297854423522949\n",
      "Epoch 8431: Training Loss: 0.11899135758479436 Validation Loss: 0.7304186224937439\n",
      "Epoch 8432: Training Loss: 0.11878656099239986 Validation Loss: 0.7307066321372986\n",
      "Epoch 8433: Training Loss: 0.1189508984486262 Validation Loss: 0.7309537529945374\n",
      "Epoch 8434: Training Loss: 0.1189838449160258 Validation Loss: 0.7303106188774109\n",
      "Epoch 8435: Training Loss: 0.11901996533075969 Validation Loss: 0.7300649285316467\n",
      "Epoch 8436: Training Loss: 0.11877710620562236 Validation Loss: 0.7302088737487793\n",
      "Epoch 8437: Training Loss: 0.11933276802301407 Validation Loss: 0.7303128242492676\n",
      "Epoch 8438: Training Loss: 0.11960850407679875 Validation Loss: 0.7300655245780945\n",
      "Epoch 8439: Training Loss: 0.11901479462782542 Validation Loss: 0.7303730845451355\n",
      "Epoch 8440: Training Loss: 0.11889000236988068 Validation Loss: 0.7302305102348328\n",
      "Epoch 8441: Training Loss: 0.11918550978104274 Validation Loss: 0.7305092811584473\n",
      "Epoch 8442: Training Loss: 0.11866628626982371 Validation Loss: 0.7305670976638794\n",
      "Epoch 8443: Training Loss: 0.11899912854035695 Validation Loss: 0.7308595180511475\n",
      "Epoch 8444: Training Loss: 0.11894842485586803 Validation Loss: 0.7303344011306763\n",
      "Epoch 8445: Training Loss: 0.11888344089190166 Validation Loss: 0.7301966547966003\n",
      "Epoch 8446: Training Loss: 0.11896942059199016 Validation Loss: 0.7302599549293518\n",
      "Epoch 8447: Training Loss: 0.119429645438989 Validation Loss: 0.7306627631187439\n",
      "Epoch 8448: Training Loss: 0.11909933388233185 Validation Loss: 0.7305066585540771\n",
      "Epoch 8449: Training Loss: 0.11923869699239731 Validation Loss: 0.7306371331214905\n",
      "Epoch 8450: Training Loss: 0.1186552569270134 Validation Loss: 0.7307364344596863\n",
      "Epoch 8451: Training Loss: 0.11886802812417348 Validation Loss: 0.730911374092102\n",
      "Epoch 8452: Training Loss: 0.11880866438150406 Validation Loss: 0.7311973571777344\n",
      "Epoch 8453: Training Loss: 0.11844431360562642 Validation Loss: 0.7307085990905762\n",
      "Epoch 8454: Training Loss: 0.11854712913433711 Validation Loss: 0.730449914932251\n",
      "Epoch 8455: Training Loss: 0.11887169132630031 Validation Loss: 0.7303991317749023\n",
      "Epoch 8456: Training Loss: 0.11851100623607635 Validation Loss: 0.7305274605751038\n",
      "Epoch 8457: Training Loss: 0.1188931589325269 Validation Loss: 0.7307258248329163\n",
      "Epoch 8458: Training Loss: 0.11871359745661418 Validation Loss: 0.7307193875312805\n",
      "Epoch 8459: Training Loss: 0.11937423547108968 Validation Loss: 0.7303522825241089\n",
      "Epoch 8460: Training Loss: 0.11846659580866496 Validation Loss: 0.7302894592285156\n",
      "Epoch 8461: Training Loss: 0.11890651782353719 Validation Loss: 0.7302888035774231\n",
      "Epoch 8462: Training Loss: 0.11946006367603938 Validation Loss: 0.7306051254272461\n",
      "Epoch 8463: Training Loss: 0.11864566057920456 Validation Loss: 0.7311669588088989\n",
      "Epoch 8464: Training Loss: 0.11877965927124023 Validation Loss: 0.7310121655464172\n",
      "Epoch 8465: Training Loss: 0.1186670462290446 Validation Loss: 0.7311497926712036\n",
      "Epoch 8466: Training Loss: 0.1180548220872879 Validation Loss: 0.7308672666549683\n",
      "Epoch 8467: Training Loss: 0.11862325668334961 Validation Loss: 0.7308294773101807\n",
      "Epoch 8468: Training Loss: 0.1185733328262965 Validation Loss: 0.7311527132987976\n",
      "Epoch 8469: Training Loss: 0.11881392697493236 Validation Loss: 0.7309842705726624\n",
      "Epoch 8470: Training Loss: 0.11853645990292232 Validation Loss: 0.7308018207550049\n",
      "Epoch 8471: Training Loss: 0.1186454047759374 Validation Loss: 0.730811595916748\n",
      "Epoch 8472: Training Loss: 0.11896713078022003 Validation Loss: 0.7307640314102173\n",
      "Epoch 8473: Training Loss: 0.11874898771444957 Validation Loss: 0.7305602431297302\n",
      "Epoch 8474: Training Loss: 0.11909494797388713 Validation Loss: 0.7306237816810608\n",
      "Epoch 8475: Training Loss: 0.11881412069002788 Validation Loss: 0.7304584383964539\n",
      "Epoch 8476: Training Loss: 0.11841854453086853 Validation Loss: 0.730722188949585\n",
      "Epoch 8477: Training Loss: 0.11845425764719646 Validation Loss: 0.7313063740730286\n",
      "Epoch 8478: Training Loss: 0.11867848287026088 Validation Loss: 0.7317183017730713\n",
      "Epoch 8479: Training Loss: 0.11867857972780864 Validation Loss: 0.7313206791877747\n",
      "Epoch 8480: Training Loss: 0.11898114035526912 Validation Loss: 0.7309308648109436\n",
      "Epoch 8481: Training Loss: 0.11832107106844585 Validation Loss: 0.7304979562759399\n",
      "Epoch 8482: Training Loss: 0.1187063604593277 Validation Loss: 0.7303856015205383\n",
      "Epoch 8483: Training Loss: 0.11856512477000554 Validation Loss: 0.7300724387168884\n",
      "Epoch 8484: Training Loss: 0.11840333541234334 Validation Loss: 0.7301634550094604\n",
      "Epoch 8485: Training Loss: 0.1182085524002711 Validation Loss: 0.7304673194885254\n",
      "Epoch 8486: Training Loss: 0.11827761679887772 Validation Loss: 0.7311832308769226\n",
      "Epoch 8487: Training Loss: 0.11850596964359283 Validation Loss: 0.7312547564506531\n",
      "Epoch 8488: Training Loss: 0.11884363989035289 Validation Loss: 0.7313070893287659\n",
      "Epoch 8489: Training Loss: 0.11830103149016698 Validation Loss: 0.7312972545623779\n",
      "Epoch 8490: Training Loss: 0.11813522130250931 Validation Loss: 0.7309173941612244\n",
      "Epoch 8491: Training Loss: 0.1182885617017746 Validation Loss: 0.7308182120323181\n",
      "Epoch 8492: Training Loss: 0.11851287384827931 Validation Loss: 0.7307689189910889\n",
      "Epoch 8493: Training Loss: 0.11811255911986034 Validation Loss: 0.7308595180511475\n",
      "Epoch 8494: Training Loss: 0.11850497374931972 Validation Loss: 0.7308706045150757\n",
      "Epoch 8495: Training Loss: 0.11829673250516255 Validation Loss: 0.7314026951789856\n",
      "Epoch 8496: Training Loss: 0.11844950914382935 Validation Loss: 0.7318192720413208\n",
      "Epoch 8497: Training Loss: 0.11826587716738383 Validation Loss: 0.7315343022346497\n",
      "Epoch 8498: Training Loss: 0.11826505263646443 Validation Loss: 0.7309134006500244\n",
      "Epoch 8499: Training Loss: 0.11794839551051457 Validation Loss: 0.7304362058639526\n",
      "Epoch 8500: Training Loss: 0.11837262411912282 Validation Loss: 0.7302547097206116\n",
      "Epoch 8501: Training Loss: 0.11871826152006786 Validation Loss: 0.7307033538818359\n",
      "Epoch 8502: Training Loss: 0.11786601195732753 Validation Loss: 0.7308884859085083\n",
      "Epoch 8503: Training Loss: 0.11839237809181213 Validation Loss: 0.7313312292098999\n",
      "Epoch 8504: Training Loss: 0.11819177865982056 Validation Loss: 0.7317628860473633\n",
      "Epoch 8505: Training Loss: 0.1181224783261617 Validation Loss: 0.7313719987869263\n",
      "Epoch 8506: Training Loss: 0.11850983897844951 Validation Loss: 0.7307326793670654\n",
      "Epoch 8507: Training Loss: 0.11813366164763768 Validation Loss: 0.7307891845703125\n",
      "Epoch 8508: Training Loss: 0.11796582738558452 Validation Loss: 0.7304674386978149\n",
      "Epoch 8509: Training Loss: 0.11978468547264735 Validation Loss: 0.7307676076889038\n",
      "Epoch 8510: Training Loss: 0.11794880032539368 Validation Loss: 0.7311873435974121\n",
      "Epoch 8511: Training Loss: 0.11811308562755585 Validation Loss: 0.7313624620437622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8512: Training Loss: 0.11834526807069778 Validation Loss: 0.7318671345710754\n",
      "Epoch 8513: Training Loss: 0.11792997519175212 Validation Loss: 0.7313909530639648\n",
      "Epoch 8514: Training Loss: 0.11822953323523204 Validation Loss: 0.73104327917099\n",
      "Epoch 8515: Training Loss: 0.11787256350119908 Validation Loss: 0.730732798576355\n",
      "Epoch 8516: Training Loss: 0.11815099169810613 Validation Loss: 0.7306729555130005\n",
      "Epoch 8517: Training Loss: 0.11867437014977138 Validation Loss: 0.7311263680458069\n",
      "Epoch 8518: Training Loss: 0.11784449219703674 Validation Loss: 0.7313547134399414\n",
      "Epoch 8519: Training Loss: 0.11811338365077972 Validation Loss: 0.7315794825553894\n",
      "Epoch 8520: Training Loss: 0.11798090984423955 Validation Loss: 0.7313904166221619\n",
      "Epoch 8521: Training Loss: 0.11850715180238088 Validation Loss: 0.7311137914657593\n",
      "Epoch 8522: Training Loss: 0.1179441586136818 Validation Loss: 0.7311493754386902\n",
      "Epoch 8523: Training Loss: 0.1181791474421819 Validation Loss: 0.7312375903129578\n",
      "Epoch 8524: Training Loss: 0.11760429789622624 Validation Loss: 0.7310473322868347\n",
      "Epoch 8525: Training Loss: 0.11825496951738994 Validation Loss: 0.731557309627533\n",
      "Epoch 8526: Training Loss: 0.11812532196442287 Validation Loss: 0.731645941734314\n",
      "Epoch 8527: Training Loss: 0.118458258608977 Validation Loss: 0.7313802242279053\n",
      "Epoch 8528: Training Loss: 0.11808453500270844 Validation Loss: 0.7311282157897949\n",
      "Epoch 8529: Training Loss: 0.11756591498851776 Validation Loss: 0.7314147353172302\n",
      "Epoch 8530: Training Loss: 0.11861280848582585 Validation Loss: 0.7316297888755798\n",
      "Epoch 8531: Training Loss: 0.11829480032126109 Validation Loss: 0.7315917611122131\n",
      "Epoch 8532: Training Loss: 0.11765968054533005 Validation Loss: 0.7316076159477234\n",
      "Epoch 8533: Training Loss: 0.11949020127455394 Validation Loss: 0.7315102219581604\n",
      "Epoch 8534: Training Loss: 0.11797554542620976 Validation Loss: 0.7319157719612122\n",
      "Epoch 8535: Training Loss: 0.11805181701978047 Validation Loss: 0.732067883014679\n",
      "Epoch 8536: Training Loss: 0.11775100976228714 Validation Loss: 0.7317461371421814\n",
      "Epoch 8537: Training Loss: 0.1177436759074529 Validation Loss: 0.7313109040260315\n",
      "Epoch 8538: Training Loss: 0.1176791066924731 Validation Loss: 0.731489896774292\n",
      "Epoch 8539: Training Loss: 0.11742254346609116 Validation Loss: 0.7316100001335144\n",
      "Epoch 8540: Training Loss: 0.1175829569498698 Validation Loss: 0.7315649390220642\n",
      "Epoch 8541: Training Loss: 0.11825549850861232 Validation Loss: 0.7311236262321472\n",
      "Epoch 8542: Training Loss: 0.117831456164519 Validation Loss: 0.731201171875\n",
      "Epoch 8543: Training Loss: 0.11819297571976979 Validation Loss: 0.7313242554664612\n",
      "Epoch 8544: Training Loss: 0.11796052753925323 Validation Loss: 0.7314020991325378\n",
      "Epoch 8545: Training Loss: 0.11826893935600917 Validation Loss: 0.7314938902854919\n",
      "Epoch 8546: Training Loss: 0.11836208154757817 Validation Loss: 0.7310953140258789\n",
      "Epoch 8547: Training Loss: 0.11774756759405136 Validation Loss: 0.7312783598899841\n",
      "Epoch 8548: Training Loss: 0.11815300583839417 Validation Loss: 0.7312062978744507\n",
      "Epoch 8549: Training Loss: 0.11746442566315334 Validation Loss: 0.7311184406280518\n",
      "Epoch 8550: Training Loss: 0.11724806825319926 Validation Loss: 0.7313969135284424\n",
      "Epoch 8551: Training Loss: 0.11800326406955719 Validation Loss: 0.7315940260887146\n",
      "Epoch 8552: Training Loss: 0.11769825965166092 Validation Loss: 0.7318413257598877\n",
      "Epoch 8553: Training Loss: 0.11737648397684097 Validation Loss: 0.7317233085632324\n",
      "Epoch 8554: Training Loss: 0.11775169024864833 Validation Loss: 0.731781005859375\n",
      "Epoch 8555: Training Loss: 0.11753680805365245 Validation Loss: 0.7321357727050781\n",
      "Epoch 8556: Training Loss: 0.11725785583257675 Validation Loss: 0.731802761554718\n",
      "Epoch 8557: Training Loss: 0.11724860966205597 Validation Loss: 0.7318138480186462\n",
      "Epoch 8558: Training Loss: 0.11829166611035664 Validation Loss: 0.732039213180542\n",
      "Epoch 8559: Training Loss: 0.11752865463495255 Validation Loss: 0.7318771481513977\n",
      "Epoch 8560: Training Loss: 0.11749670654535294 Validation Loss: 0.7317451238632202\n",
      "Epoch 8561: Training Loss: 0.11797026544809341 Validation Loss: 0.7316135764122009\n",
      "Epoch 8562: Training Loss: 0.11727434396743774 Validation Loss: 0.7319300174713135\n",
      "Epoch 8563: Training Loss: 0.11804067343473434 Validation Loss: 0.7316869497299194\n",
      "Epoch 8564: Training Loss: 0.11744708567857742 Validation Loss: 0.7315714955329895\n",
      "Epoch 8565: Training Loss: 0.11759275446335475 Validation Loss: 0.7313901782035828\n",
      "Epoch 8566: Training Loss: 0.1174410159389178 Validation Loss: 0.731624960899353\n",
      "Epoch 8567: Training Loss: 0.1173115720351537 Validation Loss: 0.7315012812614441\n",
      "Epoch 8568: Training Loss: 0.11741502583026886 Validation Loss: 0.7316202521324158\n",
      "Epoch 8569: Training Loss: 0.11752849320570628 Validation Loss: 0.7314102053642273\n",
      "Epoch 8570: Training Loss: 0.11765265216430028 Validation Loss: 0.7312645316123962\n",
      "Epoch 8571: Training Loss: 0.11893696586290996 Validation Loss: 0.7317888140678406\n",
      "Epoch 8572: Training Loss: 0.11795057356357574 Validation Loss: 0.7318699359893799\n",
      "Epoch 8573: Training Loss: 0.11882174511750539 Validation Loss: 0.7321290373802185\n",
      "Epoch 8574: Training Loss: 0.11776323864857356 Validation Loss: 0.7319916486740112\n",
      "Epoch 8575: Training Loss: 0.11756698290506999 Validation Loss: 0.7317234873771667\n",
      "Epoch 8576: Training Loss: 0.11760035902261734 Validation Loss: 0.7317169308662415\n",
      "Epoch 8577: Training Loss: 0.1176063244541486 Validation Loss: 0.7320191860198975\n",
      "Epoch 8578: Training Loss: 0.11740974833567937 Validation Loss: 0.7321363091468811\n",
      "Epoch 8579: Training Loss: 0.11743663996458054 Validation Loss: 0.7322524189949036\n",
      "Epoch 8580: Training Loss: 0.11743929733832677 Validation Loss: 0.7317323088645935\n",
      "Epoch 8581: Training Loss: 0.11730714390675227 Validation Loss: 0.731344997882843\n",
      "Epoch 8582: Training Loss: 0.1174875224630038 Validation Loss: 0.7316962480545044\n",
      "Epoch 8583: Training Loss: 0.11743734528621037 Validation Loss: 0.7320305109024048\n",
      "Epoch 8584: Training Loss: 0.1176563228170077 Validation Loss: 0.7323940396308899\n",
      "Epoch 8585: Training Loss: 0.11738617718219757 Validation Loss: 0.7323505878448486\n",
      "Epoch 8586: Training Loss: 0.11690853784481685 Validation Loss: 0.7319952249526978\n",
      "Epoch 8587: Training Loss: 0.11724494645992915 Validation Loss: 0.732032835483551\n",
      "Epoch 8588: Training Loss: 0.1169632226228714 Validation Loss: 0.7317493557929993\n",
      "Epoch 8589: Training Loss: 0.11732728530963261 Validation Loss: 0.731859028339386\n",
      "Epoch 8590: Training Loss: 0.11776077002286911 Validation Loss: 0.7317529320716858\n",
      "Epoch 8591: Training Loss: 0.11692932993173599 Validation Loss: 0.7317872047424316\n",
      "Epoch 8592: Training Loss: 0.11728728810946147 Validation Loss: 0.7314717173576355\n",
      "Epoch 8593: Training Loss: 0.1167876347899437 Validation Loss: 0.7320259213447571\n",
      "Epoch 8594: Training Loss: 0.11737543592850368 Validation Loss: 0.7319717407226562\n",
      "Epoch 8595: Training Loss: 0.11710964391628902 Validation Loss: 0.732114315032959\n",
      "Epoch 8596: Training Loss: 0.11852774024009705 Validation Loss: 0.7318766117095947\n",
      "Epoch 8597: Training Loss: 0.1175005907813708 Validation Loss: 0.7322197556495667\n",
      "Epoch 8598: Training Loss: 0.11711623271306355 Validation Loss: 0.7322123646736145\n",
      "Epoch 8599: Training Loss: 0.11736683299144109 Validation Loss: 0.7321689128875732\n",
      "Epoch 8600: Training Loss: 0.11745744695266087 Validation Loss: 0.7320671677589417\n",
      "Epoch 8601: Training Loss: 0.1174133022626241 Validation Loss: 0.7319276332855225\n",
      "Epoch 8602: Training Loss: 0.11728077630201976 Validation Loss: 0.7318727374076843\n",
      "Epoch 8603: Training Loss: 0.11828569322824478 Validation Loss: 0.7321056723594666\n",
      "Epoch 8604: Training Loss: 0.11875973641872406 Validation Loss: 0.7319082617759705\n",
      "Epoch 8605: Training Loss: 0.11705849568049113 Validation Loss: 0.7320957779884338\n",
      "Epoch 8606: Training Loss: 0.11781354000171025 Validation Loss: 0.7322337031364441\n",
      "Epoch 8607: Training Loss: 0.11735618859529495 Validation Loss: 0.7321271896362305\n",
      "Epoch 8608: Training Loss: 0.11775771031777064 Validation Loss: 0.7319259643554688\n",
      "Epoch 8609: Training Loss: 0.11693523079156876 Validation Loss: 0.7318964600563049\n",
      "Epoch 8610: Training Loss: 0.11732296148935954 Validation Loss: 0.7324647307395935\n",
      "Epoch 8611: Training Loss: 0.11741493890682857 Validation Loss: 0.7323094010353088\n",
      "Epoch 8612: Training Loss: 0.11682654668887456 Validation Loss: 0.7321983575820923\n",
      "Epoch 8613: Training Loss: 0.11694684127966563 Validation Loss: 0.7323111295700073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8614: Training Loss: 0.11710814634958903 Validation Loss: 0.7322915196418762\n",
      "Epoch 8615: Training Loss: 0.11691444118817647 Validation Loss: 0.7323411703109741\n",
      "Epoch 8616: Training Loss: 0.11700948576132457 Validation Loss: 0.732437014579773\n",
      "Epoch 8617: Training Loss: 0.11752561231454213 Validation Loss: 0.7323188185691833\n",
      "Epoch 8618: Training Loss: 0.1169402152299881 Validation Loss: 0.7320045232772827\n",
      "Epoch 8619: Training Loss: 0.11774232983589172 Validation Loss: 0.732096791267395\n",
      "Epoch 8620: Training Loss: 0.11747629443804423 Validation Loss: 0.732002854347229\n",
      "Epoch 8621: Training Loss: 0.11670092244942983 Validation Loss: 0.7320216298103333\n",
      "Epoch 8622: Training Loss: 0.11682343731323878 Validation Loss: 0.7323614358901978\n",
      "Epoch 8623: Training Loss: 0.11687769492467244 Validation Loss: 0.7326930165290833\n",
      "Epoch 8624: Training Loss: 0.11697875211636226 Validation Loss: 0.7328022718429565\n",
      "Epoch 8625: Training Loss: 0.11620804915825526 Validation Loss: 0.7322503924369812\n",
      "Epoch 8626: Training Loss: 0.11660140256086986 Validation Loss: 0.7319168448448181\n",
      "Epoch 8627: Training Loss: 0.11667599280675252 Validation Loss: 0.7319997549057007\n",
      "Epoch 8628: Training Loss: 0.116873304049174 Validation Loss: 0.7318270802497864\n",
      "Epoch 8629: Training Loss: 0.1168241376678149 Validation Loss: 0.7318083047866821\n",
      "Epoch 8630: Training Loss: 0.1170080949862798 Validation Loss: 0.7321882843971252\n",
      "Epoch 8631: Training Loss: 0.11696404715379079 Validation Loss: 0.7326611876487732\n",
      "Epoch 8632: Training Loss: 0.11759170889854431 Validation Loss: 0.7322660088539124\n",
      "Epoch 8633: Training Loss: 0.11742272228002548 Validation Loss: 0.7323209047317505\n",
      "Epoch 8634: Training Loss: 0.1166562909881274 Validation Loss: 0.7323481440544128\n",
      "Epoch 8635: Training Loss: 0.1169064889351527 Validation Loss: 0.7323063611984253\n",
      "Epoch 8636: Training Loss: 0.11745425313711166 Validation Loss: 0.7324995398521423\n",
      "Epoch 8637: Training Loss: 0.11662222693363826 Validation Loss: 0.7325613498687744\n",
      "Epoch 8638: Training Loss: 0.11789580682913463 Validation Loss: 0.7324985265731812\n",
      "Epoch 8639: Training Loss: 0.11626619348923366 Validation Loss: 0.7324830293655396\n",
      "Epoch 8640: Training Loss: 0.11667768160502116 Validation Loss: 0.7322805523872375\n",
      "Epoch 8641: Training Loss: 0.11684450507164001 Validation Loss: 0.7323266863822937\n",
      "Epoch 8642: Training Loss: 0.11674234519402187 Validation Loss: 0.7319821119308472\n",
      "Epoch 8643: Training Loss: 0.11674667398134868 Validation Loss: 0.7317734360694885\n",
      "Epoch 8644: Training Loss: 0.11715396245320638 Validation Loss: 0.7320554256439209\n",
      "Epoch 8645: Training Loss: 0.11641841133435567 Validation Loss: 0.7322450280189514\n",
      "Epoch 8646: Training Loss: 0.11784886072079341 Validation Loss: 0.732336699962616\n",
      "Epoch 8647: Training Loss: 0.11673258990049362 Validation Loss: 0.7322885394096375\n",
      "Epoch 8648: Training Loss: 0.11676470686992009 Validation Loss: 0.7323750257492065\n",
      "Epoch 8649: Training Loss: 0.11656811088323593 Validation Loss: 0.7322080731391907\n",
      "Epoch 8650: Training Loss: 0.11660122623046239 Validation Loss: 0.7320488691329956\n",
      "Epoch 8651: Training Loss: 0.11663330843051274 Validation Loss: 0.7320272326469421\n",
      "Epoch 8652: Training Loss: 0.11695663382609685 Validation Loss: 0.7319916486740112\n",
      "Epoch 8653: Training Loss: 0.11686885605255763 Validation Loss: 0.7322091460227966\n",
      "Epoch 8654: Training Loss: 0.11660318076610565 Validation Loss: 0.7323528528213501\n",
      "Epoch 8655: Training Loss: 0.11665670822064082 Validation Loss: 0.7324817776679993\n",
      "Epoch 8656: Training Loss: 0.11616965383291245 Validation Loss: 0.7329164147377014\n",
      "Epoch 8657: Training Loss: 0.11658670753240585 Validation Loss: 0.7328493595123291\n",
      "Epoch 8658: Training Loss: 0.11722107728322347 Validation Loss: 0.7330319881439209\n",
      "Epoch 8659: Training Loss: 0.11627045770486195 Validation Loss: 0.7332016825675964\n",
      "Epoch 8660: Training Loss: 0.11679907391468684 Validation Loss: 0.7331274151802063\n",
      "Epoch 8661: Training Loss: 0.11652513841787975 Validation Loss: 0.7330141663551331\n",
      "Epoch 8662: Training Loss: 0.11603514105081558 Validation Loss: 0.7328138947486877\n",
      "Epoch 8663: Training Loss: 0.11629592130581538 Validation Loss: 0.7324512004852295\n",
      "Epoch 8664: Training Loss: 0.11624959359566371 Validation Loss: 0.7323223352432251\n",
      "Epoch 8665: Training Loss: 0.11605316897233327 Validation Loss: 0.7324117422103882\n",
      "Epoch 8666: Training Loss: 0.11622337500254314 Validation Loss: 0.7324151396751404\n",
      "Epoch 8667: Training Loss: 0.11679592976967494 Validation Loss: 0.7325431704521179\n",
      "Epoch 8668: Training Loss: 0.11699253072341283 Validation Loss: 0.733220100402832\n",
      "Epoch 8669: Training Loss: 0.11648763964573543 Validation Loss: 0.7332581281661987\n",
      "Epoch 8670: Training Loss: 0.115801935394605 Validation Loss: 0.7333781719207764\n",
      "Epoch 8671: Training Loss: 0.11647624770800273 Validation Loss: 0.7333042025566101\n",
      "Epoch 8672: Training Loss: 0.11687585959831874 Validation Loss: 0.7328080534934998\n",
      "Epoch 8673: Training Loss: 0.11729327340920766 Validation Loss: 0.7329592704772949\n",
      "Epoch 8674: Training Loss: 0.1162190039952596 Validation Loss: 0.73262619972229\n",
      "Epoch 8675: Training Loss: 0.11656555781761806 Validation Loss: 0.7328580617904663\n",
      "Epoch 8676: Training Loss: 0.11662622789541881 Validation Loss: 0.7326075434684753\n",
      "Epoch 8677: Training Loss: 0.11676292369763057 Validation Loss: 0.7327250838279724\n",
      "Epoch 8678: Training Loss: 0.11630863447984059 Validation Loss: 0.7332805395126343\n",
      "Epoch 8679: Training Loss: 0.1167438452442487 Validation Loss: 0.7329704165458679\n",
      "Epoch 8680: Training Loss: 0.11618308971325557 Validation Loss: 0.7330276966094971\n",
      "Epoch 8681: Training Loss: 0.11645675450563431 Validation Loss: 0.7329369783401489\n",
      "Epoch 8682: Training Loss: 0.11632540822029114 Validation Loss: 0.7327613234519958\n",
      "Epoch 8683: Training Loss: 0.11594133575757344 Validation Loss: 0.732829213142395\n",
      "Epoch 8684: Training Loss: 0.11623458564281464 Validation Loss: 0.7331279516220093\n",
      "Epoch 8685: Training Loss: 0.1164866288503011 Validation Loss: 0.7329769730567932\n",
      "Epoch 8686: Training Loss: 0.11629420767227809 Validation Loss: 0.7326598167419434\n",
      "Epoch 8687: Training Loss: 0.11668345083793004 Validation Loss: 0.7326644062995911\n",
      "Epoch 8688: Training Loss: 0.11587926993767421 Validation Loss: 0.7327529788017273\n",
      "Epoch 8689: Training Loss: 0.11604018012682597 Validation Loss: 0.732581615447998\n",
      "Epoch 8690: Training Loss: 0.11613665024439494 Validation Loss: 0.7326894998550415\n",
      "Epoch 8691: Training Loss: 0.11518832047780354 Validation Loss: 0.7328640818595886\n",
      "Epoch 8692: Training Loss: 0.11642284939686458 Validation Loss: 0.7330846190452576\n",
      "Epoch 8693: Training Loss: 0.1159756009777387 Validation Loss: 0.7330645322799683\n",
      "Epoch 8694: Training Loss: 0.1161893904209137 Validation Loss: 0.7332437038421631\n",
      "Epoch 8695: Training Loss: 0.11596446484327316 Validation Loss: 0.7332950234413147\n",
      "Epoch 8696: Training Loss: 0.11605374266703923 Validation Loss: 0.7330337166786194\n",
      "Epoch 8697: Training Loss: 0.11665258804957072 Validation Loss: 0.7326768636703491\n",
      "Epoch 8698: Training Loss: 0.11626667280991872 Validation Loss: 0.7323284149169922\n",
      "Epoch 8699: Training Loss: 0.11611234396696091 Validation Loss: 0.7326091527938843\n",
      "Epoch 8700: Training Loss: 0.11692361781994502 Validation Loss: 0.7327520847320557\n",
      "Epoch 8701: Training Loss: 0.11620791008075078 Validation Loss: 0.7328303456306458\n",
      "Epoch 8702: Training Loss: 0.11555661509434383 Validation Loss: 0.7332007884979248\n",
      "Epoch 8703: Training Loss: 0.11623099943002065 Validation Loss: 0.7333934307098389\n",
      "Epoch 8704: Training Loss: 0.1160341277718544 Validation Loss: 0.7331510186195374\n",
      "Epoch 8705: Training Loss: 0.11566805094480515 Validation Loss: 0.7327186465263367\n",
      "Epoch 8706: Training Loss: 0.11661481608947118 Validation Loss: 0.7324404716491699\n",
      "Epoch 8707: Training Loss: 0.11655230571826299 Validation Loss: 0.7324705123901367\n",
      "Epoch 8708: Training Loss: 0.11612532536188762 Validation Loss: 0.7328349351882935\n",
      "Epoch 8709: Training Loss: 0.11607780307531357 Validation Loss: 0.733035683631897\n",
      "Epoch 8710: Training Loss: 0.11583077907562256 Validation Loss: 0.7331918478012085\n",
      "Epoch 8711: Training Loss: 0.116236113011837 Validation Loss: 0.733375608921051\n",
      "Epoch 8712: Training Loss: 0.11566407978534698 Validation Loss: 0.7329728603363037\n",
      "Epoch 8713: Training Loss: 0.11576665689547856 Validation Loss: 0.7329370379447937\n",
      "Epoch 8714: Training Loss: 0.11590161671241124 Validation Loss: 0.7331362366676331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8715: Training Loss: 0.11671942720810573 Validation Loss: 0.7335984706878662\n",
      "Epoch 8716: Training Loss: 0.11608947813510895 Validation Loss: 0.7335494756698608\n",
      "Epoch 8717: Training Loss: 0.1158333420753479 Validation Loss: 0.7336869239807129\n",
      "Epoch 8718: Training Loss: 0.11634435256322224 Validation Loss: 0.7341716885566711\n",
      "Epoch 8719: Training Loss: 0.11572072406609853 Validation Loss: 0.7336758971214294\n",
      "Epoch 8720: Training Loss: 0.11564923077821732 Validation Loss: 0.7336289286613464\n",
      "Epoch 8721: Training Loss: 0.11554473886887233 Validation Loss: 0.7330806255340576\n",
      "Epoch 8722: Training Loss: 0.11622585356235504 Validation Loss: 0.7330732941627502\n",
      "Epoch 8723: Training Loss: 0.11631759752829869 Validation Loss: 0.7336318492889404\n",
      "Epoch 8724: Training Loss: 0.11584291110436122 Validation Loss: 0.7335410714149475\n",
      "Epoch 8725: Training Loss: 0.11583159863948822 Validation Loss: 0.7334839105606079\n",
      "Epoch 8726: Training Loss: 0.11615611612796783 Validation Loss: 0.7329757213592529\n",
      "Epoch 8727: Training Loss: 0.11551471054553986 Validation Loss: 0.7328450679779053\n",
      "Epoch 8728: Training Loss: 0.11561371634403865 Validation Loss: 0.7334223985671997\n",
      "Epoch 8729: Training Loss: 0.116017480691274 Validation Loss: 0.7333433032035828\n",
      "Epoch 8730: Training Loss: 0.11581004410982132 Validation Loss: 0.7334966659545898\n",
      "Epoch 8731: Training Loss: 0.11557497829198837 Validation Loss: 0.7335465550422668\n",
      "Epoch 8732: Training Loss: 0.11566645403703053 Validation Loss: 0.7333157658576965\n",
      "Epoch 8733: Training Loss: 0.11601722985506058 Validation Loss: 0.7332655787467957\n",
      "Epoch 8734: Training Loss: 0.11622293293476105 Validation Loss: 0.7336530685424805\n",
      "Epoch 8735: Training Loss: 0.11583861460288365 Validation Loss: 0.7333666086196899\n",
      "Epoch 8736: Training Loss: 0.11572523166735967 Validation Loss: 0.733199954032898\n",
      "Epoch 8737: Training Loss: 0.11522345741589864 Validation Loss: 0.733271598815918\n",
      "Epoch 8738: Training Loss: 0.11532421410083771 Validation Loss: 0.7336603999137878\n",
      "Epoch 8739: Training Loss: 0.11603476355473201 Validation Loss: 0.7335779070854187\n",
      "Epoch 8740: Training Loss: 0.11618898808956146 Validation Loss: 0.7335939407348633\n",
      "Epoch 8741: Training Loss: 0.11586977541446686 Validation Loss: 0.733610987663269\n",
      "Epoch 8742: Training Loss: 0.1156432827313741 Validation Loss: 0.7340729236602783\n",
      "Epoch 8743: Training Loss: 0.11544185131788254 Validation Loss: 0.7336012125015259\n",
      "Epoch 8744: Training Loss: 0.11600306381781895 Validation Loss: 0.7331345677375793\n",
      "Epoch 8745: Training Loss: 0.11609331766764323 Validation Loss: 0.7328980565071106\n",
      "Epoch 8746: Training Loss: 0.11601040015618007 Validation Loss: 0.7328931093215942\n",
      "Epoch 8747: Training Loss: 0.1156347468495369 Validation Loss: 0.7330251932144165\n",
      "Epoch 8748: Training Loss: 0.1154654324054718 Validation Loss: 0.7339159846305847\n",
      "Epoch 8749: Training Loss: 0.11565408607323964 Validation Loss: 0.7340142726898193\n",
      "Epoch 8750: Training Loss: 0.11564422150452931 Validation Loss: 0.7343683242797852\n",
      "Epoch 8751: Training Loss: 0.11536415417989095 Validation Loss: 0.7335861325263977\n",
      "Epoch 8752: Training Loss: 0.11546268065770467 Validation Loss: 0.7333745360374451\n",
      "Epoch 8753: Training Loss: 0.1160310630997022 Validation Loss: 0.7328792214393616\n",
      "Epoch 8754: Training Loss: 0.11515300472577412 Validation Loss: 0.733224630355835\n",
      "Epoch 8755: Training Loss: 0.11563657720883687 Validation Loss: 0.7334986329078674\n",
      "Epoch 8756: Training Loss: 0.11567405859629314 Validation Loss: 0.7336841821670532\n",
      "Epoch 8757: Training Loss: 0.11535922437906265 Validation Loss: 0.7336997389793396\n",
      "Epoch 8758: Training Loss: 0.11546492079893748 Validation Loss: 0.733855664730072\n",
      "Epoch 8759: Training Loss: 0.11572775741418202 Validation Loss: 0.7339944839477539\n",
      "Epoch 8760: Training Loss: 0.11546796063582103 Validation Loss: 0.733809232711792\n",
      "Epoch 8761: Training Loss: 0.11493560920159022 Validation Loss: 0.7332897186279297\n",
      "Epoch 8762: Training Loss: 0.11580046514670055 Validation Loss: 0.7334774136543274\n",
      "Epoch 8763: Training Loss: 0.11558673530817032 Validation Loss: 0.7335525155067444\n",
      "Epoch 8764: Training Loss: 0.11534526447455089 Validation Loss: 0.733464241027832\n",
      "Epoch 8765: Training Loss: 0.11507524301608403 Validation Loss: 0.7334360480308533\n",
      "Epoch 8766: Training Loss: 0.11629192034403484 Validation Loss: 0.7335201501846313\n",
      "Epoch 8767: Training Loss: 0.1154577707250913 Validation Loss: 0.7336940169334412\n",
      "Epoch 8768: Training Loss: 0.11566828687985738 Validation Loss: 0.7336434125900269\n",
      "Epoch 8769: Training Loss: 0.11472351600726445 Validation Loss: 0.7335677742958069\n",
      "Epoch 8770: Training Loss: 0.1152750054995219 Validation Loss: 0.7335073351860046\n",
      "Epoch 8771: Training Loss: 0.11582424491643906 Validation Loss: 0.7334638833999634\n",
      "Epoch 8772: Training Loss: 0.11523393789927165 Validation Loss: 0.7332257628440857\n",
      "Epoch 8773: Training Loss: 0.11519648134708405 Validation Loss: 0.7334620356559753\n",
      "Epoch 8774: Training Loss: 0.11509563525517781 Validation Loss: 0.7334798574447632\n",
      "Epoch 8775: Training Loss: 0.1152329867084821 Validation Loss: 0.7338834404945374\n",
      "Epoch 8776: Training Loss: 0.11530775080124538 Validation Loss: 0.7337994575500488\n",
      "Epoch 8777: Training Loss: 0.11501658956209819 Validation Loss: 0.7339394092559814\n",
      "Epoch 8778: Training Loss: 0.11511243631442387 Validation Loss: 0.7342242002487183\n",
      "Epoch 8779: Training Loss: 0.11495758344729741 Validation Loss: 0.7338842153549194\n",
      "Epoch 8780: Training Loss: 0.11713834603627522 Validation Loss: 0.7337151169776917\n",
      "Epoch 8781: Training Loss: 0.11514368156592052 Validation Loss: 0.7336605787277222\n",
      "Epoch 8782: Training Loss: 0.11538021763165791 Validation Loss: 0.7333229780197144\n",
      "Epoch 8783: Training Loss: 0.11509588112433751 Validation Loss: 0.733557403087616\n",
      "Epoch 8784: Training Loss: 0.11517086376746495 Validation Loss: 0.734154224395752\n",
      "Epoch 8785: Training Loss: 0.11569626380999883 Validation Loss: 0.7342904210090637\n",
      "Epoch 8786: Training Loss: 0.1151958554983139 Validation Loss: 0.7338289022445679\n",
      "Epoch 8787: Training Loss: 0.11589277535676956 Validation Loss: 0.7337163686752319\n",
      "Epoch 8788: Training Loss: 0.11533030867576599 Validation Loss: 0.7335222363471985\n",
      "Epoch 8789: Training Loss: 0.11582403381665547 Validation Loss: 0.7335267663002014\n",
      "Epoch 8790: Training Loss: 0.11528723935286204 Validation Loss: 0.7341432571411133\n",
      "Epoch 8791: Training Loss: 0.11502801130215327 Validation Loss: 0.7342855334281921\n",
      "Epoch 8792: Training Loss: 0.11522184312343597 Validation Loss: 0.7342812418937683\n",
      "Epoch 8793: Training Loss: 0.11548789590597153 Validation Loss: 0.7342243790626526\n",
      "Epoch 8794: Training Loss: 0.11503636091947556 Validation Loss: 0.73430335521698\n",
      "Epoch 8795: Training Loss: 0.11541445304950078 Validation Loss: 0.734041154384613\n",
      "Epoch 8796: Training Loss: 0.11477173368136089 Validation Loss: 0.7337367534637451\n",
      "Epoch 8797: Training Loss: 0.11528952419757843 Validation Loss: 0.733925461769104\n",
      "Epoch 8798: Training Loss: 0.11483439058065414 Validation Loss: 0.7337849140167236\n",
      "Epoch 8799: Training Loss: 0.11498324821392696 Validation Loss: 0.733674943447113\n",
      "Epoch 8800: Training Loss: 0.11510128776232402 Validation Loss: 0.733471155166626\n",
      "Epoch 8801: Training Loss: 0.11500956863164902 Validation Loss: 0.733612596988678\n",
      "Epoch 8802: Training Loss: 0.1151305412252744 Validation Loss: 0.73396897315979\n",
      "Epoch 8803: Training Loss: 0.11493486414353053 Validation Loss: 0.733941376209259\n",
      "Epoch 8804: Training Loss: 0.11486030618349712 Validation Loss: 0.7339790463447571\n",
      "Epoch 8805: Training Loss: 0.11483582605918248 Validation Loss: 0.7340772747993469\n",
      "Epoch 8806: Training Loss: 0.11506987611452739 Validation Loss: 0.7339316606521606\n",
      "Epoch 8807: Training Loss: 0.11509594321250916 Validation Loss: 0.7337019443511963\n",
      "Epoch 8808: Training Loss: 0.11479708552360535 Validation Loss: 0.7337009906768799\n",
      "Epoch 8809: Training Loss: 0.11531288673480351 Validation Loss: 0.7340611815452576\n",
      "Epoch 8810: Training Loss: 0.11477057884136836 Validation Loss: 0.7340425848960876\n",
      "Epoch 8811: Training Loss: 0.11494073520104091 Validation Loss: 0.7339839339256287\n",
      "Epoch 8812: Training Loss: 0.11530592292547226 Validation Loss: 0.7341936826705933\n",
      "Epoch 8813: Training Loss: 0.11489849040905635 Validation Loss: 0.7343146800994873\n",
      "Epoch 8814: Training Loss: 0.11482320974270503 Validation Loss: 0.7341234683990479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8815: Training Loss: 0.11480264614025752 Validation Loss: 0.7343659400939941\n",
      "Epoch 8816: Training Loss: 0.11486828327178955 Validation Loss: 0.7341434359550476\n",
      "Epoch 8817: Training Loss: 0.11484709630409877 Validation Loss: 0.73358553647995\n",
      "Epoch 8818: Training Loss: 0.11586916943391164 Validation Loss: 0.73393315076828\n",
      "Epoch 8819: Training Loss: 0.11458609253168106 Validation Loss: 0.7341870069503784\n",
      "Epoch 8820: Training Loss: 0.11481651415427525 Validation Loss: 0.7342478632926941\n",
      "Epoch 8821: Training Loss: 0.11485253522793452 Validation Loss: 0.7344791293144226\n",
      "Epoch 8822: Training Loss: 0.11481358607610066 Validation Loss: 0.7345729470252991\n",
      "Epoch 8823: Training Loss: 0.11478635917107265 Validation Loss: 0.7343115210533142\n",
      "Epoch 8824: Training Loss: 0.11480216930309932 Validation Loss: 0.7340671420097351\n",
      "Epoch 8825: Training Loss: 0.11481134096781413 Validation Loss: 0.7339476346969604\n",
      "Epoch 8826: Training Loss: 0.11463843037684758 Validation Loss: 0.7340621948242188\n",
      "Epoch 8827: Training Loss: 0.11501406878232956 Validation Loss: 0.734528124332428\n",
      "Epoch 8828: Training Loss: 0.11478475481271744 Validation Loss: 0.7345616221427917\n",
      "Epoch 8829: Training Loss: 0.11424809445937474 Validation Loss: 0.7344023585319519\n",
      "Epoch 8830: Training Loss: 0.11466380208730698 Validation Loss: 0.73457932472229\n",
      "Epoch 8831: Training Loss: 0.11457854757706325 Validation Loss: 0.734169065952301\n",
      "Epoch 8832: Training Loss: 0.11458955705165863 Validation Loss: 0.7341952919960022\n",
      "Epoch 8833: Training Loss: 0.11501339326302211 Validation Loss: 0.7343448996543884\n",
      "Epoch 8834: Training Loss: 0.1145687848329544 Validation Loss: 0.7342634201049805\n",
      "Epoch 8835: Training Loss: 0.11496447523434956 Validation Loss: 0.734470009803772\n",
      "Epoch 8836: Training Loss: 0.11467559883991878 Validation Loss: 0.7347738146781921\n",
      "Epoch 8837: Training Loss: 0.11434190471967061 Validation Loss: 0.7347970008850098\n",
      "Epoch 8838: Training Loss: 0.11436593284209569 Validation Loss: 0.7342545390129089\n",
      "Epoch 8839: Training Loss: 0.11484337598085403 Validation Loss: 0.7338880896568298\n",
      "Epoch 8840: Training Loss: 0.11458684752384822 Validation Loss: 0.7342472076416016\n",
      "Epoch 8841: Training Loss: 0.11427417894204457 Validation Loss: 0.7344642877578735\n",
      "Epoch 8842: Training Loss: 0.1150962511698405 Validation Loss: 0.7346121072769165\n",
      "Epoch 8843: Training Loss: 0.11451127628485362 Validation Loss: 0.7350339293479919\n",
      "Epoch 8844: Training Loss: 0.11442980666955312 Validation Loss: 0.7349103689193726\n",
      "Epoch 8845: Training Loss: 0.11439542720715205 Validation Loss: 0.7348847985267639\n",
      "Epoch 8846: Training Loss: 0.11447861790657043 Validation Loss: 0.7344117164611816\n",
      "Epoch 8847: Training Loss: 0.1153119479616483 Validation Loss: 0.7341185212135315\n",
      "Epoch 8848: Training Loss: 0.11446923017501831 Validation Loss: 0.7338005304336548\n",
      "Epoch 8849: Training Loss: 0.11463815222183864 Validation Loss: 0.7337756752967834\n",
      "Epoch 8850: Training Loss: 0.11452849209308624 Validation Loss: 0.734067440032959\n",
      "Epoch 8851: Training Loss: 0.11438869684934616 Validation Loss: 0.7342961430549622\n",
      "Epoch 8852: Training Loss: 0.11466208100318909 Validation Loss: 0.7344367504119873\n",
      "Epoch 8853: Training Loss: 0.11449662844340007 Validation Loss: 0.73467618227005\n",
      "Epoch 8854: Training Loss: 0.11480343341827393 Validation Loss: 0.7345016002655029\n",
      "Epoch 8855: Training Loss: 0.11458552380402882 Validation Loss: 0.7341343760490417\n",
      "Epoch 8856: Training Loss: 0.11446451644102733 Validation Loss: 0.7340399026870728\n",
      "Epoch 8857: Training Loss: 0.11483456194400787 Validation Loss: 0.7338895797729492\n",
      "Epoch 8858: Training Loss: 0.11422290652990341 Validation Loss: 0.7341428995132446\n",
      "Epoch 8859: Training Loss: 0.11442583054304123 Validation Loss: 0.7346475124359131\n",
      "Epoch 8860: Training Loss: 0.11438990881045659 Validation Loss: 0.735194206237793\n",
      "Epoch 8861: Training Loss: 0.11538741985956828 Validation Loss: 0.7352505326271057\n",
      "Epoch 8862: Training Loss: 0.11428367843230565 Validation Loss: 0.7349550127983093\n",
      "Epoch 8863: Training Loss: 0.11552199721336365 Validation Loss: 0.7347984910011292\n",
      "Epoch 8864: Training Loss: 0.11417943735917409 Validation Loss: 0.7343047261238098\n",
      "Epoch 8865: Training Loss: 0.11473452051480611 Validation Loss: 0.7341586351394653\n",
      "Epoch 8866: Training Loss: 0.11451488733291626 Validation Loss: 0.7341386079788208\n",
      "Epoch 8867: Training Loss: 0.11327083160479863 Validation Loss: 0.7341933846473694\n",
      "Epoch 8868: Training Loss: 0.1145860105752945 Validation Loss: 0.7343202829360962\n",
      "Epoch 8869: Training Loss: 0.11654708037773769 Validation Loss: 0.7341625094413757\n",
      "Epoch 8870: Training Loss: 0.11459072182575862 Validation Loss: 0.7345637679100037\n",
      "Epoch 8871: Training Loss: 0.11538081119457881 Validation Loss: 0.7352603077888489\n",
      "Epoch 8872: Training Loss: 0.1141580839951833 Validation Loss: 0.7350041270256042\n",
      "Epoch 8873: Training Loss: 0.1138864556948344 Validation Loss: 0.735237181186676\n",
      "Epoch 8874: Training Loss: 0.1145896961291631 Validation Loss: 0.7355426549911499\n",
      "Epoch 8875: Training Loss: 0.1142050748070081 Validation Loss: 0.7351766228675842\n",
      "Epoch 8876: Training Loss: 0.11410742998123169 Validation Loss: 0.7348453402519226\n",
      "Epoch 8877: Training Loss: 0.11360820382833481 Validation Loss: 0.7347955107688904\n",
      "Epoch 8878: Training Loss: 0.1142142117023468 Validation Loss: 0.7350873351097107\n",
      "Epoch 8879: Training Loss: 0.11417911946773529 Validation Loss: 0.7348337769508362\n",
      "Epoch 8880: Training Loss: 0.11415903766949971 Validation Loss: 0.73464035987854\n",
      "Epoch 8881: Training Loss: 0.11458671589692433 Validation Loss: 0.7348942160606384\n",
      "Epoch 8882: Training Loss: 0.11419408520062764 Validation Loss: 0.734790563583374\n",
      "Epoch 8883: Training Loss: 0.11431656529506047 Validation Loss: 0.7347375750541687\n",
      "Epoch 8884: Training Loss: 0.11418645332256953 Validation Loss: 0.7347498536109924\n",
      "Epoch 8885: Training Loss: 0.11429381370544434 Validation Loss: 0.7344297170639038\n",
      "Epoch 8886: Training Loss: 0.11477157721916835 Validation Loss: 0.7344717383384705\n",
      "Epoch 8887: Training Loss: 0.11415183544158936 Validation Loss: 0.7344460487365723\n",
      "Epoch 8888: Training Loss: 0.11457482228676479 Validation Loss: 0.7345713973045349\n",
      "Epoch 8889: Training Loss: 0.11356872568527858 Validation Loss: 0.7350648641586304\n",
      "Epoch 8890: Training Loss: 0.11440739780664444 Validation Loss: 0.735331118106842\n",
      "Epoch 8891: Training Loss: 0.1142691398660342 Validation Loss: 0.7353558540344238\n",
      "Epoch 8892: Training Loss: 0.11400628586610158 Validation Loss: 0.7347261309623718\n",
      "Epoch 8893: Training Loss: 0.114134115477403 Validation Loss: 0.7345260977745056\n",
      "Epoch 8894: Training Loss: 0.11406837155421574 Validation Loss: 0.7345429062843323\n",
      "Epoch 8895: Training Loss: 0.11439541230599086 Validation Loss: 0.7345485091209412\n",
      "Epoch 8896: Training Loss: 0.11397633204857509 Validation Loss: 0.7349913716316223\n",
      "Epoch 8897: Training Loss: 0.11476654311021169 Validation Loss: 0.7347720265388489\n",
      "Epoch 8898: Training Loss: 0.11407922953367233 Validation Loss: 0.7349097728729248\n",
      "Epoch 8899: Training Loss: 0.11386390278736751 Validation Loss: 0.7346649765968323\n",
      "Epoch 8900: Training Loss: 0.11450271060069402 Validation Loss: 0.7350039482116699\n",
      "Epoch 8901: Training Loss: 0.11406507591406505 Validation Loss: 0.7354772090911865\n",
      "Epoch 8902: Training Loss: 0.11377142121394475 Validation Loss: 0.735084593296051\n",
      "Epoch 8903: Training Loss: 0.11403451859951019 Validation Loss: 0.7350936532020569\n",
      "Epoch 8904: Training Loss: 0.11443773160378139 Validation Loss: 0.735068678855896\n",
      "Epoch 8905: Training Loss: 0.11419978241125743 Validation Loss: 0.7347709536552429\n",
      "Epoch 8906: Training Loss: 0.11407871792713802 Validation Loss: 0.7350996732711792\n",
      "Epoch 8907: Training Loss: 0.11401740213235219 Validation Loss: 0.7346829175949097\n",
      "Epoch 8908: Training Loss: 0.1135520190000534 Validation Loss: 0.7349572777748108\n",
      "Epoch 8909: Training Loss: 0.11379766712586085 Validation Loss: 0.7352357506752014\n",
      "Epoch 8910: Training Loss: 0.11442538847525914 Validation Loss: 0.7355733513832092\n",
      "Epoch 8911: Training Loss: 0.11387167622645696 Validation Loss: 0.735687255859375\n",
      "Epoch 8912: Training Loss: 0.11385980993509293 Validation Loss: 0.7350133061408997\n",
      "Epoch 8913: Training Loss: 0.1142137348651886 Validation Loss: 0.7347011566162109\n",
      "Epoch 8914: Training Loss: 0.11374527961015701 Validation Loss: 0.7349899411201477\n",
      "Epoch 8915: Training Loss: 0.11375513921181361 Validation Loss: 0.734922468662262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8916: Training Loss: 0.1136329819758733 Validation Loss: 0.7350189089775085\n",
      "Epoch 8917: Training Loss: 0.1135952298839887 Validation Loss: 0.7349423766136169\n",
      "Epoch 8918: Training Loss: 0.11402648935715358 Validation Loss: 0.7351397275924683\n",
      "Epoch 8919: Training Loss: 0.11412475258111954 Validation Loss: 0.7350563406944275\n",
      "Epoch 8920: Training Loss: 0.11377660433451335 Validation Loss: 0.7347416877746582\n",
      "Epoch 8921: Training Loss: 0.114018514752388 Validation Loss: 0.7350122928619385\n",
      "Epoch 8922: Training Loss: 0.11393415927886963 Validation Loss: 0.7354414463043213\n",
      "Epoch 8923: Training Loss: 0.11360467225313187 Validation Loss: 0.7349305152893066\n",
      "Epoch 8924: Training Loss: 0.11352950582901637 Validation Loss: 0.7348385453224182\n",
      "Epoch 8925: Training Loss: 0.11393533150355022 Validation Loss: 0.7346644997596741\n",
      "Epoch 8926: Training Loss: 0.11359074215094249 Validation Loss: 0.7349254488945007\n",
      "Epoch 8927: Training Loss: 0.11350959787766139 Validation Loss: 0.7352790236473083\n",
      "Epoch 8928: Training Loss: 0.1135180691878001 Validation Loss: 0.7357434630393982\n",
      "Epoch 8929: Training Loss: 0.11360264072815578 Validation Loss: 0.7355660200119019\n",
      "Epoch 8930: Training Loss: 0.1140043909351031 Validation Loss: 0.7353535294532776\n",
      "Epoch 8931: Training Loss: 0.11414316793282826 Validation Loss: 0.7351850867271423\n",
      "Epoch 8932: Training Loss: 0.11384653051694234 Validation Loss: 0.7351716160774231\n",
      "Epoch 8933: Training Loss: 0.1137567013502121 Validation Loss: 0.7352909445762634\n",
      "Epoch 8934: Training Loss: 0.11440130323171616 Validation Loss: 0.7352651953697205\n",
      "Epoch 8935: Training Loss: 0.11388903607924779 Validation Loss: 0.7353975772857666\n",
      "Epoch 8936: Training Loss: 0.11380597203969955 Validation Loss: 0.7354981303215027\n",
      "Epoch 8937: Training Loss: 0.11355310926834743 Validation Loss: 0.735176682472229\n",
      "Epoch 8938: Training Loss: 0.11425291001796722 Validation Loss: 0.7352473139762878\n",
      "Epoch 8939: Training Loss: 0.11463081339995067 Validation Loss: 0.7354726195335388\n",
      "Epoch 8940: Training Loss: 0.11341894169648488 Validation Loss: 0.7356135249137878\n",
      "Epoch 8941: Training Loss: 0.1137826715906461 Validation Loss: 0.735704243183136\n",
      "Epoch 8942: Training Loss: 0.11375282456477483 Validation Loss: 0.7357498407363892\n",
      "Epoch 8943: Training Loss: 0.11309316257635753 Validation Loss: 0.7356511950492859\n",
      "Epoch 8944: Training Loss: 0.11395191152890523 Validation Loss: 0.7355660796165466\n",
      "Epoch 8945: Training Loss: 0.11342055350542068 Validation Loss: 0.7354933619499207\n",
      "Epoch 8946: Training Loss: 0.11352404206991196 Validation Loss: 0.7356473207473755\n",
      "Epoch 8947: Training Loss: 0.11353543897469838 Validation Loss: 0.7350908517837524\n",
      "Epoch 8948: Training Loss: 0.11351397633552551 Validation Loss: 0.7350753545761108\n",
      "Epoch 8949: Training Loss: 0.11421217272679011 Validation Loss: 0.7353958487510681\n",
      "Epoch 8950: Training Loss: 0.11405484875043233 Validation Loss: 0.7353312373161316\n",
      "Epoch 8951: Training Loss: 0.11351896325747173 Validation Loss: 0.7357491254806519\n",
      "Epoch 8952: Training Loss: 0.11405259122451146 Validation Loss: 0.7359200716018677\n",
      "Epoch 8953: Training Loss: 0.11338079224030177 Validation Loss: 0.7360767126083374\n",
      "Epoch 8954: Training Loss: 0.1139662116765976 Validation Loss: 0.735684871673584\n",
      "Epoch 8955: Training Loss: 0.11413451532522838 Validation Loss: 0.7354907989501953\n",
      "Epoch 8956: Training Loss: 0.11347765723864238 Validation Loss: 0.7352929711341858\n",
      "Epoch 8957: Training Loss: 0.11356272796789806 Validation Loss: 0.7349904775619507\n",
      "Epoch 8958: Training Loss: 0.11335317542155583 Validation Loss: 0.7351534366607666\n",
      "Epoch 8959: Training Loss: 0.11314916610717773 Validation Loss: 0.7348487377166748\n",
      "Epoch 8960: Training Loss: 0.11337413142124812 Validation Loss: 0.7350298762321472\n",
      "Epoch 8961: Training Loss: 0.11347854385773341 Validation Loss: 0.7352915406227112\n",
      "Epoch 8962: Training Loss: 0.1132981205979983 Validation Loss: 0.7356086373329163\n",
      "Epoch 8963: Training Loss: 0.11356478184461594 Validation Loss: 0.7355443835258484\n",
      "Epoch 8964: Training Loss: 0.11320633937915166 Validation Loss: 0.7356213331222534\n",
      "Epoch 8965: Training Loss: 0.1136947547396024 Validation Loss: 0.7356312870979309\n",
      "Epoch 8966: Training Loss: 0.11325924098491669 Validation Loss: 0.7354461550712585\n",
      "Epoch 8967: Training Loss: 0.11305059244235356 Validation Loss: 0.7354169487953186\n",
      "Epoch 8968: Training Loss: 0.11336457480986913 Validation Loss: 0.7356355786323547\n",
      "Epoch 8969: Training Loss: 0.1134459599852562 Validation Loss: 0.7357161045074463\n",
      "Epoch 8970: Training Loss: 0.11358553419510524 Validation Loss: 0.7352030277252197\n",
      "Epoch 8971: Training Loss: 0.11322425305843353 Validation Loss: 0.7352175712585449\n",
      "Epoch 8972: Training Loss: 0.11333221693833669 Validation Loss: 0.7357244491577148\n",
      "Epoch 8973: Training Loss: 0.1132393628358841 Validation Loss: 0.7356715798377991\n",
      "Epoch 8974: Training Loss: 0.11319179584582646 Validation Loss: 0.7354154586791992\n",
      "Epoch 8975: Training Loss: 0.11316932489474614 Validation Loss: 0.7355053424835205\n",
      "Epoch 8976: Training Loss: 0.11340303470691045 Validation Loss: 0.735554039478302\n",
      "Epoch 8977: Training Loss: 0.11452823380629222 Validation Loss: 0.7356202006340027\n",
      "Epoch 8978: Training Loss: 0.11329348136981328 Validation Loss: 0.7357525825500488\n",
      "Epoch 8979: Training Loss: 0.11321628590424855 Validation Loss: 0.7361270189285278\n",
      "Epoch 8980: Training Loss: 0.11326347539822261 Validation Loss: 0.7361080646514893\n",
      "Epoch 8981: Training Loss: 0.11318049331506093 Validation Loss: 0.7359715104103088\n",
      "Epoch 8982: Training Loss: 0.11357655376195908 Validation Loss: 0.7359821200370789\n",
      "Epoch 8983: Training Loss: 0.11316659549872081 Validation Loss: 0.736240565776825\n",
      "Epoch 8984: Training Loss: 0.11341277013222377 Validation Loss: 0.7356966137886047\n",
      "Epoch 8985: Training Loss: 0.11349601050217946 Validation Loss: 0.7354366779327393\n",
      "Epoch 8986: Training Loss: 0.11340602735678355 Validation Loss: 0.7352117896080017\n",
      "Epoch 8987: Training Loss: 0.1130817507704099 Validation Loss: 0.735373854637146\n",
      "Epoch 8988: Training Loss: 0.1131751760840416 Validation Loss: 0.7360857129096985\n",
      "Epoch 8989: Training Loss: 0.11302788555622101 Validation Loss: 0.7361100912094116\n",
      "Epoch 8990: Training Loss: 0.11311920483907063 Validation Loss: 0.7359924912452698\n",
      "Epoch 8991: Training Loss: 0.11301641662915547 Validation Loss: 0.735533595085144\n",
      "Epoch 8992: Training Loss: 0.11339754362901051 Validation Loss: 0.7355108857154846\n",
      "Epoch 8993: Training Loss: 0.11355320115884145 Validation Loss: 0.7355337142944336\n",
      "Epoch 8994: Training Loss: 0.11295204857985179 Validation Loss: 0.7360649108886719\n",
      "Epoch 8995: Training Loss: 0.11351305743058522 Validation Loss: 0.7362142205238342\n",
      "Epoch 8996: Training Loss: 0.11307102193435033 Validation Loss: 0.7360163331031799\n",
      "Epoch 8997: Training Loss: 0.11316309869289398 Validation Loss: 0.7360945343971252\n",
      "Epoch 8998: Training Loss: 0.11330238233009975 Validation Loss: 0.7362013459205627\n",
      "Epoch 8999: Training Loss: 0.11294658482074738 Validation Loss: 0.73595130443573\n",
      "Epoch 9000: Training Loss: 0.11316347867250443 Validation Loss: 0.7362577319145203\n",
      "Epoch 9001: Training Loss: 0.11338623861471812 Validation Loss: 0.7359020113945007\n",
      "Epoch 9002: Training Loss: 0.11420716096957524 Validation Loss: 0.7358483672142029\n",
      "Epoch 9003: Training Loss: 0.1130691443880399 Validation Loss: 0.735916018486023\n",
      "Epoch 9004: Training Loss: 0.1129704440633456 Validation Loss: 0.7359264492988586\n",
      "Epoch 9005: Training Loss: 0.1145408699909846 Validation Loss: 0.7361001968383789\n",
      "Epoch 9006: Training Loss: 0.11312916874885559 Validation Loss: 0.7360477447509766\n",
      "Epoch 9007: Training Loss: 0.11277948319911957 Validation Loss: 0.7358960509300232\n",
      "Epoch 9008: Training Loss: 0.1133873462677002 Validation Loss: 0.7358784675598145\n",
      "Epoch 9009: Training Loss: 0.11288890490929286 Validation Loss: 0.7355459928512573\n",
      "Epoch 9010: Training Loss: 0.11382001390059789 Validation Loss: 0.7355154752731323\n",
      "Epoch 9011: Training Loss: 0.11344969769318898 Validation Loss: 0.7357196807861328\n",
      "Epoch 9012: Training Loss: 0.11296035101016362 Validation Loss: 0.7360981106758118\n",
      "Epoch 9013: Training Loss: 0.11341148614883423 Validation Loss: 0.7363401055335999\n",
      "Epoch 9014: Training Loss: 0.11308008432388306 Validation Loss: 0.7362614870071411\n",
      "Epoch 9015: Training Loss: 0.11274684717257817 Validation Loss: 0.7360584139823914\n",
      "Epoch 9016: Training Loss: 0.11297932018836339 Validation Loss: 0.7361222505569458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9017: Training Loss: 0.11281606554985046 Validation Loss: 0.7363441586494446\n",
      "Epoch 9018: Training Loss: 0.11318385601043701 Validation Loss: 0.7361348271369934\n",
      "Epoch 9019: Training Loss: 0.11304920663436253 Validation Loss: 0.7359992265701294\n",
      "Epoch 9020: Training Loss: 0.11253754546244939 Validation Loss: 0.7363818883895874\n",
      "Epoch 9021: Training Loss: 0.11222158869107564 Validation Loss: 0.7361497282981873\n",
      "Epoch 9022: Training Loss: 0.1124378964304924 Validation Loss: 0.7360906600952148\n",
      "Epoch 9023: Training Loss: 0.11256655802329381 Validation Loss: 0.7362315654754639\n",
      "Epoch 9024: Training Loss: 0.1127118468284607 Validation Loss: 0.7361029982566833\n",
      "Epoch 9025: Training Loss: 0.11252352595329285 Validation Loss: 0.7360965609550476\n",
      "Epoch 9026: Training Loss: 0.11261582622925441 Validation Loss: 0.7360268235206604\n",
      "Epoch 9027: Training Loss: 0.11278617878754933 Validation Loss: 0.7358919978141785\n",
      "Epoch 9028: Training Loss: 0.11252840360005696 Validation Loss: 0.7360219955444336\n",
      "Epoch 9029: Training Loss: 0.11261916657288869 Validation Loss: 0.7365366816520691\n",
      "Epoch 9030: Training Loss: 0.11230489859978358 Validation Loss: 0.7364815473556519\n",
      "Epoch 9031: Training Loss: 0.11243001371622086 Validation Loss: 0.7362033128738403\n",
      "Epoch 9032: Training Loss: 0.11268019179503123 Validation Loss: 0.7361060380935669\n",
      "Epoch 9033: Training Loss: 0.11248883853356044 Validation Loss: 0.7365617156028748\n",
      "Epoch 9034: Training Loss: 0.11224293212095897 Validation Loss: 0.7365434765815735\n",
      "Epoch 9035: Training Loss: 0.11262615025043488 Validation Loss: 0.736444354057312\n",
      "Epoch 9036: Training Loss: 0.11300608267386754 Validation Loss: 0.736444890499115\n",
      "Epoch 9037: Training Loss: 0.11254274596770604 Validation Loss: 0.7364043593406677\n",
      "Epoch 9038: Training Loss: 0.11374729871749878 Validation Loss: 0.7364446520805359\n",
      "Epoch 9039: Training Loss: 0.11281150082747142 Validation Loss: 0.7358463406562805\n",
      "Epoch 9040: Training Loss: 0.11253805955251057 Validation Loss: 0.735886812210083\n",
      "Epoch 9041: Training Loss: 0.11256573845942815 Validation Loss: 0.7363008260726929\n",
      "Epoch 9042: Training Loss: 0.1128196120262146 Validation Loss: 0.7358942627906799\n",
      "Epoch 9043: Training Loss: 0.11367516716321309 Validation Loss: 0.735771894454956\n",
      "Epoch 9044: Training Loss: 0.11231554299592972 Validation Loss: 0.7361948490142822\n",
      "Epoch 9045: Training Loss: 0.11266696204741795 Validation Loss: 0.7364150881767273\n",
      "Epoch 9046: Training Loss: 0.11264644314845403 Validation Loss: 0.7363451719284058\n",
      "Epoch 9047: Training Loss: 0.11248805125554402 Validation Loss: 0.7360730767250061\n",
      "Epoch 9048: Training Loss: 0.11253169924020767 Validation Loss: 0.7357420325279236\n",
      "Epoch 9049: Training Loss: 0.11251404384771983 Validation Loss: 0.7357270121574402\n",
      "Epoch 9050: Training Loss: 0.11262764781713486 Validation Loss: 0.7357820868492126\n",
      "Epoch 9051: Training Loss: 0.11217443645000458 Validation Loss: 0.7362310290336609\n",
      "Epoch 9052: Training Loss: 0.11288292706012726 Validation Loss: 0.7366408705711365\n",
      "Epoch 9053: Training Loss: 0.11259003976980846 Validation Loss: 0.7372603416442871\n",
      "Epoch 9054: Training Loss: 0.11260519673426946 Validation Loss: 0.7368866801261902\n",
      "Epoch 9055: Training Loss: 0.1123917152484258 Validation Loss: 0.7363185882568359\n",
      "Epoch 9056: Training Loss: 0.11249253650506337 Validation Loss: 0.7361745834350586\n",
      "Epoch 9057: Training Loss: 0.11273484180370967 Validation Loss: 0.7360208034515381\n",
      "Epoch 9058: Training Loss: 0.11238217602173488 Validation Loss: 0.7364542484283447\n",
      "Epoch 9059: Training Loss: 0.1128593161702156 Validation Loss: 0.7368813157081604\n",
      "Epoch 9060: Training Loss: 0.11290629208087921 Validation Loss: 0.7364274859428406\n",
      "Epoch 9061: Training Loss: 0.11232331643501918 Validation Loss: 0.7366594076156616\n",
      "Epoch 9062: Training Loss: 0.11233422656853993 Validation Loss: 0.7366384267807007\n",
      "Epoch 9063: Training Loss: 0.11241059750318527 Validation Loss: 0.7368555068969727\n",
      "Epoch 9064: Training Loss: 0.11259134362141292 Validation Loss: 0.736209511756897\n",
      "Epoch 9065: Training Loss: 0.11245088279247284 Validation Loss: 0.7363494038581848\n",
      "Epoch 9066: Training Loss: 0.11229635278383891 Validation Loss: 0.7364276647567749\n",
      "Epoch 9067: Training Loss: 0.11381360640128453 Validation Loss: 0.7364888787269592\n",
      "Epoch 9068: Training Loss: 0.11274906744559605 Validation Loss: 0.7363980412483215\n",
      "Epoch 9069: Training Loss: 0.11236814657847087 Validation Loss: 0.7366049289703369\n",
      "Epoch 9070: Training Loss: 0.11226087808609009 Validation Loss: 0.7365789413452148\n",
      "Epoch 9071: Training Loss: 0.11218187709649403 Validation Loss: 0.7365871071815491\n",
      "Epoch 9072: Training Loss: 0.11253583679596584 Validation Loss: 0.7361892461776733\n",
      "Epoch 9073: Training Loss: 0.11221503466367722 Validation Loss: 0.735674262046814\n",
      "Epoch 9074: Training Loss: 0.11275394757588704 Validation Loss: 0.7359617948532104\n",
      "Epoch 9075: Training Loss: 0.1123643343647321 Validation Loss: 0.7362114787101746\n",
      "Epoch 9076: Training Loss: 0.11295199394226074 Validation Loss: 0.7368044257164001\n",
      "Epoch 9077: Training Loss: 0.1123523935675621 Validation Loss: 0.7368506193161011\n",
      "Epoch 9078: Training Loss: 0.11240459730227788 Validation Loss: 0.737007737159729\n",
      "Epoch 9079: Training Loss: 0.112749844789505 Validation Loss: 0.7371084690093994\n",
      "Epoch 9080: Training Loss: 0.11179334918657939 Validation Loss: 0.7371909022331238\n",
      "Epoch 9081: Training Loss: 0.11219022174676259 Validation Loss: 0.7371946573257446\n",
      "Epoch 9082: Training Loss: 0.11243602881828944 Validation Loss: 0.7372257709503174\n",
      "Epoch 9083: Training Loss: 0.11215577026208241 Validation Loss: 0.7367404699325562\n",
      "Epoch 9084: Training Loss: 0.11212540417909622 Validation Loss: 0.7366998791694641\n",
      "Epoch 9085: Training Loss: 0.11229727665583293 Validation Loss: 0.7368114590644836\n",
      "Epoch 9086: Training Loss: 0.11254300425450008 Validation Loss: 0.7371237277984619\n",
      "Epoch 9087: Training Loss: 0.11236940324306488 Validation Loss: 0.7370966076850891\n",
      "Epoch 9088: Training Loss: 0.11189545939366023 Validation Loss: 0.7368119359016418\n",
      "Epoch 9089: Training Loss: 0.11251752078533173 Validation Loss: 0.7368913292884827\n",
      "Epoch 9090: Training Loss: 0.11200727274020512 Validation Loss: 0.7366123795509338\n",
      "Epoch 9091: Training Loss: 0.11182555307944615 Validation Loss: 0.7365309000015259\n",
      "Epoch 9092: Training Loss: 0.11232906083265941 Validation Loss: 0.7367332577705383\n",
      "Epoch 9093: Training Loss: 0.11225784818331401 Validation Loss: 0.7366533279418945\n",
      "Epoch 9094: Training Loss: 0.11225066830714543 Validation Loss: 0.7364012002944946\n",
      "Epoch 9095: Training Loss: 0.11184510340293248 Validation Loss: 0.7366257905960083\n",
      "Epoch 9096: Training Loss: 0.11213667194048564 Validation Loss: 0.7363860607147217\n",
      "Epoch 9097: Training Loss: 0.11237625529368718 Validation Loss: 0.7365737557411194\n",
      "Epoch 9098: Training Loss: 0.11224707712729771 Validation Loss: 0.7366279363632202\n",
      "Epoch 9099: Training Loss: 0.11218273391326268 Validation Loss: 0.7367123961448669\n",
      "Epoch 9100: Training Loss: 0.11180102328459422 Validation Loss: 0.7367721796035767\n",
      "Epoch 9101: Training Loss: 0.11198774228493373 Validation Loss: 0.7369051575660706\n",
      "Epoch 9102: Training Loss: 0.11193013687928517 Validation Loss: 0.7366268634796143\n",
      "Epoch 9103: Training Loss: 0.11191106836001079 Validation Loss: 0.7362460494041443\n",
      "Epoch 9104: Training Loss: 0.11208359152078629 Validation Loss: 0.7366988062858582\n",
      "Epoch 9105: Training Loss: 0.11289690434932709 Validation Loss: 0.737031877040863\n",
      "Epoch 9106: Training Loss: 0.11198397477467854 Validation Loss: 0.7372945547103882\n",
      "Epoch 9107: Training Loss: 0.111928624411424 Validation Loss: 0.7370822429656982\n",
      "Epoch 9108: Training Loss: 0.11180052906274796 Validation Loss: 0.7372307181358337\n",
      "Epoch 9109: Training Loss: 0.1120714470744133 Validation Loss: 0.7367319464683533\n",
      "Epoch 9110: Training Loss: 0.1123120089371999 Validation Loss: 0.7366001009941101\n",
      "Epoch 9111: Training Loss: 0.11187957972288132 Validation Loss: 0.736722469329834\n",
      "Epoch 9112: Training Loss: 0.11271943897008896 Validation Loss: 0.7364205718040466\n",
      "Epoch 9113: Training Loss: 0.11253143846988678 Validation Loss: 0.7365044951438904\n",
      "Epoch 9114: Training Loss: 0.11203040679295857 Validation Loss: 0.736789882183075\n",
      "Epoch 9115: Training Loss: 0.1119008536140124 Validation Loss: 0.7372620701789856\n",
      "Epoch 9116: Training Loss: 0.1121860072016716 Validation Loss: 0.7375409007072449\n",
      "Epoch 9117: Training Loss: 0.11198675880829494 Validation Loss: 0.7376459240913391\n",
      "Epoch 9118: Training Loss: 0.112034576634566 Validation Loss: 0.7374719977378845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9119: Training Loss: 0.11223483333985011 Validation Loss: 0.7370740175247192\n",
      "Epoch 9120: Training Loss: 0.11190182467301686 Validation Loss: 0.7370379567146301\n",
      "Epoch 9121: Training Loss: 0.11182530969381332 Validation Loss: 0.737537682056427\n",
      "Epoch 9122: Training Loss: 0.11218137790759404 Validation Loss: 0.7373688220977783\n",
      "Epoch 9123: Training Loss: 0.11157005776961644 Validation Loss: 0.7373177409172058\n",
      "Epoch 9124: Training Loss: 0.11219579229752223 Validation Loss: 0.7371183633804321\n",
      "Epoch 9125: Training Loss: 0.11210994174083073 Validation Loss: 0.7371622920036316\n",
      "Epoch 9126: Training Loss: 0.11207190901041031 Validation Loss: 0.7368257641792297\n",
      "Epoch 9127: Training Loss: 0.11179083834091823 Validation Loss: 0.737004816532135\n",
      "Epoch 9128: Training Loss: 0.11177911361058553 Validation Loss: 0.737497091293335\n",
      "Epoch 9129: Training Loss: 0.11247589687506358 Validation Loss: 0.7376766204833984\n",
      "Epoch 9130: Training Loss: 0.11201258252064387 Validation Loss: 0.7377212643623352\n",
      "Epoch 9131: Training Loss: 0.11242284129063289 Validation Loss: 0.7372614145278931\n",
      "Epoch 9132: Training Loss: 0.11189774920543034 Validation Loss: 0.7374668717384338\n",
      "Epoch 9133: Training Loss: 0.11241055776675542 Validation Loss: 0.7367900609970093\n",
      "Epoch 9134: Training Loss: 0.1118929535150528 Validation Loss: 0.7370551228523254\n",
      "Epoch 9135: Training Loss: 0.11189146836598714 Validation Loss: 0.7371668219566345\n",
      "Epoch 9136: Training Loss: 0.11274927109479904 Validation Loss: 0.7373639345169067\n",
      "Epoch 9137: Training Loss: 0.11163819581270218 Validation Loss: 0.7372522950172424\n",
      "Epoch 9138: Training Loss: 0.11163744578758876 Validation Loss: 0.7373821139335632\n",
      "Epoch 9139: Training Loss: 0.11173947900533676 Validation Loss: 0.7374518513679504\n",
      "Epoch 9140: Training Loss: 0.11173940449953079 Validation Loss: 0.7373079657554626\n",
      "Epoch 9141: Training Loss: 0.11155284941196442 Validation Loss: 0.7374740839004517\n",
      "Epoch 9142: Training Loss: 0.11127302795648575 Validation Loss: 0.7375016212463379\n",
      "Epoch 9143: Training Loss: 0.111727274954319 Validation Loss: 0.7374997138977051\n",
      "Epoch 9144: Training Loss: 0.11166983594497044 Validation Loss: 0.7374163866043091\n",
      "Epoch 9145: Training Loss: 0.11159250885248184 Validation Loss: 0.7373185157775879\n",
      "Epoch 9146: Training Loss: 0.11191085974375407 Validation Loss: 0.7373389601707458\n",
      "Epoch 9147: Training Loss: 0.11162632455428441 Validation Loss: 0.7372376322746277\n",
      "Epoch 9148: Training Loss: 0.1115921934445699 Validation Loss: 0.7371847629547119\n",
      "Epoch 9149: Training Loss: 0.11170951773722966 Validation Loss: 0.7375990748405457\n",
      "Epoch 9150: Training Loss: 0.11155963440736134 Validation Loss: 0.7372934222221375\n",
      "Epoch 9151: Training Loss: 0.1114993045727412 Validation Loss: 0.7373515963554382\n",
      "Epoch 9152: Training Loss: 0.11162565896908443 Validation Loss: 0.7375956773757935\n",
      "Epoch 9153: Training Loss: 0.11158840358257294 Validation Loss: 0.7376493811607361\n",
      "Epoch 9154: Training Loss: 0.11210486541191737 Validation Loss: 0.7376184463500977\n",
      "Epoch 9155: Training Loss: 0.1117938831448555 Validation Loss: 0.7377278804779053\n",
      "Epoch 9156: Training Loss: 0.11138891925414403 Validation Loss: 0.7374440431594849\n",
      "Epoch 9157: Training Loss: 0.11164342612028122 Validation Loss: 0.7370923757553101\n",
      "Epoch 9158: Training Loss: 0.11216031511624654 Validation Loss: 0.7373169660568237\n",
      "Epoch 9159: Training Loss: 0.11143981417020161 Validation Loss: 0.7378207445144653\n",
      "Epoch 9160: Training Loss: 0.11222530404726665 Validation Loss: 0.7384292483329773\n",
      "Epoch 9161: Training Loss: 0.1115372081597646 Validation Loss: 0.7377352118492126\n",
      "Epoch 9162: Training Loss: 0.11231090128421783 Validation Loss: 0.7372342348098755\n",
      "Epoch 9163: Training Loss: 0.11121720572312672 Validation Loss: 0.7373422384262085\n",
      "Epoch 9164: Training Loss: 0.11315410335858662 Validation Loss: 0.7376343607902527\n",
      "Epoch 9165: Training Loss: 0.11153017232815425 Validation Loss: 0.7380135655403137\n",
      "Epoch 9166: Training Loss: 0.11138945321242015 Validation Loss: 0.7377510666847229\n",
      "Epoch 9167: Training Loss: 0.11193682998418808 Validation Loss: 0.7379453778266907\n",
      "Epoch 9168: Training Loss: 0.11226453383763631 Validation Loss: 0.7376764416694641\n",
      "Epoch 9169: Training Loss: 0.11150763432184856 Validation Loss: 0.737428605556488\n",
      "Epoch 9170: Training Loss: 0.1121144990126292 Validation Loss: 0.737532913684845\n",
      "Epoch 9171: Training Loss: 0.11146676788727443 Validation Loss: 0.7376500964164734\n",
      "Epoch 9172: Training Loss: 0.1115804339448611 Validation Loss: 0.737302839756012\n",
      "Epoch 9173: Training Loss: 0.11166240771611531 Validation Loss: 0.7373768091201782\n",
      "Epoch 9174: Training Loss: 0.11222829421361287 Validation Loss: 0.7377748489379883\n",
      "Epoch 9175: Training Loss: 0.11145947873592377 Validation Loss: 0.7378615736961365\n",
      "Epoch 9176: Training Loss: 0.1111582765976588 Validation Loss: 0.7379837036132812\n",
      "Epoch 9177: Training Loss: 0.11122270673513412 Validation Loss: 0.73794025182724\n",
      "Epoch 9178: Training Loss: 0.11140803744395573 Validation Loss: 0.737861156463623\n",
      "Epoch 9179: Training Loss: 0.1113327294588089 Validation Loss: 0.7370174527168274\n",
      "Epoch 9180: Training Loss: 0.11149665216604869 Validation Loss: 0.737467348575592\n",
      "Epoch 9181: Training Loss: 0.1121948833266894 Validation Loss: 0.7374978065490723\n",
      "Epoch 9182: Training Loss: 0.1116970032453537 Validation Loss: 0.7376500964164734\n",
      "Epoch 9183: Training Loss: 0.11102476219336192 Validation Loss: 0.7378460764884949\n",
      "Epoch 9184: Training Loss: 0.11124512056509654 Validation Loss: 0.7381114959716797\n",
      "Epoch 9185: Training Loss: 0.11161330342292786 Validation Loss: 0.7380892634391785\n",
      "Epoch 9186: Training Loss: 0.11131609479586284 Validation Loss: 0.7380827069282532\n",
      "Epoch 9187: Training Loss: 0.11148367325464885 Validation Loss: 0.7379326820373535\n",
      "Epoch 9188: Training Loss: 0.11112543692191441 Validation Loss: 0.7378864288330078\n",
      "Epoch 9189: Training Loss: 0.11131706337134044 Validation Loss: 0.7376079559326172\n",
      "Epoch 9190: Training Loss: 0.11161657671133678 Validation Loss: 0.7375107407569885\n",
      "Epoch 9191: Training Loss: 0.11141800632079442 Validation Loss: 0.7378482818603516\n",
      "Epoch 9192: Training Loss: 0.11146808167298634 Validation Loss: 0.7379150390625\n",
      "Epoch 9193: Training Loss: 0.11120087901751201 Validation Loss: 0.7379953265190125\n",
      "Epoch 9194: Training Loss: 0.11108344793319702 Validation Loss: 0.7380033135414124\n",
      "Epoch 9195: Training Loss: 0.11139970272779465 Validation Loss: 0.7379871606826782\n",
      "Epoch 9196: Training Loss: 0.11167524755001068 Validation Loss: 0.7380048632621765\n",
      "Epoch 9197: Training Loss: 0.11130339403947194 Validation Loss: 0.7375723719596863\n",
      "Epoch 9198: Training Loss: 0.1111712356408437 Validation Loss: 0.7375182509422302\n",
      "Epoch 9199: Training Loss: 0.11129187792539597 Validation Loss: 0.7379106283187866\n",
      "Epoch 9200: Training Loss: 0.11126367499430974 Validation Loss: 0.7379574775695801\n",
      "Epoch 9201: Training Loss: 0.11112686494986217 Validation Loss: 0.737656831741333\n",
      "Epoch 9202: Training Loss: 0.11126631498336792 Validation Loss: 0.7378367781639099\n",
      "Epoch 9203: Training Loss: 0.11140818397204082 Validation Loss: 0.7381832599639893\n",
      "Epoch 9204: Training Loss: 0.11201173812150955 Validation Loss: 0.7379027605056763\n",
      "Epoch 9205: Training Loss: 0.11052024861176808 Validation Loss: 0.7380063533782959\n",
      "Epoch 9206: Training Loss: 0.11130730559428532 Validation Loss: 0.7381293177604675\n",
      "Epoch 9207: Training Loss: 0.11108605811993282 Validation Loss: 0.7378947138786316\n",
      "Epoch 9208: Training Loss: 0.11098483701546986 Validation Loss: 0.7377331256866455\n",
      "Epoch 9209: Training Loss: 0.1108790139357249 Validation Loss: 0.737609326839447\n",
      "Epoch 9210: Training Loss: 0.11082849899927776 Validation Loss: 0.737729549407959\n",
      "Epoch 9211: Training Loss: 0.11135931809743245 Validation Loss: 0.7378959655761719\n",
      "Epoch 9212: Training Loss: 0.1119088629881541 Validation Loss: 0.7378689050674438\n",
      "Epoch 9213: Training Loss: 0.11139067510763805 Validation Loss: 0.7379535436630249\n",
      "Epoch 9214: Training Loss: 0.11111196378866832 Validation Loss: 0.7378924489021301\n",
      "Epoch 9215: Training Loss: 0.11098825683196385 Validation Loss: 0.7382818460464478\n",
      "Epoch 9216: Training Loss: 0.11121198534965515 Validation Loss: 0.7382813096046448\n",
      "Epoch 9217: Training Loss: 0.11154941966136296 Validation Loss: 0.7380842566490173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9218: Training Loss: 0.11116542915503184 Validation Loss: 0.7378676533699036\n",
      "Epoch 9219: Training Loss: 0.11103324343760808 Validation Loss: 0.7375841736793518\n",
      "Epoch 9220: Training Loss: 0.1117278461654981 Validation Loss: 0.738070011138916\n",
      "Epoch 9221: Training Loss: 0.11131056894858678 Validation Loss: 0.7381908297538757\n",
      "Epoch 9222: Training Loss: 0.11083518465360005 Validation Loss: 0.7383732199668884\n",
      "Epoch 9223: Training Loss: 0.11107408752044041 Validation Loss: 0.7382097244262695\n",
      "Epoch 9224: Training Loss: 0.11094786475102107 Validation Loss: 0.7380901575088501\n",
      "Epoch 9225: Training Loss: 0.11077834169069926 Validation Loss: 0.73806232213974\n",
      "Epoch 9226: Training Loss: 0.11089173952738444 Validation Loss: 0.7378682494163513\n",
      "Epoch 9227: Training Loss: 0.11153412858645122 Validation Loss: 0.7376102805137634\n",
      "Epoch 9228: Training Loss: 0.11095645030339558 Validation Loss: 0.7378477454185486\n",
      "Epoch 9229: Training Loss: 0.11082635819911957 Validation Loss: 0.7387164831161499\n",
      "Epoch 9230: Training Loss: 0.11074405908584595 Validation Loss: 0.7387374639511108\n",
      "Epoch 9231: Training Loss: 0.11087606350580852 Validation Loss: 0.7387715578079224\n",
      "Epoch 9232: Training Loss: 0.11063554386297862 Validation Loss: 0.7387908101081848\n",
      "Epoch 9233: Training Loss: 0.11079331487417221 Validation Loss: 0.7383329272270203\n",
      "Epoch 9234: Training Loss: 0.1113599066932996 Validation Loss: 0.7379639744758606\n",
      "Epoch 9235: Training Loss: 0.11048312236865361 Validation Loss: 0.7376471161842346\n",
      "Epoch 9236: Training Loss: 0.11080201466878255 Validation Loss: 0.7377215027809143\n",
      "Epoch 9237: Training Loss: 0.1106741229693095 Validation Loss: 0.7376310229301453\n",
      "Epoch 9238: Training Loss: 0.11081334203481674 Validation Loss: 0.7379884123802185\n",
      "Epoch 9239: Training Loss: 0.11079724629720052 Validation Loss: 0.7382887601852417\n",
      "Epoch 9240: Training Loss: 0.11108979831139247 Validation Loss: 0.7389295101165771\n",
      "Epoch 9241: Training Loss: 0.11076152573029201 Validation Loss: 0.7390745878219604\n",
      "Epoch 9242: Training Loss: 0.11056558042764664 Validation Loss: 0.7388873100280762\n",
      "Epoch 9243: Training Loss: 0.11139742036660512 Validation Loss: 0.7387888431549072\n",
      "Epoch 9244: Training Loss: 0.11075432350238164 Validation Loss: 0.7381898760795593\n",
      "Epoch 9245: Training Loss: 0.11072777956724167 Validation Loss: 0.7381399273872375\n",
      "Epoch 9246: Training Loss: 0.11063441882530849 Validation Loss: 0.737886905670166\n",
      "Epoch 9247: Training Loss: 0.11043650656938553 Validation Loss: 0.7381353378295898\n",
      "Epoch 9248: Training Loss: 0.11076226582129796 Validation Loss: 0.738318681716919\n",
      "Epoch 9249: Training Loss: 0.11025180419286092 Validation Loss: 0.7385485172271729\n",
      "Epoch 9250: Training Loss: 0.11065028111139934 Validation Loss: 0.7383192777633667\n",
      "Epoch 9251: Training Loss: 0.11138773957888286 Validation Loss: 0.7381221055984497\n",
      "Epoch 9252: Training Loss: 0.11076952268679936 Validation Loss: 0.7381495833396912\n",
      "Epoch 9253: Training Loss: 0.11112094422181447 Validation Loss: 0.7386184930801392\n",
      "Epoch 9254: Training Loss: 0.11053235332171123 Validation Loss: 0.7385461330413818\n",
      "Epoch 9255: Training Loss: 0.11063137153784434 Validation Loss: 0.7389302253723145\n",
      "Epoch 9256: Training Loss: 0.11056807388861974 Validation Loss: 0.7388482093811035\n",
      "Epoch 9257: Training Loss: 0.11251512418190639 Validation Loss: 0.7386813163757324\n",
      "Epoch 9258: Training Loss: 0.11121929188569386 Validation Loss: 0.7382004857063293\n",
      "Epoch 9259: Training Loss: 0.11057283729314804 Validation Loss: 0.7377379536628723\n",
      "Epoch 9260: Training Loss: 0.11076398938894272 Validation Loss: 0.7382599115371704\n",
      "Epoch 9261: Training Loss: 0.11122323324282964 Validation Loss: 0.7389909625053406\n",
      "Epoch 9262: Training Loss: 0.1105452428261439 Validation Loss: 0.7387572526931763\n",
      "Epoch 9263: Training Loss: 0.11023102949062984 Validation Loss: 0.7385406494140625\n",
      "Epoch 9264: Training Loss: 0.11041725178559621 Validation Loss: 0.7385803461074829\n",
      "Epoch 9265: Training Loss: 0.11053428798913956 Validation Loss: 0.7386870384216309\n",
      "Epoch 9266: Training Loss: 0.11073607951402664 Validation Loss: 0.7384569048881531\n",
      "Epoch 9267: Training Loss: 0.1101684421300888 Validation Loss: 0.7381078004837036\n",
      "Epoch 9268: Training Loss: 0.1112244501709938 Validation Loss: 0.7380997538566589\n",
      "Epoch 9269: Training Loss: 0.11063465227683385 Validation Loss: 0.7380638718605042\n",
      "Epoch 9270: Training Loss: 0.11058644702037175 Validation Loss: 0.738396406173706\n",
      "Epoch 9271: Training Loss: 0.1102428212761879 Validation Loss: 0.7387988567352295\n",
      "Epoch 9272: Training Loss: 0.11078186084826787 Validation Loss: 0.7394732236862183\n",
      "Epoch 9273: Training Loss: 0.1110052764415741 Validation Loss: 0.7393978834152222\n",
      "Epoch 9274: Training Loss: 0.11007446050643921 Validation Loss: 0.7391257286071777\n",
      "Epoch 9275: Training Loss: 0.11134153107802074 Validation Loss: 0.7390360832214355\n",
      "Epoch 9276: Training Loss: 0.11038611829280853 Validation Loss: 0.7389363646507263\n",
      "Epoch 9277: Training Loss: 0.11010037114222844 Validation Loss: 0.7384576201438904\n",
      "Epoch 9278: Training Loss: 0.1100966955224673 Validation Loss: 0.7386056184768677\n",
      "Epoch 9279: Training Loss: 0.1110607956846555 Validation Loss: 0.7386996746063232\n",
      "Epoch 9280: Training Loss: 0.11064373205105464 Validation Loss: 0.7391424775123596\n",
      "Epoch 9281: Training Loss: 0.11051399012406667 Validation Loss: 0.7395102381706238\n",
      "Epoch 9282: Training Loss: 0.11099681009848912 Validation Loss: 0.739311695098877\n",
      "Epoch 9283: Training Loss: 0.11078708122173946 Validation Loss: 0.7388959527015686\n",
      "Epoch 9284: Training Loss: 0.110159436861674 Validation Loss: 0.7389009594917297\n",
      "Epoch 9285: Training Loss: 0.1106109047929446 Validation Loss: 0.7386643290519714\n",
      "Epoch 9286: Training Loss: 0.11063032100598018 Validation Loss: 0.7387759685516357\n",
      "Epoch 9287: Training Loss: 0.11068538576364517 Validation Loss: 0.7390550374984741\n",
      "Epoch 9288: Training Loss: 0.1104385107755661 Validation Loss: 0.7394951581954956\n",
      "Epoch 9289: Training Loss: 0.1102731501062711 Validation Loss: 0.7391771674156189\n",
      "Epoch 9290: Training Loss: 0.11056399097045262 Validation Loss: 0.7386862635612488\n",
      "Epoch 9291: Training Loss: 0.11027639855941136 Validation Loss: 0.73881995677948\n",
      "Epoch 9292: Training Loss: 0.11035642524560292 Validation Loss: 0.738963782787323\n",
      "Epoch 9293: Training Loss: 0.10973360886176427 Validation Loss: 0.7390020489692688\n",
      "Epoch 9294: Training Loss: 0.11006859689950943 Validation Loss: 0.7394646406173706\n",
      "Epoch 9295: Training Loss: 0.11054719984531403 Validation Loss: 0.7394118309020996\n",
      "Epoch 9296: Training Loss: 0.11017232884963353 Validation Loss: 0.7394382953643799\n",
      "Epoch 9297: Training Loss: 0.11018433421850204 Validation Loss: 0.7389088869094849\n",
      "Epoch 9298: Training Loss: 0.11097760746876399 Validation Loss: 0.7390446662902832\n",
      "Epoch 9299: Training Loss: 0.1106148511171341 Validation Loss: 0.739259660243988\n",
      "Epoch 9300: Training Loss: 0.11160121113061905 Validation Loss: 0.739107072353363\n",
      "Epoch 9301: Training Loss: 0.10982232292493184 Validation Loss: 0.7391156554222107\n",
      "Epoch 9302: Training Loss: 0.11066935956478119 Validation Loss: 0.7390374541282654\n",
      "Epoch 9303: Training Loss: 0.11032785723606746 Validation Loss: 0.7389475107192993\n",
      "Epoch 9304: Training Loss: 0.11081222196420033 Validation Loss: 0.7383043169975281\n",
      "Epoch 9305: Training Loss: 0.1099835882584254 Validation Loss: 0.7386124730110168\n",
      "Epoch 9306: Training Loss: 0.11012065907319386 Validation Loss: 0.7389273643493652\n",
      "Epoch 9307: Training Loss: 0.11017433553934097 Validation Loss: 0.7388487458229065\n",
      "Epoch 9308: Training Loss: 0.10999429474274318 Validation Loss: 0.7393871545791626\n",
      "Epoch 9309: Training Loss: 0.11031957467397054 Validation Loss: 0.739797830581665\n",
      "Epoch 9310: Training Loss: 0.11051537344853084 Validation Loss: 0.7392997741699219\n",
      "Epoch 9311: Training Loss: 0.11004188656806946 Validation Loss: 0.7394861578941345\n",
      "Epoch 9312: Training Loss: 0.11022691428661346 Validation Loss: 0.7396657466888428\n",
      "Epoch 9313: Training Loss: 0.11050661404927571 Validation Loss: 0.7388728260993958\n",
      "Epoch 9314: Training Loss: 0.10994540403286616 Validation Loss: 0.7391763925552368\n",
      "Epoch 9315: Training Loss: 0.11001620193322499 Validation Loss: 0.7389599084854126\n",
      "Epoch 9316: Training Loss: 0.1100461483001709 Validation Loss: 0.7390707731246948\n",
      "Epoch 9317: Training Loss: 0.11054233213265736 Validation Loss: 0.7388394474983215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9318: Training Loss: 0.11019628246625264 Validation Loss: 0.7389857172966003\n",
      "Epoch 9319: Training Loss: 0.110007477303346 Validation Loss: 0.7394769191741943\n",
      "Epoch 9320: Training Loss: 0.110321044921875 Validation Loss: 0.7395363450050354\n",
      "Epoch 9321: Training Loss: 0.11044666916131973 Validation Loss: 0.7393197417259216\n",
      "Epoch 9322: Training Loss: 0.1097471018632253 Validation Loss: 0.7389187216758728\n",
      "Epoch 9323: Training Loss: 0.10989358524481456 Validation Loss: 0.7388455867767334\n",
      "Epoch 9324: Training Loss: 0.11005960156520207 Validation Loss: 0.7387361526489258\n",
      "Epoch 9325: Training Loss: 0.10998999079068501 Validation Loss: 0.7390018105506897\n",
      "Epoch 9326: Training Loss: 0.10998986661434174 Validation Loss: 0.7394397854804993\n",
      "Epoch 9327: Training Loss: 0.1104548250635465 Validation Loss: 0.7397536039352417\n",
      "Epoch 9328: Training Loss: 0.11074652274449666 Validation Loss: 0.7397410869598389\n",
      "Epoch 9329: Training Loss: 0.10979953904946645 Validation Loss: 0.7397279739379883\n",
      "Epoch 9330: Training Loss: 0.10994100570678711 Validation Loss: 0.7392815947532654\n",
      "Epoch 9331: Training Loss: 0.11051554481188457 Validation Loss: 0.7391096949577332\n",
      "Epoch 9332: Training Loss: 0.10995674381653468 Validation Loss: 0.7389410138130188\n",
      "Epoch 9333: Training Loss: 0.11010211457808812 Validation Loss: 0.7388470768928528\n",
      "Epoch 9334: Training Loss: 0.10975931336482365 Validation Loss: 0.7389434576034546\n",
      "Epoch 9335: Training Loss: 0.11030590782562892 Validation Loss: 0.738868772983551\n",
      "Epoch 9336: Training Loss: 0.11062568426132202 Validation Loss: 0.7395683526992798\n",
      "Epoch 9337: Training Loss: 0.1099823812643687 Validation Loss: 0.7394525408744812\n",
      "Epoch 9338: Training Loss: 0.10953424870967865 Validation Loss: 0.7392931580543518\n",
      "Epoch 9339: Training Loss: 0.11061142633358638 Validation Loss: 0.7392685413360596\n",
      "Epoch 9340: Training Loss: 0.10985907912254333 Validation Loss: 0.7392539978027344\n",
      "Epoch 9341: Training Loss: 0.10994893809159596 Validation Loss: 0.7391030192375183\n",
      "Epoch 9342: Training Loss: 0.10970773299535115 Validation Loss: 0.7391781210899353\n",
      "Epoch 9343: Training Loss: 0.11019238084554672 Validation Loss: 0.7398273944854736\n",
      "Epoch 9344: Training Loss: 0.11007572462161382 Validation Loss: 0.7403392195701599\n",
      "Epoch 9345: Training Loss: 0.11068600912888844 Validation Loss: 0.739596962928772\n",
      "Epoch 9346: Training Loss: 0.11023042102654775 Validation Loss: 0.7389843463897705\n",
      "Epoch 9347: Training Loss: 0.10970912377039592 Validation Loss: 0.7389758229255676\n",
      "Epoch 9348: Training Loss: 0.10986512899398804 Validation Loss: 0.7390462756156921\n",
      "Epoch 9349: Training Loss: 0.11043216039737065 Validation Loss: 0.7392786145210266\n",
      "Epoch 9350: Training Loss: 0.10980124026536942 Validation Loss: 0.7394063472747803\n",
      "Epoch 9351: Training Loss: 0.10991161564985912 Validation Loss: 0.7393949627876282\n",
      "Epoch 9352: Training Loss: 0.10962516566117604 Validation Loss: 0.7391225099563599\n",
      "Epoch 9353: Training Loss: 0.10940605401992798 Validation Loss: 0.7391267418861389\n",
      "Epoch 9354: Training Loss: 0.10951220740874608 Validation Loss: 0.7390049695968628\n",
      "Epoch 9355: Training Loss: 0.10964833945035934 Validation Loss: 0.7386632561683655\n",
      "Epoch 9356: Training Loss: 0.11000268658002217 Validation Loss: 0.7392467260360718\n",
      "Epoch 9357: Training Loss: 0.10989251484473546 Validation Loss: 0.7395224571228027\n",
      "Epoch 9358: Training Loss: 0.10972269624471664 Validation Loss: 0.7391204237937927\n",
      "Epoch 9359: Training Loss: 0.1096039538582166 Validation Loss: 0.7392056584358215\n",
      "Epoch 9360: Training Loss: 0.11015392343203227 Validation Loss: 0.7394583225250244\n",
      "Epoch 9361: Training Loss: 0.11113133033116658 Validation Loss: 0.7399731874465942\n",
      "Epoch 9362: Training Loss: 0.10971128443876903 Validation Loss: 0.7398697137832642\n",
      "Epoch 9363: Training Loss: 0.11001824587583542 Validation Loss: 0.7397982478141785\n",
      "Epoch 9364: Training Loss: 0.1096594159801801 Validation Loss: 0.7396396994590759\n",
      "Epoch 9365: Training Loss: 0.10966824491818745 Validation Loss: 0.7392135262489319\n",
      "Epoch 9366: Training Loss: 0.10958365350961685 Validation Loss: 0.7392159700393677\n",
      "Epoch 9367: Training Loss: 0.11037135620911916 Validation Loss: 0.7391137480735779\n",
      "Epoch 9368: Training Loss: 0.10983963559071223 Validation Loss: 0.739520251750946\n",
      "Epoch 9369: Training Loss: 0.10935268551111221 Validation Loss: 0.7401213645935059\n",
      "Epoch 9370: Training Loss: 0.10908312598864238 Validation Loss: 0.7405561208724976\n",
      "Epoch 9371: Training Loss: 0.10965202748775482 Validation Loss: 0.7403301000595093\n",
      "Epoch 9372: Training Loss: 0.10979545613129933 Validation Loss: 0.7400028705596924\n",
      "Epoch 9373: Training Loss: 0.11014222105344136 Validation Loss: 0.7404093146324158\n",
      "Epoch 9374: Training Loss: 0.10965624203284581 Validation Loss: 0.7402002811431885\n",
      "Epoch 9375: Training Loss: 0.11001584678888321 Validation Loss: 0.7397407293319702\n",
      "Epoch 9376: Training Loss: 0.10992346207300822 Validation Loss: 0.7394279837608337\n",
      "Epoch 9377: Training Loss: 0.10955997308095296 Validation Loss: 0.7397739291191101\n",
      "Epoch 9378: Training Loss: 0.10954524825016658 Validation Loss: 0.7397413849830627\n",
      "Epoch 9379: Training Loss: 0.10990066329638164 Validation Loss: 0.7399365901947021\n",
      "Epoch 9380: Training Loss: 0.10938440014918645 Validation Loss: 0.7398916482925415\n",
      "Epoch 9381: Training Loss: 0.10946464041868846 Validation Loss: 0.7400282621383667\n",
      "Epoch 9382: Training Loss: 0.10926589618126552 Validation Loss: 0.7399857044219971\n",
      "Epoch 9383: Training Loss: 0.10966096570094426 Validation Loss: 0.739646315574646\n",
      "Epoch 9384: Training Loss: 0.10949474821488063 Validation Loss: 0.7392949461936951\n",
      "Epoch 9385: Training Loss: 0.10947329550981522 Validation Loss: 0.7394185066223145\n",
      "Epoch 9386: Training Loss: 0.10977510611216228 Validation Loss: 0.739434003829956\n",
      "Epoch 9387: Training Loss: 0.10987040152152379 Validation Loss: 0.7395375370979309\n",
      "Epoch 9388: Training Loss: 0.10965195794900258 Validation Loss: 0.740058422088623\n",
      "Epoch 9389: Training Loss: 0.10938559969266255 Validation Loss: 0.7406738996505737\n",
      "Epoch 9390: Training Loss: 0.10950937370459239 Validation Loss: 0.7406301498413086\n",
      "Epoch 9391: Training Loss: 0.10940366486708324 Validation Loss: 0.7402613759040833\n",
      "Epoch 9392: Training Loss: 0.10940487186113994 Validation Loss: 0.7398862242698669\n",
      "Epoch 9393: Training Loss: 0.10947306950887044 Validation Loss: 0.7396108508110046\n",
      "Epoch 9394: Training Loss: 0.11074606329202652 Validation Loss: 0.7394148707389832\n",
      "Epoch 9395: Training Loss: 0.11049880335728328 Validation Loss: 0.7396628856658936\n",
      "Epoch 9396: Training Loss: 0.10929715136686961 Validation Loss: 0.7398075461387634\n",
      "Epoch 9397: Training Loss: 0.10940101246039073 Validation Loss: 0.7402212023735046\n",
      "Epoch 9398: Training Loss: 0.10947473595539729 Validation Loss: 0.7401341795921326\n",
      "Epoch 9399: Training Loss: 0.10972844064235687 Validation Loss: 0.7403762936592102\n",
      "Epoch 9400: Training Loss: 0.10933683564265569 Validation Loss: 0.7399581670761108\n",
      "Epoch 9401: Training Loss: 0.10945747544368108 Validation Loss: 0.7398771643638611\n",
      "Epoch 9402: Training Loss: 0.10926452279090881 Validation Loss: 0.739923894405365\n",
      "Epoch 9403: Training Loss: 0.1091814637184143 Validation Loss: 0.7402268648147583\n",
      "Epoch 9404: Training Loss: 0.1090221827228864 Validation Loss: 0.7403690814971924\n",
      "Epoch 9405: Training Loss: 0.11048339307308197 Validation Loss: 0.7404069304466248\n",
      "Epoch 9406: Training Loss: 0.10945770144462585 Validation Loss: 0.7404143214225769\n",
      "Epoch 9407: Training Loss: 0.10975251346826553 Validation Loss: 0.7403318881988525\n",
      "Epoch 9408: Training Loss: 0.10931253433227539 Validation Loss: 0.7395948171615601\n",
      "Epoch 9409: Training Loss: 0.10959766308466594 Validation Loss: 0.7393136024475098\n",
      "Epoch 9410: Training Loss: 0.1096635361512502 Validation Loss: 0.7395596504211426\n",
      "Epoch 9411: Training Loss: 0.10924576471249263 Validation Loss: 0.7394943237304688\n",
      "Epoch 9412: Training Loss: 0.10949036478996277 Validation Loss: 0.7397533655166626\n",
      "Epoch 9413: Training Loss: 0.10976117849349976 Validation Loss: 0.7403536438941956\n",
      "Epoch 9414: Training Loss: 0.10952443132797877 Validation Loss: 0.7402378916740417\n",
      "Epoch 9415: Training Loss: 0.10941445579131444 Validation Loss: 0.7404186129570007\n",
      "Epoch 9416: Training Loss: 0.10913743327061336 Validation Loss: 0.740056037902832\n",
      "Epoch 9417: Training Loss: 0.1090779850880305 Validation Loss: 0.7399385571479797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9418: Training Loss: 0.10941035797198613 Validation Loss: 0.7402083277702332\n",
      "Epoch 9419: Training Loss: 0.10917617132266362 Validation Loss: 0.7402639389038086\n",
      "Epoch 9420: Training Loss: 0.10931649059057236 Validation Loss: 0.7407647371292114\n",
      "Epoch 9421: Training Loss: 0.10953374703725179 Validation Loss: 0.7406109571456909\n",
      "Epoch 9422: Training Loss: 0.11058392624060313 Validation Loss: 0.7403410077095032\n",
      "Epoch 9423: Training Loss: 0.10940414667129517 Validation Loss: 0.7402247786521912\n",
      "Epoch 9424: Training Loss: 0.10964383433262508 Validation Loss: 0.74029141664505\n",
      "Epoch 9425: Training Loss: 0.1093602105975151 Validation Loss: 0.7403992414474487\n",
      "Epoch 9426: Training Loss: 0.1091877892613411 Validation Loss: 0.739922046661377\n",
      "Epoch 9427: Training Loss: 0.10920770466327667 Validation Loss: 0.7397423982620239\n",
      "Epoch 9428: Training Loss: 0.10999493549267451 Validation Loss: 0.7394986748695374\n",
      "Epoch 9429: Training Loss: 0.10920754075050354 Validation Loss: 0.739961564540863\n",
      "Epoch 9430: Training Loss: 0.10918598622083664 Validation Loss: 0.7403091192245483\n",
      "Epoch 9431: Training Loss: 0.10955995072921117 Validation Loss: 0.7404739260673523\n",
      "Epoch 9432: Training Loss: 0.10911006728808086 Validation Loss: 0.7408472299575806\n",
      "Epoch 9433: Training Loss: 0.10935305307308833 Validation Loss: 0.7402912378311157\n",
      "Epoch 9434: Training Loss: 0.10904503613710403 Validation Loss: 0.7403339743614197\n",
      "Epoch 9435: Training Loss: 0.10904454936583836 Validation Loss: 0.7400637269020081\n",
      "Epoch 9436: Training Loss: 0.10923080891370773 Validation Loss: 0.7403479218482971\n",
      "Epoch 9437: Training Loss: 0.10897792379061381 Validation Loss: 0.7404792904853821\n",
      "Epoch 9438: Training Loss: 0.10938250521818797 Validation Loss: 0.7399967312812805\n",
      "Epoch 9439: Training Loss: 0.10931859910488129 Validation Loss: 0.7401013970375061\n",
      "Epoch 9440: Training Loss: 0.10934964815775554 Validation Loss: 0.7401195764541626\n",
      "Epoch 9441: Training Loss: 0.1093524048725764 Validation Loss: 0.7401456236839294\n",
      "Epoch 9442: Training Loss: 0.10916947076718013 Validation Loss: 0.7403174042701721\n",
      "Epoch 9443: Training Loss: 0.10904387633005778 Validation Loss: 0.7403185963630676\n",
      "Epoch 9444: Training Loss: 0.10967539250850677 Validation Loss: 0.7403939366340637\n",
      "Epoch 9445: Training Loss: 0.10852106163899104 Validation Loss: 0.7406294941902161\n",
      "Epoch 9446: Training Loss: 0.10863362501064937 Validation Loss: 0.7410522699356079\n",
      "Epoch 9447: Training Loss: 0.10872240861256917 Validation Loss: 0.7411566972732544\n",
      "Epoch 9448: Training Loss: 0.11013197402159373 Validation Loss: 0.7411510944366455\n",
      "Epoch 9449: Training Loss: 0.10928435375293095 Validation Loss: 0.7405136823654175\n",
      "Epoch 9450: Training Loss: 0.10910105456908543 Validation Loss: 0.73987877368927\n",
      "Epoch 9451: Training Loss: 0.10957404226064682 Validation Loss: 0.7400511503219604\n",
      "Epoch 9452: Training Loss: 0.10894257078568141 Validation Loss: 0.7403461337089539\n",
      "Epoch 9453: Training Loss: 0.10877686987320583 Validation Loss: 0.7407954931259155\n",
      "Epoch 9454: Training Loss: 0.10895741979281108 Validation Loss: 0.7410001158714294\n",
      "Epoch 9455: Training Loss: 0.10906465351581573 Validation Loss: 0.7411138415336609\n",
      "Epoch 9456: Training Loss: 0.10888021936019261 Validation Loss: 0.7405188083648682\n",
      "Epoch 9457: Training Loss: 0.10902937998374303 Validation Loss: 0.7403934597969055\n",
      "Epoch 9458: Training Loss: 0.1089499443769455 Validation Loss: 0.7401316165924072\n",
      "Epoch 9459: Training Loss: 0.10940993577241898 Validation Loss: 0.7401916980743408\n",
      "Epoch 9460: Training Loss: 0.10934970776240031 Validation Loss: 0.7401992678642273\n",
      "Epoch 9461: Training Loss: 0.10952984790007274 Validation Loss: 0.7403908371925354\n",
      "Epoch 9462: Training Loss: 0.10887676229079564 Validation Loss: 0.7406215071678162\n",
      "Epoch 9463: Training Loss: 0.10883153975009918 Validation Loss: 0.7409476637840271\n",
      "Epoch 9464: Training Loss: 0.10869309057792027 Validation Loss: 0.7411235570907593\n",
      "Epoch 9465: Training Loss: 0.10857014606396358 Validation Loss: 0.7409070134162903\n",
      "Epoch 9466: Training Loss: 0.10883536686499913 Validation Loss: 0.7406014800071716\n",
      "Epoch 9467: Training Loss: 0.10863626499970754 Validation Loss: 0.7407940626144409\n",
      "Epoch 9468: Training Loss: 0.10851750026146571 Validation Loss: 0.7409949898719788\n",
      "Epoch 9469: Training Loss: 0.10881518572568893 Validation Loss: 0.7406630516052246\n",
      "Epoch 9470: Training Loss: 0.10905377566814423 Validation Loss: 0.7406430244445801\n",
      "Epoch 9471: Training Loss: 0.10891411453485489 Validation Loss: 0.7408645153045654\n",
      "Epoch 9472: Training Loss: 0.10896398375431697 Validation Loss: 0.7407382726669312\n",
      "Epoch 9473: Training Loss: 0.1089668944478035 Validation Loss: 0.7405562400817871\n",
      "Epoch 9474: Training Loss: 0.10873524596293767 Validation Loss: 0.7409356832504272\n",
      "Epoch 9475: Training Loss: 0.10926605761051178 Validation Loss: 0.7413221597671509\n",
      "Epoch 9476: Training Loss: 0.10882466783126195 Validation Loss: 0.7410500645637512\n",
      "Epoch 9477: Training Loss: 0.10869881759087245 Validation Loss: 0.741021454334259\n",
      "Epoch 9478: Training Loss: 0.10896735390027364 Validation Loss: 0.7409434914588928\n",
      "Epoch 9479: Training Loss: 0.10919278860092163 Validation Loss: 0.7402880191802979\n",
      "Epoch 9480: Training Loss: 0.10884053508440654 Validation Loss: 0.7406205534934998\n",
      "Epoch 9481: Training Loss: 0.10888898372650146 Validation Loss: 0.7409264445304871\n",
      "Epoch 9482: Training Loss: 0.10871104151010513 Validation Loss: 0.7414495944976807\n",
      "Epoch 9483: Training Loss: 0.1089493955175082 Validation Loss: 0.7412624359130859\n",
      "Epoch 9484: Training Loss: 0.1086092119415601 Validation Loss: 0.7407444715499878\n",
      "Epoch 9485: Training Loss: 0.1090693324804306 Validation Loss: 0.7403414249420166\n",
      "Epoch 9486: Training Loss: 0.11060042182604472 Validation Loss: 0.7403215169906616\n",
      "Epoch 9487: Training Loss: 0.10855658849080403 Validation Loss: 0.7406076788902283\n",
      "Epoch 9488: Training Loss: 0.10861799369255702 Validation Loss: 0.7411744594573975\n",
      "Epoch 9489: Training Loss: 0.10873721291621526 Validation Loss: 0.7409428954124451\n",
      "Epoch 9490: Training Loss: 0.10857022056976955 Validation Loss: 0.7415279746055603\n",
      "Epoch 9491: Training Loss: 0.10895848274230957 Validation Loss: 0.7411174774169922\n",
      "Epoch 9492: Training Loss: 0.10870222995678584 Validation Loss: 0.740446925163269\n",
      "Epoch 9493: Training Loss: 0.1083505352338155 Validation Loss: 0.7404876351356506\n",
      "Epoch 9494: Training Loss: 0.10887109736601512 Validation Loss: 0.7406918406486511\n",
      "Epoch 9495: Training Loss: 0.10899757593870163 Validation Loss: 0.7410334348678589\n",
      "Epoch 9496: Training Loss: 0.10854367166757584 Validation Loss: 0.7411642670631409\n",
      "Epoch 9497: Training Loss: 0.10838405042886734 Validation Loss: 0.7410153746604919\n",
      "Epoch 9498: Training Loss: 0.10819098601738612 Validation Loss: 0.7410008907318115\n",
      "Epoch 9499: Training Loss: 0.10836569716533025 Validation Loss: 0.7408580780029297\n",
      "Epoch 9500: Training Loss: 0.10844934731721878 Validation Loss: 0.7405736446380615\n",
      "Epoch 9501: Training Loss: 0.1081724464893341 Validation Loss: 0.740639865398407\n",
      "Epoch 9502: Training Loss: 0.10865602393945058 Validation Loss: 0.741020917892456\n",
      "Epoch 9503: Training Loss: 0.10863736768563588 Validation Loss: 0.7411551475524902\n",
      "Epoch 9504: Training Loss: 0.10846248517433803 Validation Loss: 0.7409459352493286\n",
      "Epoch 9505: Training Loss: 0.1088523839910825 Validation Loss: 0.7411691546440125\n",
      "Epoch 9506: Training Loss: 0.10844336698452632 Validation Loss: 0.7415701746940613\n",
      "Epoch 9507: Training Loss: 0.10856957981983821 Validation Loss: 0.7415362000465393\n",
      "Epoch 9508: Training Loss: 0.10880513489246368 Validation Loss: 0.7411314845085144\n",
      "Epoch 9509: Training Loss: 0.1082504391670227 Validation Loss: 0.7407522201538086\n",
      "Epoch 9510: Training Loss: 0.10796045511960983 Validation Loss: 0.7406531572341919\n",
      "Epoch 9511: Training Loss: 0.10862626383701961 Validation Loss: 0.7405862808227539\n",
      "Epoch 9512: Training Loss: 0.1097682590285937 Validation Loss: 0.7410517930984497\n",
      "Epoch 9513: Training Loss: 0.10874421149492264 Validation Loss: 0.7411232590675354\n",
      "Epoch 9514: Training Loss: 0.10836112995942433 Validation Loss: 0.7408445477485657\n",
      "Epoch 9515: Training Loss: 0.10822714120149612 Validation Loss: 0.7406222224235535\n",
      "Epoch 9516: Training Loss: 0.1085507869720459 Validation Loss: 0.7406557202339172\n",
      "Epoch 9517: Training Loss: 0.10837824394305547 Validation Loss: 0.7414025068283081\n",
      "Epoch 9518: Training Loss: 0.10853499670823415 Validation Loss: 0.7417923212051392\n",
      "Epoch 9519: Training Loss: 0.10797817508379619 Validation Loss: 0.7412940859794617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9520: Training Loss: 0.10885178297758102 Validation Loss: 0.7407947778701782\n",
      "Epoch 9521: Training Loss: 0.10791084418694179 Validation Loss: 0.7404900789260864\n",
      "Epoch 9522: Training Loss: 0.10833028455575307 Validation Loss: 0.7405672669410706\n",
      "Epoch 9523: Training Loss: 0.10810829202334087 Validation Loss: 0.7408890724182129\n",
      "Epoch 9524: Training Loss: 0.10805712391932805 Validation Loss: 0.7413500547409058\n",
      "Epoch 9525: Training Loss: 0.10803838819265366 Validation Loss: 0.7416343092918396\n",
      "Epoch 9526: Training Loss: 0.10838536421457927 Validation Loss: 0.7417543530464172\n",
      "Epoch 9527: Training Loss: 0.10826112826665242 Validation Loss: 0.7417338490486145\n",
      "Epoch 9528: Training Loss: 0.10813440630833308 Validation Loss: 0.7411901354789734\n",
      "Epoch 9529: Training Loss: 0.10831609865029652 Validation Loss: 0.7412084341049194\n",
      "Epoch 9530: Training Loss: 0.10869993269443512 Validation Loss: 0.7410833835601807\n",
      "Epoch 9531: Training Loss: 0.10820983598629634 Validation Loss: 0.7412941455841064\n",
      "Epoch 9532: Training Loss: 0.10871042062838872 Validation Loss: 0.7417222261428833\n",
      "Epoch 9533: Training Loss: 0.10830807685852051 Validation Loss: 0.7414897680282593\n",
      "Epoch 9534: Training Loss: 0.10813070336977641 Validation Loss: 0.7414400577545166\n",
      "Epoch 9535: Training Loss: 0.10792129238446553 Validation Loss: 0.7412574291229248\n",
      "Epoch 9536: Training Loss: 0.10830719769001007 Validation Loss: 0.7409308552742004\n",
      "Epoch 9537: Training Loss: 0.10852471987406413 Validation Loss: 0.7409281134605408\n",
      "Epoch 9538: Training Loss: 0.10834431648254395 Validation Loss: 0.7410099506378174\n",
      "Epoch 9539: Training Loss: 0.10799836367368698 Validation Loss: 0.7411678433418274\n",
      "Epoch 9540: Training Loss: 0.10808260490496953 Validation Loss: 0.7412008047103882\n",
      "Epoch 9541: Training Loss: 0.10789416978756587 Validation Loss: 0.7415708899497986\n",
      "Epoch 9542: Training Loss: 0.1077751691142718 Validation Loss: 0.741767168045044\n",
      "Epoch 9543: Training Loss: 0.10830432673295338 Validation Loss: 0.7415553331375122\n",
      "Epoch 9544: Training Loss: 0.10796646028757095 Validation Loss: 0.7412660121917725\n",
      "Epoch 9545: Training Loss: 0.10808540880680084 Validation Loss: 0.7411546111106873\n",
      "Epoch 9546: Training Loss: 0.1091519370675087 Validation Loss: 0.7411917448043823\n",
      "Epoch 9547: Training Loss: 0.10800483326117198 Validation Loss: 0.7414242029190063\n",
      "Epoch 9548: Training Loss: 0.10865258673826854 Validation Loss: 0.7412812113761902\n",
      "Epoch 9549: Training Loss: 0.10842662304639816 Validation Loss: 0.7414879202842712\n",
      "Epoch 9550: Training Loss: 0.10943285375833511 Validation Loss: 0.7417445778846741\n",
      "Epoch 9551: Training Loss: 0.10853242874145508 Validation Loss: 0.7418685555458069\n",
      "Epoch 9552: Training Loss: 0.10810820758342743 Validation Loss: 0.7418107390403748\n",
      "Epoch 9553: Training Loss: 0.10868501911560695 Validation Loss: 0.7416917085647583\n",
      "Epoch 9554: Training Loss: 0.10930675516525905 Validation Loss: 0.7411864995956421\n",
      "Epoch 9555: Training Loss: 0.10794346779584885 Validation Loss: 0.7410731911659241\n",
      "Epoch 9556: Training Loss: 0.10806733121474583 Validation Loss: 0.7410505414009094\n",
      "Epoch 9557: Training Loss: 0.10796323666969936 Validation Loss: 0.7415086627006531\n",
      "Epoch 9558: Training Loss: 0.10804800689220428 Validation Loss: 0.7420550584793091\n",
      "Epoch 9559: Training Loss: 0.10843026886383693 Validation Loss: 0.741641104221344\n",
      "Epoch 9560: Training Loss: 0.10842104256153107 Validation Loss: 0.741805911064148\n",
      "Epoch 9561: Training Loss: 0.10793844362099965 Validation Loss: 0.741536021232605\n",
      "Epoch 9562: Training Loss: 0.10812316834926605 Validation Loss: 0.7413510680198669\n",
      "Epoch 9563: Training Loss: 0.10803155849377315 Validation Loss: 0.7411751747131348\n",
      "Epoch 9564: Training Loss: 0.10876653840144475 Validation Loss: 0.741541862487793\n",
      "Epoch 9565: Training Loss: 0.10782887041568756 Validation Loss: 0.7416881918907166\n",
      "Epoch 9566: Training Loss: 0.10794605563084285 Validation Loss: 0.7417975068092346\n",
      "Epoch 9567: Training Loss: 0.10857781271139781 Validation Loss: 0.7420000433921814\n",
      "Epoch 9568: Training Loss: 0.10813785592714946 Validation Loss: 0.7416664361953735\n",
      "Epoch 9569: Training Loss: 0.1079676349957784 Validation Loss: 0.7417468428611755\n",
      "Epoch 9570: Training Loss: 0.1073417862256368 Validation Loss: 0.7417812943458557\n",
      "Epoch 9571: Training Loss: 0.1076010266939799 Validation Loss: 0.7415127158164978\n",
      "Epoch 9572: Training Loss: 0.10830656439065933 Validation Loss: 0.7422273755073547\n",
      "Epoch 9573: Training Loss: 0.10841454813877742 Validation Loss: 0.7426547408103943\n",
      "Epoch 9574: Training Loss: 0.10800599058469136 Validation Loss: 0.7423349022865295\n",
      "Epoch 9575: Training Loss: 0.10766427218914032 Validation Loss: 0.7421034574508667\n",
      "Epoch 9576: Training Loss: 0.108354186018308 Validation Loss: 0.7421565055847168\n",
      "Epoch 9577: Training Loss: 0.1076024721066157 Validation Loss: 0.7421985268592834\n",
      "Epoch 9578: Training Loss: 0.1076895942290624 Validation Loss: 0.7423568367958069\n",
      "Epoch 9579: Training Loss: 0.10811919967333476 Validation Loss: 0.7418901920318604\n",
      "Epoch 9580: Training Loss: 0.10791633029778798 Validation Loss: 0.7418254613876343\n",
      "Epoch 9581: Training Loss: 0.10801620284716289 Validation Loss: 0.7420409321784973\n",
      "Epoch 9582: Training Loss: 0.10792504251003265 Validation Loss: 0.7418323159217834\n",
      "Epoch 9583: Training Loss: 0.10777026414871216 Validation Loss: 0.741595447063446\n",
      "Epoch 9584: Training Loss: 0.10778801143169403 Validation Loss: 0.7415345907211304\n",
      "Epoch 9585: Training Loss: 0.1079704115788142 Validation Loss: 0.7420516610145569\n",
      "Epoch 9586: Training Loss: 0.10816011826197307 Validation Loss: 0.7420021295547485\n",
      "Epoch 9587: Training Loss: 0.10832160462935765 Validation Loss: 0.741945207118988\n",
      "Epoch 9588: Training Loss: 0.10860406110684077 Validation Loss: 0.7416004538536072\n",
      "Epoch 9589: Training Loss: 0.10772170126438141 Validation Loss: 0.7417406439781189\n",
      "Epoch 9590: Training Loss: 0.1077488362789154 Validation Loss: 0.7422282695770264\n",
      "Epoch 9591: Training Loss: 0.10791037480036418 Validation Loss: 0.7423279881477356\n",
      "Epoch 9592: Training Loss: 0.10758380591869354 Validation Loss: 0.7420306205749512\n",
      "Epoch 9593: Training Loss: 0.10801356037457784 Validation Loss: 0.7418051958084106\n",
      "Epoch 9594: Training Loss: 0.10797174523274104 Validation Loss: 0.7419730424880981\n",
      "Epoch 9595: Training Loss: 0.10768312464157741 Validation Loss: 0.7418633699417114\n",
      "Epoch 9596: Training Loss: 0.10799489418665568 Validation Loss: 0.7414707541465759\n",
      "Epoch 9597: Training Loss: 0.10769603898127873 Validation Loss: 0.7415750622749329\n",
      "Epoch 9598: Training Loss: 0.10801164309183757 Validation Loss: 0.7416715025901794\n",
      "Epoch 9599: Training Loss: 0.10815020650625229 Validation Loss: 0.7419127225875854\n",
      "Epoch 9600: Training Loss: 0.10837692270676295 Validation Loss: 0.7420904040336609\n",
      "Epoch 9601: Training Loss: 0.10755347460508347 Validation Loss: 0.7422243356704712\n",
      "Epoch 9602: Training Loss: 0.10751478622357051 Validation Loss: 0.741992712020874\n",
      "Epoch 9603: Training Loss: 0.10737236589193344 Validation Loss: 0.742003321647644\n",
      "Epoch 9604: Training Loss: 0.10768263290325801 Validation Loss: 0.7417593598365784\n",
      "Epoch 9605: Training Loss: 0.10787429163853328 Validation Loss: 0.7419343590736389\n",
      "Epoch 9606: Training Loss: 0.10767708470424016 Validation Loss: 0.7421302795410156\n",
      "Epoch 9607: Training Loss: 0.10775533318519592 Validation Loss: 0.7419493794441223\n",
      "Epoch 9608: Training Loss: 0.10794385025898616 Validation Loss: 0.7417445182800293\n",
      "Epoch 9609: Training Loss: 0.10792153825362523 Validation Loss: 0.7421874403953552\n",
      "Epoch 9610: Training Loss: 0.10767479240894318 Validation Loss: 0.7424766421318054\n",
      "Epoch 9611: Training Loss: 0.10752365241448085 Validation Loss: 0.7426097989082336\n",
      "Epoch 9612: Training Loss: 0.10771073897679646 Validation Loss: 0.742110550403595\n",
      "Epoch 9613: Training Loss: 0.10895049571990967 Validation Loss: 0.742199718952179\n",
      "Epoch 9614: Training Loss: 0.10834493239720662 Validation Loss: 0.7424044609069824\n",
      "Epoch 9615: Training Loss: 0.1083147997657458 Validation Loss: 0.7420579195022583\n",
      "Epoch 9616: Training Loss: 0.10774544129769008 Validation Loss: 0.7420392632484436\n",
      "Epoch 9617: Training Loss: 0.10796275983254115 Validation Loss: 0.7419764399528503\n",
      "Epoch 9618: Training Loss: 0.10814744730790456 Validation Loss: 0.7418959736824036\n",
      "Epoch 9619: Training Loss: 0.10740089168151219 Validation Loss: 0.7419543862342834\n",
      "Epoch 9620: Training Loss: 0.10783148805300395 Validation Loss: 0.742000937461853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9621: Training Loss: 0.10800001521905263 Validation Loss: 0.7421075105667114\n",
      "Epoch 9622: Training Loss: 0.10768651713927586 Validation Loss: 0.741839587688446\n",
      "Epoch 9623: Training Loss: 0.10745865106582642 Validation Loss: 0.741722583770752\n",
      "Epoch 9624: Training Loss: 0.10754540314277013 Validation Loss: 0.7421010732650757\n",
      "Epoch 9625: Training Loss: 0.10772843410571416 Validation Loss: 0.7423571944236755\n",
      "Epoch 9626: Training Loss: 0.10755428423484166 Validation Loss: 0.7421518564224243\n",
      "Epoch 9627: Training Loss: 0.10749867806831996 Validation Loss: 0.7417750358581543\n",
      "Epoch 9628: Training Loss: 0.1078717236717542 Validation Loss: 0.7420107126235962\n",
      "Epoch 9629: Training Loss: 0.10738208641608556 Validation Loss: 0.7419049739837646\n",
      "Epoch 9630: Training Loss: 0.10739965240160625 Validation Loss: 0.742099404335022\n",
      "Epoch 9631: Training Loss: 0.1073579266667366 Validation Loss: 0.742211103439331\n",
      "Epoch 9632: Training Loss: 0.1074557105700175 Validation Loss: 0.7421281337738037\n",
      "Epoch 9633: Training Loss: 0.10755682736635208 Validation Loss: 0.7422063946723938\n",
      "Epoch 9634: Training Loss: 0.10769996543725331 Validation Loss: 0.7422844767570496\n",
      "Epoch 9635: Training Loss: 0.10775249948104222 Validation Loss: 0.7423639297485352\n",
      "Epoch 9636: Training Loss: 0.10771902898947398 Validation Loss: 0.7421170473098755\n",
      "Epoch 9637: Training Loss: 0.10788372655709584 Validation Loss: 0.742436945438385\n",
      "Epoch 9638: Training Loss: 0.10740619897842407 Validation Loss: 0.7420135140419006\n",
      "Epoch 9639: Training Loss: 0.10830854376157124 Validation Loss: 0.7424401044845581\n",
      "Epoch 9640: Training Loss: 0.10718890031178792 Validation Loss: 0.7427770495414734\n",
      "Epoch 9641: Training Loss: 0.10745379328727722 Validation Loss: 0.7426892518997192\n",
      "Epoch 9642: Training Loss: 0.10770043730735779 Validation Loss: 0.7427013516426086\n",
      "Epoch 9643: Training Loss: 0.10755433887243271 Validation Loss: 0.7421015501022339\n",
      "Epoch 9644: Training Loss: 0.10730744153261185 Validation Loss: 0.7420874834060669\n",
      "Epoch 9645: Training Loss: 0.10733753194411595 Validation Loss: 0.7422398924827576\n",
      "Epoch 9646: Training Loss: 0.10754696528116862 Validation Loss: 0.7419605255126953\n",
      "Epoch 9647: Training Loss: 0.10741029431422551 Validation Loss: 0.7423980832099915\n",
      "Epoch 9648: Training Loss: 0.10725382467110951 Validation Loss: 0.7429302930831909\n",
      "Epoch 9649: Training Loss: 0.10792958488066991 Validation Loss: 0.7428264021873474\n",
      "Epoch 9650: Training Loss: 0.10744225978851318 Validation Loss: 0.7428761124610901\n",
      "Epoch 9651: Training Loss: 0.1083432783683141 Validation Loss: 0.7426902055740356\n",
      "Epoch 9652: Training Loss: 0.10735436032215755 Validation Loss: 0.7425944209098816\n",
      "Epoch 9653: Training Loss: 0.10736653705437978 Validation Loss: 0.7424481511116028\n",
      "Epoch 9654: Training Loss: 0.10747109105189641 Validation Loss: 0.7427078485488892\n",
      "Epoch 9655: Training Loss: 0.10770550618569057 Validation Loss: 0.7426660060882568\n",
      "Epoch 9656: Training Loss: 0.10746463139851888 Validation Loss: 0.7430980801582336\n",
      "Epoch 9657: Training Loss: 0.10720329980055492 Validation Loss: 0.7431071996688843\n",
      "Epoch 9658: Training Loss: 0.10739543288946152 Validation Loss: 0.7432621121406555\n",
      "Epoch 9659: Training Loss: 0.10723014424244563 Validation Loss: 0.7423812747001648\n",
      "Epoch 9660: Training Loss: 0.10731074462334315 Validation Loss: 0.7420793771743774\n",
      "Epoch 9661: Training Loss: 0.10730543732643127 Validation Loss: 0.7422643303871155\n",
      "Epoch 9662: Training Loss: 0.1077804093559583 Validation Loss: 0.7421572804450989\n",
      "Epoch 9663: Training Loss: 0.1070643166700999 Validation Loss: 0.7427051067352295\n",
      "Epoch 9664: Training Loss: 0.10682053615649541 Validation Loss: 0.7429620027542114\n",
      "Epoch 9665: Training Loss: 0.10718230654795964 Validation Loss: 0.7427637577056885\n",
      "Epoch 9666: Training Loss: 0.10758239775896072 Validation Loss: 0.7427953481674194\n",
      "Epoch 9667: Training Loss: 0.10702326148748398 Validation Loss: 0.743122398853302\n",
      "Epoch 9668: Training Loss: 0.10758002350727718 Validation Loss: 0.7426066398620605\n",
      "Epoch 9669: Training Loss: 0.10694028685490291 Validation Loss: 0.7428663969039917\n",
      "Epoch 9670: Training Loss: 0.10745221376419067 Validation Loss: 0.7431302070617676\n",
      "Epoch 9671: Training Loss: 0.10728339850902557 Validation Loss: 0.7425473928451538\n",
      "Epoch 9672: Training Loss: 0.10703487942616145 Validation Loss: 0.7423526644706726\n",
      "Epoch 9673: Training Loss: 0.10705219705899556 Validation Loss: 0.7425847053527832\n",
      "Epoch 9674: Training Loss: 0.1072841187318166 Validation Loss: 0.7430011630058289\n",
      "Epoch 9675: Training Loss: 0.10710663845141728 Validation Loss: 0.7431526780128479\n",
      "Epoch 9676: Training Loss: 0.10809336105982463 Validation Loss: 0.743058979511261\n",
      "Epoch 9677: Training Loss: 0.10768195738395055 Validation Loss: 0.7426159381866455\n",
      "Epoch 9678: Training Loss: 0.10721364120642345 Validation Loss: 0.742517352104187\n",
      "Epoch 9679: Training Loss: 0.10714719196160634 Validation Loss: 0.7423796653747559\n",
      "Epoch 9680: Training Loss: 0.1070459708571434 Validation Loss: 0.7427465319633484\n",
      "Epoch 9681: Training Loss: 0.10707300653060277 Validation Loss: 0.7426996827125549\n",
      "Epoch 9682: Training Loss: 0.10705032696326573 Validation Loss: 0.7424941062927246\n",
      "Epoch 9683: Training Loss: 0.10728317995866139 Validation Loss: 0.742376983165741\n",
      "Epoch 9684: Training Loss: 0.10741256674130757 Validation Loss: 0.742854118347168\n",
      "Epoch 9685: Training Loss: 0.10653449098269145 Validation Loss: 0.7430124282836914\n",
      "Epoch 9686: Training Loss: 0.10723082472880681 Validation Loss: 0.7431597709655762\n",
      "Epoch 9687: Training Loss: 0.10704948256413142 Validation Loss: 0.7429211735725403\n",
      "Epoch 9688: Training Loss: 0.10797685881455739 Validation Loss: 0.7428596615791321\n",
      "Epoch 9689: Training Loss: 0.1072818214694659 Validation Loss: 0.7433456182479858\n",
      "Epoch 9690: Training Loss: 0.10708984980980556 Validation Loss: 0.7426417469978333\n",
      "Epoch 9691: Training Loss: 0.10729364802440007 Validation Loss: 0.7427579164505005\n",
      "Epoch 9692: Training Loss: 0.10697033007939656 Validation Loss: 0.7426973581314087\n",
      "Epoch 9693: Training Loss: 0.1072532484928767 Validation Loss: 0.7429021596908569\n",
      "Epoch 9694: Training Loss: 0.10720575600862503 Validation Loss: 0.7429874539375305\n",
      "Epoch 9695: Training Loss: 0.1072108546892802 Validation Loss: 0.7430229187011719\n",
      "Epoch 9696: Training Loss: 0.10680904239416122 Validation Loss: 0.7431226968765259\n",
      "Epoch 9697: Training Loss: 0.10729445020357768 Validation Loss: 0.743130087852478\n",
      "Epoch 9698: Training Loss: 0.10751074055830638 Validation Loss: 0.7429808378219604\n",
      "Epoch 9699: Training Loss: 0.10698063174883525 Validation Loss: 0.7430227398872375\n",
      "Epoch 9700: Training Loss: 0.10690273344516754 Validation Loss: 0.7428820729255676\n",
      "Epoch 9701: Training Loss: 0.10680688420931499 Validation Loss: 0.7430278062820435\n",
      "Epoch 9702: Training Loss: 0.10762396703163783 Validation Loss: 0.7432790994644165\n",
      "Epoch 9703: Training Loss: 0.10688431809345882 Validation Loss: 0.7431962490081787\n",
      "Epoch 9704: Training Loss: 0.10788725813229878 Validation Loss: 0.7433865666389465\n",
      "Epoch 9705: Training Loss: 0.10680642475684483 Validation Loss: 0.7434387803077698\n",
      "Epoch 9706: Training Loss: 0.10700948784748714 Validation Loss: 0.7428615093231201\n",
      "Epoch 9707: Training Loss: 0.10687121500571568 Validation Loss: 0.7424046993255615\n",
      "Epoch 9708: Training Loss: 0.10687808692455292 Validation Loss: 0.7422685027122498\n",
      "Epoch 9709: Training Loss: 0.10705102483431499 Validation Loss: 0.7426022291183472\n",
      "Epoch 9710: Training Loss: 0.10694801559050877 Validation Loss: 0.7427937388420105\n",
      "Epoch 9711: Training Loss: 0.10718369235595067 Validation Loss: 0.7433460354804993\n",
      "Epoch 9712: Training Loss: 0.10735079646110535 Validation Loss: 0.7436355352401733\n",
      "Epoch 9713: Training Loss: 0.1080397292971611 Validation Loss: 0.7435439229011536\n",
      "Epoch 9714: Training Loss: 0.10677298655112584 Validation Loss: 0.7435332536697388\n",
      "Epoch 9715: Training Loss: 0.10691877951224645 Validation Loss: 0.7433947920799255\n",
      "Epoch 9716: Training Loss: 0.10651420801877975 Validation Loss: 0.7428810000419617\n",
      "Epoch 9717: Training Loss: 0.10682876656452815 Validation Loss: 0.7426742911338806\n",
      "Epoch 9718: Training Loss: 0.10691250612338384 Validation Loss: 0.7427332997322083\n",
      "Epoch 9719: Training Loss: 0.1073307991027832 Validation Loss: 0.7429484724998474\n",
      "Epoch 9720: Training Loss: 0.10651337603727977 Validation Loss: 0.7433527708053589\n",
      "Epoch 9721: Training Loss: 0.10658449679613113 Validation Loss: 0.7433813810348511\n",
      "Epoch 9722: Training Loss: 0.10651259124279022 Validation Loss: 0.7434701323509216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9723: Training Loss: 0.10716792692740758 Validation Loss: 0.7432677149772644\n",
      "Epoch 9724: Training Loss: 0.10678789267937343 Validation Loss: 0.743245542049408\n",
      "Epoch 9725: Training Loss: 0.1067074586947759 Validation Loss: 0.7429861426353455\n",
      "Epoch 9726: Training Loss: 0.10662759840488434 Validation Loss: 0.7431197166442871\n",
      "Epoch 9727: Training Loss: 0.10670426239569981 Validation Loss: 0.7425169348716736\n",
      "Epoch 9728: Training Loss: 0.10640889902909596 Validation Loss: 0.7424554228782654\n",
      "Epoch 9729: Training Loss: 0.10679661482572556 Validation Loss: 0.742965817451477\n",
      "Epoch 9730: Training Loss: 0.10668816914161046 Validation Loss: 0.7429953813552856\n",
      "Epoch 9731: Training Loss: 0.10672984272241592 Validation Loss: 0.743361234664917\n",
      "Epoch 9732: Training Loss: 0.10651593406995137 Validation Loss: 0.7439751625061035\n",
      "Epoch 9733: Training Loss: 0.10685485849777858 Validation Loss: 0.7439156770706177\n",
      "Epoch 9734: Training Loss: 0.10657027115424474 Validation Loss: 0.7435122728347778\n",
      "Epoch 9735: Training Loss: 0.10694782435894012 Validation Loss: 0.7432120442390442\n",
      "Epoch 9736: Training Loss: 0.10654501616954803 Validation Loss: 0.7432836294174194\n",
      "Epoch 9737: Training Loss: 0.10629179080327351 Validation Loss: 0.7430750727653503\n",
      "Epoch 9738: Training Loss: 0.10669732342163722 Validation Loss: 0.7430734038352966\n",
      "Epoch 9739: Training Loss: 0.10693639516830444 Validation Loss: 0.7434154152870178\n",
      "Epoch 9740: Training Loss: 0.10673309614260991 Validation Loss: 0.74332195520401\n",
      "Epoch 9741: Training Loss: 0.10684057076772054 Validation Loss: 0.743362307548523\n",
      "Epoch 9742: Training Loss: 0.1065228283405304 Validation Loss: 0.7433493137359619\n",
      "Epoch 9743: Training Loss: 0.10679943114519119 Validation Loss: 0.7434095740318298\n",
      "Epoch 9744: Training Loss: 0.10644233723481496 Validation Loss: 0.7435746788978577\n",
      "Epoch 9745: Training Loss: 0.1065243532260259 Validation Loss: 0.7434619665145874\n",
      "Epoch 9746: Training Loss: 0.10658279061317444 Validation Loss: 0.7435357570648193\n",
      "Epoch 9747: Training Loss: 0.10646247615416844 Validation Loss: 0.743474543094635\n",
      "Epoch 9748: Training Loss: 0.10655537247657776 Validation Loss: 0.7429328560829163\n",
      "Epoch 9749: Training Loss: 0.10730177660783131 Validation Loss: 0.7432510852813721\n",
      "Epoch 9750: Training Loss: 0.10647646834452947 Validation Loss: 0.743165910243988\n",
      "Epoch 9751: Training Loss: 0.10649932672580083 Validation Loss: 0.7434975504875183\n",
      "Epoch 9752: Training Loss: 0.10650744785865147 Validation Loss: 0.7437018752098083\n",
      "Epoch 9753: Training Loss: 0.10630277047554652 Validation Loss: 0.7435653805732727\n",
      "Epoch 9754: Training Loss: 0.10650020092725754 Validation Loss: 0.7436408400535583\n",
      "Epoch 9755: Training Loss: 0.10635051627953847 Validation Loss: 0.7431214451789856\n",
      "Epoch 9756: Training Loss: 0.10630409667889278 Validation Loss: 0.743263304233551\n",
      "Epoch 9757: Training Loss: 0.10719687988360722 Validation Loss: 0.743164598941803\n",
      "Epoch 9758: Training Loss: 0.1066990817586581 Validation Loss: 0.7431655526161194\n",
      "Epoch 9759: Training Loss: 0.10638391723235448 Validation Loss: 0.7433949112892151\n",
      "Epoch 9760: Training Loss: 0.10637704531351726 Validation Loss: 0.743312656879425\n",
      "Epoch 9761: Training Loss: 0.10657248646020889 Validation Loss: 0.7432551980018616\n",
      "Epoch 9762: Training Loss: 0.10648114730914433 Validation Loss: 0.7437984943389893\n",
      "Epoch 9763: Training Loss: 0.10622172554334004 Validation Loss: 0.7435768842697144\n",
      "Epoch 9764: Training Loss: 0.10675870378812154 Validation Loss: 0.7433741092681885\n",
      "Epoch 9765: Training Loss: 0.10662096738815308 Validation Loss: 0.74267578125\n",
      "Epoch 9766: Training Loss: 0.1063903992374738 Validation Loss: 0.7428155541419983\n",
      "Epoch 9767: Training Loss: 0.1066183273990949 Validation Loss: 0.7434905171394348\n",
      "Epoch 9768: Training Loss: 0.10643128057320912 Validation Loss: 0.7437471747398376\n",
      "Epoch 9769: Training Loss: 0.10685209929943085 Validation Loss: 0.7442654371261597\n",
      "Epoch 9770: Training Loss: 0.10672953973213832 Validation Loss: 0.7436259388923645\n",
      "Epoch 9771: Training Loss: 0.10674605518579483 Validation Loss: 0.7433104515075684\n",
      "Epoch 9772: Training Loss: 0.10622214029232661 Validation Loss: 0.7433575391769409\n",
      "Epoch 9773: Training Loss: 0.1067286878824234 Validation Loss: 0.7433569431304932\n",
      "Epoch 9774: Training Loss: 0.10629566262165706 Validation Loss: 0.7437225580215454\n",
      "Epoch 9775: Training Loss: 0.10645512243111928 Validation Loss: 0.7439979314804077\n",
      "Epoch 9776: Training Loss: 0.1067146509885788 Validation Loss: 0.744434654712677\n",
      "Epoch 9777: Training Loss: 0.10655490805705388 Validation Loss: 0.7441965341567993\n",
      "Epoch 9778: Training Loss: 0.10658903171618779 Validation Loss: 0.743598461151123\n",
      "Epoch 9779: Training Loss: 0.10622184723615646 Validation Loss: 0.7434017658233643\n",
      "Epoch 9780: Training Loss: 0.10639613370100658 Validation Loss: 0.7436693906784058\n",
      "Epoch 9781: Training Loss: 0.10618014136950175 Validation Loss: 0.743715226650238\n",
      "Epoch 9782: Training Loss: 0.10616171111663182 Validation Loss: 0.7439967393875122\n",
      "Epoch 9783: Training Loss: 0.10634609808524449 Validation Loss: 0.7442829608917236\n",
      "Epoch 9784: Training Loss: 0.10676916191975276 Validation Loss: 0.7441235780715942\n",
      "Epoch 9785: Training Loss: 0.1063200831413269 Validation Loss: 0.7441846132278442\n",
      "Epoch 9786: Training Loss: 0.10719538728396098 Validation Loss: 0.7439720034599304\n",
      "Epoch 9787: Training Loss: 0.1060866763194402 Validation Loss: 0.7437822222709656\n",
      "Epoch 9788: Training Loss: 0.106918103992939 Validation Loss: 0.7435833215713501\n",
      "Epoch 9789: Training Loss: 0.10620921850204468 Validation Loss: 0.743944525718689\n",
      "Epoch 9790: Training Loss: 0.10625546177228291 Validation Loss: 0.7439561486244202\n",
      "Epoch 9791: Training Loss: 0.10738053917884827 Validation Loss: 0.7441510558128357\n",
      "Epoch 9792: Training Loss: 0.10580505679051082 Validation Loss: 0.7442237734794617\n",
      "Epoch 9793: Training Loss: 0.10652921100457509 Validation Loss: 0.7444811463356018\n",
      "Epoch 9794: Training Loss: 0.10650824507077535 Validation Loss: 0.7441825866699219\n",
      "Epoch 9795: Training Loss: 0.10592868427435558 Validation Loss: 0.7438672780990601\n",
      "Epoch 9796: Training Loss: 0.10603222002585729 Validation Loss: 0.7435811758041382\n",
      "Epoch 9797: Training Loss: 0.10650530954202016 Validation Loss: 0.7436667084693909\n",
      "Epoch 9798: Training Loss: 0.1061892956495285 Validation Loss: 0.7439377903938293\n",
      "Epoch 9799: Training Loss: 0.10606058190266292 Validation Loss: 0.744314968585968\n",
      "Epoch 9800: Training Loss: 0.10634403924147288 Validation Loss: 0.7441140413284302\n",
      "Epoch 9801: Training Loss: 0.1061343898375829 Validation Loss: 0.7441582679748535\n",
      "Epoch 9802: Training Loss: 0.10690800845623016 Validation Loss: 0.7437409162521362\n",
      "Epoch 9803: Training Loss: 0.1067382941643397 Validation Loss: 0.7433585524559021\n",
      "Epoch 9804: Training Loss: 0.10670392711957295 Validation Loss: 0.7434450387954712\n",
      "Epoch 9805: Training Loss: 0.1061424861351649 Validation Loss: 0.7441391348838806\n",
      "Epoch 9806: Training Loss: 0.1062362864613533 Validation Loss: 0.7443370223045349\n",
      "Epoch 9807: Training Loss: 0.10631963610649109 Validation Loss: 0.7442549467086792\n",
      "Epoch 9808: Training Loss: 0.10628787924846013 Validation Loss: 0.7440318465232849\n",
      "Epoch 9809: Training Loss: 0.10635845859845479 Validation Loss: 0.7438854575157166\n",
      "Epoch 9810: Training Loss: 0.106085071961085 Validation Loss: 0.7439628839492798\n",
      "Epoch 9811: Training Loss: 0.10719398409128189 Validation Loss: 0.7442124485969543\n",
      "Epoch 9812: Training Loss: 0.10645888994137447 Validation Loss: 0.7444162964820862\n",
      "Epoch 9813: Training Loss: 0.10599921395381291 Validation Loss: 0.7445489168167114\n",
      "Epoch 9814: Training Loss: 0.10679136713345845 Validation Loss: 0.7444005608558655\n",
      "Epoch 9815: Training Loss: 0.10733044644196828 Validation Loss: 0.7444621324539185\n",
      "Epoch 9816: Training Loss: 0.10617792854706447 Validation Loss: 0.7442663311958313\n",
      "Epoch 9817: Training Loss: 0.10612272222836812 Validation Loss: 0.7440698742866516\n",
      "Epoch 9818: Training Loss: 0.10594586531321208 Validation Loss: 0.7442315816879272\n",
      "Epoch 9819: Training Loss: 0.10616840918858846 Validation Loss: 0.7440852522850037\n",
      "Epoch 9820: Training Loss: 0.1065260445078214 Validation Loss: 0.7437137365341187\n",
      "Epoch 9821: Training Loss: 0.10607427606980006 Validation Loss: 0.7440146207809448\n",
      "Epoch 9822: Training Loss: 0.10558051119248073 Validation Loss: 0.7440779805183411\n",
      "Epoch 9823: Training Loss: 0.10600518683592479 Validation Loss: 0.7441739439964294\n",
      "Epoch 9824: Training Loss: 0.10573591788609822 Validation Loss: 0.744144856929779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9825: Training Loss: 0.10593901822964351 Validation Loss: 0.7440328598022461\n",
      "Epoch 9826: Training Loss: 0.10590835909048717 Validation Loss: 0.7440444827079773\n",
      "Epoch 9827: Training Loss: 0.10628220687309901 Validation Loss: 0.7440967559814453\n",
      "Epoch 9828: Training Loss: 0.10593668868144353 Validation Loss: 0.7441504001617432\n",
      "Epoch 9829: Training Loss: 0.10594393312931061 Validation Loss: 0.7443132400512695\n",
      "Epoch 9830: Training Loss: 0.10605357338984807 Validation Loss: 0.7440126538276672\n",
      "Epoch 9831: Training Loss: 0.10602899889151256 Validation Loss: 0.744171142578125\n",
      "Epoch 9832: Training Loss: 0.1059071496129036 Validation Loss: 0.7441713213920593\n",
      "Epoch 9833: Training Loss: 0.10593145837386449 Validation Loss: 0.7443621158599854\n",
      "Epoch 9834: Training Loss: 0.10587362696727116 Validation Loss: 0.7438958883285522\n",
      "Epoch 9835: Training Loss: 0.10620008905728658 Validation Loss: 0.7441344261169434\n",
      "Epoch 9836: Training Loss: 0.1055964504679044 Validation Loss: 0.7439565658569336\n",
      "Epoch 9837: Training Loss: 0.10610470672448476 Validation Loss: 0.7442522644996643\n",
      "Epoch 9838: Training Loss: 0.10587696482737859 Validation Loss: 0.7441675662994385\n",
      "Epoch 9839: Training Loss: 0.10585312048594157 Validation Loss: 0.7442038655281067\n",
      "Epoch 9840: Training Loss: 0.10616512844959895 Validation Loss: 0.7444007992744446\n",
      "Epoch 9841: Training Loss: 0.1057146539290746 Validation Loss: 0.7441727519035339\n",
      "Epoch 9842: Training Loss: 0.10626906404892604 Validation Loss: 0.7443153858184814\n",
      "Epoch 9843: Training Loss: 0.10553012291590373 Validation Loss: 0.7441988587379456\n",
      "Epoch 9844: Training Loss: 0.1058315138022105 Validation Loss: 0.7442020177841187\n",
      "Epoch 9845: Training Loss: 0.10579744478066762 Validation Loss: 0.7446042895317078\n",
      "Epoch 9846: Training Loss: 0.1068647528688113 Validation Loss: 0.7442552447319031\n",
      "Epoch 9847: Training Loss: 0.10622087866067886 Validation Loss: 0.7444022297859192\n",
      "Epoch 9848: Training Loss: 0.10597523798545201 Validation Loss: 0.744438648223877\n",
      "Epoch 9849: Training Loss: 0.10628620286782582 Validation Loss: 0.7444769740104675\n",
      "Epoch 9850: Training Loss: 0.10653680562973022 Validation Loss: 0.7442353963851929\n",
      "Epoch 9851: Training Loss: 0.10595359653234482 Validation Loss: 0.7440401315689087\n",
      "Epoch 9852: Training Loss: 0.1057531734307607 Validation Loss: 0.7445623874664307\n",
      "Epoch 9853: Training Loss: 0.10599348942438762 Validation Loss: 0.7448739409446716\n",
      "Epoch 9854: Training Loss: 0.10588863492012024 Validation Loss: 0.7450304627418518\n",
      "Epoch 9855: Training Loss: 0.10581787923971812 Validation Loss: 0.7449067234992981\n",
      "Epoch 9856: Training Loss: 0.10561439394950867 Validation Loss: 0.7446452379226685\n",
      "Epoch 9857: Training Loss: 0.10604996979236603 Validation Loss: 0.744036853313446\n",
      "Epoch 9858: Training Loss: 0.10590318590402603 Validation Loss: 0.7443972826004028\n",
      "Epoch 9859: Training Loss: 0.10593481610218684 Validation Loss: 0.7447304129600525\n",
      "Epoch 9860: Training Loss: 0.10552042971054713 Validation Loss: 0.74449223279953\n",
      "Epoch 9861: Training Loss: 0.1067225510875384 Validation Loss: 0.7448629140853882\n",
      "Epoch 9862: Training Loss: 0.10591970632473628 Validation Loss: 0.7448756098747253\n",
      "Epoch 9863: Training Loss: 0.10547688603401184 Validation Loss: 0.7446833252906799\n",
      "Epoch 9864: Training Loss: 0.10565229505300522 Validation Loss: 0.7448390126228333\n",
      "Epoch 9865: Training Loss: 0.10551178207000096 Validation Loss: 0.7443351149559021\n",
      "Epoch 9866: Training Loss: 0.10574941337108612 Validation Loss: 0.7442158460617065\n",
      "Epoch 9867: Training Loss: 0.10586975266536076 Validation Loss: 0.7445749044418335\n",
      "Epoch 9868: Training Loss: 0.10572740683952968 Validation Loss: 0.7450602650642395\n",
      "Epoch 9869: Training Loss: 0.1054255614678065 Validation Loss: 0.7444210052490234\n",
      "Epoch 9870: Training Loss: 0.10509878645340602 Validation Loss: 0.7446554899215698\n",
      "Epoch 9871: Training Loss: 0.10561755796273549 Validation Loss: 0.7447479367256165\n",
      "Epoch 9872: Training Loss: 0.10562536120414734 Validation Loss: 0.7444968223571777\n",
      "Epoch 9873: Training Loss: 0.10564365983009338 Validation Loss: 0.7443303465843201\n",
      "Epoch 9874: Training Loss: 0.10579519470532735 Validation Loss: 0.7444776892662048\n",
      "Epoch 9875: Training Loss: 0.10588976244131725 Validation Loss: 0.7444111704826355\n",
      "Epoch 9876: Training Loss: 0.10557259370883305 Validation Loss: 0.7445136904716492\n",
      "Epoch 9877: Training Loss: 0.10497868557771046 Validation Loss: 0.74471116065979\n",
      "Epoch 9878: Training Loss: 0.10561999678611755 Validation Loss: 0.7446946501731873\n",
      "Epoch 9879: Training Loss: 0.10527019202709198 Validation Loss: 0.744512677192688\n",
      "Epoch 9880: Training Loss: 0.10572085281213124 Validation Loss: 0.7446475625038147\n",
      "Epoch 9881: Training Loss: 0.10562325517336528 Validation Loss: 0.7446544766426086\n",
      "Epoch 9882: Training Loss: 0.10552921642859776 Validation Loss: 0.7450272440910339\n",
      "Epoch 9883: Training Loss: 0.10561147580544154 Validation Loss: 0.7448151707649231\n",
      "Epoch 9884: Training Loss: 0.10684654861688614 Validation Loss: 0.7449570298194885\n",
      "Epoch 9885: Training Loss: 0.10579051822423935 Validation Loss: 0.745516836643219\n",
      "Epoch 9886: Training Loss: 0.1057019904255867 Validation Loss: 0.7454112768173218\n",
      "Epoch 9887: Training Loss: 0.10543568929036458 Validation Loss: 0.7447794675827026\n",
      "Epoch 9888: Training Loss: 0.10477223247289658 Validation Loss: 0.7445611357688904\n",
      "Epoch 9889: Training Loss: 0.10577307393153508 Validation Loss: 0.7444802522659302\n",
      "Epoch 9890: Training Loss: 0.10578166196743648 Validation Loss: 0.7442658543586731\n",
      "Epoch 9891: Training Loss: 0.10526976486047109 Validation Loss: 0.7448731660842896\n",
      "Epoch 9892: Training Loss: 0.10529317706823349 Validation Loss: 0.7450678944587708\n",
      "Epoch 9893: Training Loss: 0.1055618127187093 Validation Loss: 0.745597243309021\n",
      "Epoch 9894: Training Loss: 0.1055151770512263 Validation Loss: 0.7457112669944763\n",
      "Epoch 9895: Training Loss: 0.10532590001821518 Validation Loss: 0.7452791929244995\n",
      "Epoch 9896: Training Loss: 0.10551631202300389 Validation Loss: 0.7444503307342529\n",
      "Epoch 9897: Training Loss: 0.10530244559049606 Validation Loss: 0.7443736791610718\n",
      "Epoch 9898: Training Loss: 0.10544574757417043 Validation Loss: 0.74432772397995\n",
      "Epoch 9899: Training Loss: 0.10545603930950165 Validation Loss: 0.7449998259544373\n",
      "Epoch 9900: Training Loss: 0.10530244559049606 Validation Loss: 0.7449635863304138\n",
      "Epoch 9901: Training Loss: 0.1053594450155894 Validation Loss: 0.7454151511192322\n",
      "Epoch 9902: Training Loss: 0.10526230682929356 Validation Loss: 0.7453252673149109\n",
      "Epoch 9903: Training Loss: 0.10546635339657466 Validation Loss: 0.7449987530708313\n",
      "Epoch 9904: Training Loss: 0.10538378854592641 Validation Loss: 0.7448582053184509\n",
      "Epoch 9905: Training Loss: 0.10564007113377254 Validation Loss: 0.7446567416191101\n",
      "Epoch 9906: Training Loss: 0.10532169540723164 Validation Loss: 0.7446464896202087\n",
      "Epoch 9907: Training Loss: 0.10527828335762024 Validation Loss: 0.7451155781745911\n",
      "Epoch 9908: Training Loss: 0.10513792932033539 Validation Loss: 0.7453709244728088\n",
      "Epoch 9909: Training Loss: 0.10543534408013026 Validation Loss: 0.7459107637405396\n",
      "Epoch 9910: Training Loss: 0.10550009955962499 Validation Loss: 0.7459907531738281\n",
      "Epoch 9911: Training Loss: 0.10606979827086131 Validation Loss: 0.7457758188247681\n",
      "Epoch 9912: Training Loss: 0.10613287488619487 Validation Loss: 0.7451354265213013\n",
      "Epoch 9913: Training Loss: 0.10514078040917714 Validation Loss: 0.7450845837593079\n",
      "Epoch 9914: Training Loss: 0.10535348455111186 Validation Loss: 0.7447201609611511\n",
      "Epoch 9915: Training Loss: 0.10548517356316249 Validation Loss: 0.7446681261062622\n",
      "Epoch 9916: Training Loss: 0.10581208268801372 Validation Loss: 0.744788408279419\n",
      "Epoch 9917: Training Loss: 0.10579049090544383 Validation Loss: 0.7449471950531006\n",
      "Epoch 9918: Training Loss: 0.10521401713291804 Validation Loss: 0.7447542548179626\n",
      "Epoch 9919: Training Loss: 0.1052578662832578 Validation Loss: 0.7451159358024597\n",
      "Epoch 9920: Training Loss: 0.10537048677603404 Validation Loss: 0.7458639144897461\n",
      "Epoch 9921: Training Loss: 0.10534870624542236 Validation Loss: 0.7456468343734741\n",
      "Epoch 9922: Training Loss: 0.10572146872679393 Validation Loss: 0.7453566789627075\n",
      "Epoch 9923: Training Loss: 0.10526871184508006 Validation Loss: 0.745196521282196\n",
      "Epoch 9924: Training Loss: 0.10518683741490047 Validation Loss: 0.7451039552688599\n",
      "Epoch 9925: Training Loss: 0.10558546086152394 Validation Loss: 0.7453538775444031\n",
      "Epoch 9926: Training Loss: 0.10522552827994029 Validation Loss: 0.7459050416946411\n",
      "Epoch 9927: Training Loss: 0.10484611491362254 Validation Loss: 0.7462437152862549\n",
      "Epoch 9928: Training Loss: 0.10547112921873729 Validation Loss: 0.7460870742797852\n",
      "Epoch 9929: Training Loss: 0.10525069137414296 Validation Loss: 0.7458226680755615\n",
      "Epoch 9930: Training Loss: 0.10504983613888423 Validation Loss: 0.7454388737678528\n",
      "Epoch 9931: Training Loss: 0.10536350806554158 Validation Loss: 0.7451077699661255\n",
      "Epoch 9932: Training Loss: 0.10536915312210719 Validation Loss: 0.7451226115226746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9933: Training Loss: 0.10518552611271541 Validation Loss: 0.744813859462738\n",
      "Epoch 9934: Training Loss: 0.10539391885201137 Validation Loss: 0.7446977496147156\n",
      "Epoch 9935: Training Loss: 0.10485888769229253 Validation Loss: 0.7450625896453857\n",
      "Epoch 9936: Training Loss: 0.10585089772939682 Validation Loss: 0.7454167008399963\n",
      "Epoch 9937: Training Loss: 0.10501664131879807 Validation Loss: 0.7451974153518677\n",
      "Epoch 9938: Training Loss: 0.10516385734081268 Validation Loss: 0.745126485824585\n",
      "Epoch 9939: Training Loss: 0.10497020681699117 Validation Loss: 0.7450913190841675\n",
      "Epoch 9940: Training Loss: 0.10682165871063869 Validation Loss: 0.7450113892555237\n",
      "Epoch 9941: Training Loss: 0.10500060766935349 Validation Loss: 0.7448479533195496\n",
      "Epoch 9942: Training Loss: 0.10521079848210017 Validation Loss: 0.7449249625205994\n",
      "Epoch 9943: Training Loss: 0.10548115273316701 Validation Loss: 0.7453318238258362\n",
      "Epoch 9944: Training Loss: 0.10513983170191447 Validation Loss: 0.7458443641662598\n",
      "Epoch 9945: Training Loss: 0.10525366912285487 Validation Loss: 0.7460650205612183\n",
      "Epoch 9946: Training Loss: 0.10561531285444896 Validation Loss: 0.7460976243019104\n",
      "Epoch 9947: Training Loss: 0.10545169065395991 Validation Loss: 0.7464411854743958\n",
      "Epoch 9948: Training Loss: 0.10538236051797867 Validation Loss: 0.7460185289382935\n",
      "Epoch 9949: Training Loss: 0.10496375958124797 Validation Loss: 0.7457749247550964\n",
      "Epoch 9950: Training Loss: 0.10577962795893352 Validation Loss: 0.7455865740776062\n",
      "Epoch 9951: Training Loss: 0.10497498760620753 Validation Loss: 0.7453550696372986\n",
      "Epoch 9952: Training Loss: 0.10499636580546697 Validation Loss: 0.7456233501434326\n",
      "Epoch 9953: Training Loss: 0.10490553081035614 Validation Loss: 0.7459353804588318\n",
      "Epoch 9954: Training Loss: 0.10581635683774948 Validation Loss: 0.7456107139587402\n",
      "Epoch 9955: Training Loss: 0.10543930778900783 Validation Loss: 0.7453469038009644\n",
      "Epoch 9956: Training Loss: 0.10486769427855809 Validation Loss: 0.7452906370162964\n",
      "Epoch 9957: Training Loss: 0.10510973632335663 Validation Loss: 0.7458934187889099\n",
      "Epoch 9958: Training Loss: 0.10490062832832336 Validation Loss: 0.7459093332290649\n",
      "Epoch 9959: Training Loss: 0.10495989521344502 Validation Loss: 0.745533287525177\n",
      "Epoch 9960: Training Loss: 0.10497954984505971 Validation Loss: 0.7452685832977295\n",
      "Epoch 9961: Training Loss: 0.10516328116257985 Validation Loss: 0.7449329495429993\n",
      "Epoch 9962: Training Loss: 0.10471247384945552 Validation Loss: 0.7452950477600098\n",
      "Epoch 9963: Training Loss: 0.10534431785345078 Validation Loss: 0.7458509206771851\n",
      "Epoch 9964: Training Loss: 0.10488664358854294 Validation Loss: 0.745908260345459\n",
      "Epoch 9965: Training Loss: 0.10487642387549083 Validation Loss: 0.7458181381225586\n",
      "Epoch 9966: Training Loss: 0.1054348573088646 Validation Loss: 0.7455683946609497\n",
      "Epoch 9967: Training Loss: 0.10507487754027049 Validation Loss: 0.7451026439666748\n",
      "Epoch 9968: Training Loss: 0.10438343634208043 Validation Loss: 0.7453147768974304\n",
      "Epoch 9969: Training Loss: 0.10510758807261784 Validation Loss: 0.7456024289131165\n",
      "Epoch 9970: Training Loss: 0.10502712428569794 Validation Loss: 0.745734691619873\n",
      "Epoch 9971: Training Loss: 0.10462167362372081 Validation Loss: 0.7454273104667664\n",
      "Epoch 9972: Training Loss: 0.10493666678667068 Validation Loss: 0.7455263733863831\n",
      "Epoch 9973: Training Loss: 0.10512893398602803 Validation Loss: 0.7452917098999023\n",
      "Epoch 9974: Training Loss: 0.10487145682175954 Validation Loss: 0.7455968260765076\n",
      "Epoch 9975: Training Loss: 0.10497154295444489 Validation Loss: 0.7455072402954102\n",
      "Epoch 9976: Training Loss: 0.10583315044641495 Validation Loss: 0.745799720287323\n",
      "Epoch 9977: Training Loss: 0.10610152035951614 Validation Loss: 0.7458266615867615\n",
      "Epoch 9978: Training Loss: 0.1047878513733546 Validation Loss: 0.7457322478294373\n",
      "Epoch 9979: Training Loss: 0.1048167273402214 Validation Loss: 0.7461333870887756\n",
      "Epoch 9980: Training Loss: 0.10464571168025334 Validation Loss: 0.7460364103317261\n",
      "Epoch 9981: Training Loss: 0.10523810982704163 Validation Loss: 0.746103823184967\n",
      "Epoch 9982: Training Loss: 0.10485479484001796 Validation Loss: 0.7459223866462708\n",
      "Epoch 9983: Training Loss: 0.10485871881246567 Validation Loss: 0.7457024455070496\n",
      "Epoch 9984: Training Loss: 0.10498585800329845 Validation Loss: 0.7461468577384949\n",
      "Epoch 9985: Training Loss: 0.10514034082492192 Validation Loss: 0.7463185787200928\n",
      "Epoch 9986: Training Loss: 0.10468386361996333 Validation Loss: 0.7465036511421204\n",
      "Epoch 9987: Training Loss: 0.10476077347993851 Validation Loss: 0.7457963824272156\n",
      "Epoch 9988: Training Loss: 0.10484229773283005 Validation Loss: 0.7456071376800537\n",
      "Epoch 9989: Training Loss: 0.10504011313120525 Validation Loss: 0.7453881502151489\n",
      "Epoch 9990: Training Loss: 0.10475247104962666 Validation Loss: 0.7455812692642212\n",
      "Epoch 9991: Training Loss: 0.1047198474407196 Validation Loss: 0.745710015296936\n",
      "Epoch 9992: Training Loss: 0.1047266274690628 Validation Loss: 0.7461647391319275\n",
      "Epoch 9993: Training Loss: 0.10474250713984172 Validation Loss: 0.7459156513214111\n",
      "Epoch 9994: Training Loss: 0.10510979344447453 Validation Loss: 0.745793879032135\n",
      "Epoch 9995: Training Loss: 0.10471965869267781 Validation Loss: 0.7456918954849243\n",
      "Epoch 9996: Training Loss: 0.10523697982231776 Validation Loss: 0.7459347248077393\n",
      "Epoch 9997: Training Loss: 0.10485143462816875 Validation Loss: 0.7461958527565002\n",
      "Epoch 9998: Training Loss: 0.10447099804878235 Validation Loss: 0.7461023926734924\n",
      "Epoch 9999: Training Loss: 0.10472092032432556 Validation Loss: 0.7459985017776489\n",
      "Epoch 10000: Training Loss: 0.10456722478071849 Validation Loss: 0.7455059289932251\n",
      "Epoch 10001: Training Loss: 0.10445553809404373 Validation Loss: 0.7454167008399963\n",
      "Epoch 10002: Training Loss: 0.10512408365805943 Validation Loss: 0.7452488541603088\n",
      "Epoch 10003: Training Loss: 0.10485538343588512 Validation Loss: 0.7457886934280396\n",
      "Epoch 10004: Training Loss: 0.10523827373981476 Validation Loss: 0.7459417581558228\n",
      "Epoch 10005: Training Loss: 0.10474637895822525 Validation Loss: 0.7464231848716736\n",
      "Epoch 10006: Training Loss: 0.10452201465765636 Validation Loss: 0.7464399337768555\n",
      "Epoch 10007: Training Loss: 0.1042601615190506 Validation Loss: 0.7464476823806763\n",
      "Epoch 10008: Training Loss: 0.10529489815235138 Validation Loss: 0.7461372017860413\n",
      "Epoch 10009: Training Loss: 0.10549715906381607 Validation Loss: 0.7457893490791321\n",
      "Epoch 10010: Training Loss: 0.10462188969055812 Validation Loss: 0.745823085308075\n",
      "Epoch 10011: Training Loss: 0.10470923533042271 Validation Loss: 0.7462352514266968\n",
      "Epoch 10012: Training Loss: 0.10457549492518108 Validation Loss: 0.7462218999862671\n",
      "Epoch 10013: Training Loss: 0.10529294610023499 Validation Loss: 0.7462962865829468\n",
      "Epoch 10014: Training Loss: 0.10538688053687413 Validation Loss: 0.7457649111747742\n",
      "Epoch 10015: Training Loss: 0.10467993219693501 Validation Loss: 0.7454422116279602\n",
      "Epoch 10016: Training Loss: 0.10462694118420283 Validation Loss: 0.7458655834197998\n",
      "Epoch 10017: Training Loss: 0.10460584610700607 Validation Loss: 0.7461069822311401\n",
      "Epoch 10018: Training Loss: 0.10517496367295583 Validation Loss: 0.7462173700332642\n",
      "Epoch 10019: Training Loss: 0.10448152323563893 Validation Loss: 0.7462238669395447\n",
      "Epoch 10020: Training Loss: 0.10494733601808548 Validation Loss: 0.746033787727356\n",
      "Epoch 10021: Training Loss: 0.10445166379213333 Validation Loss: 0.7463079690933228\n",
      "Epoch 10022: Training Loss: 0.10398179789384206 Validation Loss: 0.7461113929748535\n",
      "Epoch 10023: Training Loss: 0.10481064766645432 Validation Loss: 0.7461267709732056\n",
      "Epoch 10024: Training Loss: 0.1043353999654452 Validation Loss: 0.7460295557975769\n",
      "Epoch 10025: Training Loss: 0.10458151996135712 Validation Loss: 0.7456774115562439\n",
      "Epoch 10026: Training Loss: 0.10429031650225322 Validation Loss: 0.7457910776138306\n",
      "Epoch 10027: Training Loss: 0.1044153372446696 Validation Loss: 0.7465395927429199\n",
      "Epoch 10028: Training Loss: 0.10421362022558849 Validation Loss: 0.7468270659446716\n",
      "Epoch 10029: Training Loss: 0.10448875029881795 Validation Loss: 0.746787428855896\n",
      "Epoch 10030: Training Loss: 0.1051704262693723 Validation Loss: 0.7464542388916016\n",
      "Epoch 10031: Training Loss: 0.10491496324539185 Validation Loss: 0.7464427351951599\n",
      "Epoch 10032: Training Loss: 0.1050134723385175 Validation Loss: 0.7462959885597229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10033: Training Loss: 0.10426146537065506 Validation Loss: 0.7458765506744385\n",
      "Epoch 10034: Training Loss: 0.10507036000490189 Validation Loss: 0.7460610866546631\n",
      "Epoch 10035: Training Loss: 0.10428829739491145 Validation Loss: 0.7461936473846436\n",
      "Epoch 10036: Training Loss: 0.1044095406929652 Validation Loss: 0.7463451623916626\n",
      "Epoch 10037: Training Loss: 0.10415622840325038 Validation Loss: 0.745988667011261\n",
      "Epoch 10038: Training Loss: 0.10435609519481659 Validation Loss: 0.7457103133201599\n",
      "Epoch 10039: Training Loss: 0.10433605810006459 Validation Loss: 0.7457818388938904\n",
      "Epoch 10040: Training Loss: 0.10457728306452434 Validation Loss: 0.7463111281394958\n",
      "Epoch 10041: Training Loss: 0.10498366008202235 Validation Loss: 0.7466176748275757\n",
      "Epoch 10042: Training Loss: 0.10435022662083308 Validation Loss: 0.7461093068122864\n",
      "Epoch 10043: Training Loss: 0.10470797369877498 Validation Loss: 0.7460561394691467\n",
      "Epoch 10044: Training Loss: 0.10427340120077133 Validation Loss: 0.7459052801132202\n",
      "Epoch 10045: Training Loss: 0.10403992980718613 Validation Loss: 0.7463036179542542\n",
      "Epoch 10046: Training Loss: 0.10477140297492345 Validation Loss: 0.7468345165252686\n",
      "Epoch 10047: Training Loss: 0.10437556107838948 Validation Loss: 0.7470284104347229\n",
      "Epoch 10048: Training Loss: 0.10470593223969142 Validation Loss: 0.7467833161354065\n",
      "Epoch 10049: Training Loss: 0.10463773210843404 Validation Loss: 0.7465617656707764\n",
      "Epoch 10050: Training Loss: 0.10423890749613444 Validation Loss: 0.7460628151893616\n",
      "Epoch 10051: Training Loss: 0.10420559346675873 Validation Loss: 0.7462151050567627\n",
      "Epoch 10052: Training Loss: 0.10416060437758763 Validation Loss: 0.7458900213241577\n",
      "Epoch 10053: Training Loss: 0.104241910080115 Validation Loss: 0.7464808225631714\n",
      "Epoch 10054: Training Loss: 0.10406723370154698 Validation Loss: 0.7466718554496765\n",
      "Epoch 10055: Training Loss: 0.10443767656882604 Validation Loss: 0.7467853426933289\n",
      "Epoch 10056: Training Loss: 0.10492527733246486 Validation Loss: 0.7463609576225281\n",
      "Epoch 10057: Training Loss: 0.10418469210465749 Validation Loss: 0.7464281916618347\n",
      "Epoch 10058: Training Loss: 0.10435406615336736 Validation Loss: 0.7463052272796631\n",
      "Epoch 10059: Training Loss: 0.10399937878052394 Validation Loss: 0.7463968992233276\n",
      "Epoch 10060: Training Loss: 0.10417315860589345 Validation Loss: 0.7465092539787292\n",
      "Epoch 10061: Training Loss: 0.10469610492388408 Validation Loss: 0.7465108633041382\n",
      "Epoch 10062: Training Loss: 0.10387980689605077 Validation Loss: 0.746380627155304\n",
      "Epoch 10063: Training Loss: 0.10428764174381892 Validation Loss: 0.7466627359390259\n",
      "Epoch 10064: Training Loss: 0.10428059597810109 Validation Loss: 0.7464980483055115\n",
      "Epoch 10065: Training Loss: 0.10411459455887477 Validation Loss: 0.7471768260002136\n",
      "Epoch 10066: Training Loss: 0.10398006439208984 Validation Loss: 0.7469688653945923\n",
      "Epoch 10067: Training Loss: 0.10444970428943634 Validation Loss: 0.7468820810317993\n",
      "Epoch 10068: Training Loss: 0.10414059956868489 Validation Loss: 0.7466310262680054\n",
      "Epoch 10069: Training Loss: 0.10426064829031627 Validation Loss: 0.7466000318527222\n",
      "Epoch 10070: Training Loss: 0.10398248831431071 Validation Loss: 0.7464812397956848\n",
      "Epoch 10071: Training Loss: 0.10390296081701915 Validation Loss: 0.7463592290878296\n",
      "Epoch 10072: Training Loss: 0.10399166246255238 Validation Loss: 0.7466157674789429\n",
      "Epoch 10073: Training Loss: 0.10487092783053716 Validation Loss: 0.7469145655632019\n",
      "Epoch 10074: Training Loss: 0.10397214194138844 Validation Loss: 0.7466462254524231\n",
      "Epoch 10075: Training Loss: 0.10408249497413635 Validation Loss: 0.7464415431022644\n",
      "Epoch 10076: Training Loss: 0.10434956848621368 Validation Loss: 0.7462975382804871\n",
      "Epoch 10077: Training Loss: 0.10406790673732758 Validation Loss: 0.7465733289718628\n",
      "Epoch 10078: Training Loss: 0.10406416406234105 Validation Loss: 0.7466077208518982\n",
      "Epoch 10079: Training Loss: 0.10419047127167384 Validation Loss: 0.746656596660614\n",
      "Epoch 10080: Training Loss: 0.10401404400666554 Validation Loss: 0.7466946840286255\n",
      "Epoch 10081: Training Loss: 0.10392814626296361 Validation Loss: 0.7469607591629028\n",
      "Epoch 10082: Training Loss: 0.10398130615552266 Validation Loss: 0.7470192313194275\n",
      "Epoch 10083: Training Loss: 0.10395590960979462 Validation Loss: 0.7469714879989624\n",
      "Epoch 10084: Training Loss: 0.10424844423929851 Validation Loss: 0.7467799782752991\n",
      "Epoch 10085: Training Loss: 0.1038019632299741 Validation Loss: 0.7466359734535217\n",
      "Epoch 10086: Training Loss: 0.10371877998113632 Validation Loss: 0.7465785145759583\n",
      "Epoch 10087: Training Loss: 0.10413879156112671 Validation Loss: 0.7469398975372314\n",
      "Epoch 10088: Training Loss: 0.10506180922190349 Validation Loss: 0.7468598484992981\n",
      "Epoch 10089: Training Loss: 0.10391770551602046 Validation Loss: 0.7466018199920654\n",
      "Epoch 10090: Training Loss: 0.10390835255384445 Validation Loss: 0.7468146681785583\n",
      "Epoch 10091: Training Loss: 0.10397625714540482 Validation Loss: 0.7468720078468323\n",
      "Epoch 10092: Training Loss: 0.10403021425008774 Validation Loss: 0.746696949005127\n",
      "Epoch 10093: Training Loss: 0.10379548122485478 Validation Loss: 0.7471276521682739\n",
      "Epoch 10094: Training Loss: 0.10420438398917516 Validation Loss: 0.746957004070282\n",
      "Epoch 10095: Training Loss: 0.10384447624286015 Validation Loss: 0.7469873428344727\n",
      "Epoch 10096: Training Loss: 0.10363306353489558 Validation Loss: 0.746604859828949\n",
      "Epoch 10097: Training Loss: 0.10445449501276016 Validation Loss: 0.7465457320213318\n",
      "Epoch 10098: Training Loss: 0.10420652230580647 Validation Loss: 0.7465904355049133\n",
      "Epoch 10099: Training Loss: 0.10430525988340378 Validation Loss: 0.7471374869346619\n",
      "Epoch 10100: Training Loss: 0.10401894400517146 Validation Loss: 0.7470954060554504\n",
      "Epoch 10101: Training Loss: 0.10395160565773646 Validation Loss: 0.7469207048416138\n",
      "Epoch 10102: Training Loss: 0.10362778355677922 Validation Loss: 0.7471545934677124\n",
      "Epoch 10103: Training Loss: 0.10480877260367076 Validation Loss: 0.7466800212860107\n",
      "Epoch 10104: Training Loss: 0.10454024374485016 Validation Loss: 0.7466701865196228\n",
      "Epoch 10105: Training Loss: 0.1044733723004659 Validation Loss: 0.7473695278167725\n",
      "Epoch 10106: Training Loss: 0.10456818342208862 Validation Loss: 0.7473754286766052\n",
      "Epoch 10107: Training Loss: 0.10392860323190689 Validation Loss: 0.746958315372467\n",
      "Epoch 10108: Training Loss: 0.1040153627594312 Validation Loss: 0.7471660375595093\n",
      "Epoch 10109: Training Loss: 0.10397509982188542 Validation Loss: 0.7471182942390442\n",
      "Epoch 10110: Training Loss: 0.10390745600064595 Validation Loss: 0.7471774816513062\n",
      "Epoch 10111: Training Loss: 0.10431613028049469 Validation Loss: 0.7472004294395447\n",
      "Epoch 10112: Training Loss: 0.10370934009552002 Validation Loss: 0.7472492456436157\n",
      "Epoch 10113: Training Loss: 0.10408912350734074 Validation Loss: 0.7472660541534424\n",
      "Epoch 10114: Training Loss: 0.1039718786875407 Validation Loss: 0.7475686073303223\n",
      "Epoch 10115: Training Loss: 0.10373289883136749 Validation Loss: 0.7474400997161865\n",
      "Epoch 10116: Training Loss: 0.10565417259931564 Validation Loss: 0.7471951246261597\n",
      "Epoch 10117: Training Loss: 0.1038930540283521 Validation Loss: 0.747251033782959\n",
      "Epoch 10118: Training Loss: 0.10381955901781718 Validation Loss: 0.7473157048225403\n",
      "Epoch 10119: Training Loss: 0.10330210874478023 Validation Loss: 0.7474808692932129\n",
      "Epoch 10120: Training Loss: 0.10391498853762944 Validation Loss: 0.7475849986076355\n",
      "Epoch 10121: Training Loss: 0.10414644330739975 Validation Loss: 0.7474995255470276\n",
      "Epoch 10122: Training Loss: 0.10378131767114003 Validation Loss: 0.7475088238716125\n",
      "Epoch 10123: Training Loss: 0.1039384976029396 Validation Loss: 0.7472144365310669\n",
      "Epoch 10124: Training Loss: 0.10385110974311829 Validation Loss: 0.7472587823867798\n",
      "Epoch 10125: Training Loss: 0.10415194431940715 Validation Loss: 0.7475261092185974\n",
      "Epoch 10126: Training Loss: 0.10346090545256932 Validation Loss: 0.7471717000007629\n",
      "Epoch 10127: Training Loss: 0.10382763296365738 Validation Loss: 0.7471758127212524\n",
      "Epoch 10128: Training Loss: 0.1038668950398763 Validation Loss: 0.7466104030609131\n",
      "Epoch 10129: Training Loss: 0.10391754905382793 Validation Loss: 0.746886134147644\n",
      "Epoch 10130: Training Loss: 0.10347209870815277 Validation Loss: 0.7470294833183289\n",
      "Epoch 10131: Training Loss: 0.10335831095774968 Validation Loss: 0.7469170093536377\n",
      "Epoch 10132: Training Loss: 0.10394412030776341 Validation Loss: 0.7471426725387573\n",
      "Epoch 10133: Training Loss: 0.10387057314316432 Validation Loss: 0.7476196885108948\n",
      "Epoch 10134: Training Loss: 0.10360988726218541 Validation Loss: 0.7475446462631226\n",
      "Epoch 10135: Training Loss: 0.10379859060049057 Validation Loss: 0.7473966479301453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10136: Training Loss: 0.10447315871715546 Validation Loss: 0.7473756074905396\n",
      "Epoch 10137: Training Loss: 0.10317130635182063 Validation Loss: 0.7473840117454529\n",
      "Epoch 10138: Training Loss: 0.10366729398568471 Validation Loss: 0.7477191686630249\n",
      "Epoch 10139: Training Loss: 0.10389334956804912 Validation Loss: 0.7474219799041748\n",
      "Epoch 10140: Training Loss: 0.10419179995854695 Validation Loss: 0.7480699419975281\n",
      "Epoch 10141: Training Loss: 0.10361643383900325 Validation Loss: 0.7479337453842163\n",
      "Epoch 10142: Training Loss: 0.10331341624259949 Validation Loss: 0.7477097511291504\n",
      "Epoch 10143: Training Loss: 0.1036343624194463 Validation Loss: 0.7477557063102722\n",
      "Epoch 10144: Training Loss: 0.10373268028100331 Validation Loss: 0.74739009141922\n",
      "Epoch 10145: Training Loss: 0.10349101076523463 Validation Loss: 0.7474879026412964\n",
      "Epoch 10146: Training Loss: 0.10365249713261922 Validation Loss: 0.7477107644081116\n",
      "Epoch 10147: Training Loss: 0.10369258125623067 Validation Loss: 0.7478113770484924\n",
      "Epoch 10148: Training Loss: 0.10386741658051808 Validation Loss: 0.7472895979881287\n",
      "Epoch 10149: Training Loss: 0.10371609528859456 Validation Loss: 0.7474949359893799\n",
      "Epoch 10150: Training Loss: 0.10407323638598125 Validation Loss: 0.7470861077308655\n",
      "Epoch 10151: Training Loss: 0.10377207398414612 Validation Loss: 0.7475242614746094\n",
      "Epoch 10152: Training Loss: 0.10334274917840958 Validation Loss: 0.7477279901504517\n",
      "Epoch 10153: Training Loss: 0.10372140010197957 Validation Loss: 0.7480792999267578\n",
      "Epoch 10154: Training Loss: 0.10371922949949901 Validation Loss: 0.7480323314666748\n",
      "Epoch 10155: Training Loss: 0.1033763736486435 Validation Loss: 0.7473512887954712\n",
      "Epoch 10156: Training Loss: 0.10373065123955409 Validation Loss: 0.7473183274269104\n",
      "Epoch 10157: Training Loss: 0.10367697228988011 Validation Loss: 0.7477898597717285\n",
      "Epoch 10158: Training Loss: 0.10347700864076614 Validation Loss: 0.7477185726165771\n",
      "Epoch 10159: Training Loss: 0.10449588547150294 Validation Loss: 0.7483083009719849\n",
      "Epoch 10160: Training Loss: 0.10348580529292424 Validation Loss: 0.7478839159011841\n",
      "Epoch 10161: Training Loss: 0.1036488264799118 Validation Loss: 0.747567892074585\n",
      "Epoch 10162: Training Loss: 0.10378708690404892 Validation Loss: 0.7473734021186829\n",
      "Epoch 10163: Training Loss: 0.10327698787053426 Validation Loss: 0.7475016713142395\n",
      "Epoch 10164: Training Loss: 0.10320418824752171 Validation Loss: 0.7481054663658142\n",
      "Epoch 10165: Training Loss: 0.10400588065385818 Validation Loss: 0.747437596321106\n",
      "Epoch 10166: Training Loss: 0.10373466461896896 Validation Loss: 0.7477936744689941\n",
      "Epoch 10167: Training Loss: 0.10397497316201527 Validation Loss: 0.7477218508720398\n",
      "Epoch 10168: Training Loss: 0.10361439734697342 Validation Loss: 0.7471997141838074\n",
      "Epoch 10169: Training Loss: 0.10324237247308095 Validation Loss: 0.7473476529121399\n",
      "Epoch 10170: Training Loss: 0.10338544100522995 Validation Loss: 0.7478172779083252\n",
      "Epoch 10171: Training Loss: 0.10355682671070099 Validation Loss: 0.7479496598243713\n",
      "Epoch 10172: Training Loss: 0.10383251309394836 Validation Loss: 0.7476730346679688\n",
      "Epoch 10173: Training Loss: 0.10352207720279694 Validation Loss: 0.7476904392242432\n",
      "Epoch 10174: Training Loss: 0.10351371765136719 Validation Loss: 0.7479778528213501\n",
      "Epoch 10175: Training Loss: 0.10348779459794362 Validation Loss: 0.7481415867805481\n",
      "Epoch 10176: Training Loss: 0.10318845510482788 Validation Loss: 0.7477983236312866\n",
      "Epoch 10177: Training Loss: 0.10384708146254222 Validation Loss: 0.7481786012649536\n",
      "Epoch 10178: Training Loss: 0.10407454272111256 Validation Loss: 0.747694194316864\n",
      "Epoch 10179: Training Loss: 0.10357596725225449 Validation Loss: 0.7473258376121521\n",
      "Epoch 10180: Training Loss: 0.10354668895403545 Validation Loss: 0.7476573586463928\n",
      "Epoch 10181: Training Loss: 0.10319699347019196 Validation Loss: 0.748152494430542\n",
      "Epoch 10182: Training Loss: 0.10311692208051682 Validation Loss: 0.7482719421386719\n",
      "Epoch 10183: Training Loss: 0.10331164300441742 Validation Loss: 0.7479681968688965\n",
      "Epoch 10184: Training Loss: 0.10364075005054474 Validation Loss: 0.7481254935264587\n",
      "Epoch 10185: Training Loss: 0.10340709984302521 Validation Loss: 0.7479730248451233\n",
      "Epoch 10186: Training Loss: 0.10361110419034958 Validation Loss: 0.7476651668548584\n",
      "Epoch 10187: Training Loss: 0.10333569347858429 Validation Loss: 0.7476310133934021\n",
      "Epoch 10188: Training Loss: 0.10378675162792206 Validation Loss: 0.747599720954895\n",
      "Epoch 10189: Training Loss: 0.10346127301454544 Validation Loss: 0.7480077743530273\n",
      "Epoch 10190: Training Loss: 0.10326661417881648 Validation Loss: 0.7481426000595093\n",
      "Epoch 10191: Training Loss: 0.10336466630299886 Validation Loss: 0.7481455206871033\n",
      "Epoch 10192: Training Loss: 0.10326373825470607 Validation Loss: 0.7482173442840576\n",
      "Epoch 10193: Training Loss: 0.10331808775663376 Validation Loss: 0.7483120560646057\n",
      "Epoch 10194: Training Loss: 0.10305873304605484 Validation Loss: 0.7476328015327454\n",
      "Epoch 10195: Training Loss: 0.10396808882554372 Validation Loss: 0.7474524974822998\n",
      "Epoch 10196: Training Loss: 0.10329683621724446 Validation Loss: 0.7476228475570679\n",
      "Epoch 10197: Training Loss: 0.10318514456351598 Validation Loss: 0.7476513981819153\n",
      "Epoch 10198: Training Loss: 0.1030377671122551 Validation Loss: 0.7478580474853516\n",
      "Epoch 10199: Training Loss: 0.10313066095113754 Validation Loss: 0.7481491565704346\n",
      "Epoch 10200: Training Loss: 0.10377416511376698 Validation Loss: 0.7480412721633911\n",
      "Epoch 10201: Training Loss: 0.10390844196081161 Validation Loss: 0.7480800747871399\n",
      "Epoch 10202: Training Loss: 0.10322492321332295 Validation Loss: 0.7480653524398804\n",
      "Epoch 10203: Training Loss: 0.1032978023091952 Validation Loss: 0.7483028173446655\n",
      "Epoch 10204: Training Loss: 0.1032731259862582 Validation Loss: 0.7481628060340881\n",
      "Epoch 10205: Training Loss: 0.10348695764938991 Validation Loss: 0.7477318048477173\n",
      "Epoch 10206: Training Loss: 0.10423017293214798 Validation Loss: 0.7478260397911072\n",
      "Epoch 10207: Training Loss: 0.10304926584164302 Validation Loss: 0.7480887770652771\n",
      "Epoch 10208: Training Loss: 0.1037630985180537 Validation Loss: 0.7480361461639404\n",
      "Epoch 10209: Training Loss: 0.1030769149462382 Validation Loss: 0.7480120062828064\n",
      "Epoch 10210: Training Loss: 0.10318080335855484 Validation Loss: 0.7479580044746399\n",
      "Epoch 10211: Training Loss: 0.10325703769922256 Validation Loss: 0.7485454082489014\n",
      "Epoch 10212: Training Loss: 0.10279119263092677 Validation Loss: 0.7481510639190674\n",
      "Epoch 10213: Training Loss: 0.10321248819430669 Validation Loss: 0.7487550377845764\n",
      "Epoch 10214: Training Loss: 0.10303639868895213 Validation Loss: 0.7482491135597229\n",
      "Epoch 10215: Training Loss: 0.1032143384218216 Validation Loss: 0.748272180557251\n",
      "Epoch 10216: Training Loss: 0.1031213899453481 Validation Loss: 0.7479075193405151\n",
      "Epoch 10217: Training Loss: 0.10328206171592076 Validation Loss: 0.7478889226913452\n",
      "Epoch 10218: Training Loss: 0.10323882848024368 Validation Loss: 0.7484617233276367\n",
      "Epoch 10219: Training Loss: 0.10324820627768834 Validation Loss: 0.7480341196060181\n",
      "Epoch 10220: Training Loss: 0.10331075638532639 Validation Loss: 0.7480043172836304\n",
      "Epoch 10221: Training Loss: 0.10333144416411717 Validation Loss: 0.7478607296943665\n",
      "Epoch 10222: Training Loss: 0.1031406819820404 Validation Loss: 0.7480524182319641\n",
      "Epoch 10223: Training Loss: 0.10275347779194514 Validation Loss: 0.7483339905738831\n",
      "Epoch 10224: Training Loss: 0.10357573380072911 Validation Loss: 0.7483875751495361\n",
      "Epoch 10225: Training Loss: 0.1037173221508662 Validation Loss: 0.7487252354621887\n",
      "Epoch 10226: Training Loss: 0.10395883023738861 Validation Loss: 0.7484692335128784\n",
      "Epoch 10227: Training Loss: 0.10305629670619965 Validation Loss: 0.748349666595459\n",
      "Epoch 10228: Training Loss: 0.10225468128919601 Validation Loss: 0.7484698295593262\n",
      "Epoch 10229: Training Loss: 0.1029245878259341 Validation Loss: 0.7485908269882202\n",
      "Epoch 10230: Training Loss: 0.1026752417286237 Validation Loss: 0.7482997179031372\n",
      "Epoch 10231: Training Loss: 0.10317324846982956 Validation Loss: 0.748258650302887\n",
      "Epoch 10232: Training Loss: 0.10389212022225063 Validation Loss: 0.7485745549201965\n",
      "Epoch 10233: Training Loss: 0.10369382550319035 Validation Loss: 0.748100757598877\n",
      "Epoch 10234: Training Loss: 0.10307061423858006 Validation Loss: 0.7475698590278625\n",
      "Epoch 10235: Training Loss: 0.10317859550317128 Validation Loss: 0.7476984858512878\n",
      "Epoch 10236: Training Loss: 0.10318805525700252 Validation Loss: 0.7477155923843384\n",
      "Epoch 10237: Training Loss: 0.10365500052769978 Validation Loss: 0.7482628226280212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10238: Training Loss: 0.10303239027659099 Validation Loss: 0.7483619451522827\n",
      "Epoch 10239: Training Loss: 0.10306025793155034 Validation Loss: 0.7490432262420654\n",
      "Epoch 10240: Training Loss: 0.10300403585036595 Validation Loss: 0.7488265633583069\n",
      "Epoch 10241: Training Loss: 0.10331656287113826 Validation Loss: 0.7486127018928528\n",
      "Epoch 10242: Training Loss: 0.10358871271212895 Validation Loss: 0.7485659718513489\n",
      "Epoch 10243: Training Loss: 0.10309477647145589 Validation Loss: 0.7485798001289368\n",
      "Epoch 10244: Training Loss: 0.10306511571009953 Validation Loss: 0.7481120824813843\n",
      "Epoch 10245: Training Loss: 0.10251756757497787 Validation Loss: 0.7482321262359619\n",
      "Epoch 10246: Training Loss: 0.10297136753797531 Validation Loss: 0.7486697435379028\n",
      "Epoch 10247: Training Loss: 0.10269813736279805 Validation Loss: 0.7487403154373169\n",
      "Epoch 10248: Training Loss: 0.10234349469343822 Validation Loss: 0.7491432428359985\n",
      "Epoch 10249: Training Loss: 0.10300003737211227 Validation Loss: 0.7493523955345154\n",
      "Epoch 10250: Training Loss: 0.10292508701483409 Validation Loss: 0.7492548227310181\n",
      "Epoch 10251: Training Loss: 0.10305951535701752 Validation Loss: 0.7486526370048523\n",
      "Epoch 10252: Training Loss: 0.10285386939843495 Validation Loss: 0.7485863566398621\n",
      "Epoch 10253: Training Loss: 0.10283659398555756 Validation Loss: 0.7486321330070496\n",
      "Epoch 10254: Training Loss: 0.10324844469626744 Validation Loss: 0.7485670447349548\n",
      "Epoch 10255: Training Loss: 0.10317374517520268 Validation Loss: 0.7487691044807434\n",
      "Epoch 10256: Training Loss: 0.10303798069556554 Validation Loss: 0.748559296131134\n",
      "Epoch 10257: Training Loss: 0.1027304654320081 Validation Loss: 0.7483599185943604\n",
      "Epoch 10258: Training Loss: 0.1027798776825269 Validation Loss: 0.7484777569770813\n",
      "Epoch 10259: Training Loss: 0.10271951307853062 Validation Loss: 0.7483118772506714\n",
      "Epoch 10260: Training Loss: 0.10264828552802403 Validation Loss: 0.7485077381134033\n",
      "Epoch 10261: Training Loss: 0.10313692192236583 Validation Loss: 0.7486424446105957\n",
      "Epoch 10262: Training Loss: 0.10277380297581355 Validation Loss: 0.748698890209198\n",
      "Epoch 10263: Training Loss: 0.10268922646840413 Validation Loss: 0.7489207983016968\n",
      "Epoch 10264: Training Loss: 0.10267837842305501 Validation Loss: 0.748849093914032\n",
      "Epoch 10265: Training Loss: 0.10286474227905273 Validation Loss: 0.7486271262168884\n",
      "Epoch 10266: Training Loss: 0.10297183195749919 Validation Loss: 0.7484024167060852\n",
      "Epoch 10267: Training Loss: 0.10277699927488963 Validation Loss: 0.7484234571456909\n",
      "Epoch 10268: Training Loss: 0.10286752382914226 Validation Loss: 0.7486656308174133\n",
      "Epoch 10269: Training Loss: 0.10259950657685597 Validation Loss: 0.7488429546356201\n",
      "Epoch 10270: Training Loss: 0.10315316915512085 Validation Loss: 0.7494613528251648\n",
      "Epoch 10271: Training Loss: 0.10258100430170695 Validation Loss: 0.7490436434745789\n",
      "Epoch 10272: Training Loss: 0.10266282161076863 Validation Loss: 0.7486692667007446\n",
      "Epoch 10273: Training Loss: 0.10277940332889557 Validation Loss: 0.7482455372810364\n",
      "Epoch 10274: Training Loss: 0.10258567084868749 Validation Loss: 0.7486494183540344\n",
      "Epoch 10275: Training Loss: 0.10330502937237422 Validation Loss: 0.7484933733940125\n",
      "Epoch 10276: Training Loss: 0.10306164373954137 Validation Loss: 0.7485923171043396\n",
      "Epoch 10277: Training Loss: 0.10237692048152287 Validation Loss: 0.7488654255867004\n",
      "Epoch 10278: Training Loss: 0.10263501604398091 Validation Loss: 0.7488271594047546\n",
      "Epoch 10279: Training Loss: 0.10300680746634801 Validation Loss: 0.7489280104637146\n",
      "Epoch 10280: Training Loss: 0.1026374027132988 Validation Loss: 0.7489745616912842\n",
      "Epoch 10281: Training Loss: 0.10277145604292552 Validation Loss: 0.7489175796508789\n",
      "Epoch 10282: Training Loss: 0.10285796225070953 Validation Loss: 0.7490985989570618\n",
      "Epoch 10283: Training Loss: 0.10263197620709737 Validation Loss: 0.7488603591918945\n",
      "Epoch 10284: Training Loss: 0.10292935619751613 Validation Loss: 0.7487507462501526\n",
      "Epoch 10285: Training Loss: 0.10266854614019394 Validation Loss: 0.7487858533859253\n",
      "Epoch 10286: Training Loss: 0.10305562615394592 Validation Loss: 0.7486916780471802\n",
      "Epoch 10287: Training Loss: 0.10298371563355128 Validation Loss: 0.7488158941268921\n",
      "Epoch 10288: Training Loss: 0.10246135791142781 Validation Loss: 0.749046802520752\n",
      "Epoch 10289: Training Loss: 0.10254936168591182 Validation Loss: 0.7492757439613342\n",
      "Epoch 10290: Training Loss: 0.10249764968951543 Validation Loss: 0.7489608526229858\n",
      "Epoch 10291: Training Loss: 0.1024200866619746 Validation Loss: 0.7489144206047058\n",
      "Epoch 10292: Training Loss: 0.10257403800884883 Validation Loss: 0.7491018176078796\n",
      "Epoch 10293: Training Loss: 0.10312541574239731 Validation Loss: 0.7491342425346375\n",
      "Epoch 10294: Training Loss: 0.10263658811648686 Validation Loss: 0.7490565180778503\n",
      "Epoch 10295: Training Loss: 0.10260503739118576 Validation Loss: 0.7484722137451172\n",
      "Epoch 10296: Training Loss: 0.10311858852704366 Validation Loss: 0.7490324378013611\n",
      "Epoch 10297: Training Loss: 0.10246187448501587 Validation Loss: 0.7492787837982178\n",
      "Epoch 10298: Training Loss: 0.10253622631231944 Validation Loss: 0.7493022084236145\n",
      "Epoch 10299: Training Loss: 0.10254839807748795 Validation Loss: 0.7490872144699097\n",
      "Epoch 10300: Training Loss: 0.1024643878142039 Validation Loss: 0.7487351894378662\n",
      "Epoch 10301: Training Loss: 0.10258106887340546 Validation Loss: 0.7485188841819763\n",
      "Epoch 10302: Training Loss: 0.1024473508199056 Validation Loss: 0.7481447458267212\n",
      "Epoch 10303: Training Loss: 0.10240257034699123 Validation Loss: 0.7485224604606628\n",
      "Epoch 10304: Training Loss: 0.10261652618646622 Validation Loss: 0.7489070892333984\n",
      "Epoch 10305: Training Loss: 0.1020660325884819 Validation Loss: 0.749053418636322\n",
      "Epoch 10306: Training Loss: 0.10331786672274272 Validation Loss: 0.7494753003120422\n",
      "Epoch 10307: Training Loss: 0.10276846836010615 Validation Loss: 0.7496500611305237\n",
      "Epoch 10308: Training Loss: 0.10322718322277069 Validation Loss: 0.7491958141326904\n",
      "Epoch 10309: Training Loss: 0.10242796689271927 Validation Loss: 0.7488427758216858\n",
      "Epoch 10310: Training Loss: 0.10266803453365962 Validation Loss: 0.7485784888267517\n",
      "Epoch 10311: Training Loss: 0.10312909881273906 Validation Loss: 0.7487865686416626\n",
      "Epoch 10312: Training Loss: 0.10236740112304688 Validation Loss: 0.7493763566017151\n",
      "Epoch 10313: Training Loss: 0.10207504530747731 Validation Loss: 0.7494676113128662\n",
      "Epoch 10314: Training Loss: 0.10253755499919255 Validation Loss: 0.7497549057006836\n",
      "Epoch 10315: Training Loss: 0.10245648523171742 Validation Loss: 0.7496973276138306\n",
      "Epoch 10316: Training Loss: 0.10236000766356786 Validation Loss: 0.7491820454597473\n",
      "Epoch 10317: Training Loss: 0.10256961733102798 Validation Loss: 0.7489959001541138\n",
      "Epoch 10318: Training Loss: 0.10259800652662913 Validation Loss: 0.7487936019897461\n",
      "Epoch 10319: Training Loss: 0.1031455248594284 Validation Loss: 0.7492119073867798\n",
      "Epoch 10320: Training Loss: 0.10255847374598186 Validation Loss: 0.7492139935493469\n",
      "Epoch 10321: Training Loss: 0.10253724704186122 Validation Loss: 0.7488422989845276\n",
      "Epoch 10322: Training Loss: 0.102385679880778 Validation Loss: 0.7493169903755188\n",
      "Epoch 10323: Training Loss: 0.10235389073689778 Validation Loss: 0.7498461008071899\n",
      "Epoch 10324: Training Loss: 0.10290801028410594 Validation Loss: 0.7497817277908325\n",
      "Epoch 10325: Training Loss: 0.10258615761995316 Validation Loss: 0.7492055296897888\n",
      "Epoch 10326: Training Loss: 0.102423757314682 Validation Loss: 0.7488605976104736\n",
      "Epoch 10327: Training Loss: 0.10254199306170146 Validation Loss: 0.7487868666648865\n",
      "Epoch 10328: Training Loss: 0.1027088537812233 Validation Loss: 0.7488563060760498\n",
      "Epoch 10329: Training Loss: 0.10233765095472336 Validation Loss: 0.7492733597755432\n",
      "Epoch 10330: Training Loss: 0.10240007936954498 Validation Loss: 0.7495354413986206\n",
      "Epoch 10331: Training Loss: 0.1022878885269165 Validation Loss: 0.7497132420539856\n",
      "Epoch 10332: Training Loss: 0.10303189605474472 Validation Loss: 0.749556839466095\n",
      "Epoch 10333: Training Loss: 0.10231597224871318 Validation Loss: 0.7493593692779541\n",
      "Epoch 10334: Training Loss: 0.10215945541858673 Validation Loss: 0.7490493655204773\n",
      "Epoch 10335: Training Loss: 0.10292220860719681 Validation Loss: 0.7491217851638794\n",
      "Epoch 10336: Training Loss: 0.10295339673757553 Validation Loss: 0.749001145362854\n",
      "Epoch 10337: Training Loss: 0.10243684301773708 Validation Loss: 0.748837947845459\n",
      "Epoch 10338: Training Loss: 0.10228296866019566 Validation Loss: 0.7491572499275208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10339: Training Loss: 0.1022961934407552 Validation Loss: 0.7492935657501221\n",
      "Epoch 10340: Training Loss: 0.10300604502360027 Validation Loss: 0.7494075298309326\n",
      "Epoch 10341: Training Loss: 0.10219376782576244 Validation Loss: 0.7494017481803894\n",
      "Epoch 10342: Training Loss: 0.10350162535905838 Validation Loss: 0.7495130300521851\n",
      "Epoch 10343: Training Loss: 0.1022209698955218 Validation Loss: 0.7495515942573547\n",
      "Epoch 10344: Training Loss: 0.10234178354342778 Validation Loss: 0.7494337558746338\n",
      "Epoch 10345: Training Loss: 0.10234308242797852 Validation Loss: 0.749709963798523\n",
      "Epoch 10346: Training Loss: 0.10240557293097179 Validation Loss: 0.7496140599250793\n",
      "Epoch 10347: Training Loss: 0.102478322883447 Validation Loss: 0.7495110630989075\n",
      "Epoch 10348: Training Loss: 0.10203990588585536 Validation Loss: 0.7492109537124634\n",
      "Epoch 10349: Training Loss: 0.10223768403132756 Validation Loss: 0.7490702867507935\n",
      "Epoch 10350: Training Loss: 0.10271855443716049 Validation Loss: 0.7490162253379822\n",
      "Epoch 10351: Training Loss: 0.10226952532927196 Validation Loss: 0.749576210975647\n",
      "Epoch 10352: Training Loss: 0.10214373966058095 Validation Loss: 0.7499167919158936\n",
      "Epoch 10353: Training Loss: 0.10244804620742798 Validation Loss: 0.7501474022865295\n",
      "Epoch 10354: Training Loss: 0.10229133566220601 Validation Loss: 0.74959796667099\n",
      "Epoch 10355: Training Loss: 0.10182705769936244 Validation Loss: 0.7492889165878296\n",
      "Epoch 10356: Training Loss: 0.10236786305904388 Validation Loss: 0.749340295791626\n",
      "Epoch 10357: Training Loss: 0.1021544560790062 Validation Loss: 0.7495512962341309\n",
      "Epoch 10358: Training Loss: 0.10200067609548569 Validation Loss: 0.7496944665908813\n",
      "Epoch 10359: Training Loss: 0.10228119045495987 Validation Loss: 0.7498330473899841\n",
      "Epoch 10360: Training Loss: 0.10245565573374431 Validation Loss: 0.749644935131073\n",
      "Epoch 10361: Training Loss: 0.1016468455394109 Validation Loss: 0.7495270371437073\n",
      "Epoch 10362: Training Loss: 0.10287880897521973 Validation Loss: 0.7495459318161011\n",
      "Epoch 10363: Training Loss: 0.10209327936172485 Validation Loss: 0.7494820356369019\n",
      "Epoch 10364: Training Loss: 0.10310483475526173 Validation Loss: 0.7501193881034851\n",
      "Epoch 10365: Training Loss: 0.10204929858446121 Validation Loss: 0.7500044107437134\n",
      "Epoch 10366: Training Loss: 0.10190761586030324 Validation Loss: 0.7498714327812195\n",
      "Epoch 10367: Training Loss: 0.10219452281792958 Validation Loss: 0.7496733665466309\n",
      "Epoch 10368: Training Loss: 0.10205752154191335 Validation Loss: 0.7498824596405029\n",
      "Epoch 10369: Training Loss: 0.10237343360980351 Validation Loss: 0.7498685121536255\n",
      "Epoch 10370: Training Loss: 0.10294182846943538 Validation Loss: 0.7498064637184143\n",
      "Epoch 10371: Training Loss: 0.10212141027053197 Validation Loss: 0.7498317956924438\n",
      "Epoch 10372: Training Loss: 0.10203229387601216 Validation Loss: 0.749653697013855\n",
      "Epoch 10373: Training Loss: 0.10211467494567235 Validation Loss: 0.7496442794799805\n",
      "Epoch 10374: Training Loss: 0.10254535575707753 Validation Loss: 0.750041127204895\n",
      "Epoch 10375: Training Loss: 0.1019347111384074 Validation Loss: 0.7495678067207336\n",
      "Epoch 10376: Training Loss: 0.10232088963190715 Validation Loss: 0.7495981454849243\n",
      "Epoch 10377: Training Loss: 0.1028313860297203 Validation Loss: 0.7496859431266785\n",
      "Epoch 10378: Training Loss: 0.1020380010207494 Validation Loss: 0.7494637966156006\n",
      "Epoch 10379: Training Loss: 0.10202180594205856 Validation Loss: 0.7495088577270508\n",
      "Epoch 10380: Training Loss: 0.10194553186496098 Validation Loss: 0.7493354678153992\n",
      "Epoch 10381: Training Loss: 0.10328896592060725 Validation Loss: 0.7493705153465271\n",
      "Epoch 10382: Training Loss: 0.10182118664185207 Validation Loss: 0.7497249841690063\n",
      "Epoch 10383: Training Loss: 0.10205646107594173 Validation Loss: 0.7494043707847595\n",
      "Epoch 10384: Training Loss: 0.10210053871075313 Validation Loss: 0.749678909778595\n",
      "Epoch 10385: Training Loss: 0.10194860647122066 Validation Loss: 0.7497881650924683\n",
      "Epoch 10386: Training Loss: 0.10199871410926183 Validation Loss: 0.7496892809867859\n",
      "Epoch 10387: Training Loss: 0.10175137470165889 Validation Loss: 0.7496086955070496\n",
      "Epoch 10388: Training Loss: 0.10217676311731339 Validation Loss: 0.7502004504203796\n",
      "Epoch 10389: Training Loss: 0.1019857053955396 Validation Loss: 0.7502721548080444\n",
      "Epoch 10390: Training Loss: 0.1024682770172755 Validation Loss: 0.7500817775726318\n",
      "Epoch 10391: Training Loss: 0.10192028433084488 Validation Loss: 0.7500149011611938\n",
      "Epoch 10392: Training Loss: 0.10196196536223094 Validation Loss: 0.7496935129165649\n",
      "Epoch 10393: Training Loss: 0.1019172469774882 Validation Loss: 0.7499107718467712\n",
      "Epoch 10394: Training Loss: 0.10298483818769455 Validation Loss: 0.7496669888496399\n",
      "Epoch 10395: Training Loss: 0.10188800344864528 Validation Loss: 0.749771773815155\n",
      "Epoch 10396: Training Loss: 0.10241866360108058 Validation Loss: 0.7496941685676575\n",
      "Epoch 10397: Training Loss: 0.10183890163898468 Validation Loss: 0.7498157620429993\n",
      "Epoch 10398: Training Loss: 0.10148416459560394 Validation Loss: 0.750028133392334\n",
      "Epoch 10399: Training Loss: 0.10211702932914098 Validation Loss: 0.7498648762702942\n",
      "Epoch 10400: Training Loss: 0.10159614682197571 Validation Loss: 0.7500879764556885\n",
      "Epoch 10401: Training Loss: 0.10189516842365265 Validation Loss: 0.7501402497291565\n",
      "Epoch 10402: Training Loss: 0.10320033878087997 Validation Loss: 0.7503922581672668\n",
      "Epoch 10403: Training Loss: 0.1021138106783231 Validation Loss: 0.750099778175354\n",
      "Epoch 10404: Training Loss: 0.1028438334663709 Validation Loss: 0.7504135966300964\n",
      "Epoch 10405: Training Loss: 0.1025438904762268 Validation Loss: 0.7499197721481323\n",
      "Epoch 10406: Training Loss: 0.10219134390354156 Validation Loss: 0.749638020992279\n",
      "Epoch 10407: Training Loss: 0.10323341190814972 Validation Loss: 0.7497305274009705\n",
      "Epoch 10408: Training Loss: 0.10191616415977478 Validation Loss: 0.7500799298286438\n",
      "Epoch 10409: Training Loss: 0.10193763921658199 Validation Loss: 0.7498528957366943\n",
      "Epoch 10410: Training Loss: 0.10191266238689423 Validation Loss: 0.7499864101409912\n",
      "Epoch 10411: Training Loss: 0.1019085372487704 Validation Loss: 0.7501699328422546\n",
      "Epoch 10412: Training Loss: 0.10178405791521072 Validation Loss: 0.7500502467155457\n",
      "Epoch 10413: Training Loss: 0.10185620437065761 Validation Loss: 0.7500226497650146\n",
      "Epoch 10414: Training Loss: 0.10192976146936417 Validation Loss: 0.7499355673789978\n",
      "Epoch 10415: Training Loss: 0.10256432741880417 Validation Loss: 0.7498502731323242\n",
      "Epoch 10416: Training Loss: 0.10190056016047795 Validation Loss: 0.7503503561019897\n",
      "Epoch 10417: Training Loss: 0.10179881751537323 Validation Loss: 0.7501912117004395\n",
      "Epoch 10418: Training Loss: 0.10171455144882202 Validation Loss: 0.7500707507133484\n",
      "Epoch 10419: Training Loss: 0.10277753323316574 Validation Loss: 0.7502335906028748\n",
      "Epoch 10420: Training Loss: 0.10182662804921468 Validation Loss: 0.7500118613243103\n",
      "Epoch 10421: Training Loss: 0.10184613863627116 Validation Loss: 0.7495781183242798\n",
      "Epoch 10422: Training Loss: 0.10179435710112254 Validation Loss: 0.7499503493309021\n",
      "Epoch 10423: Training Loss: 0.10218638678391774 Validation Loss: 0.7502564191818237\n",
      "Epoch 10424: Training Loss: 0.10176508625348409 Validation Loss: 0.7502774000167847\n",
      "Epoch 10425: Training Loss: 0.10187410314877827 Validation Loss: 0.7503829002380371\n",
      "Epoch 10426: Training Loss: 0.1017532820502917 Validation Loss: 0.7503268122673035\n",
      "Epoch 10427: Training Loss: 0.10180909186601639 Validation Loss: 0.7499225735664368\n",
      "Epoch 10428: Training Loss: 0.10190689812103908 Validation Loss: 0.7496956586837769\n",
      "Epoch 10429: Training Loss: 0.10214283814032872 Validation Loss: 0.7499943971633911\n",
      "Epoch 10430: Training Loss: 0.10156067212422688 Validation Loss: 0.7500770688056946\n",
      "Epoch 10431: Training Loss: 0.10171767075856526 Validation Loss: 0.7505099773406982\n",
      "Epoch 10432: Training Loss: 0.10188671201467514 Validation Loss: 0.7506765127182007\n",
      "Epoch 10433: Training Loss: 0.1021596093972524 Validation Loss: 0.7507382035255432\n",
      "Epoch 10434: Training Loss: 0.10225269943475723 Validation Loss: 0.7499375939369202\n",
      "Epoch 10435: Training Loss: 0.10143205771843593 Validation Loss: 0.7500025033950806\n",
      "Epoch 10436: Training Loss: 0.10139146695534389 Validation Loss: 0.7503199577331543\n",
      "Epoch 10437: Training Loss: 0.10122917840878169 Validation Loss: 0.7505861520767212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10438: Training Loss: 0.10224004338184993 Validation Loss: 0.7508566379547119\n",
      "Epoch 10439: Training Loss: 0.10218500594298045 Validation Loss: 0.7502752542495728\n",
      "Epoch 10440: Training Loss: 0.1024694840113322 Validation Loss: 0.7509930729866028\n",
      "Epoch 10441: Training Loss: 0.10160254935423534 Validation Loss: 0.7510485053062439\n",
      "Epoch 10442: Training Loss: 0.10176435112953186 Validation Loss: 0.7509466409683228\n",
      "Epoch 10443: Training Loss: 0.1015852044026057 Validation Loss: 0.7507529854774475\n",
      "Epoch 10444: Training Loss: 0.10182289282480876 Validation Loss: 0.7506290078163147\n",
      "Epoch 10445: Training Loss: 0.10182630022366841 Validation Loss: 0.7505238652229309\n",
      "Epoch 10446: Training Loss: 0.10229325791200002 Validation Loss: 0.7503120303153992\n",
      "Epoch 10447: Training Loss: 0.10144708305597305 Validation Loss: 0.750212550163269\n",
      "Epoch 10448: Training Loss: 0.1015972172220548 Validation Loss: 0.7506133913993835\n",
      "Epoch 10449: Training Loss: 0.1016826182603836 Validation Loss: 0.7512821555137634\n",
      "Epoch 10450: Training Loss: 0.10157827784617741 Validation Loss: 0.7509301900863647\n",
      "Epoch 10451: Training Loss: 0.10151697198549907 Validation Loss: 0.7509153485298157\n",
      "Epoch 10452: Training Loss: 0.10182691365480423 Validation Loss: 0.7506895661354065\n",
      "Epoch 10453: Training Loss: 0.10201420386632283 Validation Loss: 0.750738263130188\n",
      "Epoch 10454: Training Loss: 0.10183987021446228 Validation Loss: 0.7505497336387634\n",
      "Epoch 10455: Training Loss: 0.10175985346237819 Validation Loss: 0.750695526599884\n",
      "Epoch 10456: Training Loss: 0.10153233508268993 Validation Loss: 0.7508107423782349\n",
      "Epoch 10457: Training Loss: 0.10169881085554759 Validation Loss: 0.7506677508354187\n",
      "Epoch 10458: Training Loss: 0.10159662117560704 Validation Loss: 0.7507084012031555\n",
      "Epoch 10459: Training Loss: 0.10151550422112147 Validation Loss: 0.7511457800865173\n",
      "Epoch 10460: Training Loss: 0.1015155886610349 Validation Loss: 0.751144528388977\n",
      "Epoch 10461: Training Loss: 0.10128317773342133 Validation Loss: 0.7509753108024597\n",
      "Epoch 10462: Training Loss: 0.10221773634354274 Validation Loss: 0.7507294416427612\n",
      "Epoch 10463: Training Loss: 0.10144190986951192 Validation Loss: 0.7505975365638733\n",
      "Epoch 10464: Training Loss: 0.10131359100341797 Validation Loss: 0.7507560849189758\n",
      "Epoch 10465: Training Loss: 0.10146066298087437 Validation Loss: 0.7510331869125366\n",
      "Epoch 10466: Training Loss: 0.101301242907842 Validation Loss: 0.7507714629173279\n",
      "Epoch 10467: Training Loss: 0.10167535146077473 Validation Loss: 0.7506667971611023\n",
      "Epoch 10468: Training Loss: 0.10181659708420436 Validation Loss: 0.7504259943962097\n",
      "Epoch 10469: Training Loss: 0.10150698820749919 Validation Loss: 0.7505571246147156\n",
      "Epoch 10470: Training Loss: 0.10138266781965892 Validation Loss: 0.7504726648330688\n",
      "Epoch 10471: Training Loss: 0.10127831747134526 Validation Loss: 0.750328004360199\n",
      "Epoch 10472: Training Loss: 0.101707324385643 Validation Loss: 0.7507988810539246\n",
      "Epoch 10473: Training Loss: 0.10130293170611064 Validation Loss: 0.750726044178009\n",
      "Epoch 10474: Training Loss: 0.10165890554587047 Validation Loss: 0.7507419586181641\n",
      "Epoch 10475: Training Loss: 0.10163089136282603 Validation Loss: 0.7507240772247314\n",
      "Epoch 10476: Training Loss: 0.10154460618893306 Validation Loss: 0.750938892364502\n",
      "Epoch 10477: Training Loss: 0.10124098012844722 Validation Loss: 0.7505742311477661\n",
      "Epoch 10478: Training Loss: 0.1013213520248731 Validation Loss: 0.7502777576446533\n",
      "Epoch 10479: Training Loss: 0.1022033045689265 Validation Loss: 0.7501146197319031\n",
      "Epoch 10480: Training Loss: 0.10203677415847778 Validation Loss: 0.7502671480178833\n",
      "Epoch 10481: Training Loss: 0.10130519171555837 Validation Loss: 0.7506661415100098\n",
      "Epoch 10482: Training Loss: 0.10145500302314758 Validation Loss: 0.7507498264312744\n",
      "Epoch 10483: Training Loss: 0.1014668916662534 Validation Loss: 0.7511034607887268\n",
      "Epoch 10484: Training Loss: 0.10136017203330994 Validation Loss: 0.7506914138793945\n",
      "Epoch 10485: Training Loss: 0.1013252983490626 Validation Loss: 0.750714898109436\n",
      "Epoch 10486: Training Loss: 0.10139593730370204 Validation Loss: 0.7505966424942017\n",
      "Epoch 10487: Training Loss: 0.10148906459410985 Validation Loss: 0.7505815029144287\n",
      "Epoch 10488: Training Loss: 0.10161025077104568 Validation Loss: 0.7510090470314026\n",
      "Epoch 10489: Training Loss: 0.10164988785982132 Validation Loss: 0.7509700655937195\n",
      "Epoch 10490: Training Loss: 0.10124179969231288 Validation Loss: 0.7508496642112732\n",
      "Epoch 10491: Training Loss: 0.10122707734505336 Validation Loss: 0.750849723815918\n",
      "Epoch 10492: Training Loss: 0.10107481727997462 Validation Loss: 0.7509468197822571\n",
      "Epoch 10493: Training Loss: 0.10172529021898906 Validation Loss: 0.7510395646095276\n",
      "Epoch 10494: Training Loss: 0.1012571503718694 Validation Loss: 0.7509652376174927\n",
      "Epoch 10495: Training Loss: 0.10123133411010106 Validation Loss: 0.7513327598571777\n",
      "Epoch 10496: Training Loss: 0.1012217899163564 Validation Loss: 0.7510374188423157\n",
      "Epoch 10497: Training Loss: 0.10180381933848064 Validation Loss: 0.7509976625442505\n",
      "Epoch 10498: Training Loss: 0.10136238982280095 Validation Loss: 0.7509526014328003\n",
      "Epoch 10499: Training Loss: 0.10101076712210973 Validation Loss: 0.751089870929718\n",
      "Epoch 10500: Training Loss: 0.10123217354218166 Validation Loss: 0.751116156578064\n",
      "Epoch 10501: Training Loss: 0.10122139006853104 Validation Loss: 0.7512936592102051\n",
      "Epoch 10502: Training Loss: 0.10279487818479538 Validation Loss: 0.7510251402854919\n",
      "Epoch 10503: Training Loss: 0.10156484693288803 Validation Loss: 0.7509628534317017\n",
      "Epoch 10504: Training Loss: 0.10118736575047176 Validation Loss: 0.7512144446372986\n",
      "Epoch 10505: Training Loss: 0.10138014455636342 Validation Loss: 0.7505868077278137\n",
      "Epoch 10506: Training Loss: 0.10188812514146169 Validation Loss: 0.7506374716758728\n",
      "Epoch 10507: Training Loss: 0.10155415038267772 Validation Loss: 0.7511613965034485\n",
      "Epoch 10508: Training Loss: 0.10136004537343979 Validation Loss: 0.751151442527771\n",
      "Epoch 10509: Training Loss: 0.10171185185511906 Validation Loss: 0.7507451772689819\n",
      "Epoch 10510: Training Loss: 0.10127100845177968 Validation Loss: 0.7510861754417419\n",
      "Epoch 10511: Training Loss: 0.10184609889984131 Validation Loss: 0.7512183785438538\n",
      "Epoch 10512: Training Loss: 0.10107973714669545 Validation Loss: 0.7515129446983337\n",
      "Epoch 10513: Training Loss: 0.1016822134455045 Validation Loss: 0.7513424754142761\n",
      "Epoch 10514: Training Loss: 0.10158846030632655 Validation Loss: 0.7513307929039001\n",
      "Epoch 10515: Training Loss: 0.10187421242396037 Validation Loss: 0.7512675523757935\n",
      "Epoch 10516: Training Loss: 0.10117957492669423 Validation Loss: 0.751107931137085\n",
      "Epoch 10517: Training Loss: 0.10119881977637608 Validation Loss: 0.7509572505950928\n",
      "Epoch 10518: Training Loss: 0.10105492174625397 Validation Loss: 0.7510331869125366\n",
      "Epoch 10519: Training Loss: 0.10112285614013672 Validation Loss: 0.7511796951293945\n",
      "Epoch 10520: Training Loss: 0.10120564450820287 Validation Loss: 0.7516481280326843\n",
      "Epoch 10521: Training Loss: 0.10120360553264618 Validation Loss: 0.7515482902526855\n",
      "Epoch 10522: Training Loss: 0.10102786620457967 Validation Loss: 0.7514282464981079\n",
      "Epoch 10523: Training Loss: 0.10115479429562886 Validation Loss: 0.7514603734016418\n",
      "Epoch 10524: Training Loss: 0.10168676575024922 Validation Loss: 0.7511835098266602\n",
      "Epoch 10525: Training Loss: 0.1010492021838824 Validation Loss: 0.7510164380073547\n",
      "Epoch 10526: Training Loss: 0.10113584498564403 Validation Loss: 0.7512567639350891\n",
      "Epoch 10527: Training Loss: 0.10186336189508438 Validation Loss: 0.7512221932411194\n",
      "Epoch 10528: Training Loss: 0.1010023405154546 Validation Loss: 0.7511923909187317\n",
      "Epoch 10529: Training Loss: 0.10104346772034963 Validation Loss: 0.7505877614021301\n",
      "Epoch 10530: Training Loss: 0.10088904947042465 Validation Loss: 0.7508921027183533\n",
      "Epoch 10531: Training Loss: 0.10150730858246486 Validation Loss: 0.7510013580322266\n",
      "Epoch 10532: Training Loss: 0.10086869448423386 Validation Loss: 0.750907838344574\n",
      "Epoch 10533: Training Loss: 0.1011342853307724 Validation Loss: 0.7515763640403748\n",
      "Epoch 10534: Training Loss: 0.1018009955684344 Validation Loss: 0.7517738938331604\n",
      "Epoch 10535: Training Loss: 0.10115327189366023 Validation Loss: 0.751268744468689\n",
      "Epoch 10536: Training Loss: 0.10118500391642253 Validation Loss: 0.7509739995002747\n",
      "Epoch 10537: Training Loss: 0.10111762831608455 Validation Loss: 0.7511729001998901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10538: Training Loss: 0.10115092247724533 Validation Loss: 0.7512982487678528\n",
      "Epoch 10539: Training Loss: 0.10117723296085994 Validation Loss: 0.7516583204269409\n",
      "Epoch 10540: Training Loss: 0.10145021975040436 Validation Loss: 0.7514864206314087\n",
      "Epoch 10541: Training Loss: 0.10103460898001988 Validation Loss: 0.7515233755111694\n",
      "Epoch 10542: Training Loss: 0.10107701520125072 Validation Loss: 0.751864492893219\n",
      "Epoch 10543: Training Loss: 0.10097406307856242 Validation Loss: 0.7514572739601135\n",
      "Epoch 10544: Training Loss: 0.10124818980693817 Validation Loss: 0.7514715194702148\n",
      "Epoch 10545: Training Loss: 0.10076077779134114 Validation Loss: 0.7514780163764954\n",
      "Epoch 10546: Training Loss: 0.10110101352135341 Validation Loss: 0.751365602016449\n",
      "Epoch 10547: Training Loss: 0.10090489437182744 Validation Loss: 0.751410722732544\n",
      "Epoch 10548: Training Loss: 0.1008455107609431 Validation Loss: 0.751528799533844\n",
      "Epoch 10549: Training Loss: 0.10070185611645381 Validation Loss: 0.7513971924781799\n",
      "Epoch 10550: Training Loss: 0.10138481358687083 Validation Loss: 0.7515563368797302\n",
      "Epoch 10551: Training Loss: 0.10102172195911407 Validation Loss: 0.7511757612228394\n",
      "Epoch 10552: Training Loss: 0.10100275526444118 Validation Loss: 0.7507531046867371\n",
      "Epoch 10553: Training Loss: 0.10041504104932149 Validation Loss: 0.7509304881095886\n",
      "Epoch 10554: Training Loss: 0.1012578730781873 Validation Loss: 0.7509182095527649\n",
      "Epoch 10555: Training Loss: 0.1008322462439537 Validation Loss: 0.7514364719390869\n",
      "Epoch 10556: Training Loss: 0.10102360198895137 Validation Loss: 0.7519587874412537\n",
      "Epoch 10557: Training Loss: 0.10144520550966263 Validation Loss: 0.7526882290840149\n",
      "Epoch 10558: Training Loss: 0.10156068454186122 Validation Loss: 0.7526299953460693\n",
      "Epoch 10559: Training Loss: 0.10135091592868169 Validation Loss: 0.7518777847290039\n",
      "Epoch 10560: Training Loss: 0.1008385568857193 Validation Loss: 0.7519080638885498\n",
      "Epoch 10561: Training Loss: 0.10230488330125809 Validation Loss: 0.7514359354972839\n",
      "Epoch 10562: Training Loss: 0.10110886146624883 Validation Loss: 0.7522100210189819\n",
      "Epoch 10563: Training Loss: 0.10080831497907639 Validation Loss: 0.752375066280365\n",
      "Epoch 10564: Training Loss: 0.10050928095976512 Validation Loss: 0.7519344091415405\n",
      "Epoch 10565: Training Loss: 0.10065189749002457 Validation Loss: 0.7518474459648132\n",
      "Epoch 10566: Training Loss: 0.10060258706410725 Validation Loss: 0.7520057559013367\n",
      "Epoch 10567: Training Loss: 0.10095753024021785 Validation Loss: 0.7519944906234741\n",
      "Epoch 10568: Training Loss: 0.10078895588715871 Validation Loss: 0.7519247531890869\n",
      "Epoch 10569: Training Loss: 0.1001737837990125 Validation Loss: 0.7521224021911621\n",
      "Epoch 10570: Training Loss: 0.10078034301598866 Validation Loss: 0.7524508237838745\n",
      "Epoch 10571: Training Loss: 0.10073130577802658 Validation Loss: 0.7521806955337524\n",
      "Epoch 10572: Training Loss: 0.10117279489835103 Validation Loss: 0.75194251537323\n",
      "Epoch 10573: Training Loss: 0.10104325165351231 Validation Loss: 0.7519019842147827\n",
      "Epoch 10574: Training Loss: 0.10060947388410568 Validation Loss: 0.7518537640571594\n",
      "Epoch 10575: Training Loss: 0.10123905291159947 Validation Loss: 0.7513600587844849\n",
      "Epoch 10576: Training Loss: 0.10098742445309956 Validation Loss: 0.7514075636863708\n",
      "Epoch 10577: Training Loss: 0.1007492368419965 Validation Loss: 0.7519095540046692\n",
      "Epoch 10578: Training Loss: 0.10078380505243938 Validation Loss: 0.7521581649780273\n",
      "Epoch 10579: Training Loss: 0.10083696494499843 Validation Loss: 0.7519730925559998\n",
      "Epoch 10580: Training Loss: 0.10108505686124165 Validation Loss: 0.7519188523292542\n",
      "Epoch 10581: Training Loss: 0.1008740911881129 Validation Loss: 0.7515077590942383\n",
      "Epoch 10582: Training Loss: 0.10014732430378596 Validation Loss: 0.7511870861053467\n",
      "Epoch 10583: Training Loss: 0.10077617069085439 Validation Loss: 0.7514243721961975\n",
      "Epoch 10584: Training Loss: 0.10197598238786061 Validation Loss: 0.75203537940979\n",
      "Epoch 10585: Training Loss: 0.09998473525047302 Validation Loss: 0.7522797584533691\n",
      "Epoch 10586: Training Loss: 0.10074302305777867 Validation Loss: 0.7526504993438721\n",
      "Epoch 10587: Training Loss: 0.10164603590965271 Validation Loss: 0.7527623772621155\n",
      "Epoch 10588: Training Loss: 0.10053875794013341 Validation Loss: 0.7521681785583496\n",
      "Epoch 10589: Training Loss: 0.10060195376475652 Validation Loss: 0.7522391080856323\n",
      "Epoch 10590: Training Loss: 0.10063644001881282 Validation Loss: 0.7521837949752808\n",
      "Epoch 10591: Training Loss: 0.10101836919784546 Validation Loss: 0.752382755279541\n",
      "Epoch 10592: Training Loss: 0.10137611130873363 Validation Loss: 0.7519662380218506\n",
      "Epoch 10593: Training Loss: 0.10067115227381389 Validation Loss: 0.7517016530036926\n",
      "Epoch 10594: Training Loss: 0.10062512258688609 Validation Loss: 0.751550555229187\n",
      "Epoch 10595: Training Loss: 0.1008778711160024 Validation Loss: 0.7515817880630493\n",
      "Epoch 10596: Training Loss: 0.10081135481595993 Validation Loss: 0.7518171072006226\n",
      "Epoch 10597: Training Loss: 0.10115358978509903 Validation Loss: 0.7526305317878723\n",
      "Epoch 10598: Training Loss: 0.10052237162987392 Validation Loss: 0.7523271441459656\n",
      "Epoch 10599: Training Loss: 0.10105360299348831 Validation Loss: 0.7517668008804321\n",
      "Epoch 10600: Training Loss: 0.10069870203733444 Validation Loss: 0.751899003982544\n",
      "Epoch 10601: Training Loss: 0.10063620160023372 Validation Loss: 0.7522944211959839\n",
      "Epoch 10602: Training Loss: 0.10100359221299489 Validation Loss: 0.7520295977592468\n",
      "Epoch 10603: Training Loss: 0.1008160983522733 Validation Loss: 0.7519528865814209\n",
      "Epoch 10604: Training Loss: 0.10046740372975667 Validation Loss: 0.7521416544914246\n",
      "Epoch 10605: Training Loss: 0.10045735041300456 Validation Loss: 0.7520782351493835\n",
      "Epoch 10606: Training Loss: 0.10025374342997868 Validation Loss: 0.7519999742507935\n",
      "Epoch 10607: Training Loss: 0.10069877405961354 Validation Loss: 0.752016544342041\n",
      "Epoch 10608: Training Loss: 0.10050121198097865 Validation Loss: 0.7524620890617371\n",
      "Epoch 10609: Training Loss: 0.10050856322050095 Validation Loss: 0.7526105642318726\n",
      "Epoch 10610: Training Loss: 0.09999581426382065 Validation Loss: 0.752097487449646\n",
      "Epoch 10611: Training Loss: 0.10076914230982463 Validation Loss: 0.7517532110214233\n",
      "Epoch 10612: Training Loss: 0.10015824437141418 Validation Loss: 0.7519642114639282\n",
      "Epoch 10613: Training Loss: 0.10066817949215572 Validation Loss: 0.7520152926445007\n",
      "Epoch 10614: Training Loss: 0.10076375057299931 Validation Loss: 0.7525053024291992\n",
      "Epoch 10615: Training Loss: 0.10051299879948299 Validation Loss: 0.7525156736373901\n",
      "Epoch 10616: Training Loss: 0.10115675876537959 Validation Loss: 0.7523002028465271\n",
      "Epoch 10617: Training Loss: 0.1013452410697937 Validation Loss: 0.7524480223655701\n",
      "Epoch 10618: Training Loss: 0.10074432939291 Validation Loss: 0.7519547939300537\n",
      "Epoch 10619: Training Loss: 0.10059347997109096 Validation Loss: 0.7519881725311279\n",
      "Epoch 10620: Training Loss: 0.10070454080899556 Validation Loss: 0.7522203326225281\n",
      "Epoch 10621: Training Loss: 0.10072758297125499 Validation Loss: 0.7524570822715759\n",
      "Epoch 10622: Training Loss: 0.1000588908791542 Validation Loss: 0.7527459859848022\n",
      "Epoch 10623: Training Loss: 0.10053000847498576 Validation Loss: 0.7528908848762512\n",
      "Epoch 10624: Training Loss: 0.10090733071168263 Validation Loss: 0.7525251507759094\n",
      "Epoch 10625: Training Loss: 0.10039133330186208 Validation Loss: 0.7522059082984924\n",
      "Epoch 10626: Training Loss: 0.10006472965081532 Validation Loss: 0.7522867918014526\n",
      "Epoch 10627: Training Loss: 0.1007745588819186 Validation Loss: 0.7521098852157593\n",
      "Epoch 10628: Training Loss: 0.10027876247962315 Validation Loss: 0.7520050406455994\n",
      "Epoch 10629: Training Loss: 0.10050858308871587 Validation Loss: 0.7521127462387085\n",
      "Epoch 10630: Training Loss: 0.10051149874925613 Validation Loss: 0.7524080276489258\n",
      "Epoch 10631: Training Loss: 0.10016370316346486 Validation Loss: 0.7526968121528625\n",
      "Epoch 10632: Training Loss: 0.1005624309182167 Validation Loss: 0.7527188658714294\n",
      "Epoch 10633: Training Loss: 0.10040327409903209 Validation Loss: 0.7522262334823608\n",
      "Epoch 10634: Training Loss: 0.1002534752090772 Validation Loss: 0.7518641352653503\n",
      "Epoch 10635: Training Loss: 0.10070405652125676 Validation Loss: 0.7520110011100769\n",
      "Epoch 10636: Training Loss: 0.09999906520048778 Validation Loss: 0.7520536184310913\n",
      "Epoch 10637: Training Loss: 0.1007905329267184 Validation Loss: 0.7520946860313416\n",
      "Epoch 10638: Training Loss: 0.10031997660795848 Validation Loss: 0.752129316329956\n",
      "Epoch 10639: Training Loss: 0.1003270372748375 Validation Loss: 0.7528125047683716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10640: Training Loss: 0.10039714227120082 Validation Loss: 0.752868115901947\n",
      "Epoch 10641: Training Loss: 0.10066072642803192 Validation Loss: 0.752414882183075\n",
      "Epoch 10642: Training Loss: 0.1001875102519989 Validation Loss: 0.7524078488349915\n",
      "Epoch 10643: Training Loss: 0.10034952809413274 Validation Loss: 0.75284343957901\n",
      "Epoch 10644: Training Loss: 0.1005115235845248 Validation Loss: 0.7529100179672241\n",
      "Epoch 10645: Training Loss: 0.10060833642880122 Validation Loss: 0.7525566220283508\n",
      "Epoch 10646: Training Loss: 0.10043409715096156 Validation Loss: 0.7523464560508728\n",
      "Epoch 10647: Training Loss: 0.10092735290527344 Validation Loss: 0.7526301145553589\n",
      "Epoch 10648: Training Loss: 0.10116948435703914 Validation Loss: 0.752898633480072\n",
      "Epoch 10649: Training Loss: 0.10028110941251119 Validation Loss: 0.7524489164352417\n",
      "Epoch 10650: Training Loss: 0.10018530984719594 Validation Loss: 0.7525466084480286\n",
      "Epoch 10651: Training Loss: 0.10052855809529622 Validation Loss: 0.752302348613739\n",
      "Epoch 10652: Training Loss: 0.1005568578839302 Validation Loss: 0.7523657083511353\n",
      "Epoch 10653: Training Loss: 0.1004777376850446 Validation Loss: 0.7528542280197144\n",
      "Epoch 10654: Training Loss: 0.1002027119199435 Validation Loss: 0.752731442451477\n",
      "Epoch 10655: Training Loss: 0.10053828358650208 Validation Loss: 0.7528616189956665\n",
      "Epoch 10656: Training Loss: 0.1002994254231453 Validation Loss: 0.7525865435600281\n",
      "Epoch 10657: Training Loss: 0.10022967805465062 Validation Loss: 0.7525835633277893\n",
      "Epoch 10658: Training Loss: 0.10072974115610123 Validation Loss: 0.7526470422744751\n",
      "Epoch 10659: Training Loss: 0.1006028080979983 Validation Loss: 0.752912163734436\n",
      "Epoch 10660: Training Loss: 0.1005733956893285 Validation Loss: 0.7526129484176636\n",
      "Epoch 10661: Training Loss: 0.1002868264913559 Validation Loss: 0.7524433135986328\n",
      "Epoch 10662: Training Loss: 0.10086460163195927 Validation Loss: 0.7526386380195618\n",
      "Epoch 10663: Training Loss: 0.10036938140789668 Validation Loss: 0.7526428699493408\n",
      "Epoch 10664: Training Loss: 0.10080478340387344 Validation Loss: 0.7527860403060913\n",
      "Epoch 10665: Training Loss: 0.10005716234445572 Validation Loss: 0.752787709236145\n",
      "Epoch 10666: Training Loss: 0.10070768495400746 Validation Loss: 0.7528620362281799\n",
      "Epoch 10667: Training Loss: 0.10047691563765208 Validation Loss: 0.7529524564743042\n",
      "Epoch 10668: Training Loss: 0.10086727390686671 Validation Loss: 0.7523453831672668\n",
      "Epoch 10669: Training Loss: 0.10034013042847316 Validation Loss: 0.7523639798164368\n",
      "Epoch 10670: Training Loss: 0.10039054850737254 Validation Loss: 0.7530131340026855\n",
      "Epoch 10671: Training Loss: 0.10017373412847519 Validation Loss: 0.7533407211303711\n",
      "Epoch 10672: Training Loss: 0.10015955567359924 Validation Loss: 0.7532799243927002\n",
      "Epoch 10673: Training Loss: 0.10062944640715916 Validation Loss: 0.7527081370353699\n",
      "Epoch 10674: Training Loss: 0.10057678818702698 Validation Loss: 0.7530612945556641\n",
      "Epoch 10675: Training Loss: 0.10027267287174861 Validation Loss: 0.7525392174720764\n",
      "Epoch 10676: Training Loss: 0.10009568681319554 Validation Loss: 0.7523882389068604\n",
      "Epoch 10677: Training Loss: 0.10047346850236256 Validation Loss: 0.7528071403503418\n",
      "Epoch 10678: Training Loss: 0.09964601198832194 Validation Loss: 0.7527021169662476\n",
      "Epoch 10679: Training Loss: 0.10042282690604527 Validation Loss: 0.7530359029769897\n",
      "Epoch 10680: Training Loss: 0.10027885437011719 Validation Loss: 0.7530046701431274\n",
      "Epoch 10681: Training Loss: 0.10017193853855133 Validation Loss: 0.7531189322471619\n",
      "Epoch 10682: Training Loss: 0.10019428034623463 Validation Loss: 0.7528889775276184\n",
      "Epoch 10683: Training Loss: 0.10041970759630203 Validation Loss: 0.7532905340194702\n",
      "Epoch 10684: Training Loss: 0.10006932417551677 Validation Loss: 0.7530504465103149\n",
      "Epoch 10685: Training Loss: 0.10003958642482758 Validation Loss: 0.752768337726593\n",
      "Epoch 10686: Training Loss: 0.1001236413915952 Validation Loss: 0.752962052822113\n",
      "Epoch 10687: Training Loss: 0.10142836968104045 Validation Loss: 0.7531708478927612\n",
      "Epoch 10688: Training Loss: 0.10065941264231999 Validation Loss: 0.7531681656837463\n",
      "Epoch 10689: Training Loss: 0.10020504395167033 Validation Loss: 0.7534780502319336\n",
      "Epoch 10690: Training Loss: 0.10035106788078944 Validation Loss: 0.7531154751777649\n",
      "Epoch 10691: Training Loss: 0.10032602896293004 Validation Loss: 0.7526912093162537\n",
      "Epoch 10692: Training Loss: 0.10000734527905782 Validation Loss: 0.7526624798774719\n",
      "Epoch 10693: Training Loss: 0.10032932211955388 Validation Loss: 0.7527849078178406\n",
      "Epoch 10694: Training Loss: 0.10007335245609283 Validation Loss: 0.7527600526809692\n",
      "Epoch 10695: Training Loss: 0.09995732208093007 Validation Loss: 0.7530768513679504\n",
      "Epoch 10696: Training Loss: 0.09986981749534607 Validation Loss: 0.7529296875\n",
      "Epoch 10697: Training Loss: 0.10026669502258301 Validation Loss: 0.7532209753990173\n",
      "Epoch 10698: Training Loss: 0.09991487115621567 Validation Loss: 0.7526799440383911\n",
      "Epoch 10699: Training Loss: 0.10002634177605312 Validation Loss: 0.752857506275177\n",
      "Epoch 10700: Training Loss: 0.10003307958443959 Validation Loss: 0.7526607513427734\n",
      "Epoch 10701: Training Loss: 0.09993358204762141 Validation Loss: 0.7528770565986633\n",
      "Epoch 10702: Training Loss: 0.1001468226313591 Validation Loss: 0.7530102133750916\n",
      "Epoch 10703: Training Loss: 0.1003610889116923 Validation Loss: 0.752724826335907\n",
      "Epoch 10704: Training Loss: 0.09999408076206844 Validation Loss: 0.7526950836181641\n",
      "Epoch 10705: Training Loss: 0.10039664059877396 Validation Loss: 0.7531193494796753\n",
      "Epoch 10706: Training Loss: 0.100033238530159 Validation Loss: 0.7531357407569885\n",
      "Epoch 10707: Training Loss: 0.10031443337599437 Validation Loss: 0.7531862854957581\n",
      "Epoch 10708: Training Loss: 0.09971540421247482 Validation Loss: 0.7531671524047852\n",
      "Epoch 10709: Training Loss: 0.10055774947007497 Validation Loss: 0.7528985142707825\n",
      "Epoch 10710: Training Loss: 0.10039638976256053 Validation Loss: 0.7525594234466553\n",
      "Epoch 10711: Training Loss: 0.10016550868749619 Validation Loss: 0.752848207950592\n",
      "Epoch 10712: Training Loss: 0.10055290162563324 Validation Loss: 0.7529048323631287\n",
      "Epoch 10713: Training Loss: 0.10007147242625554 Validation Loss: 0.7533310055732727\n",
      "Epoch 10714: Training Loss: 0.09959760804971059 Validation Loss: 0.7532074451446533\n",
      "Epoch 10715: Training Loss: 0.10024749487638474 Validation Loss: 0.7528824806213379\n",
      "Epoch 10716: Training Loss: 0.099946195880572 Validation Loss: 0.7529816031455994\n",
      "Epoch 10717: Training Loss: 0.09945182502269745 Validation Loss: 0.7532790303230286\n",
      "Epoch 10718: Training Loss: 0.10038437942663829 Validation Loss: 0.7537795305252075\n",
      "Epoch 10719: Training Loss: 0.09987556437651317 Validation Loss: 0.753250002861023\n",
      "Epoch 10720: Training Loss: 0.09993734210729599 Validation Loss: 0.7531247138977051\n",
      "Epoch 10721: Training Loss: 0.09979907174905141 Validation Loss: 0.7530657052993774\n",
      "Epoch 10722: Training Loss: 0.09979887803395589 Validation Loss: 0.7534138560295105\n",
      "Epoch 10723: Training Loss: 0.09964744249979655 Validation Loss: 0.7533164620399475\n",
      "Epoch 10724: Training Loss: 0.09991612285375595 Validation Loss: 0.7529209852218628\n",
      "Epoch 10725: Training Loss: 0.10002820442120235 Validation Loss: 0.7527852058410645\n",
      "Epoch 10726: Training Loss: 0.09991511950890224 Validation Loss: 0.7531034350395203\n",
      "Epoch 10727: Training Loss: 0.09983256707588832 Validation Loss: 0.7530955076217651\n",
      "Epoch 10728: Training Loss: 0.09976683805386226 Validation Loss: 0.7530352473258972\n",
      "Epoch 10729: Training Loss: 0.10029008487860362 Validation Loss: 0.7531270384788513\n",
      "Epoch 10730: Training Loss: 0.09994246065616608 Validation Loss: 0.753024160861969\n",
      "Epoch 10731: Training Loss: 0.09977731605370839 Validation Loss: 0.7529439330101013\n",
      "Epoch 10732: Training Loss: 0.10030873864889145 Validation Loss: 0.7530821561813354\n",
      "Epoch 10733: Training Loss: 0.09963452319304149 Validation Loss: 0.7529480457305908\n",
      "Epoch 10734: Training Loss: 0.0997244268655777 Validation Loss: 0.7532749176025391\n",
      "Epoch 10735: Training Loss: 0.09989021966854732 Validation Loss: 0.7537119388580322\n",
      "Epoch 10736: Training Loss: 0.0998588427901268 Validation Loss: 0.7535868287086487\n",
      "Epoch 10737: Training Loss: 0.10036056985457738 Validation Loss: 0.7533928751945496\n",
      "Epoch 10738: Training Loss: 0.10014723738034566 Validation Loss: 0.753180205821991\n",
      "Epoch 10739: Training Loss: 0.09994435807069142 Validation Loss: 0.752966046333313\n",
      "Epoch 10740: Training Loss: 0.09947909911473592 Validation Loss: 0.7529415488243103\n",
      "Epoch 10741: Training Loss: 0.10046335558096568 Validation Loss: 0.7528091073036194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10742: Training Loss: 0.09988603244225185 Validation Loss: 0.7531354427337646\n",
      "Epoch 10743: Training Loss: 0.0996422991156578 Validation Loss: 0.7532467842102051\n",
      "Epoch 10744: Training Loss: 0.10007595270872116 Validation Loss: 0.7533332705497742\n",
      "Epoch 10745: Training Loss: 0.09984611968199412 Validation Loss: 0.7537102103233337\n",
      "Epoch 10746: Training Loss: 0.09977866212526958 Validation Loss: 0.7534721493721008\n",
      "Epoch 10747: Training Loss: 0.10040841003259023 Validation Loss: 0.7532928586006165\n",
      "Epoch 10748: Training Loss: 0.10033911714951198 Validation Loss: 0.753022313117981\n",
      "Epoch 10749: Training Loss: 0.09994003921747208 Validation Loss: 0.7530574798583984\n",
      "Epoch 10750: Training Loss: 0.0997765635450681 Validation Loss: 0.7536324858665466\n",
      "Epoch 10751: Training Loss: 0.10072838018337886 Validation Loss: 0.754065990447998\n",
      "Epoch 10752: Training Loss: 0.09983010838429134 Validation Loss: 0.7541695237159729\n",
      "Epoch 10753: Training Loss: 0.09982335070768993 Validation Loss: 0.7540537118911743\n",
      "Epoch 10754: Training Loss: 0.09975781540075938 Validation Loss: 0.7535618543624878\n",
      "Epoch 10755: Training Loss: 0.09994212538003922 Validation Loss: 0.7534335851669312\n",
      "Epoch 10756: Training Loss: 0.09978023916482925 Validation Loss: 0.75306636095047\n",
      "Epoch 10757: Training Loss: 0.09977808346350987 Validation Loss: 0.753206729888916\n",
      "Epoch 10758: Training Loss: 0.10007295260826747 Validation Loss: 0.7537158131599426\n",
      "Epoch 10759: Training Loss: 0.09975122660398483 Validation Loss: 0.753905177116394\n",
      "Epoch 10760: Training Loss: 0.09952285140752792 Validation Loss: 0.7539455890655518\n",
      "Epoch 10761: Training Loss: 0.0999288558959961 Validation Loss: 0.7535454034805298\n",
      "Epoch 10762: Training Loss: 0.10032196839650472 Validation Loss: 0.7534182667732239\n",
      "Epoch 10763: Training Loss: 0.09957929948965709 Validation Loss: 0.7533525228500366\n",
      "Epoch 10764: Training Loss: 0.09963108599185944 Validation Loss: 0.7530406713485718\n",
      "Epoch 10765: Training Loss: 0.1014297679066658 Validation Loss: 0.7526741623878479\n",
      "Epoch 10766: Training Loss: 0.09988640993833542 Validation Loss: 0.7528378963470459\n",
      "Epoch 10767: Training Loss: 0.09963551660378774 Validation Loss: 0.7532708644866943\n",
      "Epoch 10768: Training Loss: 0.09966507802406947 Validation Loss: 0.753815770149231\n",
      "Epoch 10769: Training Loss: 0.10011549045642217 Validation Loss: 0.7536959052085876\n",
      "Epoch 10770: Training Loss: 0.10007546097040176 Validation Loss: 0.7533670663833618\n",
      "Epoch 10771: Training Loss: 0.09979370484749477 Validation Loss: 0.7541589140892029\n",
      "Epoch 10772: Training Loss: 0.09992791960636775 Validation Loss: 0.75399249792099\n",
      "Epoch 10773: Training Loss: 0.09968800594409306 Validation Loss: 0.7541807293891907\n",
      "Epoch 10774: Training Loss: 0.09959228833516438 Validation Loss: 0.7537649869918823\n",
      "Epoch 10775: Training Loss: 0.0995305006702741 Validation Loss: 0.7533385157585144\n",
      "Epoch 10776: Training Loss: 0.10055741171042125 Validation Loss: 0.7532405257225037\n",
      "Epoch 10777: Training Loss: 0.09997465958197911 Validation Loss: 0.7538350820541382\n",
      "Epoch 10778: Training Loss: 0.10000747442245483 Validation Loss: 0.7537705898284912\n",
      "Epoch 10779: Training Loss: 0.09971993416547775 Validation Loss: 0.7539368867874146\n",
      "Epoch 10780: Training Loss: 0.09977118174235027 Validation Loss: 0.7536312937736511\n",
      "Epoch 10781: Training Loss: 0.09970997273921967 Validation Loss: 0.7537392973899841\n",
      "Epoch 10782: Training Loss: 0.09952514121929805 Validation Loss: 0.7532303929328918\n",
      "Epoch 10783: Training Loss: 0.09955784181753795 Validation Loss: 0.7531750202178955\n",
      "Epoch 10784: Training Loss: 0.0995462288459142 Validation Loss: 0.7533883452415466\n",
      "Epoch 10785: Training Loss: 0.09918432682752609 Validation Loss: 0.753657877445221\n",
      "Epoch 10786: Training Loss: 0.0995238721370697 Validation Loss: 0.7540814280509949\n",
      "Epoch 10787: Training Loss: 0.09919515748818715 Validation Loss: 0.7544177174568176\n",
      "Epoch 10788: Training Loss: 0.09913978228966396 Validation Loss: 0.7538556456565857\n",
      "Epoch 10789: Training Loss: 0.09946909795204799 Validation Loss: 0.7537577152252197\n",
      "Epoch 10790: Training Loss: 0.09979840864737828 Validation Loss: 0.7536490559577942\n",
      "Epoch 10791: Training Loss: 0.09939088424046834 Validation Loss: 0.7536974549293518\n",
      "Epoch 10792: Training Loss: 0.10015800098578136 Validation Loss: 0.753905713558197\n",
      "Epoch 10793: Training Loss: 0.10009768605232239 Validation Loss: 0.7540620565414429\n",
      "Epoch 10794: Training Loss: 0.09948852409919103 Validation Loss: 0.7541699409484863\n",
      "Epoch 10795: Training Loss: 0.09988292803366979 Validation Loss: 0.7540960311889648\n",
      "Epoch 10796: Training Loss: 0.09955249975124995 Validation Loss: 0.753974437713623\n",
      "Epoch 10797: Training Loss: 0.09989285717407863 Validation Loss: 0.7539634704589844\n",
      "Epoch 10798: Training Loss: 0.09941256294647853 Validation Loss: 0.7543527483940125\n",
      "Epoch 10799: Training Loss: 0.09968894720077515 Validation Loss: 0.7536348700523376\n",
      "Epoch 10800: Training Loss: 0.09920277446508408 Validation Loss: 0.7538266181945801\n",
      "Epoch 10801: Training Loss: 0.0997878834605217 Validation Loss: 0.7536109089851379\n",
      "Epoch 10802: Training Loss: 0.09917300939559937 Validation Loss: 0.7539007663726807\n",
      "Epoch 10803: Training Loss: 0.09843567510445912 Validation Loss: 0.7539138197898865\n",
      "Epoch 10804: Training Loss: 0.09918519606192906 Validation Loss: 0.7532926797866821\n",
      "Epoch 10805: Training Loss: 0.09933142860730489 Validation Loss: 0.7535208463668823\n",
      "Epoch 10806: Training Loss: 0.09932464609543483 Validation Loss: 0.7538331151008606\n",
      "Epoch 10807: Training Loss: 0.09961801022291183 Validation Loss: 0.7545116543769836\n",
      "Epoch 10808: Training Loss: 0.09943770368893941 Validation Loss: 0.754364550113678\n",
      "Epoch 10809: Training Loss: 0.10025292634963989 Validation Loss: 0.7543672919273376\n",
      "Epoch 10810: Training Loss: 0.09910261382659276 Validation Loss: 0.7545421719551086\n",
      "Epoch 10811: Training Loss: 0.0996656393011411 Validation Loss: 0.7540286779403687\n",
      "Epoch 10812: Training Loss: 0.09925299386183421 Validation Loss: 0.7541913390159607\n",
      "Epoch 10813: Training Loss: 0.09950416783491771 Validation Loss: 0.7543537020683289\n",
      "Epoch 10814: Training Loss: 0.09888049711783727 Validation Loss: 0.7541730403900146\n",
      "Epoch 10815: Training Loss: 0.09934928516546886 Validation Loss: 0.7545125484466553\n",
      "Epoch 10816: Training Loss: 0.09961735208829244 Validation Loss: 0.7540223598480225\n",
      "Epoch 10817: Training Loss: 0.09949653844038646 Validation Loss: 0.754078209400177\n",
      "Epoch 10818: Training Loss: 0.09914704660574596 Validation Loss: 0.7544827461242676\n",
      "Epoch 10819: Training Loss: 0.09952058146397273 Validation Loss: 0.7543045282363892\n",
      "Epoch 10820: Training Loss: 0.10004610568284988 Validation Loss: 0.7548393607139587\n",
      "Epoch 10821: Training Loss: 0.09917909900347392 Validation Loss: 0.7547478079795837\n",
      "Epoch 10822: Training Loss: 0.09895249456167221 Validation Loss: 0.7544421553611755\n",
      "Epoch 10823: Training Loss: 0.09898619602123897 Validation Loss: 0.7540883421897888\n",
      "Epoch 10824: Training Loss: 0.1000027706225713 Validation Loss: 0.7542547583580017\n",
      "Epoch 10825: Training Loss: 0.09920698901017506 Validation Loss: 0.7543802261352539\n",
      "Epoch 10826: Training Loss: 0.0988773579398791 Validation Loss: 0.7544025778770447\n",
      "Epoch 10827: Training Loss: 0.09927136699358623 Validation Loss: 0.7541828155517578\n",
      "Epoch 10828: Training Loss: 0.09927780677874883 Validation Loss: 0.7541921734809875\n",
      "Epoch 10829: Training Loss: 0.09990742554267247 Validation Loss: 0.753926694393158\n",
      "Epoch 10830: Training Loss: 0.09923394024372101 Validation Loss: 0.7540006637573242\n",
      "Epoch 10831: Training Loss: 0.09908994783957799 Validation Loss: 0.7545236945152283\n",
      "Epoch 10832: Training Loss: 0.09980716307957967 Validation Loss: 0.7545397877693176\n",
      "Epoch 10833: Training Loss: 0.09979576865832011 Validation Loss: 0.7543061971664429\n",
      "Epoch 10834: Training Loss: 0.09935679038365682 Validation Loss: 0.7547643184661865\n",
      "Epoch 10835: Training Loss: 0.09946265816688538 Validation Loss: 0.755034863948822\n",
      "Epoch 10836: Training Loss: 0.09900173793236415 Validation Loss: 0.7548004388809204\n",
      "Epoch 10837: Training Loss: 0.10055922468503316 Validation Loss: 0.7540035247802734\n",
      "Epoch 10838: Training Loss: 0.09921516229708989 Validation Loss: 0.7541779279708862\n",
      "Epoch 10839: Training Loss: 0.0991828516125679 Validation Loss: 0.7538861632347107\n",
      "Epoch 10840: Training Loss: 0.09935642778873444 Validation Loss: 0.7541528344154358\n",
      "Epoch 10841: Training Loss: 0.09971230725447337 Validation Loss: 0.7545837759971619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10842: Training Loss: 0.09897961467504501 Validation Loss: 0.7549362182617188\n",
      "Epoch 10843: Training Loss: 0.0992409015695254 Validation Loss: 0.7544350028038025\n",
      "Epoch 10844: Training Loss: 0.09886251389980316 Validation Loss: 0.7543972730636597\n",
      "Epoch 10845: Training Loss: 0.09973022590080897 Validation Loss: 0.7541833519935608\n",
      "Epoch 10846: Training Loss: 0.0996104155977567 Validation Loss: 0.7540948390960693\n",
      "Epoch 10847: Training Loss: 0.0987245241800944 Validation Loss: 0.7541407942771912\n",
      "Epoch 10848: Training Loss: 0.09911909699440002 Validation Loss: 0.7543056011199951\n",
      "Epoch 10849: Training Loss: 0.09897608309984207 Validation Loss: 0.7545047998428345\n",
      "Epoch 10850: Training Loss: 0.10018092890580495 Validation Loss: 0.7542834281921387\n",
      "Epoch 10851: Training Loss: 0.09901411831378937 Validation Loss: 0.7542005777359009\n",
      "Epoch 10852: Training Loss: 0.09987946103016536 Validation Loss: 0.7544631958007812\n",
      "Epoch 10853: Training Loss: 0.09927962968746822 Validation Loss: 0.7546120285987854\n",
      "Epoch 10854: Training Loss: 0.09932037194569905 Validation Loss: 0.7546660304069519\n",
      "Epoch 10855: Training Loss: 0.09907151758670807 Validation Loss: 0.7548447847366333\n",
      "Epoch 10856: Training Loss: 0.09899091720581055 Validation Loss: 0.754584014415741\n",
      "Epoch 10857: Training Loss: 0.09961410363515218 Validation Loss: 0.7544939517974854\n",
      "Epoch 10858: Training Loss: 0.09884736935297649 Validation Loss: 0.7544702887535095\n",
      "Epoch 10859: Training Loss: 0.09929704417785008 Validation Loss: 0.7545823454856873\n",
      "Epoch 10860: Training Loss: 0.09909287343422572 Validation Loss: 0.7544403076171875\n",
      "Epoch 10861: Training Loss: 0.09902711709340413 Validation Loss: 0.7546877861022949\n",
      "Epoch 10862: Training Loss: 0.09922728687524796 Validation Loss: 0.7546634674072266\n",
      "Epoch 10863: Training Loss: 0.09891624748706818 Validation Loss: 0.7545379996299744\n",
      "Epoch 10864: Training Loss: 0.09893491864204407 Validation Loss: 0.7544255256652832\n",
      "Epoch 10865: Training Loss: 0.09944063673416774 Validation Loss: 0.7542523145675659\n",
      "Epoch 10866: Training Loss: 0.09909514337778091 Validation Loss: 0.7544970512390137\n",
      "Epoch 10867: Training Loss: 0.09911865492661794 Validation Loss: 0.7547549605369568\n",
      "Epoch 10868: Training Loss: 0.09904551257689793 Validation Loss: 0.755307137966156\n",
      "Epoch 10869: Training Loss: 0.09918318688869476 Validation Loss: 0.7551074028015137\n",
      "Epoch 10870: Training Loss: 0.09919182459513347 Validation Loss: 0.754778265953064\n",
      "Epoch 10871: Training Loss: 0.098564513027668 Validation Loss: 0.7547898888587952\n",
      "Epoch 10872: Training Loss: 0.09910714626312256 Validation Loss: 0.7545381784439087\n",
      "Epoch 10873: Training Loss: 0.09899958471457164 Validation Loss: 0.7543722987174988\n",
      "Epoch 10874: Training Loss: 0.09898202121257782 Validation Loss: 0.7546867728233337\n",
      "Epoch 10875: Training Loss: 0.10007832696040471 Validation Loss: 0.7551538944244385\n",
      "Epoch 10876: Training Loss: 0.09895794341961543 Validation Loss: 0.7550088167190552\n",
      "Epoch 10877: Training Loss: 0.09892506649096806 Validation Loss: 0.7545526027679443\n",
      "Epoch 10878: Training Loss: 0.09887314836184184 Validation Loss: 0.7544660568237305\n",
      "Epoch 10879: Training Loss: 0.09917505085468292 Validation Loss: 0.7545076608657837\n",
      "Epoch 10880: Training Loss: 0.0990973562002182 Validation Loss: 0.7545650601387024\n",
      "Epoch 10881: Training Loss: 0.09895261625448863 Validation Loss: 0.7548002600669861\n",
      "Epoch 10882: Training Loss: 0.09902588278055191 Validation Loss: 0.7549747228622437\n",
      "Epoch 10883: Training Loss: 0.0988922268152237 Validation Loss: 0.754945695400238\n",
      "Epoch 10884: Training Loss: 0.09905169407526652 Validation Loss: 0.7547996044158936\n",
      "Epoch 10885: Training Loss: 0.09891467541456223 Validation Loss: 0.7546373009681702\n",
      "Epoch 10886: Training Loss: 0.09883592774470647 Validation Loss: 0.7546785473823547\n",
      "Epoch 10887: Training Loss: 0.09886946529150009 Validation Loss: 0.7548009753227234\n",
      "Epoch 10888: Training Loss: 0.09903561572233836 Validation Loss: 0.7549995183944702\n",
      "Epoch 10889: Training Loss: 0.09879334519306819 Validation Loss: 0.7553950548171997\n",
      "Epoch 10890: Training Loss: 0.09924484044313431 Validation Loss: 0.7551783323287964\n",
      "Epoch 10891: Training Loss: 0.09921974937121074 Validation Loss: 0.7550696134567261\n",
      "Epoch 10892: Training Loss: 0.09893962492545445 Validation Loss: 0.754936695098877\n",
      "Epoch 10893: Training Loss: 0.09891280035177867 Validation Loss: 0.7547106146812439\n",
      "Epoch 10894: Training Loss: 0.09976318975289662 Validation Loss: 0.7547557950019836\n",
      "Epoch 10895: Training Loss: 0.09874744961659114 Validation Loss: 0.7547687888145447\n",
      "Epoch 10896: Training Loss: 0.09877067804336548 Validation Loss: 0.754788339138031\n",
      "Epoch 10897: Training Loss: 0.09888631602128346 Validation Loss: 0.7551469802856445\n",
      "Epoch 10898: Training Loss: 0.09895837803681691 Validation Loss: 0.7549644112586975\n",
      "Epoch 10899: Training Loss: 0.09934808810551961 Validation Loss: 0.7548210024833679\n",
      "Epoch 10900: Training Loss: 0.09893213709195454 Validation Loss: 0.7547118067741394\n",
      "Epoch 10901: Training Loss: 0.09899457295735677 Validation Loss: 0.7544945478439331\n",
      "Epoch 10902: Training Loss: 0.09876629213492076 Validation Loss: 0.7548415660858154\n",
      "Epoch 10903: Training Loss: 0.09910593430201213 Validation Loss: 0.7547860145568848\n",
      "Epoch 10904: Training Loss: 0.09867071111996968 Validation Loss: 0.7548098564147949\n",
      "Epoch 10905: Training Loss: 0.09862044701973598 Validation Loss: 0.7556995749473572\n",
      "Epoch 10906: Training Loss: 0.0993672286470731 Validation Loss: 0.7553144693374634\n",
      "Epoch 10907: Training Loss: 0.0990264688928922 Validation Loss: 0.7552613019943237\n",
      "Epoch 10908: Training Loss: 0.09891169269879659 Validation Loss: 0.7551252841949463\n",
      "Epoch 10909: Training Loss: 0.09857131789127986 Validation Loss: 0.7548170685768127\n",
      "Epoch 10910: Training Loss: 0.09885802368323009 Validation Loss: 0.7549831867218018\n",
      "Epoch 10911: Training Loss: 0.09878007819255193 Validation Loss: 0.7550586462020874\n",
      "Epoch 10912: Training Loss: 0.09862196197112401 Validation Loss: 0.7551558613777161\n",
      "Epoch 10913: Training Loss: 0.0990428055326144 Validation Loss: 0.7555949687957764\n",
      "Epoch 10914: Training Loss: 0.0986449271440506 Validation Loss: 0.7551611065864563\n",
      "Epoch 10915: Training Loss: 0.09841268261273702 Validation Loss: 0.7551940083503723\n",
      "Epoch 10916: Training Loss: 0.09859553972880046 Validation Loss: 0.7550129294395447\n",
      "Epoch 10917: Training Loss: 0.0988532801469167 Validation Loss: 0.7549552917480469\n",
      "Epoch 10918: Training Loss: 0.09813087681929271 Validation Loss: 0.7553805708885193\n",
      "Epoch 10919: Training Loss: 0.09890945255756378 Validation Loss: 0.7552580237388611\n",
      "Epoch 10920: Training Loss: 0.09867775191863377 Validation Loss: 0.7552708387374878\n",
      "Epoch 10921: Training Loss: 0.09851878881454468 Validation Loss: 0.7555768489837646\n",
      "Epoch 10922: Training Loss: 0.0986515407760938 Validation Loss: 0.7557790279388428\n",
      "Epoch 10923: Training Loss: 0.09904943654934566 Validation Loss: 0.755836009979248\n",
      "Epoch 10924: Training Loss: 0.09862583875656128 Validation Loss: 0.7556469440460205\n",
      "Epoch 10925: Training Loss: 0.09859958042701085 Validation Loss: 0.7548499703407288\n",
      "Epoch 10926: Training Loss: 0.09871124724547069 Validation Loss: 0.7551212906837463\n",
      "Epoch 10927: Training Loss: 0.0986665536959966 Validation Loss: 0.7548686265945435\n",
      "Epoch 10928: Training Loss: 0.0989622101187706 Validation Loss: 0.7546039819717407\n",
      "Epoch 10929: Training Loss: 0.09877406805753708 Validation Loss: 0.7547881603240967\n",
      "Epoch 10930: Training Loss: 0.09846080591281255 Validation Loss: 0.7550194263458252\n",
      "Epoch 10931: Training Loss: 0.09854671359062195 Validation Loss: 0.7554341554641724\n",
      "Epoch 10932: Training Loss: 0.09873683998982112 Validation Loss: 0.7555009722709656\n",
      "Epoch 10933: Training Loss: 0.09861073891321818 Validation Loss: 0.7554612755775452\n",
      "Epoch 10934: Training Loss: 0.09869760771592458 Validation Loss: 0.7555586695671082\n",
      "Epoch 10935: Training Loss: 0.0989288588364919 Validation Loss: 0.7554569244384766\n",
      "Epoch 10936: Training Loss: 0.09859974930683772 Validation Loss: 0.7553251385688782\n",
      "Epoch 10937: Training Loss: 0.09855340421199799 Validation Loss: 0.7555158734321594\n",
      "Epoch 10938: Training Loss: 0.09869448592265447 Validation Loss: 0.7551169991493225\n",
      "Epoch 10939: Training Loss: 0.09849394361178081 Validation Loss: 0.755585253238678\n",
      "Epoch 10940: Training Loss: 0.09869631628195445 Validation Loss: 0.7555558085441589\n",
      "Epoch 10941: Training Loss: 0.09869542966286342 Validation Loss: 0.755352795124054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10942: Training Loss: 0.09856659670670827 Validation Loss: 0.7553138732910156\n",
      "Epoch 10943: Training Loss: 0.09848353266716003 Validation Loss: 0.7550610303878784\n",
      "Epoch 10944: Training Loss: 0.09915043661991756 Validation Loss: 0.7548598051071167\n",
      "Epoch 10945: Training Loss: 0.09874453395605087 Validation Loss: 0.7549840807914734\n",
      "Epoch 10946: Training Loss: 0.09856691211462021 Validation Loss: 0.7548819184303284\n",
      "Epoch 10947: Training Loss: 0.09883642196655273 Validation Loss: 0.7556513547897339\n",
      "Epoch 10948: Training Loss: 0.09879841407140096 Validation Loss: 0.7559213638305664\n",
      "Epoch 10949: Training Loss: 0.0983701969186465 Validation Loss: 0.7562472224235535\n",
      "Epoch 10950: Training Loss: 0.0991055245200793 Validation Loss: 0.7554983496665955\n",
      "Epoch 10951: Training Loss: 0.09857786943515141 Validation Loss: 0.7549043893814087\n",
      "Epoch 10952: Training Loss: 0.0986401637395223 Validation Loss: 0.7549452185630798\n",
      "Epoch 10953: Training Loss: 0.0989623839656512 Validation Loss: 0.7553995251655579\n",
      "Epoch 10954: Training Loss: 0.09887553006410599 Validation Loss: 0.7550421953201294\n",
      "Epoch 10955: Training Loss: 0.09840216984351476 Validation Loss: 0.7552680373191833\n",
      "Epoch 10956: Training Loss: 0.0982586070895195 Validation Loss: 0.755776047706604\n",
      "Epoch 10957: Training Loss: 0.09871105353037517 Validation Loss: 0.7558616399765015\n",
      "Epoch 10958: Training Loss: 0.09839593867460887 Validation Loss: 0.755744457244873\n",
      "Epoch 10959: Training Loss: 0.09856954962015152 Validation Loss: 0.7557949423789978\n",
      "Epoch 10960: Training Loss: 0.09872394055128098 Validation Loss: 0.7561346292495728\n",
      "Epoch 10961: Training Loss: 0.09876165042320888 Validation Loss: 0.7557684183120728\n",
      "Epoch 10962: Training Loss: 0.0982540026307106 Validation Loss: 0.7554314136505127\n",
      "Epoch 10963: Training Loss: 0.09857084353764851 Validation Loss: 0.7548835873603821\n",
      "Epoch 10964: Training Loss: 0.09927341590325038 Validation Loss: 0.7550590634346008\n",
      "Epoch 10965: Training Loss: 0.09845956911643346 Validation Loss: 0.755748987197876\n",
      "Epoch 10966: Training Loss: 0.09854919215043385 Validation Loss: 0.7559764981269836\n",
      "Epoch 10967: Training Loss: 0.09843046714862187 Validation Loss: 0.7561807036399841\n",
      "Epoch 10968: Training Loss: 0.09849331031243007 Validation Loss: 0.7556474804878235\n",
      "Epoch 10969: Training Loss: 0.0983206257224083 Validation Loss: 0.7556785941123962\n",
      "Epoch 10970: Training Loss: 0.0981950710217158 Validation Loss: 0.7551714777946472\n",
      "Epoch 10971: Training Loss: 0.09857987612485886 Validation Loss: 0.7549014091491699\n",
      "Epoch 10972: Training Loss: 0.09890527278184891 Validation Loss: 0.7551295161247253\n",
      "Epoch 10973: Training Loss: 0.09849419941504796 Validation Loss: 0.7554347515106201\n",
      "Epoch 10974: Training Loss: 0.09847858548164368 Validation Loss: 0.7560227513313293\n",
      "Epoch 10975: Training Loss: 0.09890790035327275 Validation Loss: 0.7561131119728088\n",
      "Epoch 10976: Training Loss: 0.09834640224774678 Validation Loss: 0.755976676940918\n",
      "Epoch 10977: Training Loss: 0.09829951326052348 Validation Loss: 0.7559908032417297\n",
      "Epoch 10978: Training Loss: 0.09852381547292073 Validation Loss: 0.7558333277702332\n",
      "Epoch 10979: Training Loss: 0.09836774071057637 Validation Loss: 0.7554051280021667\n",
      "Epoch 10980: Training Loss: 0.09843896081050237 Validation Loss: 0.7556160092353821\n",
      "Epoch 10981: Training Loss: 0.09775022665659587 Validation Loss: 0.7558974623680115\n",
      "Epoch 10982: Training Loss: 0.09828528016805649 Validation Loss: 0.7559759616851807\n",
      "Epoch 10983: Training Loss: 0.09812924762566884 Validation Loss: 0.7557674050331116\n",
      "Epoch 10984: Training Loss: 0.09838613867759705 Validation Loss: 0.755792498588562\n",
      "Epoch 10985: Training Loss: 0.09875097622474034 Validation Loss: 0.7561812400817871\n",
      "Epoch 10986: Training Loss: 0.09851844360431035 Validation Loss: 0.7558301091194153\n",
      "Epoch 10987: Training Loss: 0.09848181903362274 Validation Loss: 0.7558096647262573\n",
      "Epoch 10988: Training Loss: 0.09856174637873967 Validation Loss: 0.7564849853515625\n",
      "Epoch 10989: Training Loss: 0.09819156179825465 Validation Loss: 0.7562215924263\n",
      "Epoch 10990: Training Loss: 0.09824992716312408 Validation Loss: 0.7556628584861755\n",
      "Epoch 10991: Training Loss: 0.09899649272362392 Validation Loss: 0.7554700374603271\n",
      "Epoch 10992: Training Loss: 0.09878804534673691 Validation Loss: 0.7561166882514954\n",
      "Epoch 10993: Training Loss: 0.09849249323209126 Validation Loss: 0.7560334801673889\n",
      "Epoch 10994: Training Loss: 0.09826707591613133 Validation Loss: 0.7561397552490234\n",
      "Epoch 10995: Training Loss: 0.09840875118970871 Validation Loss: 0.7560794353485107\n",
      "Epoch 10996: Training Loss: 0.09818015247583389 Validation Loss: 0.756641149520874\n",
      "Epoch 10997: Training Loss: 0.09806488702694575 Validation Loss: 0.7564620971679688\n",
      "Epoch 10998: Training Loss: 0.09858260303735733 Validation Loss: 0.7563964128494263\n",
      "Epoch 10999: Training Loss: 0.09881124645471573 Validation Loss: 0.7558046579360962\n",
      "Epoch 11000: Training Loss: 0.09836654365062714 Validation Loss: 0.7558342814445496\n",
      "Epoch 11001: Training Loss: 0.09833610554536183 Validation Loss: 0.7558340430259705\n",
      "Epoch 11002: Training Loss: 0.0982452780008316 Validation Loss: 0.7556596398353577\n",
      "Epoch 11003: Training Loss: 0.09867902596791585 Validation Loss: 0.7556189298629761\n",
      "Epoch 11004: Training Loss: 0.09810331960519154 Validation Loss: 0.7558476328849792\n",
      "Epoch 11005: Training Loss: 0.09843396892150243 Validation Loss: 0.7559326887130737\n",
      "Epoch 11006: Training Loss: 0.09795699765284856 Validation Loss: 0.7560160160064697\n",
      "Epoch 11007: Training Loss: 0.09981954842805862 Validation Loss: 0.7561625838279724\n",
      "Epoch 11008: Training Loss: 0.09840118636687596 Validation Loss: 0.7560707330703735\n",
      "Epoch 11009: Training Loss: 0.09818046043316524 Validation Loss: 0.7561313509941101\n",
      "Epoch 11010: Training Loss: 0.09818822393814723 Validation Loss: 0.7560896277427673\n",
      "Epoch 11011: Training Loss: 0.0985306700070699 Validation Loss: 0.7564811110496521\n",
      "Epoch 11012: Training Loss: 0.09800814340511958 Validation Loss: 0.7564134001731873\n",
      "Epoch 11013: Training Loss: 0.09800263245900472 Validation Loss: 0.7565730214118958\n",
      "Epoch 11014: Training Loss: 0.09800715496142705 Validation Loss: 0.7564797401428223\n",
      "Epoch 11015: Training Loss: 0.09856539219617844 Validation Loss: 0.7562630772590637\n",
      "Epoch 11016: Training Loss: 0.09829453378915787 Validation Loss: 0.7561776041984558\n",
      "Epoch 11017: Training Loss: 0.09835266570250194 Validation Loss: 0.7559016942977905\n",
      "Epoch 11018: Training Loss: 0.09826157242059708 Validation Loss: 0.7560012340545654\n",
      "Epoch 11019: Training Loss: 0.09878422319889069 Validation Loss: 0.7561467289924622\n",
      "Epoch 11020: Training Loss: 0.0983060027162234 Validation Loss: 0.7563015818595886\n",
      "Epoch 11021: Training Loss: 0.09825259695450465 Validation Loss: 0.7564389705657959\n",
      "Epoch 11022: Training Loss: 0.09832982470591863 Validation Loss: 0.7563204765319824\n",
      "Epoch 11023: Training Loss: 0.09826447069644928 Validation Loss: 0.7562127113342285\n",
      "Epoch 11024: Training Loss: 0.09873570005098979 Validation Loss: 0.7562220692634583\n",
      "Epoch 11025: Training Loss: 0.0979911411801974 Validation Loss: 0.7563899755477905\n",
      "Epoch 11026: Training Loss: 0.09847632547219594 Validation Loss: 0.756131112575531\n",
      "Epoch 11027: Training Loss: 0.09815368056297302 Validation Loss: 0.7559874653816223\n",
      "Epoch 11028: Training Loss: 0.09826600799957912 Validation Loss: 0.7563993334770203\n",
      "Epoch 11029: Training Loss: 0.09830120454231898 Validation Loss: 0.7561411261558533\n",
      "Epoch 11030: Training Loss: 0.09890489528576533 Validation Loss: 0.7567136287689209\n",
      "Epoch 11031: Training Loss: 0.09816306084394455 Validation Loss: 0.756585955619812\n",
      "Epoch 11032: Training Loss: 0.09847783048947652 Validation Loss: 0.756402313709259\n",
      "Epoch 11033: Training Loss: 0.09824288388093312 Validation Loss: 0.7562503814697266\n",
      "Epoch 11034: Training Loss: 0.09823736548423767 Validation Loss: 0.7563576698303223\n",
      "Epoch 11035: Training Loss: 0.09807554135719936 Validation Loss: 0.7566471099853516\n",
      "Epoch 11036: Training Loss: 0.09814553956190745 Validation Loss: 0.7564597129821777\n",
      "Epoch 11037: Training Loss: 0.09793443232774734 Validation Loss: 0.756102979183197\n",
      "Epoch 11038: Training Loss: 0.09782848507165909 Validation Loss: 0.7559227347373962\n",
      "Epoch 11039: Training Loss: 0.09794055422147115 Validation Loss: 0.7564805150032043\n",
      "Epoch 11040: Training Loss: 0.09836709499359131 Validation Loss: 0.756594717502594\n",
      "Epoch 11041: Training Loss: 0.0981201281150182 Validation Loss: 0.7567068338394165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11042: Training Loss: 0.09848868101835251 Validation Loss: 0.7569141387939453\n",
      "Epoch 11043: Training Loss: 0.09828570733467738 Validation Loss: 0.7564160227775574\n",
      "Epoch 11044: Training Loss: 0.09880313525597255 Validation Loss: 0.7563915252685547\n",
      "Epoch 11045: Training Loss: 0.0978575274348259 Validation Loss: 0.7566206455230713\n",
      "Epoch 11046: Training Loss: 0.09787842134634654 Validation Loss: 0.7568253874778748\n",
      "Epoch 11047: Training Loss: 0.09814632932345073 Validation Loss: 0.7566956877708435\n",
      "Epoch 11048: Training Loss: 0.09792938083410263 Validation Loss: 0.7566936612129211\n",
      "Epoch 11049: Training Loss: 0.09834047903617223 Validation Loss: 0.7564945220947266\n",
      "Epoch 11050: Training Loss: 0.09808571636676788 Validation Loss: 0.755812406539917\n",
      "Epoch 11051: Training Loss: 0.09802753974994023 Validation Loss: 0.7559261322021484\n",
      "Epoch 11052: Training Loss: 0.09862119952837627 Validation Loss: 0.756438672542572\n",
      "Epoch 11053: Training Loss: 0.09814497828483582 Validation Loss: 0.7568003535270691\n",
      "Epoch 11054: Training Loss: 0.09810510277748108 Validation Loss: 0.7569016814231873\n",
      "Epoch 11055: Training Loss: 0.09793978681166966 Validation Loss: 0.7570940256118774\n",
      "Epoch 11056: Training Loss: 0.09810681144396464 Validation Loss: 0.7564182877540588\n",
      "Epoch 11057: Training Loss: 0.09802117943763733 Validation Loss: 0.7564958333969116\n",
      "Epoch 11058: Training Loss: 0.09806090344985326 Validation Loss: 0.7565444111824036\n",
      "Epoch 11059: Training Loss: 0.09793044130007426 Validation Loss: 0.7561579346656799\n",
      "Epoch 11060: Training Loss: 0.09809942295153935 Validation Loss: 0.7566218972206116\n",
      "Epoch 11061: Training Loss: 0.09791306406259537 Validation Loss: 0.7568981051445007\n",
      "Epoch 11062: Training Loss: 0.09826154510180156 Validation Loss: 0.7560141682624817\n",
      "Epoch 11063: Training Loss: 0.0978723516066869 Validation Loss: 0.7560490965843201\n",
      "Epoch 11064: Training Loss: 0.09788433959086736 Validation Loss: 0.7561701536178589\n",
      "Epoch 11065: Training Loss: 0.09792460004488628 Validation Loss: 0.7562409043312073\n",
      "Epoch 11066: Training Loss: 0.09755968550841014 Validation Loss: 0.7565338611602783\n",
      "Epoch 11067: Training Loss: 0.09857004880905151 Validation Loss: 0.7566381096839905\n",
      "Epoch 11068: Training Loss: 0.09806749721368153 Validation Loss: 0.756696343421936\n",
      "Epoch 11069: Training Loss: 0.0980919674038887 Validation Loss: 0.7566264271736145\n",
      "Epoch 11070: Training Loss: 0.09786235044399898 Validation Loss: 0.7563797235488892\n",
      "Epoch 11071: Training Loss: 0.09887006133794785 Validation Loss: 0.7566224336624146\n",
      "Epoch 11072: Training Loss: 0.09826906522115071 Validation Loss: 0.7561624646186829\n",
      "Epoch 11073: Training Loss: 0.09795074164867401 Validation Loss: 0.7559972405433655\n",
      "Epoch 11074: Training Loss: 0.09767254441976547 Validation Loss: 0.7561728954315186\n",
      "Epoch 11075: Training Loss: 0.09870483726263046 Validation Loss: 0.7565754652023315\n",
      "Epoch 11076: Training Loss: 0.09781643003225327 Validation Loss: 0.7571238875389099\n",
      "Epoch 11077: Training Loss: 0.09779672821362813 Validation Loss: 0.7576226592063904\n",
      "Epoch 11078: Training Loss: 0.09748686850070953 Validation Loss: 0.7572208642959595\n",
      "Epoch 11079: Training Loss: 0.09800836940606435 Validation Loss: 0.7565697431564331\n",
      "Epoch 11080: Training Loss: 0.09712378929058711 Validation Loss: 0.7563902735710144\n",
      "Epoch 11081: Training Loss: 0.09833378344774246 Validation Loss: 0.756423830986023\n",
      "Epoch 11082: Training Loss: 0.09787307182947795 Validation Loss: 0.7566642165184021\n",
      "Epoch 11083: Training Loss: 0.09767660995324452 Validation Loss: 0.7570427656173706\n",
      "Epoch 11084: Training Loss: 0.09770684689283371 Validation Loss: 0.7572939395904541\n",
      "Epoch 11085: Training Loss: 0.09774047633012135 Validation Loss: 0.7571291923522949\n",
      "Epoch 11086: Training Loss: 0.09775884946187337 Validation Loss: 0.7564554810523987\n",
      "Epoch 11087: Training Loss: 0.09832292050123215 Validation Loss: 0.7567077875137329\n",
      "Epoch 11088: Training Loss: 0.0979129175345103 Validation Loss: 0.7568625807762146\n",
      "Epoch 11089: Training Loss: 0.09788417319456737 Validation Loss: 0.7565379738807678\n",
      "Epoch 11090: Training Loss: 0.09758299092451732 Validation Loss: 0.7562052011489868\n",
      "Epoch 11091: Training Loss: 0.09778635203838348 Validation Loss: 0.7562547326087952\n",
      "Epoch 11092: Training Loss: 0.0975703274210294 Validation Loss: 0.7564031481742859\n",
      "Epoch 11093: Training Loss: 0.09795199086268742 Validation Loss: 0.7568443417549133\n",
      "Epoch 11094: Training Loss: 0.09767561902602513 Validation Loss: 0.7570366263389587\n",
      "Epoch 11095: Training Loss: 0.09755990405877431 Validation Loss: 0.7570607662200928\n",
      "Epoch 11096: Training Loss: 0.0976052184899648 Validation Loss: 0.7569328546524048\n",
      "Epoch 11097: Training Loss: 0.09812857458988826 Validation Loss: 0.756928026676178\n",
      "Epoch 11098: Training Loss: 0.09712957342465718 Validation Loss: 0.756779670715332\n",
      "Epoch 11099: Training Loss: 0.09749468912680943 Validation Loss: 0.7566050887107849\n",
      "Epoch 11100: Training Loss: 0.0980302095413208 Validation Loss: 0.7565687894821167\n",
      "Epoch 11101: Training Loss: 0.09944674869378407 Validation Loss: 0.7568556070327759\n",
      "Epoch 11102: Training Loss: 0.09768166393041611 Validation Loss: 0.7573208808898926\n",
      "Epoch 11103: Training Loss: 0.09753814339637756 Validation Loss: 0.7571260929107666\n",
      "Epoch 11104: Training Loss: 0.09764264772335689 Validation Loss: 0.7573257088661194\n",
      "Epoch 11105: Training Loss: 0.09825423111518224 Validation Loss: 0.7573819160461426\n",
      "Epoch 11106: Training Loss: 0.0976018210252126 Validation Loss: 0.7570129036903381\n",
      "Epoch 11107: Training Loss: 0.09766432146231334 Validation Loss: 0.7570152282714844\n",
      "Epoch 11108: Training Loss: 0.09728955725828807 Validation Loss: 0.7571616768836975\n",
      "Epoch 11109: Training Loss: 0.0982338214914004 Validation Loss: 0.7575562000274658\n",
      "Epoch 11110: Training Loss: 0.09848739951848984 Validation Loss: 0.7573826313018799\n",
      "Epoch 11111: Training Loss: 0.09800471365451813 Validation Loss: 0.756958544254303\n",
      "Epoch 11112: Training Loss: 0.09807848185300827 Validation Loss: 0.7572982907295227\n",
      "Epoch 11113: Training Loss: 0.09733390559752782 Validation Loss: 0.7574483156204224\n",
      "Epoch 11114: Training Loss: 0.0979148546854655 Validation Loss: 0.7575935125350952\n",
      "Epoch 11115: Training Loss: 0.09807626406351726 Validation Loss: 0.7568272948265076\n",
      "Epoch 11116: Training Loss: 0.09761678924163182 Validation Loss: 0.7566357851028442\n",
      "Epoch 11117: Training Loss: 0.0977894018093745 Validation Loss: 0.7569552063941956\n",
      "Epoch 11118: Training Loss: 0.09825041393438975 Validation Loss: 0.7569141387939453\n",
      "Epoch 11119: Training Loss: 0.09900137533744176 Validation Loss: 0.756744921207428\n",
      "Epoch 11120: Training Loss: 0.09794392436742783 Validation Loss: 0.7571552395820618\n",
      "Epoch 11121: Training Loss: 0.09754648307959239 Validation Loss: 0.756828784942627\n",
      "Epoch 11122: Training Loss: 0.09741414089997609 Validation Loss: 0.7568533420562744\n",
      "Epoch 11123: Training Loss: 0.09740722924470901 Validation Loss: 0.7569151520729065\n",
      "Epoch 11124: Training Loss: 0.09754768510659535 Validation Loss: 0.7572489380836487\n",
      "Epoch 11125: Training Loss: 0.09827364981174469 Validation Loss: 0.7575080990791321\n",
      "Epoch 11126: Training Loss: 0.09758070856332779 Validation Loss: 0.7577961683273315\n",
      "Epoch 11127: Training Loss: 0.09809844692548116 Validation Loss: 0.7571895718574524\n",
      "Epoch 11128: Training Loss: 0.09750916808843613 Validation Loss: 0.7570073008537292\n",
      "Epoch 11129: Training Loss: 0.0984834482272466 Validation Loss: 0.7568952441215515\n",
      "Epoch 11130: Training Loss: 0.09778892993927002 Validation Loss: 0.7569224834442139\n",
      "Epoch 11131: Training Loss: 0.09753013402223587 Validation Loss: 0.7570749521255493\n",
      "Epoch 11132: Training Loss: 0.09742882599433263 Validation Loss: 0.7572453618049622\n",
      "Epoch 11133: Training Loss: 0.09723356614510219 Validation Loss: 0.7576045989990234\n",
      "Epoch 11134: Training Loss: 0.09738047669331233 Validation Loss: 0.7578561305999756\n",
      "Epoch 11135: Training Loss: 0.09754805515209834 Validation Loss: 0.7579482197761536\n",
      "Epoch 11136: Training Loss: 0.09737359980742137 Validation Loss: 0.7584951519966125\n",
      "Epoch 11137: Training Loss: 0.09728583445151646 Validation Loss: 0.7584817409515381\n",
      "Epoch 11138: Training Loss: 0.09772322823603947 Validation Loss: 0.758093535900116\n",
      "Epoch 11139: Training Loss: 0.09751883894205093 Validation Loss: 0.757703959941864\n",
      "Epoch 11140: Training Loss: 0.0974745253721873 Validation Loss: 0.757391631603241\n",
      "Epoch 11141: Training Loss: 0.09753148506085078 Validation Loss: 0.7572887539863586\n",
      "Epoch 11142: Training Loss: 0.09746873875459035 Validation Loss: 0.757849395275116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11143: Training Loss: 0.09753148009379704 Validation Loss: 0.7577704787254333\n",
      "Epoch 11144: Training Loss: 0.09767942627271016 Validation Loss: 0.757633626461029\n",
      "Epoch 11145: Training Loss: 0.09758937358856201 Validation Loss: 0.757587730884552\n",
      "Epoch 11146: Training Loss: 0.09742262711127599 Validation Loss: 0.7574889063835144\n",
      "Epoch 11147: Training Loss: 0.09739389022191365 Validation Loss: 0.7572841644287109\n",
      "Epoch 11148: Training Loss: 0.09806630512078603 Validation Loss: 0.757051944732666\n",
      "Epoch 11149: Training Loss: 0.09756925453742345 Validation Loss: 0.7569171786308289\n",
      "Epoch 11150: Training Loss: 0.09726331134637196 Validation Loss: 0.7571178674697876\n",
      "Epoch 11151: Training Loss: 0.09763871630032857 Validation Loss: 0.7574388980865479\n",
      "Epoch 11152: Training Loss: 0.0974617525935173 Validation Loss: 0.7576111555099487\n",
      "Epoch 11153: Training Loss: 0.09760689238707225 Validation Loss: 0.7578304409980774\n",
      "Epoch 11154: Training Loss: 0.09724405904610951 Validation Loss: 0.7570900917053223\n",
      "Epoch 11155: Training Loss: 0.09737178931633632 Validation Loss: 0.7568546533584595\n",
      "Epoch 11156: Training Loss: 0.09736232459545135 Validation Loss: 0.7570058703422546\n",
      "Epoch 11157: Training Loss: 0.0976330116391182 Validation Loss: 0.7576361894607544\n",
      "Epoch 11158: Training Loss: 0.09727127104997635 Validation Loss: 0.7576058506965637\n",
      "Epoch 11159: Training Loss: 0.09747349470853806 Validation Loss: 0.7576804161071777\n",
      "Epoch 11160: Training Loss: 0.09758050243059795 Validation Loss: 0.7573605179786682\n",
      "Epoch 11161: Training Loss: 0.09741342564423879 Validation Loss: 0.7576460242271423\n",
      "Epoch 11162: Training Loss: 0.09731907149155934 Validation Loss: 0.7572652697563171\n",
      "Epoch 11163: Training Loss: 0.09738959868748982 Validation Loss: 0.7572039365768433\n",
      "Epoch 11164: Training Loss: 0.09736190487941106 Validation Loss: 0.7572476267814636\n",
      "Epoch 11165: Training Loss: 0.09827161331971486 Validation Loss: 0.7574698328971863\n",
      "Epoch 11166: Training Loss: 0.09724429994821548 Validation Loss: 0.7578645348548889\n",
      "Epoch 11167: Training Loss: 0.0973492960135142 Validation Loss: 0.758144736289978\n",
      "Epoch 11168: Training Loss: 0.09757140775521596 Validation Loss: 0.7573937773704529\n",
      "Epoch 11169: Training Loss: 0.09752110143502553 Validation Loss: 0.7573245763778687\n",
      "Epoch 11170: Training Loss: 0.09717624882857005 Validation Loss: 0.7576495409011841\n",
      "Epoch 11171: Training Loss: 0.09732842942078908 Validation Loss: 0.7580278515815735\n",
      "Epoch 11172: Training Loss: 0.09838056067625682 Validation Loss: 0.7575692534446716\n",
      "Epoch 11173: Training Loss: 0.09786308060089748 Validation Loss: 0.7572044730186462\n",
      "Epoch 11174: Training Loss: 0.09703142940998077 Validation Loss: 0.7571851015090942\n",
      "Epoch 11175: Training Loss: 0.09724566340446472 Validation Loss: 0.7571082711219788\n",
      "Epoch 11176: Training Loss: 0.09733601659536362 Validation Loss: 0.7575035095214844\n",
      "Epoch 11177: Training Loss: 0.0976647362112999 Validation Loss: 0.7578070759773254\n",
      "Epoch 11178: Training Loss: 0.09743955731391907 Validation Loss: 0.7574094533920288\n",
      "Epoch 11179: Training Loss: 0.09788894901673 Validation Loss: 0.7574735283851624\n",
      "Epoch 11180: Training Loss: 0.09743008762598038 Validation Loss: 0.7572435140609741\n",
      "Epoch 11181: Training Loss: 0.09764392673969269 Validation Loss: 0.7571338415145874\n",
      "Epoch 11182: Training Loss: 0.097465381026268 Validation Loss: 0.757347822189331\n",
      "Epoch 11183: Training Loss: 0.09719159702459972 Validation Loss: 0.7578685879707336\n",
      "Epoch 11184: Training Loss: 0.09800340980291367 Validation Loss: 0.7574948072433472\n",
      "Epoch 11185: Training Loss: 0.09756863862276077 Validation Loss: 0.7580511569976807\n",
      "Epoch 11186: Training Loss: 0.09719757239023845 Validation Loss: 0.7581803202629089\n",
      "Epoch 11187: Training Loss: 0.09733203301827113 Validation Loss: 0.7580534815788269\n",
      "Epoch 11188: Training Loss: 0.09706950436035792 Validation Loss: 0.7580080032348633\n",
      "Epoch 11189: Training Loss: 0.09746793160835902 Validation Loss: 0.75826495885849\n",
      "Epoch 11190: Training Loss: 0.09711248179276784 Validation Loss: 0.7580298185348511\n",
      "Epoch 11191: Training Loss: 0.09702540934085846 Validation Loss: 0.7578960657119751\n",
      "Epoch 11192: Training Loss: 0.09704963117837906 Validation Loss: 0.7578538060188293\n",
      "Epoch 11193: Training Loss: 0.09754974395036697 Validation Loss: 0.7579003572463989\n",
      "Epoch 11194: Training Loss: 0.09695214778184891 Validation Loss: 0.7580142617225647\n",
      "Epoch 11195: Training Loss: 0.0973388801018397 Validation Loss: 0.7575812935829163\n",
      "Epoch 11196: Training Loss: 0.09696530550718307 Validation Loss: 0.757581889629364\n",
      "Epoch 11197: Training Loss: 0.0971719076236089 Validation Loss: 0.7578669190406799\n",
      "Epoch 11198: Training Loss: 0.09743304302295049 Validation Loss: 0.7577319741249084\n",
      "Epoch 11199: Training Loss: 0.09762368599573772 Validation Loss: 0.7581371068954468\n",
      "Epoch 11200: Training Loss: 0.09747625639041264 Validation Loss: 0.7586196064949036\n",
      "Epoch 11201: Training Loss: 0.09788889686266582 Validation Loss: 0.7582627534866333\n",
      "Epoch 11202: Training Loss: 0.09741193304459254 Validation Loss: 0.7575945258140564\n",
      "Epoch 11203: Training Loss: 0.09711086750030518 Validation Loss: 0.7572435140609741\n",
      "Epoch 11204: Training Loss: 0.09792032837867737 Validation Loss: 0.7579638957977295\n",
      "Epoch 11205: Training Loss: 0.09759190181891124 Validation Loss: 0.7583998441696167\n",
      "Epoch 11206: Training Loss: 0.09716331958770752 Validation Loss: 0.7580726742744446\n",
      "Epoch 11207: Training Loss: 0.0982190544406573 Validation Loss: 0.7579248547554016\n",
      "Epoch 11208: Training Loss: 0.09704677263895671 Validation Loss: 0.7576164603233337\n",
      "Epoch 11209: Training Loss: 0.09712476283311844 Validation Loss: 0.7579227685928345\n",
      "Epoch 11210: Training Loss: 0.09627960125605266 Validation Loss: 0.7577696442604065\n",
      "Epoch 11211: Training Loss: 0.09692069639762242 Validation Loss: 0.7576596736907959\n",
      "Epoch 11212: Training Loss: 0.0970373625556628 Validation Loss: 0.7576260566711426\n",
      "Epoch 11213: Training Loss: 0.09735109657049179 Validation Loss: 0.758482038974762\n",
      "Epoch 11214: Training Loss: 0.09667081385850906 Validation Loss: 0.7588003873825073\n",
      "Epoch 11215: Training Loss: 0.09712138026952744 Validation Loss: 0.758483350276947\n",
      "Epoch 11216: Training Loss: 0.09709294388691585 Validation Loss: 0.757584273815155\n",
      "Epoch 11217: Training Loss: 0.0970612366994222 Validation Loss: 0.7574102878570557\n",
      "Epoch 11218: Training Loss: 0.0969960168004036 Validation Loss: 0.7576559782028198\n",
      "Epoch 11219: Training Loss: 0.09731325507164001 Validation Loss: 0.7577275037765503\n",
      "Epoch 11220: Training Loss: 0.09721086422602336 Validation Loss: 0.7578045129776001\n",
      "Epoch 11221: Training Loss: 0.09689866006374359 Validation Loss: 0.7582444548606873\n",
      "Epoch 11222: Training Loss: 0.09766165415445964 Validation Loss: 0.7586871981620789\n",
      "Epoch 11223: Training Loss: 0.09680711229642232 Validation Loss: 0.7583467960357666\n",
      "Epoch 11224: Training Loss: 0.09720856944719951 Validation Loss: 0.7581492066383362\n",
      "Epoch 11225: Training Loss: 0.0967082033554713 Validation Loss: 0.757898211479187\n",
      "Epoch 11226: Training Loss: 0.09677557647228241 Validation Loss: 0.7579177021980286\n",
      "Epoch 11227: Training Loss: 0.09719653179248174 Validation Loss: 0.7575240135192871\n",
      "Epoch 11228: Training Loss: 0.09700010220209758 Validation Loss: 0.7575782537460327\n",
      "Epoch 11229: Training Loss: 0.0968076412876447 Validation Loss: 0.7581371068954468\n",
      "Epoch 11230: Training Loss: 0.09700964391231537 Validation Loss: 0.7586231231689453\n",
      "Epoch 11231: Training Loss: 0.09742964307467143 Validation Loss: 0.758368194103241\n",
      "Epoch 11232: Training Loss: 0.09728010495503743 Validation Loss: 0.7579348087310791\n",
      "Epoch 11233: Training Loss: 0.09691167622804642 Validation Loss: 0.7579150795936584\n",
      "Epoch 11234: Training Loss: 0.09725003192822139 Validation Loss: 0.7576084136962891\n",
      "Epoch 11235: Training Loss: 0.09705868115027745 Validation Loss: 0.7577597498893738\n",
      "Epoch 11236: Training Loss: 0.09684082865715027 Validation Loss: 0.7584770917892456\n",
      "Epoch 11237: Training Loss: 0.09694615751504898 Validation Loss: 0.758870542049408\n",
      "Epoch 11238: Training Loss: 0.09762688726186752 Validation Loss: 0.7587504386901855\n",
      "Epoch 11239: Training Loss: 0.09703010569016139 Validation Loss: 0.7585756182670593\n",
      "Epoch 11240: Training Loss: 0.09680441518624623 Validation Loss: 0.7582442760467529\n",
      "Epoch 11241: Training Loss: 0.09816583494345348 Validation Loss: 0.7580981850624084\n",
      "Epoch 11242: Training Loss: 0.09765939166148503 Validation Loss: 0.7581512331962585\n",
      "Epoch 11243: Training Loss: 0.0970171590646108 Validation Loss: 0.7584792971611023\n",
      "Epoch 11244: Training Loss: 0.09694118052721024 Validation Loss: 0.7581153512001038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11245: Training Loss: 0.09703514476617177 Validation Loss: 0.7583906054496765\n",
      "Epoch 11246: Training Loss: 0.09654417634010315 Validation Loss: 0.7586243748664856\n",
      "Epoch 11247: Training Loss: 0.09689984967311223 Validation Loss: 0.7583087086677551\n",
      "Epoch 11248: Training Loss: 0.09699683636426926 Validation Loss: 0.7583105564117432\n",
      "Epoch 11249: Training Loss: 0.09694247196118037 Validation Loss: 0.7581104040145874\n",
      "Epoch 11250: Training Loss: 0.09711411346991856 Validation Loss: 0.7581937909126282\n",
      "Epoch 11251: Training Loss: 0.09806966036558151 Validation Loss: 0.7587151527404785\n",
      "Epoch 11252: Training Loss: 0.09708779056866963 Validation Loss: 0.7584674954414368\n",
      "Epoch 11253: Training Loss: 0.09801853199799855 Validation Loss: 0.7587379217147827\n",
      "Epoch 11254: Training Loss: 0.09677899132172267 Validation Loss: 0.7585120797157288\n",
      "Epoch 11255: Training Loss: 0.09710807849963506 Validation Loss: 0.7579943537712097\n",
      "Epoch 11256: Training Loss: 0.09684545298417409 Validation Loss: 0.7576881051063538\n",
      "Epoch 11257: Training Loss: 0.09704709053039551 Validation Loss: 0.7583085894584656\n",
      "Epoch 11258: Training Loss: 0.09678737819194794 Validation Loss: 0.7585338950157166\n",
      "Epoch 11259: Training Loss: 0.09720452129840851 Validation Loss: 0.7590418457984924\n",
      "Epoch 11260: Training Loss: 0.09732792029778163 Validation Loss: 0.7589176297187805\n",
      "Epoch 11261: Training Loss: 0.0970144271850586 Validation Loss: 0.7587540745735168\n",
      "Epoch 11262: Training Loss: 0.09747553616762161 Validation Loss: 0.7582643628120422\n",
      "Epoch 11263: Training Loss: 0.0967984249194463 Validation Loss: 0.7581722140312195\n",
      "Epoch 11264: Training Loss: 0.09667398035526276 Validation Loss: 0.758186936378479\n",
      "Epoch 11265: Training Loss: 0.09651112059752147 Validation Loss: 0.7581758499145508\n",
      "Epoch 11266: Training Loss: 0.09714063753684361 Validation Loss: 0.7585347890853882\n",
      "Epoch 11267: Training Loss: 0.09675231079260509 Validation Loss: 0.7589304447174072\n",
      "Epoch 11268: Training Loss: 0.0970074658592542 Validation Loss: 0.759168803691864\n",
      "Epoch 11269: Training Loss: 0.09677758812904358 Validation Loss: 0.7585161328315735\n",
      "Epoch 11270: Training Loss: 0.09695790459712346 Validation Loss: 0.7585060000419617\n",
      "Epoch 11271: Training Loss: 0.09675277024507523 Validation Loss: 0.75829017162323\n",
      "Epoch 11272: Training Loss: 0.09663363546133041 Validation Loss: 0.7584447264671326\n",
      "Epoch 11273: Training Loss: 0.09717355668544769 Validation Loss: 0.7586644887924194\n",
      "Epoch 11274: Training Loss: 0.09718188643455505 Validation Loss: 0.7591785788536072\n",
      "Epoch 11275: Training Loss: 0.09728461503982544 Validation Loss: 0.7588264346122742\n",
      "Epoch 11276: Training Loss: 0.09665084381898244 Validation Loss: 0.7582961916923523\n",
      "Epoch 11277: Training Loss: 0.09701644380887349 Validation Loss: 0.7583743929862976\n",
      "Epoch 11278: Training Loss: 0.096810482442379 Validation Loss: 0.7585026621818542\n",
      "Epoch 11279: Training Loss: 0.09659621367851894 Validation Loss: 0.7586952447891235\n",
      "Epoch 11280: Training Loss: 0.09692615270614624 Validation Loss: 0.7583466172218323\n",
      "Epoch 11281: Training Loss: 0.09716582049926122 Validation Loss: 0.7586719989776611\n",
      "Epoch 11282: Training Loss: 0.09703728059927623 Validation Loss: 0.7584454417228699\n",
      "Epoch 11283: Training Loss: 0.09667608886957169 Validation Loss: 0.7586674094200134\n",
      "Epoch 11284: Training Loss: 0.09612145523230235 Validation Loss: 0.7591334581375122\n",
      "Epoch 11285: Training Loss: 0.0979605217774709 Validation Loss: 0.7586652636528015\n",
      "Epoch 11286: Training Loss: 0.09674824525912602 Validation Loss: 0.7586367130279541\n",
      "Epoch 11287: Training Loss: 0.09654468546311061 Validation Loss: 0.7587409615516663\n",
      "Epoch 11288: Training Loss: 0.0968043381969134 Validation Loss: 0.7582362294197083\n",
      "Epoch 11289: Training Loss: 0.0968223586678505 Validation Loss: 0.7586219310760498\n",
      "Epoch 11290: Training Loss: 0.09652932981650035 Validation Loss: 0.7593082785606384\n",
      "Epoch 11291: Training Loss: 0.09683667620023091 Validation Loss: 0.7593202590942383\n",
      "Epoch 11292: Training Loss: 0.09673554698626201 Validation Loss: 0.7592023015022278\n",
      "Epoch 11293: Training Loss: 0.09742668271064758 Validation Loss: 0.7585145235061646\n",
      "Epoch 11294: Training Loss: 0.09672155479590099 Validation Loss: 0.7584880590438843\n",
      "Epoch 11295: Training Loss: 0.09657359619935353 Validation Loss: 0.7586707472801208\n",
      "Epoch 11296: Training Loss: 0.09667576601107915 Validation Loss: 0.7590928077697754\n",
      "Epoch 11297: Training Loss: 0.09703613569339116 Validation Loss: 0.7595096826553345\n",
      "Epoch 11298: Training Loss: 0.09670708328485489 Validation Loss: 0.7591590881347656\n",
      "Epoch 11299: Training Loss: 0.09694250921408336 Validation Loss: 0.7588310241699219\n",
      "Epoch 11300: Training Loss: 0.0967516874273618 Validation Loss: 0.7586290240287781\n",
      "Epoch 11301: Training Loss: 0.09654552241166432 Validation Loss: 0.7588704228401184\n",
      "Epoch 11302: Training Loss: 0.09649711598952611 Validation Loss: 0.7588452100753784\n",
      "Epoch 11303: Training Loss: 0.09642289578914642 Validation Loss: 0.7595365047454834\n",
      "Epoch 11304: Training Loss: 0.09658362964789073 Validation Loss: 0.7590620517730713\n",
      "Epoch 11305: Training Loss: 0.09652911126613617 Validation Loss: 0.7586742639541626\n",
      "Epoch 11306: Training Loss: 0.09655437370141347 Validation Loss: 0.7582190632820129\n",
      "Epoch 11307: Training Loss: 0.09736556808153789 Validation Loss: 0.7584642171859741\n",
      "Epoch 11308: Training Loss: 0.09640674789746602 Validation Loss: 0.7587558031082153\n",
      "Epoch 11309: Training Loss: 0.09645837545394897 Validation Loss: 0.759168803691864\n",
      "Epoch 11310: Training Loss: 0.09673325469096501 Validation Loss: 0.759395182132721\n",
      "Epoch 11311: Training Loss: 0.09707736472288768 Validation Loss: 0.7587764263153076\n",
      "Epoch 11312: Training Loss: 0.09648499141136806 Validation Loss: 0.7584987282752991\n",
      "Epoch 11313: Training Loss: 0.09657894819974899 Validation Loss: 0.7592893838882446\n",
      "Epoch 11314: Training Loss: 0.0967254787683487 Validation Loss: 0.7589807510375977\n",
      "Epoch 11315: Training Loss: 0.0962990125020345 Validation Loss: 0.7593358755111694\n",
      "Epoch 11316: Training Loss: 0.09655727446079254 Validation Loss: 0.7592719793319702\n",
      "Epoch 11317: Training Loss: 0.09660699466864268 Validation Loss: 0.7591462731361389\n",
      "Epoch 11318: Training Loss: 0.09627398600180943 Validation Loss: 0.7587587833404541\n",
      "Epoch 11319: Training Loss: 0.09666893879572551 Validation Loss: 0.7589271664619446\n",
      "Epoch 11320: Training Loss: 0.09645204742749532 Validation Loss: 0.7591946125030518\n",
      "Epoch 11321: Training Loss: 0.0964907854795456 Validation Loss: 0.7589141726493835\n",
      "Epoch 11322: Training Loss: 0.09728665401538213 Validation Loss: 0.7590444684028625\n",
      "Epoch 11323: Training Loss: 0.09648110717535019 Validation Loss: 0.7591536045074463\n",
      "Epoch 11324: Training Loss: 0.09666622430086136 Validation Loss: 0.7590271234512329\n",
      "Epoch 11325: Training Loss: 0.09654557953278224 Validation Loss: 0.7587632536888123\n",
      "Epoch 11326: Training Loss: 0.09660263607899348 Validation Loss: 0.7588955760002136\n",
      "Epoch 11327: Training Loss: 0.09663602461417516 Validation Loss: 0.7588850259780884\n",
      "Epoch 11328: Training Loss: 0.0966509183247884 Validation Loss: 0.7590457797050476\n",
      "Epoch 11329: Training Loss: 0.09640406568845113 Validation Loss: 0.7593319416046143\n",
      "Epoch 11330: Training Loss: 0.09661081930001576 Validation Loss: 0.7592398524284363\n",
      "Epoch 11331: Training Loss: 0.09624190628528595 Validation Loss: 0.7594847679138184\n",
      "Epoch 11332: Training Loss: 0.09713368366161983 Validation Loss: 0.7595461010932922\n",
      "Epoch 11333: Training Loss: 0.0967509647210439 Validation Loss: 0.7594414949417114\n",
      "Epoch 11334: Training Loss: 0.09659293045600255 Validation Loss: 0.7587916254997253\n",
      "Epoch 11335: Training Loss: 0.09652192890644073 Validation Loss: 0.7588400840759277\n",
      "Epoch 11336: Training Loss: 0.09683369845151901 Validation Loss: 0.7590041160583496\n",
      "Epoch 11337: Training Loss: 0.0964485729734103 Validation Loss: 0.7586981058120728\n",
      "Epoch 11338: Training Loss: 0.09648965547482173 Validation Loss: 0.7592372298240662\n",
      "Epoch 11339: Training Loss: 0.09657977769772212 Validation Loss: 0.759510338306427\n",
      "Epoch 11340: Training Loss: 0.09641049057245255 Validation Loss: 0.760002076625824\n",
      "Epoch 11341: Training Loss: 0.09641311317682266 Validation Loss: 0.7595569491386414\n",
      "Epoch 11342: Training Loss: 0.0963054100672404 Validation Loss: 0.7589811086654663\n",
      "Epoch 11343: Training Loss: 0.09647323439518611 Validation Loss: 0.7589434385299683\n",
      "Epoch 11344: Training Loss: 0.09643002102772395 Validation Loss: 0.7594598531723022\n",
      "Epoch 11345: Training Loss: 0.09652214745680492 Validation Loss: 0.7594801783561707\n",
      "Epoch 11346: Training Loss: 0.0964771310488383 Validation Loss: 0.7598832845687866\n",
      "Epoch 11347: Training Loss: 0.09612493465344112 Validation Loss: 0.7597423195838928\n",
      "Epoch 11348: Training Loss: 0.0964209313193957 Validation Loss: 0.7599099278450012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11349: Training Loss: 0.09635576357444127 Validation Loss: 0.759515106678009\n",
      "Epoch 11350: Training Loss: 0.09636569271485011 Validation Loss: 0.7595458626747131\n",
      "Epoch 11351: Training Loss: 0.096641110877196 Validation Loss: 0.7596771717071533\n",
      "Epoch 11352: Training Loss: 0.09613878776629765 Validation Loss: 0.7594772577285767\n",
      "Epoch 11353: Training Loss: 0.09626182665427525 Validation Loss: 0.7596678137779236\n",
      "Epoch 11354: Training Loss: 0.0964456523458163 Validation Loss: 0.7598320245742798\n",
      "Epoch 11355: Training Loss: 0.09642210602760315 Validation Loss: 0.7595314979553223\n",
      "Epoch 11356: Training Loss: 0.09630805253982544 Validation Loss: 0.7595275640487671\n",
      "Epoch 11357: Training Loss: 0.09653906524181366 Validation Loss: 0.759453535079956\n",
      "Epoch 11358: Training Loss: 0.09644883871078491 Validation Loss: 0.7591548562049866\n",
      "Epoch 11359: Training Loss: 0.09598260621229808 Validation Loss: 0.759347677230835\n",
      "Epoch 11360: Training Loss: 0.09630346794923146 Validation Loss: 0.7594540119171143\n",
      "Epoch 11361: Training Loss: 0.09667661041021347 Validation Loss: 0.7596564888954163\n",
      "Epoch 11362: Training Loss: 0.09613349785407384 Validation Loss: 0.7596167325973511\n",
      "Epoch 11363: Training Loss: 0.0965142697095871 Validation Loss: 0.7592902779579163\n",
      "Epoch 11364: Training Loss: 0.09712402025858562 Validation Loss: 0.7594205141067505\n",
      "Epoch 11365: Training Loss: 0.09642377247412999 Validation Loss: 0.7593267560005188\n",
      "Epoch 11366: Training Loss: 0.09644410759210587 Validation Loss: 0.7595399022102356\n",
      "Epoch 11367: Training Loss: 0.0965551882982254 Validation Loss: 0.7598708868026733\n",
      "Epoch 11368: Training Loss: 0.09614281604687373 Validation Loss: 0.7596358060836792\n",
      "Epoch 11369: Training Loss: 0.0961737980445226 Validation Loss: 0.7596929669380188\n",
      "Epoch 11370: Training Loss: 0.09610962371031444 Validation Loss: 0.7596500515937805\n",
      "Epoch 11371: Training Loss: 0.0968243380387624 Validation Loss: 0.7592917084693909\n",
      "Epoch 11372: Training Loss: 0.09644100815057755 Validation Loss: 0.7587507367134094\n",
      "Epoch 11373: Training Loss: 0.09642112503449123 Validation Loss: 0.7589079737663269\n",
      "Epoch 11374: Training Loss: 0.09643888225158055 Validation Loss: 0.7601876854896545\n",
      "Epoch 11375: Training Loss: 0.09645054986079533 Validation Loss: 0.7600978016853333\n",
      "Epoch 11376: Training Loss: 0.09637166559696198 Validation Loss: 0.7605757713317871\n",
      "Epoch 11377: Training Loss: 0.09666413813829422 Validation Loss: 0.759945809841156\n",
      "Epoch 11378: Training Loss: 0.096376766761144 Validation Loss: 0.7598094940185547\n",
      "Epoch 11379: Training Loss: 0.09591284642616908 Validation Loss: 0.7597252726554871\n",
      "Epoch 11380: Training Loss: 0.09693818539381027 Validation Loss: 0.7595199942588806\n",
      "Epoch 11381: Training Loss: 0.09801409393548965 Validation Loss: 0.759448230266571\n",
      "Epoch 11382: Training Loss: 0.09622926265001297 Validation Loss: 0.7598013281822205\n",
      "Epoch 11383: Training Loss: 0.09602297594149907 Validation Loss: 0.7598977088928223\n",
      "Epoch 11384: Training Loss: 0.09632192800442378 Validation Loss: 0.7600849270820618\n",
      "Epoch 11385: Training Loss: 0.09639247010151546 Validation Loss: 0.7605108618736267\n",
      "Epoch 11386: Training Loss: 0.09658667196830113 Validation Loss: 0.7594533562660217\n",
      "Epoch 11387: Training Loss: 0.09635566920042038 Validation Loss: 0.759278416633606\n",
      "Epoch 11388: Training Loss: 0.09614140292008717 Validation Loss: 0.7596708536148071\n",
      "Epoch 11389: Training Loss: 0.09619345267613728 Validation Loss: 0.759526789188385\n",
      "Epoch 11390: Training Loss: 0.09712828695774078 Validation Loss: 0.7595862150192261\n",
      "Epoch 11391: Training Loss: 0.09613730013370514 Validation Loss: 0.7596872448921204\n",
      "Epoch 11392: Training Loss: 0.09612090637286504 Validation Loss: 0.7597833871841431\n",
      "Epoch 11393: Training Loss: 0.09665671984354655 Validation Loss: 0.76025390625\n",
      "Epoch 11394: Training Loss: 0.09642345954974492 Validation Loss: 0.7603340148925781\n",
      "Epoch 11395: Training Loss: 0.09642876933018367 Validation Loss: 0.7601050734519958\n",
      "Epoch 11396: Training Loss: 0.09658028185367584 Validation Loss: 0.7599173188209534\n",
      "Epoch 11397: Training Loss: 0.09598011771837871 Validation Loss: 0.7593730092048645\n",
      "Epoch 11398: Training Loss: 0.09627784043550491 Validation Loss: 0.7590839266777039\n",
      "Epoch 11399: Training Loss: 0.09623922655979793 Validation Loss: 0.7591172456741333\n",
      "Epoch 11400: Training Loss: 0.09635121872027715 Validation Loss: 0.7595310807228088\n",
      "Epoch 11401: Training Loss: 0.09723542879025142 Validation Loss: 0.7596798539161682\n",
      "Epoch 11402: Training Loss: 0.0963814506928126 Validation Loss: 0.7602658271789551\n",
      "Epoch 11403: Training Loss: 0.0961002657810847 Validation Loss: 0.7601130604743958\n",
      "Epoch 11404: Training Loss: 0.0961249793569247 Validation Loss: 0.7601981163024902\n",
      "Epoch 11405: Training Loss: 0.09608275443315506 Validation Loss: 0.7599464654922485\n",
      "Epoch 11406: Training Loss: 0.097368523478508 Validation Loss: 0.7602556347846985\n",
      "Epoch 11407: Training Loss: 0.09613899389902751 Validation Loss: 0.760326087474823\n",
      "Epoch 11408: Training Loss: 0.09570380300283432 Validation Loss: 0.7600221633911133\n",
      "Epoch 11409: Training Loss: 0.09616890052954356 Validation Loss: 0.7595205903053284\n",
      "Epoch 11410: Training Loss: 0.0963146761059761 Validation Loss: 0.7590848207473755\n",
      "Epoch 11411: Training Loss: 0.09609382847944896 Validation Loss: 0.759568989276886\n",
      "Epoch 11412: Training Loss: 0.0958347221215566 Validation Loss: 0.7603997588157654\n",
      "Epoch 11413: Training Loss: 0.09661371757586797 Validation Loss: 0.7606065273284912\n",
      "Epoch 11414: Training Loss: 0.09612779567639033 Validation Loss: 0.7609637975692749\n",
      "Epoch 11415: Training Loss: 0.09600370874007542 Validation Loss: 0.7602536678314209\n",
      "Epoch 11416: Training Loss: 0.09596146394809087 Validation Loss: 0.7599783539772034\n",
      "Epoch 11417: Training Loss: 0.09636046985785167 Validation Loss: 0.7596812844276428\n",
      "Epoch 11418: Training Loss: 0.09596341848373413 Validation Loss: 0.7593745589256287\n",
      "Epoch 11419: Training Loss: 0.09575180212656657 Validation Loss: 0.7595420479774475\n",
      "Epoch 11420: Training Loss: 0.09613323211669922 Validation Loss: 0.7599915266036987\n",
      "Epoch 11421: Training Loss: 0.09625449279944102 Validation Loss: 0.7604128122329712\n",
      "Epoch 11422: Training Loss: 0.09617365896701813 Validation Loss: 0.7604128122329712\n",
      "Epoch 11423: Training Loss: 0.0966786394516627 Validation Loss: 0.7601718306541443\n",
      "Epoch 11424: Training Loss: 0.09583270301421483 Validation Loss: 0.7600496411323547\n",
      "Epoch 11425: Training Loss: 0.09593094885349274 Validation Loss: 0.7599643468856812\n",
      "Epoch 11426: Training Loss: 0.09601472566525142 Validation Loss: 0.7598012685775757\n",
      "Epoch 11427: Training Loss: 0.09611174712578456 Validation Loss: 0.7598903179168701\n",
      "Epoch 11428: Training Loss: 0.09615820646286011 Validation Loss: 0.7604644298553467\n",
      "Epoch 11429: Training Loss: 0.09675704936186473 Validation Loss: 0.7602314949035645\n",
      "Epoch 11430: Training Loss: 0.09595544884602229 Validation Loss: 0.7603116631507874\n",
      "Epoch 11431: Training Loss: 0.09580391397078832 Validation Loss: 0.7603577375411987\n",
      "Epoch 11432: Training Loss: 0.09590959300597508 Validation Loss: 0.7602143883705139\n",
      "Epoch 11433: Training Loss: 0.09582415719827016 Validation Loss: 0.7603216171264648\n",
      "Epoch 11434: Training Loss: 0.09610741088787715 Validation Loss: 0.7601413726806641\n",
      "Epoch 11435: Training Loss: 0.095732015868028 Validation Loss: 0.760375440120697\n",
      "Epoch 11436: Training Loss: 0.09649956226348877 Validation Loss: 0.7603794932365417\n",
      "Epoch 11437: Training Loss: 0.09697005152702332 Validation Loss: 0.7599557638168335\n",
      "Epoch 11438: Training Loss: 0.0958587850133578 Validation Loss: 0.7604983448982239\n",
      "Epoch 11439: Training Loss: 0.09654477735360463 Validation Loss: 0.7609347105026245\n",
      "Epoch 11440: Training Loss: 0.09588342656691869 Validation Loss: 0.7605748772621155\n",
      "Epoch 11441: Training Loss: 0.09527445584535599 Validation Loss: 0.7606078386306763\n",
      "Epoch 11442: Training Loss: 0.09712822735309601 Validation Loss: 0.7606517672538757\n",
      "Epoch 11443: Training Loss: 0.09636518359184265 Validation Loss: 0.7604526281356812\n",
      "Epoch 11444: Training Loss: 0.09586704770723979 Validation Loss: 0.760089635848999\n",
      "Epoch 11445: Training Loss: 0.09611280759175618 Validation Loss: 0.7599655985832214\n",
      "Epoch 11446: Training Loss: 0.09602949768304825 Validation Loss: 0.75994473695755\n",
      "Epoch 11447: Training Loss: 0.09604788074890773 Validation Loss: 0.7604047060012817\n",
      "Epoch 11448: Training Loss: 0.09575692315896352 Validation Loss: 0.7607552409172058\n",
      "Epoch 11449: Training Loss: 0.09619684020678203 Validation Loss: 0.7605718374252319\n",
      "Epoch 11450: Training Loss: 0.09583195298910141 Validation Loss: 0.7609460353851318\n",
      "Epoch 11451: Training Loss: 0.09572803477446239 Validation Loss: 0.7607399225234985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11452: Training Loss: 0.09604112803936005 Validation Loss: 0.7604576945304871\n",
      "Epoch 11453: Training Loss: 0.09586191177368164 Validation Loss: 0.7602530717849731\n",
      "Epoch 11454: Training Loss: 0.0965426042675972 Validation Loss: 0.7600287199020386\n",
      "Epoch 11455: Training Loss: 0.09572642793258031 Validation Loss: 0.7603062391281128\n",
      "Epoch 11456: Training Loss: 0.09600703169902165 Validation Loss: 0.7602196335792542\n",
      "Epoch 11457: Training Loss: 0.09581192582845688 Validation Loss: 0.7600960731506348\n",
      "Epoch 11458: Training Loss: 0.09555317461490631 Validation Loss: 0.7605487108230591\n",
      "Epoch 11459: Training Loss: 0.09589344263076782 Validation Loss: 0.7608630061149597\n",
      "Epoch 11460: Training Loss: 0.09579395999511083 Validation Loss: 0.7603967189788818\n",
      "Epoch 11461: Training Loss: 0.09558340907096863 Validation Loss: 0.7605835795402527\n",
      "Epoch 11462: Training Loss: 0.09589687983194987 Validation Loss: 0.7607877254486084\n",
      "Epoch 11463: Training Loss: 0.09577529629071553 Validation Loss: 0.7608067989349365\n",
      "Epoch 11464: Training Loss: 0.09584008157253265 Validation Loss: 0.7606685161590576\n",
      "Epoch 11465: Training Loss: 0.09608892599741618 Validation Loss: 0.7606133818626404\n",
      "Epoch 11466: Training Loss: 0.095906267563502 Validation Loss: 0.7604579329490662\n",
      "Epoch 11467: Training Loss: 0.09578368067741394 Validation Loss: 0.7607420682907104\n",
      "Epoch 11468: Training Loss: 0.0959323098262151 Validation Loss: 0.7605187296867371\n",
      "Epoch 11469: Training Loss: 0.09602776666482289 Validation Loss: 0.7606027126312256\n",
      "Epoch 11470: Training Loss: 0.09570663670698802 Validation Loss: 0.760471761226654\n",
      "Epoch 11471: Training Loss: 0.09605746964613597 Validation Loss: 0.7606486082077026\n",
      "Epoch 11472: Training Loss: 0.0955335721373558 Validation Loss: 0.7609368562698364\n",
      "Epoch 11473: Training Loss: 0.09569556514422099 Validation Loss: 0.7610054612159729\n",
      "Epoch 11474: Training Loss: 0.09579205513000488 Validation Loss: 0.7601943016052246\n",
      "Epoch 11475: Training Loss: 0.0957458366950353 Validation Loss: 0.7604315280914307\n",
      "Epoch 11476: Training Loss: 0.09584683179855347 Validation Loss: 0.7605730891227722\n",
      "Epoch 11477: Training Loss: 0.09643649806578954 Validation Loss: 0.7609387040138245\n",
      "Epoch 11478: Training Loss: 0.09574818362792333 Validation Loss: 0.7607871294021606\n",
      "Epoch 11479: Training Loss: 0.09637615829706192 Validation Loss: 0.7603514194488525\n",
      "Epoch 11480: Training Loss: 0.09565110504627228 Validation Loss: 0.7604464888572693\n",
      "Epoch 11481: Training Loss: 0.09541575113932292 Validation Loss: 0.7605453729629517\n",
      "Epoch 11482: Training Loss: 0.09617882966995239 Validation Loss: 0.7609549760818481\n",
      "Epoch 11483: Training Loss: 0.09613585223754247 Validation Loss: 0.7610413432121277\n",
      "Epoch 11484: Training Loss: 0.09542165944973628 Validation Loss: 0.7607784271240234\n",
      "Epoch 11485: Training Loss: 0.0958625003695488 Validation Loss: 0.7607683539390564\n",
      "Epoch 11486: Training Loss: 0.09599944452444713 Validation Loss: 0.7603772282600403\n",
      "Epoch 11487: Training Loss: 0.09596944103638332 Validation Loss: 0.7607510089874268\n",
      "Epoch 11488: Training Loss: 0.09554672986268997 Validation Loss: 0.7608475089073181\n",
      "Epoch 11489: Training Loss: 0.09577444195747375 Validation Loss: 0.7612040042877197\n",
      "Epoch 11490: Training Loss: 0.09598660717407863 Validation Loss: 0.761023998260498\n",
      "Epoch 11491: Training Loss: 0.095560222864151 Validation Loss: 0.7612457871437073\n",
      "Epoch 11492: Training Loss: 0.09589701890945435 Validation Loss: 0.7616184949874878\n",
      "Epoch 11493: Training Loss: 0.09540751079718272 Validation Loss: 0.7607607245445251\n",
      "Epoch 11494: Training Loss: 0.09556594242652257 Validation Loss: 0.7600073218345642\n",
      "Epoch 11495: Training Loss: 0.09561370809872945 Validation Loss: 0.7602072358131409\n",
      "Epoch 11496: Training Loss: 0.09529395401477814 Validation Loss: 0.7607700824737549\n",
      "Epoch 11497: Training Loss: 0.09615915517012279 Validation Loss: 0.7608802914619446\n",
      "Epoch 11498: Training Loss: 0.09558738519748051 Validation Loss: 0.7610311508178711\n",
      "Epoch 11499: Training Loss: 0.09578174352645874 Validation Loss: 0.7606120705604553\n",
      "Epoch 11500: Training Loss: 0.09556399782498677 Validation Loss: 0.760512113571167\n",
      "Epoch 11501: Training Loss: 0.09561194976170857 Validation Loss: 0.7605531215667725\n",
      "Epoch 11502: Training Loss: 0.09553814182678859 Validation Loss: 0.7603905200958252\n",
      "Epoch 11503: Training Loss: 0.09544749806324641 Validation Loss: 0.7607000470161438\n",
      "Epoch 11504: Training Loss: 0.0956316664814949 Validation Loss: 0.7609859704971313\n",
      "Epoch 11505: Training Loss: 0.09609162310759227 Validation Loss: 0.7612303495407104\n",
      "Epoch 11506: Training Loss: 0.09581706672906876 Validation Loss: 0.7613986730575562\n",
      "Epoch 11507: Training Loss: 0.0958964799841245 Validation Loss: 0.7608053684234619\n",
      "Epoch 11508: Training Loss: 0.0954884613553683 Validation Loss: 0.760507345199585\n",
      "Epoch 11509: Training Loss: 0.09638477365175883 Validation Loss: 0.7605015635490417\n",
      "Epoch 11510: Training Loss: 0.09544346481561661 Validation Loss: 0.7606320977210999\n",
      "Epoch 11511: Training Loss: 0.09465157488981883 Validation Loss: 0.7606562972068787\n",
      "Epoch 11512: Training Loss: 0.09596940875053406 Validation Loss: 0.7601342797279358\n",
      "Epoch 11513: Training Loss: 0.09571465601523717 Validation Loss: 0.7604426741600037\n",
      "Epoch 11514: Training Loss: 0.09565854569276173 Validation Loss: 0.7610172629356384\n",
      "Epoch 11515: Training Loss: 0.09592062731583913 Validation Loss: 0.7615430355072021\n",
      "Epoch 11516: Training Loss: 0.09583959976832072 Validation Loss: 0.7613916397094727\n",
      "Epoch 11517: Training Loss: 0.09567173570394516 Validation Loss: 0.7611297369003296\n",
      "Epoch 11518: Training Loss: 0.0955354372660319 Validation Loss: 0.7603139281272888\n",
      "Epoch 11519: Training Loss: 0.09572369356950124 Validation Loss: 0.7603959441184998\n",
      "Epoch 11520: Training Loss: 0.09571722149848938 Validation Loss: 0.7605330944061279\n",
      "Epoch 11521: Training Loss: 0.09547750155131023 Validation Loss: 0.7606276273727417\n",
      "Epoch 11522: Training Loss: 0.09583166738351186 Validation Loss: 0.7607543468475342\n",
      "Epoch 11523: Training Loss: 0.09542188296715419 Validation Loss: 0.7610405087471008\n",
      "Epoch 11524: Training Loss: 0.09527916212876637 Validation Loss: 0.760516345500946\n",
      "Epoch 11525: Training Loss: 0.09524209797382355 Validation Loss: 0.7606549263000488\n",
      "Epoch 11526: Training Loss: 0.09569079428911209 Validation Loss: 0.7610657215118408\n",
      "Epoch 11527: Training Loss: 0.09567093104124069 Validation Loss: 0.7605123519897461\n",
      "Epoch 11528: Training Loss: 0.09559441357851028 Validation Loss: 0.760443925857544\n",
      "Epoch 11529: Training Loss: 0.09551293154557546 Validation Loss: 0.7607842683792114\n",
      "Epoch 11530: Training Loss: 0.0959067444006602 Validation Loss: 0.7613092660903931\n",
      "Epoch 11531: Training Loss: 0.09557627389828365 Validation Loss: 0.7611789107322693\n",
      "Epoch 11532: Training Loss: 0.09551222870747249 Validation Loss: 0.7611360549926758\n",
      "Epoch 11533: Training Loss: 0.09664553900559743 Validation Loss: 0.7609975337982178\n",
      "Epoch 11534: Training Loss: 0.09566820661226909 Validation Loss: 0.7612934112548828\n",
      "Epoch 11535: Training Loss: 0.0950286438067754 Validation Loss: 0.7612205743789673\n",
      "Epoch 11536: Training Loss: 0.09589924663305283 Validation Loss: 0.7612828612327576\n",
      "Epoch 11537: Training Loss: 0.09538156539201736 Validation Loss: 0.7612500190734863\n",
      "Epoch 11538: Training Loss: 0.09536786874135335 Validation Loss: 0.7611508369445801\n",
      "Epoch 11539: Training Loss: 0.09549549967050552 Validation Loss: 0.760999858379364\n",
      "Epoch 11540: Training Loss: 0.09564951558907826 Validation Loss: 0.7616609334945679\n",
      "Epoch 11541: Training Loss: 0.09584417194128036 Validation Loss: 0.7615232467651367\n",
      "Epoch 11542: Training Loss: 0.09613070140282313 Validation Loss: 0.7611537575721741\n",
      "Epoch 11543: Training Loss: 0.09588172535101573 Validation Loss: 0.7613686323165894\n",
      "Epoch 11544: Training Loss: 0.09540961682796478 Validation Loss: 0.761680006980896\n",
      "Epoch 11545: Training Loss: 0.09535038222869237 Validation Loss: 0.7615355849266052\n",
      "Epoch 11546: Training Loss: 0.09528451909621556 Validation Loss: 0.7618549466133118\n",
      "Epoch 11547: Training Loss: 0.09603723138570786 Validation Loss: 0.7616488933563232\n",
      "Epoch 11548: Training Loss: 0.09545460591713588 Validation Loss: 0.7611487507820129\n",
      "Epoch 11549: Training Loss: 0.09564007073640823 Validation Loss: 0.7609241604804993\n",
      "Epoch 11550: Training Loss: 0.09599910428126653 Validation Loss: 0.7608535885810852\n",
      "Epoch 11551: Training Loss: 0.09543510526418686 Validation Loss: 0.7610899209976196\n",
      "Epoch 11552: Training Loss: 0.09537297238906224 Validation Loss: 0.7609360814094543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11553: Training Loss: 0.09519901623328526 Validation Loss: 0.7610002160072327\n",
      "Epoch 11554: Training Loss: 0.09536706656217575 Validation Loss: 0.7613918781280518\n",
      "Epoch 11555: Training Loss: 0.09527970602114995 Validation Loss: 0.7616533637046814\n",
      "Epoch 11556: Training Loss: 0.09531314422686894 Validation Loss: 0.761681854724884\n",
      "Epoch 11557: Training Loss: 0.09528035173813502 Validation Loss: 0.7612677812576294\n",
      "Epoch 11558: Training Loss: 0.0950937420129776 Validation Loss: 0.7612096667289734\n",
      "Epoch 11559: Training Loss: 0.09548935542503993 Validation Loss: 0.761627197265625\n",
      "Epoch 11560: Training Loss: 0.09509092320998509 Validation Loss: 0.7615721225738525\n",
      "Epoch 11561: Training Loss: 0.09553181380033493 Validation Loss: 0.7613801956176758\n",
      "Epoch 11562: Training Loss: 0.09563815593719482 Validation Loss: 0.7608771920204163\n",
      "Epoch 11563: Training Loss: 0.09511378407478333 Validation Loss: 0.760891318321228\n",
      "Epoch 11564: Training Loss: 0.09560343871514003 Validation Loss: 0.7613538503646851\n",
      "Epoch 11565: Training Loss: 0.09523828576008479 Validation Loss: 0.7612491250038147\n",
      "Epoch 11566: Training Loss: 0.09552409003178279 Validation Loss: 0.7610202431678772\n",
      "Epoch 11567: Training Loss: 0.09508435428142548 Validation Loss: 0.7611250281333923\n",
      "Epoch 11568: Training Loss: 0.09549255917469661 Validation Loss: 0.7613455653190613\n",
      "Epoch 11569: Training Loss: 0.09535183509190877 Validation Loss: 0.7617955207824707\n",
      "Epoch 11570: Training Loss: 0.09608267992734909 Validation Loss: 0.761781632900238\n",
      "Epoch 11571: Training Loss: 0.09534601867198944 Validation Loss: 0.7620053291320801\n",
      "Epoch 11572: Training Loss: 0.09556529422601064 Validation Loss: 0.7623879909515381\n",
      "Epoch 11573: Training Loss: 0.09534650047620137 Validation Loss: 0.7617929577827454\n",
      "Epoch 11574: Training Loss: 0.09582079450289409 Validation Loss: 0.7609303593635559\n",
      "Epoch 11575: Training Loss: 0.09578654418389003 Validation Loss: 0.761105477809906\n",
      "Epoch 11576: Training Loss: 0.09558034936587016 Validation Loss: 0.7618074417114258\n",
      "Epoch 11577: Training Loss: 0.09560306866963704 Validation Loss: 0.7623732089996338\n",
      "Epoch 11578: Training Loss: 0.09560349335273106 Validation Loss: 0.7622132897377014\n",
      "Epoch 11579: Training Loss: 0.09512736399968465 Validation Loss: 0.7618706822395325\n",
      "Epoch 11580: Training Loss: 0.09506411850452423 Validation Loss: 0.761723518371582\n",
      "Epoch 11581: Training Loss: 0.09502569337685902 Validation Loss: 0.7615570425987244\n",
      "Epoch 11582: Training Loss: 0.09527257084846497 Validation Loss: 0.7615050077438354\n",
      "Epoch 11583: Training Loss: 0.09613857169946034 Validation Loss: 0.7616653442382812\n",
      "Epoch 11584: Training Loss: 0.09541996320088704 Validation Loss: 0.7619262337684631\n",
      "Epoch 11585: Training Loss: 0.09525782118240993 Validation Loss: 0.7615087032318115\n",
      "Epoch 11586: Training Loss: 0.09527082989613216 Validation Loss: 0.761191725730896\n",
      "Epoch 11587: Training Loss: 0.09551686296860377 Validation Loss: 0.7612726092338562\n",
      "Epoch 11588: Training Loss: 0.09514094889163971 Validation Loss: 0.7619062662124634\n",
      "Epoch 11589: Training Loss: 0.09537990391254425 Validation Loss: 0.7618967294692993\n",
      "Epoch 11590: Training Loss: 0.09521177411079407 Validation Loss: 0.7616744041442871\n",
      "Epoch 11591: Training Loss: 0.09540586173534393 Validation Loss: 0.76185142993927\n",
      "Epoch 11592: Training Loss: 0.09483599911133449 Validation Loss: 0.761741042137146\n",
      "Epoch 11593: Training Loss: 0.09511636197566986 Validation Loss: 0.7614846229553223\n",
      "Epoch 11594: Training Loss: 0.09509618580341339 Validation Loss: 0.7614755034446716\n",
      "Epoch 11595: Training Loss: 0.09507724891106288 Validation Loss: 0.761576235294342\n",
      "Epoch 11596: Training Loss: 0.09575006117423375 Validation Loss: 0.7619532346725464\n",
      "Epoch 11597: Training Loss: 0.09559377531210582 Validation Loss: 0.7617661952972412\n",
      "Epoch 11598: Training Loss: 0.09552867710590363 Validation Loss: 0.7623736262321472\n",
      "Epoch 11599: Training Loss: 0.09483700493971507 Validation Loss: 0.7624678611755371\n",
      "Epoch 11600: Training Loss: 0.09525030851364136 Validation Loss: 0.7625681161880493\n",
      "Epoch 11601: Training Loss: 0.09523579229911168 Validation Loss: 0.7616801857948303\n",
      "Epoch 11602: Training Loss: 0.09516647706429164 Validation Loss: 0.7609830498695374\n",
      "Epoch 11603: Training Loss: 0.09634690980116527 Validation Loss: 0.7612128257751465\n",
      "Epoch 11604: Training Loss: 0.09550414979457855 Validation Loss: 0.761470377445221\n",
      "Epoch 11605: Training Loss: 0.09527726223071416 Validation Loss: 0.7619037628173828\n",
      "Epoch 11606: Training Loss: 0.09468641877174377 Validation Loss: 0.761889636516571\n",
      "Epoch 11607: Training Loss: 0.09538436681032181 Validation Loss: 0.7624384760856628\n",
      "Epoch 11608: Training Loss: 0.09516870478789012 Validation Loss: 0.7624449729919434\n",
      "Epoch 11609: Training Loss: 0.09529041002194087 Validation Loss: 0.7623481750488281\n",
      "Epoch 11610: Training Loss: 0.09569433828194936 Validation Loss: 0.7619889378547668\n",
      "Epoch 11611: Training Loss: 0.09533422440290451 Validation Loss: 0.7621739506721497\n",
      "Epoch 11612: Training Loss: 0.09533962607383728 Validation Loss: 0.7618666887283325\n",
      "Epoch 11613: Training Loss: 0.09548937280972798 Validation Loss: 0.7621329426765442\n",
      "Epoch 11614: Training Loss: 0.09523949027061462 Validation Loss: 0.7619280815124512\n",
      "Epoch 11615: Training Loss: 0.09499866763750713 Validation Loss: 0.7619724869728088\n",
      "Epoch 11616: Training Loss: 0.09583868086338043 Validation Loss: 0.7625923156738281\n",
      "Epoch 11617: Training Loss: 0.09525065372387569 Validation Loss: 0.7624701857566833\n",
      "Epoch 11618: Training Loss: 0.09514557321866353 Validation Loss: 0.7623489499092102\n",
      "Epoch 11619: Training Loss: 0.0950599933664004 Validation Loss: 0.7623587846755981\n",
      "Epoch 11620: Training Loss: 0.09473865230878194 Validation Loss: 0.7620173096656799\n",
      "Epoch 11621: Training Loss: 0.09505293021599452 Validation Loss: 0.7615774273872375\n",
      "Epoch 11622: Training Loss: 0.09565188487370808 Validation Loss: 0.7615327835083008\n",
      "Epoch 11623: Training Loss: 0.09515953560670216 Validation Loss: 0.7618116736412048\n",
      "Epoch 11624: Training Loss: 0.09539335717757542 Validation Loss: 0.7618480920791626\n",
      "Epoch 11625: Training Loss: 0.09515427301327388 Validation Loss: 0.7620474696159363\n",
      "Epoch 11626: Training Loss: 0.09480767448743184 Validation Loss: 0.7620155215263367\n",
      "Epoch 11627: Training Loss: 0.0951587234934171 Validation Loss: 0.7627339363098145\n",
      "Epoch 11628: Training Loss: 0.09533169865608215 Validation Loss: 0.7629803419113159\n",
      "Epoch 11629: Training Loss: 0.09494941433270772 Validation Loss: 0.7626335024833679\n",
      "Epoch 11630: Training Loss: 0.09486427406469981 Validation Loss: 0.7623291015625\n",
      "Epoch 11631: Training Loss: 0.09499234954516093 Validation Loss: 0.7621456384658813\n",
      "Epoch 11632: Training Loss: 0.09517739216486613 Validation Loss: 0.7622674107551575\n",
      "Epoch 11633: Training Loss: 0.09522803872823715 Validation Loss: 0.7623687386512756\n",
      "Epoch 11634: Training Loss: 0.09460355838139851 Validation Loss: 0.7620987296104431\n",
      "Epoch 11635: Training Loss: 0.09493354956309001 Validation Loss: 0.7622323036193848\n",
      "Epoch 11636: Training Loss: 0.0945788969596227 Validation Loss: 0.762001633644104\n",
      "Epoch 11637: Training Loss: 0.09524235129356384 Validation Loss: 0.7619475722312927\n",
      "Epoch 11638: Training Loss: 0.0950184961160024 Validation Loss: 0.7617336511611938\n",
      "Epoch 11639: Training Loss: 0.09547694772481918 Validation Loss: 0.7622225880622864\n",
      "Epoch 11640: Training Loss: 0.09483107179403305 Validation Loss: 0.7623944282531738\n",
      "Epoch 11641: Training Loss: 0.09535910189151764 Validation Loss: 0.7619795799255371\n",
      "Epoch 11642: Training Loss: 0.09506813685099284 Validation Loss: 0.7620347142219543\n",
      "Epoch 11643: Training Loss: 0.09502206246058147 Validation Loss: 0.7618664503097534\n",
      "Epoch 11644: Training Loss: 0.09526267647743225 Validation Loss: 0.7615172863006592\n",
      "Epoch 11645: Training Loss: 0.09478378544251125 Validation Loss: 0.7619484663009644\n",
      "Epoch 11646: Training Loss: 0.09477157890796661 Validation Loss: 0.7622180581092834\n",
      "Epoch 11647: Training Loss: 0.09505621095498402 Validation Loss: 0.762100338935852\n",
      "Epoch 11648: Training Loss: 0.09481593469778697 Validation Loss: 0.7618014216423035\n",
      "Epoch 11649: Training Loss: 0.09560053298870723 Validation Loss: 0.762146532535553\n",
      "Epoch 11650: Training Loss: 0.09520704795916875 Validation Loss: 0.7621272802352905\n",
      "Epoch 11651: Training Loss: 0.09502913057804108 Validation Loss: 0.7625455260276794\n",
      "Epoch 11652: Training Loss: 0.09503958374261856 Validation Loss: 0.7628765106201172\n",
      "Epoch 11653: Training Loss: 0.09477733323971431 Validation Loss: 0.7624053955078125\n",
      "Epoch 11654: Training Loss: 0.09498487909634908 Validation Loss: 0.762539803981781\n",
      "Epoch 11655: Training Loss: 0.09504545976718266 Validation Loss: 0.761873722076416\n",
      "Epoch 11656: Training Loss: 0.09507477035125096 Validation Loss: 0.7619467973709106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11657: Training Loss: 0.09572500983874004 Validation Loss: 0.7620280981063843\n",
      "Epoch 11658: Training Loss: 0.09475008646647136 Validation Loss: 0.7623111009597778\n",
      "Epoch 11659: Training Loss: 0.09505382925271988 Validation Loss: 0.7621965408325195\n",
      "Epoch 11660: Training Loss: 0.09491108606259029 Validation Loss: 0.7624030113220215\n",
      "Epoch 11661: Training Loss: 0.0946395993232727 Validation Loss: 0.76286780834198\n",
      "Epoch 11662: Training Loss: 0.09478582938512166 Validation Loss: 0.7629242539405823\n",
      "Epoch 11663: Training Loss: 0.09481308112541835 Validation Loss: 0.7629069089889526\n",
      "Epoch 11664: Training Loss: 0.0947016254067421 Validation Loss: 0.762654721736908\n",
      "Epoch 11665: Training Loss: 0.09482265015443166 Validation Loss: 0.7618992924690247\n",
      "Epoch 11666: Training Loss: 0.09479200094938278 Validation Loss: 0.7617368698120117\n",
      "Epoch 11667: Training Loss: 0.09558892746766408 Validation Loss: 0.7627306580543518\n",
      "Epoch 11668: Training Loss: 0.094901442527771 Validation Loss: 0.7629536986351013\n",
      "Epoch 11669: Training Loss: 0.09519702444473903 Validation Loss: 0.762760579586029\n",
      "Epoch 11670: Training Loss: 0.09480175127585729 Validation Loss: 0.7632848024368286\n",
      "Epoch 11671: Training Loss: 0.09475294997294743 Validation Loss: 0.7631787657737732\n",
      "Epoch 11672: Training Loss: 0.09469642986853917 Validation Loss: 0.762799084186554\n",
      "Epoch 11673: Training Loss: 0.0951381375392278 Validation Loss: 0.7624241709709167\n",
      "Epoch 11674: Training Loss: 0.09500949829816818 Validation Loss: 0.7620331048965454\n",
      "Epoch 11675: Training Loss: 0.09501493225495021 Validation Loss: 0.7624338865280151\n",
      "Epoch 11676: Training Loss: 0.09465976059436798 Validation Loss: 0.7626523971557617\n",
      "Epoch 11677: Training Loss: 0.09490529199441274 Validation Loss: 0.7625989317893982\n",
      "Epoch 11678: Training Loss: 0.0958880086739858 Validation Loss: 0.7624356150627136\n",
      "Epoch 11679: Training Loss: 0.09530432025591533 Validation Loss: 0.7628812193870544\n",
      "Epoch 11680: Training Loss: 0.09451866646607716 Validation Loss: 0.7624841332435608\n",
      "Epoch 11681: Training Loss: 0.09512088199456532 Validation Loss: 0.7625093460083008\n",
      "Epoch 11682: Training Loss: 0.09456748267014821 Validation Loss: 0.762347400188446\n",
      "Epoch 11683: Training Loss: 0.09439668307701747 Validation Loss: 0.7628095149993896\n",
      "Epoch 11684: Training Loss: 0.0953436146179835 Validation Loss: 0.7628991007804871\n",
      "Epoch 11685: Training Loss: 0.09445592512687047 Validation Loss: 0.7634568214416504\n",
      "Epoch 11686: Training Loss: 0.09442926694949468 Validation Loss: 0.7627459168434143\n",
      "Epoch 11687: Training Loss: 0.09461582203706105 Validation Loss: 0.7627052068710327\n",
      "Epoch 11688: Training Loss: 0.09504551688830058 Validation Loss: 0.7624022960662842\n",
      "Epoch 11689: Training Loss: 0.09466389069954555 Validation Loss: 0.7622674703598022\n",
      "Epoch 11690: Training Loss: 0.09482429424921672 Validation Loss: 0.7627503275871277\n",
      "Epoch 11691: Training Loss: 0.0963228518764178 Validation Loss: 0.7632076740264893\n",
      "Epoch 11692: Training Loss: 0.09474979341030121 Validation Loss: 0.7631833553314209\n",
      "Epoch 11693: Training Loss: 0.09457096457481384 Validation Loss: 0.7630464434623718\n",
      "Epoch 11694: Training Loss: 0.09475266685088475 Validation Loss: 0.7626611590385437\n",
      "Epoch 11695: Training Loss: 0.0943721483151118 Validation Loss: 0.7626983523368835\n",
      "Epoch 11696: Training Loss: 0.09466147671143214 Validation Loss: 0.7625805139541626\n",
      "Epoch 11697: Training Loss: 0.09466852496067683 Validation Loss: 0.7629737257957458\n",
      "Epoch 11698: Training Loss: 0.0946524515748024 Validation Loss: 0.7627321481704712\n",
      "Epoch 11699: Training Loss: 0.09506332874298096 Validation Loss: 0.7626442313194275\n",
      "Epoch 11700: Training Loss: 0.09483586500088374 Validation Loss: 0.7623836398124695\n",
      "Epoch 11701: Training Loss: 0.09483203540245692 Validation Loss: 0.762497067451477\n",
      "Epoch 11702: Training Loss: 0.0947751800219218 Validation Loss: 0.762638509273529\n",
      "Epoch 11703: Training Loss: 0.09523459772268932 Validation Loss: 0.7631103992462158\n",
      "Epoch 11704: Training Loss: 0.0950652410586675 Validation Loss: 0.7635374069213867\n",
      "Epoch 11705: Training Loss: 0.0947464182972908 Validation Loss: 0.7636768817901611\n",
      "Epoch 11706: Training Loss: 0.09431484838326772 Validation Loss: 0.7631096243858337\n",
      "Epoch 11707: Training Loss: 0.0951872393488884 Validation Loss: 0.7625096440315247\n",
      "Epoch 11708: Training Loss: 0.09490569432576497 Validation Loss: 0.7622689604759216\n",
      "Epoch 11709: Training Loss: 0.09473231931527455 Validation Loss: 0.7626904249191284\n",
      "Epoch 11710: Training Loss: 0.09454208115736644 Validation Loss: 0.7628745436668396\n",
      "Epoch 11711: Training Loss: 0.09482965618371964 Validation Loss: 0.762727677822113\n",
      "Epoch 11712: Training Loss: 0.0951861838499705 Validation Loss: 0.7629537582397461\n",
      "Epoch 11713: Training Loss: 0.09476259847482045 Validation Loss: 0.76286780834198\n",
      "Epoch 11714: Training Loss: 0.09412282953659694 Validation Loss: 0.7631680965423584\n",
      "Epoch 11715: Training Loss: 0.09464822212855022 Validation Loss: 0.7629238367080688\n",
      "Epoch 11716: Training Loss: 0.09442401925722758 Validation Loss: 0.7629384994506836\n",
      "Epoch 11717: Training Loss: 0.09399508188168208 Validation Loss: 0.7630228996276855\n",
      "Epoch 11718: Training Loss: 0.09479762613773346 Validation Loss: 0.7631897330284119\n",
      "Epoch 11719: Training Loss: 0.09479967504739761 Validation Loss: 0.7633214592933655\n",
      "Epoch 11720: Training Loss: 0.0952769195040067 Validation Loss: 0.7633445858955383\n",
      "Epoch 11721: Training Loss: 0.09458003689845403 Validation Loss: 0.7630486488342285\n",
      "Epoch 11722: Training Loss: 0.09428179760773976 Validation Loss: 0.7627711296081543\n",
      "Epoch 11723: Training Loss: 0.09442906826734543 Validation Loss: 0.7627532482147217\n",
      "Epoch 11724: Training Loss: 0.09408079087734222 Validation Loss: 0.7627576589584351\n",
      "Epoch 11725: Training Loss: 0.09475384404261906 Validation Loss: 0.7629532217979431\n",
      "Epoch 11726: Training Loss: 0.09451175977786382 Validation Loss: 0.7631645798683167\n",
      "Epoch 11727: Training Loss: 0.09461368868748347 Validation Loss: 0.7635560631752014\n",
      "Epoch 11728: Training Loss: 0.09446533521016438 Validation Loss: 0.7635670900344849\n",
      "Epoch 11729: Training Loss: 0.09443819274504979 Validation Loss: 0.7632268071174622\n",
      "Epoch 11730: Training Loss: 0.09440474212169647 Validation Loss: 0.7632108926773071\n",
      "Epoch 11731: Training Loss: 0.09453289459149043 Validation Loss: 0.7631456255912781\n",
      "Epoch 11732: Training Loss: 0.0947303647796313 Validation Loss: 0.7632832527160645\n",
      "Epoch 11733: Training Loss: 0.09525682032108307 Validation Loss: 0.7630929350852966\n",
      "Epoch 11734: Training Loss: 0.09436930964390437 Validation Loss: 0.763051450252533\n",
      "Epoch 11735: Training Loss: 0.09432944407065709 Validation Loss: 0.7631179690361023\n",
      "Epoch 11736: Training Loss: 0.09464839100837708 Validation Loss: 0.7637786269187927\n",
      "Epoch 11737: Training Loss: 0.09433710078398387 Validation Loss: 0.7637428045272827\n",
      "Epoch 11738: Training Loss: 0.09483502060174942 Validation Loss: 0.7632409930229187\n",
      "Epoch 11739: Training Loss: 0.09462729096412659 Validation Loss: 0.7631009817123413\n",
      "Epoch 11740: Training Loss: 0.09472692757844925 Validation Loss: 0.7628510594367981\n",
      "Epoch 11741: Training Loss: 0.09449207534392674 Validation Loss: 0.7629727721214294\n",
      "Epoch 11742: Training Loss: 0.09474168966213863 Validation Loss: 0.7632564902305603\n",
      "Epoch 11743: Training Loss: 0.09477465599775314 Validation Loss: 0.7636955380439758\n",
      "Epoch 11744: Training Loss: 0.09454065064589183 Validation Loss: 0.7634434103965759\n",
      "Epoch 11745: Training Loss: 0.0947846844792366 Validation Loss: 0.7631914019584656\n",
      "Epoch 11746: Training Loss: 0.09455608079830806 Validation Loss: 0.7631919384002686\n",
      "Epoch 11747: Training Loss: 0.09482233474651973 Validation Loss: 0.7635229229927063\n",
      "Epoch 11748: Training Loss: 0.09477030734221141 Validation Loss: 0.7639991641044617\n",
      "Epoch 11749: Training Loss: 0.09448964397112529 Validation Loss: 0.7641287446022034\n",
      "Epoch 11750: Training Loss: 0.09429024904966354 Validation Loss: 0.7635447978973389\n",
      "Epoch 11751: Training Loss: 0.09409709026416142 Validation Loss: 0.7636743783950806\n",
      "Epoch 11752: Training Loss: 0.09396208822727203 Validation Loss: 0.7634937167167664\n",
      "Epoch 11753: Training Loss: 0.09441183010737102 Validation Loss: 0.7633044719696045\n",
      "Epoch 11754: Training Loss: 0.09454287588596344 Validation Loss: 0.7630186676979065\n",
      "Epoch 11755: Training Loss: 0.0943496177593867 Validation Loss: 0.7629784345626831\n",
      "Epoch 11756: Training Loss: 0.09419021010398865 Validation Loss: 0.7632548809051514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11757: Training Loss: 0.09454973042011261 Validation Loss: 0.7633019685745239\n",
      "Epoch 11758: Training Loss: 0.09421359996000926 Validation Loss: 0.7632530331611633\n",
      "Epoch 11759: Training Loss: 0.0946594700217247 Validation Loss: 0.7632723450660706\n",
      "Epoch 11760: Training Loss: 0.09452556570370992 Validation Loss: 0.7631696462631226\n",
      "Epoch 11761: Training Loss: 0.09451913585265477 Validation Loss: 0.7634750604629517\n",
      "Epoch 11762: Training Loss: 0.0948120579123497 Validation Loss: 0.7635220289230347\n",
      "Epoch 11763: Training Loss: 0.09452995161215465 Validation Loss: 0.7631562948226929\n",
      "Epoch 11764: Training Loss: 0.09459462761878967 Validation Loss: 0.763055145740509\n",
      "Epoch 11765: Training Loss: 0.09437380482753117 Validation Loss: 0.7632648348808289\n",
      "Epoch 11766: Training Loss: 0.09458681692679723 Validation Loss: 0.7632747888565063\n",
      "Epoch 11767: Training Loss: 0.09459047267834346 Validation Loss: 0.7640634179115295\n",
      "Epoch 11768: Training Loss: 0.09446010241905849 Validation Loss: 0.7638676166534424\n",
      "Epoch 11769: Training Loss: 0.09416566540797551 Validation Loss: 0.7635985016822815\n",
      "Epoch 11770: Training Loss: 0.09468616048494975 Validation Loss: 0.764091968536377\n",
      "Epoch 11771: Training Loss: 0.09446659187475841 Validation Loss: 0.7639001607894897\n",
      "Epoch 11772: Training Loss: 0.0948876291513443 Validation Loss: 0.7637523412704468\n",
      "Epoch 11773: Training Loss: 0.0944972758491834 Validation Loss: 0.7635121941566467\n",
      "Epoch 11774: Training Loss: 0.09422763188680013 Validation Loss: 0.7635638117790222\n",
      "Epoch 11775: Training Loss: 0.09417188912630081 Validation Loss: 0.7637419104576111\n",
      "Epoch 11776: Training Loss: 0.09455715864896774 Validation Loss: 0.7640193104743958\n",
      "Epoch 11777: Training Loss: 0.09421717623869579 Validation Loss: 0.7638658881187439\n",
      "Epoch 11778: Training Loss: 0.09447945406039555 Validation Loss: 0.7641793489456177\n",
      "Epoch 11779: Training Loss: 0.09425111611684163 Validation Loss: 0.7639100551605225\n",
      "Epoch 11780: Training Loss: 0.09464909136295319 Validation Loss: 0.7639095187187195\n",
      "Epoch 11781: Training Loss: 0.09400354325771332 Validation Loss: 0.763845682144165\n",
      "Epoch 11782: Training Loss: 0.09428928047418594 Validation Loss: 0.7636442184448242\n",
      "Epoch 11783: Training Loss: 0.09415119141340256 Validation Loss: 0.7633812427520752\n",
      "Epoch 11784: Training Loss: 0.09416923423608144 Validation Loss: 0.7632878422737122\n",
      "Epoch 11785: Training Loss: 0.09436740477879842 Validation Loss: 0.7632016539573669\n",
      "Epoch 11786: Training Loss: 0.09409726162751515 Validation Loss: 0.7634602785110474\n",
      "Epoch 11787: Training Loss: 0.09431635588407516 Validation Loss: 0.7636896967887878\n",
      "Epoch 11788: Training Loss: 0.09456032514572144 Validation Loss: 0.763328492641449\n",
      "Epoch 11789: Training Loss: 0.09483925501505534 Validation Loss: 0.7638729810714722\n",
      "Epoch 11790: Training Loss: 0.09410704175631206 Validation Loss: 0.7641874551773071\n",
      "Epoch 11791: Training Loss: 0.0945078432559967 Validation Loss: 0.7640509009361267\n",
      "Epoch 11792: Training Loss: 0.09482613205909729 Validation Loss: 0.7636125087738037\n",
      "Epoch 11793: Training Loss: 0.09442899127801259 Validation Loss: 0.7635136842727661\n",
      "Epoch 11794: Training Loss: 0.09415947397549947 Validation Loss: 0.7633368372917175\n",
      "Epoch 11795: Training Loss: 0.09409693131844203 Validation Loss: 0.7630069851875305\n",
      "Epoch 11796: Training Loss: 0.094176118572553 Validation Loss: 0.7632255554199219\n",
      "Epoch 11797: Training Loss: 0.0945888285835584 Validation Loss: 0.7633672952651978\n",
      "Epoch 11798: Training Loss: 0.09409331033627193 Validation Loss: 0.7633820176124573\n",
      "Epoch 11799: Training Loss: 0.09341870993375778 Validation Loss: 0.7637643218040466\n",
      "Epoch 11800: Training Loss: 0.0940482219060262 Validation Loss: 0.7641273736953735\n",
      "Epoch 11801: Training Loss: 0.0941178525487582 Validation Loss: 0.7640044689178467\n",
      "Epoch 11802: Training Loss: 0.09429651250441869 Validation Loss: 0.7641422152519226\n",
      "Epoch 11803: Training Loss: 0.094334381322066 Validation Loss: 0.7636159658432007\n",
      "Epoch 11804: Training Loss: 0.09423513958851497 Validation Loss: 0.7632189989089966\n",
      "Epoch 11805: Training Loss: 0.09453433752059937 Validation Loss: 0.763066291809082\n",
      "Epoch 11806: Training Loss: 0.09430001427729924 Validation Loss: 0.7635049223899841\n",
      "Epoch 11807: Training Loss: 0.09435136367877324 Validation Loss: 0.7641019821166992\n",
      "Epoch 11808: Training Loss: 0.09448436896006267 Validation Loss: 0.764506459236145\n",
      "Epoch 11809: Training Loss: 0.0943320964773496 Validation Loss: 0.7642814517021179\n",
      "Epoch 11810: Training Loss: 0.09534833580255508 Validation Loss: 0.7641863822937012\n",
      "Epoch 11811: Training Loss: 0.09381987899541855 Validation Loss: 0.7639506459236145\n",
      "Epoch 11812: Training Loss: 0.0946528787414233 Validation Loss: 0.7636992335319519\n",
      "Epoch 11813: Training Loss: 0.094175323843956 Validation Loss: 0.763772189617157\n",
      "Epoch 11814: Training Loss: 0.0942730853954951 Validation Loss: 0.7636600732803345\n",
      "Epoch 11815: Training Loss: 0.09401203443606694 Validation Loss: 0.7639012932777405\n",
      "Epoch 11816: Training Loss: 0.09392055372397105 Validation Loss: 0.7640948295593262\n",
      "Epoch 11817: Training Loss: 0.09406925241152446 Validation Loss: 0.7641453742980957\n",
      "Epoch 11818: Training Loss: 0.09480340282122295 Validation Loss: 0.7642350196838379\n",
      "Epoch 11819: Training Loss: 0.09437890350818634 Validation Loss: 0.7641550302505493\n",
      "Epoch 11820: Training Loss: 0.09486430138349533 Validation Loss: 0.7640689611434937\n",
      "Epoch 11821: Training Loss: 0.09407047679026921 Validation Loss: 0.7643845677375793\n",
      "Epoch 11822: Training Loss: 0.09359418352444966 Validation Loss: 0.764240026473999\n",
      "Epoch 11823: Training Loss: 0.09400038421154022 Validation Loss: 0.7637363076210022\n",
      "Epoch 11824: Training Loss: 0.09393673886855443 Validation Loss: 0.7633410096168518\n",
      "Epoch 11825: Training Loss: 0.09395695974429448 Validation Loss: 0.7634668946266174\n",
      "Epoch 11826: Training Loss: 0.09357331444819768 Validation Loss: 0.7637144923210144\n",
      "Epoch 11827: Training Loss: 0.09398563702901204 Validation Loss: 0.7641145586967468\n",
      "Epoch 11828: Training Loss: 0.09462970991929372 Validation Loss: 0.7641410827636719\n",
      "Epoch 11829: Training Loss: 0.0935913696885109 Validation Loss: 0.7641406655311584\n",
      "Epoch 11830: Training Loss: 0.09351933747529984 Validation Loss: 0.7638857364654541\n",
      "Epoch 11831: Training Loss: 0.09385929753383 Validation Loss: 0.7637383341789246\n",
      "Epoch 11832: Training Loss: 0.09427976608276367 Validation Loss: 0.7637332677841187\n",
      "Epoch 11833: Training Loss: 0.09409532447655995 Validation Loss: 0.7637616395950317\n",
      "Epoch 11834: Training Loss: 0.09385998547077179 Validation Loss: 0.7640164494514465\n",
      "Epoch 11835: Training Loss: 0.09379938989877701 Validation Loss: 0.7645644545555115\n",
      "Epoch 11836: Training Loss: 0.09482382486263911 Validation Loss: 0.7643851041793823\n",
      "Epoch 11837: Training Loss: 0.09467398623625438 Validation Loss: 0.7643311619758606\n",
      "Epoch 11838: Training Loss: 0.09362377723058064 Validation Loss: 0.7639412879943848\n",
      "Epoch 11839: Training Loss: 0.0940224826335907 Validation Loss: 0.7640759348869324\n",
      "Epoch 11840: Training Loss: 0.0937917133172353 Validation Loss: 0.763644278049469\n",
      "Epoch 11841: Training Loss: 0.09372153878211975 Validation Loss: 0.7641464471817017\n",
      "Epoch 11842: Training Loss: 0.09401637812455495 Validation Loss: 0.7645490765571594\n",
      "Epoch 11843: Training Loss: 0.09422919899225235 Validation Loss: 0.7643764019012451\n",
      "Epoch 11844: Training Loss: 0.09392240146795909 Validation Loss: 0.7644606828689575\n",
      "Epoch 11845: Training Loss: 0.09381030996640523 Validation Loss: 0.7641927003860474\n",
      "Epoch 11846: Training Loss: 0.09393072873353958 Validation Loss: 0.7642433643341064\n",
      "Epoch 11847: Training Loss: 0.09491126239299774 Validation Loss: 0.7647857666015625\n",
      "Epoch 11848: Training Loss: 0.09395162512858708 Validation Loss: 0.764802098274231\n",
      "Epoch 11849: Training Loss: 0.09366545081138611 Validation Loss: 0.7645891308784485\n",
      "Epoch 11850: Training Loss: 0.0943042238553365 Validation Loss: 0.7642817497253418\n",
      "Epoch 11851: Training Loss: 0.09360431879758835 Validation Loss: 0.7644949555397034\n",
      "Epoch 11852: Training Loss: 0.09368825703859329 Validation Loss: 0.7640876770019531\n",
      "Epoch 11853: Training Loss: 0.09386814882357915 Validation Loss: 0.7640472650527954\n",
      "Epoch 11854: Training Loss: 0.09375079224507014 Validation Loss: 0.7641602754592896\n",
      "Epoch 11855: Training Loss: 0.09394462903340657 Validation Loss: 0.7638067603111267\n",
      "Epoch 11856: Training Loss: 0.09471278140942256 Validation Loss: 0.7639551162719727\n",
      "Epoch 11857: Training Loss: 0.09405224521954854 Validation Loss: 0.7643358707427979\n",
      "Epoch 11858: Training Loss: 0.09418259561061859 Validation Loss: 0.7642756700515747\n",
      "Epoch 11859: Training Loss: 0.093824436267217 Validation Loss: 0.7647319436073303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11860: Training Loss: 0.09391992042462032 Validation Loss: 0.7645797729492188\n",
      "Epoch 11861: Training Loss: 0.0937361146012942 Validation Loss: 0.764384925365448\n",
      "Epoch 11862: Training Loss: 0.09373895327250163 Validation Loss: 0.764434814453125\n",
      "Epoch 11863: Training Loss: 0.09435505171616872 Validation Loss: 0.7648509740829468\n",
      "Epoch 11864: Training Loss: 0.09380556643009186 Validation Loss: 0.7645944952964783\n",
      "Epoch 11865: Training Loss: 0.09359622001647949 Validation Loss: 0.7642938494682312\n",
      "Epoch 11866: Training Loss: 0.09373513360818227 Validation Loss: 0.7644054889678955\n",
      "Epoch 11867: Training Loss: 0.09428518265485764 Validation Loss: 0.7644102573394775\n",
      "Epoch 11868: Training Loss: 0.09390087177356084 Validation Loss: 0.7645865082740784\n",
      "Epoch 11869: Training Loss: 0.09460100034872691 Validation Loss: 0.7641697525978088\n",
      "Epoch 11870: Training Loss: 0.09429727991422017 Validation Loss: 0.7644878029823303\n",
      "Epoch 11871: Training Loss: 0.09412384033203125 Validation Loss: 0.7649953365325928\n",
      "Epoch 11872: Training Loss: 0.09395846724510193 Validation Loss: 0.7654373645782471\n",
      "Epoch 11873: Training Loss: 0.09382671614487965 Validation Loss: 0.7648355960845947\n",
      "Epoch 11874: Training Loss: 0.09421552469333012 Validation Loss: 0.7641674280166626\n",
      "Epoch 11875: Training Loss: 0.09388525784015656 Validation Loss: 0.764028787612915\n",
      "Epoch 11876: Training Loss: 0.09465962648391724 Validation Loss: 0.7639454007148743\n",
      "Epoch 11877: Training Loss: 0.09400729089975357 Validation Loss: 0.7642279267311096\n",
      "Epoch 11878: Training Loss: 0.09368850042422612 Validation Loss: 0.7641701102256775\n",
      "Epoch 11879: Training Loss: 0.09391573071479797 Validation Loss: 0.7643210291862488\n",
      "Epoch 11880: Training Loss: 0.09385147442420323 Validation Loss: 0.7643566727638245\n",
      "Epoch 11881: Training Loss: 0.09391006579001744 Validation Loss: 0.764143705368042\n",
      "Epoch 11882: Training Loss: 0.0939880833029747 Validation Loss: 0.7644782066345215\n",
      "Epoch 11883: Training Loss: 0.09407726178566615 Validation Loss: 0.7645455002784729\n",
      "Epoch 11884: Training Loss: 0.09380078067382176 Validation Loss: 0.7643115520477295\n",
      "Epoch 11885: Training Loss: 0.09421792129675548 Validation Loss: 0.7647619843482971\n",
      "Epoch 11886: Training Loss: 0.09382603317499161 Validation Loss: 0.7650245428085327\n",
      "Epoch 11887: Training Loss: 0.09380273520946503 Validation Loss: 0.7645956873893738\n",
      "Epoch 11888: Training Loss: 0.09428311387697856 Validation Loss: 0.7643997669219971\n",
      "Epoch 11889: Training Loss: 0.0939026027917862 Validation Loss: 0.7647114992141724\n",
      "Epoch 11890: Training Loss: 0.0941692665219307 Validation Loss: 0.7650653719902039\n",
      "Epoch 11891: Training Loss: 0.09370389332373937 Validation Loss: 0.7654170989990234\n",
      "Epoch 11892: Training Loss: 0.09486354142427444 Validation Loss: 0.765238344669342\n",
      "Epoch 11893: Training Loss: 0.09385839601357777 Validation Loss: 0.764667272567749\n",
      "Epoch 11894: Training Loss: 0.09369196742773056 Validation Loss: 0.7643386125564575\n",
      "Epoch 11895: Training Loss: 0.09414171427488327 Validation Loss: 0.7643483281135559\n",
      "Epoch 11896: Training Loss: 0.09336194147666295 Validation Loss: 0.7644681334495544\n",
      "Epoch 11897: Training Loss: 0.0937344878911972 Validation Loss: 0.7649866342544556\n",
      "Epoch 11898: Training Loss: 0.095167376101017 Validation Loss: 0.765285074710846\n",
      "Epoch 11899: Training Loss: 0.09390842169523239 Validation Loss: 0.7653286457061768\n",
      "Epoch 11900: Training Loss: 0.0935864473382632 Validation Loss: 0.7647023797035217\n",
      "Epoch 11901: Training Loss: 0.0936436802148819 Validation Loss: 0.7644809484481812\n",
      "Epoch 11902: Training Loss: 0.09398576617240906 Validation Loss: 0.7645692229270935\n",
      "Epoch 11903: Training Loss: 0.09403812388579051 Validation Loss: 0.7645002007484436\n",
      "Epoch 11904: Training Loss: 0.09389437486728032 Validation Loss: 0.7647696137428284\n",
      "Epoch 11905: Training Loss: 0.09366816530625026 Validation Loss: 0.7647659778594971\n",
      "Epoch 11906: Training Loss: 0.09373888621727626 Validation Loss: 0.7643937468528748\n",
      "Epoch 11907: Training Loss: 0.09402873367071152 Validation Loss: 0.7641052007675171\n",
      "Epoch 11908: Training Loss: 0.09356152266263962 Validation Loss: 0.7641593217849731\n",
      "Epoch 11909: Training Loss: 0.09445963551600774 Validation Loss: 0.7642026543617249\n",
      "Epoch 11910: Training Loss: 0.0935461198290189 Validation Loss: 0.7644945383071899\n",
      "Epoch 11911: Training Loss: 0.09397527078787486 Validation Loss: 0.7648649215698242\n",
      "Epoch 11912: Training Loss: 0.09373346467812856 Validation Loss: 0.7650429606437683\n",
      "Epoch 11913: Training Loss: 0.09348522871732712 Validation Loss: 0.7650686502456665\n",
      "Epoch 11914: Training Loss: 0.09426358342170715 Validation Loss: 0.7648276686668396\n",
      "Epoch 11915: Training Loss: 0.09368334462245305 Validation Loss: 0.7644890546798706\n",
      "Epoch 11916: Training Loss: 0.09351680676142375 Validation Loss: 0.7643818855285645\n",
      "Epoch 11917: Training Loss: 0.09380798786878586 Validation Loss: 0.7646929621696472\n",
      "Epoch 11918: Training Loss: 0.09379583845535915 Validation Loss: 0.765033483505249\n",
      "Epoch 11919: Training Loss: 0.0935876692334811 Validation Loss: 0.7650277614593506\n",
      "Epoch 11920: Training Loss: 0.09341913958390553 Validation Loss: 0.7650049924850464\n",
      "Epoch 11921: Training Loss: 0.09347015370925267 Validation Loss: 0.7648226618766785\n",
      "Epoch 11922: Training Loss: 0.0933833693464597 Validation Loss: 0.7643342614173889\n",
      "Epoch 11923: Training Loss: 0.09371915459632874 Validation Loss: 0.7642120718955994\n",
      "Epoch 11924: Training Loss: 0.09361013025045395 Validation Loss: 0.7642375230789185\n",
      "Epoch 11925: Training Loss: 0.09345906724532445 Validation Loss: 0.7647885084152222\n",
      "Epoch 11926: Training Loss: 0.09425211697816849 Validation Loss: 0.7651073932647705\n",
      "Epoch 11927: Training Loss: 0.09332039455572765 Validation Loss: 0.7650765776634216\n",
      "Epoch 11928: Training Loss: 0.09381779034932454 Validation Loss: 0.7649730443954468\n",
      "Epoch 11929: Training Loss: 0.09466429799795151 Validation Loss: 0.7651007175445557\n",
      "Epoch 11930: Training Loss: 0.09373294562101364 Validation Loss: 0.7654693722724915\n",
      "Epoch 11931: Training Loss: 0.09350990255673726 Validation Loss: 0.7650492191314697\n",
      "Epoch 11932: Training Loss: 0.09341912964979808 Validation Loss: 0.7646048069000244\n",
      "Epoch 11933: Training Loss: 0.093812162677447 Validation Loss: 0.7642090320587158\n",
      "Epoch 11934: Training Loss: 0.09398429840803146 Validation Loss: 0.7645547986030579\n",
      "Epoch 11935: Training Loss: 0.09390750775734584 Validation Loss: 0.7648969888687134\n",
      "Epoch 11936: Training Loss: 0.09329417844613393 Validation Loss: 0.7648632526397705\n",
      "Epoch 11937: Training Loss: 0.09345579147338867 Validation Loss: 0.7650319337844849\n",
      "Epoch 11938: Training Loss: 0.09415241579214732 Validation Loss: 0.7652074098587036\n",
      "Epoch 11939: Training Loss: 0.0935313453276952 Validation Loss: 0.765955924987793\n",
      "Epoch 11940: Training Loss: 0.09357182681560516 Validation Loss: 0.7656543254852295\n",
      "Epoch 11941: Training Loss: 0.09361915290355682 Validation Loss: 0.7654424905776978\n",
      "Epoch 11942: Training Loss: 0.09342335661252339 Validation Loss: 0.7649281620979309\n",
      "Epoch 11943: Training Loss: 0.09409408022960027 Validation Loss: 0.7648459672927856\n",
      "Epoch 11944: Training Loss: 0.09375155965487163 Validation Loss: 0.7648404240608215\n",
      "Epoch 11945: Training Loss: 0.09345608452955882 Validation Loss: 0.7655796408653259\n",
      "Epoch 11946: Training Loss: 0.0939137265086174 Validation Loss: 0.765322744846344\n",
      "Epoch 11947: Training Loss: 0.093448373178641 Validation Loss: 0.765558123588562\n",
      "Epoch 11948: Training Loss: 0.0934911494453748 Validation Loss: 0.7654696106910706\n",
      "Epoch 11949: Training Loss: 0.09389908860127132 Validation Loss: 0.7657796740531921\n",
      "Epoch 11950: Training Loss: 0.09303566565116246 Validation Loss: 0.7652409076690674\n",
      "Epoch 11951: Training Loss: 0.09315744539101918 Validation Loss: 0.7651968598365784\n",
      "Epoch 11952: Training Loss: 0.09337723006804784 Validation Loss: 0.7653933167457581\n",
      "Epoch 11953: Training Loss: 0.09343534459670384 Validation Loss: 0.7654788494110107\n",
      "Epoch 11954: Training Loss: 0.09338505566120148 Validation Loss: 0.7654851675033569\n",
      "Epoch 11955: Training Loss: 0.09347190956274669 Validation Loss: 0.7650207281112671\n",
      "Epoch 11956: Training Loss: 0.09366276860237122 Validation Loss: 0.7654991149902344\n",
      "Epoch 11957: Training Loss: 0.0936225454012553 Validation Loss: 0.7653176188468933\n",
      "Epoch 11958: Training Loss: 0.09418819348017375 Validation Loss: 0.7647885680198669\n",
      "Epoch 11959: Training Loss: 0.09328500678141911 Validation Loss: 0.7653923034667969\n",
      "Epoch 11960: Training Loss: 0.09389472007751465 Validation Loss: 0.7655800580978394\n",
      "Epoch 11961: Training Loss: 0.09344484905401866 Validation Loss: 0.7657142281532288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11962: Training Loss: 0.09339416523774464 Validation Loss: 0.7658123970031738\n",
      "Epoch 11963: Training Loss: 0.09345202893018723 Validation Loss: 0.7657270431518555\n",
      "Epoch 11964: Training Loss: 0.09378653764724731 Validation Loss: 0.7651318907737732\n",
      "Epoch 11965: Training Loss: 0.0929656649629275 Validation Loss: 0.7651480436325073\n",
      "Epoch 11966: Training Loss: 0.09330810109774272 Validation Loss: 0.765369176864624\n",
      "Epoch 11967: Training Loss: 0.09378647059202194 Validation Loss: 0.7656195163726807\n",
      "Epoch 11968: Training Loss: 0.09300703555345535 Validation Loss: 0.7655893564224243\n",
      "Epoch 11969: Training Loss: 0.09347128371397655 Validation Loss: 0.7657523155212402\n",
      "Epoch 11970: Training Loss: 0.09349048882722855 Validation Loss: 0.7657455801963806\n",
      "Epoch 11971: Training Loss: 0.09318181375662486 Validation Loss: 0.7656611800193787\n",
      "Epoch 11972: Training Loss: 0.0934055323402087 Validation Loss: 0.7655883431434631\n",
      "Epoch 11973: Training Loss: 0.09280878553787868 Validation Loss: 0.7654971480369568\n",
      "Epoch 11974: Training Loss: 0.09319876631100972 Validation Loss: 0.7654227614402771\n",
      "Epoch 11975: Training Loss: 0.0931489219268163 Validation Loss: 0.7656332850456238\n",
      "Epoch 11976: Training Loss: 0.09379404286543529 Validation Loss: 0.7653447985649109\n",
      "Epoch 11977: Training Loss: 0.0934447596470515 Validation Loss: 0.7654120922088623\n",
      "Epoch 11978: Training Loss: 0.09331825375556946 Validation Loss: 0.7655704021453857\n",
      "Epoch 11979: Training Loss: 0.09355574597915013 Validation Loss: 0.765595555305481\n",
      "Epoch 11980: Training Loss: 0.09322855124870937 Validation Loss: 0.7652345895767212\n",
      "Epoch 11981: Training Loss: 0.093662994603316 Validation Loss: 0.7650821208953857\n",
      "Epoch 11982: Training Loss: 0.09310206025838852 Validation Loss: 0.7651433944702148\n",
      "Epoch 11983: Training Loss: 0.0932735080520312 Validation Loss: 0.7652347683906555\n",
      "Epoch 11984: Training Loss: 0.0936154176791509 Validation Loss: 0.7649732828140259\n",
      "Epoch 11985: Training Loss: 0.09374448905388515 Validation Loss: 0.7652617692947388\n",
      "Epoch 11986: Training Loss: 0.09337944785753886 Validation Loss: 0.7655889391899109\n",
      "Epoch 11987: Training Loss: 0.09344870845476787 Validation Loss: 0.765569269657135\n",
      "Epoch 11988: Training Loss: 0.09373192985852559 Validation Loss: 0.7654550075531006\n",
      "Epoch 11989: Training Loss: 0.09356275200843811 Validation Loss: 0.7661468982696533\n",
      "Epoch 11990: Training Loss: 0.0934821218252182 Validation Loss: 0.7662649750709534\n",
      "Epoch 11991: Training Loss: 0.09323201328516006 Validation Loss: 0.7660804986953735\n",
      "Epoch 11992: Training Loss: 0.09370343138774236 Validation Loss: 0.7656275033950806\n",
      "Epoch 11993: Training Loss: 0.09327095746994019 Validation Loss: 0.7653151750564575\n",
      "Epoch 11994: Training Loss: 0.09396489957968394 Validation Loss: 0.7655772566795349\n",
      "Epoch 11995: Training Loss: 0.09303442637125652 Validation Loss: 0.7657305002212524\n",
      "Epoch 11996: Training Loss: 0.09303938349088033 Validation Loss: 0.7659275531768799\n",
      "Epoch 11997: Training Loss: 0.09328783800204594 Validation Loss: 0.7657440900802612\n",
      "Epoch 11998: Training Loss: 0.09323907146851222 Validation Loss: 0.7653209567070007\n",
      "Epoch 11999: Training Loss: 0.09399435420831044 Validation Loss: 0.7654328346252441\n",
      "Epoch 12000: Training Loss: 0.09278351068496704 Validation Loss: 0.7656225562095642\n",
      "Epoch 12001: Training Loss: 0.09384722759326299 Validation Loss: 0.7660329341888428\n",
      "Epoch 12002: Training Loss: 0.09329040348529816 Validation Loss: 0.7662681937217712\n",
      "Epoch 12003: Training Loss: 0.09312737236420314 Validation Loss: 0.765942394733429\n",
      "Epoch 12004: Training Loss: 0.09408233563105266 Validation Loss: 0.7654169201850891\n",
      "Epoch 12005: Training Loss: 0.09326439847548802 Validation Loss: 0.7657390236854553\n",
      "Epoch 12006: Training Loss: 0.09322527547677358 Validation Loss: 0.7659800052642822\n",
      "Epoch 12007: Training Loss: 0.0933684508005778 Validation Loss: 0.7661417722702026\n",
      "Epoch 12008: Training Loss: 0.09343362599611282 Validation Loss: 0.7657918930053711\n",
      "Epoch 12009: Training Loss: 0.09325125813484192 Validation Loss: 0.7661334872245789\n",
      "Epoch 12010: Training Loss: 0.09323616822560628 Validation Loss: 0.7664259672164917\n",
      "Epoch 12011: Training Loss: 0.09369961420694987 Validation Loss: 0.7660729289054871\n",
      "Epoch 12012: Training Loss: 0.09365189323822658 Validation Loss: 0.7653221487998962\n",
      "Epoch 12013: Training Loss: 0.09278581539789836 Validation Loss: 0.7657389640808105\n",
      "Epoch 12014: Training Loss: 0.0930052101612091 Validation Loss: 0.7661815881729126\n",
      "Epoch 12015: Training Loss: 0.09307764718929927 Validation Loss: 0.7661314606666565\n",
      "Epoch 12016: Training Loss: 0.09307913730541865 Validation Loss: 0.7660478353500366\n",
      "Epoch 12017: Training Loss: 0.09365350256363551 Validation Loss: 0.7659693956375122\n",
      "Epoch 12018: Training Loss: 0.09293096264203389 Validation Loss: 0.7660254836082458\n",
      "Epoch 12019: Training Loss: 0.09290329863627751 Validation Loss: 0.7658891081809998\n",
      "Epoch 12020: Training Loss: 0.09321443984905879 Validation Loss: 0.7656687498092651\n",
      "Epoch 12021: Training Loss: 0.09342693289120992 Validation Loss: 0.7654988765716553\n",
      "Epoch 12022: Training Loss: 0.0935774768392245 Validation Loss: 0.7650774121284485\n",
      "Epoch 12023: Training Loss: 0.09319740285476048 Validation Loss: 0.7654320597648621\n",
      "Epoch 12024: Training Loss: 0.09309111038843791 Validation Loss: 0.7657721638679504\n",
      "Epoch 12025: Training Loss: 0.09281784296035767 Validation Loss: 0.7659209370613098\n",
      "Epoch 12026: Training Loss: 0.09312201042970021 Validation Loss: 0.7661539316177368\n",
      "Epoch 12027: Training Loss: 0.09325367957353592 Validation Loss: 0.7656700611114502\n",
      "Epoch 12028: Training Loss: 0.09288142124811809 Validation Loss: 0.7656033635139465\n",
      "Epoch 12029: Training Loss: 0.09277861813704173 Validation Loss: 0.7656002044677734\n",
      "Epoch 12030: Training Loss: 0.09310461829106013 Validation Loss: 0.7657607793807983\n",
      "Epoch 12031: Training Loss: 0.09294751038153966 Validation Loss: 0.7657071352005005\n",
      "Epoch 12032: Training Loss: 0.09309874226649602 Validation Loss: 0.7658756375312805\n",
      "Epoch 12033: Training Loss: 0.09325341135263443 Validation Loss: 0.7657790184020996\n",
      "Epoch 12034: Training Loss: 0.09372409929831822 Validation Loss: 0.7658085823059082\n",
      "Epoch 12035: Training Loss: 0.09328177074591319 Validation Loss: 0.7658942341804504\n",
      "Epoch 12036: Training Loss: 0.09306294471025467 Validation Loss: 0.7658002376556396\n",
      "Epoch 12037: Training Loss: 0.09306529661019643 Validation Loss: 0.7659461498260498\n",
      "Epoch 12038: Training Loss: 0.09309101849794388 Validation Loss: 0.7661942839622498\n",
      "Epoch 12039: Training Loss: 0.0929996023575465 Validation Loss: 0.7660621404647827\n",
      "Epoch 12040: Training Loss: 0.09304620822270711 Validation Loss: 0.7657550573348999\n",
      "Epoch 12041: Training Loss: 0.09320241957902908 Validation Loss: 0.7661598324775696\n",
      "Epoch 12042: Training Loss: 0.09315448254346848 Validation Loss: 0.7660551071166992\n",
      "Epoch 12043: Training Loss: 0.09360814839601517 Validation Loss: 0.7658529877662659\n",
      "Epoch 12044: Training Loss: 0.09327289462089539 Validation Loss: 0.766562819480896\n",
      "Epoch 12045: Training Loss: 0.09323105961084366 Validation Loss: 0.7670016288757324\n",
      "Epoch 12046: Training Loss: 0.09364120413859685 Validation Loss: 0.7662218809127808\n",
      "Epoch 12047: Training Loss: 0.09345872203509013 Validation Loss: 0.765614926815033\n",
      "Epoch 12048: Training Loss: 0.09300518284241359 Validation Loss: 0.7654137015342712\n",
      "Epoch 12049: Training Loss: 0.09302399059136708 Validation Loss: 0.7657189965248108\n",
      "Epoch 12050: Training Loss: 0.09311287353436153 Validation Loss: 0.7660962343215942\n",
      "Epoch 12051: Training Loss: 0.09291118880112965 Validation Loss: 0.7659054398536682\n",
      "Epoch 12052: Training Loss: 0.09301283210515976 Validation Loss: 0.7657327651977539\n",
      "Epoch 12053: Training Loss: 0.09356427937746048 Validation Loss: 0.7661252021789551\n",
      "Epoch 12054: Training Loss: 0.09292830278476079 Validation Loss: 0.766538679599762\n",
      "Epoch 12055: Training Loss: 0.0943479264775912 Validation Loss: 0.766753077507019\n",
      "Epoch 12056: Training Loss: 0.0939113621910413 Validation Loss: 0.7666229009628296\n",
      "Epoch 12057: Training Loss: 0.09321056803067525 Validation Loss: 0.7661357522010803\n",
      "Epoch 12058: Training Loss: 0.09320225318272908 Validation Loss: 0.7659521698951721\n",
      "Epoch 12059: Training Loss: 0.09318061918020248 Validation Loss: 0.765507161617279\n",
      "Epoch 12060: Training Loss: 0.0932072252035141 Validation Loss: 0.7662044763565063\n",
      "Epoch 12061: Training Loss: 0.09317951152722041 Validation Loss: 0.7660679221153259\n",
      "Epoch 12062: Training Loss: 0.09297967453797658 Validation Loss: 0.7660905122756958\n",
      "Epoch 12063: Training Loss: 0.09352536996205647 Validation Loss: 0.7656762003898621\n",
      "Epoch 12064: Training Loss: 0.09322250634431839 Validation Loss: 0.7654266953468323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12065: Training Loss: 0.0928510253628095 Validation Loss: 0.7654417753219604\n",
      "Epoch 12066: Training Loss: 0.09299852202335994 Validation Loss: 0.7653168439865112\n",
      "Epoch 12067: Training Loss: 0.0930073211590449 Validation Loss: 0.7659823894500732\n",
      "Epoch 12068: Training Loss: 0.09266404062509537 Validation Loss: 0.7665700316429138\n",
      "Epoch 12069: Training Loss: 0.09305358678102493 Validation Loss: 0.7672621011734009\n",
      "Epoch 12070: Training Loss: 0.09323679655790329 Validation Loss: 0.7671029567718506\n",
      "Epoch 12071: Training Loss: 0.09410893420378368 Validation Loss: 0.7667466998100281\n",
      "Epoch 12072: Training Loss: 0.09282921999692917 Validation Loss: 0.7666240334510803\n",
      "Epoch 12073: Training Loss: 0.09312286972999573 Validation Loss: 0.7662909626960754\n",
      "Epoch 12074: Training Loss: 0.09262257814407349 Validation Loss: 0.7662028670310974\n",
      "Epoch 12075: Training Loss: 0.09305231273174286 Validation Loss: 0.7666046023368835\n",
      "Epoch 12076: Training Loss: 0.09300216535727183 Validation Loss: 0.7665268182754517\n",
      "Epoch 12077: Training Loss: 0.09291398028532664 Validation Loss: 0.7663911581039429\n",
      "Epoch 12078: Training Loss: 0.09298305710156758 Validation Loss: 0.7668201327323914\n",
      "Epoch 12079: Training Loss: 0.09292777131001155 Validation Loss: 0.7667136192321777\n",
      "Epoch 12080: Training Loss: 0.09294264515240987 Validation Loss: 0.7663277387619019\n",
      "Epoch 12081: Training Loss: 0.0927260269721349 Validation Loss: 0.76614910364151\n",
      "Epoch 12082: Training Loss: 0.0931187445918719 Validation Loss: 0.7663589715957642\n",
      "Epoch 12083: Training Loss: 0.09267290929953258 Validation Loss: 0.7662365436553955\n",
      "Epoch 12084: Training Loss: 0.09281991422176361 Validation Loss: 0.7662524580955505\n",
      "Epoch 12085: Training Loss: 0.09277652949094772 Validation Loss: 0.7663758993148804\n",
      "Epoch 12086: Training Loss: 0.09253405282894771 Validation Loss: 0.7665206789970398\n",
      "Epoch 12087: Training Loss: 0.09287726134061813 Validation Loss: 0.7671658992767334\n",
      "Epoch 12088: Training Loss: 0.09287456671396892 Validation Loss: 0.7668060064315796\n",
      "Epoch 12089: Training Loss: 0.09274397045373917 Validation Loss: 0.766389012336731\n",
      "Epoch 12090: Training Loss: 0.09407082945108414 Validation Loss: 0.7661025524139404\n",
      "Epoch 12091: Training Loss: 0.09324528276920319 Validation Loss: 0.7664809823036194\n",
      "Epoch 12092: Training Loss: 0.09277039269606273 Validation Loss: 0.7666413187980652\n",
      "Epoch 12093: Training Loss: 0.09259859969218572 Validation Loss: 0.7668324112892151\n",
      "Epoch 12094: Training Loss: 0.09289829681317012 Validation Loss: 0.766898512840271\n",
      "Epoch 12095: Training Loss: 0.09274661044279735 Validation Loss: 0.7666900753974915\n",
      "Epoch 12096: Training Loss: 0.092933955291907 Validation Loss: 0.7668535113334656\n",
      "Epoch 12097: Training Loss: 0.09257263938585918 Validation Loss: 0.7667038440704346\n",
      "Epoch 12098: Training Loss: 0.09276759872833888 Validation Loss: 0.7666762471199036\n",
      "Epoch 12099: Training Loss: 0.09314870089292526 Validation Loss: 0.7663585543632507\n",
      "Epoch 12100: Training Loss: 0.09297729283571243 Validation Loss: 0.7663915157318115\n",
      "Epoch 12101: Training Loss: 0.09303502241770427 Validation Loss: 0.7660616636276245\n",
      "Epoch 12102: Training Loss: 0.09261281291643779 Validation Loss: 0.7663569450378418\n",
      "Epoch 12103: Training Loss: 0.09290539224942525 Validation Loss: 0.7670382261276245\n",
      "Epoch 12104: Training Loss: 0.09278342872858047 Validation Loss: 0.7671695351600647\n",
      "Epoch 12105: Training Loss: 0.09304023037354152 Validation Loss: 0.7670251131057739\n",
      "Epoch 12106: Training Loss: 0.09258706122636795 Validation Loss: 0.7668167352676392\n",
      "Epoch 12107: Training Loss: 0.09292994687954585 Validation Loss: 0.7669851779937744\n",
      "Epoch 12108: Training Loss: 0.09281303981939952 Validation Loss: 0.7668365836143494\n",
      "Epoch 12109: Training Loss: 0.093113308151563 Validation Loss: 0.7668808698654175\n",
      "Epoch 12110: Training Loss: 0.0927324468890826 Validation Loss: 0.7667005062103271\n",
      "Epoch 12111: Training Loss: 0.09335434188445409 Validation Loss: 0.7664668560028076\n",
      "Epoch 12112: Training Loss: 0.09264459957679112 Validation Loss: 0.7662472724914551\n",
      "Epoch 12113: Training Loss: 0.09262767185767491 Validation Loss: 0.76603764295578\n",
      "Epoch 12114: Training Loss: 0.09248600900173187 Validation Loss: 0.7665308713912964\n",
      "Epoch 12115: Training Loss: 0.09329664955536525 Validation Loss: 0.766502320766449\n",
      "Epoch 12116: Training Loss: 0.09270079185565312 Validation Loss: 0.7671861052513123\n",
      "Epoch 12117: Training Loss: 0.0929249922434489 Validation Loss: 0.7676275372505188\n",
      "Epoch 12118: Training Loss: 0.09285070498784383 Validation Loss: 0.7671858668327332\n",
      "Epoch 12119: Training Loss: 0.09277515361706416 Validation Loss: 0.7670342922210693\n",
      "Epoch 12120: Training Loss: 0.09309132893880208 Validation Loss: 0.766562283039093\n",
      "Epoch 12121: Training Loss: 0.09282481422026952 Validation Loss: 0.7660437226295471\n",
      "Epoch 12122: Training Loss: 0.0927935242652893 Validation Loss: 0.7662387490272522\n",
      "Epoch 12123: Training Loss: 0.09332034240166347 Validation Loss: 0.7671026587486267\n",
      "Epoch 12124: Training Loss: 0.09298717727263768 Validation Loss: 0.7669907212257385\n",
      "Epoch 12125: Training Loss: 0.09303866823514302 Validation Loss: 0.7669151425361633\n",
      "Epoch 12126: Training Loss: 0.09290108333031337 Validation Loss: 0.7671452760696411\n",
      "Epoch 12127: Training Loss: 0.09279562036196391 Validation Loss: 0.7675826549530029\n",
      "Epoch 12128: Training Loss: 0.09276609867811203 Validation Loss: 0.766994297504425\n",
      "Epoch 12129: Training Loss: 0.09287469337383907 Validation Loss: 0.7671343684196472\n",
      "Epoch 12130: Training Loss: 0.09278291463851929 Validation Loss: 0.7673553824424744\n",
      "Epoch 12131: Training Loss: 0.09287866453329723 Validation Loss: 0.7674633264541626\n",
      "Epoch 12132: Training Loss: 0.09253764649232228 Validation Loss: 0.7672795653343201\n",
      "Epoch 12133: Training Loss: 0.09289763867855072 Validation Loss: 0.7671573758125305\n",
      "Epoch 12134: Training Loss: 0.0926993985970815 Validation Loss: 0.7673289179801941\n",
      "Epoch 12135: Training Loss: 0.09284266332785289 Validation Loss: 0.7675495743751526\n",
      "Epoch 12136: Training Loss: 0.09261138240496318 Validation Loss: 0.767713725566864\n",
      "Epoch 12137: Training Loss: 0.09291936705509822 Validation Loss: 0.7672121524810791\n",
      "Epoch 12138: Training Loss: 0.09253997852404912 Validation Loss: 0.767251193523407\n",
      "Epoch 12139: Training Loss: 0.09253238141536713 Validation Loss: 0.7671635150909424\n",
      "Epoch 12140: Training Loss: 0.09288576742013295 Validation Loss: 0.7669137120246887\n",
      "Epoch 12141: Training Loss: 0.0934783269961675 Validation Loss: 0.7667140960693359\n",
      "Epoch 12142: Training Loss: 0.09304045885801315 Validation Loss: 0.7667622566223145\n",
      "Epoch 12143: Training Loss: 0.09286316732565562 Validation Loss: 0.7666674256324768\n",
      "Epoch 12144: Training Loss: 0.0928576687971751 Validation Loss: 0.7674499154090881\n",
      "Epoch 12145: Training Loss: 0.09277360637982686 Validation Loss: 0.76744145154953\n",
      "Epoch 12146: Training Loss: 0.0924251601099968 Validation Loss: 0.7673011422157288\n",
      "Epoch 12147: Training Loss: 0.09297378112872441 Validation Loss: 0.7670332193374634\n",
      "Epoch 12148: Training Loss: 0.0923369899392128 Validation Loss: 0.7668507099151611\n",
      "Epoch 12149: Training Loss: 0.09315589567025502 Validation Loss: 0.7670904994010925\n",
      "Epoch 12150: Training Loss: 0.09240876883268356 Validation Loss: 0.7670830488204956\n",
      "Epoch 12151: Training Loss: 0.09251925349235535 Validation Loss: 0.7674129605293274\n",
      "Epoch 12152: Training Loss: 0.09270580609639485 Validation Loss: 0.7676198482513428\n",
      "Epoch 12153: Training Loss: 0.09262652446826299 Validation Loss: 0.7672897577285767\n",
      "Epoch 12154: Training Loss: 0.09265996764103572 Validation Loss: 0.7672477960586548\n",
      "Epoch 12155: Training Loss: 0.09255430847406387 Validation Loss: 0.766927182674408\n",
      "Epoch 12156: Training Loss: 0.09260790050029755 Validation Loss: 0.7669357657432556\n",
      "Epoch 12157: Training Loss: 0.09256544212500255 Validation Loss: 0.7669146060943604\n",
      "Epoch 12158: Training Loss: 0.09338788936535518 Validation Loss: 0.7670657634735107\n",
      "Epoch 12159: Training Loss: 0.09254541993141174 Validation Loss: 0.767660915851593\n",
      "Epoch 12160: Training Loss: 0.09273522098859151 Validation Loss: 0.7676633596420288\n",
      "Epoch 12161: Training Loss: 0.09259647379318874 Validation Loss: 0.7671279907226562\n",
      "Epoch 12162: Training Loss: 0.09256406625111897 Validation Loss: 0.7668277025222778\n",
      "Epoch 12163: Training Loss: 0.09315163393815358 Validation Loss: 0.7666520476341248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12164: Training Loss: 0.09235335886478424 Validation Loss: 0.7670179009437561\n",
      "Epoch 12165: Training Loss: 0.0928334320584933 Validation Loss: 0.767341673374176\n",
      "Epoch 12166: Training Loss: 0.09250152359406154 Validation Loss: 0.7678977251052856\n",
      "Epoch 12167: Training Loss: 0.0925423229734103 Validation Loss: 0.7681776881217957\n",
      "Epoch 12168: Training Loss: 0.09236794461806615 Validation Loss: 0.7677541971206665\n",
      "Epoch 12169: Training Loss: 0.09269606073697408 Validation Loss: 0.7673683762550354\n",
      "Epoch 12170: Training Loss: 0.09266096105178197 Validation Loss: 0.7672019600868225\n",
      "Epoch 12171: Training Loss: 0.0924408237139384 Validation Loss: 0.7675285339355469\n",
      "Epoch 12172: Training Loss: 0.09254897385835648 Validation Loss: 0.7673683166503906\n",
      "Epoch 12173: Training Loss: 0.09274217238028844 Validation Loss: 0.767210066318512\n",
      "Epoch 12174: Training Loss: 0.09271553407112758 Validation Loss: 0.7670407295227051\n",
      "Epoch 12175: Training Loss: 0.09246902912855148 Validation Loss: 0.7671787142753601\n",
      "Epoch 12176: Training Loss: 0.09299248208602269 Validation Loss: 0.7674379944801331\n",
      "Epoch 12177: Training Loss: 0.09260280927022298 Validation Loss: 0.7678959965705872\n",
      "Epoch 12178: Training Loss: 0.09256019939978917 Validation Loss: 0.7680029273033142\n",
      "Epoch 12179: Training Loss: 0.09241828322410583 Validation Loss: 0.7674924731254578\n",
      "Epoch 12180: Training Loss: 0.0925753116607666 Validation Loss: 0.7670574188232422\n",
      "Epoch 12181: Training Loss: 0.09339933842420578 Validation Loss: 0.7673463821411133\n",
      "Epoch 12182: Training Loss: 0.09220952540636063 Validation Loss: 0.7670685648918152\n",
      "Epoch 12183: Training Loss: 0.09246841073036194 Validation Loss: 0.7671996355056763\n",
      "Epoch 12184: Training Loss: 0.09238513807455699 Validation Loss: 0.7671826481819153\n",
      "Epoch 12185: Training Loss: 0.09251786023378372 Validation Loss: 0.7673736214637756\n",
      "Epoch 12186: Training Loss: 0.09319225947062175 Validation Loss: 0.7676233053207397\n",
      "Epoch 12187: Training Loss: 0.09262704600890477 Validation Loss: 0.767694354057312\n",
      "Epoch 12188: Training Loss: 0.09254101912180583 Validation Loss: 0.7669998407363892\n",
      "Epoch 12189: Training Loss: 0.09297140687704086 Validation Loss: 0.7671658396720886\n",
      "Epoch 12190: Training Loss: 0.09223131587107976 Validation Loss: 0.7677764296531677\n",
      "Epoch 12191: Training Loss: 0.09300626814365387 Validation Loss: 0.7675445079803467\n",
      "Epoch 12192: Training Loss: 0.0923951988418897 Validation Loss: 0.7676106095314026\n",
      "Epoch 12193: Training Loss: 0.09304687877496083 Validation Loss: 0.7675237655639648\n",
      "Epoch 12194: Training Loss: 0.09245417763789494 Validation Loss: 0.7675626277923584\n",
      "Epoch 12195: Training Loss: 0.09248432517051697 Validation Loss: 0.7673027515411377\n",
      "Epoch 12196: Training Loss: 0.09258996695280075 Validation Loss: 0.7672324776649475\n",
      "Epoch 12197: Training Loss: 0.09290322164694469 Validation Loss: 0.7669687867164612\n",
      "Epoch 12198: Training Loss: 0.09268944710493088 Validation Loss: 0.7674069404602051\n",
      "Epoch 12199: Training Loss: 0.09236307442188263 Validation Loss: 0.7674217224121094\n",
      "Epoch 12200: Training Loss: 0.09241213649511337 Validation Loss: 0.7677301168441772\n",
      "Epoch 12201: Training Loss: 0.09244552254676819 Validation Loss: 0.7676219344139099\n",
      "Epoch 12202: Training Loss: 0.09221027791500092 Validation Loss: 0.7678195238113403\n",
      "Epoch 12203: Training Loss: 0.09224175661802292 Validation Loss: 0.7676789164543152\n",
      "Epoch 12204: Training Loss: 0.0923908253510793 Validation Loss: 0.767980694770813\n",
      "Epoch 12205: Training Loss: 0.09289752940336864 Validation Loss: 0.7679677605628967\n",
      "Epoch 12206: Training Loss: 0.09229192634423573 Validation Loss: 0.7681136727333069\n",
      "Epoch 12207: Training Loss: 0.09238268931706746 Validation Loss: 0.7677626013755798\n",
      "Epoch 12208: Training Loss: 0.09304588039716084 Validation Loss: 0.7681639790534973\n",
      "Epoch 12209: Training Loss: 0.09279965857664745 Validation Loss: 0.76763916015625\n",
      "Epoch 12210: Training Loss: 0.09190526604652405 Validation Loss: 0.7674856781959534\n",
      "Epoch 12211: Training Loss: 0.09233003358046214 Validation Loss: 0.7679874897003174\n",
      "Epoch 12212: Training Loss: 0.09228920688231786 Validation Loss: 0.7684835195541382\n",
      "Epoch 12213: Training Loss: 0.09225569913784663 Validation Loss: 0.7682523131370544\n",
      "Epoch 12214: Training Loss: 0.09234043210744858 Validation Loss: 0.7681390643119812\n",
      "Epoch 12215: Training Loss: 0.09185795982678731 Validation Loss: 0.7680470943450928\n",
      "Epoch 12216: Training Loss: 0.09260311971108119 Validation Loss: 0.7681155204772949\n",
      "Epoch 12217: Training Loss: 0.09232708563407262 Validation Loss: 0.768246054649353\n",
      "Epoch 12218: Training Loss: 0.09258998433748881 Validation Loss: 0.7678503394126892\n",
      "Epoch 12219: Training Loss: 0.0921294093132019 Validation Loss: 0.7676559090614319\n",
      "Epoch 12220: Training Loss: 0.09242422133684158 Validation Loss: 0.7671491503715515\n",
      "Epoch 12221: Training Loss: 0.09234687437613805 Validation Loss: 0.7672434449195862\n",
      "Epoch 12222: Training Loss: 0.09217144797245662 Validation Loss: 0.7675537467002869\n",
      "Epoch 12223: Training Loss: 0.0922927384575208 Validation Loss: 0.7681746482849121\n",
      "Epoch 12224: Training Loss: 0.09284526358048122 Validation Loss: 0.7690107822418213\n",
      "Epoch 12225: Training Loss: 0.09210422883431117 Validation Loss: 0.7684346437454224\n",
      "Epoch 12226: Training Loss: 0.09233889480431874 Validation Loss: 0.7679862976074219\n",
      "Epoch 12227: Training Loss: 0.092163381477197 Validation Loss: 0.7677057385444641\n",
      "Epoch 12228: Training Loss: 0.09275886664787929 Validation Loss: 0.7673012614250183\n",
      "Epoch 12229: Training Loss: 0.09298210342725118 Validation Loss: 0.7677170038223267\n",
      "Epoch 12230: Training Loss: 0.09218761573235194 Validation Loss: 0.7677022218704224\n",
      "Epoch 12231: Training Loss: 0.0918667068084081 Validation Loss: 0.7681421637535095\n",
      "Epoch 12232: Training Loss: 0.09212527424097061 Validation Loss: 0.7680566906929016\n",
      "Epoch 12233: Training Loss: 0.09227164834737778 Validation Loss: 0.7684847116470337\n",
      "Epoch 12234: Training Loss: 0.09217799206574757 Validation Loss: 0.768130898475647\n",
      "Epoch 12235: Training Loss: 0.0923881009221077 Validation Loss: 0.7678655982017517\n",
      "Epoch 12236: Training Loss: 0.09240786482890447 Validation Loss: 0.7676473259925842\n",
      "Epoch 12237: Training Loss: 0.09221933285395305 Validation Loss: 0.7674581408500671\n",
      "Epoch 12238: Training Loss: 0.09269628177086513 Validation Loss: 0.7675360441207886\n",
      "Epoch 12239: Training Loss: 0.09230129420757294 Validation Loss: 0.7677930593490601\n",
      "Epoch 12240: Training Loss: 0.09184500575065613 Validation Loss: 0.7680090069770813\n",
      "Epoch 12241: Training Loss: 0.09202190240224202 Validation Loss: 0.7680920362472534\n",
      "Epoch 12242: Training Loss: 0.09233589470386505 Validation Loss: 0.7678142786026001\n",
      "Epoch 12243: Training Loss: 0.09236627568801244 Validation Loss: 0.7678420543670654\n",
      "Epoch 12244: Training Loss: 0.0920784796277682 Validation Loss: 0.7680767178535461\n",
      "Epoch 12245: Training Loss: 0.09210960318644841 Validation Loss: 0.7681747078895569\n",
      "Epoch 12246: Training Loss: 0.09233251959085464 Validation Loss: 0.7680522799491882\n",
      "Epoch 12247: Training Loss: 0.09252625703811646 Validation Loss: 0.768267810344696\n",
      "Epoch 12248: Training Loss: 0.09233409663041432 Validation Loss: 0.7675139904022217\n",
      "Epoch 12249: Training Loss: 0.09207942833503087 Validation Loss: 0.7675681710243225\n",
      "Epoch 12250: Training Loss: 0.0922117531299591 Validation Loss: 0.7679097652435303\n",
      "Epoch 12251: Training Loss: 0.09203999489545822 Validation Loss: 0.7684167623519897\n",
      "Epoch 12252: Training Loss: 0.09122705956300099 Validation Loss: 0.7687976956367493\n",
      "Epoch 12253: Training Loss: 0.09225738048553467 Validation Loss: 0.768220067024231\n",
      "Epoch 12254: Training Loss: 0.09265620509783427 Validation Loss: 0.7675853967666626\n",
      "Epoch 12255: Training Loss: 0.0921875461935997 Validation Loss: 0.7673132419586182\n",
      "Epoch 12256: Training Loss: 0.09199752410252889 Validation Loss: 0.7674688696861267\n",
      "Epoch 12257: Training Loss: 0.09234556059042613 Validation Loss: 0.7678987979888916\n",
      "Epoch 12258: Training Loss: 0.09237558643023173 Validation Loss: 0.7685768008232117\n",
      "Epoch 12259: Training Loss: 0.09191169341405232 Validation Loss: 0.768470048904419\n",
      "Epoch 12260: Training Loss: 0.09254329899946849 Validation Loss: 0.7686357498168945\n",
      "Epoch 12261: Training Loss: 0.09208046893278758 Validation Loss: 0.7683250308036804\n",
      "Epoch 12262: Training Loss: 0.0923160786430041 Validation Loss: 0.7683987617492676\n",
      "Epoch 12263: Training Loss: 0.09132532527049382 Validation Loss: 0.7682873010635376\n",
      "Epoch 12264: Training Loss: 0.09219329059123993 Validation Loss: 0.7682387232780457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12265: Training Loss: 0.09201503296693166 Validation Loss: 0.7682821154594421\n",
      "Epoch 12266: Training Loss: 0.09157093117634456 Validation Loss: 0.7683731913566589\n",
      "Epoch 12267: Training Loss: 0.09217085192600886 Validation Loss: 0.7683192491531372\n",
      "Epoch 12268: Training Loss: 0.09206429372231166 Validation Loss: 0.7689655423164368\n",
      "Epoch 12269: Training Loss: 0.0922158161799113 Validation Loss: 0.7684798836708069\n",
      "Epoch 12270: Training Loss: 0.09210521976153056 Validation Loss: 0.7682469487190247\n",
      "Epoch 12271: Training Loss: 0.09210764865080516 Validation Loss: 0.7683432102203369\n",
      "Epoch 12272: Training Loss: 0.09210508813460667 Validation Loss: 0.7679686546325684\n",
      "Epoch 12273: Training Loss: 0.0920858159661293 Validation Loss: 0.768096923828125\n",
      "Epoch 12274: Training Loss: 0.09268443038066228 Validation Loss: 0.7680321335792542\n",
      "Epoch 12275: Training Loss: 0.09196179608503978 Validation Loss: 0.7680137753486633\n",
      "Epoch 12276: Training Loss: 0.09216004610061646 Validation Loss: 0.7680335640907288\n",
      "Epoch 12277: Training Loss: 0.09217765182256699 Validation Loss: 0.7674188613891602\n",
      "Epoch 12278: Training Loss: 0.09199339151382446 Validation Loss: 0.7677754759788513\n",
      "Epoch 12279: Training Loss: 0.09210614611705144 Validation Loss: 0.7678440809249878\n",
      "Epoch 12280: Training Loss: 0.09317670265833537 Validation Loss: 0.7679423689842224\n",
      "Epoch 12281: Training Loss: 0.0919029066960017 Validation Loss: 0.7682132720947266\n",
      "Epoch 12282: Training Loss: 0.09225578606128693 Validation Loss: 0.7684087157249451\n",
      "Epoch 12283: Training Loss: 0.09223036219676335 Validation Loss: 0.7683483362197876\n",
      "Epoch 12284: Training Loss: 0.0921573539574941 Validation Loss: 0.7687121033668518\n",
      "Epoch 12285: Training Loss: 0.09243168185154597 Validation Loss: 0.7686622142791748\n",
      "Epoch 12286: Training Loss: 0.09236280620098114 Validation Loss: 0.7684235572814941\n",
      "Epoch 12287: Training Loss: 0.09238541622956593 Validation Loss: 0.7682536840438843\n",
      "Epoch 12288: Training Loss: 0.09194691975911458 Validation Loss: 0.7682980895042419\n",
      "Epoch 12289: Training Loss: 0.09225882838169734 Validation Loss: 0.7685219049453735\n",
      "Epoch 12290: Training Loss: 0.09187556058168411 Validation Loss: 0.768551766872406\n",
      "Epoch 12291: Training Loss: 0.09201570351918538 Validation Loss: 0.7684628963470459\n",
      "Epoch 12292: Training Loss: 0.09219515571991603 Validation Loss: 0.7685500383377075\n",
      "Epoch 12293: Training Loss: 0.09249437352021535 Validation Loss: 0.7682846784591675\n",
      "Epoch 12294: Training Loss: 0.09192028641700745 Validation Loss: 0.7682680487632751\n",
      "Epoch 12295: Training Loss: 0.0918171579639117 Validation Loss: 0.768448531627655\n",
      "Epoch 12296: Training Loss: 0.09187621126572292 Validation Loss: 0.768659234046936\n",
      "Epoch 12297: Training Loss: 0.09203546494245529 Validation Loss: 0.7687408924102783\n",
      "Epoch 12298: Training Loss: 0.09225176026423772 Validation Loss: 0.7688944935798645\n",
      "Epoch 12299: Training Loss: 0.09208766867717107 Validation Loss: 0.7688976526260376\n",
      "Epoch 12300: Training Loss: 0.09201268603404363 Validation Loss: 0.7693054676055908\n",
      "Epoch 12301: Training Loss: 0.09221057593822479 Validation Loss: 0.7690417766571045\n",
      "Epoch 12302: Training Loss: 0.09217244386672974 Validation Loss: 0.7681347727775574\n",
      "Epoch 12303: Training Loss: 0.09189063807328542 Validation Loss: 0.767885148525238\n",
      "Epoch 12304: Training Loss: 0.09204612175623576 Validation Loss: 0.7680519223213196\n",
      "Epoch 12305: Training Loss: 0.09195176512002945 Validation Loss: 0.7680792808532715\n",
      "Epoch 12306: Training Loss: 0.0916603331764539 Validation Loss: 0.7689896821975708\n",
      "Epoch 12307: Training Loss: 0.0921644593278567 Validation Loss: 0.7691047787666321\n",
      "Epoch 12308: Training Loss: 0.09205768009026845 Validation Loss: 0.7684726715087891\n",
      "Epoch 12309: Training Loss: 0.09185214589039485 Validation Loss: 0.7684844732284546\n",
      "Epoch 12310: Training Loss: 0.09243348985910416 Validation Loss: 0.7683659791946411\n",
      "Epoch 12311: Training Loss: 0.09162502735853195 Validation Loss: 0.7685686945915222\n",
      "Epoch 12312: Training Loss: 0.09248066196839015 Validation Loss: 0.7681695818901062\n",
      "Epoch 12313: Training Loss: 0.09187146524588267 Validation Loss: 0.7683826684951782\n",
      "Epoch 12314: Training Loss: 0.09188245733579 Validation Loss: 0.7684237957000732\n",
      "Epoch 12315: Training Loss: 0.09181370089451472 Validation Loss: 0.7684631943702698\n",
      "Epoch 12316: Training Loss: 0.09197575847307841 Validation Loss: 0.7684439420700073\n",
      "Epoch 12317: Training Loss: 0.09221929808457692 Validation Loss: 0.7683680057525635\n",
      "Epoch 12318: Training Loss: 0.09186715135971706 Validation Loss: 0.7686765193939209\n",
      "Epoch 12319: Training Loss: 0.09173932919899623 Validation Loss: 0.7688723206520081\n",
      "Epoch 12320: Training Loss: 0.0919966995716095 Validation Loss: 0.769278347492218\n",
      "Epoch 12321: Training Loss: 0.0918457458416621 Validation Loss: 0.7691526412963867\n",
      "Epoch 12322: Training Loss: 0.09216591467459996 Validation Loss: 0.7687646150588989\n",
      "Epoch 12323: Training Loss: 0.09228897591431935 Validation Loss: 0.7686561942100525\n",
      "Epoch 12324: Training Loss: 0.09257126599550247 Validation Loss: 0.7687520384788513\n",
      "Epoch 12325: Training Loss: 0.09210080653429031 Validation Loss: 0.7689160704612732\n",
      "Epoch 12326: Training Loss: 0.09206418444712956 Validation Loss: 0.7691255211830139\n",
      "Epoch 12327: Training Loss: 0.09223067263762157 Validation Loss: 0.7686923146247864\n",
      "Epoch 12328: Training Loss: 0.09167191882928212 Validation Loss: 0.7689567804336548\n",
      "Epoch 12329: Training Loss: 0.0923283522327741 Validation Loss: 0.7685959339141846\n",
      "Epoch 12330: Training Loss: 0.09215639034907024 Validation Loss: 0.7685627341270447\n",
      "Epoch 12331: Training Loss: 0.09185682485500972 Validation Loss: 0.769110381603241\n",
      "Epoch 12332: Training Loss: 0.09199893474578857 Validation Loss: 0.7691174745559692\n",
      "Epoch 12333: Training Loss: 0.09175239751736324 Validation Loss: 0.7687240839004517\n",
      "Epoch 12334: Training Loss: 0.09198333819707234 Validation Loss: 0.7684264779090881\n",
      "Epoch 12335: Training Loss: 0.09192641327778499 Validation Loss: 0.7687709927558899\n",
      "Epoch 12336: Training Loss: 0.09165540337562561 Validation Loss: 0.7689467072486877\n",
      "Epoch 12337: Training Loss: 0.09180557727813721 Validation Loss: 0.769584596157074\n",
      "Epoch 12338: Training Loss: 0.09245502203702927 Validation Loss: 0.7693631052970886\n",
      "Epoch 12339: Training Loss: 0.09161095569531123 Validation Loss: 0.7687199711799622\n",
      "Epoch 12340: Training Loss: 0.0924158642689387 Validation Loss: 0.7688570022583008\n",
      "Epoch 12341: Training Loss: 0.09181225299835205 Validation Loss: 0.7691738605499268\n",
      "Epoch 12342: Training Loss: 0.09210966279109319 Validation Loss: 0.7695510387420654\n",
      "Epoch 12343: Training Loss: 0.09235241015752156 Validation Loss: 0.7697070837020874\n",
      "Epoch 12344: Training Loss: 0.0919052039583524 Validation Loss: 0.7690811157226562\n",
      "Epoch 12345: Training Loss: 0.09164962420860927 Validation Loss: 0.7688962817192078\n",
      "Epoch 12346: Training Loss: 0.09176445752382278 Validation Loss: 0.7689279317855835\n",
      "Epoch 12347: Training Loss: 0.09197370707988739 Validation Loss: 0.7687979340553284\n",
      "Epoch 12348: Training Loss: 0.09172352155049641 Validation Loss: 0.7693615555763245\n",
      "Epoch 12349: Training Loss: 0.09197829415400822 Validation Loss: 0.7695092558860779\n",
      "Epoch 12350: Training Loss: 0.09183855106433232 Validation Loss: 0.7698440551757812\n",
      "Epoch 12351: Training Loss: 0.09154001623392105 Validation Loss: 0.7693725228309631\n",
      "Epoch 12352: Training Loss: 0.09210742513338725 Validation Loss: 0.7696171402931213\n",
      "Epoch 12353: Training Loss: 0.09171127527952194 Validation Loss: 0.7692753672599792\n",
      "Epoch 12354: Training Loss: 0.09202070037523906 Validation Loss: 0.7688865661621094\n",
      "Epoch 12355: Training Loss: 0.09216056515773137 Validation Loss: 0.7682096362113953\n",
      "Epoch 12356: Training Loss: 0.09194738666216533 Validation Loss: 0.7684116363525391\n",
      "Epoch 12357: Training Loss: 0.09112666298945744 Validation Loss: 0.7683537602424622\n",
      "Epoch 12358: Training Loss: 0.09151823818683624 Validation Loss: 0.7685567140579224\n",
      "Epoch 12359: Training Loss: 0.09152602156003316 Validation Loss: 0.7686430811882019\n",
      "Epoch 12360: Training Loss: 0.09215335051218669 Validation Loss: 0.768883228302002\n",
      "Epoch 12361: Training Loss: 0.09165770312150319 Validation Loss: 0.7687160968780518\n",
      "Epoch 12362: Training Loss: 0.09178287535905838 Validation Loss: 0.7687802910804749\n",
      "Epoch 12363: Training Loss: 0.09176600724458694 Validation Loss: 0.7691859006881714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12364: Training Loss: 0.09183949728806813 Validation Loss: 0.7692230343818665\n",
      "Epoch 12365: Training Loss: 0.09199430048465729 Validation Loss: 0.7694452404975891\n",
      "Epoch 12366: Training Loss: 0.09173533817132314 Validation Loss: 0.7692803144454956\n",
      "Epoch 12367: Training Loss: 0.09201307594776154 Validation Loss: 0.7688817977905273\n",
      "Epoch 12368: Training Loss: 0.09188829610745113 Validation Loss: 0.7688791155815125\n",
      "Epoch 12369: Training Loss: 0.09213537971178691 Validation Loss: 0.7695134878158569\n",
      "Epoch 12370: Training Loss: 0.09157759447892506 Validation Loss: 0.7693961262702942\n",
      "Epoch 12371: Training Loss: 0.09161911904811859 Validation Loss: 0.7690588235855103\n",
      "Epoch 12372: Training Loss: 0.09167469541231792 Validation Loss: 0.7690490484237671\n",
      "Epoch 12373: Training Loss: 0.09162432948748271 Validation Loss: 0.7690076231956482\n",
      "Epoch 12374: Training Loss: 0.09189613660176595 Validation Loss: 0.7687362432479858\n",
      "Epoch 12375: Training Loss: 0.09149309744437535 Validation Loss: 0.7692191004753113\n",
      "Epoch 12376: Training Loss: 0.09165358295043309 Validation Loss: 0.7692316770553589\n",
      "Epoch 12377: Training Loss: 0.0915721853574117 Validation Loss: 0.7691195011138916\n",
      "Epoch 12378: Training Loss: 0.09113314002752304 Validation Loss: 0.7689235806465149\n",
      "Epoch 12379: Training Loss: 0.09140358865261078 Validation Loss: 0.7687920331954956\n",
      "Epoch 12380: Training Loss: 0.0918432002266248 Validation Loss: 0.7690727710723877\n",
      "Epoch 12381: Training Loss: 0.0914279172817866 Validation Loss: 0.7694535851478577\n",
      "Epoch 12382: Training Loss: 0.09165684133768082 Validation Loss: 0.7698177099227905\n",
      "Epoch 12383: Training Loss: 0.09157147755225499 Validation Loss: 0.7694173455238342\n",
      "Epoch 12384: Training Loss: 0.09145734459161758 Validation Loss: 0.7696227431297302\n",
      "Epoch 12385: Training Loss: 0.09163123369216919 Validation Loss: 0.7688792943954468\n",
      "Epoch 12386: Training Loss: 0.09171490371227264 Validation Loss: 0.7684675455093384\n",
      "Epoch 12387: Training Loss: 0.09199540068705876 Validation Loss: 0.7687597274780273\n",
      "Epoch 12388: Training Loss: 0.09166313956181209 Validation Loss: 0.7688665986061096\n",
      "Epoch 12389: Training Loss: 0.09174435337384541 Validation Loss: 0.7695000171661377\n",
      "Epoch 12390: Training Loss: 0.09134113291899364 Validation Loss: 0.7694525122642517\n",
      "Epoch 12391: Training Loss: 0.0920843134323756 Validation Loss: 0.769137978553772\n",
      "Epoch 12392: Training Loss: 0.09166386475165685 Validation Loss: 0.7692792415618896\n",
      "Epoch 12393: Training Loss: 0.0915164848168691 Validation Loss: 0.7694982886314392\n",
      "Epoch 12394: Training Loss: 0.09153476605812709 Validation Loss: 0.7689215540885925\n",
      "Epoch 12395: Training Loss: 0.0917384351293246 Validation Loss: 0.7685725688934326\n",
      "Epoch 12396: Training Loss: 0.09155854086081187 Validation Loss: 0.7686627507209778\n",
      "Epoch 12397: Training Loss: 0.09135657797257106 Validation Loss: 0.7689854502677917\n",
      "Epoch 12398: Training Loss: 0.09141041090091069 Validation Loss: 0.7694105505943298\n",
      "Epoch 12399: Training Loss: 0.09166086713473003 Validation Loss: 0.769808292388916\n",
      "Epoch 12400: Training Loss: 0.09132887423038483 Validation Loss: 0.769599199295044\n",
      "Epoch 12401: Training Loss: 0.09134592364231746 Validation Loss: 0.7699988484382629\n",
      "Epoch 12402: Training Loss: 0.09142401069402695 Validation Loss: 0.7704148292541504\n",
      "Epoch 12403: Training Loss: 0.09172958632310231 Validation Loss: 0.7697067260742188\n",
      "Epoch 12404: Training Loss: 0.09237329910198848 Validation Loss: 0.7690188884735107\n",
      "Epoch 12405: Training Loss: 0.09166042258342107 Validation Loss: 0.7691345810890198\n",
      "Epoch 12406: Training Loss: 0.09152743717034657 Validation Loss: 0.7690542340278625\n",
      "Epoch 12407: Training Loss: 0.09098598112662633 Validation Loss: 0.7696369886398315\n",
      "Epoch 12408: Training Loss: 0.09205715109904607 Validation Loss: 0.7692223191261292\n",
      "Epoch 12409: Training Loss: 0.09141334146261215 Validation Loss: 0.7692052125930786\n",
      "Epoch 12410: Training Loss: 0.09139006336530049 Validation Loss: 0.7691565752029419\n",
      "Epoch 12411: Training Loss: 0.09164916475613911 Validation Loss: 0.7692877054214478\n",
      "Epoch 12412: Training Loss: 0.09124586482842763 Validation Loss: 0.7691517472267151\n",
      "Epoch 12413: Training Loss: 0.09145696709553401 Validation Loss: 0.769391655921936\n",
      "Epoch 12414: Training Loss: 0.09254911790291469 Validation Loss: 0.7692059874534607\n",
      "Epoch 12415: Training Loss: 0.09144642700751622 Validation Loss: 0.7693831324577332\n",
      "Epoch 12416: Training Loss: 0.09189946452776591 Validation Loss: 0.7696542143821716\n",
      "Epoch 12417: Training Loss: 0.09141857673724492 Validation Loss: 0.7694926857948303\n",
      "Epoch 12418: Training Loss: 0.09242342164119084 Validation Loss: 0.7698994874954224\n",
      "Epoch 12419: Training Loss: 0.09117924173672994 Validation Loss: 0.7699193954467773\n",
      "Epoch 12420: Training Loss: 0.09172313660383224 Validation Loss: 0.7700290679931641\n",
      "Epoch 12421: Training Loss: 0.09215312699476878 Validation Loss: 0.7699306607246399\n",
      "Epoch 12422: Training Loss: 0.09190935889879863 Validation Loss: 0.7699788808822632\n",
      "Epoch 12423: Training Loss: 0.09117741634448369 Validation Loss: 0.7698916792869568\n",
      "Epoch 12424: Training Loss: 0.09215211619933446 Validation Loss: 0.7701243162155151\n",
      "Epoch 12425: Training Loss: 0.09167548020680745 Validation Loss: 0.7701245546340942\n",
      "Epoch 12426: Training Loss: 0.09118011345465978 Validation Loss: 0.7702301144599915\n",
      "Epoch 12427: Training Loss: 0.09157762428124745 Validation Loss: 0.7703830003738403\n",
      "Epoch 12428: Training Loss: 0.0916142463684082 Validation Loss: 0.7694947123527527\n",
      "Epoch 12429: Training Loss: 0.09189133097728093 Validation Loss: 0.769600510597229\n",
      "Epoch 12430: Training Loss: 0.09144672999779384 Validation Loss: 0.7695468068122864\n",
      "Epoch 12431: Training Loss: 0.09134199966986974 Validation Loss: 0.7694507837295532\n",
      "Epoch 12432: Training Loss: 0.0915792907277743 Validation Loss: 0.7699904441833496\n",
      "Epoch 12433: Training Loss: 0.09104862064123154 Validation Loss: 0.7700315117835999\n",
      "Epoch 12434: Training Loss: 0.09164009739955266 Validation Loss: 0.7700879573822021\n",
      "Epoch 12435: Training Loss: 0.09169120341539383 Validation Loss: 0.7701877951622009\n",
      "Epoch 12436: Training Loss: 0.09213079263766606 Validation Loss: 0.7698285579681396\n",
      "Epoch 12437: Training Loss: 0.09143972148497899 Validation Loss: 0.7697145342826843\n",
      "Epoch 12438: Training Loss: 0.09124784668286641 Validation Loss: 0.7698636054992676\n",
      "Epoch 12439: Training Loss: 0.09132270514965057 Validation Loss: 0.7698540091514587\n",
      "Epoch 12440: Training Loss: 0.09186808268229167 Validation Loss: 0.7700066566467285\n",
      "Epoch 12441: Training Loss: 0.09130047261714935 Validation Loss: 0.7700191736221313\n",
      "Epoch 12442: Training Loss: 0.09129406015078227 Validation Loss: 0.7701818346977234\n",
      "Epoch 12443: Training Loss: 0.09187310685714085 Validation Loss: 0.7699089050292969\n",
      "Epoch 12444: Training Loss: 0.09130830814441045 Validation Loss: 0.7693440318107605\n",
      "Epoch 12445: Training Loss: 0.09113984058300655 Validation Loss: 0.7695848345756531\n",
      "Epoch 12446: Training Loss: 0.09187442312637965 Validation Loss: 0.7690126895904541\n",
      "Epoch 12447: Training Loss: 0.09144637236992519 Validation Loss: 0.7698686122894287\n",
      "Epoch 12448: Training Loss: 0.09127642214298248 Validation Loss: 0.7701746821403503\n",
      "Epoch 12449: Training Loss: 0.09112115701039632 Validation Loss: 0.7705080509185791\n",
      "Epoch 12450: Training Loss: 0.0913172538081805 Validation Loss: 0.7702175378799438\n",
      "Epoch 12451: Training Loss: 0.09204862763484319 Validation Loss: 0.7700763940811157\n",
      "Epoch 12452: Training Loss: 0.09151224046945572 Validation Loss: 0.7702330946922302\n",
      "Epoch 12453: Training Loss: 0.0910961925983429 Validation Loss: 0.7701494693756104\n",
      "Epoch 12454: Training Loss: 0.09149585415919621 Validation Loss: 0.7699373364448547\n",
      "Epoch 12455: Training Loss: 0.09119879702727 Validation Loss: 0.7699058651924133\n",
      "Epoch 12456: Training Loss: 0.09142558773358662 Validation Loss: 0.7701579332351685\n",
      "Epoch 12457: Training Loss: 0.09254337350527446 Validation Loss: 0.769938588142395\n",
      "Epoch 12458: Training Loss: 0.09123981992403667 Validation Loss: 0.7702542543411255\n",
      "Epoch 12459: Training Loss: 0.09127010156710942 Validation Loss: 0.7700862288475037\n",
      "Epoch 12460: Training Loss: 0.09167290230592091 Validation Loss: 0.7696207761764526\n",
      "Epoch 12461: Training Loss: 0.09185660630464554 Validation Loss: 0.7693706154823303\n",
      "Epoch 12462: Training Loss: 0.09098976105451584 Validation Loss: 0.7696261405944824\n",
      "Epoch 12463: Training Loss: 0.09158355245987575 Validation Loss: 0.7700773477554321\n",
      "Epoch 12464: Training Loss: 0.09139617780844371 Validation Loss: 0.7704271674156189\n",
      "Epoch 12465: Training Loss: 0.09130668391784032 Validation Loss: 0.7710204124450684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12466: Training Loss: 0.09148299942413966 Validation Loss: 0.7708003520965576\n",
      "Epoch 12467: Training Loss: 0.091897152364254 Validation Loss: 0.7699396014213562\n",
      "Epoch 12468: Training Loss: 0.09208890050649643 Validation Loss: 0.7695332169532776\n",
      "Epoch 12469: Training Loss: 0.09135053555170695 Validation Loss: 0.769708514213562\n",
      "Epoch 12470: Training Loss: 0.09132257848978043 Validation Loss: 0.7697713971138\n",
      "Epoch 12471: Training Loss: 0.09167498101790746 Validation Loss: 0.7704527974128723\n",
      "Epoch 12472: Training Loss: 0.09137839327255885 Validation Loss: 0.7705914974212646\n",
      "Epoch 12473: Training Loss: 0.09221142282088597 Validation Loss: 0.770703911781311\n",
      "Epoch 12474: Training Loss: 0.09189736346403758 Validation Loss: 0.7705234289169312\n",
      "Epoch 12475: Training Loss: 0.09118699779113133 Validation Loss: 0.7703143358230591\n",
      "Epoch 12476: Training Loss: 0.09109911322593689 Validation Loss: 0.7703566551208496\n",
      "Epoch 12477: Training Loss: 0.09135333200295766 Validation Loss: 0.7706813812255859\n",
      "Epoch 12478: Training Loss: 0.09251685192187627 Validation Loss: 0.7709971070289612\n",
      "Epoch 12479: Training Loss: 0.0909888967871666 Validation Loss: 0.7704547047615051\n",
      "Epoch 12480: Training Loss: 0.09196189045906067 Validation Loss: 0.7698853611946106\n",
      "Epoch 12481: Training Loss: 0.0911322608590126 Validation Loss: 0.7698036432266235\n",
      "Epoch 12482: Training Loss: 0.09123639265696208 Validation Loss: 0.7699400186538696\n",
      "Epoch 12483: Training Loss: 0.09142956634362538 Validation Loss: 0.7703061103820801\n",
      "Epoch 12484: Training Loss: 0.0919715091586113 Validation Loss: 0.7705522775650024\n",
      "Epoch 12485: Training Loss: 0.09116793672243755 Validation Loss: 0.7704408764839172\n",
      "Epoch 12486: Training Loss: 0.0907471776008606 Validation Loss: 0.7702044248580933\n",
      "Epoch 12487: Training Loss: 0.09137128293514252 Validation Loss: 0.7703852653503418\n",
      "Epoch 12488: Training Loss: 0.091138889392217 Validation Loss: 0.7694666385650635\n",
      "Epoch 12489: Training Loss: 0.09126033137242 Validation Loss: 0.7690220475196838\n",
      "Epoch 12490: Training Loss: 0.09078305214643478 Validation Loss: 0.7695589065551758\n",
      "Epoch 12491: Training Loss: 0.09153220802545547 Validation Loss: 0.7698284387588501\n",
      "Epoch 12492: Training Loss: 0.09134256094694138 Validation Loss: 0.7703679800033569\n",
      "Epoch 12493: Training Loss: 0.09191744774580002 Validation Loss: 0.770598292350769\n",
      "Epoch 12494: Training Loss: 0.09115047256151836 Validation Loss: 0.7708905935287476\n",
      "Epoch 12495: Training Loss: 0.09153310706218083 Validation Loss: 0.7700124382972717\n",
      "Epoch 12496: Training Loss: 0.09115208437045415 Validation Loss: 0.7699788808822632\n",
      "Epoch 12497: Training Loss: 0.09118775775035222 Validation Loss: 0.7697911262512207\n",
      "Epoch 12498: Training Loss: 0.09217725445826848 Validation Loss: 0.7703335285186768\n",
      "Epoch 12499: Training Loss: 0.09158490101496379 Validation Loss: 0.7709600925445557\n",
      "Epoch 12500: Training Loss: 0.09119731684525807 Validation Loss: 0.7709140181541443\n",
      "Epoch 12501: Training Loss: 0.0908418595790863 Validation Loss: 0.7708092927932739\n",
      "Epoch 12502: Training Loss: 0.09173459559679031 Validation Loss: 0.7705860733985901\n",
      "Epoch 12503: Training Loss: 0.09137207518021266 Validation Loss: 0.770464301109314\n",
      "Epoch 12504: Training Loss: 0.09097331762313843 Validation Loss: 0.7703688740730286\n",
      "Epoch 12505: Training Loss: 0.09116329252719879 Validation Loss: 0.770503580570221\n",
      "Epoch 12506: Training Loss: 0.09118205557266872 Validation Loss: 0.7705039978027344\n",
      "Epoch 12507: Training Loss: 0.0915689468383789 Validation Loss: 0.7702291011810303\n",
      "Epoch 12508: Training Loss: 0.0910062442223231 Validation Loss: 0.7706619501113892\n",
      "Epoch 12509: Training Loss: 0.09137874344984691 Validation Loss: 0.7709386944770813\n",
      "Epoch 12510: Training Loss: 0.0915960023800532 Validation Loss: 0.7712046504020691\n",
      "Epoch 12511: Training Loss: 0.09126922984917958 Validation Loss: 0.7701843976974487\n",
      "Epoch 12512: Training Loss: 0.09126785645882289 Validation Loss: 0.7697190642356873\n",
      "Epoch 12513: Training Loss: 0.09119672079881032 Validation Loss: 0.7698588371276855\n",
      "Epoch 12514: Training Loss: 0.0915820300579071 Validation Loss: 0.7705286145210266\n",
      "Epoch 12515: Training Loss: 0.09107393523057301 Validation Loss: 0.7704988121986389\n",
      "Epoch 12516: Training Loss: 0.09112864981094997 Validation Loss: 0.7705466747283936\n",
      "Epoch 12517: Training Loss: 0.09115678817033768 Validation Loss: 0.7703756093978882\n",
      "Epoch 12518: Training Loss: 0.0914137785633405 Validation Loss: 0.7707245349884033\n",
      "Epoch 12519: Training Loss: 0.09111602107683818 Validation Loss: 0.7704847455024719\n",
      "Epoch 12520: Training Loss: 0.09117258340120316 Validation Loss: 0.7706742882728577\n",
      "Epoch 12521: Training Loss: 0.09087882936000824 Validation Loss: 0.7705592513084412\n",
      "Epoch 12522: Training Loss: 0.09102958937486012 Validation Loss: 0.7703643441200256\n",
      "Epoch 12523: Training Loss: 0.09092334906260173 Validation Loss: 0.7706207633018494\n",
      "Epoch 12524: Training Loss: 0.09139711161454518 Validation Loss: 0.7705379128456116\n",
      "Epoch 12525: Training Loss: 0.091354434688886 Validation Loss: 0.7710115313529968\n",
      "Epoch 12526: Training Loss: 0.09114698817332585 Validation Loss: 0.770910918712616\n",
      "Epoch 12527: Training Loss: 0.09134363383054733 Validation Loss: 0.7704749703407288\n",
      "Epoch 12528: Training Loss: 0.09122064461310704 Validation Loss: 0.7702093720436096\n",
      "Epoch 12529: Training Loss: 0.09085125972827275 Validation Loss: 0.7707458734512329\n",
      "Epoch 12530: Training Loss: 0.09072869767745335 Validation Loss: 0.7706441283226013\n",
      "Epoch 12531: Training Loss: 0.09088676919539769 Validation Loss: 0.7703685164451599\n",
      "Epoch 12532: Training Loss: 0.09121524294217427 Validation Loss: 0.7706018686294556\n",
      "Epoch 12533: Training Loss: 0.09103788435459137 Validation Loss: 0.7706021666526794\n",
      "Epoch 12534: Training Loss: 0.0909316713611285 Validation Loss: 0.7709580063819885\n",
      "Epoch 12535: Training Loss: 0.09121486296256383 Validation Loss: 0.770645260810852\n",
      "Epoch 12536: Training Loss: 0.09094795833031337 Validation Loss: 0.7704609036445618\n",
      "Epoch 12537: Training Loss: 0.09094493091106415 Validation Loss: 0.770670473575592\n",
      "Epoch 12538: Training Loss: 0.09095426152149837 Validation Loss: 0.7706241011619568\n",
      "Epoch 12539: Training Loss: 0.09102707356214523 Validation Loss: 0.7707692384719849\n",
      "Epoch 12540: Training Loss: 0.0910970742503802 Validation Loss: 0.7708398103713989\n",
      "Epoch 12541: Training Loss: 0.09151784827311833 Validation Loss: 0.7704897522926331\n",
      "Epoch 12542: Training Loss: 0.09107630948225658 Validation Loss: 0.770450234413147\n",
      "Epoch 12543: Training Loss: 0.09087937076886494 Validation Loss: 0.770733654499054\n",
      "Epoch 12544: Training Loss: 0.09125229467948277 Validation Loss: 0.770797610282898\n",
      "Epoch 12545: Training Loss: 0.09137451400359471 Validation Loss: 0.7713009119033813\n",
      "Epoch 12546: Training Loss: 0.09079618503650029 Validation Loss: 0.7716164588928223\n",
      "Epoch 12547: Training Loss: 0.09213437139987946 Validation Loss: 0.7715858817100525\n",
      "Epoch 12548: Training Loss: 0.09092815220355988 Validation Loss: 0.7706339359283447\n",
      "Epoch 12549: Training Loss: 0.09074730674425761 Validation Loss: 0.7700752019882202\n",
      "Epoch 12550: Training Loss: 0.09049152831236522 Validation Loss: 0.7697219848632812\n",
      "Epoch 12551: Training Loss: 0.09080412487188975 Validation Loss: 0.7698473930358887\n",
      "Epoch 12552: Training Loss: 0.09065853555997212 Validation Loss: 0.7701711058616638\n",
      "Epoch 12553: Training Loss: 0.09106169889370601 Validation Loss: 0.7704169750213623\n",
      "Epoch 12554: Training Loss: 0.09135893980662028 Validation Loss: 0.7707507610321045\n",
      "Epoch 12555: Training Loss: 0.0914765199025472 Validation Loss: 0.7710482478141785\n",
      "Epoch 12556: Training Loss: 0.0912844588359197 Validation Loss: 0.7711580395698547\n",
      "Epoch 12557: Training Loss: 0.09094989796479543 Validation Loss: 0.7708963751792908\n",
      "Epoch 12558: Training Loss: 0.0909664159019788 Validation Loss: 0.7708114385604858\n",
      "Epoch 12559: Training Loss: 0.09104703366756439 Validation Loss: 0.7706801891326904\n",
      "Epoch 12560: Training Loss: 0.09095144023497899 Validation Loss: 0.7701326012611389\n",
      "Epoch 12561: Training Loss: 0.09135222931702931 Validation Loss: 0.770332932472229\n",
      "Epoch 12562: Training Loss: 0.09108702590068181 Validation Loss: 0.770869255065918\n",
      "Epoch 12563: Training Loss: 0.0920445757607619 Validation Loss: 0.7714410424232483\n",
      "Epoch 12564: Training Loss: 0.09082126865784328 Validation Loss: 0.7713494300842285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12565: Training Loss: 0.0905240923166275 Validation Loss: 0.7711499929428101\n",
      "Epoch 12566: Training Loss: 0.09091549615065257 Validation Loss: 0.770824670791626\n",
      "Epoch 12567: Training Loss: 0.09083817899227142 Validation Loss: 0.7710272073745728\n",
      "Epoch 12568: Training Loss: 0.09077902883291245 Validation Loss: 0.7710280418395996\n",
      "Epoch 12569: Training Loss: 0.09069276601076126 Validation Loss: 0.7710258960723877\n",
      "Epoch 12570: Training Loss: 0.09089870502551396 Validation Loss: 0.7711194753646851\n",
      "Epoch 12571: Training Loss: 0.09161335478226344 Validation Loss: 0.7706544399261475\n",
      "Epoch 12572: Training Loss: 0.09091849376757939 Validation Loss: 0.771255612373352\n",
      "Epoch 12573: Training Loss: 0.09082154929637909 Validation Loss: 0.7711105346679688\n",
      "Epoch 12574: Training Loss: 0.09076956907908122 Validation Loss: 0.7713605761528015\n",
      "Epoch 12575: Training Loss: 0.0911448523402214 Validation Loss: 0.7720970511436462\n",
      "Epoch 12576: Training Loss: 0.09099709242582321 Validation Loss: 0.7720433473587036\n",
      "Epoch 12577: Training Loss: 0.09099407245715459 Validation Loss: 0.7716852426528931\n",
      "Epoch 12578: Training Loss: 0.09104336053133011 Validation Loss: 0.7709088325500488\n",
      "Epoch 12579: Training Loss: 0.09085823098818462 Validation Loss: 0.7706423997879028\n",
      "Epoch 12580: Training Loss: 0.09171655277411143 Validation Loss: 0.7708720564842224\n",
      "Epoch 12581: Training Loss: 0.09061634292205174 Validation Loss: 0.7714048624038696\n",
      "Epoch 12582: Training Loss: 0.09046633789936702 Validation Loss: 0.7714685797691345\n",
      "Epoch 12583: Training Loss: 0.09093177318572998 Validation Loss: 0.7719322443008423\n",
      "Epoch 12584: Training Loss: 0.09123545140028 Validation Loss: 0.7715187668800354\n",
      "Epoch 12585: Training Loss: 0.0907626822590828 Validation Loss: 0.7712731957435608\n",
      "Epoch 12586: Training Loss: 0.09034217149019241 Validation Loss: 0.7711871266365051\n",
      "Epoch 12587: Training Loss: 0.09089192499717076 Validation Loss: 0.7716684341430664\n",
      "Epoch 12588: Training Loss: 0.09076496710379918 Validation Loss: 0.7719120979309082\n",
      "Epoch 12589: Training Loss: 0.09050415207942326 Validation Loss: 0.7714701294898987\n",
      "Epoch 12590: Training Loss: 0.09096009532610576 Validation Loss: 0.771501898765564\n",
      "Epoch 12591: Training Loss: 0.09085157016913097 Validation Loss: 0.7714343667030334\n",
      "Epoch 12592: Training Loss: 0.09078814834356308 Validation Loss: 0.771947979927063\n",
      "Epoch 12593: Training Loss: 0.0906677395105362 Validation Loss: 0.7713726162910461\n",
      "Epoch 12594: Training Loss: 0.09076907237370808 Validation Loss: 0.7713443040847778\n",
      "Epoch 12595: Training Loss: 0.09131164600451787 Validation Loss: 0.7708933353424072\n",
      "Epoch 12596: Training Loss: 0.09113810459772746 Validation Loss: 0.7706542611122131\n",
      "Epoch 12597: Training Loss: 0.09119999408721924 Validation Loss: 0.7715142965316772\n",
      "Epoch 12598: Training Loss: 0.09090162565310796 Validation Loss: 0.7713920474052429\n",
      "Epoch 12599: Training Loss: 0.09072177608807881 Validation Loss: 0.7715069055557251\n",
      "Epoch 12600: Training Loss: 0.09061981737613678 Validation Loss: 0.771574854850769\n",
      "Epoch 12601: Training Loss: 0.09065994868675868 Validation Loss: 0.770982563495636\n",
      "Epoch 12602: Training Loss: 0.09064535051584244 Validation Loss: 0.7706502676010132\n",
      "Epoch 12603: Training Loss: 0.0913498525818189 Validation Loss: 0.7708116769790649\n",
      "Epoch 12604: Training Loss: 0.09079608817895253 Validation Loss: 0.7716960310935974\n",
      "Epoch 12605: Training Loss: 0.09065884351730347 Validation Loss: 0.7715463042259216\n",
      "Epoch 12606: Training Loss: 0.09102562566598256 Validation Loss: 0.7714033722877502\n",
      "Epoch 12607: Training Loss: 0.09061654408772786 Validation Loss: 0.7712253332138062\n",
      "Epoch 12608: Training Loss: 0.09064699957768123 Validation Loss: 0.7712152600288391\n",
      "Epoch 12609: Training Loss: 0.09088076402743657 Validation Loss: 0.7711406350135803\n",
      "Epoch 12610: Training Loss: 0.09064455330371857 Validation Loss: 0.7715527415275574\n",
      "Epoch 12611: Training Loss: 0.09095637996991475 Validation Loss: 0.7717705965042114\n",
      "Epoch 12612: Training Loss: 0.09072872996330261 Validation Loss: 0.7720432281494141\n",
      "Epoch 12613: Training Loss: 0.09078701833883922 Validation Loss: 0.7716330289840698\n",
      "Epoch 12614: Training Loss: 0.09105749676624934 Validation Loss: 0.7714959979057312\n",
      "Epoch 12615: Training Loss: 0.0907074585556984 Validation Loss: 0.7708306908607483\n",
      "Epoch 12616: Training Loss: 0.09106162190437317 Validation Loss: 0.7707622647285461\n",
      "Epoch 12617: Training Loss: 0.09106113016605377 Validation Loss: 0.7708057165145874\n",
      "Epoch 12618: Training Loss: 0.09094661474227905 Validation Loss: 0.7715092897415161\n",
      "Epoch 12619: Training Loss: 0.09069405496120453 Validation Loss: 0.7722658514976501\n",
      "Epoch 12620: Training Loss: 0.09072910994291306 Validation Loss: 0.7721483707427979\n",
      "Epoch 12621: Training Loss: 0.0908190483848254 Validation Loss: 0.7721642851829529\n",
      "Epoch 12622: Training Loss: 0.09114345908164978 Validation Loss: 0.7715744972229004\n",
      "Epoch 12623: Training Loss: 0.09073172509670258 Validation Loss: 0.7714167237281799\n",
      "Epoch 12624: Training Loss: 0.09159025053183238 Validation Loss: 0.7714548110961914\n",
      "Epoch 12625: Training Loss: 0.09109080086151759 Validation Loss: 0.7717669606208801\n",
      "Epoch 12626: Training Loss: 0.09051944315433502 Validation Loss: 0.7719649076461792\n",
      "Epoch 12627: Training Loss: 0.09045617779095967 Validation Loss: 0.7719199657440186\n",
      "Epoch 12628: Training Loss: 0.09135345617930095 Validation Loss: 0.7718622088432312\n",
      "Epoch 12629: Training Loss: 0.09051656971375148 Validation Loss: 0.771784782409668\n",
      "Epoch 12630: Training Loss: 0.09078629066546758 Validation Loss: 0.7718216180801392\n",
      "Epoch 12631: Training Loss: 0.09176238626241684 Validation Loss: 0.7714430689811707\n",
      "Epoch 12632: Training Loss: 0.0908220907052358 Validation Loss: 0.7711998224258423\n",
      "Epoch 12633: Training Loss: 0.09037488947312038 Validation Loss: 0.771068274974823\n",
      "Epoch 12634: Training Loss: 0.09058033674955368 Validation Loss: 0.7719884514808655\n",
      "Epoch 12635: Training Loss: 0.09081420550743739 Validation Loss: 0.7722606062889099\n",
      "Epoch 12636: Training Loss: 0.09063641726970673 Validation Loss: 0.7722527980804443\n",
      "Epoch 12637: Training Loss: 0.09057498474915822 Validation Loss: 0.771860659122467\n",
      "Epoch 12638: Training Loss: 0.09059636791547139 Validation Loss: 0.7714748382568359\n",
      "Epoch 12639: Training Loss: 0.0905306339263916 Validation Loss: 0.7711221575737\n",
      "Epoch 12640: Training Loss: 0.0907631441950798 Validation Loss: 0.7711299061775208\n",
      "Epoch 12641: Training Loss: 0.0908676063021024 Validation Loss: 0.7713980674743652\n",
      "Epoch 12642: Training Loss: 0.09077851970990498 Validation Loss: 0.7719975113868713\n",
      "Epoch 12643: Training Loss: 0.09086999793847401 Validation Loss: 0.7722536325454712\n",
      "Epoch 12644: Training Loss: 0.09070512155691783 Validation Loss: 0.7722171545028687\n",
      "Epoch 12645: Training Loss: 0.09035950402418773 Validation Loss: 0.7720174193382263\n",
      "Epoch 12646: Training Loss: 0.0907481461763382 Validation Loss: 0.7721643447875977\n",
      "Epoch 12647: Training Loss: 0.09051443388064702 Validation Loss: 0.7721937298774719\n",
      "Epoch 12648: Training Loss: 0.09058740983406703 Validation Loss: 0.7714006304740906\n",
      "Epoch 12649: Training Loss: 0.09060461074113846 Validation Loss: 0.7711781859397888\n",
      "Epoch 12650: Training Loss: 0.09086313595374425 Validation Loss: 0.7713220715522766\n",
      "Epoch 12651: Training Loss: 0.0905278151233991 Validation Loss: 0.7720969915390015\n",
      "Epoch 12652: Training Loss: 0.0905384694536527 Validation Loss: 0.7720842957496643\n",
      "Epoch 12653: Training Loss: 0.09068240722020467 Validation Loss: 0.7717507481575012\n",
      "Epoch 12654: Training Loss: 0.09165520469347636 Validation Loss: 0.7716310620307922\n",
      "Epoch 12655: Training Loss: 0.09069529672463734 Validation Loss: 0.7725228667259216\n",
      "Epoch 12656: Training Loss: 0.09124244004487991 Validation Loss: 0.7725019454956055\n",
      "Epoch 12657: Training Loss: 0.09071409205595653 Validation Loss: 0.772335946559906\n",
      "Epoch 12658: Training Loss: 0.0902905489007632 Validation Loss: 0.7716291546821594\n",
      "Epoch 12659: Training Loss: 0.09066844979921977 Validation Loss: 0.7712550163269043\n",
      "Epoch 12660: Training Loss: 0.09053417046864827 Validation Loss: 0.7713102102279663\n",
      "Epoch 12661: Training Loss: 0.09061003476381302 Validation Loss: 0.7717159986495972\n",
      "Epoch 12662: Training Loss: 0.09033634265263875 Validation Loss: 0.7719830870628357\n",
      "Epoch 12663: Training Loss: 0.09054552515347798 Validation Loss: 0.7719834446907043\n",
      "Epoch 12664: Training Loss: 0.09127960602442424 Validation Loss: 0.7721273899078369\n",
      "Epoch 12665: Training Loss: 0.0903552199403445 Validation Loss: 0.7722657322883606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12666: Training Loss: 0.09075994789600372 Validation Loss: 0.7723228335380554\n",
      "Epoch 12667: Training Loss: 0.09050480276346207 Validation Loss: 0.7722712159156799\n",
      "Epoch 12668: Training Loss: 0.0907577599088351 Validation Loss: 0.7718779444694519\n",
      "Epoch 12669: Training Loss: 0.09067105253537495 Validation Loss: 0.7716482281684875\n",
      "Epoch 12670: Training Loss: 0.09040001531442006 Validation Loss: 0.7716776728630066\n",
      "Epoch 12671: Training Loss: 0.09062107155720393 Validation Loss: 0.7717455625534058\n",
      "Epoch 12672: Training Loss: 0.09066840012868245 Validation Loss: 0.7718672752380371\n",
      "Epoch 12673: Training Loss: 0.09026440481344859 Validation Loss: 0.7720215320587158\n",
      "Epoch 12674: Training Loss: 0.09041472772757213 Validation Loss: 0.7726486921310425\n",
      "Epoch 12675: Training Loss: 0.09106061855951945 Validation Loss: 0.7725830078125\n",
      "Epoch 12676: Training Loss: 0.09136564284563065 Validation Loss: 0.7723187804222107\n",
      "Epoch 12677: Training Loss: 0.09039793411890666 Validation Loss: 0.7719013094902039\n",
      "Epoch 12678: Training Loss: 0.09046996384859085 Validation Loss: 0.7718142867088318\n",
      "Epoch 12679: Training Loss: 0.09055518358945847 Validation Loss: 0.7717270255088806\n",
      "Epoch 12680: Training Loss: 0.09022527933120728 Validation Loss: 0.7715972661972046\n",
      "Epoch 12681: Training Loss: 0.0903327465057373 Validation Loss: 0.7721547484397888\n",
      "Epoch 12682: Training Loss: 0.09042551120122273 Validation Loss: 0.7723268270492554\n",
      "Epoch 12683: Training Loss: 0.09032362947861354 Validation Loss: 0.7722947597503662\n",
      "Epoch 12684: Training Loss: 0.09071201086044312 Validation Loss: 0.7719947099685669\n",
      "Epoch 12685: Training Loss: 0.09082166850566864 Validation Loss: 0.7718139886856079\n",
      "Epoch 12686: Training Loss: 0.09016735355059306 Validation Loss: 0.7718433141708374\n",
      "Epoch 12687: Training Loss: 0.09055632849534352 Validation Loss: 0.771685004234314\n",
      "Epoch 12688: Training Loss: 0.0904288316766421 Validation Loss: 0.7723275423049927\n",
      "Epoch 12689: Training Loss: 0.09071333209673564 Validation Loss: 0.7727071046829224\n",
      "Epoch 12690: Training Loss: 0.09033179531494777 Validation Loss: 0.7730361223220825\n",
      "Epoch 12691: Training Loss: 0.09032822648684184 Validation Loss: 0.7724056243896484\n",
      "Epoch 12692: Training Loss: 0.09056264410416286 Validation Loss: 0.7718690037727356\n",
      "Epoch 12693: Training Loss: 0.09035922586917877 Validation Loss: 0.7722539305686951\n",
      "Epoch 12694: Training Loss: 0.09030591696500778 Validation Loss: 0.7726216316223145\n",
      "Epoch 12695: Training Loss: 0.090348685781161 Validation Loss: 0.7732396721839905\n",
      "Epoch 12696: Training Loss: 0.09092483669519424 Validation Loss: 0.7727375626564026\n",
      "Epoch 12697: Training Loss: 0.09030988812446594 Validation Loss: 0.772957980632782\n",
      "Epoch 12698: Training Loss: 0.09035207082827885 Validation Loss: 0.7724721431732178\n",
      "Epoch 12699: Training Loss: 0.09077905863523483 Validation Loss: 0.7721912860870361\n",
      "Epoch 12700: Training Loss: 0.09063937266667683 Validation Loss: 0.7721736431121826\n",
      "Epoch 12701: Training Loss: 0.09041503568490346 Validation Loss: 0.7719182372093201\n",
      "Epoch 12702: Training Loss: 0.09036237994829814 Validation Loss: 0.7725826501846313\n",
      "Epoch 12703: Training Loss: 0.09028304119904836 Validation Loss: 0.7730443477630615\n",
      "Epoch 12704: Training Loss: 0.0908161128560702 Validation Loss: 0.7733312249183655\n",
      "Epoch 12705: Training Loss: 0.09079693506161372 Validation Loss: 0.7725211977958679\n",
      "Epoch 12706: Training Loss: 0.09024884551763535 Validation Loss: 0.7722604870796204\n",
      "Epoch 12707: Training Loss: 0.0902500922481219 Validation Loss: 0.7716601490974426\n",
      "Epoch 12708: Training Loss: 0.09034958233435948 Validation Loss: 0.7717794179916382\n",
      "Epoch 12709: Training Loss: 0.09041352073351543 Validation Loss: 0.7723872065544128\n",
      "Epoch 12710: Training Loss: 0.09029547621806462 Validation Loss: 0.772628903388977\n",
      "Epoch 12711: Training Loss: 0.09056912114222844 Validation Loss: 0.7724429368972778\n",
      "Epoch 12712: Training Loss: 0.09014766911665599 Validation Loss: 0.7725176215171814\n",
      "Epoch 12713: Training Loss: 0.09036116302013397 Validation Loss: 0.7726842761039734\n",
      "Epoch 12714: Training Loss: 0.09027549376090367 Validation Loss: 0.772813081741333\n",
      "Epoch 12715: Training Loss: 0.09025714546442032 Validation Loss: 0.772757351398468\n",
      "Epoch 12716: Training Loss: 0.09068220605452855 Validation Loss: 0.7722224593162537\n",
      "Epoch 12717: Training Loss: 0.09043723593155543 Validation Loss: 0.7720201015472412\n",
      "Epoch 12718: Training Loss: 0.09080569197734197 Validation Loss: 0.7723363041877747\n",
      "Epoch 12719: Training Loss: 0.09040006995201111 Validation Loss: 0.7728960514068604\n",
      "Epoch 12720: Training Loss: 0.09040725231170654 Validation Loss: 0.7727957367897034\n",
      "Epoch 12721: Training Loss: 0.08990874389807384 Validation Loss: 0.7726308703422546\n",
      "Epoch 12722: Training Loss: 0.09021551410357158 Validation Loss: 0.7722092866897583\n",
      "Epoch 12723: Training Loss: 0.09060180683930714 Validation Loss: 0.7723629474639893\n",
      "Epoch 12724: Training Loss: 0.09009409447511037 Validation Loss: 0.7721874713897705\n",
      "Epoch 12725: Training Loss: 0.09015722821156184 Validation Loss: 0.772357165813446\n",
      "Epoch 12726: Training Loss: 0.09055979549884796 Validation Loss: 0.7725855112075806\n",
      "Epoch 12727: Training Loss: 0.08991331110397975 Validation Loss: 0.7722499966621399\n",
      "Epoch 12728: Training Loss: 0.09026981641848882 Validation Loss: 0.7723702788352966\n",
      "Epoch 12729: Training Loss: 0.09019273022810619 Validation Loss: 0.7719248533248901\n",
      "Epoch 12730: Training Loss: 0.08993388960758845 Validation Loss: 0.7719486951828003\n",
      "Epoch 12731: Training Loss: 0.09019766996304195 Validation Loss: 0.7719708681106567\n",
      "Epoch 12732: Training Loss: 0.09058070927858353 Validation Loss: 0.7727509140968323\n",
      "Epoch 12733: Training Loss: 0.0900561511516571 Validation Loss: 0.7728744745254517\n",
      "Epoch 12734: Training Loss: 0.09083935618400574 Validation Loss: 0.7726860642433167\n",
      "Epoch 12735: Training Loss: 0.09013661245505016 Validation Loss: 0.7725812792778015\n",
      "Epoch 12736: Training Loss: 0.09032331903775533 Validation Loss: 0.7725281119346619\n",
      "Epoch 12737: Training Loss: 0.09105811764796574 Validation Loss: 0.7728605270385742\n",
      "Epoch 12738: Training Loss: 0.09038281937440236 Validation Loss: 0.7724950909614563\n",
      "Epoch 12739: Training Loss: 0.09047327190637589 Validation Loss: 0.7726916670799255\n",
      "Epoch 12740: Training Loss: 0.0911265288790067 Validation Loss: 0.7726415991783142\n",
      "Epoch 12741: Training Loss: 0.09015843520561855 Validation Loss: 0.7730824947357178\n",
      "Epoch 12742: Training Loss: 0.09029733389616013 Validation Loss: 0.7725308537483215\n",
      "Epoch 12743: Training Loss: 0.09012425194183986 Validation Loss: 0.7727041244506836\n",
      "Epoch 12744: Training Loss: 0.08990651369094849 Validation Loss: 0.7726264595985413\n",
      "Epoch 12745: Training Loss: 0.09065453211466472 Validation Loss: 0.7725416421890259\n",
      "Epoch 12746: Training Loss: 0.0902502362926801 Validation Loss: 0.77244633436203\n",
      "Epoch 12747: Training Loss: 0.09007690101861954 Validation Loss: 0.7726345658302307\n",
      "Epoch 12748: Training Loss: 0.0908018946647644 Validation Loss: 0.7729377150535583\n",
      "Epoch 12749: Training Loss: 0.09015702207883199 Validation Loss: 0.7727034091949463\n",
      "Epoch 12750: Training Loss: 0.0900263786315918 Validation Loss: 0.7725008726119995\n",
      "Epoch 12751: Training Loss: 0.09010777125755946 Validation Loss: 0.772623598575592\n",
      "Epoch 12752: Training Loss: 0.0905298466483752 Validation Loss: 0.7728146910667419\n",
      "Epoch 12753: Training Loss: 0.09126107146342595 Validation Loss: 0.7728030681610107\n",
      "Epoch 12754: Training Loss: 0.0902169073621432 Validation Loss: 0.7727394700050354\n",
      "Epoch 12755: Training Loss: 0.0898531253139178 Validation Loss: 0.772819459438324\n",
      "Epoch 12756: Training Loss: 0.09004306048154831 Validation Loss: 0.7733304500579834\n",
      "Epoch 12757: Training Loss: 0.09015819678703944 Validation Loss: 0.7727969884872437\n",
      "Epoch 12758: Training Loss: 0.09093362341324489 Validation Loss: 0.7728080153465271\n",
      "Epoch 12759: Training Loss: 0.09032788127660751 Validation Loss: 0.7727460861206055\n",
      "Epoch 12760: Training Loss: 0.09039309372504552 Validation Loss: 0.7724465727806091\n",
      "Epoch 12761: Training Loss: 0.09028492371241252 Validation Loss: 0.7726178765296936\n",
      "Epoch 12762: Training Loss: 0.09004294623931249 Validation Loss: 0.7727998495101929\n",
      "Epoch 12763: Training Loss: 0.09023996690909068 Validation Loss: 0.7735296487808228\n",
      "Epoch 12764: Training Loss: 0.09029929091533025 Validation Loss: 0.773624837398529\n",
      "Epoch 12765: Training Loss: 0.09087081998586655 Validation Loss: 0.7734597325325012\n",
      "Epoch 12766: Training Loss: 0.09015332659085591 Validation Loss: 0.7731740474700928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12767: Training Loss: 0.09058783948421478 Validation Loss: 0.772958517074585\n",
      "Epoch 12768: Training Loss: 0.09004228313763936 Validation Loss: 0.7725278735160828\n",
      "Epoch 12769: Training Loss: 0.09063069025675456 Validation Loss: 0.7727433443069458\n",
      "Epoch 12770: Training Loss: 0.08990599711736043 Validation Loss: 0.7727560997009277\n",
      "Epoch 12771: Training Loss: 0.08990276604890823 Validation Loss: 0.7729227542877197\n",
      "Epoch 12772: Training Loss: 0.09014382213354111 Validation Loss: 0.7734179496765137\n",
      "Epoch 12773: Training Loss: 0.0903929074605306 Validation Loss: 0.7730894684791565\n",
      "Epoch 12774: Training Loss: 0.08998323231935501 Validation Loss: 0.7728594541549683\n",
      "Epoch 12775: Training Loss: 0.08993704865376155 Validation Loss: 0.772895872592926\n",
      "Epoch 12776: Training Loss: 0.09012830505768459 Validation Loss: 0.7731255292892456\n",
      "Epoch 12777: Training Loss: 0.09060004850228627 Validation Loss: 0.7727867960929871\n",
      "Epoch 12778: Training Loss: 0.090184785425663 Validation Loss: 0.7731962203979492\n",
      "Epoch 12779: Training Loss: 0.08984618385632832 Validation Loss: 0.7733409404754639\n",
      "Epoch 12780: Training Loss: 0.09001084913810094 Validation Loss: 0.7731638550758362\n",
      "Epoch 12781: Training Loss: 0.0899606595436732 Validation Loss: 0.772948682308197\n",
      "Epoch 12782: Training Loss: 0.09096481402715047 Validation Loss: 0.7726547122001648\n",
      "Epoch 12783: Training Loss: 0.09006402641534805 Validation Loss: 0.7732108235359192\n",
      "Epoch 12784: Training Loss: 0.09023653467496236 Validation Loss: 0.7736636996269226\n",
      "Epoch 12785: Training Loss: 0.09025751054286957 Validation Loss: 0.7739631533622742\n",
      "Epoch 12786: Training Loss: 0.08993318676948547 Validation Loss: 0.7736566662788391\n",
      "Epoch 12787: Training Loss: 0.09003087629874547 Validation Loss: 0.7731775641441345\n",
      "Epoch 12788: Training Loss: 0.0900609369079272 Validation Loss: 0.7729702591896057\n",
      "Epoch 12789: Training Loss: 0.09002343068520229 Validation Loss: 0.7728528380393982\n",
      "Epoch 12790: Training Loss: 0.08991483102242152 Validation Loss: 0.773079514503479\n",
      "Epoch 12791: Training Loss: 0.09015382081270218 Validation Loss: 0.7733146548271179\n",
      "Epoch 12792: Training Loss: 0.0901367316643397 Validation Loss: 0.7734522819519043\n",
      "Epoch 12793: Training Loss: 0.09064264347155888 Validation Loss: 0.7733345031738281\n",
      "Epoch 12794: Training Loss: 0.08997475852568944 Validation Loss: 0.7735846042633057\n",
      "Epoch 12795: Training Loss: 0.0901473139723142 Validation Loss: 0.772982120513916\n",
      "Epoch 12796: Training Loss: 0.09038524826367696 Validation Loss: 0.7730847001075745\n",
      "Epoch 12797: Training Loss: 0.08989515900611877 Validation Loss: 0.773600697517395\n",
      "Epoch 12798: Training Loss: 0.08994284023841222 Validation Loss: 0.7740636467933655\n",
      "Epoch 12799: Training Loss: 0.0902114858229955 Validation Loss: 0.7733427286148071\n",
      "Epoch 12800: Training Loss: 0.09007256478071213 Validation Loss: 0.7729093432426453\n",
      "Epoch 12801: Training Loss: 0.08996526151895523 Validation Loss: 0.7727974653244019\n",
      "Epoch 12802: Training Loss: 0.09013986090819041 Validation Loss: 0.7730878591537476\n",
      "Epoch 12803: Training Loss: 0.09022147456804912 Validation Loss: 0.7733423113822937\n",
      "Epoch 12804: Training Loss: 0.08980440100034077 Validation Loss: 0.7732793688774109\n",
      "Epoch 12805: Training Loss: 0.09080532193183899 Validation Loss: 0.7733429670333862\n",
      "Epoch 12806: Training Loss: 0.09112019091844559 Validation Loss: 0.7736355066299438\n",
      "Epoch 12807: Training Loss: 0.08988215774297714 Validation Loss: 0.773186445236206\n",
      "Epoch 12808: Training Loss: 0.089531143506368 Validation Loss: 0.773013174533844\n",
      "Epoch 12809: Training Loss: 0.09102713068326314 Validation Loss: 0.7726095914840698\n",
      "Epoch 12810: Training Loss: 0.09002275764942169 Validation Loss: 0.7728092074394226\n",
      "Epoch 12811: Training Loss: 0.08980969587961833 Validation Loss: 0.7727428078651428\n",
      "Epoch 12812: Training Loss: 0.09031585355599721 Validation Loss: 0.772972822189331\n",
      "Epoch 12813: Training Loss: 0.08947326739629109 Validation Loss: 0.7732565999031067\n",
      "Epoch 12814: Training Loss: 0.08961845437685649 Validation Loss: 0.7734886407852173\n",
      "Epoch 12815: Training Loss: 0.0900957261522611 Validation Loss: 0.7734655737876892\n",
      "Epoch 12816: Training Loss: 0.09006558855374654 Validation Loss: 0.7737035155296326\n",
      "Epoch 12817: Training Loss: 0.08959127217531204 Validation Loss: 0.7735689878463745\n",
      "Epoch 12818: Training Loss: 0.08982039988040924 Validation Loss: 0.7736433744430542\n",
      "Epoch 12819: Training Loss: 0.08941053847471873 Validation Loss: 0.773451030254364\n",
      "Epoch 12820: Training Loss: 0.09157566477855046 Validation Loss: 0.7732033729553223\n",
      "Epoch 12821: Training Loss: 0.08963149289290111 Validation Loss: 0.7734459638595581\n",
      "Epoch 12822: Training Loss: 0.08946442852417628 Validation Loss: 0.7730830311775208\n",
      "Epoch 12823: Training Loss: 0.08993774652481079 Validation Loss: 0.7738723754882812\n",
      "Epoch 12824: Training Loss: 0.08974069108565648 Validation Loss: 0.7738426923751831\n",
      "Epoch 12825: Training Loss: 0.09020758668581645 Validation Loss: 0.773452877998352\n",
      "Epoch 12826: Training Loss: 0.08974027633666992 Validation Loss: 0.7736501097679138\n",
      "Epoch 12827: Training Loss: 0.0903371994694074 Validation Loss: 0.773576021194458\n",
      "Epoch 12828: Training Loss: 0.08966957281033199 Validation Loss: 0.7734811305999756\n",
      "Epoch 12829: Training Loss: 0.08991048236687978 Validation Loss: 0.7735485434532166\n",
      "Epoch 12830: Training Loss: 0.09049562861522038 Validation Loss: 0.7732046246528625\n",
      "Epoch 12831: Training Loss: 0.08994229137897491 Validation Loss: 0.7735588550567627\n",
      "Epoch 12832: Training Loss: 0.09028519441684087 Validation Loss: 0.7735042572021484\n",
      "Epoch 12833: Training Loss: 0.09021267294883728 Validation Loss: 0.7737458348274231\n",
      "Epoch 12834: Training Loss: 0.08976934105157852 Validation Loss: 0.7736722826957703\n",
      "Epoch 12835: Training Loss: 0.09047246972719829 Validation Loss: 0.7735493183135986\n",
      "Epoch 12836: Training Loss: 0.09016744792461395 Validation Loss: 0.7735462188720703\n",
      "Epoch 12837: Training Loss: 0.08960521717866261 Validation Loss: 0.7738577127456665\n",
      "Epoch 12838: Training Loss: 0.09043719122807185 Validation Loss: 0.7739004492759705\n",
      "Epoch 12839: Training Loss: 0.08962354560693105 Validation Loss: 0.7737348079681396\n",
      "Epoch 12840: Training Loss: 0.08970336119333903 Validation Loss: 0.773662269115448\n",
      "Epoch 12841: Training Loss: 0.08990662048260371 Validation Loss: 0.7735457420349121\n",
      "Epoch 12842: Training Loss: 0.09013313303391139 Validation Loss: 0.7738901972770691\n",
      "Epoch 12843: Training Loss: 0.08957844475905101 Validation Loss: 0.7737761735916138\n",
      "Epoch 12844: Training Loss: 0.08967743317286174 Validation Loss: 0.7737138867378235\n",
      "Epoch 12845: Training Loss: 0.09060208375255267 Validation Loss: 0.7735680341720581\n",
      "Epoch 12846: Training Loss: 0.08985063930352528 Validation Loss: 0.7734445333480835\n",
      "Epoch 12847: Training Loss: 0.09024708718061447 Validation Loss: 0.773809015750885\n",
      "Epoch 12848: Training Loss: 0.08978750556707382 Validation Loss: 0.7734563946723938\n",
      "Epoch 12849: Training Loss: 0.08999721457560857 Validation Loss: 0.7734362483024597\n",
      "Epoch 12850: Training Loss: 0.09044507145881653 Validation Loss: 0.7731334567070007\n",
      "Epoch 12851: Training Loss: 0.09015397975842158 Validation Loss: 0.7733169198036194\n",
      "Epoch 12852: Training Loss: 0.08983055253823598 Validation Loss: 0.7732957005500793\n",
      "Epoch 12853: Training Loss: 0.08988117178281148 Validation Loss: 0.7734354734420776\n",
      "Epoch 12854: Training Loss: 0.089565875629584 Validation Loss: 0.7731211185455322\n",
      "Epoch 12855: Training Loss: 0.08975885063409805 Validation Loss: 0.7737182974815369\n",
      "Epoch 12856: Training Loss: 0.0900786245862643 Validation Loss: 0.7735720872879028\n",
      "Epoch 12857: Training Loss: 0.08985435714324315 Validation Loss: 0.7736681699752808\n",
      "Epoch 12858: Training Loss: 0.08993157744407654 Validation Loss: 0.7740308046340942\n",
      "Epoch 12859: Training Loss: 0.08963803698619206 Validation Loss: 0.7740852236747742\n",
      "Epoch 12860: Training Loss: 0.08973798155784607 Validation Loss: 0.7734218239784241\n",
      "Epoch 12861: Training Loss: 0.0902056023478508 Validation Loss: 0.7734103202819824\n",
      "Epoch 12862: Training Loss: 0.08992743740479152 Validation Loss: 0.7735254764556885\n",
      "Epoch 12863: Training Loss: 0.0895474726955096 Validation Loss: 0.7735326886177063\n",
      "Epoch 12864: Training Loss: 0.08993004510800044 Validation Loss: 0.7735505700111389\n",
      "Epoch 12865: Training Loss: 0.08966925988594691 Validation Loss: 0.7733372449874878\n",
      "Epoch 12866: Training Loss: 0.08984808623790741 Validation Loss: 0.7738619446754456\n",
      "Epoch 12867: Training Loss: 0.08910440653562546 Validation Loss: 0.7738245129585266\n",
      "Epoch 12868: Training Loss: 0.0896712417403857 Validation Loss: 0.7741690874099731\n",
      "Epoch 12869: Training Loss: 0.089689036210378 Validation Loss: 0.7741438746452332\n",
      "Epoch 12870: Training Loss: 0.0900263140598933 Validation Loss: 0.7739018797874451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12871: Training Loss: 0.08923661708831787 Validation Loss: 0.7736380100250244\n",
      "Epoch 12872: Training Loss: 0.08963451782862346 Validation Loss: 0.7737718820571899\n",
      "Epoch 12873: Training Loss: 0.08983526627222697 Validation Loss: 0.7738932967185974\n",
      "Epoch 12874: Training Loss: 0.08947889506816864 Validation Loss: 0.7739312052726746\n",
      "Epoch 12875: Training Loss: 0.08944332599639893 Validation Loss: 0.7735665440559387\n",
      "Epoch 12876: Training Loss: 0.09066005299488704 Validation Loss: 0.7730470299720764\n",
      "Epoch 12877: Training Loss: 0.0900543083747228 Validation Loss: 0.773108184337616\n",
      "Epoch 12878: Training Loss: 0.08956526964902878 Validation Loss: 0.7734836339950562\n",
      "Epoch 12879: Training Loss: 0.08956346909205119 Validation Loss: 0.7736586928367615\n",
      "Epoch 12880: Training Loss: 0.09031451245148976 Validation Loss: 0.7740882039070129\n",
      "Epoch 12881: Training Loss: 0.0902026320497195 Validation Loss: 0.7738310098648071\n",
      "Epoch 12882: Training Loss: 0.08940104395151138 Validation Loss: 0.7740948796272278\n",
      "Epoch 12883: Training Loss: 0.08986747761567433 Validation Loss: 0.7746335864067078\n",
      "Epoch 12884: Training Loss: 0.0897492269674937 Validation Loss: 0.7742908000946045\n",
      "Epoch 12885: Training Loss: 0.08935452004273732 Validation Loss: 0.7741543054580688\n",
      "Epoch 12886: Training Loss: 0.08966214954853058 Validation Loss: 0.7742739915847778\n",
      "Epoch 12887: Training Loss: 0.08962808549404144 Validation Loss: 0.774398684501648\n",
      "Epoch 12888: Training Loss: 0.08988101532061894 Validation Loss: 0.7742005586624146\n",
      "Epoch 12889: Training Loss: 0.08967298020919164 Validation Loss: 0.7740824818611145\n",
      "Epoch 12890: Training Loss: 0.08979826172192891 Validation Loss: 0.774558424949646\n",
      "Epoch 12891: Training Loss: 0.08958301693201065 Validation Loss: 0.7744948267936707\n",
      "Epoch 12892: Training Loss: 0.09022786716620128 Validation Loss: 0.7742297053337097\n",
      "Epoch 12893: Training Loss: 0.08926571905612946 Validation Loss: 0.7738761901855469\n",
      "Epoch 12894: Training Loss: 0.08953484892845154 Validation Loss: 0.7734034061431885\n",
      "Epoch 12895: Training Loss: 0.08964961270491283 Validation Loss: 0.7732322812080383\n",
      "Epoch 12896: Training Loss: 0.08984359353780746 Validation Loss: 0.7738885283470154\n",
      "Epoch 12897: Training Loss: 0.0895145336786906 Validation Loss: 0.774642288684845\n",
      "Epoch 12898: Training Loss: 0.08992816756169002 Validation Loss: 0.7750706672668457\n",
      "Epoch 12899: Training Loss: 0.08985979855060577 Validation Loss: 0.7745712399482727\n",
      "Epoch 12900: Training Loss: 0.0893339862426122 Validation Loss: 0.7746471762657166\n",
      "Epoch 12901: Training Loss: 0.0896680901447932 Validation Loss: 0.773927628993988\n",
      "Epoch 12902: Training Loss: 0.08949306358893712 Validation Loss: 0.7743188142776489\n",
      "Epoch 12903: Training Loss: 0.08953037361303966 Validation Loss: 0.774236261844635\n",
      "Epoch 12904: Training Loss: 0.0903305783867836 Validation Loss: 0.774642825126648\n",
      "Epoch 12905: Training Loss: 0.0897080550591151 Validation Loss: 0.7747926115989685\n",
      "Epoch 12906: Training Loss: 0.08966085563103358 Validation Loss: 0.774739146232605\n",
      "Epoch 12907: Training Loss: 0.09051946550607681 Validation Loss: 0.7735828757286072\n",
      "Epoch 12908: Training Loss: 0.0895559440056483 Validation Loss: 0.7733662724494934\n",
      "Epoch 12909: Training Loss: 0.08982506146033604 Validation Loss: 0.7734726667404175\n",
      "Epoch 12910: Training Loss: 0.089944988489151 Validation Loss: 0.7740910053253174\n",
      "Epoch 12911: Training Loss: 0.08956529945135117 Validation Loss: 0.7744747400283813\n",
      "Epoch 12912: Training Loss: 0.08963358650604884 Validation Loss: 0.7743059992790222\n",
      "Epoch 12913: Training Loss: 0.08959446847438812 Validation Loss: 0.7746589779853821\n",
      "Epoch 12914: Training Loss: 0.08995840450127919 Validation Loss: 0.7750142812728882\n",
      "Epoch 12915: Training Loss: 0.08925168961286545 Validation Loss: 0.7747528553009033\n",
      "Epoch 12916: Training Loss: 0.08946431676546733 Validation Loss: 0.7745087742805481\n",
      "Epoch 12917: Training Loss: 0.08965923637151718 Validation Loss: 0.7740864157676697\n",
      "Epoch 12918: Training Loss: 0.08940706650416057 Validation Loss: 0.7739232778549194\n",
      "Epoch 12919: Training Loss: 0.0904545858502388 Validation Loss: 0.7745241522789001\n",
      "Epoch 12920: Training Loss: 0.09069891770680745 Validation Loss: 0.774844765663147\n",
      "Epoch 12921: Training Loss: 0.08951890220244725 Validation Loss: 0.7753485441207886\n",
      "Epoch 12922: Training Loss: 0.08959375073512395 Validation Loss: 0.7750653624534607\n",
      "Epoch 12923: Training Loss: 0.08922472099463145 Validation Loss: 0.7746531367301941\n",
      "Epoch 12924: Training Loss: 0.08950842916965485 Validation Loss: 0.7744050025939941\n",
      "Epoch 12925: Training Loss: 0.08953368167082469 Validation Loss: 0.7744248509407043\n",
      "Epoch 12926: Training Loss: 0.08948710560798645 Validation Loss: 0.7739499807357788\n",
      "Epoch 12927: Training Loss: 0.09005444745222728 Validation Loss: 0.7741817235946655\n",
      "Epoch 12928: Training Loss: 0.08940598368644714 Validation Loss: 0.7744498252868652\n",
      "Epoch 12929: Training Loss: 0.08998341113328934 Validation Loss: 0.7748656272888184\n",
      "Epoch 12930: Training Loss: 0.08958571900924046 Validation Loss: 0.7745709419250488\n",
      "Epoch 12931: Training Loss: 0.09003264456987381 Validation Loss: 0.7745006680488586\n",
      "Epoch 12932: Training Loss: 0.08928265670935313 Validation Loss: 0.7746811509132385\n",
      "Epoch 12933: Training Loss: 0.08939272413651149 Validation Loss: 0.7745612859725952\n",
      "Epoch 12934: Training Loss: 0.08956004679203033 Validation Loss: 0.774259090423584\n",
      "Epoch 12935: Training Loss: 0.08938575784365337 Validation Loss: 0.7749567627906799\n",
      "Epoch 12936: Training Loss: 0.08959200729926427 Validation Loss: 0.7750513553619385\n",
      "Epoch 12937: Training Loss: 0.0899277205268542 Validation Loss: 0.7753031849861145\n",
      "Epoch 12938: Training Loss: 0.09003521998723348 Validation Loss: 0.7752963304519653\n",
      "Epoch 12939: Training Loss: 0.08954652895530064 Validation Loss: 0.7748908996582031\n",
      "Epoch 12940: Training Loss: 0.08958896497885387 Validation Loss: 0.7748584747314453\n",
      "Epoch 12941: Training Loss: 0.08987041066090266 Validation Loss: 0.7746400833129883\n",
      "Epoch 12942: Training Loss: 0.08990612377723058 Validation Loss: 0.774177610874176\n",
      "Epoch 12943: Training Loss: 0.08963947743177414 Validation Loss: 0.7744429111480713\n",
      "Epoch 12944: Training Loss: 0.08930192639430364 Validation Loss: 0.7747336626052856\n",
      "Epoch 12945: Training Loss: 0.08940730492273967 Validation Loss: 0.774677038192749\n",
      "Epoch 12946: Training Loss: 0.08947529643774033 Validation Loss: 0.774401843547821\n",
      "Epoch 12947: Training Loss: 0.08947108189264934 Validation Loss: 0.7748651504516602\n",
      "Epoch 12948: Training Loss: 0.08978493014971416 Validation Loss: 0.7746139168739319\n",
      "Epoch 12949: Training Loss: 0.08933085948228836 Validation Loss: 0.7744789719581604\n",
      "Epoch 12950: Training Loss: 0.08967806647221248 Validation Loss: 0.7744427919387817\n",
      "Epoch 12951: Training Loss: 0.0895325963695844 Validation Loss: 0.7743531465530396\n",
      "Epoch 12952: Training Loss: 0.08955229818820953 Validation Loss: 0.7744662165641785\n",
      "Epoch 12953: Training Loss: 0.0892983724673589 Validation Loss: 0.7747414708137512\n",
      "Epoch 12954: Training Loss: 0.08952439576387405 Validation Loss: 0.7748423218727112\n",
      "Epoch 12955: Training Loss: 0.08939669529596965 Validation Loss: 0.7746257781982422\n",
      "Epoch 12956: Training Loss: 0.08955984065930049 Validation Loss: 0.7748397588729858\n",
      "Epoch 12957: Training Loss: 0.08941726634899776 Validation Loss: 0.7746496796607971\n",
      "Epoch 12958: Training Loss: 0.08926135301589966 Validation Loss: 0.774543046951294\n",
      "Epoch 12959: Training Loss: 0.0894166926542918 Validation Loss: 0.7737724184989929\n",
      "Epoch 12960: Training Loss: 0.0896824449300766 Validation Loss: 0.7738491892814636\n",
      "Epoch 12961: Training Loss: 0.08964210003614426 Validation Loss: 0.7747238874435425\n",
      "Epoch 12962: Training Loss: 0.08929560581843059 Validation Loss: 0.774456262588501\n",
      "Epoch 12963: Training Loss: 0.08954619864622752 Validation Loss: 0.7749192118644714\n",
      "Epoch 12964: Training Loss: 0.08924407511949539 Validation Loss: 0.7747033834457397\n",
      "Epoch 12965: Training Loss: 0.08952782799800237 Validation Loss: 0.7750393152236938\n",
      "Epoch 12966: Training Loss: 0.08932705968618393 Validation Loss: 0.7752676010131836\n",
      "Epoch 12967: Training Loss: 0.08961621175209682 Validation Loss: 0.7748284339904785\n",
      "Epoch 12968: Training Loss: 0.08938792596260707 Validation Loss: 0.774125874042511\n",
      "Epoch 12969: Training Loss: 0.09022638946771622 Validation Loss: 0.7740271687507629\n",
      "Epoch 12970: Training Loss: 0.08945652842521667 Validation Loss: 0.774631142616272\n",
      "Epoch 12971: Training Loss: 0.08944632609685262 Validation Loss: 0.7746209502220154\n",
      "Epoch 12972: Training Loss: 0.08936407417058945 Validation Loss: 0.774735689163208\n",
      "Epoch 12973: Training Loss: 0.08926451702912648 Validation Loss: 0.7745947241783142\n",
      "Epoch 12974: Training Loss: 0.08925708631674449 Validation Loss: 0.7748883366584778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12975: Training Loss: 0.08919831365346909 Validation Loss: 0.774956464767456\n",
      "Epoch 12976: Training Loss: 0.08951531598965327 Validation Loss: 0.7748465538024902\n",
      "Epoch 12977: Training Loss: 0.08987149099508922 Validation Loss: 0.7743581533432007\n",
      "Epoch 12978: Training Loss: 0.08930929253498714 Validation Loss: 0.7747968435287476\n",
      "Epoch 12979: Training Loss: 0.08948756506045659 Validation Loss: 0.7748357057571411\n",
      "Epoch 12980: Training Loss: 0.08978843688964844 Validation Loss: 0.7749977111816406\n",
      "Epoch 12981: Training Loss: 0.08894831438859303 Validation Loss: 0.7748333215713501\n",
      "Epoch 12982: Training Loss: 0.08914092183113098 Validation Loss: 0.7748032808303833\n",
      "Epoch 12983: Training Loss: 0.08947473019361496 Validation Loss: 0.7747567296028137\n",
      "Epoch 12984: Training Loss: 0.08934776733318965 Validation Loss: 0.7749410271644592\n",
      "Epoch 12985: Training Loss: 0.0896500696738561 Validation Loss: 0.7748058438301086\n",
      "Epoch 12986: Training Loss: 0.08920969814062119 Validation Loss: 0.7747803330421448\n",
      "Epoch 12987: Training Loss: 0.08930702755848567 Validation Loss: 0.7749178409576416\n",
      "Epoch 12988: Training Loss: 0.08931330343087514 Validation Loss: 0.7748330235481262\n",
      "Epoch 12989: Training Loss: 0.08920822789271672 Validation Loss: 0.7748421430587769\n",
      "Epoch 12990: Training Loss: 0.08915995806455612 Validation Loss: 0.7749558687210083\n",
      "Epoch 12991: Training Loss: 0.08923196295897166 Validation Loss: 0.7748690843582153\n",
      "Epoch 12992: Training Loss: 0.0890779619415601 Validation Loss: 0.7747203707695007\n",
      "Epoch 12993: Training Loss: 0.08915799856185913 Validation Loss: 0.7746977210044861\n",
      "Epoch 12994: Training Loss: 0.08936111629009247 Validation Loss: 0.774773895740509\n",
      "Epoch 12995: Training Loss: 0.08907821277777354 Validation Loss: 0.7749381065368652\n",
      "Epoch 12996: Training Loss: 0.09009893238544464 Validation Loss: 0.7746952176094055\n",
      "Epoch 12997: Training Loss: 0.0890427827835083 Validation Loss: 0.7748923301696777\n",
      "Epoch 12998: Training Loss: 0.08917867143948872 Validation Loss: 0.7749454379081726\n",
      "Epoch 12999: Training Loss: 0.0894317552447319 Validation Loss: 0.7749528884887695\n",
      "Epoch 13000: Training Loss: 0.08964371432860692 Validation Loss: 0.7750144600868225\n",
      "Epoch 13001: Training Loss: 0.08957105626662572 Validation Loss: 0.7750612497329712\n",
      "Epoch 13002: Training Loss: 0.08929705371459325 Validation Loss: 0.7755016088485718\n",
      "Epoch 13003: Training Loss: 0.08945790429910024 Validation Loss: 0.7751424908638\n",
      "Epoch 13004: Training Loss: 0.08970549205938975 Validation Loss: 0.7748552560806274\n",
      "Epoch 13005: Training Loss: 0.08911181489626567 Validation Loss: 0.7747858762741089\n",
      "Epoch 13006: Training Loss: 0.08939415216445923 Validation Loss: 0.7747976779937744\n",
      "Epoch 13007: Training Loss: 0.08920432378848393 Validation Loss: 0.7747591137886047\n",
      "Epoch 13008: Training Loss: 0.08982393393913905 Validation Loss: 0.7745431661605835\n",
      "Epoch 13009: Training Loss: 0.08935106048981349 Validation Loss: 0.7752823829650879\n",
      "Epoch 13010: Training Loss: 0.08913284540176392 Validation Loss: 0.7753623723983765\n",
      "Epoch 13011: Training Loss: 0.08917776495218277 Validation Loss: 0.7753205299377441\n",
      "Epoch 13012: Training Loss: 0.08923439433177312 Validation Loss: 0.7751205563545227\n",
      "Epoch 13013: Training Loss: 0.08941802630821864 Validation Loss: 0.7747640609741211\n",
      "Epoch 13014: Training Loss: 0.08920627335707347 Validation Loss: 0.7749180197715759\n",
      "Epoch 13015: Training Loss: 0.0893489345908165 Validation Loss: 0.7748906016349792\n",
      "Epoch 13016: Training Loss: 0.08922365059455235 Validation Loss: 0.7747943997383118\n",
      "Epoch 13017: Training Loss: 0.08924879878759384 Validation Loss: 0.7753032445907593\n",
      "Epoch 13018: Training Loss: 0.08934212227662404 Validation Loss: 0.7753921151161194\n",
      "Epoch 13019: Training Loss: 0.08927798022826512 Validation Loss: 0.7753946185112\n",
      "Epoch 13020: Training Loss: 0.08914392193158467 Validation Loss: 0.7754217386245728\n",
      "Epoch 13021: Training Loss: 0.08944192032019298 Validation Loss: 0.7749828100204468\n",
      "Epoch 13022: Training Loss: 0.08910050491491954 Validation Loss: 0.7750893235206604\n",
      "Epoch 13023: Training Loss: 0.08895282695690791 Validation Loss: 0.7752927541732788\n",
      "Epoch 13024: Training Loss: 0.08950681487719218 Validation Loss: 0.7754480242729187\n",
      "Epoch 13025: Training Loss: 0.08942249168952306 Validation Loss: 0.7755609154701233\n",
      "Epoch 13026: Training Loss: 0.08896862715482712 Validation Loss: 0.7752998471260071\n",
      "Epoch 13027: Training Loss: 0.08943396310011546 Validation Loss: 0.7749254703521729\n",
      "Epoch 13028: Training Loss: 0.08924454947312672 Validation Loss: 0.7750623822212219\n",
      "Epoch 13029: Training Loss: 0.08907487740119298 Validation Loss: 0.7753263115882874\n",
      "Epoch 13030: Training Loss: 0.0891447514295578 Validation Loss: 0.775702953338623\n",
      "Epoch 13031: Training Loss: 0.0892310564716657 Validation Loss: 0.7754341959953308\n",
      "Epoch 13032: Training Loss: 0.08909257004658382 Validation Loss: 0.7753587961196899\n",
      "Epoch 13033: Training Loss: 0.08945770313342412 Validation Loss: 0.7753332257270813\n",
      "Epoch 13034: Training Loss: 0.08927787592013676 Validation Loss: 0.7754552960395813\n",
      "Epoch 13035: Training Loss: 0.0896088679631551 Validation Loss: 0.7754648327827454\n",
      "Epoch 13036: Training Loss: 0.08927123496929805 Validation Loss: 0.7751553654670715\n",
      "Epoch 13037: Training Loss: 0.08924565215905507 Validation Loss: 0.7747219204902649\n",
      "Epoch 13038: Training Loss: 0.08898560454448064 Validation Loss: 0.7745700478553772\n",
      "Epoch 13039: Training Loss: 0.08950874209403992 Validation Loss: 0.7751588821411133\n",
      "Epoch 13040: Training Loss: 0.08912512411673863 Validation Loss: 0.7760794758796692\n",
      "Epoch 13041: Training Loss: 0.08901799221833546 Validation Loss: 0.7762052416801453\n",
      "Epoch 13042: Training Loss: 0.08918907741705577 Validation Loss: 0.7758622765541077\n",
      "Epoch 13043: Training Loss: 0.08897790809472401 Validation Loss: 0.7755054831504822\n",
      "Epoch 13044: Training Loss: 0.08994124581416447 Validation Loss: 0.775102436542511\n",
      "Epoch 13045: Training Loss: 0.08956020573774974 Validation Loss: 0.7750908732414246\n",
      "Epoch 13046: Training Loss: 0.08920928339163463 Validation Loss: 0.775554358959198\n",
      "Epoch 13047: Training Loss: 0.08918508390585582 Validation Loss: 0.7752534747123718\n",
      "Epoch 13048: Training Loss: 0.08995134631792705 Validation Loss: 0.7757906317710876\n",
      "Epoch 13049: Training Loss: 0.0891937439640363 Validation Loss: 0.7756526470184326\n",
      "Epoch 13050: Training Loss: 0.08893980085849762 Validation Loss: 0.7752651572227478\n",
      "Epoch 13051: Training Loss: 0.08879907180865605 Validation Loss: 0.7749736905097961\n",
      "Epoch 13052: Training Loss: 0.0889981339375178 Validation Loss: 0.7751741409301758\n",
      "Epoch 13053: Training Loss: 0.08865411827961604 Validation Loss: 0.7753793597221375\n",
      "Epoch 13054: Training Loss: 0.08918445805708568 Validation Loss: 0.7758793830871582\n",
      "Epoch 13055: Training Loss: 0.0893611212571462 Validation Loss: 0.7761080265045166\n",
      "Epoch 13056: Training Loss: 0.0890801524122556 Validation Loss: 0.7764025330543518\n",
      "Epoch 13057: Training Loss: 0.08920046438773473 Validation Loss: 0.7754954099655151\n",
      "Epoch 13058: Training Loss: 0.08890252808729808 Validation Loss: 0.7749919295310974\n",
      "Epoch 13059: Training Loss: 0.08874127268791199 Validation Loss: 0.7749201655387878\n",
      "Epoch 13060: Training Loss: 0.0892184649904569 Validation Loss: 0.7752515077590942\n",
      "Epoch 13061: Training Loss: 0.08894734581311543 Validation Loss: 0.7757435441017151\n",
      "Epoch 13062: Training Loss: 0.08984545121590297 Validation Loss: 0.7759842872619629\n",
      "Epoch 13063: Training Loss: 0.08906202266613643 Validation Loss: 0.7758678793907166\n",
      "Epoch 13064: Training Loss: 0.08882054686546326 Validation Loss: 0.7760071158409119\n",
      "Epoch 13065: Training Loss: 0.08920278896888097 Validation Loss: 0.7756580710411072\n",
      "Epoch 13066: Training Loss: 0.0889573444922765 Validation Loss: 0.7758708596229553\n",
      "Epoch 13067: Training Loss: 0.08939717461665471 Validation Loss: 0.7754215002059937\n",
      "Epoch 13068: Training Loss: 0.08891633401314418 Validation Loss: 0.7757028341293335\n",
      "Epoch 13069: Training Loss: 0.08897232512633006 Validation Loss: 0.7762334942817688\n",
      "Epoch 13070: Training Loss: 0.08929534256458282 Validation Loss: 0.7761333584785461\n",
      "Epoch 13071: Training Loss: 0.08936841537555058 Validation Loss: 0.7758014798164368\n",
      "Epoch 13072: Training Loss: 0.08909330020348231 Validation Loss: 0.7754834890365601\n",
      "Epoch 13073: Training Loss: 0.0890473226706187 Validation Loss: 0.7754889130592346\n",
      "Epoch 13074: Training Loss: 0.08898111432790756 Validation Loss: 0.7754887342453003\n",
      "Epoch 13075: Training Loss: 0.08919921517372131 Validation Loss: 0.7759782075881958\n",
      "Epoch 13076: Training Loss: 0.08895621697107951 Validation Loss: 0.7758941650390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13077: Training Loss: 0.08865581701199214 Validation Loss: 0.7754946947097778\n",
      "Epoch 13078: Training Loss: 0.08855544279019038 Validation Loss: 0.7750088572502136\n",
      "Epoch 13079: Training Loss: 0.08919864147901535 Validation Loss: 0.7748271822929382\n",
      "Epoch 13080: Training Loss: 0.08905609200398128 Validation Loss: 0.7754408121109009\n",
      "Epoch 13081: Training Loss: 0.08906003832817078 Validation Loss: 0.7756280899047852\n",
      "Epoch 13082: Training Loss: 0.08981337398290634 Validation Loss: 0.776211678981781\n",
      "Epoch 13083: Training Loss: 0.08905291805664699 Validation Loss: 0.7762413620948792\n",
      "Epoch 13084: Training Loss: 0.0889926627278328 Validation Loss: 0.7759990096092224\n",
      "Epoch 13085: Training Loss: 0.08870551735162735 Validation Loss: 0.7755869626998901\n",
      "Epoch 13086: Training Loss: 0.08905516813198726 Validation Loss: 0.7757770419120789\n",
      "Epoch 13087: Training Loss: 0.08997941265503566 Validation Loss: 0.7760441303253174\n",
      "Epoch 13088: Training Loss: 0.08909221738576889 Validation Loss: 0.775820255279541\n",
      "Epoch 13089: Training Loss: 0.08891328672568004 Validation Loss: 0.7757325172424316\n",
      "Epoch 13090: Training Loss: 0.08894212047259013 Validation Loss: 0.7758312821388245\n",
      "Epoch 13091: Training Loss: 0.08894779533147812 Validation Loss: 0.7760993838310242\n",
      "Epoch 13092: Training Loss: 0.08954123904307683 Validation Loss: 0.7758312821388245\n",
      "Epoch 13093: Training Loss: 0.08893843491872151 Validation Loss: 0.7761120200157166\n",
      "Epoch 13094: Training Loss: 0.08911234885454178 Validation Loss: 0.7763700485229492\n",
      "Epoch 13095: Training Loss: 0.08924922347068787 Validation Loss: 0.7766699194908142\n",
      "Epoch 13096: Training Loss: 0.08932524174451828 Validation Loss: 0.7764201164245605\n",
      "Epoch 13097: Training Loss: 0.08884941786527634 Validation Loss: 0.7760729789733887\n",
      "Epoch 13098: Training Loss: 0.08932392050822575 Validation Loss: 0.7756096124649048\n",
      "Epoch 13099: Training Loss: 0.08959614237149556 Validation Loss: 0.7753258347511292\n",
      "Epoch 13100: Training Loss: 0.08905943979819615 Validation Loss: 0.7753913998603821\n",
      "Epoch 13101: Training Loss: 0.0891001174847285 Validation Loss: 0.7756767868995667\n",
      "Epoch 13102: Training Loss: 0.08877281596263249 Validation Loss: 0.7763599157333374\n",
      "Epoch 13103: Training Loss: 0.0890827625989914 Validation Loss: 0.7762981057167053\n",
      "Epoch 13104: Training Loss: 0.08882063378890355 Validation Loss: 0.7762003540992737\n",
      "Epoch 13105: Training Loss: 0.0891097088654836 Validation Loss: 0.7757533192634583\n",
      "Epoch 13106: Training Loss: 0.0888682355483373 Validation Loss: 0.775429368019104\n",
      "Epoch 13107: Training Loss: 0.08890790989001592 Validation Loss: 0.7752408385276794\n",
      "Epoch 13108: Training Loss: 0.08895260343949 Validation Loss: 0.7757068872451782\n",
      "Epoch 13109: Training Loss: 0.08902581036090851 Validation Loss: 0.7758125066757202\n",
      "Epoch 13110: Training Loss: 0.08897896856069565 Validation Loss: 0.7757128477096558\n",
      "Epoch 13111: Training Loss: 0.08901278922955196 Validation Loss: 0.77608722448349\n",
      "Epoch 13112: Training Loss: 0.08895716319481532 Validation Loss: 0.7759639620780945\n",
      "Epoch 13113: Training Loss: 0.08889010548591614 Validation Loss: 0.7759990096092224\n",
      "Epoch 13114: Training Loss: 0.08887957284847896 Validation Loss: 0.7758280038833618\n",
      "Epoch 13115: Training Loss: 0.08845735589663188 Validation Loss: 0.775795578956604\n",
      "Epoch 13116: Training Loss: 0.08879173050324123 Validation Loss: 0.7758929133415222\n",
      "Epoch 13117: Training Loss: 0.08992498616377513 Validation Loss: 0.7758954167366028\n",
      "Epoch 13118: Training Loss: 0.08932400743166606 Validation Loss: 0.7761635184288025\n",
      "Epoch 13119: Training Loss: 0.08918163925409317 Validation Loss: 0.7762754559516907\n",
      "Epoch 13120: Training Loss: 0.08943675955136617 Validation Loss: 0.7757701277732849\n",
      "Epoch 13121: Training Loss: 0.08874692519505818 Validation Loss: 0.7762104868888855\n",
      "Epoch 13122: Training Loss: 0.08916226277748744 Validation Loss: 0.776107132434845\n",
      "Epoch 13123: Training Loss: 0.08866864442825317 Validation Loss: 0.7760012149810791\n",
      "Epoch 13124: Training Loss: 0.08906174078583717 Validation Loss: 0.7762767672538757\n",
      "Epoch 13125: Training Loss: 0.08888643731673558 Validation Loss: 0.7765393257141113\n",
      "Epoch 13126: Training Loss: 0.08903823047876358 Validation Loss: 0.776546835899353\n",
      "Epoch 13127: Training Loss: 0.08849270641803741 Validation Loss: 0.7762213349342346\n",
      "Epoch 13128: Training Loss: 0.08885154624780019 Validation Loss: 0.7760225534439087\n",
      "Epoch 13129: Training Loss: 0.08882968872785568 Validation Loss: 0.775698184967041\n",
      "Epoch 13130: Training Loss: 0.08914417028427124 Validation Loss: 0.7757259011268616\n",
      "Epoch 13131: Training Loss: 0.08943664034207661 Validation Loss: 0.7756664156913757\n",
      "Epoch 13132: Training Loss: 0.08894208073616028 Validation Loss: 0.7763331532478333\n",
      "Epoch 13133: Training Loss: 0.08956901480754216 Validation Loss: 0.7759914398193359\n",
      "Epoch 13134: Training Loss: 0.08917531371116638 Validation Loss: 0.775457501411438\n",
      "Epoch 13135: Training Loss: 0.08875249077876408 Validation Loss: 0.7758499979972839\n",
      "Epoch 13136: Training Loss: 0.0882446418205897 Validation Loss: 0.776283860206604\n",
      "Epoch 13137: Training Loss: 0.08891939868529637 Validation Loss: 0.7762307524681091\n",
      "Epoch 13138: Training Loss: 0.08876160780588786 Validation Loss: 0.7758435606956482\n",
      "Epoch 13139: Training Loss: 0.08905503898859024 Validation Loss: 0.7762410044670105\n",
      "Epoch 13140: Training Loss: 0.08830262472232182 Validation Loss: 0.7759953141212463\n",
      "Epoch 13141: Training Loss: 0.08938989291588466 Validation Loss: 0.776151180267334\n",
      "Epoch 13142: Training Loss: 0.089392751455307 Validation Loss: 0.7759410738945007\n",
      "Epoch 13143: Training Loss: 0.08885980894168218 Validation Loss: 0.7765394449234009\n",
      "Epoch 13144: Training Loss: 0.08841123183568318 Validation Loss: 0.7763839960098267\n",
      "Epoch 13145: Training Loss: 0.08868767321109772 Validation Loss: 0.7764767408370972\n",
      "Epoch 13146: Training Loss: 0.08865842968225479 Validation Loss: 0.776530385017395\n",
      "Epoch 13147: Training Loss: 0.08862785498301189 Validation Loss: 0.7764493227005005\n",
      "Epoch 13148: Training Loss: 0.08948665112257004 Validation Loss: 0.7761451005935669\n",
      "Epoch 13149: Training Loss: 0.08815680195887883 Validation Loss: 0.7758731245994568\n",
      "Epoch 13150: Training Loss: 0.08916662136713664 Validation Loss: 0.7759466171264648\n",
      "Epoch 13151: Training Loss: 0.08900552739699681 Validation Loss: 0.7764975428581238\n",
      "Epoch 13152: Training Loss: 0.08876657237609227 Validation Loss: 0.7770599126815796\n",
      "Epoch 13153: Training Loss: 0.08938039342562358 Validation Loss: 0.776498556137085\n",
      "Epoch 13154: Training Loss: 0.08943194647630055 Validation Loss: 0.7761459350585938\n",
      "Epoch 13155: Training Loss: 0.08861381560564041 Validation Loss: 0.7760685682296753\n",
      "Epoch 13156: Training Loss: 0.08929867049058278 Validation Loss: 0.7757202386856079\n",
      "Epoch 13157: Training Loss: 0.09029650688171387 Validation Loss: 0.7762652039527893\n",
      "Epoch 13158: Training Loss: 0.08919846266508102 Validation Loss: 0.7766300439834595\n",
      "Epoch 13159: Training Loss: 0.08902574330568314 Validation Loss: 0.7762558460235596\n",
      "Epoch 13160: Training Loss: 0.08828484266996384 Validation Loss: 0.7762960195541382\n",
      "Epoch 13161: Training Loss: 0.08862105011940002 Validation Loss: 0.776013970375061\n",
      "Epoch 13162: Training Loss: 0.08824148774147034 Validation Loss: 0.776149570941925\n",
      "Epoch 13163: Training Loss: 0.08877849082152049 Validation Loss: 0.776061475276947\n",
      "Epoch 13164: Training Loss: 0.08876846730709076 Validation Loss: 0.7758498191833496\n",
      "Epoch 13165: Training Loss: 0.08871568739414215 Validation Loss: 0.7759360671043396\n",
      "Epoch 13166: Training Loss: 0.08865171174208324 Validation Loss: 0.7762551307678223\n",
      "Epoch 13167: Training Loss: 0.08891451358795166 Validation Loss: 0.7770093083381653\n",
      "Epoch 13168: Training Loss: 0.08862171570460002 Validation Loss: 0.7765620946884155\n",
      "Epoch 13169: Training Loss: 0.08871916184822719 Validation Loss: 0.7763674259185791\n",
      "Epoch 13170: Training Loss: 0.08882884432872136 Validation Loss: 0.7764197587966919\n",
      "Epoch 13171: Training Loss: 0.08847165107727051 Validation Loss: 0.7764582633972168\n",
      "Epoch 13172: Training Loss: 0.08833478142817815 Validation Loss: 0.7766750454902649\n",
      "Epoch 13173: Training Loss: 0.08863906313975652 Validation Loss: 0.7763648629188538\n",
      "Epoch 13174: Training Loss: 0.08855414887269338 Validation Loss: 0.7768173813819885\n",
      "Epoch 13175: Training Loss: 0.08855842302242915 Validation Loss: 0.7767517566680908\n",
      "Epoch 13176: Training Loss: 0.08824538936217625 Validation Loss: 0.7761443853378296\n",
      "Epoch 13177: Training Loss: 0.0886918157339096 Validation Loss: 0.7760369777679443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13178: Training Loss: 0.0887153148651123 Validation Loss: 0.7768555283546448\n",
      "Epoch 13179: Training Loss: 0.08866518487532933 Validation Loss: 0.7767966985702515\n",
      "Epoch 13180: Training Loss: 0.0904816339413325 Validation Loss: 0.7769013047218323\n",
      "Epoch 13181: Training Loss: 0.08864951878786087 Validation Loss: 0.776778519153595\n",
      "Epoch 13182: Training Loss: 0.08856464177370071 Validation Loss: 0.7762745022773743\n",
      "Epoch 13183: Training Loss: 0.08900839338699977 Validation Loss: 0.7762627601623535\n",
      "Epoch 13184: Training Loss: 0.088630477587382 Validation Loss: 0.7768529653549194\n",
      "Epoch 13185: Training Loss: 0.08846950282653172 Validation Loss: 0.7768963575363159\n",
      "Epoch 13186: Training Loss: 0.08852456013361613 Validation Loss: 0.7769317030906677\n",
      "Epoch 13187: Training Loss: 0.08830039948225021 Validation Loss: 0.7769075036048889\n",
      "Epoch 13188: Training Loss: 0.08850674331188202 Validation Loss: 0.7765678763389587\n",
      "Epoch 13189: Training Loss: 0.0885016346971194 Validation Loss: 0.7764784693717957\n",
      "Epoch 13190: Training Loss: 0.08845414966344833 Validation Loss: 0.7764829397201538\n",
      "Epoch 13191: Training Loss: 0.08913969745238622 Validation Loss: 0.7768914103507996\n",
      "Epoch 13192: Training Loss: 0.08834818750619888 Validation Loss: 0.7771784067153931\n",
      "Epoch 13193: Training Loss: 0.08874476204315822 Validation Loss: 0.7775354981422424\n",
      "Epoch 13194: Training Loss: 0.08861665676037471 Validation Loss: 0.7769706845283508\n",
      "Epoch 13195: Training Loss: 0.08862684667110443 Validation Loss: 0.7767190933227539\n",
      "Epoch 13196: Training Loss: 0.08879889796177547 Validation Loss: 0.7766394019126892\n",
      "Epoch 13197: Training Loss: 0.08807438611984253 Validation Loss: 0.7766782641410828\n",
      "Epoch 13198: Training Loss: 0.08832284063100815 Validation Loss: 0.7769471406936646\n",
      "Epoch 13199: Training Loss: 0.08910016467173894 Validation Loss: 0.776974618434906\n",
      "Epoch 13200: Training Loss: 0.08876217653354009 Validation Loss: 0.7772150635719299\n",
      "Epoch 13201: Training Loss: 0.0885232537984848 Validation Loss: 0.7769378423690796\n",
      "Epoch 13202: Training Loss: 0.08843714992205302 Validation Loss: 0.7775375843048096\n",
      "Epoch 13203: Training Loss: 0.08853676915168762 Validation Loss: 0.7774661779403687\n",
      "Epoch 13204: Training Loss: 0.08840437730153401 Validation Loss: 0.7771196961402893\n",
      "Epoch 13205: Training Loss: 0.08838640401760738 Validation Loss: 0.7765170931816101\n",
      "Epoch 13206: Training Loss: 0.08881845325231552 Validation Loss: 0.7764525413513184\n",
      "Epoch 13207: Training Loss: 0.08860057095686595 Validation Loss: 0.7764208316802979\n",
      "Epoch 13208: Training Loss: 0.08877039452393849 Validation Loss: 0.7768251299858093\n",
      "Epoch 13209: Training Loss: 0.08829052249590556 Validation Loss: 0.7772446870803833\n",
      "Epoch 13210: Training Loss: 0.08899575223525365 Validation Loss: 0.7772946357727051\n",
      "Epoch 13211: Training Loss: 0.08848956227302551 Validation Loss: 0.7773543000221252\n",
      "Epoch 13212: Training Loss: 0.08847968777020772 Validation Loss: 0.7772402167320251\n",
      "Epoch 13213: Training Loss: 0.08839535713195801 Validation Loss: 0.7766157984733582\n",
      "Epoch 13214: Training Loss: 0.08929737160603206 Validation Loss: 0.7764238119125366\n",
      "Epoch 13215: Training Loss: 0.08832921584447224 Validation Loss: 0.7765746712684631\n",
      "Epoch 13216: Training Loss: 0.08833477894465129 Validation Loss: 0.7771273851394653\n",
      "Epoch 13217: Training Loss: 0.08857756356398265 Validation Loss: 0.7775706648826599\n",
      "Epoch 13218: Training Loss: 0.0885889505346616 Validation Loss: 0.7775791883468628\n",
      "Epoch 13219: Training Loss: 0.08862722168366115 Validation Loss: 0.7763353586196899\n",
      "Epoch 13220: Training Loss: 0.08866836627324422 Validation Loss: 0.7765478491783142\n",
      "Epoch 13221: Training Loss: 0.08889736235141754 Validation Loss: 0.7768707871437073\n",
      "Epoch 13222: Training Loss: 0.08866024265686671 Validation Loss: 0.7769134044647217\n",
      "Epoch 13223: Training Loss: 0.0885968307654063 Validation Loss: 0.7770057320594788\n",
      "Epoch 13224: Training Loss: 0.0886370837688446 Validation Loss: 0.776225209236145\n",
      "Epoch 13225: Training Loss: 0.08857921014229457 Validation Loss: 0.7760725617408752\n",
      "Epoch 13226: Training Loss: 0.08882023890813191 Validation Loss: 0.7766371369361877\n",
      "Epoch 13227: Training Loss: 0.08828731874624889 Validation Loss: 0.7774028778076172\n",
      "Epoch 13228: Training Loss: 0.08853839834531148 Validation Loss: 0.7775600552558899\n",
      "Epoch 13229: Training Loss: 0.08837081740299861 Validation Loss: 0.7774895429611206\n",
      "Epoch 13230: Training Loss: 0.0882986734310786 Validation Loss: 0.7772057056427002\n",
      "Epoch 13231: Training Loss: 0.08831134686867396 Validation Loss: 0.7768929600715637\n",
      "Epoch 13232: Training Loss: 0.08875658611456554 Validation Loss: 0.7769889235496521\n",
      "Epoch 13233: Training Loss: 0.08887235323588054 Validation Loss: 0.77708500623703\n",
      "Epoch 13234: Training Loss: 0.08854381491740544 Validation Loss: 0.7771936058998108\n",
      "Epoch 13235: Training Loss: 0.08835126211245854 Validation Loss: 0.7770057916641235\n",
      "Epoch 13236: Training Loss: 0.08891524622837703 Validation Loss: 0.7769638895988464\n",
      "Epoch 13237: Training Loss: 0.08869606504837672 Validation Loss: 0.7770470976829529\n",
      "Epoch 13238: Training Loss: 0.08984717478354771 Validation Loss: 0.7770750522613525\n",
      "Epoch 13239: Training Loss: 0.08911561220884323 Validation Loss: 0.7776373028755188\n",
      "Epoch 13240: Training Loss: 0.08844601362943649 Validation Loss: 0.7776180505752563\n",
      "Epoch 13241: Training Loss: 0.08846572289864223 Validation Loss: 0.7775704860687256\n",
      "Epoch 13242: Training Loss: 0.08858675012985866 Validation Loss: 0.7772577404975891\n",
      "Epoch 13243: Training Loss: 0.08838578810294469 Validation Loss: 0.7767910957336426\n",
      "Epoch 13244: Training Loss: 0.08848648269971211 Validation Loss: 0.7766990661621094\n",
      "Epoch 13245: Training Loss: 0.08819953600565593 Validation Loss: 0.7773424983024597\n",
      "Epoch 13246: Training Loss: 0.0887165442109108 Validation Loss: 0.7774879932403564\n",
      "Epoch 13247: Training Loss: 0.08836825440327327 Validation Loss: 0.7779277563095093\n",
      "Epoch 13248: Training Loss: 0.08859891941150029 Validation Loss: 0.7779770493507385\n",
      "Epoch 13249: Training Loss: 0.08942037324110667 Validation Loss: 0.7774021625518799\n",
      "Epoch 13250: Training Loss: 0.08831373850504558 Validation Loss: 0.7769988775253296\n",
      "Epoch 13251: Training Loss: 0.08812683324019115 Validation Loss: 0.7767494320869446\n",
      "Epoch 13252: Training Loss: 0.08889508247375488 Validation Loss: 0.777093231678009\n",
      "Epoch 13253: Training Loss: 0.0887451320886612 Validation Loss: 0.7774470448493958\n",
      "Epoch 13254: Training Loss: 0.08849538614352544 Validation Loss: 0.7771205306053162\n",
      "Epoch 13255: Training Loss: 0.08836661279201508 Validation Loss: 0.7779929637908936\n",
      "Epoch 13256: Training Loss: 0.08878049502770106 Validation Loss: 0.778632640838623\n",
      "Epoch 13257: Training Loss: 0.0886309544245402 Validation Loss: 0.7780279517173767\n",
      "Epoch 13258: Training Loss: 0.08803335825602214 Validation Loss: 0.7777152061462402\n",
      "Epoch 13259: Training Loss: 0.08842069904009502 Validation Loss: 0.7773167490959167\n",
      "Epoch 13260: Training Loss: 0.08838303138812383 Validation Loss: 0.7773545384407043\n",
      "Epoch 13261: Training Loss: 0.08912651240825653 Validation Loss: 0.7777073383331299\n",
      "Epoch 13262: Training Loss: 0.08818189054727554 Validation Loss: 0.7778593897819519\n",
      "Epoch 13263: Training Loss: 0.08825925489266713 Validation Loss: 0.7775490283966064\n",
      "Epoch 13264: Training Loss: 0.08894812315702438 Validation Loss: 0.7776451706886292\n",
      "Epoch 13265: Training Loss: 0.08866167565186818 Validation Loss: 0.7775666117668152\n",
      "Epoch 13266: Training Loss: 0.08838385591904323 Validation Loss: 0.7778707146644592\n",
      "Epoch 13267: Training Loss: 0.08785801380872726 Validation Loss: 0.7781289219856262\n",
      "Epoch 13268: Training Loss: 0.08833901584148407 Validation Loss: 0.7782680988311768\n",
      "Epoch 13269: Training Loss: 0.08860632280508678 Validation Loss: 0.7773973345756531\n",
      "Epoch 13270: Training Loss: 0.08819455405076344 Validation Loss: 0.7773048877716064\n",
      "Epoch 13271: Training Loss: 0.08823053538799286 Validation Loss: 0.7771748304367065\n",
      "Epoch 13272: Training Loss: 0.0879921242594719 Validation Loss: 0.7773603200912476\n",
      "Epoch 13273: Training Loss: 0.08817267666260402 Validation Loss: 0.7778734564781189\n",
      "Epoch 13274: Training Loss: 0.08865163971980412 Validation Loss: 0.7778582572937012\n",
      "Epoch 13275: Training Loss: 0.0882888063788414 Validation Loss: 0.778005838394165\n",
      "Epoch 13276: Training Loss: 0.08908440421024959 Validation Loss: 0.7783305644989014\n",
      "Epoch 13277: Training Loss: 0.08839936554431915 Validation Loss: 0.7779409885406494\n",
      "Epoch 13278: Training Loss: 0.08846639841794968 Validation Loss: 0.7782711386680603\n",
      "Epoch 13279: Training Loss: 0.08823505540688832 Validation Loss: 0.7780318260192871\n",
      "Epoch 13280: Training Loss: 0.08810851971308391 Validation Loss: 0.7778245210647583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13281: Training Loss: 0.08819346378246944 Validation Loss: 0.7777862548828125\n",
      "Epoch 13282: Training Loss: 0.08825985838969548 Validation Loss: 0.777963399887085\n",
      "Epoch 13283: Training Loss: 0.08905727416276932 Validation Loss: 0.7781130075454712\n",
      "Epoch 13284: Training Loss: 0.08876634885867436 Validation Loss: 0.777846097946167\n",
      "Epoch 13285: Training Loss: 0.08803364882866542 Validation Loss: 0.777441680431366\n",
      "Epoch 13286: Training Loss: 0.0882103939851125 Validation Loss: 0.7775955200195312\n",
      "Epoch 13287: Training Loss: 0.08820505191882451 Validation Loss: 0.7772766947746277\n",
      "Epoch 13288: Training Loss: 0.08836262921492259 Validation Loss: 0.7774871587753296\n",
      "Epoch 13289: Training Loss: 0.08851644645134608 Validation Loss: 0.77757328748703\n",
      "Epoch 13290: Training Loss: 0.08862840384244919 Validation Loss: 0.7772248983383179\n",
      "Epoch 13291: Training Loss: 0.08806600918372472 Validation Loss: 0.7778231501579285\n",
      "Epoch 13292: Training Loss: 0.0881628692150116 Validation Loss: 0.7782052755355835\n",
      "Epoch 13293: Training Loss: 0.0883622740705808 Validation Loss: 0.777732789516449\n",
      "Epoch 13294: Training Loss: 0.08848818143208821 Validation Loss: 0.7774020433425903\n",
      "Epoch 13295: Training Loss: 0.08960604916016261 Validation Loss: 0.7775222659111023\n",
      "Epoch 13296: Training Loss: 0.08831924696763356 Validation Loss: 0.7775797843933105\n",
      "Epoch 13297: Training Loss: 0.08809889853000641 Validation Loss: 0.7778151631355286\n",
      "Epoch 13298: Training Loss: 0.0881256212790807 Validation Loss: 0.7778002619743347\n",
      "Epoch 13299: Training Loss: 0.08841953923304875 Validation Loss: 0.7777786254882812\n",
      "Epoch 13300: Training Loss: 0.08832511057456334 Validation Loss: 0.777764618396759\n",
      "Epoch 13301: Training Loss: 0.08784868816534679 Validation Loss: 0.7773575782775879\n",
      "Epoch 13302: Training Loss: 0.08828548590342204 Validation Loss: 0.7771206498146057\n",
      "Epoch 13303: Training Loss: 0.08825390537579854 Validation Loss: 0.7775956988334656\n",
      "Epoch 13304: Training Loss: 0.08808834105730057 Validation Loss: 0.7774074673652649\n",
      "Epoch 13305: Training Loss: 0.08829741179943085 Validation Loss: 0.7781955599784851\n",
      "Epoch 13306: Training Loss: 0.08829623460769653 Validation Loss: 0.7783303260803223\n",
      "Epoch 13307: Training Loss: 0.0880665456255277 Validation Loss: 0.778282880783081\n",
      "Epoch 13308: Training Loss: 0.08824141571919124 Validation Loss: 0.7779391407966614\n",
      "Epoch 13309: Training Loss: 0.08858772118886311 Validation Loss: 0.7778210043907166\n",
      "Epoch 13310: Training Loss: 0.08764131367206573 Validation Loss: 0.7775896191596985\n",
      "Epoch 13311: Training Loss: 0.0881216824054718 Validation Loss: 0.7778019905090332\n",
      "Epoch 13312: Training Loss: 0.0877952699859937 Validation Loss: 0.7780341506004333\n",
      "Epoch 13313: Training Loss: 0.08794780572255452 Validation Loss: 0.7782738208770752\n",
      "Epoch 13314: Training Loss: 0.08833202222983043 Validation Loss: 0.7777882218360901\n",
      "Epoch 13315: Training Loss: 0.08804292480150859 Validation Loss: 0.77785325050354\n",
      "Epoch 13316: Training Loss: 0.08855028450489044 Validation Loss: 0.7780137658119202\n",
      "Epoch 13317: Training Loss: 0.08836993326743443 Validation Loss: 0.7777789235115051\n",
      "Epoch 13318: Training Loss: 0.08791880557934444 Validation Loss: 0.7775980830192566\n",
      "Epoch 13319: Training Loss: 0.08815628786881764 Validation Loss: 0.7777277827262878\n",
      "Epoch 13320: Training Loss: 0.088169959684213 Validation Loss: 0.7781068086624146\n",
      "Epoch 13321: Training Loss: 0.08801735192537308 Validation Loss: 0.777794361114502\n",
      "Epoch 13322: Training Loss: 0.08834657818078995 Validation Loss: 0.7780489921569824\n",
      "Epoch 13323: Training Loss: 0.0880474125345548 Validation Loss: 0.7782501578330994\n",
      "Epoch 13324: Training Loss: 0.08822513371706009 Validation Loss: 0.7779452204704285\n",
      "Epoch 13325: Training Loss: 0.08831845720609029 Validation Loss: 0.7780323624610901\n",
      "Epoch 13326: Training Loss: 0.08784246693054835 Validation Loss: 0.778075635433197\n",
      "Epoch 13327: Training Loss: 0.08817745248476665 Validation Loss: 0.7776074409484863\n",
      "Epoch 13328: Training Loss: 0.0879630371928215 Validation Loss: 0.7778757214546204\n",
      "Epoch 13329: Training Loss: 0.08810067425171535 Validation Loss: 0.7782323360443115\n",
      "Epoch 13330: Training Loss: 0.08870738744735718 Validation Loss: 0.7778889536857605\n",
      "Epoch 13331: Training Loss: 0.08808361987272899 Validation Loss: 0.7781230807304382\n",
      "Epoch 13332: Training Loss: 0.08818388233582179 Validation Loss: 0.7778781652450562\n",
      "Epoch 13333: Training Loss: 0.08893019830187161 Validation Loss: 0.778049111366272\n",
      "Epoch 13334: Training Loss: 0.0880312646428744 Validation Loss: 0.7779420018196106\n",
      "Epoch 13335: Training Loss: 0.08792752027511597 Validation Loss: 0.7777833938598633\n",
      "Epoch 13336: Training Loss: 0.08792230238517125 Validation Loss: 0.7777600884437561\n",
      "Epoch 13337: Training Loss: 0.08805147806803386 Validation Loss: 0.778049647808075\n",
      "Epoch 13338: Training Loss: 0.0883027787009875 Validation Loss: 0.7778493165969849\n",
      "Epoch 13339: Training Loss: 0.08867719521125157 Validation Loss: 0.7776947021484375\n",
      "Epoch 13340: Training Loss: 0.08796082933743794 Validation Loss: 0.7777844071388245\n",
      "Epoch 13341: Training Loss: 0.08794635534286499 Validation Loss: 0.7775977849960327\n",
      "Epoch 13342: Training Loss: 0.08798369020223618 Validation Loss: 0.777911901473999\n",
      "Epoch 13343: Training Loss: 0.08812761555115382 Validation Loss: 0.7779660224914551\n",
      "Epoch 13344: Training Loss: 0.0880313515663147 Validation Loss: 0.7781329154968262\n",
      "Epoch 13345: Training Loss: 0.08837040513753891 Validation Loss: 0.7784599661827087\n",
      "Epoch 13346: Training Loss: 0.08813401311635971 Validation Loss: 0.7788828611373901\n",
      "Epoch 13347: Training Loss: 0.08824685215950012 Validation Loss: 0.7788291573524475\n",
      "Epoch 13348: Training Loss: 0.08811180790265401 Validation Loss: 0.778398871421814\n",
      "Epoch 13349: Training Loss: 0.08833589901526769 Validation Loss: 0.7780070304870605\n",
      "Epoch 13350: Training Loss: 0.08804494390885036 Validation Loss: 0.7778856754302979\n",
      "Epoch 13351: Training Loss: 0.08817428598801295 Validation Loss: 0.7779693007469177\n",
      "Epoch 13352: Training Loss: 0.0881046752134959 Validation Loss: 0.7781265377998352\n",
      "Epoch 13353: Training Loss: 0.08870306114355724 Validation Loss: 0.7781133651733398\n",
      "Epoch 13354: Training Loss: 0.08820152779420216 Validation Loss: 0.7789027690887451\n",
      "Epoch 13355: Training Loss: 0.088040791451931 Validation Loss: 0.7790758013725281\n",
      "Epoch 13356: Training Loss: 0.08848153054714203 Validation Loss: 0.7790915966033936\n",
      "Epoch 13357: Training Loss: 0.08810318509737651 Validation Loss: 0.7783742547035217\n",
      "Epoch 13358: Training Loss: 0.08805909256140391 Validation Loss: 0.7775870561599731\n",
      "Epoch 13359: Training Loss: 0.08861605077981949 Validation Loss: 0.7773318886756897\n",
      "Epoch 13360: Training Loss: 0.08815825482209523 Validation Loss: 0.7776004076004028\n",
      "Epoch 13361: Training Loss: 0.08829867839813232 Validation Loss: 0.7782341837882996\n",
      "Epoch 13362: Training Loss: 0.08784596870342891 Validation Loss: 0.7784125804901123\n",
      "Epoch 13363: Training Loss: 0.08805288126071294 Validation Loss: 0.7785576581954956\n",
      "Epoch 13364: Training Loss: 0.08793920775254567 Validation Loss: 0.7785115242004395\n",
      "Epoch 13365: Training Loss: 0.0878542239467303 Validation Loss: 0.7786796689033508\n",
      "Epoch 13366: Training Loss: 0.08821246773004532 Validation Loss: 0.7786774039268494\n",
      "Epoch 13367: Training Loss: 0.08755900214115779 Validation Loss: 0.7781033515930176\n",
      "Epoch 13368: Training Loss: 0.08873760451873143 Validation Loss: 0.7780605554580688\n",
      "Epoch 13369: Training Loss: 0.08787819494803746 Validation Loss: 0.7779661417007446\n",
      "Epoch 13370: Training Loss: 0.08829479664564133 Validation Loss: 0.7777479887008667\n",
      "Epoch 13371: Training Loss: 0.08797690272331238 Validation Loss: 0.778182327747345\n",
      "Epoch 13372: Training Loss: 0.08805612474679947 Validation Loss: 0.7784776091575623\n",
      "Epoch 13373: Training Loss: 0.08800179759661357 Validation Loss: 0.7785106897354126\n",
      "Epoch 13374: Training Loss: 0.08846533050139745 Validation Loss: 0.7782441973686218\n",
      "Epoch 13375: Training Loss: 0.087825708091259 Validation Loss: 0.7781315445899963\n",
      "Epoch 13376: Training Loss: 0.08866114169359207 Validation Loss: 0.7780864834785461\n",
      "Epoch 13377: Training Loss: 0.0884622832139333 Validation Loss: 0.7778521180152893\n",
      "Epoch 13378: Training Loss: 0.08788595348596573 Validation Loss: 0.7780791521072388\n",
      "Epoch 13379: Training Loss: 0.08791357278823853 Validation Loss: 0.7778939604759216\n",
      "Epoch 13380: Training Loss: 0.08829468737045924 Validation Loss: 0.7778793573379517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13381: Training Loss: 0.08861741423606873 Validation Loss: 0.7780303955078125\n",
      "Epoch 13382: Training Loss: 0.08805775394042333 Validation Loss: 0.7783498764038086\n",
      "Epoch 13383: Training Loss: 0.08803207675615947 Validation Loss: 0.7786634564399719\n",
      "Epoch 13384: Training Loss: 0.0891088272134463 Validation Loss: 0.7792238593101501\n",
      "Epoch 13385: Training Loss: 0.08835811167955399 Validation Loss: 0.7791244387626648\n",
      "Epoch 13386: Training Loss: 0.08793812493483226 Validation Loss: 0.7786074280738831\n",
      "Epoch 13387: Training Loss: 0.08792940527200699 Validation Loss: 0.7782982587814331\n",
      "Epoch 13388: Training Loss: 0.0875793347756068 Validation Loss: 0.7781277894973755\n",
      "Epoch 13389: Training Loss: 0.08773908267418544 Validation Loss: 0.7785674929618835\n",
      "Epoch 13390: Training Loss: 0.08831750601530075 Validation Loss: 0.7790634036064148\n",
      "Epoch 13391: Training Loss: 0.08824693908294041 Validation Loss: 0.7791818380355835\n",
      "Epoch 13392: Training Loss: 0.08811725427707036 Validation Loss: 0.7790386080741882\n",
      "Epoch 13393: Training Loss: 0.08791447927554448 Validation Loss: 0.778769850730896\n",
      "Epoch 13394: Training Loss: 0.08804730077584584 Validation Loss: 0.7787778377532959\n",
      "Epoch 13395: Training Loss: 0.08733312040567398 Validation Loss: 0.7783028483390808\n",
      "Epoch 13396: Training Loss: 0.08746131509542465 Validation Loss: 0.7787504196166992\n",
      "Epoch 13397: Training Loss: 0.0879505177338918 Validation Loss: 0.7789345979690552\n",
      "Epoch 13398: Training Loss: 0.08784923702478409 Validation Loss: 0.7789004445075989\n",
      "Epoch 13399: Training Loss: 0.08783022562662761 Validation Loss: 0.7789602875709534\n",
      "Epoch 13400: Training Loss: 0.08831546207269032 Validation Loss: 0.778456449508667\n",
      "Epoch 13401: Training Loss: 0.08806271106004715 Validation Loss: 0.7786750793457031\n",
      "Epoch 13402: Training Loss: 0.08779737104972203 Validation Loss: 0.7785546779632568\n",
      "Epoch 13403: Training Loss: 0.08770933002233505 Validation Loss: 0.7788745760917664\n",
      "Epoch 13404: Training Loss: 0.08794070780277252 Validation Loss: 0.7788362503051758\n",
      "Epoch 13405: Training Loss: 0.08959562828143437 Validation Loss: 0.7789101004600525\n",
      "Epoch 13406: Training Loss: 0.08828240881363551 Validation Loss: 0.7792319655418396\n",
      "Epoch 13407: Training Loss: 0.08768879622220993 Validation Loss: 0.7789587378501892\n",
      "Epoch 13408: Training Loss: 0.08761804550886154 Validation Loss: 0.7787160277366638\n",
      "Epoch 13409: Training Loss: 0.08826651175816853 Validation Loss: 0.778639018535614\n",
      "Epoch 13410: Training Loss: 0.08782880504926045 Validation Loss: 0.7790052890777588\n",
      "Epoch 13411: Training Loss: 0.0880608856678009 Validation Loss: 0.7785070538520813\n",
      "Epoch 13412: Training Loss: 0.08782029400269191 Validation Loss: 0.7785691022872925\n",
      "Epoch 13413: Training Loss: 0.08778638392686844 Validation Loss: 0.7783007621765137\n",
      "Epoch 13414: Training Loss: 0.08771144598722458 Validation Loss: 0.778193473815918\n",
      "Epoch 13415: Training Loss: 0.0878082886338234 Validation Loss: 0.7785161733627319\n",
      "Epoch 13416: Training Loss: 0.08778780202070872 Validation Loss: 0.7784681916236877\n",
      "Epoch 13417: Training Loss: 0.08767838776111603 Validation Loss: 0.7788182497024536\n",
      "Epoch 13418: Training Loss: 0.08819605658451717 Validation Loss: 0.7785144448280334\n",
      "Epoch 13419: Training Loss: 0.08791351070006688 Validation Loss: 0.7785694599151611\n",
      "Epoch 13420: Training Loss: 0.08775320897499721 Validation Loss: 0.7786731719970703\n",
      "Epoch 13421: Training Loss: 0.08810522655646007 Validation Loss: 0.7791851758956909\n",
      "Epoch 13422: Training Loss: 0.08779874940713246 Validation Loss: 0.7791948318481445\n",
      "Epoch 13423: Training Loss: 0.08818337072928746 Validation Loss: 0.7790740132331848\n",
      "Epoch 13424: Training Loss: 0.08794428656498592 Validation Loss: 0.778759241104126\n",
      "Epoch 13425: Training Loss: 0.08791307111581166 Validation Loss: 0.7785367965698242\n",
      "Epoch 13426: Training Loss: 0.08800329764684041 Validation Loss: 0.7781693339347839\n",
      "Epoch 13427: Training Loss: 0.08878632883230846 Validation Loss: 0.7784716486930847\n",
      "Epoch 13428: Training Loss: 0.08809264997641246 Validation Loss: 0.7787596583366394\n",
      "Epoch 13429: Training Loss: 0.08809029559294383 Validation Loss: 0.7790209054946899\n",
      "Epoch 13430: Training Loss: 0.08875709275404613 Validation Loss: 0.779608964920044\n",
      "Epoch 13431: Training Loss: 0.08813120424747467 Validation Loss: 0.7792448401451111\n",
      "Epoch 13432: Training Loss: 0.08786068111658096 Validation Loss: 0.7795314192771912\n",
      "Epoch 13433: Training Loss: 0.08761292944351833 Validation Loss: 0.7794941067695618\n",
      "Epoch 13434: Training Loss: 0.08755783985058467 Validation Loss: 0.7788033485412598\n",
      "Epoch 13435: Training Loss: 0.08799844980239868 Validation Loss: 0.7785377502441406\n",
      "Epoch 13436: Training Loss: 0.0881217395265897 Validation Loss: 0.7784183621406555\n",
      "Epoch 13437: Training Loss: 0.08744567632675171 Validation Loss: 0.7785918116569519\n",
      "Epoch 13438: Training Loss: 0.08798431605100632 Validation Loss: 0.7789979577064514\n",
      "Epoch 13439: Training Loss: 0.0883565644423167 Validation Loss: 0.778770387172699\n",
      "Epoch 13440: Training Loss: 0.08813755214214325 Validation Loss: 0.7787215709686279\n",
      "Epoch 13441: Training Loss: 0.08746538311243057 Validation Loss: 0.7788562178611755\n",
      "Epoch 13442: Training Loss: 0.08830168594916661 Validation Loss: 0.7789661288261414\n",
      "Epoch 13443: Training Loss: 0.08758339534203212 Validation Loss: 0.7792477011680603\n",
      "Epoch 13444: Training Loss: 0.08838188648223877 Validation Loss: 0.7794809341430664\n",
      "Epoch 13445: Training Loss: 0.08783053110043208 Validation Loss: 0.7787414193153381\n",
      "Epoch 13446: Training Loss: 0.08795568346977234 Validation Loss: 0.7788408398628235\n",
      "Epoch 13447: Training Loss: 0.08799702177445094 Validation Loss: 0.7782928347587585\n",
      "Epoch 13448: Training Loss: 0.08754981309175491 Validation Loss: 0.7780819535255432\n",
      "Epoch 13449: Training Loss: 0.08785003175338109 Validation Loss: 0.7781001329421997\n",
      "Epoch 13450: Training Loss: 0.08786362409591675 Validation Loss: 0.7788903117179871\n",
      "Epoch 13451: Training Loss: 0.08752188334862392 Validation Loss: 0.7793709635734558\n",
      "Epoch 13452: Training Loss: 0.08781906962394714 Validation Loss: 0.7795500159263611\n",
      "Epoch 13453: Training Loss: 0.08793477217356364 Validation Loss: 0.778976559638977\n",
      "Epoch 13454: Training Loss: 0.08769621948401134 Validation Loss: 0.7786269783973694\n",
      "Epoch 13455: Training Loss: 0.0876963883638382 Validation Loss: 0.7787519693374634\n",
      "Epoch 13456: Training Loss: 0.0877870445450147 Validation Loss: 0.7790808081626892\n",
      "Epoch 13457: Training Loss: 0.08691602200269699 Validation Loss: 0.7795546054840088\n",
      "Epoch 13458: Training Loss: 0.08791861434777577 Validation Loss: 0.779434084892273\n",
      "Epoch 13459: Training Loss: 0.08745447794596355 Validation Loss: 0.7797529697418213\n",
      "Epoch 13460: Training Loss: 0.08783703049023946 Validation Loss: 0.7795014381408691\n",
      "Epoch 13461: Training Loss: 0.08791259427865346 Validation Loss: 0.7795114517211914\n",
      "Epoch 13462: Training Loss: 0.08785971005757649 Validation Loss: 0.7793022990226746\n",
      "Epoch 13463: Training Loss: 0.08757554988066356 Validation Loss: 0.7789211869239807\n",
      "Epoch 13464: Training Loss: 0.08751214047273 Validation Loss: 0.778878390789032\n",
      "Epoch 13465: Training Loss: 0.08789243549108505 Validation Loss: 0.7787913084030151\n",
      "Epoch 13466: Training Loss: 0.08803465217351913 Validation Loss: 0.7788087129592896\n",
      "Epoch 13467: Training Loss: 0.08761343359947205 Validation Loss: 0.7794445157051086\n",
      "Epoch 13468: Training Loss: 0.08742297440767288 Validation Loss: 0.7794051766395569\n",
      "Epoch 13469: Training Loss: 0.08760688205560048 Validation Loss: 0.7798479199409485\n",
      "Epoch 13470: Training Loss: 0.08795984337727229 Validation Loss: 0.7795583009719849\n",
      "Epoch 13471: Training Loss: 0.08814178903897603 Validation Loss: 0.7788276672363281\n",
      "Epoch 13472: Training Loss: 0.08800340443849564 Validation Loss: 0.7790597677230835\n",
      "Epoch 13473: Training Loss: 0.08758091181516647 Validation Loss: 0.779072105884552\n",
      "Epoch 13474: Training Loss: 0.0883268266916275 Validation Loss: 0.7789165377616882\n",
      "Epoch 13475: Training Loss: 0.08799637357393901 Validation Loss: 0.7790307998657227\n",
      "Epoch 13476: Training Loss: 0.08762015402317047 Validation Loss: 0.7795215845108032\n",
      "Epoch 13477: Training Loss: 0.08749381949504216 Validation Loss: 0.7796445488929749\n",
      "Epoch 13478: Training Loss: 0.08737729986508687 Validation Loss: 0.7801674008369446\n",
      "Epoch 13479: Training Loss: 0.08785658329725266 Validation Loss: 0.7793757915496826\n",
      "Epoch 13480: Training Loss: 0.08782982329527537 Validation Loss: 0.7793232202529907\n",
      "Epoch 13481: Training Loss: 0.08727488418420155 Validation Loss: 0.7790043950080872\n",
      "Epoch 13482: Training Loss: 0.0881931260228157 Validation Loss: 0.779384970664978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13483: Training Loss: 0.08743931353092194 Validation Loss: 0.7796135544776917\n",
      "Epoch 13484: Training Loss: 0.08794723947842915 Validation Loss: 0.7796443104743958\n",
      "Epoch 13485: Training Loss: 0.0875972534219424 Validation Loss: 0.779206395149231\n",
      "Epoch 13486: Training Loss: 0.08759272346893947 Validation Loss: 0.779918909072876\n",
      "Epoch 13487: Training Loss: 0.0883855124314626 Validation Loss: 0.7797232866287231\n",
      "Epoch 13488: Training Loss: 0.08751380195220311 Validation Loss: 0.7793043851852417\n",
      "Epoch 13489: Training Loss: 0.08756549656391144 Validation Loss: 0.7793114185333252\n",
      "Epoch 13490: Training Loss: 0.08743142833312352 Validation Loss: 0.7792823314666748\n",
      "Epoch 13491: Training Loss: 0.08753829449415207 Validation Loss: 0.7794299721717834\n",
      "Epoch 13492: Training Loss: 0.0875485564271609 Validation Loss: 0.7797549366950989\n",
      "Epoch 13493: Training Loss: 0.08735838284095128 Validation Loss: 0.7802005410194397\n",
      "Epoch 13494: Training Loss: 0.08852651963631313 Validation Loss: 0.7804889678955078\n",
      "Epoch 13495: Training Loss: 0.0880168005824089 Validation Loss: 0.7797632813453674\n",
      "Epoch 13496: Training Loss: 0.0875431274374326 Validation Loss: 0.7798543572425842\n",
      "Epoch 13497: Training Loss: 0.0880544309814771 Validation Loss: 0.7797722220420837\n",
      "Epoch 13498: Training Loss: 0.08726453284422557 Validation Loss: 0.7800319790840149\n",
      "Epoch 13499: Training Loss: 0.08783099800348282 Validation Loss: 0.7793999910354614\n",
      "Epoch 13500: Training Loss: 0.087513933579127 Validation Loss: 0.7797472476959229\n",
      "Epoch 13501: Training Loss: 0.08757735292116801 Validation Loss: 0.7797122001647949\n",
      "Epoch 13502: Training Loss: 0.08718753854433696 Validation Loss: 0.7799963355064392\n",
      "Epoch 13503: Training Loss: 0.08727373431126277 Validation Loss: 0.7796640992164612\n",
      "Epoch 13504: Training Loss: 0.08770825465520223 Validation Loss: 0.7791313529014587\n",
      "Epoch 13505: Training Loss: 0.08727638671795528 Validation Loss: 0.7792968153953552\n",
      "Epoch 13506: Training Loss: 0.0877553125222524 Validation Loss: 0.7792544960975647\n",
      "Epoch 13507: Training Loss: 0.0886319230000178 Validation Loss: 0.7799098491668701\n",
      "Epoch 13508: Training Loss: 0.08869795749584834 Validation Loss: 0.7799448370933533\n",
      "Epoch 13509: Training Loss: 0.08764577408631642 Validation Loss: 0.7795972228050232\n",
      "Epoch 13510: Training Loss: 0.08771927158037822 Validation Loss: 0.7796379327774048\n",
      "Epoch 13511: Training Loss: 0.08755433311065038 Validation Loss: 0.7797207832336426\n",
      "Epoch 13512: Training Loss: 0.08722731967767079 Validation Loss: 0.7798870205879211\n",
      "Epoch 13513: Training Loss: 0.0877274548014005 Validation Loss: 0.7799628973007202\n",
      "Epoch 13514: Training Loss: 0.08804772545893987 Validation Loss: 0.7802101969718933\n",
      "Epoch 13515: Training Loss: 0.08702850093444188 Validation Loss: 0.7801187634468079\n",
      "Epoch 13516: Training Loss: 0.08752123266458511 Validation Loss: 0.7796775698661804\n",
      "Epoch 13517: Training Loss: 0.08863153556982677 Validation Loss: 0.7796385288238525\n",
      "Epoch 13518: Training Loss: 0.08745380491018295 Validation Loss: 0.7796883583068848\n",
      "Epoch 13519: Training Loss: 0.0877110833923022 Validation Loss: 0.7795454263687134\n",
      "Epoch 13520: Training Loss: 0.08746639142433803 Validation Loss: 0.7800043821334839\n",
      "Epoch 13521: Training Loss: 0.08740359793106715 Validation Loss: 0.7796437740325928\n",
      "Epoch 13522: Training Loss: 0.0872735505302747 Validation Loss: 0.7795283198356628\n",
      "Epoch 13523: Training Loss: 0.08766035735607147 Validation Loss: 0.7790113687515259\n",
      "Epoch 13524: Training Loss: 0.08727192133665085 Validation Loss: 0.779155433177948\n",
      "Epoch 13525: Training Loss: 0.08738294492165248 Validation Loss: 0.7795750498771667\n",
      "Epoch 13526: Training Loss: 0.08749502648909886 Validation Loss: 0.7797832489013672\n",
      "Epoch 13527: Training Loss: 0.0874469925959905 Validation Loss: 0.779694676399231\n",
      "Epoch 13528: Training Loss: 0.08789986620346706 Validation Loss: 0.7796605825424194\n",
      "Epoch 13529: Training Loss: 0.08754957467317581 Validation Loss: 0.7794035077095032\n",
      "Epoch 13530: Training Loss: 0.08739056438207626 Validation Loss: 0.7795929312705994\n",
      "Epoch 13531: Training Loss: 0.08689257750908534 Validation Loss: 0.7796814441680908\n",
      "Epoch 13532: Training Loss: 0.08750640104214351 Validation Loss: 0.7799885869026184\n",
      "Epoch 13533: Training Loss: 0.08702519039312999 Validation Loss: 0.7796787023544312\n",
      "Epoch 13534: Training Loss: 0.08721312135457993 Validation Loss: 0.7794587016105652\n",
      "Epoch 13535: Training Loss: 0.08755136281251907 Validation Loss: 0.7795278429985046\n",
      "Epoch 13536: Training Loss: 0.08786646276712418 Validation Loss: 0.7797585725784302\n",
      "Epoch 13537: Training Loss: 0.08751177787780762 Validation Loss: 0.7800570726394653\n",
      "Epoch 13538: Training Loss: 0.0874981905023257 Validation Loss: 0.7796502709388733\n",
      "Epoch 13539: Training Loss: 0.08705622951189677 Validation Loss: 0.7794333100318909\n",
      "Epoch 13540: Training Loss: 0.08798923095067342 Validation Loss: 0.7795976996421814\n",
      "Epoch 13541: Training Loss: 0.0875883474946022 Validation Loss: 0.7800283432006836\n",
      "Epoch 13542: Training Loss: 0.08745559056599934 Validation Loss: 0.7801839113235474\n",
      "Epoch 13543: Training Loss: 0.08718831340471904 Validation Loss: 0.7799617052078247\n",
      "Epoch 13544: Training Loss: 0.08734896530707677 Validation Loss: 0.7798150181770325\n",
      "Epoch 13545: Training Loss: 0.08751779049634933 Validation Loss: 0.7799282073974609\n",
      "Epoch 13546: Training Loss: 0.08777240663766861 Validation Loss: 0.7801699042320251\n",
      "Epoch 13547: Training Loss: 0.08691318954030673 Validation Loss: 0.7799491882324219\n",
      "Epoch 13548: Training Loss: 0.08733106404542923 Validation Loss: 0.77994704246521\n",
      "Epoch 13549: Training Loss: 0.0873892034093539 Validation Loss: 0.7801693677902222\n",
      "Epoch 13550: Training Loss: 0.08738359560569127 Validation Loss: 0.7798097133636475\n",
      "Epoch 13551: Training Loss: 0.08764605472485225 Validation Loss: 0.7796950340270996\n",
      "Epoch 13552: Training Loss: 0.08721212297677994 Validation Loss: 0.780056357383728\n",
      "Epoch 13553: Training Loss: 0.08756520847479503 Validation Loss: 0.7801294922828674\n",
      "Epoch 13554: Training Loss: 0.08734092116355896 Validation Loss: 0.7798440456390381\n",
      "Epoch 13555: Training Loss: 0.08732551584641139 Validation Loss: 0.7796427011489868\n",
      "Epoch 13556: Training Loss: 0.08719964077075322 Validation Loss: 0.7798227071762085\n",
      "Epoch 13557: Training Loss: 0.088031272093455 Validation Loss: 0.7802494168281555\n",
      "Epoch 13558: Training Loss: 0.08743006239334743 Validation Loss: 0.7806105017662048\n",
      "Epoch 13559: Training Loss: 0.08672199646631877 Validation Loss: 0.7803153395652771\n",
      "Epoch 13560: Training Loss: 0.08692183842261632 Validation Loss: 0.77989661693573\n",
      "Epoch 13561: Training Loss: 0.08711509654919307 Validation Loss: 0.7795559763908386\n",
      "Epoch 13562: Training Loss: 0.08735335369904836 Validation Loss: 0.7795896530151367\n",
      "Epoch 13563: Training Loss: 0.08727111419041951 Validation Loss: 0.7799128293991089\n",
      "Epoch 13564: Training Loss: 0.08767280479272206 Validation Loss: 0.7802488803863525\n",
      "Epoch 13565: Training Loss: 0.08747317145268123 Validation Loss: 0.7802644371986389\n",
      "Epoch 13566: Training Loss: 0.08789199342330296 Validation Loss: 0.7802225947380066\n",
      "Epoch 13567: Training Loss: 0.08788152287403743 Validation Loss: 0.7805465459823608\n",
      "Epoch 13568: Training Loss: 0.08733546237150829 Validation Loss: 0.779998242855072\n",
      "Epoch 13569: Training Loss: 0.08714441706736882 Validation Loss: 0.7799211144447327\n",
      "Epoch 13570: Training Loss: 0.08789976686239243 Validation Loss: 0.7799600958824158\n",
      "Epoch 13571: Training Loss: 0.08830370754003525 Validation Loss: 0.7802762389183044\n",
      "Epoch 13572: Training Loss: 0.08729613324006398 Validation Loss: 0.7801005244255066\n",
      "Epoch 13573: Training Loss: 0.08815761158863704 Validation Loss: 0.7799944877624512\n",
      "Epoch 13574: Training Loss: 0.08697102467219035 Validation Loss: 0.7803208231925964\n",
      "Epoch 13575: Training Loss: 0.08712053547302882 Validation Loss: 0.7802977561950684\n",
      "Epoch 13576: Training Loss: 0.08739209175109863 Validation Loss: 0.7805159687995911\n",
      "Epoch 13577: Training Loss: 0.08728593587875366 Validation Loss: 0.7803971171379089\n",
      "Epoch 13578: Training Loss: 0.08867189784844716 Validation Loss: 0.7800229787826538\n",
      "Epoch 13579: Training Loss: 0.08759445945421855 Validation Loss: 0.7797353267669678\n",
      "Epoch 13580: Training Loss: 0.08754963676134746 Validation Loss: 0.7799105048179626\n",
      "Epoch 13581: Training Loss: 0.08740777770678203 Validation Loss: 0.7802347540855408\n",
      "Epoch 13582: Training Loss: 0.08813321093718211 Validation Loss: 0.7803934812545776\n",
      "Epoch 13583: Training Loss: 0.08735654503107071 Validation Loss: 0.7804605960845947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13584: Training Loss: 0.08731610824664433 Validation Loss: 0.7801764607429504\n",
      "Epoch 13585: Training Loss: 0.08726028352975845 Validation Loss: 0.7801017165184021\n",
      "Epoch 13586: Training Loss: 0.087151899933815 Validation Loss: 0.7801754474639893\n",
      "Epoch 13587: Training Loss: 0.08789420872926712 Validation Loss: 0.7805832624435425\n",
      "Epoch 13588: Training Loss: 0.08763063947359721 Validation Loss: 0.7804678082466125\n",
      "Epoch 13589: Training Loss: 0.08714273323615392 Validation Loss: 0.7805141806602478\n",
      "Epoch 13590: Training Loss: 0.08712936689456303 Validation Loss: 0.7800634503364563\n",
      "Epoch 13591: Training Loss: 0.08740448951721191 Validation Loss: 0.7803015112876892\n",
      "Epoch 13592: Training Loss: 0.087135615448157 Validation Loss: 0.7801737189292908\n",
      "Epoch 13593: Training Loss: 0.0871292178829511 Validation Loss: 0.7802718877792358\n",
      "Epoch 13594: Training Loss: 0.08783769359191258 Validation Loss: 0.780866801738739\n",
      "Epoch 13595: Training Loss: 0.08702788750330608 Validation Loss: 0.7800460457801819\n",
      "Epoch 13596: Training Loss: 0.08850105106830597 Validation Loss: 0.7802397012710571\n",
      "Epoch 13597: Training Loss: 0.08728804190953572 Validation Loss: 0.7801976203918457\n",
      "Epoch 13598: Training Loss: 0.08828937510649364 Validation Loss: 0.7809473276138306\n",
      "Epoch 13599: Training Loss: 0.08707286914189656 Validation Loss: 0.7806276082992554\n",
      "Epoch 13600: Training Loss: 0.08783110231161118 Validation Loss: 0.781055748462677\n",
      "Epoch 13601: Training Loss: 0.08720168471336365 Validation Loss: 0.780964195728302\n",
      "Epoch 13602: Training Loss: 0.08754505465428035 Validation Loss: 0.7805518507957458\n",
      "Epoch 13603: Training Loss: 0.0870318462451299 Validation Loss: 0.780712366104126\n",
      "Epoch 13604: Training Loss: 0.08701770007610321 Validation Loss: 0.7805662155151367\n",
      "Epoch 13605: Training Loss: 0.08731100708246231 Validation Loss: 0.7807479500770569\n",
      "Epoch 13606: Training Loss: 0.08708716928958893 Validation Loss: 0.7807580828666687\n",
      "Epoch 13607: Training Loss: 0.08695913106203079 Validation Loss: 0.7809436321258545\n",
      "Epoch 13608: Training Loss: 0.08737070858478546 Validation Loss: 0.7807367444038391\n",
      "Epoch 13609: Training Loss: 0.08731959015130997 Validation Loss: 0.7805867195129395\n",
      "Epoch 13610: Training Loss: 0.08777560293674469 Validation Loss: 0.7806529998779297\n",
      "Epoch 13611: Training Loss: 0.08785127103328705 Validation Loss: 0.7801647782325745\n",
      "Epoch 13612: Training Loss: 0.08727518717447917 Validation Loss: 0.7804072499275208\n",
      "Epoch 13613: Training Loss: 0.08717685441176097 Validation Loss: 0.7806488871574402\n",
      "Epoch 13614: Training Loss: 0.08722188323736191 Validation Loss: 0.7807088494300842\n",
      "Epoch 13615: Training Loss: 0.08706867198149364 Validation Loss: 0.7807270884513855\n",
      "Epoch 13616: Training Loss: 0.08740915606419246 Validation Loss: 0.7806052565574646\n",
      "Epoch 13617: Training Loss: 0.08701713879903157 Validation Loss: 0.7803394794464111\n",
      "Epoch 13618: Training Loss: 0.08705175916353862 Validation Loss: 0.780326247215271\n",
      "Epoch 13619: Training Loss: 0.08693404247363408 Validation Loss: 0.7805353999137878\n",
      "Epoch 13620: Training Loss: 0.0865604504942894 Validation Loss: 0.7801163196563721\n",
      "Epoch 13621: Training Loss: 0.08697275817394257 Validation Loss: 0.780670702457428\n",
      "Epoch 13622: Training Loss: 0.0872723584373792 Validation Loss: 0.7809573411941528\n",
      "Epoch 13623: Training Loss: 0.08664105832576752 Validation Loss: 0.780789315700531\n",
      "Epoch 13624: Training Loss: 0.08695947627226512 Validation Loss: 0.7807641625404358\n",
      "Epoch 13625: Training Loss: 0.08717076480388641 Validation Loss: 0.7809141278266907\n",
      "Epoch 13626: Training Loss: 0.08722628653049469 Validation Loss: 0.7800748944282532\n",
      "Epoch 13627: Training Loss: 0.08769358446200688 Validation Loss: 0.7804341912269592\n",
      "Epoch 13628: Training Loss: 0.08762907981872559 Validation Loss: 0.780353844165802\n",
      "Epoch 13629: Training Loss: 0.08766449739535649 Validation Loss: 0.7805156707763672\n",
      "Epoch 13630: Training Loss: 0.08694508175055186 Validation Loss: 0.7808717489242554\n",
      "Epoch 13631: Training Loss: 0.08705607801675797 Validation Loss: 0.7811433672904968\n",
      "Epoch 13632: Training Loss: 0.08714895943800609 Validation Loss: 0.7805699706077576\n",
      "Epoch 13633: Training Loss: 0.08737795054912567 Validation Loss: 0.779958963394165\n",
      "Epoch 13634: Training Loss: 0.08734157681465149 Validation Loss: 0.779748260974884\n",
      "Epoch 13635: Training Loss: 0.08718956013520558 Validation Loss: 0.7803303599357605\n",
      "Epoch 13636: Training Loss: 0.08736454943815868 Validation Loss: 0.7803751230239868\n",
      "Epoch 13637: Training Loss: 0.08715767165025075 Validation Loss: 0.7806931138038635\n",
      "Epoch 13638: Training Loss: 0.08733004083236058 Validation Loss: 0.780935525894165\n",
      "Epoch 13639: Training Loss: 0.08704497416814168 Validation Loss: 0.7809528112411499\n",
      "Epoch 13640: Training Loss: 0.08730229487021764 Validation Loss: 0.7810108661651611\n",
      "Epoch 13641: Training Loss: 0.08691551287968953 Validation Loss: 0.7811031937599182\n",
      "Epoch 13642: Training Loss: 0.08716631432374318 Validation Loss: 0.7809533476829529\n",
      "Epoch 13643: Training Loss: 0.08721318592627843 Validation Loss: 0.7808970212936401\n",
      "Epoch 13644: Training Loss: 0.08706057320038478 Validation Loss: 0.781021237373352\n",
      "Epoch 13645: Training Loss: 0.08777141074339549 Validation Loss: 0.7809489369392395\n",
      "Epoch 13646: Training Loss: 0.08720119545857112 Validation Loss: 0.7807504534721375\n",
      "Epoch 13647: Training Loss: 0.0870344266295433 Validation Loss: 0.78072589635849\n",
      "Epoch 13648: Training Loss: 0.08691818763812383 Validation Loss: 0.7805768847465515\n",
      "Epoch 13649: Training Loss: 0.08647122979164124 Validation Loss: 0.7804864048957825\n",
      "Epoch 13650: Training Loss: 0.08745210369427998 Validation Loss: 0.7810206413269043\n",
      "Epoch 13651: Training Loss: 0.08725612113873164 Validation Loss: 0.780997097492218\n",
      "Epoch 13652: Training Loss: 0.08721834296981494 Validation Loss: 0.7815378904342651\n",
      "Epoch 13653: Training Loss: 0.0869945560892423 Validation Loss: 0.7822322845458984\n",
      "Epoch 13654: Training Loss: 0.08712183932463329 Validation Loss: 0.7825993299484253\n",
      "Epoch 13655: Training Loss: 0.08723387867212296 Validation Loss: 0.7816566228866577\n",
      "Epoch 13656: Training Loss: 0.08676433563232422 Validation Loss: 0.7807844877243042\n",
      "Epoch 13657: Training Loss: 0.08732418964306514 Validation Loss: 0.7804358601570129\n",
      "Epoch 13658: Training Loss: 0.0867028534412384 Validation Loss: 0.7804325819015503\n",
      "Epoch 13659: Training Loss: 0.08682625989119212 Validation Loss: 0.7810642719268799\n",
      "Epoch 13660: Training Loss: 0.08704657852649689 Validation Loss: 0.7809560894966125\n",
      "Epoch 13661: Training Loss: 0.08699772506952286 Validation Loss: 0.7808478474617004\n",
      "Epoch 13662: Training Loss: 0.08695644636948903 Validation Loss: 0.780938446521759\n",
      "Epoch 13663: Training Loss: 0.08713527272144954 Validation Loss: 0.7808473110198975\n",
      "Epoch 13664: Training Loss: 0.08709901819626491 Validation Loss: 0.7811770439147949\n",
      "Epoch 13665: Training Loss: 0.08683602760235469 Validation Loss: 0.7816829681396484\n",
      "Epoch 13666: Training Loss: 0.0871895228823026 Validation Loss: 0.7811927795410156\n",
      "Epoch 13667: Training Loss: 0.087259441614151 Validation Loss: 0.7816553115844727\n",
      "Epoch 13668: Training Loss: 0.086918406188488 Validation Loss: 0.7811997532844543\n",
      "Epoch 13669: Training Loss: 0.08702222009499867 Validation Loss: 0.7807248830795288\n",
      "Epoch 13670: Training Loss: 0.0870002880692482 Validation Loss: 0.7804632782936096\n",
      "Epoch 13671: Training Loss: 0.08662557353576024 Validation Loss: 0.7805778384208679\n",
      "Epoch 13672: Training Loss: 0.08705095201730728 Validation Loss: 0.7804335355758667\n",
      "Epoch 13673: Training Loss: 0.08673065900802612 Validation Loss: 0.7807341814041138\n",
      "Epoch 13674: Training Loss: 0.08695414910713832 Validation Loss: 0.781399130821228\n",
      "Epoch 13675: Training Loss: 0.08693849295377731 Validation Loss: 0.7819764018058777\n",
      "Epoch 13676: Training Loss: 0.08759212742249171 Validation Loss: 0.781744122505188\n",
      "Epoch 13677: Training Loss: 0.08711929619312286 Validation Loss: 0.7816681861877441\n",
      "Epoch 13678: Training Loss: 0.08675711353619893 Validation Loss: 0.7811163663864136\n",
      "Epoch 13679: Training Loss: 0.08690967907508214 Validation Loss: 0.7809863686561584\n",
      "Epoch 13680: Training Loss: 0.08697113146384557 Validation Loss: 0.7811152338981628\n",
      "Epoch 13681: Training Loss: 0.08737284441788991 Validation Loss: 0.7812780737876892\n",
      "Epoch 13682: Training Loss: 0.08709222078323364 Validation Loss: 0.7811312675476074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13683: Training Loss: 0.08671395480632782 Validation Loss: 0.7809789180755615\n",
      "Epoch 13684: Training Loss: 0.08694540709257126 Validation Loss: 0.7812639474868774\n",
      "Epoch 13685: Training Loss: 0.08704773833354314 Validation Loss: 0.7808760404586792\n",
      "Epoch 13686: Training Loss: 0.08658869316180547 Validation Loss: 0.7814949154853821\n",
      "Epoch 13687: Training Loss: 0.08694181591272354 Validation Loss: 0.7812420129776001\n",
      "Epoch 13688: Training Loss: 0.08704219510157903 Validation Loss: 0.780917763710022\n",
      "Epoch 13689: Training Loss: 0.08693741758664449 Validation Loss: 0.7805072665214539\n",
      "Epoch 13690: Training Loss: 0.08690820386012395 Validation Loss: 0.7807604670524597\n",
      "Epoch 13691: Training Loss: 0.08707206944624583 Validation Loss: 0.7813505530357361\n",
      "Epoch 13692: Training Loss: 0.08719094097614288 Validation Loss: 0.7811229228973389\n",
      "Epoch 13693: Training Loss: 0.08745213846365611 Validation Loss: 0.7806824445724487\n",
      "Epoch 13694: Training Loss: 0.0873270829518636 Validation Loss: 0.7809575200080872\n",
      "Epoch 13695: Training Loss: 0.08707056194543839 Validation Loss: 0.7811602354049683\n",
      "Epoch 13696: Training Loss: 0.08737764755884807 Validation Loss: 0.7810434103012085\n",
      "Epoch 13697: Training Loss: 0.08687628557284673 Validation Loss: 0.7814244627952576\n",
      "Epoch 13698: Training Loss: 0.08705855906009674 Validation Loss: 0.7816565632820129\n",
      "Epoch 13699: Training Loss: 0.08677411079406738 Validation Loss: 0.7814186215400696\n",
      "Epoch 13700: Training Loss: 0.08691529432932536 Validation Loss: 0.7809521555900574\n",
      "Epoch 13701: Training Loss: 0.08723426858584087 Validation Loss: 0.7808780074119568\n",
      "Epoch 13702: Training Loss: 0.08689112961292267 Validation Loss: 0.7807922959327698\n",
      "Epoch 13703: Training Loss: 0.08694483836491902 Validation Loss: 0.7806394100189209\n",
      "Epoch 13704: Training Loss: 0.08693314592043559 Validation Loss: 0.7812474966049194\n",
      "Epoch 13705: Training Loss: 0.08705192555983861 Validation Loss: 0.7817704081535339\n",
      "Epoch 13706: Training Loss: 0.087076835334301 Validation Loss: 0.7819404602050781\n",
      "Epoch 13707: Training Loss: 0.08679274717966716 Validation Loss: 0.7812916040420532\n",
      "Epoch 13708: Training Loss: 0.0871807982524236 Validation Loss: 0.7807236909866333\n",
      "Epoch 13709: Training Loss: 0.08712830146153767 Validation Loss: 0.78084397315979\n",
      "Epoch 13710: Training Loss: 0.08715311686197917 Validation Loss: 0.780677318572998\n",
      "Epoch 13711: Training Loss: 0.08690813183784485 Validation Loss: 0.7815189361572266\n",
      "Epoch 13712: Training Loss: 0.0867576797803243 Validation Loss: 0.7816630005836487\n",
      "Epoch 13713: Training Loss: 0.0867819885412852 Validation Loss: 0.7817047834396362\n",
      "Epoch 13714: Training Loss: 0.08668794731299083 Validation Loss: 0.7813535332679749\n",
      "Epoch 13715: Training Loss: 0.08681876957416534 Validation Loss: 0.7808886766433716\n",
      "Epoch 13716: Training Loss: 0.08681267748276393 Validation Loss: 0.7814585566520691\n",
      "Epoch 13717: Training Loss: 0.0869284172852834 Validation Loss: 0.7811701893806458\n",
      "Epoch 13718: Training Loss: 0.0870269387960434 Validation Loss: 0.7814175486564636\n",
      "Epoch 13719: Training Loss: 0.08772330482800801 Validation Loss: 0.7810566425323486\n",
      "Epoch 13720: Training Loss: 0.08682807783285777 Validation Loss: 0.7817028760910034\n",
      "Epoch 13721: Training Loss: 0.08695013696948688 Validation Loss: 0.7814518809318542\n",
      "Epoch 13722: Training Loss: 0.08694719771544139 Validation Loss: 0.7817217707633972\n",
      "Epoch 13723: Training Loss: 0.08731747666994731 Validation Loss: 0.7812129259109497\n",
      "Epoch 13724: Training Loss: 0.08686894675095876 Validation Loss: 0.7811834216117859\n",
      "Epoch 13725: Training Loss: 0.086423359811306 Validation Loss: 0.7813688516616821\n",
      "Epoch 13726: Training Loss: 0.0873586858312289 Validation Loss: 0.7824381589889526\n",
      "Epoch 13727: Training Loss: 0.08699360986550649 Validation Loss: 0.7823716402053833\n",
      "Epoch 13728: Training Loss: 0.08650058259566624 Validation Loss: 0.7812764048576355\n",
      "Epoch 13729: Training Loss: 0.08687994629144669 Validation Loss: 0.7807856798171997\n",
      "Epoch 13730: Training Loss: 0.08688550690809886 Validation Loss: 0.7808945775032043\n",
      "Epoch 13731: Training Loss: 0.08705472697814305 Validation Loss: 0.7812045216560364\n",
      "Epoch 13732: Training Loss: 0.08702532698710759 Validation Loss: 0.7810154557228088\n",
      "Epoch 13733: Training Loss: 0.08681908746560414 Validation Loss: 0.781769871711731\n",
      "Epoch 13734: Training Loss: 0.08696647236744563 Validation Loss: 0.78208988904953\n",
      "Epoch 13735: Training Loss: 0.08674572159846623 Validation Loss: 0.7816653847694397\n",
      "Epoch 13736: Training Loss: 0.08714003364245097 Validation Loss: 0.7814961671829224\n",
      "Epoch 13737: Training Loss: 0.08667707691589992 Validation Loss: 0.7813639640808105\n",
      "Epoch 13738: Training Loss: 0.0872449278831482 Validation Loss: 0.7812144160270691\n",
      "Epoch 13739: Training Loss: 0.08677275975545247 Validation Loss: 0.781448245048523\n",
      "Epoch 13740: Training Loss: 0.08687925587097804 Validation Loss: 0.781905472278595\n",
      "Epoch 13741: Training Loss: 0.08707229793071747 Validation Loss: 0.7826061844825745\n",
      "Epoch 13742: Training Loss: 0.08713515847921371 Validation Loss: 0.7821764349937439\n",
      "Epoch 13743: Training Loss: 0.0872703269124031 Validation Loss: 0.781747043132782\n",
      "Epoch 13744: Training Loss: 0.0867269014318784 Validation Loss: 0.7815349102020264\n",
      "Epoch 13745: Training Loss: 0.0865739310781161 Validation Loss: 0.7811114192008972\n",
      "Epoch 13746: Training Loss: 0.08716714382171631 Validation Loss: 0.7814460396766663\n",
      "Epoch 13747: Training Loss: 0.08710867414871852 Validation Loss: 0.7815254330635071\n",
      "Epoch 13748: Training Loss: 0.0869244635105133 Validation Loss: 0.7819560170173645\n",
      "Epoch 13749: Training Loss: 0.08650961269934972 Validation Loss: 0.7820978760719299\n",
      "Epoch 13750: Training Loss: 0.08680586020151775 Validation Loss: 0.782367467880249\n",
      "Epoch 13751: Training Loss: 0.08743681261936824 Validation Loss: 0.7823757529258728\n",
      "Epoch 13752: Training Loss: 0.08673680822054546 Validation Loss: 0.7822912931442261\n",
      "Epoch 13753: Training Loss: 0.08725992341836293 Validation Loss: 0.7822316884994507\n",
      "Epoch 13754: Training Loss: 0.08641998966534932 Validation Loss: 0.7822616100311279\n",
      "Epoch 13755: Training Loss: 0.08665412912766139 Validation Loss: 0.7818651795387268\n",
      "Epoch 13756: Training Loss: 0.08636437853177388 Validation Loss: 0.781575620174408\n",
      "Epoch 13757: Training Loss: 0.08752571294705074 Validation Loss: 0.781630277633667\n",
      "Epoch 13758: Training Loss: 0.08669757843017578 Validation Loss: 0.7821350693702698\n",
      "Epoch 13759: Training Loss: 0.08666629840930302 Validation Loss: 0.7823242545127869\n",
      "Epoch 13760: Training Loss: 0.08678240825732549 Validation Loss: 0.7817453145980835\n",
      "Epoch 13761: Training Loss: 0.08673330396413803 Validation Loss: 0.7818626761436462\n",
      "Epoch 13762: Training Loss: 0.0878003587325414 Validation Loss: 0.782089352607727\n",
      "Epoch 13763: Training Loss: 0.08693200846513112 Validation Loss: 0.7823131084442139\n",
      "Epoch 13764: Training Loss: 0.08624021708965302 Validation Loss: 0.7822133898735046\n",
      "Epoch 13765: Training Loss: 0.08698680996894836 Validation Loss: 0.7820078134536743\n",
      "Epoch 13766: Training Loss: 0.08770014842351277 Validation Loss: 0.7814781069755554\n",
      "Epoch 13767: Training Loss: 0.0868981530268987 Validation Loss: 0.7817603945732117\n",
      "Epoch 13768: Training Loss: 0.08781720449527104 Validation Loss: 0.7815653681755066\n",
      "Epoch 13769: Training Loss: 0.08720311522483826 Validation Loss: 0.7813931703567505\n",
      "Epoch 13770: Training Loss: 0.08677770445744197 Validation Loss: 0.7818008065223694\n",
      "Epoch 13771: Training Loss: 0.08693355073531468 Validation Loss: 0.7821328043937683\n",
      "Epoch 13772: Training Loss: 0.0866708904504776 Validation Loss: 0.7821525931358337\n",
      "Epoch 13773: Training Loss: 0.086790531873703 Validation Loss: 0.7820786833763123\n",
      "Epoch 13774: Training Loss: 0.08649919182062149 Validation Loss: 0.7820211052894592\n",
      "Epoch 13775: Training Loss: 0.08618103712797165 Validation Loss: 0.7818102836608887\n",
      "Epoch 13776: Training Loss: 0.08700920144716899 Validation Loss: 0.7810270190238953\n",
      "Epoch 13777: Training Loss: 0.08655312409003575 Validation Loss: 0.7814525961875916\n",
      "Epoch 13778: Training Loss: 0.08655823270479839 Validation Loss: 0.7816894054412842\n",
      "Epoch 13779: Training Loss: 0.08734920869270961 Validation Loss: 0.7820197939872742\n",
      "Epoch 13780: Training Loss: 0.08655310422182083 Validation Loss: 0.7822165489196777\n",
      "Epoch 13781: Training Loss: 0.08705719808737437 Validation Loss: 0.7819586396217346\n",
      "Epoch 13782: Training Loss: 0.08668810377518336 Validation Loss: 0.7815930247306824\n",
      "Epoch 13783: Training Loss: 0.08641629666090012 Validation Loss: 0.7816703915596008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13784: Training Loss: 0.08667096743981044 Validation Loss: 0.782471239566803\n",
      "Epoch 13785: Training Loss: 0.08673466245333354 Validation Loss: 0.7828529477119446\n",
      "Epoch 13786: Training Loss: 0.08648584534724553 Validation Loss: 0.7827216982841492\n",
      "Epoch 13787: Training Loss: 0.08683069795370102 Validation Loss: 0.7823792099952698\n",
      "Epoch 13788: Training Loss: 0.08651240915060043 Validation Loss: 0.7819798588752747\n",
      "Epoch 13789: Training Loss: 0.08685068041086197 Validation Loss: 0.7817729711532593\n",
      "Epoch 13790: Training Loss: 0.08750999222199123 Validation Loss: 0.7815631628036499\n",
      "Epoch 13791: Training Loss: 0.08656023442745209 Validation Loss: 0.7821358442306519\n",
      "Epoch 13792: Training Loss: 0.08726235975821812 Validation Loss: 0.7827873229980469\n",
      "Epoch 13793: Training Loss: 0.08712351322174072 Validation Loss: 0.7823460698127747\n",
      "Epoch 13794: Training Loss: 0.08696337292591731 Validation Loss: 0.7826601266860962\n",
      "Epoch 13795: Training Loss: 0.08652689059575398 Validation Loss: 0.7825942039489746\n",
      "Epoch 13796: Training Loss: 0.08689375718434651 Validation Loss: 0.7824618816375732\n",
      "Epoch 13797: Training Loss: 0.08616873621940613 Validation Loss: 0.7821287512779236\n",
      "Epoch 13798: Training Loss: 0.0866488590836525 Validation Loss: 0.7821839451789856\n",
      "Epoch 13799: Training Loss: 0.08661073197921117 Validation Loss: 0.781994104385376\n",
      "Epoch 13800: Training Loss: 0.08724753806988399 Validation Loss: 0.7827308773994446\n",
      "Epoch 13801: Training Loss: 0.08658677836259206 Validation Loss: 0.7828287482261658\n",
      "Epoch 13802: Training Loss: 0.08689551303784053 Validation Loss: 0.7825837731361389\n",
      "Epoch 13803: Training Loss: 0.08756861090660095 Validation Loss: 0.782253086566925\n",
      "Epoch 13804: Training Loss: 0.08652210732301076 Validation Loss: 0.7821029424667358\n",
      "Epoch 13805: Training Loss: 0.08634380499521892 Validation Loss: 0.781826913356781\n",
      "Epoch 13806: Training Loss: 0.086527186135451 Validation Loss: 0.7817410826683044\n",
      "Epoch 13807: Training Loss: 0.08720957736174266 Validation Loss: 0.7819383144378662\n",
      "Epoch 13808: Training Loss: 0.08650004615386327 Validation Loss: 0.7820213437080383\n",
      "Epoch 13809: Training Loss: 0.0868593876560529 Validation Loss: 0.7824538350105286\n",
      "Epoch 13810: Training Loss: 0.08647289127111435 Validation Loss: 0.7827108502388\n",
      "Epoch 13811: Training Loss: 0.08713231359918912 Validation Loss: 0.782669723033905\n",
      "Epoch 13812: Training Loss: 0.08661612619956334 Validation Loss: 0.7822999954223633\n",
      "Epoch 13813: Training Loss: 0.08665923525889714 Validation Loss: 0.7819064855575562\n",
      "Epoch 13814: Training Loss: 0.08827079956730206 Validation Loss: 0.7815874814987183\n",
      "Epoch 13815: Training Loss: 0.08663804580767949 Validation Loss: 0.7821789979934692\n",
      "Epoch 13816: Training Loss: 0.08673376341660817 Validation Loss: 0.7824898362159729\n",
      "Epoch 13817: Training Loss: 0.08674321075280507 Validation Loss: 0.7829391956329346\n",
      "Epoch 13818: Training Loss: 0.08647298067808151 Validation Loss: 0.7824729681015015\n",
      "Epoch 13819: Training Loss: 0.08626045286655426 Validation Loss: 0.7823783159255981\n",
      "Epoch 13820: Training Loss: 0.08663229644298553 Validation Loss: 0.782308042049408\n",
      "Epoch 13821: Training Loss: 0.08739989002545674 Validation Loss: 0.7820758819580078\n",
      "Epoch 13822: Training Loss: 0.08631034443775813 Validation Loss: 0.7825785875320435\n",
      "Epoch 13823: Training Loss: 0.08656280239423116 Validation Loss: 0.7827288508415222\n",
      "Epoch 13824: Training Loss: 0.08712585767110188 Validation Loss: 0.7829718589782715\n",
      "Epoch 13825: Training Loss: 0.08708340053757031 Validation Loss: 0.782616376876831\n",
      "Epoch 13826: Training Loss: 0.08624286949634552 Validation Loss: 0.7822388410568237\n",
      "Epoch 13827: Training Loss: 0.08634727944930394 Validation Loss: 0.7823466062545776\n",
      "Epoch 13828: Training Loss: 0.08642647415399551 Validation Loss: 0.7824720740318298\n",
      "Epoch 13829: Training Loss: 0.0862321878472964 Validation Loss: 0.7819088697433472\n",
      "Epoch 13830: Training Loss: 0.08667563895384471 Validation Loss: 0.7816430926322937\n",
      "Epoch 13831: Training Loss: 0.08633080373207729 Validation Loss: 0.7824090719223022\n",
      "Epoch 13832: Training Loss: 0.08717906226714452 Validation Loss: 0.7828842997550964\n",
      "Epoch 13833: Training Loss: 0.0862956369916598 Validation Loss: 0.7831069231033325\n",
      "Epoch 13834: Training Loss: 0.08621133615573247 Validation Loss: 0.7829216718673706\n",
      "Epoch 13835: Training Loss: 0.08683860550324123 Validation Loss: 0.7829355001449585\n",
      "Epoch 13836: Training Loss: 0.08654153098662694 Validation Loss: 0.7826869487762451\n",
      "Epoch 13837: Training Loss: 0.08588057259718578 Validation Loss: 0.7826993465423584\n",
      "Epoch 13838: Training Loss: 0.08632942040761311 Validation Loss: 0.7822041511535645\n",
      "Epoch 13839: Training Loss: 0.0863455558816592 Validation Loss: 0.7821791768074036\n",
      "Epoch 13840: Training Loss: 0.08667844037214915 Validation Loss: 0.782069742679596\n",
      "Epoch 13841: Training Loss: 0.08630938331286113 Validation Loss: 0.7826040983200073\n",
      "Epoch 13842: Training Loss: 0.08661555995543797 Validation Loss: 0.7829487323760986\n",
      "Epoch 13843: Training Loss: 0.08657835920651753 Validation Loss: 0.7825393080711365\n",
      "Epoch 13844: Training Loss: 0.08679374555746715 Validation Loss: 0.7828230857849121\n",
      "Epoch 13845: Training Loss: 0.08641335368156433 Validation Loss: 0.782796323299408\n",
      "Epoch 13846: Training Loss: 0.0865910400946935 Validation Loss: 0.7821870446205139\n",
      "Epoch 13847: Training Loss: 0.08651069800059001 Validation Loss: 0.7827767729759216\n",
      "Epoch 13848: Training Loss: 0.0867995669444402 Validation Loss: 0.7825393080711365\n",
      "Epoch 13849: Training Loss: 0.0863976925611496 Validation Loss: 0.7825033664703369\n",
      "Epoch 13850: Training Loss: 0.08679957687854767 Validation Loss: 0.7821917533874512\n",
      "Epoch 13851: Training Loss: 0.08657858769098918 Validation Loss: 0.782295823097229\n",
      "Epoch 13852: Training Loss: 0.08671452353398006 Validation Loss: 0.7831428647041321\n",
      "Epoch 13853: Training Loss: 0.08639789372682571 Validation Loss: 0.7833936810493469\n",
      "Epoch 13854: Training Loss: 0.08587867021560669 Validation Loss: 0.7833449244499207\n",
      "Epoch 13855: Training Loss: 0.08572906255722046 Validation Loss: 0.782711386680603\n",
      "Epoch 13856: Training Loss: 0.08663087338209152 Validation Loss: 0.7826687097549438\n",
      "Epoch 13857: Training Loss: 0.08639066169659297 Validation Loss: 0.7824872136116028\n",
      "Epoch 13858: Training Loss: 0.08651998887459438 Validation Loss: 0.7825014591217041\n",
      "Epoch 13859: Training Loss: 0.08668381969134013 Validation Loss: 0.782069206237793\n",
      "Epoch 13860: Training Loss: 0.08638249337673187 Validation Loss: 0.7825822830200195\n",
      "Epoch 13861: Training Loss: 0.08728735893964767 Validation Loss: 0.782647967338562\n",
      "Epoch 13862: Training Loss: 0.08639105409383774 Validation Loss: 0.7827984690666199\n",
      "Epoch 13863: Training Loss: 0.0864936535557111 Validation Loss: 0.7829339504241943\n",
      "Epoch 13864: Training Loss: 0.08686160792907079 Validation Loss: 0.7828542590141296\n",
      "Epoch 13865: Training Loss: 0.08605567614237468 Validation Loss: 0.782500147819519\n",
      "Epoch 13866: Training Loss: 0.0864840845266978 Validation Loss: 0.7823200821876526\n",
      "Epoch 13867: Training Loss: 0.08635254949331284 Validation Loss: 0.7823353409767151\n",
      "Epoch 13868: Training Loss: 0.08654969930648804 Validation Loss: 0.7830202579498291\n",
      "Epoch 13869: Training Loss: 0.08652343104283015 Validation Loss: 0.7830515503883362\n",
      "Epoch 13870: Training Loss: 0.0868245263894399 Validation Loss: 0.7830573320388794\n",
      "Epoch 13871: Training Loss: 0.08616804579893748 Validation Loss: 0.7831631898880005\n",
      "Epoch 13872: Training Loss: 0.08612112452586491 Validation Loss: 0.7829139828681946\n",
      "Epoch 13873: Training Loss: 0.08637767285108566 Validation Loss: 0.7826366424560547\n",
      "Epoch 13874: Training Loss: 0.08646018554766972 Validation Loss: 0.7825981974601746\n",
      "Epoch 13875: Training Loss: 0.0865947852532069 Validation Loss: 0.782664954662323\n",
      "Epoch 13876: Training Loss: 0.08668613682190578 Validation Loss: 0.7827903628349304\n",
      "Epoch 13877: Training Loss: 0.08644704520702362 Validation Loss: 0.7833252549171448\n",
      "Epoch 13878: Training Loss: 0.08636062343915303 Validation Loss: 0.7832815051078796\n",
      "Epoch 13879: Training Loss: 0.08661558479070663 Validation Loss: 0.783154308795929\n",
      "Epoch 13880: Training Loss: 0.08620635668436687 Validation Loss: 0.7832143902778625\n",
      "Epoch 13881: Training Loss: 0.08706655104955037 Validation Loss: 0.7831477522850037\n",
      "Epoch 13882: Training Loss: 0.08615398903687795 Validation Loss: 0.7829355001449585\n",
      "Epoch 13883: Training Loss: 0.08659766366084416 Validation Loss: 0.7835785150527954\n",
      "Epoch 13884: Training Loss: 0.08690459529558818 Validation Loss: 0.7833809852600098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13885: Training Loss: 0.08646818747123082 Validation Loss: 0.7831662893295288\n",
      "Epoch 13886: Training Loss: 0.0865322748819987 Validation Loss: 0.7833758592605591\n",
      "Epoch 13887: Training Loss: 0.08666228502988815 Validation Loss: 0.7830836176872253\n",
      "Epoch 13888: Training Loss: 0.0864575629432996 Validation Loss: 0.7829862236976624\n",
      "Epoch 13889: Training Loss: 0.08699454615513484 Validation Loss: 0.7828769087791443\n",
      "Epoch 13890: Training Loss: 0.0865557665626208 Validation Loss: 0.7833725214004517\n",
      "Epoch 13891: Training Loss: 0.08635247002045314 Validation Loss: 0.7828723788261414\n",
      "Epoch 13892: Training Loss: 0.08645730217297871 Validation Loss: 0.7828482389450073\n",
      "Epoch 13893: Training Loss: 0.08628420531749725 Validation Loss: 0.7827888131141663\n",
      "Epoch 13894: Training Loss: 0.08619894087314606 Validation Loss: 0.7827311158180237\n",
      "Epoch 13895: Training Loss: 0.08634568254152934 Validation Loss: 0.783115804195404\n",
      "Epoch 13896: Training Loss: 0.08608746776978175 Validation Loss: 0.7836362719535828\n",
      "Epoch 13897: Training Loss: 0.08687228709459305 Validation Loss: 0.7832716107368469\n",
      "Epoch 13898: Training Loss: 0.08638676504294078 Validation Loss: 0.7826809883117676\n",
      "Epoch 13899: Training Loss: 0.08637645095586777 Validation Loss: 0.7828989028930664\n",
      "Epoch 13900: Training Loss: 0.08691166341304779 Validation Loss: 0.7828810214996338\n",
      "Epoch 13901: Training Loss: 0.08630896111329396 Validation Loss: 0.7830725908279419\n",
      "Epoch 13902: Training Loss: 0.08639900386333466 Validation Loss: 0.7833892107009888\n",
      "Epoch 13903: Training Loss: 0.08591549843549728 Validation Loss: 0.7830187678337097\n",
      "Epoch 13904: Training Loss: 0.08623340477546056 Validation Loss: 0.7832279801368713\n",
      "Epoch 13905: Training Loss: 0.0863037829597791 Validation Loss: 0.7829444408416748\n",
      "Epoch 13906: Training Loss: 0.0863845944404602 Validation Loss: 0.7827058434486389\n",
      "Epoch 13907: Training Loss: 0.0861020137866338 Validation Loss: 0.7829334139823914\n",
      "Epoch 13908: Training Loss: 0.08601762602726619 Validation Loss: 0.7826040983200073\n",
      "Epoch 13909: Training Loss: 0.08620776484409969 Validation Loss: 0.7827800512313843\n",
      "Epoch 13910: Training Loss: 0.08640475571155548 Validation Loss: 0.7835646867752075\n",
      "Epoch 13911: Training Loss: 0.08665533115466435 Validation Loss: 0.7833310961723328\n",
      "Epoch 13912: Training Loss: 0.08659013609091441 Validation Loss: 0.7832685112953186\n",
      "Epoch 13913: Training Loss: 0.08674786860744159 Validation Loss: 0.7835407853126526\n",
      "Epoch 13914: Training Loss: 0.08683191736539204 Validation Loss: 0.7831142544746399\n",
      "Epoch 13915: Training Loss: 0.08695995559295018 Validation Loss: 0.7829383611679077\n",
      "Epoch 13916: Training Loss: 0.08681627362966537 Validation Loss: 0.7830387353897095\n",
      "Epoch 13917: Training Loss: 0.08655438323815663 Validation Loss: 0.783267080783844\n",
      "Epoch 13918: Training Loss: 0.08646715184052785 Validation Loss: 0.7831524610519409\n",
      "Epoch 13919: Training Loss: 0.08628421773513158 Validation Loss: 0.7830443382263184\n",
      "Epoch 13920: Training Loss: 0.0863368089000384 Validation Loss: 0.7834616899490356\n",
      "Epoch 13921: Training Loss: 0.08622638632853825 Validation Loss: 0.7835410833358765\n",
      "Epoch 13922: Training Loss: 0.08648252735535304 Validation Loss: 0.783589243888855\n",
      "Epoch 13923: Training Loss: 0.08624288191397984 Validation Loss: 0.78319251537323\n",
      "Epoch 13924: Training Loss: 0.08629595985015233 Validation Loss: 0.7829561233520508\n",
      "Epoch 13925: Training Loss: 0.08610811829566956 Validation Loss: 0.782805323600769\n",
      "Epoch 13926: Training Loss: 0.0856948271393776 Validation Loss: 0.78290194272995\n",
      "Epoch 13927: Training Loss: 0.08622915546099345 Validation Loss: 0.7832295298576355\n",
      "Epoch 13928: Training Loss: 0.08666521559158961 Validation Loss: 0.7832707762718201\n",
      "Epoch 13929: Training Loss: 0.0862146367629369 Validation Loss: 0.7828682661056519\n",
      "Epoch 13930: Training Loss: 0.08615299314260483 Validation Loss: 0.783347487449646\n",
      "Epoch 13931: Training Loss: 0.08624432732661565 Validation Loss: 0.7832363843917847\n",
      "Epoch 13932: Training Loss: 0.0860735575358073 Validation Loss: 0.7831771373748779\n",
      "Epoch 13933: Training Loss: 0.0864471619327863 Validation Loss: 0.7825400829315186\n",
      "Epoch 13934: Training Loss: 0.08660272012154262 Validation Loss: 0.7829289436340332\n",
      "Epoch 13935: Training Loss: 0.0869577204187711 Validation Loss: 0.7829426527023315\n",
      "Epoch 13936: Training Loss: 0.08594201505184174 Validation Loss: 0.7832244634628296\n",
      "Epoch 13937: Training Loss: 0.08633796249826749 Validation Loss: 0.7833403944969177\n",
      "Epoch 13938: Training Loss: 0.08629850546518962 Validation Loss: 0.7836382389068604\n",
      "Epoch 13939: Training Loss: 0.08628916988770167 Validation Loss: 0.784015417098999\n",
      "Epoch 13940: Training Loss: 0.08679298311471939 Validation Loss: 0.7837902903556824\n",
      "Epoch 13941: Training Loss: 0.08653429398934047 Validation Loss: 0.7825748324394226\n",
      "Epoch 13942: Training Loss: 0.08671370645364125 Validation Loss: 0.7827202081680298\n",
      "Epoch 13943: Training Loss: 0.0861529956261317 Validation Loss: 0.7827473282814026\n",
      "Epoch 13944: Training Loss: 0.08678530156612396 Validation Loss: 0.7830044627189636\n",
      "Epoch 13945: Training Loss: 0.08631567656993866 Validation Loss: 0.7832546830177307\n",
      "Epoch 13946: Training Loss: 0.08604158461093903 Validation Loss: 0.7833657264709473\n",
      "Epoch 13947: Training Loss: 0.08644959330558777 Validation Loss: 0.7835383415222168\n",
      "Epoch 13948: Training Loss: 0.08634931594133377 Validation Loss: 0.7838044762611389\n",
      "Epoch 13949: Training Loss: 0.08624910563230515 Validation Loss: 0.7835078239440918\n",
      "Epoch 13950: Training Loss: 0.0867885301510493 Validation Loss: 0.7830761671066284\n",
      "Epoch 13951: Training Loss: 0.08624111115932465 Validation Loss: 0.7824764251708984\n",
      "Epoch 13952: Training Loss: 0.0860207627216975 Validation Loss: 0.7826818823814392\n",
      "Epoch 13953: Training Loss: 0.0861493448416392 Validation Loss: 0.7833594083786011\n",
      "Epoch 13954: Training Loss: 0.08600573986768723 Validation Loss: 0.7836095690727234\n",
      "Epoch 13955: Training Loss: 0.08628045270840327 Validation Loss: 0.7838157415390015\n",
      "Epoch 13956: Training Loss: 0.08635793129603068 Validation Loss: 0.7838467955589294\n",
      "Epoch 13957: Training Loss: 0.08657287806272507 Validation Loss: 0.7834572792053223\n",
      "Epoch 13958: Training Loss: 0.0862823302547137 Validation Loss: 0.7832284569740295\n",
      "Epoch 13959: Training Loss: 0.08608832955360413 Validation Loss: 0.7827374339103699\n",
      "Epoch 13960: Training Loss: 0.08595475802818935 Validation Loss: 0.7833030223846436\n",
      "Epoch 13961: Training Loss: 0.0861367757121722 Validation Loss: 0.7838547825813293\n",
      "Epoch 13962: Training Loss: 0.08610665301481883 Validation Loss: 0.7837558388710022\n",
      "Epoch 13963: Training Loss: 0.086460975309213 Validation Loss: 0.7843118906021118\n",
      "Epoch 13964: Training Loss: 0.08614439020554225 Validation Loss: 0.7838255167007446\n",
      "Epoch 13965: Training Loss: 0.08582269897063573 Validation Loss: 0.7836748957633972\n",
      "Epoch 13966: Training Loss: 0.08606293052434921 Validation Loss: 0.7838686108589172\n",
      "Epoch 13967: Training Loss: 0.08597105493148167 Validation Loss: 0.7832675576210022\n",
      "Epoch 13968: Training Loss: 0.08627786735693614 Validation Loss: 0.7829920649528503\n",
      "Epoch 13969: Training Loss: 0.08586834619442622 Validation Loss: 0.7833175659179688\n",
      "Epoch 13970: Training Loss: 0.0867885872721672 Validation Loss: 0.7837978601455688\n",
      "Epoch 13971: Training Loss: 0.08641037344932556 Validation Loss: 0.7841947674751282\n",
      "Epoch 13972: Training Loss: 0.08652808268864949 Validation Loss: 0.7839784622192383\n",
      "Epoch 13973: Training Loss: 0.08613426486651103 Validation Loss: 0.7839040160179138\n",
      "Epoch 13974: Training Loss: 0.0858670100569725 Validation Loss: 0.7837980389595032\n",
      "Epoch 13975: Training Loss: 0.08660854895909627 Validation Loss: 0.7836966514587402\n",
      "Epoch 13976: Training Loss: 0.08628773440917333 Validation Loss: 0.7836719155311584\n",
      "Epoch 13977: Training Loss: 0.08643500258525212 Validation Loss: 0.7837821841239929\n",
      "Epoch 13978: Training Loss: 0.08637966463963191 Validation Loss: 0.784186840057373\n",
      "Epoch 13979: Training Loss: 0.08601585278908412 Validation Loss: 0.7844634652137756\n",
      "Epoch 13980: Training Loss: 0.08599769572416942 Validation Loss: 0.7843309044837952\n",
      "Epoch 13981: Training Loss: 0.0862149919072787 Validation Loss: 0.7840074300765991\n",
      "Epoch 13982: Training Loss: 0.08578804135322571 Validation Loss: 0.7833731174468994\n",
      "Epoch 13983: Training Loss: 0.08612139771382014 Validation Loss: 0.7834751605987549\n",
      "Epoch 13984: Training Loss: 0.08721860001484553 Validation Loss: 0.7837138175964355\n",
      "Epoch 13985: Training Loss: 0.08677159001429875 Validation Loss: 0.7833064198493958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13986: Training Loss: 0.08614375938971837 Validation Loss: 0.7837167978286743\n",
      "Epoch 13987: Training Loss: 0.0869623323281606 Validation Loss: 0.7835935354232788\n",
      "Epoch 13988: Training Loss: 0.08622738470633824 Validation Loss: 0.7834828495979309\n",
      "Epoch 13989: Training Loss: 0.08518936236699422 Validation Loss: 0.7835719585418701\n",
      "Epoch 13990: Training Loss: 0.08592428515354793 Validation Loss: 0.7836776971817017\n",
      "Epoch 13991: Training Loss: 0.08605375389258067 Validation Loss: 0.783558189868927\n",
      "Epoch 13992: Training Loss: 0.08660643299420674 Validation Loss: 0.7835504412651062\n",
      "Epoch 13993: Training Loss: 0.08613354961077373 Validation Loss: 0.7840023636817932\n",
      "Epoch 13994: Training Loss: 0.08607126772403717 Validation Loss: 0.7841073870658875\n",
      "Epoch 13995: Training Loss: 0.08600451797246933 Validation Loss: 0.783901572227478\n",
      "Epoch 13996: Training Loss: 0.08671873311201732 Validation Loss: 0.7838297486305237\n",
      "Epoch 13997: Training Loss: 0.08612100034952164 Validation Loss: 0.7836886644363403\n",
      "Epoch 13998: Training Loss: 0.08600923915704091 Validation Loss: 0.7841341495513916\n",
      "Epoch 13999: Training Loss: 0.08646343151728313 Validation Loss: 0.7837828397750854\n",
      "Epoch 14000: Training Loss: 0.08580734332402547 Validation Loss: 0.7835211753845215\n",
      "Epoch 14001: Training Loss: 0.08605369677146275 Validation Loss: 0.7839420437812805\n",
      "Epoch 14002: Training Loss: 0.0865227182706197 Validation Loss: 0.7838060855865479\n",
      "Epoch 14003: Training Loss: 0.08595791459083557 Validation Loss: 0.7843137383460999\n",
      "Epoch 14004: Training Loss: 0.08646208792924881 Validation Loss: 0.7844166159629822\n",
      "Epoch 14005: Training Loss: 0.08639838298161824 Validation Loss: 0.7844927310943604\n",
      "Epoch 14006: Training Loss: 0.08630009740591049 Validation Loss: 0.7848873138427734\n",
      "Epoch 14007: Training Loss: 0.08598184833923976 Validation Loss: 0.7842511534690857\n",
      "Epoch 14008: Training Loss: 0.08565767109394073 Validation Loss: 0.7834595441818237\n",
      "Epoch 14009: Training Loss: 0.08637466405828793 Validation Loss: 0.7835483551025391\n",
      "Epoch 14010: Training Loss: 0.08600607266028722 Validation Loss: 0.7835397720336914\n",
      "Epoch 14011: Training Loss: 0.08582296967506409 Validation Loss: 0.7838017344474792\n",
      "Epoch 14012: Training Loss: 0.08585427949825923 Validation Loss: 0.7839317917823792\n",
      "Epoch 14013: Training Loss: 0.08647507429122925 Validation Loss: 0.7837544083595276\n",
      "Epoch 14014: Training Loss: 0.08475660781065623 Validation Loss: 0.7842947840690613\n",
      "Epoch 14015: Training Loss: 0.08628734201192856 Validation Loss: 0.7848082780838013\n",
      "Epoch 14016: Training Loss: 0.0864340290427208 Validation Loss: 0.7845070958137512\n",
      "Epoch 14017: Training Loss: 0.085943470398585 Validation Loss: 0.7840840220451355\n",
      "Epoch 14018: Training Loss: 0.08618209014336269 Validation Loss: 0.7837651371955872\n",
      "Epoch 14019: Training Loss: 0.08582247048616409 Validation Loss: 0.7839986085891724\n",
      "Epoch 14020: Training Loss: 0.08611934383710225 Validation Loss: 0.7845414876937866\n",
      "Epoch 14021: Training Loss: 0.08567327757676442 Validation Loss: 0.7839962244033813\n",
      "Epoch 14022: Training Loss: 0.08617013196150462 Validation Loss: 0.7839488983154297\n",
      "Epoch 14023: Training Loss: 0.08591065555810928 Validation Loss: 0.7842961549758911\n",
      "Epoch 14024: Training Loss: 0.08588282763957977 Validation Loss: 0.7843835949897766\n",
      "Epoch 14025: Training Loss: 0.08584072192509969 Validation Loss: 0.7846415042877197\n",
      "Epoch 14026: Training Loss: 0.08680129299561183 Validation Loss: 0.784470796585083\n",
      "Epoch 14027: Training Loss: 0.08603345851103465 Validation Loss: 0.7843559384346008\n",
      "Epoch 14028: Training Loss: 0.08578584591547649 Validation Loss: 0.7837472558021545\n",
      "Epoch 14029: Training Loss: 0.08540352433919907 Validation Loss: 0.7834824323654175\n",
      "Epoch 14030: Training Loss: 0.08569274346033733 Validation Loss: 0.7836000323295593\n",
      "Epoch 14031: Training Loss: 0.08626146117846172 Validation Loss: 0.7841444611549377\n",
      "Epoch 14032: Training Loss: 0.08573938657840093 Validation Loss: 0.7841747999191284\n",
      "Epoch 14033: Training Loss: 0.08596641818682353 Validation Loss: 0.7839787006378174\n",
      "Epoch 14034: Training Loss: 0.08613208681344986 Validation Loss: 0.7839378714561462\n",
      "Epoch 14035: Training Loss: 0.08618543297052383 Validation Loss: 0.7843993306159973\n",
      "Epoch 14036: Training Loss: 0.08644496897856395 Validation Loss: 0.7842562198638916\n",
      "Epoch 14037: Training Loss: 0.08548786491155624 Validation Loss: 0.7841013073921204\n",
      "Epoch 14038: Training Loss: 0.08618930727243423 Validation Loss: 0.7840543985366821\n",
      "Epoch 14039: Training Loss: 0.08591938763856888 Validation Loss: 0.7842058539390564\n",
      "Epoch 14040: Training Loss: 0.08629678934812546 Validation Loss: 0.7850281596183777\n",
      "Epoch 14041: Training Loss: 0.08618266135454178 Validation Loss: 0.7851532697677612\n",
      "Epoch 14042: Training Loss: 0.08561019102732341 Validation Loss: 0.7848166227340698\n",
      "Epoch 14043: Training Loss: 0.08599197864532471 Validation Loss: 0.7847996354103088\n",
      "Epoch 14044: Training Loss: 0.08568754295508067 Validation Loss: 0.7845628261566162\n",
      "Epoch 14045: Training Loss: 0.08577633400758107 Validation Loss: 0.7844860553741455\n",
      "Epoch 14046: Training Loss: 0.08605458587408066 Validation Loss: 0.7842847108840942\n",
      "Epoch 14047: Training Loss: 0.0862186998128891 Validation Loss: 0.7844797372817993\n",
      "Epoch 14048: Training Loss: 0.0863755817214648 Validation Loss: 0.7843320369720459\n",
      "Epoch 14049: Training Loss: 0.08549632628758748 Validation Loss: 0.7845317125320435\n",
      "Epoch 14050: Training Loss: 0.08565960824489594 Validation Loss: 0.784501850605011\n",
      "Epoch 14051: Training Loss: 0.085678997139136 Validation Loss: 0.784713625907898\n",
      "Epoch 14052: Training Loss: 0.08560824394226074 Validation Loss: 0.784287691116333\n",
      "Epoch 14053: Training Loss: 0.08571325490872066 Validation Loss: 0.7844971418380737\n",
      "Epoch 14054: Training Loss: 0.08653668314218521 Validation Loss: 0.7848201990127563\n",
      "Epoch 14055: Training Loss: 0.0858547215660413 Validation Loss: 0.7847965955734253\n",
      "Epoch 14056: Training Loss: 0.08565343668063481 Validation Loss: 0.784527599811554\n",
      "Epoch 14057: Training Loss: 0.08624667425950368 Validation Loss: 0.7851224541664124\n",
      "Epoch 14058: Training Loss: 0.08583554873863856 Validation Loss: 0.7849889993667603\n",
      "Epoch 14059: Training Loss: 0.08601780484120052 Validation Loss: 0.7848069667816162\n",
      "Epoch 14060: Training Loss: 0.0870790183544159 Validation Loss: 0.7842368483543396\n",
      "Epoch 14061: Training Loss: 0.08596016714970271 Validation Loss: 0.7846117615699768\n",
      "Epoch 14062: Training Loss: 0.08599873632192612 Validation Loss: 0.7848508954048157\n",
      "Epoch 14063: Training Loss: 0.08614104241132736 Validation Loss: 0.7848924994468689\n",
      "Epoch 14064: Training Loss: 0.08609730998675029 Validation Loss: 0.7841154932975769\n",
      "Epoch 14065: Training Loss: 0.0857284019390742 Validation Loss: 0.7839820384979248\n",
      "Epoch 14066: Training Loss: 0.08583603302637736 Validation Loss: 0.7840462923049927\n",
      "Epoch 14067: Training Loss: 0.08568392445643742 Validation Loss: 0.7843331098556519\n",
      "Epoch 14068: Training Loss: 0.0855952724814415 Validation Loss: 0.7846276760101318\n",
      "Epoch 14069: Training Loss: 0.08564551174640656 Validation Loss: 0.7850457429885864\n",
      "Epoch 14070: Training Loss: 0.0857233131925265 Validation Loss: 0.7846024632453918\n",
      "Epoch 14071: Training Loss: 0.08566418290138245 Validation Loss: 0.7846736311912537\n",
      "Epoch 14072: Training Loss: 0.08573134988546371 Validation Loss: 0.784717321395874\n",
      "Epoch 14073: Training Loss: 0.08604460582137108 Validation Loss: 0.7848227620124817\n",
      "Epoch 14074: Training Loss: 0.085515891512235 Validation Loss: 0.7847733497619629\n",
      "Epoch 14075: Training Loss: 0.08603512495756149 Validation Loss: 0.7846760153770447\n",
      "Epoch 14076: Training Loss: 0.08582585056622823 Validation Loss: 0.7843910455703735\n",
      "Epoch 14077: Training Loss: 0.08544475336869557 Validation Loss: 0.7843306064605713\n",
      "Epoch 14078: Training Loss: 0.08580574641625087 Validation Loss: 0.7847015261650085\n",
      "Epoch 14079: Training Loss: 0.0856510375936826 Validation Loss: 0.7850995659828186\n",
      "Epoch 14080: Training Loss: 0.08604876448710759 Validation Loss: 0.7846590280532837\n",
      "Epoch 14081: Training Loss: 0.08561282356580098 Validation Loss: 0.7847291231155396\n",
      "Epoch 14082: Training Loss: 0.08560844510793686 Validation Loss: 0.7848118543624878\n",
      "Epoch 14083: Training Loss: 0.08584361275037129 Validation Loss: 0.7848015427589417\n",
      "Epoch 14084: Training Loss: 0.08661256482203801 Validation Loss: 0.7849740386009216\n",
      "Epoch 14085: Training Loss: 0.085768128434817 Validation Loss: 0.7849750518798828\n",
      "Epoch 14086: Training Loss: 0.08631762613852818 Validation Loss: 0.7851822972297668\n",
      "Epoch 14087: Training Loss: 0.0856119046608607 Validation Loss: 0.7852966785430908\n",
      "Epoch 14088: Training Loss: 0.08575245241324107 Validation Loss: 0.7853512763977051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14089: Training Loss: 0.0861252744992574 Validation Loss: 0.7851557731628418\n",
      "Epoch 14090: Training Loss: 0.08597000191609065 Validation Loss: 0.7847753763198853\n",
      "Epoch 14091: Training Loss: 0.08586837599674861 Validation Loss: 0.7846662998199463\n",
      "Epoch 14092: Training Loss: 0.0856543059150378 Validation Loss: 0.7846009135246277\n",
      "Epoch 14093: Training Loss: 0.08572880178689957 Validation Loss: 0.7842313647270203\n",
      "Epoch 14094: Training Loss: 0.08589640259742737 Validation Loss: 0.7846122980117798\n",
      "Epoch 14095: Training Loss: 0.08559589336315791 Validation Loss: 0.7851079702377319\n",
      "Epoch 14096: Training Loss: 0.08469220002492268 Validation Loss: 0.7852330207824707\n",
      "Epoch 14097: Training Loss: 0.08579200754563014 Validation Loss: 0.7850798964500427\n",
      "Epoch 14098: Training Loss: 0.08571658780177434 Validation Loss: 0.7850717902183533\n",
      "Epoch 14099: Training Loss: 0.08547543237606685 Validation Loss: 0.7847146391868591\n",
      "Epoch 14100: Training Loss: 0.08539394537607829 Validation Loss: 0.78424471616745\n",
      "Epoch 14101: Training Loss: 0.08609554668267567 Validation Loss: 0.7842965722084045\n",
      "Epoch 14102: Training Loss: 0.08629309882720311 Validation Loss: 0.7848137617111206\n",
      "Epoch 14103: Training Loss: 0.08638067295153935 Validation Loss: 0.7853550314903259\n",
      "Epoch 14104: Training Loss: 0.08608365803956985 Validation Loss: 0.7857317924499512\n",
      "Epoch 14105: Training Loss: 0.08563031752904256 Validation Loss: 0.785447895526886\n",
      "Epoch 14106: Training Loss: 0.08600480854511261 Validation Loss: 0.785323977470398\n",
      "Epoch 14107: Training Loss: 0.08611810952425003 Validation Loss: 0.7849976420402527\n",
      "Epoch 14108: Training Loss: 0.08592010289430618 Validation Loss: 0.7852412462234497\n",
      "Epoch 14109: Training Loss: 0.08551212151845296 Validation Loss: 0.7847959995269775\n",
      "Epoch 14110: Training Loss: 0.08600234736998875 Validation Loss: 0.7842912673950195\n",
      "Epoch 14111: Training Loss: 0.08562853435675304 Validation Loss: 0.7841630578041077\n",
      "Epoch 14112: Training Loss: 0.08553349723418553 Validation Loss: 0.7843526601791382\n",
      "Epoch 14113: Training Loss: 0.08570924401283264 Validation Loss: 0.7851016521453857\n",
      "Epoch 14114: Training Loss: 0.08613306532303493 Validation Loss: 0.7852323651313782\n",
      "Epoch 14115: Training Loss: 0.08643770466248195 Validation Loss: 0.7854263186454773\n",
      "Epoch 14116: Training Loss: 0.08558626721302669 Validation Loss: 0.7851352691650391\n",
      "Epoch 14117: Training Loss: 0.08570580184459686 Validation Loss: 0.7847882509231567\n",
      "Epoch 14118: Training Loss: 0.08552095045646031 Validation Loss: 0.7849820256233215\n",
      "Epoch 14119: Training Loss: 0.08551633854707082 Validation Loss: 0.7845757007598877\n",
      "Epoch 14120: Training Loss: 0.08537251750628154 Validation Loss: 0.7846362590789795\n",
      "Epoch 14121: Training Loss: 0.08566309760014217 Validation Loss: 0.7848700284957886\n",
      "Epoch 14122: Training Loss: 0.08547082295020421 Validation Loss: 0.7851331830024719\n",
      "Epoch 14123: Training Loss: 0.08549499015013377 Validation Loss: 0.7852855920791626\n",
      "Epoch 14124: Training Loss: 0.08562455823024114 Validation Loss: 0.785309910774231\n",
      "Epoch 14125: Training Loss: 0.08570557336012523 Validation Loss: 0.7848719954490662\n",
      "Epoch 14126: Training Loss: 0.08573016027609508 Validation Loss: 0.7853085994720459\n",
      "Epoch 14127: Training Loss: 0.08566275735696156 Validation Loss: 0.7847356796264648\n",
      "Epoch 14128: Training Loss: 0.08537983894348145 Validation Loss: 0.7848028540611267\n",
      "Epoch 14129: Training Loss: 0.08568270752827327 Validation Loss: 0.7850437164306641\n",
      "Epoch 14130: Training Loss: 0.08569245288769405 Validation Loss: 0.7853404879570007\n",
      "Epoch 14131: Training Loss: 0.08553950488567352 Validation Loss: 0.7855688333511353\n",
      "Epoch 14132: Training Loss: 0.08562034120162328 Validation Loss: 0.7851555943489075\n",
      "Epoch 14133: Training Loss: 0.08559068789084752 Validation Loss: 0.7850914001464844\n",
      "Epoch 14134: Training Loss: 0.08513807008663814 Validation Loss: 0.7848910689353943\n",
      "Epoch 14135: Training Loss: 0.08543729037046432 Validation Loss: 0.7850399017333984\n",
      "Epoch 14136: Training Loss: 0.08538748323917389 Validation Loss: 0.7848477363586426\n",
      "Epoch 14137: Training Loss: 0.0860627144575119 Validation Loss: 0.7847245335578918\n",
      "Epoch 14138: Training Loss: 0.08578406522671382 Validation Loss: 0.784561038017273\n",
      "Epoch 14139: Training Loss: 0.08591197927792867 Validation Loss: 0.7840196490287781\n",
      "Epoch 14140: Training Loss: 0.08565331250429153 Validation Loss: 0.7844559550285339\n",
      "Epoch 14141: Training Loss: 0.08581366141637166 Validation Loss: 0.785084068775177\n",
      "Epoch 14142: Training Loss: 0.08526522169510524 Validation Loss: 0.7855818271636963\n",
      "Epoch 14143: Training Loss: 0.08541947106520335 Validation Loss: 0.7855916619300842\n",
      "Epoch 14144: Training Loss: 0.08530851205190022 Validation Loss: 0.7859314680099487\n",
      "Epoch 14145: Training Loss: 0.08578201134999593 Validation Loss: 0.7854573726654053\n",
      "Epoch 14146: Training Loss: 0.08555374046166737 Validation Loss: 0.7847981452941895\n",
      "Epoch 14147: Training Loss: 0.08555841694275539 Validation Loss: 0.7847720980644226\n",
      "Epoch 14148: Training Loss: 0.0867603247364362 Validation Loss: 0.7850924134254456\n",
      "Epoch 14149: Training Loss: 0.0860532820224762 Validation Loss: 0.7852944135665894\n",
      "Epoch 14150: Training Loss: 0.08586899936199188 Validation Loss: 0.7851478457450867\n",
      "Epoch 14151: Training Loss: 0.08598414808511734 Validation Loss: 0.7851342558860779\n",
      "Epoch 14152: Training Loss: 0.08550802866617839 Validation Loss: 0.7850605249404907\n",
      "Epoch 14153: Training Loss: 0.08554774026076 Validation Loss: 0.7853550910949707\n",
      "Epoch 14154: Training Loss: 0.08534516642491023 Validation Loss: 0.7853372097015381\n",
      "Epoch 14155: Training Loss: 0.08560072382291158 Validation Loss: 0.7854438424110413\n",
      "Epoch 14156: Training Loss: 0.08542535454034805 Validation Loss: 0.7850970029830933\n",
      "Epoch 14157: Training Loss: 0.08537948131561279 Validation Loss: 0.7853731513023376\n",
      "Epoch 14158: Training Loss: 0.08542926857868831 Validation Loss: 0.7851372957229614\n",
      "Epoch 14159: Training Loss: 0.08585402866204579 Validation Loss: 0.7852697372436523\n",
      "Epoch 14160: Training Loss: 0.08539582292238872 Validation Loss: 0.7854084372520447\n",
      "Epoch 14161: Training Loss: 0.0857470432917277 Validation Loss: 0.785568356513977\n",
      "Epoch 14162: Training Loss: 0.08579535285631816 Validation Loss: 0.7854507565498352\n",
      "Epoch 14163: Training Loss: 0.08541581779718399 Validation Loss: 0.7850419282913208\n",
      "Epoch 14164: Training Loss: 0.085494431356589 Validation Loss: 0.7847459316253662\n",
      "Epoch 14165: Training Loss: 0.08562923471132915 Validation Loss: 0.7852444648742676\n",
      "Epoch 14166: Training Loss: 0.08537544806798299 Validation Loss: 0.7849346995353699\n",
      "Epoch 14167: Training Loss: 0.08553942541281383 Validation Loss: 0.7851635217666626\n",
      "Epoch 14168: Training Loss: 0.0855871910850207 Validation Loss: 0.7854933738708496\n",
      "Epoch 14169: Training Loss: 0.08551725496848424 Validation Loss: 0.7856714725494385\n",
      "Epoch 14170: Training Loss: 0.08577309548854828 Validation Loss: 0.7855509519577026\n",
      "Epoch 14171: Training Loss: 0.08550137529770534 Validation Loss: 0.7852271199226379\n",
      "Epoch 14172: Training Loss: 0.08555952707926433 Validation Loss: 0.7850945591926575\n",
      "Epoch 14173: Training Loss: 0.08544661353031795 Validation Loss: 0.7849181890487671\n",
      "Epoch 14174: Training Loss: 0.0861939291159312 Validation Loss: 0.7856016159057617\n",
      "Epoch 14175: Training Loss: 0.08571427563826244 Validation Loss: 0.7851682901382446\n",
      "Epoch 14176: Training Loss: 0.0864251380165418 Validation Loss: 0.7848763465881348\n",
      "Epoch 14177: Training Loss: 0.08533274382352829 Validation Loss: 0.7851377129554749\n",
      "Epoch 14178: Training Loss: 0.0856123020251592 Validation Loss: 0.7854747772216797\n",
      "Epoch 14179: Training Loss: 0.08571262160936992 Validation Loss: 0.7856561541557312\n",
      "Epoch 14180: Training Loss: 0.08527068048715591 Validation Loss: 0.7854981422424316\n",
      "Epoch 14181: Training Loss: 0.08540270725886027 Validation Loss: 0.7849723100662231\n",
      "Epoch 14182: Training Loss: 0.08557943503061931 Validation Loss: 0.784834086894989\n",
      "Epoch 14183: Training Loss: 0.08490443229675293 Validation Loss: 0.7851051092147827\n",
      "Epoch 14184: Training Loss: 0.08574638764063518 Validation Loss: 0.7854312658309937\n",
      "Epoch 14185: Training Loss: 0.08568914482990901 Validation Loss: 0.785295307636261\n",
      "Epoch 14186: Training Loss: 0.08520643909772237 Validation Loss: 0.7854886651039124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14187: Training Loss: 0.08515620479981105 Validation Loss: 0.7855437994003296\n",
      "Epoch 14188: Training Loss: 0.0856689711411794 Validation Loss: 0.786354660987854\n",
      "Epoch 14189: Training Loss: 0.08550688376029332 Validation Loss: 0.7862709164619446\n",
      "Epoch 14190: Training Loss: 0.08519534021615982 Validation Loss: 0.7863022685050964\n",
      "Epoch 14191: Training Loss: 0.08545316259066264 Validation Loss: 0.7861415147781372\n",
      "Epoch 14192: Training Loss: 0.08508567760388057 Validation Loss: 0.7856006622314453\n",
      "Epoch 14193: Training Loss: 0.08572207639614741 Validation Loss: 0.7851148843765259\n",
      "Epoch 14194: Training Loss: 0.08577977120876312 Validation Loss: 0.7853721976280212\n",
      "Epoch 14195: Training Loss: 0.08538372814655304 Validation Loss: 0.7858583331108093\n",
      "Epoch 14196: Training Loss: 0.08538435647885005 Validation Loss: 0.7861683964729309\n",
      "Epoch 14197: Training Loss: 0.08584788193305333 Validation Loss: 0.7859372496604919\n",
      "Epoch 14198: Training Loss: 0.08549229552348454 Validation Loss: 0.7854354977607727\n",
      "Epoch 14199: Training Loss: 0.08560652534166972 Validation Loss: 0.7850620150566101\n",
      "Epoch 14200: Training Loss: 0.08577090750137965 Validation Loss: 0.7859565615653992\n",
      "Epoch 14201: Training Loss: 0.08513601372639339 Validation Loss: 0.7861876487731934\n",
      "Epoch 14202: Training Loss: 0.08591931561628978 Validation Loss: 0.7857016921043396\n",
      "Epoch 14203: Training Loss: 0.08551303545633952 Validation Loss: 0.7852593660354614\n",
      "Epoch 14204: Training Loss: 0.08572997401158015 Validation Loss: 0.7851079702377319\n",
      "Epoch 14205: Training Loss: 0.08527107040087382 Validation Loss: 0.7850546836853027\n",
      "Epoch 14206: Training Loss: 0.08566306531429291 Validation Loss: 0.7851647734642029\n",
      "Epoch 14207: Training Loss: 0.0854045773545901 Validation Loss: 0.7856945395469666\n",
      "Epoch 14208: Training Loss: 0.08494735012451808 Validation Loss: 0.7857517004013062\n",
      "Epoch 14209: Training Loss: 0.08529164890448253 Validation Loss: 0.7860726714134216\n",
      "Epoch 14210: Training Loss: 0.08596852173407872 Validation Loss: 0.7861951589584351\n",
      "Epoch 14211: Training Loss: 0.0852896769841512 Validation Loss: 0.7860268354415894\n",
      "Epoch 14212: Training Loss: 0.08569445957740147 Validation Loss: 0.7858167886734009\n",
      "Epoch 14213: Training Loss: 0.08505761126677196 Validation Loss: 0.7859042286872864\n",
      "Epoch 14214: Training Loss: 0.08626196036736171 Validation Loss: 0.7860526442527771\n",
      "Epoch 14215: Training Loss: 0.0852178434530894 Validation Loss: 0.7856402397155762\n",
      "Epoch 14216: Training Loss: 0.0853073497613271 Validation Loss: 0.7851162552833557\n",
      "Epoch 14217: Training Loss: 0.08538384735584259 Validation Loss: 0.7851447463035583\n",
      "Epoch 14218: Training Loss: 0.08566353470087051 Validation Loss: 0.7858637571334839\n",
      "Epoch 14219: Training Loss: 0.08519363154967625 Validation Loss: 0.7860246300697327\n",
      "Epoch 14220: Training Loss: 0.0857089211543401 Validation Loss: 0.7856300473213196\n",
      "Epoch 14221: Training Loss: 0.08532359451055527 Validation Loss: 0.7854041457176208\n",
      "Epoch 14222: Training Loss: 0.08549955983956654 Validation Loss: 0.7847831845283508\n",
      "Epoch 14223: Training Loss: 0.08476707339286804 Validation Loss: 0.7853587865829468\n",
      "Epoch 14224: Training Loss: 0.0852135494351387 Validation Loss: 0.7857477068901062\n",
      "Epoch 14225: Training Loss: 0.08535750210285187 Validation Loss: 0.7858307957649231\n",
      "Epoch 14226: Training Loss: 0.08568871766328812 Validation Loss: 0.7859850525856018\n",
      "Epoch 14227: Training Loss: 0.08566640317440033 Validation Loss: 0.7859737873077393\n",
      "Epoch 14228: Training Loss: 0.08539443711439769 Validation Loss: 0.7861865162849426\n",
      "Epoch 14229: Training Loss: 0.08560004830360413 Validation Loss: 0.785930335521698\n",
      "Epoch 14230: Training Loss: 0.08522079388300578 Validation Loss: 0.7855040431022644\n",
      "Epoch 14231: Training Loss: 0.08558757851521175 Validation Loss: 0.7857547998428345\n",
      "Epoch 14232: Training Loss: 0.08541792631149292 Validation Loss: 0.7862513065338135\n",
      "Epoch 14233: Training Loss: 0.08511791378259659 Validation Loss: 0.7863604426383972\n",
      "Epoch 14234: Training Loss: 0.08597324540217717 Validation Loss: 0.7862752676010132\n",
      "Epoch 14235: Training Loss: 0.08508879443009694 Validation Loss: 0.7854993939399719\n",
      "Epoch 14236: Training Loss: 0.08556703726450603 Validation Loss: 0.7856707572937012\n",
      "Epoch 14237: Training Loss: 0.08511049548784892 Validation Loss: 0.7858152389526367\n",
      "Epoch 14238: Training Loss: 0.08623844136794408 Validation Loss: 0.7851142883300781\n",
      "Epoch 14239: Training Loss: 0.08531498908996582 Validation Loss: 0.7855503559112549\n",
      "Epoch 14240: Training Loss: 0.08568505942821503 Validation Loss: 0.7863022685050964\n",
      "Epoch 14241: Training Loss: 0.08638512591520946 Validation Loss: 0.7865564227104187\n",
      "Epoch 14242: Training Loss: 0.08536830544471741 Validation Loss: 0.7858771085739136\n",
      "Epoch 14243: Training Loss: 0.08562500029802322 Validation Loss: 0.7857931852340698\n",
      "Epoch 14244: Training Loss: 0.08652402957280476 Validation Loss: 0.7863064408302307\n",
      "Epoch 14245: Training Loss: 0.08508709569772084 Validation Loss: 0.7858846783638\n",
      "Epoch 14246: Training Loss: 0.08497112492720287 Validation Loss: 0.7857175469398499\n",
      "Epoch 14247: Training Loss: 0.08624777322014172 Validation Loss: 0.7861862182617188\n",
      "Epoch 14248: Training Loss: 0.0850412944952647 Validation Loss: 0.786008358001709\n",
      "Epoch 14249: Training Loss: 0.08526312311490376 Validation Loss: 0.7864307165145874\n",
      "Epoch 14250: Training Loss: 0.08527099341154099 Validation Loss: 0.7861019968986511\n",
      "Epoch 14251: Training Loss: 0.0852900817990303 Validation Loss: 0.7859824299812317\n",
      "Epoch 14252: Training Loss: 0.08586780975262324 Validation Loss: 0.7859988212585449\n",
      "Epoch 14253: Training Loss: 0.08593126634756725 Validation Loss: 0.785968542098999\n",
      "Epoch 14254: Training Loss: 0.08530053993066151 Validation Loss: 0.7868764996528625\n",
      "Epoch 14255: Training Loss: 0.08510777602593105 Validation Loss: 0.786507248878479\n",
      "Epoch 14256: Training Loss: 0.08537136018276215 Validation Loss: 0.786422848701477\n",
      "Epoch 14257: Training Loss: 0.08523945262034734 Validation Loss: 0.7861732244491577\n",
      "Epoch 14258: Training Loss: 0.0852810616294543 Validation Loss: 0.7860208749771118\n",
      "Epoch 14259: Training Loss: 0.08575253064433734 Validation Loss: 0.7857567071914673\n",
      "Epoch 14260: Training Loss: 0.0856910099585851 Validation Loss: 0.7861888408660889\n",
      "Epoch 14261: Training Loss: 0.08549056202173233 Validation Loss: 0.7864208817481995\n",
      "Epoch 14262: Training Loss: 0.08540670077006023 Validation Loss: 0.7866107225418091\n",
      "Epoch 14263: Training Loss: 0.08514246841271718 Validation Loss: 0.7864429354667664\n",
      "Epoch 14264: Training Loss: 0.0852588415145874 Validation Loss: 0.7859997153282166\n",
      "Epoch 14265: Training Loss: 0.08565683166186015 Validation Loss: 0.7861278057098389\n",
      "Epoch 14266: Training Loss: 0.08548386643330257 Validation Loss: 0.7861457467079163\n",
      "Epoch 14267: Training Loss: 0.08519710352023442 Validation Loss: 0.7859183549880981\n",
      "Epoch 14268: Training Loss: 0.08503250529368718 Validation Loss: 0.7859987616539001\n",
      "Epoch 14269: Training Loss: 0.08514901250600815 Validation Loss: 0.7859303951263428\n",
      "Epoch 14270: Training Loss: 0.08550410966078441 Validation Loss: 0.7862604260444641\n",
      "Epoch 14271: Training Loss: 0.085249329606692 Validation Loss: 0.7860820293426514\n",
      "Epoch 14272: Training Loss: 0.08524331450462341 Validation Loss: 0.7863043546676636\n",
      "Epoch 14273: Training Loss: 0.0853101188937823 Validation Loss: 0.7863852381706238\n",
      "Epoch 14274: Training Loss: 0.08545310546954472 Validation Loss: 0.7871620059013367\n",
      "Epoch 14275: Training Loss: 0.08524210999409358 Validation Loss: 0.7869404554367065\n",
      "Epoch 14276: Training Loss: 0.08563047647476196 Validation Loss: 0.786005973815918\n",
      "Epoch 14277: Training Loss: 0.08530261367559433 Validation Loss: 0.785692572593689\n",
      "Epoch 14278: Training Loss: 0.08518418669700623 Validation Loss: 0.7856705188751221\n",
      "Epoch 14279: Training Loss: 0.08545500536759694 Validation Loss: 0.7861624360084534\n",
      "Epoch 14280: Training Loss: 0.08505102495352428 Validation Loss: 0.7863138318061829\n",
      "Epoch 14281: Training Loss: 0.08501923829317093 Validation Loss: 0.7867050766944885\n",
      "Epoch 14282: Training Loss: 0.08466889957586925 Validation Loss: 0.7876119017601013\n",
      "Epoch 14283: Training Loss: 0.08557187269131343 Validation Loss: 0.7872923612594604\n",
      "Epoch 14284: Training Loss: 0.0853664403160413 Validation Loss: 0.7871092557907104\n",
      "Epoch 14285: Training Loss: 0.08580512553453445 Validation Loss: 0.7864789366722107\n",
      "Epoch 14286: Training Loss: 0.08555538207292557 Validation Loss: 0.7864639163017273\n",
      "Epoch 14287: Training Loss: 0.08506691207488377 Validation Loss: 0.786006510257721\n",
      "Epoch 14288: Training Loss: 0.08516952147086461 Validation Loss: 0.7859987020492554\n",
      "Epoch 14289: Training Loss: 0.0857303539911906 Validation Loss: 0.7865365147590637\n",
      "Epoch 14290: Training Loss: 0.08529577652613322 Validation Loss: 0.7863606810569763\n",
      "Epoch 14291: Training Loss: 0.08538575222094853 Validation Loss: 0.7873738408088684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14292: Training Loss: 0.08544703076283137 Validation Loss: 0.7874314785003662\n",
      "Epoch 14293: Training Loss: 0.08475060760974884 Validation Loss: 0.7871638536453247\n",
      "Epoch 14294: Training Loss: 0.08584875365098317 Validation Loss: 0.7866246104240417\n",
      "Epoch 14295: Training Loss: 0.08553157995144527 Validation Loss: 0.78607177734375\n",
      "Epoch 14296: Training Loss: 0.08532252907752991 Validation Loss: 0.7862156629562378\n",
      "Epoch 14297: Training Loss: 0.08562522878249486 Validation Loss: 0.7865657806396484\n",
      "Epoch 14298: Training Loss: 0.08529311915238698 Validation Loss: 0.7868644595146179\n",
      "Epoch 14299: Training Loss: 0.08521097153425217 Validation Loss: 0.7869169116020203\n",
      "Epoch 14300: Training Loss: 0.08518049369255702 Validation Loss: 0.7868983149528503\n",
      "Epoch 14301: Training Loss: 0.08507182945807774 Validation Loss: 0.7867695093154907\n",
      "Epoch 14302: Training Loss: 0.0852129931251208 Validation Loss: 0.7868787050247192\n",
      "Epoch 14303: Training Loss: 0.08504811922709148 Validation Loss: 0.7868263721466064\n",
      "Epoch 14304: Training Loss: 0.08494108418623607 Validation Loss: 0.7861122488975525\n",
      "Epoch 14305: Training Loss: 0.08506125211715698 Validation Loss: 0.7863752245903015\n",
      "Epoch 14306: Training Loss: 0.08503586302200954 Validation Loss: 0.7864471673965454\n",
      "Epoch 14307: Training Loss: 0.08512682716051738 Validation Loss: 0.7865961790084839\n",
      "Epoch 14308: Training Loss: 0.08471703777710597 Validation Loss: 0.7867220640182495\n",
      "Epoch 14309: Training Loss: 0.08521084239085515 Validation Loss: 0.7866360545158386\n",
      "Epoch 14310: Training Loss: 0.08528400709231694 Validation Loss: 0.7864067554473877\n",
      "Epoch 14311: Training Loss: 0.08507516731818517 Validation Loss: 0.7864833474159241\n",
      "Epoch 14312: Training Loss: 0.08516254772742589 Validation Loss: 0.7865403890609741\n",
      "Epoch 14313: Training Loss: 0.08516169339418411 Validation Loss: 0.7871356010437012\n",
      "Epoch 14314: Training Loss: 0.0850401520729065 Validation Loss: 0.7872755527496338\n",
      "Epoch 14315: Training Loss: 0.08600786328315735 Validation Loss: 0.7875941395759583\n",
      "Epoch 14316: Training Loss: 0.08542038003603618 Validation Loss: 0.7871606945991516\n",
      "Epoch 14317: Training Loss: 0.08522646129131317 Validation Loss: 0.7868098616600037\n",
      "Epoch 14318: Training Loss: 0.0851070483525594 Validation Loss: 0.7867197394371033\n",
      "Epoch 14319: Training Loss: 0.08516388883193333 Validation Loss: 0.7866707444190979\n",
      "Epoch 14320: Training Loss: 0.08495574196179707 Validation Loss: 0.7864855527877808\n",
      "Epoch 14321: Training Loss: 0.08572160700956981 Validation Loss: 0.7862968444824219\n",
      "Epoch 14322: Training Loss: 0.08504995703697205 Validation Loss: 0.7865926027297974\n",
      "Epoch 14323: Training Loss: 0.0851749728123347 Validation Loss: 0.7867386341094971\n",
      "Epoch 14324: Training Loss: 0.08495742579301198 Validation Loss: 0.7867998480796814\n",
      "Epoch 14325: Training Loss: 0.08485658715168636 Validation Loss: 0.7870359420776367\n",
      "Epoch 14326: Training Loss: 0.0870009462038676 Validation Loss: 0.7871323823928833\n",
      "Epoch 14327: Training Loss: 0.08540114512046178 Validation Loss: 0.7869969606399536\n",
      "Epoch 14328: Training Loss: 0.084803673128287 Validation Loss: 0.7869733572006226\n",
      "Epoch 14329: Training Loss: 0.08511261641979218 Validation Loss: 0.7866364121437073\n",
      "Epoch 14330: Training Loss: 0.08512420952320099 Validation Loss: 0.7872304916381836\n",
      "Epoch 14331: Training Loss: 0.08503361543019612 Validation Loss: 0.7872061729431152\n",
      "Epoch 14332: Training Loss: 0.08506098637978236 Validation Loss: 0.7879831790924072\n",
      "Epoch 14333: Training Loss: 0.08489648501078288 Validation Loss: 0.7880094051361084\n",
      "Epoch 14334: Training Loss: 0.0855815460284551 Validation Loss: 0.7874292135238647\n",
      "Epoch 14335: Training Loss: 0.08519537995258968 Validation Loss: 0.7865663170814514\n",
      "Epoch 14336: Training Loss: 0.08507450173298518 Validation Loss: 0.7864876985549927\n",
      "Epoch 14337: Training Loss: 0.08502967904011409 Validation Loss: 0.7867024540901184\n",
      "Epoch 14338: Training Loss: 0.08508672565221786 Validation Loss: 0.7869696617126465\n",
      "Epoch 14339: Training Loss: 0.08517035345236461 Validation Loss: 0.7870943546295166\n",
      "Epoch 14340: Training Loss: 0.08498647560675938 Validation Loss: 0.7867614030838013\n",
      "Epoch 14341: Training Loss: 0.0846320812900861 Validation Loss: 0.786531925201416\n",
      "Epoch 14342: Training Loss: 0.08471350868542989 Validation Loss: 0.7865033149719238\n",
      "Epoch 14343: Training Loss: 0.08508451531330745 Validation Loss: 0.7862430810928345\n",
      "Epoch 14344: Training Loss: 0.08481414119402568 Validation Loss: 0.7865626811981201\n",
      "Epoch 14345: Training Loss: 0.08488665521144867 Validation Loss: 0.7865967750549316\n",
      "Epoch 14346: Training Loss: 0.08522539089123408 Validation Loss: 0.7868018746376038\n",
      "Epoch 14347: Training Loss: 0.08549681554238002 Validation Loss: 0.7867709398269653\n",
      "Epoch 14348: Training Loss: 0.08532180388768514 Validation Loss: 0.7872280478477478\n",
      "Epoch 14349: Training Loss: 0.08495546132326126 Validation Loss: 0.7869123816490173\n",
      "Epoch 14350: Training Loss: 0.08491133153438568 Validation Loss: 0.7867354154586792\n",
      "Epoch 14351: Training Loss: 0.08510503669579823 Validation Loss: 0.7863864898681641\n",
      "Epoch 14352: Training Loss: 0.0850675826271375 Validation Loss: 0.7865327000617981\n",
      "Epoch 14353: Training Loss: 0.08530315260092418 Validation Loss: 0.7868586182594299\n",
      "Epoch 14354: Training Loss: 0.08497316390275955 Validation Loss: 0.7872465252876282\n",
      "Epoch 14355: Training Loss: 0.08524743467569351 Validation Loss: 0.7874366641044617\n",
      "Epoch 14356: Training Loss: 0.08523279180129369 Validation Loss: 0.7874823212623596\n",
      "Epoch 14357: Training Loss: 0.08521288136641185 Validation Loss: 0.787456750869751\n",
      "Epoch 14358: Training Loss: 0.08495404322942098 Validation Loss: 0.7871455550193787\n",
      "Epoch 14359: Training Loss: 0.08530813703934352 Validation Loss: 0.7869284749031067\n",
      "Epoch 14360: Training Loss: 0.08468437194824219 Validation Loss: 0.7866813540458679\n",
      "Epoch 14361: Training Loss: 0.08538511395454407 Validation Loss: 0.7870885729789734\n",
      "Epoch 14362: Training Loss: 0.08519722769657771 Validation Loss: 0.7873573899269104\n",
      "Epoch 14363: Training Loss: 0.08495937784512837 Validation Loss: 0.7877478003501892\n",
      "Epoch 14364: Training Loss: 0.08554958552122116 Validation Loss: 0.787408709526062\n",
      "Epoch 14365: Training Loss: 0.08486665536959966 Validation Loss: 0.7870403528213501\n",
      "Epoch 14366: Training Loss: 0.08506063123544057 Validation Loss: 0.7873890995979309\n",
      "Epoch 14367: Training Loss: 0.0851504107316335 Validation Loss: 0.7870792746543884\n",
      "Epoch 14368: Training Loss: 0.08491985251506169 Validation Loss: 0.7870466113090515\n",
      "Epoch 14369: Training Loss: 0.08485190570354462 Validation Loss: 0.7871560454368591\n",
      "Epoch 14370: Training Loss: 0.08521644522746404 Validation Loss: 0.7871030569076538\n",
      "Epoch 14371: Training Loss: 0.08495672047138214 Validation Loss: 0.7877417802810669\n",
      "Epoch 14372: Training Loss: 0.08526611824830373 Validation Loss: 0.7876995801925659\n",
      "Epoch 14373: Training Loss: 0.08554590990146001 Validation Loss: 0.786747932434082\n",
      "Epoch 14374: Training Loss: 0.0845933531721433 Validation Loss: 0.7868836522102356\n",
      "Epoch 14375: Training Loss: 0.08474337309598923 Validation Loss: 0.7868835926055908\n",
      "Epoch 14376: Training Loss: 0.08548300961653392 Validation Loss: 0.7877628803253174\n",
      "Epoch 14377: Training Loss: 0.08497521529595058 Validation Loss: 0.7875977754592896\n",
      "Epoch 14378: Training Loss: 0.08506511151790619 Validation Loss: 0.7874712944030762\n",
      "Epoch 14379: Training Loss: 0.08481418838103612 Validation Loss: 0.7866839170455933\n",
      "Epoch 14380: Training Loss: 0.08499801407257716 Validation Loss: 0.7866726517677307\n",
      "Epoch 14381: Training Loss: 0.08475174009799957 Validation Loss: 0.7866457104682922\n",
      "Epoch 14382: Training Loss: 0.0849922647078832 Validation Loss: 0.7874254584312439\n",
      "Epoch 14383: Training Loss: 0.08477848768234253 Validation Loss: 0.7874540090560913\n",
      "Epoch 14384: Training Loss: 0.08520060405135155 Validation Loss: 0.7877668738365173\n",
      "Epoch 14385: Training Loss: 0.08478271216154099 Validation Loss: 0.7872210741043091\n",
      "Epoch 14386: Training Loss: 0.08521286398172379 Validation Loss: 0.7874507904052734\n",
      "Epoch 14387: Training Loss: 0.08488140751918156 Validation Loss: 0.7870421409606934\n",
      "Epoch 14388: Training Loss: 0.08521968126296997 Validation Loss: 0.7870161533355713\n",
      "Epoch 14389: Training Loss: 0.08512208859125774 Validation Loss: 0.7875404357910156\n",
      "Epoch 14390: Training Loss: 0.0842958614230156 Validation Loss: 0.7875600457191467\n",
      "Epoch 14391: Training Loss: 0.08494045585393906 Validation Loss: 0.7869720458984375\n",
      "Epoch 14392: Training Loss: 0.08482349663972855 Validation Loss: 0.7865135669708252\n",
      "Epoch 14393: Training Loss: 0.08482484519481659 Validation Loss: 0.7864705324172974\n",
      "Epoch 14394: Training Loss: 0.08499320844809215 Validation Loss: 0.7868822813034058\n",
      "Epoch 14395: Training Loss: 0.08514036983251572 Validation Loss: 0.7876291275024414\n",
      "Epoch 14396: Training Loss: 0.08488161861896515 Validation Loss: 0.7872591018676758\n",
      "Epoch 14397: Training Loss: 0.0845990131298701 Validation Loss: 0.787010908126831\n",
      "Epoch 14398: Training Loss: 0.08594161768754323 Validation Loss: 0.7868439555168152\n",
      "Epoch 14399: Training Loss: 0.0846331665913264 Validation Loss: 0.7868539690971375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14400: Training Loss: 0.08490284035603206 Validation Loss: 0.7871350646018982\n",
      "Epoch 14401: Training Loss: 0.08494782199462254 Validation Loss: 0.7874540090560913\n",
      "Epoch 14402: Training Loss: 0.08541069428126018 Validation Loss: 0.7873486280441284\n",
      "Epoch 14403: Training Loss: 0.08480689922968547 Validation Loss: 0.7870956659317017\n",
      "Epoch 14404: Training Loss: 0.08481803039709727 Validation Loss: 0.7870215773582458\n",
      "Epoch 14405: Training Loss: 0.0858706459403038 Validation Loss: 0.7869718074798584\n",
      "Epoch 14406: Training Loss: 0.08480340987443924 Validation Loss: 0.7874338626861572\n",
      "Epoch 14407: Training Loss: 0.08488873392343521 Validation Loss: 0.7877510786056519\n",
      "Epoch 14408: Training Loss: 0.08482420444488525 Validation Loss: 0.7875336408615112\n",
      "Epoch 14409: Training Loss: 0.08467774589856465 Validation Loss: 0.7877540588378906\n",
      "Epoch 14410: Training Loss: 0.08521794776121776 Validation Loss: 0.7876781225204468\n",
      "Epoch 14411: Training Loss: 0.08488136033217113 Validation Loss: 0.7878122925758362\n",
      "Epoch 14412: Training Loss: 0.08488218734661739 Validation Loss: 0.7876781225204468\n",
      "Epoch 14413: Training Loss: 0.08478036274512608 Validation Loss: 0.7874480485916138\n",
      "Epoch 14414: Training Loss: 0.08467088888088863 Validation Loss: 0.7873996496200562\n",
      "Epoch 14415: Training Loss: 0.08471030741930008 Validation Loss: 0.7874338626861572\n",
      "Epoch 14416: Training Loss: 0.08463965107997258 Validation Loss: 0.7879106998443604\n",
      "Epoch 14417: Training Loss: 0.08491923660039902 Validation Loss: 0.788007915019989\n",
      "Epoch 14418: Training Loss: 0.08468929678201675 Validation Loss: 0.787748396396637\n",
      "Epoch 14419: Training Loss: 0.08488534390926361 Validation Loss: 0.7880362272262573\n",
      "Epoch 14420: Training Loss: 0.08489639312028885 Validation Loss: 0.7878603935241699\n",
      "Epoch 14421: Training Loss: 0.08489730954170227 Validation Loss: 0.7879047393798828\n",
      "Epoch 14422: Training Loss: 0.08503721157709758 Validation Loss: 0.7877408862113953\n",
      "Epoch 14423: Training Loss: 0.08495770394802094 Validation Loss: 0.7877688407897949\n",
      "Epoch 14424: Training Loss: 0.0848243460059166 Validation Loss: 0.7880871295928955\n",
      "Epoch 14425: Training Loss: 0.08488654345273972 Validation Loss: 0.7878832221031189\n",
      "Epoch 14426: Training Loss: 0.08479352543751399 Validation Loss: 0.7879086136817932\n",
      "Epoch 14427: Training Loss: 0.0844068576892217 Validation Loss: 0.7875310182571411\n",
      "Epoch 14428: Training Loss: 0.08490712443987529 Validation Loss: 0.7874558568000793\n",
      "Epoch 14429: Training Loss: 0.08475544800360997 Validation Loss: 0.7874783873558044\n",
      "Epoch 14430: Training Loss: 0.08469910422960918 Validation Loss: 0.7879406809806824\n",
      "Epoch 14431: Training Loss: 0.08493607739607494 Validation Loss: 0.7884324789047241\n",
      "Epoch 14432: Training Loss: 0.08534122506777446 Validation Loss: 0.7884817719459534\n",
      "Epoch 14433: Training Loss: 0.08485771218935649 Validation Loss: 0.7875731587409973\n",
      "Epoch 14434: Training Loss: 0.08487369865179062 Validation Loss: 0.7871525883674622\n",
      "Epoch 14435: Training Loss: 0.08452566713094711 Validation Loss: 0.7872657775878906\n",
      "Epoch 14436: Training Loss: 0.08466683328151703 Validation Loss: 0.7879695892333984\n",
      "Epoch 14437: Training Loss: 0.08486060549815495 Validation Loss: 0.7886897921562195\n",
      "Epoch 14438: Training Loss: 0.08467405041058858 Validation Loss: 0.7885141372680664\n",
      "Epoch 14439: Training Loss: 0.0852367269496123 Validation Loss: 0.7880368828773499\n",
      "Epoch 14440: Training Loss: 0.08457433432340622 Validation Loss: 0.7879570722579956\n",
      "Epoch 14441: Training Loss: 0.08461905519167583 Validation Loss: 0.7879363298416138\n",
      "Epoch 14442: Training Loss: 0.08500990271568298 Validation Loss: 0.78790682554245\n",
      "Epoch 14443: Training Loss: 0.08590799818436305 Validation Loss: 0.7882741093635559\n",
      "Epoch 14444: Training Loss: 0.08472817142804463 Validation Loss: 0.7879894971847534\n",
      "Epoch 14445: Training Loss: 0.08494599411884944 Validation Loss: 0.7873803973197937\n",
      "Epoch 14446: Training Loss: 0.08486521740754445 Validation Loss: 0.7874788641929626\n",
      "Epoch 14447: Training Loss: 0.08476389944553375 Validation Loss: 0.7875025868415833\n",
      "Epoch 14448: Training Loss: 0.08472329874833424 Validation Loss: 0.7877180576324463\n",
      "Epoch 14449: Training Loss: 0.08515591671069463 Validation Loss: 0.7879258990287781\n",
      "Epoch 14450: Training Loss: 0.084548386434714 Validation Loss: 0.788033664226532\n",
      "Epoch 14451: Training Loss: 0.08441358059644699 Validation Loss: 0.7883104681968689\n",
      "Epoch 14452: Training Loss: 0.08457011977831523 Validation Loss: 0.7883108854293823\n",
      "Epoch 14453: Training Loss: 0.08513614286979039 Validation Loss: 0.788850724697113\n",
      "Epoch 14454: Training Loss: 0.08472875257333119 Validation Loss: 0.7882816195487976\n",
      "Epoch 14455: Training Loss: 0.0846220130721728 Validation Loss: 0.7882009148597717\n",
      "Epoch 14456: Training Loss: 0.08484558016061783 Validation Loss: 0.7877982258796692\n",
      "Epoch 14457: Training Loss: 0.08470544964075089 Validation Loss: 0.7881641387939453\n",
      "Epoch 14458: Training Loss: 0.08477518459161122 Validation Loss: 0.7885922789573669\n",
      "Epoch 14459: Training Loss: 0.08479378372430801 Validation Loss: 0.788143515586853\n",
      "Epoch 14460: Training Loss: 0.08465857555468877 Validation Loss: 0.7882362604141235\n",
      "Epoch 14461: Training Loss: 0.08513544003168742 Validation Loss: 0.7881708145141602\n",
      "Epoch 14462: Training Loss: 0.08526525894800822 Validation Loss: 0.7881582379341125\n",
      "Epoch 14463: Training Loss: 0.08475888768831889 Validation Loss: 0.7881574034690857\n",
      "Epoch 14464: Training Loss: 0.08455762267112732 Validation Loss: 0.7882227301597595\n",
      "Epoch 14465: Training Loss: 0.085469800978899 Validation Loss: 0.7884009480476379\n",
      "Epoch 14466: Training Loss: 0.08446444074312846 Validation Loss: 0.7881858348846436\n",
      "Epoch 14467: Training Loss: 0.08445478479067485 Validation Loss: 0.7877121567726135\n",
      "Epoch 14468: Training Loss: 0.08483722805976868 Validation Loss: 0.7876165509223938\n",
      "Epoch 14469: Training Loss: 0.08474282672007878 Validation Loss: 0.7882856130599976\n",
      "Epoch 14470: Training Loss: 0.08485665420691173 Validation Loss: 0.7877998948097229\n",
      "Epoch 14471: Training Loss: 0.08493543912967046 Validation Loss: 0.7877896428108215\n",
      "Epoch 14472: Training Loss: 0.08461539695660274 Validation Loss: 0.7878944277763367\n",
      "Epoch 14473: Training Loss: 0.08472689737876256 Validation Loss: 0.78825843334198\n",
      "Epoch 14474: Training Loss: 0.08470883717139562 Validation Loss: 0.7889050841331482\n",
      "Epoch 14475: Training Loss: 0.08529382447401683 Validation Loss: 0.7886274456977844\n",
      "Epoch 14476: Training Loss: 0.0849849134683609 Validation Loss: 0.78810715675354\n",
      "Epoch 14477: Training Loss: 0.08507052809000015 Validation Loss: 0.7878925204277039\n",
      "Epoch 14478: Training Loss: 0.08454190939664841 Validation Loss: 0.7877873182296753\n",
      "Epoch 14479: Training Loss: 0.08518579726417859 Validation Loss: 0.787738025188446\n",
      "Epoch 14480: Training Loss: 0.08459549893935521 Validation Loss: 0.7881516814231873\n",
      "Epoch 14481: Training Loss: 0.08508867522080739 Validation Loss: 0.7879469394683838\n",
      "Epoch 14482: Training Loss: 0.08441928774118423 Validation Loss: 0.7881205677986145\n",
      "Epoch 14483: Training Loss: 0.08553039655089378 Validation Loss: 0.7885251045227051\n",
      "Epoch 14484: Training Loss: 0.08420621107021968 Validation Loss: 0.7889747023582458\n",
      "Epoch 14485: Training Loss: 0.08473397543032964 Validation Loss: 0.7887588739395142\n",
      "Epoch 14486: Training Loss: 0.08460917075475057 Validation Loss: 0.7885084748268127\n",
      "Epoch 14487: Training Loss: 0.08456425120433171 Validation Loss: 0.7881529331207275\n",
      "Epoch 14488: Training Loss: 0.08399191002051036 Validation Loss: 0.7881942391395569\n",
      "Epoch 14489: Training Loss: 0.08474151045084 Validation Loss: 0.7882328629493713\n",
      "Epoch 14490: Training Loss: 0.08452505618333817 Validation Loss: 0.7883445620536804\n",
      "Epoch 14491: Training Loss: 0.08439632008473079 Validation Loss: 0.7883173227310181\n",
      "Epoch 14492: Training Loss: 0.08581032355626424 Validation Loss: 0.7885755896568298\n",
      "Epoch 14493: Training Loss: 0.08443892747163773 Validation Loss: 0.7885797023773193\n",
      "Epoch 14494: Training Loss: 0.0845474327603976 Validation Loss: 0.7885864973068237\n",
      "Epoch 14495: Training Loss: 0.08476107319196065 Validation Loss: 0.7886910438537598\n",
      "Epoch 14496: Training Loss: 0.08402033150196075 Validation Loss: 0.788965106010437\n",
      "Epoch 14497: Training Loss: 0.08465137829383214 Validation Loss: 0.7892103791236877\n",
      "Epoch 14498: Training Loss: 0.0847630724310875 Validation Loss: 0.7886719703674316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14499: Training Loss: 0.08457742383082707 Validation Loss: 0.7882786989212036\n",
      "Epoch 14500: Training Loss: 0.08445119857788086 Validation Loss: 0.7877306938171387\n",
      "Epoch 14501: Training Loss: 0.08453748375177383 Validation Loss: 0.7876390218734741\n",
      "Epoch 14502: Training Loss: 0.08450989176829656 Validation Loss: 0.788601279258728\n",
      "Epoch 14503: Training Loss: 0.08465865751107533 Validation Loss: 0.7885322570800781\n",
      "Epoch 14504: Training Loss: 0.08456629763046901 Validation Loss: 0.789070188999176\n",
      "Epoch 14505: Training Loss: 0.08550744007031123 Validation Loss: 0.7886941432952881\n",
      "Epoch 14506: Training Loss: 0.08528066426515579 Validation Loss: 0.7884061336517334\n",
      "Epoch 14507: Training Loss: 0.08492444703976314 Validation Loss: 0.7882944941520691\n",
      "Epoch 14508: Training Loss: 0.08433654407660167 Validation Loss: 0.7883002758026123\n",
      "Epoch 14509: Training Loss: 0.08444919437170029 Validation Loss: 0.7880194187164307\n",
      "Epoch 14510: Training Loss: 0.08455274999141693 Validation Loss: 0.788449227809906\n",
      "Epoch 14511: Training Loss: 0.0846052219470342 Validation Loss: 0.7881726622581482\n",
      "Epoch 14512: Training Loss: 0.08484914650519688 Validation Loss: 0.7883318662643433\n",
      "Epoch 14513: Training Loss: 0.0845186859369278 Validation Loss: 0.7888297438621521\n",
      "Epoch 14514: Training Loss: 0.08462174485127132 Validation Loss: 0.7889702916145325\n",
      "Epoch 14515: Training Loss: 0.08446403096119563 Validation Loss: 0.7886966466903687\n",
      "Epoch 14516: Training Loss: 0.08450881391763687 Validation Loss: 0.7889924645423889\n",
      "Epoch 14517: Training Loss: 0.08472979565461476 Validation Loss: 0.7888393402099609\n",
      "Epoch 14518: Training Loss: 0.08583629131317139 Validation Loss: 0.7886235117912292\n",
      "Epoch 14519: Training Loss: 0.08473305900891621 Validation Loss: 0.7883057594299316\n",
      "Epoch 14520: Training Loss: 0.08469246576229732 Validation Loss: 0.7887381911277771\n",
      "Epoch 14521: Training Loss: 0.08479659756024678 Validation Loss: 0.7892829179763794\n",
      "Epoch 14522: Training Loss: 0.08426944663127263 Validation Loss: 0.789029061794281\n",
      "Epoch 14523: Training Loss: 0.08544272929430008 Validation Loss: 0.788619875907898\n",
      "Epoch 14524: Training Loss: 0.08455613255500793 Validation Loss: 0.7887845635414124\n",
      "Epoch 14525: Training Loss: 0.08467532694339752 Validation Loss: 0.7879217863082886\n",
      "Epoch 14526: Training Loss: 0.08462139467398326 Validation Loss: 0.7879605293273926\n",
      "Epoch 14527: Training Loss: 0.084563543399175 Validation Loss: 0.7879530787467957\n",
      "Epoch 14528: Training Loss: 0.0844850664337476 Validation Loss: 0.7882950901985168\n",
      "Epoch 14529: Training Loss: 0.08456992606321971 Validation Loss: 0.7881641387939453\n",
      "Epoch 14530: Training Loss: 0.08479700485865276 Validation Loss: 0.7885758280754089\n",
      "Epoch 14531: Training Loss: 0.08385297159353892 Validation Loss: 0.7884202003479004\n",
      "Epoch 14532: Training Loss: 0.08514323085546494 Validation Loss: 0.7886002659797668\n",
      "Epoch 14533: Training Loss: 0.08490674197673798 Validation Loss: 0.7883715033531189\n",
      "Epoch 14534: Training Loss: 0.08436235040426254 Validation Loss: 0.7881594300270081\n",
      "Epoch 14535: Training Loss: 0.08457875996828079 Validation Loss: 0.7877823114395142\n",
      "Epoch 14536: Training Loss: 0.08469380686680476 Validation Loss: 0.7875776290893555\n",
      "Epoch 14537: Training Loss: 0.08440673351287842 Validation Loss: 0.788585901260376\n",
      "Epoch 14538: Training Loss: 0.08487992485364278 Validation Loss: 0.7885346412658691\n",
      "Epoch 14539: Training Loss: 0.0847683697938919 Validation Loss: 0.7891305088996887\n",
      "Epoch 14540: Training Loss: 0.08442401637633641 Validation Loss: 0.7894954085350037\n",
      "Epoch 14541: Training Loss: 0.08454941585659981 Validation Loss: 0.7893057465553284\n",
      "Epoch 14542: Training Loss: 0.084601824482282 Validation Loss: 0.7889938950538635\n",
      "Epoch 14543: Training Loss: 0.08433047930399577 Validation Loss: 0.7886216044425964\n",
      "Epoch 14544: Training Loss: 0.08495469142993291 Validation Loss: 0.7876358032226562\n",
      "Epoch 14545: Training Loss: 0.08448699861764908 Validation Loss: 0.7880992889404297\n",
      "Epoch 14546: Training Loss: 0.08456780264774959 Validation Loss: 0.788726806640625\n",
      "Epoch 14547: Training Loss: 0.08442686746517818 Validation Loss: 0.7892680764198303\n",
      "Epoch 14548: Training Loss: 0.0844264527161916 Validation Loss: 0.7893671989440918\n",
      "Epoch 14549: Training Loss: 0.08434141675631206 Validation Loss: 0.7893747687339783\n",
      "Epoch 14550: Training Loss: 0.0846103032430013 Validation Loss: 0.789975106716156\n",
      "Epoch 14551: Training Loss: 0.08486049125591914 Validation Loss: 0.7890703082084656\n",
      "Epoch 14552: Training Loss: 0.08448611944913864 Validation Loss: 0.7885019779205322\n",
      "Epoch 14553: Training Loss: 0.08459536482890447 Validation Loss: 0.7883025407791138\n",
      "Epoch 14554: Training Loss: 0.08431755006313324 Validation Loss: 0.7882447242736816\n",
      "Epoch 14555: Training Loss: 0.08473438769578934 Validation Loss: 0.788780927658081\n",
      "Epoch 14556: Training Loss: 0.08460398266712825 Validation Loss: 0.7895305752754211\n",
      "Epoch 14557: Training Loss: 0.08484816302855809 Validation Loss: 0.7889384627342224\n",
      "Epoch 14558: Training Loss: 0.08444559574127197 Validation Loss: 0.7888824939727783\n",
      "Epoch 14559: Training Loss: 0.08456221719582875 Validation Loss: 0.7885437607765198\n",
      "Epoch 14560: Training Loss: 0.08445674429337184 Validation Loss: 0.7885406613349915\n",
      "Epoch 14561: Training Loss: 0.08421539266904195 Validation Loss: 0.7889195084571838\n",
      "Epoch 14562: Training Loss: 0.08432609836260478 Validation Loss: 0.7889264225959778\n",
      "Epoch 14563: Training Loss: 0.08492348343133926 Validation Loss: 0.7891363501548767\n",
      "Epoch 14564: Training Loss: 0.08455683042605718 Validation Loss: 0.7883486151695251\n",
      "Epoch 14565: Training Loss: 0.08432363718748093 Validation Loss: 0.7880960702896118\n",
      "Epoch 14566: Training Loss: 0.0844248856107394 Validation Loss: 0.7883701324462891\n",
      "Epoch 14567: Training Loss: 0.08423932393391927 Validation Loss: 0.7886714935302734\n",
      "Epoch 14568: Training Loss: 0.08429835736751556 Validation Loss: 0.7887778282165527\n",
      "Epoch 14569: Training Loss: 0.08468275517225266 Validation Loss: 0.7886877655982971\n",
      "Epoch 14570: Training Loss: 0.08459693441788356 Validation Loss: 0.7892671823501587\n",
      "Epoch 14571: Training Loss: 0.08438387264808019 Validation Loss: 0.7889499664306641\n",
      "Epoch 14572: Training Loss: 0.08448383957147598 Validation Loss: 0.7882182002067566\n",
      "Epoch 14573: Training Loss: 0.0843361218770345 Validation Loss: 0.788209855556488\n",
      "Epoch 14574: Training Loss: 0.08411095291376114 Validation Loss: 0.7886319160461426\n",
      "Epoch 14575: Training Loss: 0.08508491019407909 Validation Loss: 0.7887583374977112\n",
      "Epoch 14576: Training Loss: 0.08486638963222504 Validation Loss: 0.7892159223556519\n",
      "Epoch 14577: Training Loss: 0.08433478077252705 Validation Loss: 0.7896071672439575\n",
      "Epoch 14578: Training Loss: 0.08481321980555852 Validation Loss: 0.789447009563446\n",
      "Epoch 14579: Training Loss: 0.08441756169001262 Validation Loss: 0.78867506980896\n",
      "Epoch 14580: Training Loss: 0.08424996087948482 Validation Loss: 0.7884705662727356\n",
      "Epoch 14581: Training Loss: 0.08410826077063878 Validation Loss: 0.7885345220565796\n",
      "Epoch 14582: Training Loss: 0.08453625440597534 Validation Loss: 0.789021909236908\n",
      "Epoch 14583: Training Loss: 0.08421511699755986 Validation Loss: 0.7892665266990662\n",
      "Epoch 14584: Training Loss: 0.08425100644429524 Validation Loss: 0.7890681624412537\n",
      "Epoch 14585: Training Loss: 0.08466128011544545 Validation Loss: 0.7890913486480713\n",
      "Epoch 14586: Training Loss: 0.08437109738588333 Validation Loss: 0.7894369959831238\n",
      "Epoch 14587: Training Loss: 0.08453776687383652 Validation Loss: 0.7892227172851562\n",
      "Epoch 14588: Training Loss: 0.08439349631468455 Validation Loss: 0.7891942858695984\n",
      "Epoch 14589: Training Loss: 0.08446199695269267 Validation Loss: 0.7891510725021362\n",
      "Epoch 14590: Training Loss: 0.08463495473066966 Validation Loss: 0.7888966202735901\n",
      "Epoch 14591: Training Loss: 0.08449933926264445 Validation Loss: 0.7890487313270569\n",
      "Epoch 14592: Training Loss: 0.0852753333747387 Validation Loss: 0.789055585861206\n",
      "Epoch 14593: Training Loss: 0.08444534987211227 Validation Loss: 0.7883754372596741\n",
      "Epoch 14594: Training Loss: 0.08445668717225392 Validation Loss: 0.7890387177467346\n",
      "Epoch 14595: Training Loss: 0.08414817849795024 Validation Loss: 0.7888098955154419\n",
      "Epoch 14596: Training Loss: 0.08457340548435847 Validation Loss: 0.7897038459777832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14597: Training Loss: 0.08434999982515971 Validation Loss: 0.7894991040229797\n",
      "Epoch 14598: Training Loss: 0.08448369552691777 Validation Loss: 0.7890793085098267\n",
      "Epoch 14599: Training Loss: 0.0846958930293719 Validation Loss: 0.7889817357063293\n",
      "Epoch 14600: Training Loss: 0.08442703634500504 Validation Loss: 0.7891337871551514\n",
      "Epoch 14601: Training Loss: 0.08407493680715561 Validation Loss: 0.7889010906219482\n",
      "Epoch 14602: Training Loss: 0.08456483980019887 Validation Loss: 0.7885016202926636\n",
      "Epoch 14603: Training Loss: 0.08421353995800018 Validation Loss: 0.7885677218437195\n",
      "Epoch 14604: Training Loss: 0.08462634434302647 Validation Loss: 0.789057195186615\n",
      "Epoch 14605: Training Loss: 0.08429107815027237 Validation Loss: 0.7891985177993774\n",
      "Epoch 14606: Training Loss: 0.08458903680245082 Validation Loss: 0.7899319529533386\n",
      "Epoch 14607: Training Loss: 0.08435595780611038 Validation Loss: 0.7898470163345337\n",
      "Epoch 14608: Training Loss: 0.08421069631973903 Validation Loss: 0.7898598909378052\n",
      "Epoch 14609: Training Loss: 0.08469630529483159 Validation Loss: 0.7896283864974976\n",
      "Epoch 14610: Training Loss: 0.08433044205109279 Validation Loss: 0.7896286845207214\n",
      "Epoch 14611: Training Loss: 0.08443983147541682 Validation Loss: 0.7893889546394348\n",
      "Epoch 14612: Training Loss: 0.08456373463074367 Validation Loss: 0.7888854146003723\n",
      "Epoch 14613: Training Loss: 0.08445753405491511 Validation Loss: 0.789404571056366\n",
      "Epoch 14614: Training Loss: 0.08447258919477463 Validation Loss: 0.7888469696044922\n",
      "Epoch 14615: Training Loss: 0.08418175081411998 Validation Loss: 0.7892099022865295\n",
      "Epoch 14616: Training Loss: 0.08423273762067159 Validation Loss: 0.789299726486206\n",
      "Epoch 14617: Training Loss: 0.08418820301691692 Validation Loss: 0.7895181775093079\n",
      "Epoch 14618: Training Loss: 0.08431645234425862 Validation Loss: 0.7892545461654663\n",
      "Epoch 14619: Training Loss: 0.08451726784308751 Validation Loss: 0.7889593243598938\n",
      "Epoch 14620: Training Loss: 0.08547232051690419 Validation Loss: 0.7895049452781677\n",
      "Epoch 14621: Training Loss: 0.0846860483288765 Validation Loss: 0.7896041870117188\n",
      "Epoch 14622: Training Loss: 0.08423731476068497 Validation Loss: 0.7895008325576782\n",
      "Epoch 14623: Training Loss: 0.08447159826755524 Validation Loss: 0.7890608906745911\n",
      "Epoch 14624: Training Loss: 0.08580901970465978 Validation Loss: 0.7891478538513184\n",
      "Epoch 14625: Training Loss: 0.08409023781617482 Validation Loss: 0.7894797325134277\n",
      "Epoch 14626: Training Loss: 0.08431911965211232 Validation Loss: 0.7897812128067017\n",
      "Epoch 14627: Training Loss: 0.0842239186167717 Validation Loss: 0.7894492745399475\n",
      "Epoch 14628: Training Loss: 0.08432320753733318 Validation Loss: 0.7897235155105591\n",
      "Epoch 14629: Training Loss: 0.0842226321498553 Validation Loss: 0.7896066904067993\n",
      "Epoch 14630: Training Loss: 0.08459153274695079 Validation Loss: 0.7894079089164734\n",
      "Epoch 14631: Training Loss: 0.08416426678498586 Validation Loss: 0.7894941568374634\n",
      "Epoch 14632: Training Loss: 0.08476449052492778 Validation Loss: 0.7893197536468506\n",
      "Epoch 14633: Training Loss: 0.08410438646872838 Validation Loss: 0.7894324660301208\n",
      "Epoch 14634: Training Loss: 0.08444992949565251 Validation Loss: 0.78972327709198\n",
      "Epoch 14635: Training Loss: 0.08431621392567952 Validation Loss: 0.789203405380249\n",
      "Epoch 14636: Training Loss: 0.08412788311640422 Validation Loss: 0.7891960144042969\n",
      "Epoch 14637: Training Loss: 0.08417586237192154 Validation Loss: 0.7896343469619751\n",
      "Epoch 14638: Training Loss: 0.08389469981193542 Validation Loss: 0.7896913886070251\n",
      "Epoch 14639: Training Loss: 0.08432553708553314 Validation Loss: 0.7893726229667664\n",
      "Epoch 14640: Training Loss: 0.08398803199330966 Validation Loss: 0.7896857261657715\n",
      "Epoch 14641: Training Loss: 0.08435824016729991 Validation Loss: 0.7896436452865601\n",
      "Epoch 14642: Training Loss: 0.0846214269598325 Validation Loss: 0.7895954847335815\n",
      "Epoch 14643: Training Loss: 0.0840028425057729 Validation Loss: 0.7891753911972046\n",
      "Epoch 14644: Training Loss: 0.08397778620322545 Validation Loss: 0.7894554138183594\n",
      "Epoch 14645: Training Loss: 0.08454151699940364 Validation Loss: 0.78969407081604\n",
      "Epoch 14646: Training Loss: 0.0845456396540006 Validation Loss: 0.7892410159111023\n",
      "Epoch 14647: Training Loss: 0.08405892799297969 Validation Loss: 0.7896901965141296\n",
      "Epoch 14648: Training Loss: 0.08407437801361084 Validation Loss: 0.789659857749939\n",
      "Epoch 14649: Training Loss: 0.08393273502588272 Validation Loss: 0.7900904417037964\n",
      "Epoch 14650: Training Loss: 0.08379458387692769 Validation Loss: 0.7903354167938232\n",
      "Epoch 14651: Training Loss: 0.08463118722041447 Validation Loss: 0.7896542549133301\n",
      "Epoch 14652: Training Loss: 0.08409347136815389 Validation Loss: 0.789290189743042\n",
      "Epoch 14653: Training Loss: 0.08592600623766582 Validation Loss: 0.7893298864364624\n",
      "Epoch 14654: Training Loss: 0.08401028563578923 Validation Loss: 0.7898741364479065\n",
      "Epoch 14655: Training Loss: 0.08473975956439972 Validation Loss: 0.7904296517372131\n",
      "Epoch 14656: Training Loss: 0.08441919336716334 Validation Loss: 0.7904176712036133\n",
      "Epoch 14657: Training Loss: 0.08455060919125874 Validation Loss: 0.7902767062187195\n",
      "Epoch 14658: Training Loss: 0.0839826911687851 Validation Loss: 0.789701521396637\n",
      "Epoch 14659: Training Loss: 0.08420497924089432 Validation Loss: 0.7896620035171509\n",
      "Epoch 14660: Training Loss: 0.08501096069812775 Validation Loss: 0.7893686294555664\n",
      "Epoch 14661: Training Loss: 0.0841701477766037 Validation Loss: 0.789993166923523\n",
      "Epoch 14662: Training Loss: 0.0839679737885793 Validation Loss: 0.7901499271392822\n",
      "Epoch 14663: Training Loss: 0.08465762933095296 Validation Loss: 0.7904971837997437\n",
      "Epoch 14664: Training Loss: 0.08434628943602245 Validation Loss: 0.7901681065559387\n",
      "Epoch 14665: Training Loss: 0.08426498621702194 Validation Loss: 0.7900651693344116\n",
      "Epoch 14666: Training Loss: 0.0842230295141538 Validation Loss: 0.7897758483886719\n",
      "Epoch 14667: Training Loss: 0.08411446958780289 Validation Loss: 0.7898697853088379\n",
      "Epoch 14668: Training Loss: 0.08426245798667271 Validation Loss: 0.789825975894928\n",
      "Epoch 14669: Training Loss: 0.08440875882903735 Validation Loss: 0.7899232506752014\n",
      "Epoch 14670: Training Loss: 0.08384900291760762 Validation Loss: 0.7896798849105835\n",
      "Epoch 14671: Training Loss: 0.0843758409221967 Validation Loss: 0.7899991869926453\n",
      "Epoch 14672: Training Loss: 0.08411289254824321 Validation Loss: 0.7900627851486206\n",
      "Epoch 14673: Training Loss: 0.08413407454888026 Validation Loss: 0.7899448275566101\n",
      "Epoch 14674: Training Loss: 0.08398335427045822 Validation Loss: 0.79005366563797\n",
      "Epoch 14675: Training Loss: 0.08439953873554866 Validation Loss: 0.7899942994117737\n",
      "Epoch 14676: Training Loss: 0.08418408657113712 Validation Loss: 0.7900821566581726\n",
      "Epoch 14677: Training Loss: 0.08385318020979564 Validation Loss: 0.790351152420044\n",
      "Epoch 14678: Training Loss: 0.08467272917429607 Validation Loss: 0.790206253528595\n",
      "Epoch 14679: Training Loss: 0.08401533464590709 Validation Loss: 0.7902229428291321\n",
      "Epoch 14680: Training Loss: 0.08437015861272812 Validation Loss: 0.7903571724891663\n",
      "Epoch 14681: Training Loss: 0.08405741304159164 Validation Loss: 0.7900735139846802\n",
      "Epoch 14682: Training Loss: 0.08435695866743724 Validation Loss: 0.7900387048721313\n",
      "Epoch 14683: Training Loss: 0.08408290396134059 Validation Loss: 0.7901005148887634\n",
      "Epoch 14684: Training Loss: 0.08481118579705556 Validation Loss: 0.7901731729507446\n",
      "Epoch 14685: Training Loss: 0.08507965256770451 Validation Loss: 0.7900556325912476\n",
      "Epoch 14686: Training Loss: 0.08435955892006557 Validation Loss: 0.7900447845458984\n",
      "Epoch 14687: Training Loss: 0.08420423169930775 Validation Loss: 0.790062427520752\n",
      "Epoch 14688: Training Loss: 0.08439481010039647 Validation Loss: 0.7899952530860901\n",
      "Epoch 14689: Training Loss: 0.08428071935971577 Validation Loss: 0.7895447015762329\n",
      "Epoch 14690: Training Loss: 0.0840600257118543 Validation Loss: 0.7896426916122437\n",
      "Epoch 14691: Training Loss: 0.08498043442765872 Validation Loss: 0.7899711728096008\n",
      "Epoch 14692: Training Loss: 0.0840333104133606 Validation Loss: 0.7901244759559631\n",
      "Epoch 14693: Training Loss: 0.0837623452146848 Validation Loss: 0.7900814414024353\n",
      "Epoch 14694: Training Loss: 0.08492104957501094 Validation Loss: 0.7902265191078186\n",
      "Epoch 14695: Training Loss: 0.08417865385611852 Validation Loss: 0.7902613878250122\n",
      "Epoch 14696: Training Loss: 0.08436843504508336 Validation Loss: 0.7901791930198669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14697: Training Loss: 0.08401954174041748 Validation Loss: 0.7898522615432739\n",
      "Epoch 14698: Training Loss: 0.08436873306830724 Validation Loss: 0.7894845008850098\n",
      "Epoch 14699: Training Loss: 0.084341694911321 Validation Loss: 0.7899808287620544\n",
      "Epoch 14700: Training Loss: 0.0846173365910848 Validation Loss: 0.790522575378418\n",
      "Epoch 14701: Training Loss: 0.08423776427904765 Validation Loss: 0.7907114028930664\n",
      "Epoch 14702: Training Loss: 0.0842142105102539 Validation Loss: 0.7911213040351868\n",
      "Epoch 14703: Training Loss: 0.08414179335037868 Validation Loss: 0.7904083132743835\n",
      "Epoch 14704: Training Loss: 0.083961288134257 Validation Loss: 0.7893303632736206\n",
      "Epoch 14705: Training Loss: 0.08397355924050014 Validation Loss: 0.7896798849105835\n",
      "Epoch 14706: Training Loss: 0.08464345832665761 Validation Loss: 0.7894500494003296\n",
      "Epoch 14707: Training Loss: 0.08414241919914882 Validation Loss: 0.7901519536972046\n",
      "Epoch 14708: Training Loss: 0.08413832137982051 Validation Loss: 0.7908037304878235\n",
      "Epoch 14709: Training Loss: 0.08419232318798701 Validation Loss: 0.7902268171310425\n",
      "Epoch 14710: Training Loss: 0.08418458203474681 Validation Loss: 0.7898696660995483\n",
      "Epoch 14711: Training Loss: 0.083888774116834 Validation Loss: 0.7897418141365051\n",
      "Epoch 14712: Training Loss: 0.08394763618707657 Validation Loss: 0.7896206378936768\n",
      "Epoch 14713: Training Loss: 0.0836638833085696 Validation Loss: 0.7898926734924316\n",
      "Epoch 14714: Training Loss: 0.08366125822067261 Validation Loss: 0.7900483012199402\n",
      "Epoch 14715: Training Loss: 0.0840187097589175 Validation Loss: 0.7901782393455505\n",
      "Epoch 14716: Training Loss: 0.0838854710261027 Validation Loss: 0.790601372718811\n",
      "Epoch 14717: Training Loss: 0.08425909529129665 Validation Loss: 0.7911624908447266\n",
      "Epoch 14718: Training Loss: 0.08402388542890549 Validation Loss: 0.7904645800590515\n",
      "Epoch 14719: Training Loss: 0.0844927504658699 Validation Loss: 0.7899757623672485\n",
      "Epoch 14720: Training Loss: 0.08371090392271678 Validation Loss: 0.7898546457290649\n",
      "Epoch 14721: Training Loss: 0.08374324937661488 Validation Loss: 0.7899158596992493\n",
      "Epoch 14722: Training Loss: 0.08458300928274791 Validation Loss: 0.79031902551651\n",
      "Epoch 14723: Training Loss: 0.08389940609534581 Validation Loss: 0.7907060980796814\n",
      "Epoch 14724: Training Loss: 0.0841482604543368 Validation Loss: 0.7909525632858276\n",
      "Epoch 14725: Training Loss: 0.08448858559131622 Validation Loss: 0.7902655005455017\n",
      "Epoch 14726: Training Loss: 0.08418138821919759 Validation Loss: 0.7898272275924683\n",
      "Epoch 14727: Training Loss: 0.08450482537349065 Validation Loss: 0.7897896766662598\n",
      "Epoch 14728: Training Loss: 0.08363505452871323 Validation Loss: 0.7901751399040222\n",
      "Epoch 14729: Training Loss: 0.08394911388556163 Validation Loss: 0.7906136512756348\n",
      "Epoch 14730: Training Loss: 0.0839318260550499 Validation Loss: 0.7905828952789307\n",
      "Epoch 14731: Training Loss: 0.08405314882596333 Validation Loss: 0.7904265522956848\n",
      "Epoch 14732: Training Loss: 0.08408216387033463 Validation Loss: 0.7902862429618835\n",
      "Epoch 14733: Training Loss: 0.08408488084872563 Validation Loss: 0.7903887629508972\n",
      "Epoch 14734: Training Loss: 0.08408304303884506 Validation Loss: 0.7904408574104309\n",
      "Epoch 14735: Training Loss: 0.08437323570251465 Validation Loss: 0.7903837561607361\n",
      "Epoch 14736: Training Loss: 0.08396839102109273 Validation Loss: 0.7907384037971497\n",
      "Epoch 14737: Training Loss: 0.08376665165026982 Validation Loss: 0.7910497784614563\n",
      "Epoch 14738: Training Loss: 0.08454189201196034 Validation Loss: 0.7910988330841064\n",
      "Epoch 14739: Training Loss: 0.08415561666091283 Validation Loss: 0.7905063629150391\n",
      "Epoch 14740: Training Loss: 0.084433913230896 Validation Loss: 0.7897047400474548\n",
      "Epoch 14741: Training Loss: 0.08440119524796803 Validation Loss: 0.7899060845375061\n",
      "Epoch 14742: Training Loss: 0.08376486599445343 Validation Loss: 0.7900280952453613\n",
      "Epoch 14743: Training Loss: 0.08383041620254517 Validation Loss: 0.790168285369873\n",
      "Epoch 14744: Training Loss: 0.08411219467719395 Validation Loss: 0.7906015515327454\n",
      "Epoch 14745: Training Loss: 0.08394748717546463 Validation Loss: 0.7909187078475952\n",
      "Epoch 14746: Training Loss: 0.08366074413061142 Validation Loss: 0.7905148267745972\n",
      "Epoch 14747: Training Loss: 0.08420706540346146 Validation Loss: 0.7898038625717163\n",
      "Epoch 14748: Training Loss: 0.08414845168590546 Validation Loss: 0.7896856069564819\n",
      "Epoch 14749: Training Loss: 0.08374433716138203 Validation Loss: 0.7902635335922241\n",
      "Epoch 14750: Training Loss: 0.08381011585394542 Validation Loss: 0.7902808785438538\n",
      "Epoch 14751: Training Loss: 0.08413932224114735 Validation Loss: 0.7905707955360413\n",
      "Epoch 14752: Training Loss: 0.08392670005559921 Validation Loss: 0.7904413938522339\n",
      "Epoch 14753: Training Loss: 0.08390073478221893 Validation Loss: 0.790554940700531\n",
      "Epoch 14754: Training Loss: 0.08453880002101262 Validation Loss: 0.7897673845291138\n",
      "Epoch 14755: Training Loss: 0.0840506578485171 Validation Loss: 0.789826512336731\n",
      "Epoch 14756: Training Loss: 0.08345908671617508 Validation Loss: 0.7899028658866882\n",
      "Epoch 14757: Training Loss: 0.08418305466572444 Validation Loss: 0.7902462482452393\n",
      "Epoch 14758: Training Loss: 0.08395054936408997 Validation Loss: 0.7901305556297302\n",
      "Epoch 14759: Training Loss: 0.08340848237276077 Validation Loss: 0.7904184460639954\n",
      "Epoch 14760: Training Loss: 0.08390681942303975 Validation Loss: 0.7907233834266663\n",
      "Epoch 14761: Training Loss: 0.08388034999370575 Validation Loss: 0.790412425994873\n",
      "Epoch 14762: Training Loss: 0.08358929802974065 Validation Loss: 0.7910833358764648\n",
      "Epoch 14763: Training Loss: 0.0838086207707723 Validation Loss: 0.7909142971038818\n",
      "Epoch 14764: Training Loss: 0.08395368854204814 Validation Loss: 0.7909138798713684\n",
      "Epoch 14765: Training Loss: 0.08404748638470967 Validation Loss: 0.7911396622657776\n",
      "Epoch 14766: Training Loss: 0.08426305403312047 Validation Loss: 0.7908399105072021\n",
      "Epoch 14767: Training Loss: 0.08411682397127151 Validation Loss: 0.7906516194343567\n",
      "Epoch 14768: Training Loss: 0.08363215625286102 Validation Loss: 0.7903247475624084\n",
      "Epoch 14769: Training Loss: 0.0838432436188062 Validation Loss: 0.7905499339103699\n",
      "Epoch 14770: Training Loss: 0.0843003938595454 Validation Loss: 0.790939211845398\n",
      "Epoch 14771: Training Loss: 0.08387958506743114 Validation Loss: 0.7913663983345032\n",
      "Epoch 14772: Training Loss: 0.08387985825538635 Validation Loss: 0.7915388941764832\n",
      "Epoch 14773: Training Loss: 0.08396622041861217 Validation Loss: 0.7908657193183899\n",
      "Epoch 14774: Training Loss: 0.08404970665772755 Validation Loss: 0.7905833125114441\n",
      "Epoch 14775: Training Loss: 0.08400025218725204 Validation Loss: 0.7905392050743103\n",
      "Epoch 14776: Training Loss: 0.08400784681240718 Validation Loss: 0.7903977632522583\n",
      "Epoch 14777: Training Loss: 0.08376347770293553 Validation Loss: 0.7908849120140076\n",
      "Epoch 14778: Training Loss: 0.08381725599368413 Validation Loss: 0.7914171814918518\n",
      "Epoch 14779: Training Loss: 0.08363681038220723 Validation Loss: 0.7909478545188904\n",
      "Epoch 14780: Training Loss: 0.08388977497816086 Validation Loss: 0.7909244894981384\n",
      "Epoch 14781: Training Loss: 0.08380019664764404 Validation Loss: 0.7904918193817139\n",
      "Epoch 14782: Training Loss: 0.08411275098721187 Validation Loss: 0.7901949882507324\n",
      "Epoch 14783: Training Loss: 0.08390467365582784 Validation Loss: 0.7901726365089417\n",
      "Epoch 14784: Training Loss: 0.08382806430260341 Validation Loss: 0.7902142405509949\n",
      "Epoch 14785: Training Loss: 0.08378884692986806 Validation Loss: 0.790778636932373\n",
      "Epoch 14786: Training Loss: 0.08347214261690776 Validation Loss: 0.7908257246017456\n",
      "Epoch 14787: Training Loss: 0.08374473204215367 Validation Loss: 0.791000247001648\n",
      "Epoch 14788: Training Loss: 0.08403344452381134 Validation Loss: 0.7910298705101013\n",
      "Epoch 14789: Training Loss: 0.08393169691165288 Validation Loss: 0.7906296253204346\n",
      "Epoch 14790: Training Loss: 0.08370181669791539 Validation Loss: 0.7904080748558044\n",
      "Epoch 14791: Training Loss: 0.08406737943490346 Validation Loss: 0.7904064059257507\n",
      "Epoch 14792: Training Loss: 0.08409210791190465 Validation Loss: 0.790509819984436\n",
      "Epoch 14793: Training Loss: 0.08328819026549657 Validation Loss: 0.7908168435096741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14794: Training Loss: 0.08401954670747121 Validation Loss: 0.7909708023071289\n",
      "Epoch 14795: Training Loss: 0.08427160481611888 Validation Loss: 0.7912390232086182\n",
      "Epoch 14796: Training Loss: 0.08378654470046361 Validation Loss: 0.7912914752960205\n",
      "Epoch 14797: Training Loss: 0.08396054804325104 Validation Loss: 0.7907359600067139\n",
      "Epoch 14798: Training Loss: 0.08377251774072647 Validation Loss: 0.7904557585716248\n",
      "Epoch 14799: Training Loss: 0.08373460918664932 Validation Loss: 0.7907143235206604\n",
      "Epoch 14800: Training Loss: 0.08381718148787816 Validation Loss: 0.7913063168525696\n",
      "Epoch 14801: Training Loss: 0.08374432474374771 Validation Loss: 0.7913651466369629\n",
      "Epoch 14802: Training Loss: 0.08488442003726959 Validation Loss: 0.7918769121170044\n",
      "Epoch 14803: Training Loss: 0.08497470368941624 Validation Loss: 0.7917439341545105\n",
      "Epoch 14804: Training Loss: 0.08391811698675156 Validation Loss: 0.7910709381103516\n",
      "Epoch 14805: Training Loss: 0.08417130510012309 Validation Loss: 0.790238082408905\n",
      "Epoch 14806: Training Loss: 0.08438786367575328 Validation Loss: 0.7907617092132568\n",
      "Epoch 14807: Training Loss: 0.0836417203148206 Validation Loss: 0.7908903956413269\n",
      "Epoch 14808: Training Loss: 0.08390437066555023 Validation Loss: 0.7907251119613647\n",
      "Epoch 14809: Training Loss: 0.08366193373998006 Validation Loss: 0.7907423973083496\n",
      "Epoch 14810: Training Loss: 0.08440399294098218 Validation Loss: 0.7914353609085083\n",
      "Epoch 14811: Training Loss: 0.08476417139172554 Validation Loss: 0.790849506855011\n",
      "Epoch 14812: Training Loss: 0.08450923363367717 Validation Loss: 0.791010320186615\n",
      "Epoch 14813: Training Loss: 0.08371492971976598 Validation Loss: 0.79079669713974\n",
      "Epoch 14814: Training Loss: 0.0839261735479037 Validation Loss: 0.791094958782196\n",
      "Epoch 14815: Training Loss: 0.0835811917980512 Validation Loss: 0.7909791469573975\n",
      "Epoch 14816: Training Loss: 0.08343961586554845 Validation Loss: 0.7913055419921875\n",
      "Epoch 14817: Training Loss: 0.08394091079632442 Validation Loss: 0.7912039756774902\n",
      "Epoch 14818: Training Loss: 0.08364084859689076 Validation Loss: 0.7910934686660767\n",
      "Epoch 14819: Training Loss: 0.08377418915430705 Validation Loss: 0.7913637757301331\n",
      "Epoch 14820: Training Loss: 0.08373209585746129 Validation Loss: 0.791273295879364\n",
      "Epoch 14821: Training Loss: 0.08362942934036255 Validation Loss: 0.7915679216384888\n",
      "Epoch 14822: Training Loss: 0.08374562362829845 Validation Loss: 0.7911328077316284\n",
      "Epoch 14823: Training Loss: 0.08352924386660258 Validation Loss: 0.7910800576210022\n",
      "Epoch 14824: Training Loss: 0.08451760684450467 Validation Loss: 0.7907945513725281\n",
      "Epoch 14825: Training Loss: 0.08355535318454106 Validation Loss: 0.7908673882484436\n",
      "Epoch 14826: Training Loss: 0.08370569845040639 Validation Loss: 0.7908971309661865\n",
      "Epoch 14827: Training Loss: 0.08361673603455226 Validation Loss: 0.7909567952156067\n",
      "Epoch 14828: Training Loss: 0.08375640213489532 Validation Loss: 0.7907743453979492\n",
      "Epoch 14829: Training Loss: 0.08412400260567665 Validation Loss: 0.7910997867584229\n",
      "Epoch 14830: Training Loss: 0.0837575023372968 Validation Loss: 0.7912738919258118\n",
      "Epoch 14831: Training Loss: 0.08381063242753346 Validation Loss: 0.7910888195037842\n",
      "Epoch 14832: Training Loss: 0.08390537152687709 Validation Loss: 0.7907794117927551\n",
      "Epoch 14833: Training Loss: 0.08393565565347672 Validation Loss: 0.7911115884780884\n",
      "Epoch 14834: Training Loss: 0.08380831778049469 Validation Loss: 0.7911320924758911\n",
      "Epoch 14835: Training Loss: 0.0837177187204361 Validation Loss: 0.7911403775215149\n",
      "Epoch 14836: Training Loss: 0.08372343579928081 Validation Loss: 0.7912600636482239\n",
      "Epoch 14837: Training Loss: 0.08351642141739528 Validation Loss: 0.7912874221801758\n",
      "Epoch 14838: Training Loss: 0.08523224045832951 Validation Loss: 0.7915536165237427\n",
      "Epoch 14839: Training Loss: 0.08348032832145691 Validation Loss: 0.791513204574585\n",
      "Epoch 14840: Training Loss: 0.08362094312906265 Validation Loss: 0.7921332120895386\n",
      "Epoch 14841: Training Loss: 0.0837994987765948 Validation Loss: 0.7912712693214417\n",
      "Epoch 14842: Training Loss: 0.08355585982402165 Validation Loss: 0.7914177179336548\n",
      "Epoch 14843: Training Loss: 0.08361331870158513 Validation Loss: 0.7911214232444763\n",
      "Epoch 14844: Training Loss: 0.08351704229911168 Validation Loss: 0.7912474274635315\n",
      "Epoch 14845: Training Loss: 0.08382624387741089 Validation Loss: 0.7914415597915649\n",
      "Epoch 14846: Training Loss: 0.08363751322031021 Validation Loss: 0.7923970222473145\n",
      "Epoch 14847: Training Loss: 0.08371164401372273 Validation Loss: 0.7921649813652039\n",
      "Epoch 14848: Training Loss: 0.08396603415409724 Validation Loss: 0.7918004989624023\n",
      "Epoch 14849: Training Loss: 0.08362556497255962 Validation Loss: 0.7913032174110413\n",
      "Epoch 14850: Training Loss: 0.08372349043687184 Validation Loss: 0.7910674214363098\n",
      "Epoch 14851: Training Loss: 0.083653358121713 Validation Loss: 0.791444718837738\n",
      "Epoch 14852: Training Loss: 0.08358881870905559 Validation Loss: 0.7910184860229492\n",
      "Epoch 14853: Training Loss: 0.08361913512150447 Validation Loss: 0.7913937568664551\n",
      "Epoch 14854: Training Loss: 0.08404206732908885 Validation Loss: 0.7918516993522644\n",
      "Epoch 14855: Training Loss: 0.08535336206356685 Validation Loss: 0.7919920086860657\n",
      "Epoch 14856: Training Loss: 0.0834781676530838 Validation Loss: 0.7910670638084412\n",
      "Epoch 14857: Training Loss: 0.08355200290679932 Validation Loss: 0.7906780242919922\n",
      "Epoch 14858: Training Loss: 0.08362241337696712 Validation Loss: 0.7906175851821899\n",
      "Epoch 14859: Training Loss: 0.0835271676381429 Validation Loss: 0.7907887697219849\n",
      "Epoch 14860: Training Loss: 0.08361261089642842 Validation Loss: 0.7909979224205017\n",
      "Epoch 14861: Training Loss: 0.0838002289334933 Validation Loss: 0.791703462600708\n",
      "Epoch 14862: Training Loss: 0.08371951927741368 Validation Loss: 0.7926161289215088\n",
      "Epoch 14863: Training Loss: 0.0835493157307307 Validation Loss: 0.7922587394714355\n",
      "Epoch 14864: Training Loss: 0.08338924249013265 Validation Loss: 0.7911096215248108\n",
      "Epoch 14865: Training Loss: 0.08331874012947083 Validation Loss: 0.7908104062080383\n",
      "Epoch 14866: Training Loss: 0.0842110849916935 Validation Loss: 0.7906847596168518\n",
      "Epoch 14867: Training Loss: 0.08358872433503468 Validation Loss: 0.7912365198135376\n",
      "Epoch 14868: Training Loss: 0.08345072468121846 Validation Loss: 0.7911403179168701\n",
      "Epoch 14869: Training Loss: 0.08392686148484547 Validation Loss: 0.7911161780357361\n",
      "Epoch 14870: Training Loss: 0.08363484342892964 Validation Loss: 0.7910224795341492\n",
      "Epoch 14871: Training Loss: 0.08420614401499431 Validation Loss: 0.7912893891334534\n",
      "Epoch 14872: Training Loss: 0.08352911720673244 Validation Loss: 0.7912584543228149\n",
      "Epoch 14873: Training Loss: 0.08364350845416386 Validation Loss: 0.7914103269577026\n",
      "Epoch 14874: Training Loss: 0.08397376288970311 Validation Loss: 0.7912532091140747\n",
      "Epoch 14875: Training Loss: 0.08448509375254314 Validation Loss: 0.7913979887962341\n",
      "Epoch 14876: Training Loss: 0.08297431220610936 Validation Loss: 0.7913583517074585\n",
      "Epoch 14877: Training Loss: 0.08354192972183228 Validation Loss: 0.7914395928382874\n",
      "Epoch 14878: Training Loss: 0.0837265948454539 Validation Loss: 0.7911102771759033\n",
      "Epoch 14879: Training Loss: 0.08377428849538167 Validation Loss: 0.791488528251648\n",
      "Epoch 14880: Training Loss: 0.0840427428483963 Validation Loss: 0.7911868691444397\n",
      "Epoch 14881: Training Loss: 0.0838662659128507 Validation Loss: 0.7909384965896606\n",
      "Epoch 14882: Training Loss: 0.08422890553871791 Validation Loss: 0.790894091129303\n",
      "Epoch 14883: Training Loss: 0.08338793615500133 Validation Loss: 0.7913798093795776\n",
      "Epoch 14884: Training Loss: 0.08419832587242126 Validation Loss: 0.7924118041992188\n",
      "Epoch 14885: Training Loss: 0.08355140934387843 Validation Loss: 0.7921574711799622\n",
      "Epoch 14886: Training Loss: 0.08358804136514664 Validation Loss: 0.7916802167892456\n",
      "Epoch 14887: Training Loss: 0.0834502453605334 Validation Loss: 0.79115891456604\n",
      "Epoch 14888: Training Loss: 0.08388748889168103 Validation Loss: 0.7910709977149963\n",
      "Epoch 14889: Training Loss: 0.0835237850745519 Validation Loss: 0.7915371060371399\n",
      "Epoch 14890: Training Loss: 0.08366333444913228 Validation Loss: 0.791443943977356\n",
      "Epoch 14891: Training Loss: 0.08518868933121364 Validation Loss: 0.7916054725646973\n",
      "Epoch 14892: Training Loss: 0.0836764673391978 Validation Loss: 0.7917966246604919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14893: Training Loss: 0.08365948001543681 Validation Loss: 0.7915828824043274\n",
      "Epoch 14894: Training Loss: 0.08346207688252132 Validation Loss: 0.7915189266204834\n",
      "Epoch 14895: Training Loss: 0.08338585495948792 Validation Loss: 0.7919602394104004\n",
      "Epoch 14896: Training Loss: 0.08402787148952484 Validation Loss: 0.7922428846359253\n",
      "Epoch 14897: Training Loss: 0.08359065403540929 Validation Loss: 0.7923917174339294\n",
      "Epoch 14898: Training Loss: 0.083360160390536 Validation Loss: 0.7922154068946838\n",
      "Epoch 14899: Training Loss: 0.08362385630607605 Validation Loss: 0.7920441031455994\n",
      "Epoch 14900: Training Loss: 0.08435828735431035 Validation Loss: 0.7913820743560791\n",
      "Epoch 14901: Training Loss: 0.08314357449611028 Validation Loss: 0.7917908430099487\n",
      "Epoch 14902: Training Loss: 0.08440675338109334 Validation Loss: 0.7915835976600647\n",
      "Epoch 14903: Training Loss: 0.08353390793005626 Validation Loss: 0.7918387055397034\n",
      "Epoch 14904: Training Loss: 0.08344022184610367 Validation Loss: 0.7921614050865173\n",
      "Epoch 14905: Training Loss: 0.08323429524898529 Validation Loss: 0.792256772518158\n",
      "Epoch 14906: Training Loss: 0.0833009456594785 Validation Loss: 0.7920969724655151\n",
      "Epoch 14907: Training Loss: 0.08380385984977086 Validation Loss: 0.7921022176742554\n",
      "Epoch 14908: Training Loss: 0.08374134699503581 Validation Loss: 0.7919972538948059\n",
      "Epoch 14909: Training Loss: 0.08355349798997243 Validation Loss: 0.792352020740509\n",
      "Epoch 14910: Training Loss: 0.08342007795969646 Validation Loss: 0.7919796705245972\n",
      "Epoch 14911: Training Loss: 0.08450948943694432 Validation Loss: 0.7918446660041809\n",
      "Epoch 14912: Training Loss: 0.08328869690497716 Validation Loss: 0.7914894819259644\n",
      "Epoch 14913: Training Loss: 0.08345309396584828 Validation Loss: 0.7914831042289734\n",
      "Epoch 14914: Training Loss: 0.08357040832440059 Validation Loss: 0.7915829420089722\n",
      "Epoch 14915: Training Loss: 0.08332968999942143 Validation Loss: 0.7913259863853455\n",
      "Epoch 14916: Training Loss: 0.08346550911664963 Validation Loss: 0.7913393378257751\n",
      "Epoch 14917: Training Loss: 0.08393387123942375 Validation Loss: 0.7916144728660583\n",
      "Epoch 14918: Training Loss: 0.08392863720655441 Validation Loss: 0.7917962670326233\n",
      "Epoch 14919: Training Loss: 0.08373086899518967 Validation Loss: 0.792255163192749\n",
      "Epoch 14920: Training Loss: 0.08325928697983424 Validation Loss: 0.7921873331069946\n",
      "Epoch 14921: Training Loss: 0.08395521839459737 Validation Loss: 0.7922805547714233\n",
      "Epoch 14922: Training Loss: 0.08336673676967621 Validation Loss: 0.7915332317352295\n",
      "Epoch 14923: Training Loss: 0.08354413509368896 Validation Loss: 0.7915352582931519\n",
      "Epoch 14924: Training Loss: 0.08410909151037534 Validation Loss: 0.7916389107704163\n",
      "Epoch 14925: Training Loss: 0.08351784199476242 Validation Loss: 0.7916662096977234\n",
      "Epoch 14926: Training Loss: 0.08371241639057796 Validation Loss: 0.792177677154541\n",
      "Epoch 14927: Training Loss: 0.08340698480606079 Validation Loss: 0.7926045060157776\n",
      "Epoch 14928: Training Loss: 0.08347087353467941 Validation Loss: 0.7926492094993591\n",
      "Epoch 14929: Training Loss: 0.08353420346975327 Validation Loss: 0.7926126718521118\n",
      "Epoch 14930: Training Loss: 0.08333474149306615 Validation Loss: 0.791814923286438\n",
      "Epoch 14931: Training Loss: 0.08356486757596333 Validation Loss: 0.7912073731422424\n",
      "Epoch 14932: Training Loss: 0.08339436848958333 Validation Loss: 0.7910149097442627\n",
      "Epoch 14933: Training Loss: 0.08362060536940892 Validation Loss: 0.7913441061973572\n",
      "Epoch 14934: Training Loss: 0.08353885759909947 Validation Loss: 0.7916029691696167\n",
      "Epoch 14935: Training Loss: 0.08335675299167633 Validation Loss: 0.7920199632644653\n",
      "Epoch 14936: Training Loss: 0.08333365619182587 Validation Loss: 0.7921116948127747\n",
      "Epoch 14937: Training Loss: 0.08395627389351527 Validation Loss: 0.792334258556366\n",
      "Epoch 14938: Training Loss: 0.08292574683825175 Validation Loss: 0.7926744222640991\n",
      "Epoch 14939: Training Loss: 0.08409613122542699 Validation Loss: 0.7926110029220581\n",
      "Epoch 14940: Training Loss: 0.08415449659029643 Validation Loss: 0.792252779006958\n",
      "Epoch 14941: Training Loss: 0.08342992514371872 Validation Loss: 0.7917289733886719\n",
      "Epoch 14942: Training Loss: 0.08331464727719624 Validation Loss: 0.7918959856033325\n",
      "Epoch 14943: Training Loss: 0.08384181559085846 Validation Loss: 0.7918276190757751\n",
      "Epoch 14944: Training Loss: 0.08351536343495052 Validation Loss: 0.7921147346496582\n",
      "Epoch 14945: Training Loss: 0.08345116178194682 Validation Loss: 0.7918079495429993\n",
      "Epoch 14946: Training Loss: 0.08328130344549815 Validation Loss: 0.7919333577156067\n",
      "Epoch 14947: Training Loss: 0.08345254013935725 Validation Loss: 0.7919413447380066\n",
      "Epoch 14948: Training Loss: 0.08323827137549718 Validation Loss: 0.7917490005493164\n",
      "Epoch 14949: Training Loss: 0.08349296698967616 Validation Loss: 0.7918313145637512\n",
      "Epoch 14950: Training Loss: 0.08324161916971207 Validation Loss: 0.7921070456504822\n",
      "Epoch 14951: Training Loss: 0.08390066275993983 Validation Loss: 0.7923263907432556\n",
      "Epoch 14952: Training Loss: 0.08325236787398656 Validation Loss: 0.7917954325675964\n",
      "Epoch 14953: Training Loss: 0.08345130831003189 Validation Loss: 0.7917003035545349\n",
      "Epoch 14954: Training Loss: 0.08371904989083608 Validation Loss: 0.7915919423103333\n",
      "Epoch 14955: Training Loss: 0.08292676260073979 Validation Loss: 0.7918100953102112\n",
      "Epoch 14956: Training Loss: 0.08350565532843272 Validation Loss: 0.7919731736183167\n",
      "Epoch 14957: Training Loss: 0.08348127951224645 Validation Loss: 0.7922443747520447\n",
      "Epoch 14958: Training Loss: 0.08334511270125707 Validation Loss: 0.7923794388771057\n",
      "Epoch 14959: Training Loss: 0.08326564729213715 Validation Loss: 0.7924671769142151\n",
      "Epoch 14960: Training Loss: 0.0833132416009903 Validation Loss: 0.7922282814979553\n",
      "Epoch 14961: Training Loss: 0.08451468497514725 Validation Loss: 0.7923438549041748\n",
      "Epoch 14962: Training Loss: 0.08316037555535634 Validation Loss: 0.7916311621665955\n",
      "Epoch 14963: Training Loss: 0.08320192247629166 Validation Loss: 0.7919400334358215\n",
      "Epoch 14964: Training Loss: 0.08291736741860707 Validation Loss: 0.7920946478843689\n",
      "Epoch 14965: Training Loss: 0.08303056160608928 Validation Loss: 0.7925195693969727\n",
      "Epoch 14966: Training Loss: 0.0837364395459493 Validation Loss: 0.7931841611862183\n",
      "Epoch 14967: Training Loss: 0.08317216237386067 Validation Loss: 0.7931913733482361\n",
      "Epoch 14968: Training Loss: 0.08351847281058629 Validation Loss: 0.7927766442298889\n",
      "Epoch 14969: Training Loss: 0.0838295469681422 Validation Loss: 0.7920078039169312\n",
      "Epoch 14970: Training Loss: 0.08346348255872726 Validation Loss: 0.7923715114593506\n",
      "Epoch 14971: Training Loss: 0.08379920820395152 Validation Loss: 0.7924423813819885\n",
      "Epoch 14972: Training Loss: 0.08456960196296374 Validation Loss: 0.7926241159439087\n",
      "Epoch 14973: Training Loss: 0.08370933681726456 Validation Loss: 0.7925810217857361\n",
      "Epoch 14974: Training Loss: 0.084224633872509 Validation Loss: 0.7925143241882324\n",
      "Epoch 14975: Training Loss: 0.08328744769096375 Validation Loss: 0.792765736579895\n",
      "Epoch 14976: Training Loss: 0.08350506176551183 Validation Loss: 0.7927873134613037\n",
      "Epoch 14977: Training Loss: 0.0834490458170573 Validation Loss: 0.7927888035774231\n",
      "Epoch 14978: Training Loss: 0.08319342136383057 Validation Loss: 0.7928023934364319\n",
      "Epoch 14979: Training Loss: 0.08326360334952672 Validation Loss: 0.7923083901405334\n",
      "Epoch 14980: Training Loss: 0.08352429668108623 Validation Loss: 0.7918232679367065\n",
      "Epoch 14981: Training Loss: 0.08345109472672145 Validation Loss: 0.7922067642211914\n",
      "Epoch 14982: Training Loss: 0.08339246362447739 Validation Loss: 0.7925794720649719\n",
      "Epoch 14983: Training Loss: 0.08314022173484166 Validation Loss: 0.792919397354126\n",
      "Epoch 14984: Training Loss: 0.08323973913987477 Validation Loss: 0.7928351759910583\n",
      "Epoch 14985: Training Loss: 0.08314465979735057 Validation Loss: 0.7927519083023071\n",
      "Epoch 14986: Training Loss: 0.08320922156174977 Validation Loss: 0.7921915650367737\n",
      "Epoch 14987: Training Loss: 0.08333667119344075 Validation Loss: 0.7920779585838318\n",
      "Epoch 14988: Training Loss: 0.08369037260611852 Validation Loss: 0.7924802303314209\n",
      "Epoch 14989: Training Loss: 0.08315879106521606 Validation Loss: 0.7928499579429626\n",
      "Epoch 14990: Training Loss: 0.08328050623337428 Validation Loss: 0.7928489446640015\n",
      "Epoch 14991: Training Loss: 0.08310649047295253 Validation Loss: 0.7925133109092712\n",
      "Epoch 14992: Training Loss: 0.0835478703180949 Validation Loss: 0.7927759289741516\n",
      "Epoch 14993: Training Loss: 0.08331624666849773 Validation Loss: 0.792938768863678\n",
      "Epoch 14994: Training Loss: 0.08346476405858994 Validation Loss: 0.7925985455513\n",
      "Epoch 14995: Training Loss: 0.08312708636124928 Validation Loss: 0.7926285862922668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14996: Training Loss: 0.08362149198849995 Validation Loss: 0.7924947738647461\n",
      "Epoch 14997: Training Loss: 0.08338791131973267 Validation Loss: 0.7928057909011841\n",
      "Epoch 14998: Training Loss: 0.08330820004145305 Validation Loss: 0.793096125125885\n",
      "Epoch 14999: Training Loss: 0.08338627715905507 Validation Loss: 0.7927207946777344\n",
      "Epoch 15000: Training Loss: 0.08314133435487747 Validation Loss: 0.7923822999000549\n"
     ]
    }
   ],
   "source": [
    "fit(Xtra, ytra, Xtes, ytes, net, optimizer, criterion, n_epochs, n_batches, device, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data\\\\model_checkpoint.pt'\n",
    "device = torch.device('cpu')\n",
    "net = Net1()\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.700065016746521\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    inputs = torch.FloatTensor(Xtes)\n",
    "    labels = torch.tensor(ytes, dtype=torch.long)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net.forward(inputs)\n",
    "    loss = error(outputs, labels) \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.700065016746521"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.700065016746521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "for v,p in enumerate(net.parameters()):\n",
    "    param_dict[v] = p.data.numpy()\n",
    "    \n",
    "W0 = param_dict[0].T\n",
    "b0 = param_dict[1]\n",
    "W1 = param_dict[2].T\n",
    "b1 = param_dict[3]\n",
    "\n",
    "\n",
    "h0 = np.matmul(Xtes, W0) + b0\n",
    "h1 = np.tanh(h0)\n",
    "h2 = np.matmul(h1, W1) + b1\n",
    "h3 = np.exp(h2)\n",
    "o = h3/np.sum(h3,axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data\\\\param_dict.npy', param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(prob, topics):    \n",
    "    return topics[np.argmax(prob)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "topics = ['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
    "for c in o:\n",
    "    pred_labels.append(y_hat(c, topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = data.Pregunta[btes]\n",
    "labels = data.Tema[btes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtes = np.column_stack((preguntas, labels, pred_labels, Xtes, o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(Xtes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load('Data\\\\vocab.npy', allow_pickle=True)\n",
    "colnames = np.concatenate((['pregunta', 'label', 'pred_label'], vocab, ['n_token', 'perc_greet', 'polarity'], ['Jubilacion Patronal', 'Consultoria', \\\n",
    "                                                                                                     'Renuncia/Despido/Desahucio', 'IESS', 'Greeting', \\\n",
    "                                                                                                         'Contacto', 'No Topic', 'Queja', 'Otros servicios', \\\n",
    "                                                                                                 'Charlas/Capacitaciones', 'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']))\n",
    "data2.columns = colnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregunta</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>buen</th>\n",
       "      <th>dia</th>\n",
       "      <th>mi</th>\n",
       "      <th>nombr</th>\n",
       "      <th>llam</th>\n",
       "      <th>empres</th>\n",
       "      <th>me</th>\n",
       "      <th>...</th>\n",
       "      <th>IESS</th>\n",
       "      <th>Greeting</th>\n",
       "      <th>Contacto</th>\n",
       "      <th>No Topic</th>\n",
       "      <th>Queja</th>\n",
       "      <th>Otros servicios</th>\n",
       "      <th>Charlas/Capacitaciones</th>\n",
       "      <th>Hi Five</th>\n",
       "      <th>job seeker</th>\n",
       "      <th>Facturacion/Retencion/Cobros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hay posiciones abiertas en ACTUARIA?</td>\n",
       "      <td>job seeker</td>\n",
       "      <td>Contacto</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0117327</td>\n",
       "      <td>0.056362</td>\n",
       "      <td>0.402604</td>\n",
       "      <td>0.068683</td>\n",
       "      <td>0.0550525</td>\n",
       "      <td>0.01466</td>\n",
       "      <td>0.0308319</td>\n",
       "      <td>0.123469</td>\n",
       "      <td>0.129792</td>\n",
       "      <td>0.0181059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>necesito servicios de capacitacion para obtene...</td>\n",
       "      <td>Charlas/Capacitaciones</td>\n",
       "      <td>Jubilacion Patronal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0101813</td>\n",
       "      <td>0.00103417</td>\n",
       "      <td>0.00517052</td>\n",
       "      <td>0.197485</td>\n",
       "      <td>0.0532342</td>\n",
       "      <td>0.00110557</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>0.00744999</td>\n",
       "      <td>0.000334574</td>\n",
       "      <td>0.00783009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Han trabajado con Modelos de Desercin Cliente...</td>\n",
       "      <td>Consultoria</td>\n",
       "      <td>Consultoria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00177936</td>\n",
       "      <td>0.000785761</td>\n",
       "      <td>0.0314653</td>\n",
       "      <td>0.0114919</td>\n",
       "      <td>0.0369084</td>\n",
       "      <td>0.0104964</td>\n",
       "      <td>0.0119156</td>\n",
       "      <td>0.000759216</td>\n",
       "      <td>0.123256</td>\n",
       "      <td>0.0131245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  587 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pregunta                   label  \\\n",
       "0               Hay posiciones abiertas en ACTUARIA?              job seeker   \n",
       "1  necesito servicios de capacitacion para obtene...  Charlas/Capacitaciones   \n",
       "2  Han trabajado con Modelos de Desercin Cliente...             Consultoria   \n",
       "\n",
       "            pred_label buen dia mi nombr llam empres me  ...        IESS  \\\n",
       "0             Contacto    0   0  0     0    0      0  0  ...   0.0117327   \n",
       "1  Jubilacion Patronal    0   0  0     0    0      0  0  ...   0.0101813   \n",
       "2          Consultoria    0   0  0     0    0      0  0  ...  0.00177936   \n",
       "\n",
       "      Greeting    Contacto   No Topic      Queja Otros servicios  \\\n",
       "0     0.056362    0.402604   0.068683  0.0550525         0.01466   \n",
       "1   0.00103417  0.00517052   0.197485  0.0532342      0.00110557   \n",
       "2  0.000785761   0.0314653  0.0114919  0.0369084       0.0104964   \n",
       "\n",
       "  Charlas/Capacitaciones      Hi Five   job seeker  \\\n",
       "0              0.0308319     0.123469     0.129792   \n",
       "1               0.109853   0.00744999  0.000334574   \n",
       "2              0.0119156  0.000759216     0.123256   \n",
       "\n",
       "  Facturacion/Retencion/Cobros  \n",
       "0                    0.0181059  \n",
       "1                   0.00783009  \n",
       "2                    0.0131245  \n",
       "\n",
       "[3 rows x 587 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('Data\\\\Xtes.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
