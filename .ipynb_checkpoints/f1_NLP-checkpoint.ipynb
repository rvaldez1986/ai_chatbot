{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pattern.es import parsetree\n",
    "import unidecode\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import json\n",
    "\n",
    "from urls import u_IESS, u_RDD, u_JP, u_CONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_funcionales = nltk.corpus.stopwords.words(\"spanish\")    \n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def trim_sent(sentence):    \n",
    "    return ' '.join(sentence.split())\n",
    "\n",
    "def prepare_text(text): \n",
    "    try:\n",
    "        text = trim_sent(text).lower()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('Exception en prepare_text: {0}'.format(e))\n",
    "        return None       \n",
    "\n",
    "\n",
    "def hasNumbers(string):\n",
    "    return bool(re.search(r'\\d', string))\n",
    "\n",
    "\n",
    "def hasBC(string):\n",
    "    i = string.find('/')\n",
    "    return bool(i != -1)\n",
    "\n",
    "\n",
    "def other_check(token):    \n",
    "    b1 = not hasNumbers(token)\n",
    "    b2 = not hasBC(token)\n",
    "    return (b1 and b2)\n",
    "\n",
    "def remove_accent(word):\n",
    "    return unidecode.unidecode(word)\n",
    "\n",
    "def stem_lemma(word):     \n",
    "    word = parsetree(word, lemmata=True)[0].lemmata[0]\n",
    "    word = stemmer.stem(word) \n",
    "    return word\n",
    "\n",
    "\n",
    "def token_and_clean(texto): \n",
    "    tokens = nltk.word_tokenize(texto, \"spanish\")\n",
    "    token_list = []\n",
    "    for token in tokens:        \n",
    "        if token not in palabras_funcionales:\n",
    "            token = stem_lemma(token)\n",
    "            token = remove_accent(token)\n",
    "            if len(token) >= 2 and other_check(token):\n",
    "                token_list.append(token)                \n",
    "        \n",
    "    return token_list   \n",
    "\n",
    "\n",
    "def vectorize_phrase(texto, vocab):\n",
    "    try:\n",
    "        tokens = token_and_clean(texto)\n",
    "        vector = np.zeros(len(vocab))\n",
    "        for t in tokens:\n",
    "            if t in vocab:\n",
    "                vector[vocab.index(t)] = 1\n",
    "        return vector\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception en vectorize_phrase: {0}'.format(e))\n",
    "        return None  \n",
    "    \n",
    "\n",
    "def n_token(sentence):\n",
    "    token_list = token_and_clean(sentence) \n",
    "    return len(token_list) \n",
    "\n",
    "\n",
    "def polarity_and_lang(message): #blob has a limit on api calls\n",
    "    \n",
    "    try:\n",
    "        if len(message) > 2:\n",
    "    \n",
    "            blob = TextBlob(message)    \n",
    "        \n",
    "            leng = blob.detect_language()\n",
    "            text = ''\n",
    "            if leng == 'es':\n",
    "                blob = blob.translate(to='en').lower() \n",
    "                text = message\n",
    "            else:\n",
    "                blob = blob.lower() \n",
    "                text = blob.translate(to='es').lower().raw \n",
    "            \n",
    "            pol = blob.sentiment[0]        \n",
    "        else:\n",
    "            print('Se paso a polarity_and_lang un texto menor que 3 caracters')\n",
    "            pol = 0\n",
    "            text = message            \n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "            print('Exception en polarity_and_lang: {0}'.format(e))\n",
    "            pol = 0\n",
    "            text = None            \n",
    "    \n",
    "    return (pol, text)\n",
    "\n",
    "\n",
    "def percent_greet(sentence):\n",
    "    tgreet = ['hol', 'buen', 'tard', 'dia', 'noch']\n",
    "    count = 0\n",
    "    tokens = token_and_clean(sentence)\n",
    "    for w in tokens:       \n",
    "        if w in tgreet:\n",
    "            count += 1  \n",
    "    if len(tokens) > 0:\n",
    "        return count/len(tokens)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def pred_prob(text):\n",
    "    try:        \n",
    "    \n",
    "        vocab = np.load('Data\\\\vocab.npy', allow_pickle=True)\n",
    "        vocab = list(vocab)\n",
    "    \n",
    "        ldata = np.load('Data\\\\param_dict.npy', allow_pickle=True)\n",
    "        param_dict = ldata.item() \n",
    "        W0 = param_dict[0].T\n",
    "        b0 = param_dict[1]\n",
    "        W1 = param_dict[2].T\n",
    "        b1 = param_dict[3]\n",
    "        \n",
    "        pol, text = polarity_and_lang(text)\n",
    "        \n",
    "        if text:\n",
    "    \n",
    "            x = vectorize_phrase(text, vocab)\n",
    "            if x.any():\n",
    "                x = np.append(x, n_token(text))\n",
    "                x = np.append(x, percent_greet(text))\n",
    "                x = np.append(x, pol)   \n",
    "    \n",
    "                h0 = np.matmul(x, W0) + b0\n",
    "                h1 = np.tanh(h0)\n",
    "                h2 = np.matmul(h1, W1) + b1\n",
    "                h3 = np.exp(h2)\n",
    "                prob = h3/np.sum(h3)\n",
    "        \n",
    "                return (prob, pol, text)  \n",
    "            else:\n",
    "                return(None, None, None)\n",
    "        \n",
    "        else:\n",
    "            return (None, None, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception en predTop_prob: {0}'.format(e))\n",
    "        return (None, None, None)\n",
    "    \n",
    "    \n",
    "def predict_topic(sentence):\n",
    "    topics = ['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
    "    \n",
    "    sentence = prepare_text(sentence)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        if sentence:\n",
    "            prob, pol, text = pred_prob(sentence)\n",
    "            if prob.all():\n",
    "                return (topics[np.argmax(prob)], pol, text)\n",
    "            else:\n",
    "                return('No Topic', 0, None) \n",
    "        else:\n",
    "            return('No Topic', 0, None) \n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Exception en predict_topic: {0}'.format(e))\n",
    "        return('No Topic', 0, None)  \n",
    "    \n",
    "    \n",
    "def get_score(q, u, td):\n",
    "    dt = td[u][1]\n",
    "    dt2 = td[u][2]    \n",
    "    \n",
    "    q = prepare_text(q)  #no analysis needed, q already belongs to a topic (previous states)  \n",
    "    q_tokens =  token_and_clean(q)\n",
    "    puntaje = 0\n",
    "    k = 1  #factor for header\n",
    "    for t in q_tokens:  \n",
    "        t = stem_lemma(t)\n",
    "        if t in dt:\n",
    "            puntaje += dt[t]\n",
    "        if t in dt2:\n",
    "            puntaje += dt[t] * k\n",
    "        \n",
    "    return puntaje  \n",
    "\n",
    "\n",
    "\n",
    "def suggest_url(question, topic):    \n",
    "    \n",
    "    try:    \n",
    "        texts_data = json.load(open(\"Data\\\\texts_data.txt\"))        \n",
    "        ranking = []\n",
    "        \n",
    "        \n",
    "        if topic == \"Jubilacion Patronal\":\n",
    "            ulist = u_JP            \n",
    "        elif topic == \"Renuncia/Despido/Desahucio\":\n",
    "            ulist = u_RDD \n",
    "        elif topic == \"IESS\":\n",
    "            ulist = u_IESS\n",
    "        else:\n",
    "            ulist = u_CONS \n",
    "        \n",
    "        for u in ulist:\n",
    "            score = get_score(question, u, texts_data)\n",
    "            ranking.append(score)\n",
    "            \n",
    "        if np.max(ranking) > 0:\n",
    "            url_res = ulist[np.argmax(ranking)]\n",
    "            return url_res\n",
    "        else:\n",
    "            return None           \n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Exception en suggest_url: {0}'.format(e))\n",
    "        return None      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Mi esposo se jubilo por invalidez total absoluta Que tipo de indemnización debe tener? O que fondos debería retirar'\n",
    "topic = 'IESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://actuaria.com.ec/coberturas-y-requisitos-del-seguro-de-invalidez-vejez-y-muerte-del-iess/'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggest_url(q, topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
