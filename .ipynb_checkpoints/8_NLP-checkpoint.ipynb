{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data\\\\preguntas.csv\", sep=',', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Data\\\\X.npy', allow_pickle=True)\n",
    "y = np.load('Data\\\\y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_oh = np.zeros((y.shape[0], max(y)+1))\n",
    "#y_oh[np.arange(y.shape[0]), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "idx = np.arange(X.shape[0])\n",
    "random.shuffle(idx)  \n",
    "btra = np.random.choice(idx, int(0.8*X.shape[0]), replace=False)\n",
    "btes = [i for i in idx if i not in btra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtra = X[btra]\n",
    "ytra = y[btra]\n",
    "\n",
    "Xtes = X[btes]\n",
    "ytes = y[btes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 468)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, n_batches):  \n",
    "    \n",
    "    random.seed(123)\n",
    "    \n",
    "    batch_size = X.shape[0] // n_batches\n",
    "    \n",
    "    idx = np.arange(X.shape[0])\n",
    "    random.shuffle(idx)    \n",
    "    idx = idx[:n_batches*batch_size]\n",
    "        \n",
    "    for i in range(n_batches):            \n",
    "        bi = np.random.choice(idx, batch_size, replace=False)\n",
    "        X_batch = X[bi]\n",
    "        Y_batch = Y[bi]\n",
    "        idx = [i for i in idx if i not in bi]\n",
    "        yield (X_batch,Y_batch)       \n",
    "        \n",
    "        \n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc11 = nn.Linear(468, 24, bias=True)\n",
    "        self.fc12 = nn.Linear(24, 13, bias=True) \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = torch.tanh(self.fc11(x))\n",
    "        x1 = self.fc12(x1)     \n",
    "        return x1\n",
    "    \n",
    "    \n",
    "def fit(X, Y, Xt, Yt, net, optimizer, error, n_epochs, n_batches, device, PATH, min_val_loss = float('inf')):\n",
    "    \n",
    "    net = net.to(device)    \n",
    "    losses = []    \n",
    "    val_losses = []\n",
    "\n",
    "    val_inputs = torch.FloatTensor(Xt)\n",
    "    val_labels = torch.tensor(Yt, dtype=torch.long)\n",
    "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)  \n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "        running_loss = 0  \n",
    "         \n",
    "        \n",
    "        for batch_x, batch_y in batch_generator(X, Y, n_batches):  \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the inputs\n",
    "            inputs = torch.FloatTensor(batch_x)\n",
    "            labels = torch.tensor(batch_y, dtype=torch.long)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)             \n",
    "                \n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = error(outputs, labels)\n",
    "                        \n",
    "            loss.backward()    #obtain gradients      \n",
    "            optimizer.step()   #optimize\n",
    "                \n",
    "            running_loss += loss.item()      \n",
    "                \n",
    "        running_loss = running_loss/n_batches    \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_outputs = net.forward(val_inputs)\n",
    "            val_loss = error(val_outputs, val_labels) \n",
    "        \n",
    "        losses.append(running_loss)   \n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        \n",
    "        print('Epoch {0}: Training Loss: {1} Validation Loss: {2}'.format(epoch+1, running_loss, val_loss.item()))\n",
    "        \n",
    "        if val_loss.item() < min_val_loss:\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            print('New Checkpoint Saved into PATH')\n",
    "            min_val_loss = val_loss.item()\n",
    "        \n",
    "        \n",
    "def weights_init_normal(m):     \n",
    "    classname = m.__class__.__name__\n",
    "    torch.manual_seed(0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.normal_(0, y)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.normal_(0, y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "lr = 0.001\n",
    "n_batches = 3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH = 'Data\\\\model_checkpoint.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net1()\n",
    "net.apply(weights_init_normal)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 2.540684143702189 Validation Loss: 2.554556131362915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2: Training Loss: 2.5284109115600586 Validation Loss: 2.533421516418457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3: Training Loss: 2.5106197198232016 Validation Loss: 2.5088155269622803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4: Training Loss: 2.49190886815389 Validation Loss: 2.4839248657226562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5: Training Loss: 2.4714925289154053 Validation Loss: 2.4592032432556152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 6: Training Loss: 2.4543023109436035 Validation Loss: 2.434687852859497\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 7: Training Loss: 2.434699773788452 Validation Loss: 2.4093384742736816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 8: Training Loss: 2.4169743061065674 Validation Loss: 2.384373188018799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 9: Training Loss: 2.4014358520507812 Validation Loss: 2.3609113693237305\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 10: Training Loss: 2.3881454467773438 Validation Loss: 2.345045804977417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 11: Training Loss: 2.3789377212524414 Validation Loss: 2.334104299545288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 12: Training Loss: 2.3717413743336997 Validation Loss: 2.3253297805786133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 13: Training Loss: 2.365328391393026 Validation Loss: 2.319558620452881\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 14: Training Loss: 2.3595544497172036 Validation Loss: 2.314429521560669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 15: Training Loss: 2.3548282782236734 Validation Loss: 2.310248613357544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 16: Training Loss: 2.3504416147867837 Validation Loss: 2.306723117828369\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 17: Training Loss: 2.345223585764567 Validation Loss: 2.3011016845703125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 18: Training Loss: 2.3409060637156167 Validation Loss: 2.2928004264831543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 19: Training Loss: 2.3358543713887534 Validation Loss: 2.285837173461914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 20: Training Loss: 2.331411282221476 Validation Loss: 2.2775204181671143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 21: Training Loss: 2.326411803563436 Validation Loss: 2.2698378562927246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 22: Training Loss: 2.322080373764038 Validation Loss: 2.2632296085357666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 23: Training Loss: 2.318084796269735 Validation Loss: 2.2588953971862793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 24: Training Loss: 2.314267953236898 Validation Loss: 2.255998373031616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 25: Training Loss: 2.3111515045166016 Validation Loss: 2.253283739089966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 26: Training Loss: 2.30783478418986 Validation Loss: 2.250128746032715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 27: Training Loss: 2.3050906658172607 Validation Loss: 2.245835065841675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 28: Training Loss: 2.3021326859792075 Validation Loss: 2.241705894470215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 29: Training Loss: 2.29951278368632 Validation Loss: 2.236642360687256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 30: Training Loss: 2.296618938446045 Validation Loss: 2.233400344848633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 31: Training Loss: 2.2940311431884766 Validation Loss: 2.229891300201416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 32: Training Loss: 2.2917398611704507 Validation Loss: 2.2273330688476562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 33: Training Loss: 2.289189418156942 Validation Loss: 2.2254624366760254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 34: Training Loss: 2.286565144856771 Validation Loss: 2.221919059753418\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 35: Training Loss: 2.2843681971232095 Validation Loss: 2.2191083431243896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 36: Training Loss: 2.2819366455078125 Validation Loss: 2.215874195098877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 37: Training Loss: 2.279856522878011 Validation Loss: 2.2135188579559326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 38: Training Loss: 2.277355670928955 Validation Loss: 2.2101247310638428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 39: Training Loss: 2.2752573490142822 Validation Loss: 2.208280086517334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 40: Training Loss: 2.2731473445892334 Validation Loss: 2.2053654193878174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 41: Training Loss: 2.2709928353627524 Validation Loss: 2.2027294635772705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 42: Training Loss: 2.268753925959269 Validation Loss: 2.200836658477783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 43: Training Loss: 2.266641060511271 Validation Loss: 2.199697256088257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 44: Training Loss: 2.2645254135131836 Validation Loss: 2.1982083320617676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 45: Training Loss: 2.262427012125651 Validation Loss: 2.1968812942504883\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 46: Training Loss: 2.260765711466471 Validation Loss: 2.1937339305877686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 47: Training Loss: 2.2584992249806723 Validation Loss: 2.1922385692596436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 48: Training Loss: 2.256307363510132 Validation Loss: 2.1906545162200928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 49: Training Loss: 2.25445548693339 Validation Loss: 2.1897008419036865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 50: Training Loss: 2.2523459593454995 Validation Loss: 2.187514305114746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 51: Training Loss: 2.250377337137858 Validation Loss: 2.184875726699829\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 52: Training Loss: 2.2485803763071694 Validation Loss: 2.181814193725586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 53: Training Loss: 2.246586799621582 Validation Loss: 2.1802473068237305\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 54: Training Loss: 2.244473695755005 Validation Loss: 2.1782119274139404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 55: Training Loss: 2.2427056630452475 Validation Loss: 2.1769046783447266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 56: Training Loss: 2.2407963275909424 Validation Loss: 2.1754584312438965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 57: Training Loss: 2.238776763280233 Validation Loss: 2.173366069793701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 58: Training Loss: 2.236910343170166 Validation Loss: 2.1712584495544434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 59: Training Loss: 2.234981377919515 Validation Loss: 2.169816732406616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 60: Training Loss: 2.2331672509511313 Validation Loss: 2.1681244373321533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 61: Training Loss: 2.2312838236490884 Validation Loss: 2.1675548553466797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 62: Training Loss: 2.2295366128285727 Validation Loss: 2.165363311767578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 63: Training Loss: 2.2277400493621826 Validation Loss: 2.1637256145477295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 64: Training Loss: 2.225698391596476 Validation Loss: 2.1620006561279297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 65: Training Loss: 2.2238341172536216 Validation Loss: 2.1606571674346924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 66: Training Loss: 2.222085952758789 Validation Loss: 2.1596968173980713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 67: Training Loss: 2.220239798227946 Validation Loss: 2.1582112312316895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 68: Training Loss: 2.2184645334879556 Validation Loss: 2.155806064605713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 69: Training Loss: 2.216593345006307 Validation Loss: 2.152524709701538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 70: Training Loss: 2.214946667353312 Validation Loss: 2.150120735168457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 71: Training Loss: 2.2129525343577066 Validation Loss: 2.1487019062042236\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 72: Training Loss: 2.2111573219299316 Validation Loss: 2.1484158039093018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 73: Training Loss: 2.2093214988708496 Validation Loss: 2.1480700969696045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 74: Training Loss: 2.2075573603312173 Validation Loss: 2.147031307220459\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 75: Training Loss: 2.2057915528615317 Validation Loss: 2.1442155838012695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 76: Training Loss: 2.203882376352946 Validation Loss: 2.1421844959259033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 77: Training Loss: 2.2022605737050376 Validation Loss: 2.141275644302368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 78: Training Loss: 2.2007385889689126 Validation Loss: 2.1379504203796387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 79: Training Loss: 2.198489268620809 Validation Loss: 2.136251926422119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 80: Training Loss: 2.196745236714681 Validation Loss: 2.135190486907959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 81: Training Loss: 2.194838364919027 Validation Loss: 2.1341910362243652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 82: Training Loss: 2.193137248357137 Validation Loss: 2.13313364982605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 83: Training Loss: 2.1915839513142905 Validation Loss: 2.1334245204925537\n",
      "Epoch 84: Training Loss: 2.1895345052083335 Validation Loss: 2.1308836936950684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 85: Training Loss: 2.1876839796702066 Validation Loss: 2.128386974334717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 86: Training Loss: 2.185959736506144 Validation Loss: 2.1264119148254395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 87: Training Loss: 2.1845359802246094 Validation Loss: 2.1234521865844727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 88: Training Loss: 2.182447671890259 Validation Loss: 2.122577667236328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 89: Training Loss: 2.18070912361145 Validation Loss: 2.122844696044922\n",
      "Epoch 90: Training Loss: 2.178827921549479 Validation Loss: 2.121446132659912\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 91: Training Loss: 2.176962693532308 Validation Loss: 2.119689464569092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 92: Training Loss: 2.175415277481079 Validation Loss: 2.1184587478637695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 93: Training Loss: 2.173647880554199 Validation Loss: 2.1160736083984375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 94: Training Loss: 2.1718317667643228 Validation Loss: 2.112924575805664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 95: Training Loss: 2.169914960861206 Validation Loss: 2.1117987632751465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 96: Training Loss: 2.16819437344869 Validation Loss: 2.1117501258850098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 97: Training Loss: 2.1663153171539307 Validation Loss: 2.109971284866333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 98: Training Loss: 2.1645888487497964 Validation Loss: 2.1077721118927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 99: Training Loss: 2.1626876990000405 Validation Loss: 2.106065034866333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 100: Training Loss: 2.161032517751058 Validation Loss: 2.105029582977295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 101: Training Loss: 2.159415324529012 Validation Loss: 2.1023337841033936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 102: Training Loss: 2.1574532190958657 Validation Loss: 2.101609945297241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 103: Training Loss: 2.155688444773356 Validation Loss: 2.101376533508301\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 104: Training Loss: 2.1538426081339517 Validation Loss: 2.099647283554077\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 105: Training Loss: 2.152152140935262 Validation Loss: 2.098306655883789\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 106: Training Loss: 2.150451183319092 Validation Loss: 2.0959904193878174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 107: Training Loss: 2.148717006047567 Validation Loss: 2.0933892726898193\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 108: Training Loss: 2.147160450617472 Validation Loss: 2.0904839038848877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 109: Training Loss: 2.145278056462606 Validation Loss: 2.0884242057800293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 110: Training Loss: 2.1434129079182944 Validation Loss: 2.090362071990967\n",
      "Epoch 111: Training Loss: 2.141714096069336 Validation Loss: 2.0912513732910156\n",
      "Epoch 112: Training Loss: 2.1399638652801514 Validation Loss: 2.0873019695281982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 113: Training Loss: 2.1377487977345786 Validation Loss: 2.085272789001465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 114: Training Loss: 2.1359571615854898 Validation Loss: 2.083500862121582\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 115: Training Loss: 2.1343115170796714 Validation Loss: 2.0806732177734375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 116: Training Loss: 2.1323300202687583 Validation Loss: 2.0790586471557617\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 117: Training Loss: 2.1304845015207925 Validation Loss: 2.078223466873169\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 118: Training Loss: 2.128744602203369 Validation Loss: 2.077899694442749\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 119: Training Loss: 2.1270639896392822 Validation Loss: 2.0764830112457275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 120: Training Loss: 2.125349998474121 Validation Loss: 2.07319712638855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 121: Training Loss: 2.1234800020853677 Validation Loss: 2.0730092525482178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 122: Training Loss: 2.1216654777526855 Validation Loss: 2.0719380378723145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 123: Training Loss: 2.1197315057118735 Validation Loss: 2.0691394805908203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 124: Training Loss: 2.1183017094930015 Validation Loss: 2.0657191276550293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 125: Training Loss: 2.116325616836548 Validation Loss: 2.0648958683013916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 126: Training Loss: 2.1145355701446533 Validation Loss: 2.0658106803894043\n",
      "Epoch 127: Training Loss: 2.1126283009847007 Validation Loss: 2.064457416534424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 128: Training Loss: 2.1108530362447104 Validation Loss: 2.062816619873047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 129: Training Loss: 2.1093457539876304 Validation Loss: 2.061169385910034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 130: Training Loss: 2.107405503590902 Validation Loss: 2.056366443634033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 131: Training Loss: 2.105508327484131 Validation Loss: 2.0545055866241455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 132: Training Loss: 2.103569269180298 Validation Loss: 2.0533740520477295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 133: Training Loss: 2.1020835240681968 Validation Loss: 2.054983377456665\n",
      "Epoch 134: Training Loss: 2.100025177001953 Validation Loss: 2.0549533367156982\n",
      "Epoch 135: Training Loss: 2.0983963012695312 Validation Loss: 2.05181622505188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 136: Training Loss: 2.0963078339894614 Validation Loss: 2.0494868755340576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 137: Training Loss: 2.0945982138315835 Validation Loss: 2.0464930534362793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 138: Training Loss: 2.0926407972971597 Validation Loss: 2.045372724533081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 139: Training Loss: 2.0908966859181723 Validation Loss: 2.0449023246765137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 140: Training Loss: 2.0894864400227866 Validation Loss: 2.0422112941741943\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 141: Training Loss: 2.087648868560791 Validation Loss: 2.043704032897949\n",
      "Epoch 142: Training Loss: 2.0854685306549072 Validation Loss: 2.0414791107177734\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 143: Training Loss: 2.0837313334147134 Validation Loss: 2.039015531539917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 144: Training Loss: 2.0824591318766275 Validation Loss: 2.0390796661376953\n",
      "Epoch 145: Training Loss: 2.080479065577189 Validation Loss: 2.0332796573638916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 146: Training Loss: 2.0783817768096924 Validation Loss: 2.0322399139404297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 147: Training Loss: 2.0765329202016196 Validation Loss: 2.0304813385009766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 148: Training Loss: 2.074826995531718 Validation Loss: 2.031372308731079\n",
      "Epoch 149: Training Loss: 2.073009808858236 Validation Loss: 2.0290839672088623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 150: Training Loss: 2.0710295836130777 Validation Loss: 2.0276601314544678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 151: Training Loss: 2.0692056020100913 Validation Loss: 2.027869462966919\n",
      "Epoch 152: Training Loss: 2.068128824234009 Validation Loss: 2.0283162593841553\n",
      "Epoch 153: Training Loss: 2.065641403198242 Validation Loss: 2.0243468284606934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 154: Training Loss: 2.063998301823934 Validation Loss: 2.019491672515869\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 155: Training Loss: 2.0622547467549643 Validation Loss: 2.0165889263153076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 156: Training Loss: 2.060323794682821 Validation Loss: 2.016923427581787\n",
      "Epoch 157: Training Loss: 2.0581193765004477 Validation Loss: 2.017646551132202\n",
      "Epoch 158: Training Loss: 2.056642929712931 Validation Loss: 2.01893949508667\n",
      "Epoch 159: Training Loss: 2.054877281188965 Validation Loss: 2.0175070762634277\n",
      "Epoch 160: Training Loss: 2.0530402660369873 Validation Loss: 2.012972593307495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 161: Training Loss: 2.0514803727467856 Validation Loss: 2.008657455444336\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162: Training Loss: 2.0493961175282798 Validation Loss: 2.00840163230896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 163: Training Loss: 2.047572294871012 Validation Loss: 2.007171869277954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 164: Training Loss: 2.045844554901123 Validation Loss: 2.0075089931488037\n",
      "Epoch 165: Training Loss: 2.043759822845459 Validation Loss: 2.007556676864624\n",
      "Epoch 166: Training Loss: 2.0419774850209556 Validation Loss: 2.0049819946289062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 167: Training Loss: 2.040222446123759 Validation Loss: 2.001438617706299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 168: Training Loss: 2.0382893880208335 Validation Loss: 1.9991812705993652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 169: Training Loss: 2.036406993865967 Validation Loss: 1.998136281967163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 170: Training Loss: 2.0348229010899863 Validation Loss: 1.9972248077392578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 171: Training Loss: 2.0332559744517007 Validation Loss: 1.9983640909194946\n",
      "Epoch 172: Training Loss: 2.0310215950012207 Validation Loss: 1.995725154876709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 173: Training Loss: 2.0292028983434043 Validation Loss: 1.9929449558258057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 174: Training Loss: 2.027487556139628 Validation Loss: 1.9905842542648315\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 175: Training Loss: 2.025601545969645 Validation Loss: 1.9892045259475708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 176: Training Loss: 2.0242233276367188 Validation Loss: 1.989285945892334\n",
      "Epoch 177: Training Loss: 2.0220221281051636 Validation Loss: 1.9880702495574951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 178: Training Loss: 2.020429809888204 Validation Loss: 1.9836028814315796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 179: Training Loss: 2.0188245375951133 Validation Loss: 1.9801474809646606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 180: Training Loss: 2.016898512840271 Validation Loss: 1.980644702911377\n",
      "Epoch 181: Training Loss: 2.0144440730412803 Validation Loss: 1.9819427728652954\n",
      "Epoch 182: Training Loss: 2.0130306482315063 Validation Loss: 1.9834792613983154\n",
      "Epoch 183: Training Loss: 2.0113184452056885 Validation Loss: 1.9804099798202515\n",
      "Epoch 184: Training Loss: 2.0097427368164062 Validation Loss: 1.9741950035095215\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 185: Training Loss: 2.007593353589376 Validation Loss: 1.9713773727416992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 186: Training Loss: 2.0055883328119912 Validation Loss: 1.9706172943115234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 187: Training Loss: 2.0037293831507363 Validation Loss: 1.972816824913025\n",
      "Epoch 188: Training Loss: 2.00187619527181 Validation Loss: 1.9737060070037842\n",
      "Epoch 189: Training Loss: 2.0001362959543862 Validation Loss: 1.9716769456863403\n",
      "Epoch 190: Training Loss: 1.9982669353485107 Validation Loss: 1.9657732248306274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 191: Training Loss: 1.996476411819458 Validation Loss: 1.9633433818817139\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 192: Training Loss: 1.9946955839792888 Validation Loss: 1.9617862701416016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 193: Training Loss: 1.9928412437438965 Validation Loss: 1.9622130393981934\n",
      "Epoch 194: Training Loss: 1.9915650685628254 Validation Loss: 1.9625738859176636\n",
      "Epoch 195: Training Loss: 1.989126205444336 Validation Loss: 1.961008906364441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 196: Training Loss: 1.9872570435206096 Validation Loss: 1.9563508033752441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 197: Training Loss: 1.9855250120162964 Validation Loss: 1.9542348384857178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 198: Training Loss: 1.9834864139556885 Validation Loss: 1.953719139099121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 199: Training Loss: 1.9815824031829834 Validation Loss: 1.9537501335144043\n",
      "Epoch 200: Training Loss: 1.979925553003947 Validation Loss: 1.95247220993042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 201: Training Loss: 1.9780951738357544 Validation Loss: 1.951154351234436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 202: Training Loss: 1.9766641855239868 Validation Loss: 1.9470134973526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 203: Training Loss: 1.9744608799616497 Validation Loss: 1.9468032121658325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 204: Training Loss: 1.9725670019785564 Validation Loss: 1.9456859827041626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 205: Training Loss: 1.9707791805267334 Validation Loss: 1.9425193071365356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 206: Training Loss: 1.9689104557037354 Validation Loss: 1.9416258335113525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 207: Training Loss: 1.9670383532842 Validation Loss: 1.939889669418335\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 208: Training Loss: 1.965227762858073 Validation Loss: 1.9383610486984253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 209: Training Loss: 1.9634490013122559 Validation Loss: 1.9383032321929932\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 210: Training Loss: 1.9616070588429768 Validation Loss: 1.9366215467453003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 211: Training Loss: 1.959797461827596 Validation Loss: 1.934012532234192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 212: Training Loss: 1.9581234852472942 Validation Loss: 1.9334734678268433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 213: Training Loss: 1.9562486410140991 Validation Loss: 1.9301505088806152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 214: Training Loss: 1.954431176185608 Validation Loss: 1.9280421733856201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 215: Training Loss: 1.9525624513626099 Validation Loss: 1.9284627437591553\n",
      "Epoch 216: Training Loss: 1.9510775009791057 Validation Loss: 1.9305533170700073\n",
      "Epoch 217: Training Loss: 1.9490362008412678 Validation Loss: 1.9267932176589966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 218: Training Loss: 1.9472102324167888 Validation Loss: 1.9223246574401855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 219: Training Loss: 1.9455668528874714 Validation Loss: 1.9199905395507812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 220: Training Loss: 1.9441059430440266 Validation Loss: 1.9217697381973267\n",
      "Epoch 221: Training Loss: 1.9416477282842 Validation Loss: 1.9187443256378174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 222: Training Loss: 1.9397290150324504 Validation Loss: 1.916926622390747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 223: Training Loss: 1.9378363688786824 Validation Loss: 1.9158493280410767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 224: Training Loss: 1.936435341835022 Validation Loss: 1.9122679233551025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 225: Training Loss: 1.9341909885406494 Validation Loss: 1.9114923477172852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 226: Training Loss: 1.9324790239334106 Validation Loss: 1.9119367599487305\n",
      "Epoch 227: Training Loss: 1.9306116898854573 Validation Loss: 1.911024808883667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 228: Training Loss: 1.9290472666422527 Validation Loss: 1.9115142822265625\n",
      "Epoch 229: Training Loss: 1.9268910884857178 Validation Loss: 1.907994270324707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 230: Training Loss: 1.9251130819320679 Validation Loss: 1.9037219285964966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 231: Training Loss: 1.923457423845927 Validation Loss: 1.9018595218658447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 232: Training Loss: 1.9215575059254963 Validation Loss: 1.9007580280303955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 233: Training Loss: 1.9196709791819255 Validation Loss: 1.900109887123108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 234: Training Loss: 1.9179455836613972 Validation Loss: 1.8981159925460815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 235: Training Loss: 1.9161893924077351 Validation Loss: 1.8962922096252441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 236: Training Loss: 1.9144311745961506 Validation Loss: 1.894923448562622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 237: Training Loss: 1.912842075030009 Validation Loss: 1.8980873823165894\n",
      "Epoch 238: Training Loss: 1.9107972383499146 Validation Loss: 1.8941800594329834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 239: Training Loss: 1.9086454312006633 Validation Loss: 1.8922744989395142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 240: Training Loss: 1.9070664644241333 Validation Loss: 1.8882522583007812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 241: Training Loss: 1.9051393270492554 Validation Loss: 1.886500597000122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 242: Training Loss: 1.903948982556661 Validation Loss: 1.8843350410461426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 243: Training Loss: 1.9017427762349446 Validation Loss: 1.8873841762542725\n",
      "Epoch 244: Training Loss: 1.8997370799382527 Validation Loss: 1.8862996101379395\n",
      "Epoch 245: Training Loss: 1.898203730583191 Validation Loss: 1.8823707103729248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 246: Training Loss: 1.896445910135905 Validation Loss: 1.8785814046859741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 247: Training Loss: 1.8946079810460408 Validation Loss: 1.8792145252227783\n",
      "Epoch 248: Training Loss: 1.8930145104726155 Validation Loss: 1.8800114393234253\n",
      "Epoch 249: Training Loss: 1.8911550839742024 Validation Loss: 1.878274917602539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 250: Training Loss: 1.8896323839823406 Validation Loss: 1.8705288171768188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 251: Training Loss: 1.8876090447107952 Validation Loss: 1.8718106746673584\n",
      "Epoch 252: Training Loss: 1.885483702023824 Validation Loss: 1.869613528251648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 253: Training Loss: 1.8833419879277546 Validation Loss: 1.8700650930404663\n",
      "Epoch 254: Training Loss: 1.8817482391993205 Validation Loss: 1.869027018547058\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 255: Training Loss: 1.8800022602081299 Validation Loss: 1.8685332536697388\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 256: Training Loss: 1.8780768314997356 Validation Loss: 1.8655502796173096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 257: Training Loss: 1.8761054674784343 Validation Loss: 1.8623944520950317\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 258: Training Loss: 1.874863584836324 Validation Loss: 1.8598568439483643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 259: Training Loss: 1.8730290333429973 Validation Loss: 1.861117959022522\n",
      "Epoch 260: Training Loss: 1.8707624673843384 Validation Loss: 1.861059308052063\n",
      "Epoch 261: Training Loss: 1.8693467378616333 Validation Loss: 1.861240029335022\n",
      "Epoch 262: Training Loss: 1.867471973101298 Validation Loss: 1.8555972576141357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 263: Training Loss: 1.8653679291407268 Validation Loss: 1.8527812957763672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 264: Training Loss: 1.863633672396342 Validation Loss: 1.8528093099594116\n",
      "Epoch 265: Training Loss: 1.8621070782343547 Validation Loss: 1.8530597686767578\n",
      "Epoch 266: Training Loss: 1.8601539532343547 Validation Loss: 1.8514255285263062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 267: Training Loss: 1.858013908068339 Validation Loss: 1.8471554517745972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 268: Training Loss: 1.8563997745513916 Validation Loss: 1.8452210426330566\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 269: Training Loss: 1.8548834721247356 Validation Loss: 1.842461109161377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 270: Training Loss: 1.8528562386830647 Validation Loss: 1.8436437845230103\n",
      "Epoch 271: Training Loss: 1.8510326147079468 Validation Loss: 1.8440860509872437\n",
      "Epoch 272: Training Loss: 1.8494056860605876 Validation Loss: 1.8437540531158447\n",
      "Epoch 273: Training Loss: 1.8474709192911785 Validation Loss: 1.840103030204773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 274: Training Loss: 1.8462928930918376 Validation Loss: 1.8352282047271729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 275: Training Loss: 1.844032923380534 Validation Loss: 1.8344706296920776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 276: Training Loss: 1.841983675956726 Validation Loss: 1.8364720344543457\n",
      "Epoch 277: Training Loss: 1.8405815760294597 Validation Loss: 1.8379439115524292\n",
      "Epoch 278: Training Loss: 1.8389403025309246 Validation Loss: 1.832226037979126\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 279: Training Loss: 1.8368202050526936 Validation Loss: 1.8282603025436401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 280: Training Loss: 1.8349730571111043 Validation Loss: 1.8268461227416992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 281: Training Loss: 1.8338220914204915 Validation Loss: 1.8292063474655151\n",
      "Epoch 282: Training Loss: 1.8315062522888184 Validation Loss: 1.8265615701675415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 283: Training Loss: 1.830124298731486 Validation Loss: 1.8260703086853027\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 284: Training Loss: 1.827853759129842 Validation Loss: 1.822033166885376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 285: Training Loss: 1.8263688484827678 Validation Loss: 1.8192131519317627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 286: Training Loss: 1.8244192202885945 Validation Loss: 1.82023024559021\n",
      "Epoch 287: Training Loss: 1.8224421342213948 Validation Loss: 1.8197035789489746\n",
      "Epoch 288: Training Loss: 1.8207242091496785 Validation Loss: 1.8181003332138062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 289: Training Loss: 1.8189416726430256 Validation Loss: 1.81594979763031\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 290: Training Loss: 1.8171192804972331 Validation Loss: 1.812924861907959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 291: Training Loss: 1.8155065377553303 Validation Loss: 1.811660885810852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 292: Training Loss: 1.8136927286783855 Validation Loss: 1.8118237257003784\n",
      "Epoch 293: Training Loss: 1.8119130929311116 Validation Loss: 1.8106940984725952\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 294: Training Loss: 1.8101567029953003 Validation Loss: 1.8091909885406494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 295: Training Loss: 1.8087236881256104 Validation Loss: 1.8076997995376587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 296: Training Loss: 1.8064402341842651 Validation Loss: 1.8036097288131714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 297: Training Loss: 1.8052623271942139 Validation Loss: 1.8004400730133057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 298: Training Loss: 1.804090102513631 Validation Loss: 1.8034842014312744\n",
      "Epoch 299: Training Loss: 1.801532467206319 Validation Loss: 1.802649736404419\n",
      "Epoch 300: Training Loss: 1.7998613516489665 Validation Loss: 1.8000084161758423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 301: Training Loss: 1.7976671854654949 Validation Loss: 1.7957770824432373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 302: Training Loss: 1.7966597477595012 Validation Loss: 1.7935774326324463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 303: Training Loss: 1.7946125666300456 Validation Loss: 1.7960408926010132\n",
      "Epoch 304: Training Loss: 1.7928760846455891 Validation Loss: 1.7976150512695312\n",
      "Epoch 305: Training Loss: 1.791243076324463 Validation Loss: 1.7945114374160767\n",
      "Epoch 306: Training Loss: 1.7893212238947551 Validation Loss: 1.7870994806289673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 307: Training Loss: 1.7874863545099895 Validation Loss: 1.7851823568344116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 308: Training Loss: 1.786087155342102 Validation Loss: 1.7853015661239624\n",
      "Epoch 309: Training Loss: 1.7837122281392415 Validation Loss: 1.787298321723938\n",
      "Epoch 310: Training Loss: 1.7825970649719238 Validation Loss: 1.788907766342163\n",
      "Epoch 311: Training Loss: 1.7807530562082927 Validation Loss: 1.7844637632369995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 312: Training Loss: 1.7784556945164998 Validation Loss: 1.7803841829299927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 313: Training Loss: 1.776989499727885 Validation Loss: 1.7788724899291992\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 314: Training Loss: 1.7750410636266072 Validation Loss: 1.7781903743743896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 315: Training Loss: 1.7737698554992676 Validation Loss: 1.7762140035629272\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 316: Training Loss: 1.7721091508865356 Validation Loss: 1.7793248891830444\n",
      "Epoch 317: Training Loss: 1.7703635692596436 Validation Loss: 1.7790052890777588\n",
      "Epoch 318: Training Loss: 1.7682081460952759 Validation Loss: 1.7738155126571655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 319: Training Loss: 1.7671396334966023 Validation Loss: 1.7672877311706543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 320: Training Loss: 1.764962116877238 Validation Loss: 1.7684303522109985\n",
      "Epoch 321: Training Loss: 1.7627586126327515 Validation Loss: 1.7688809633255005\n",
      "Epoch 322: Training Loss: 1.7612310647964478 Validation Loss: 1.7683982849121094\n",
      "Epoch 323: Training Loss: 1.759737769762675 Validation Loss: 1.7685842514038086\n",
      "Epoch 324: Training Loss: 1.7581676642100017 Validation Loss: 1.766565203666687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 325: Training Loss: 1.7559644381205242 Validation Loss: 1.7616064548492432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 326: Training Loss: 1.7550411224365234 Validation Loss: 1.7566156387329102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 327: Training Loss: 1.7529572645823162 Validation Loss: 1.7576886415481567\n",
      "Epoch 328: Training Loss: 1.7505881388982136 Validation Loss: 1.7601706981658936\n",
      "Epoch 329: Training Loss: 1.7490585247675579 Validation Loss: 1.7618262767791748\n",
      "Epoch 330: Training Loss: 1.7480094035466511 Validation Loss: 1.7600198984146118\n",
      "Epoch 331: Training Loss: 1.7456742922465007 Validation Loss: 1.7531119585037231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 332: Training Loss: 1.743892788887024 Validation Loss: 1.750030517578125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 333: Training Loss: 1.742997407913208 Validation Loss: 1.7477284669876099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 334: Training Loss: 1.7409275770187378 Validation Loss: 1.7520527839660645\n",
      "Epoch 335: Training Loss: 1.7385828892389934 Validation Loss: 1.7513054609298706\n",
      "Epoch 336: Training Loss: 1.737482229868571 Validation Loss: 1.7499841451644897\n",
      "Epoch 337: Training Loss: 1.7352912823359172 Validation Loss: 1.7442030906677246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 338: Training Loss: 1.7353840271631877 Validation Loss: 1.737149715423584\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339: Training Loss: 1.7334162791570027 Validation Loss: 1.738683819770813\n",
      "Epoch 340: Training Loss: 1.7302478949228923 Validation Loss: 1.7463558912277222\n",
      "Epoch 341: Training Loss: 1.7298332452774048 Validation Loss: 1.7485135793685913\n",
      "Epoch 342: Training Loss: 1.7274491786956787 Validation Loss: 1.7402626276016235\n",
      "Epoch 343: Training Loss: 1.7250785827636719 Validation Loss: 1.7331318855285645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 344: Training Loss: 1.7237000465393066 Validation Loss: 1.733264684677124\n",
      "Epoch 345: Training Loss: 1.721915324529012 Validation Loss: 1.7327206134796143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 346: Training Loss: 1.7207426230112712 Validation Loss: 1.7345166206359863\n",
      "Epoch 347: Training Loss: 1.7184545596440632 Validation Loss: 1.733074426651001\n",
      "Epoch 348: Training Loss: 1.7171351512273152 Validation Loss: 1.7292619943618774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 349: Training Loss: 1.7149675687154133 Validation Loss: 1.7290676832199097\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 350: Training Loss: 1.7134557167689006 Validation Loss: 1.7273094654083252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 351: Training Loss: 1.7112977902094524 Validation Loss: 1.7251884937286377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 352: Training Loss: 1.7096821467081706 Validation Loss: 1.723206877708435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 353: Training Loss: 1.7082125743230183 Validation Loss: 1.7210719585418701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 354: Training Loss: 1.706613262494405 Validation Loss: 1.7225855588912964\n",
      "Epoch 355: Training Loss: 1.705527702967326 Validation Loss: 1.724480152130127\n",
      "Epoch 356: Training Loss: 1.7031900882720947 Validation Loss: 1.7176010608673096\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 357: Training Loss: 1.7013276815414429 Validation Loss: 1.714428424835205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 358: Training Loss: 1.7000571091969807 Validation Loss: 1.7114254236221313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 359: Training Loss: 1.6983030637105305 Validation Loss: 1.7114890813827515\n",
      "Epoch 360: Training Loss: 1.6965103546778362 Validation Loss: 1.7159698009490967\n",
      "Epoch 361: Training Loss: 1.6945053339004517 Validation Loss: 1.7141940593719482\n",
      "Epoch 362: Training Loss: 1.6931424140930176 Validation Loss: 1.7126743793487549\n",
      "Epoch 363: Training Loss: 1.6916069587071736 Validation Loss: 1.7060389518737793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 364: Training Loss: 1.6901100873947144 Validation Loss: 1.7051645517349243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 365: Training Loss: 1.6900932391484578 Validation Loss: 1.7119553089141846\n",
      "Epoch 366: Training Loss: 1.6865898768107097 Validation Loss: 1.7050687074661255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 367: Training Loss: 1.6853172779083252 Validation Loss: 1.6986974477767944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 368: Training Loss: 1.6828940709431965 Validation Loss: 1.698567509651184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 369: Training Loss: 1.681073745091756 Validation Loss: 1.6989076137542725\n",
      "Epoch 370: Training Loss: 1.6801481644312541 Validation Loss: 1.7034177780151367\n",
      "Epoch 371: Training Loss: 1.6780502398808796 Validation Loss: 1.6987751722335815\n",
      "Epoch 372: Training Loss: 1.6760384639104207 Validation Loss: 1.695133090019226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 373: Training Loss: 1.6746905247370403 Validation Loss: 1.6927694082260132\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 374: Training Loss: 1.6727441946665447 Validation Loss: 1.691691517829895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 375: Training Loss: 1.6710551579793294 Validation Loss: 1.6930381059646606\n",
      "Epoch 376: Training Loss: 1.6696059703826904 Validation Loss: 1.6897021532058716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 377: Training Loss: 1.6677117745081584 Validation Loss: 1.689170479774475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 378: Training Loss: 1.6662903626759846 Validation Loss: 1.6881455183029175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 379: Training Loss: 1.6644861300786336 Validation Loss: 1.6860774755477905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 380: Training Loss: 1.6640571355819702 Validation Loss: 1.6800332069396973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 381: Training Loss: 1.6610779364903767 Validation Loss: 1.6819754838943481\n",
      "Epoch 382: Training Loss: 1.6600828965504963 Validation Loss: 1.686955213546753\n",
      "Epoch 383: Training Loss: 1.6585018237431843 Validation Loss: 1.685580849647522\n",
      "Epoch 384: Training Loss: 1.6564439535140991 Validation Loss: 1.6789368391036987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 385: Training Loss: 1.6553852955500286 Validation Loss: 1.6736027002334595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 386: Training Loss: 1.6532970666885376 Validation Loss: 1.6755932569503784\n",
      "Epoch 387: Training Loss: 1.6517432530721028 Validation Loss: 1.6732571125030518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 388: Training Loss: 1.649854024251302 Validation Loss: 1.6746912002563477\n",
      "Epoch 389: Training Loss: 1.6478908856709797 Validation Loss: 1.676653504371643\n",
      "Epoch 390: Training Loss: 1.6473746299743652 Validation Loss: 1.6769607067108154\n",
      "Epoch 391: Training Loss: 1.644913951555888 Validation Loss: 1.6684296131134033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 392: Training Loss: 1.643050233523051 Validation Loss: 1.66439950466156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 393: Training Loss: 1.6417463620503743 Validation Loss: 1.6640210151672363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 394: Training Loss: 1.6398494243621826 Validation Loss: 1.6664241552352905\n",
      "Epoch 395: Training Loss: 1.638335943222046 Validation Loss: 1.6684925556182861\n",
      "Epoch 396: Training Loss: 1.6369881629943848 Validation Loss: 1.6645026206970215\n",
      "Epoch 397: Training Loss: 1.6350953976313274 Validation Loss: 1.660239338874817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 398: Training Loss: 1.6332673629124959 Validation Loss: 1.6598260402679443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 399: Training Loss: 1.6317882935206096 Validation Loss: 1.6592501401901245\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 400: Training Loss: 1.62999161084493 Validation Loss: 1.657353162765503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 401: Training Loss: 1.6285337209701538 Validation Loss: 1.6569492816925049\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 402: Training Loss: 1.6269845167795818 Validation Loss: 1.6572983264923096\n",
      "Epoch 403: Training Loss: 1.625456690788269 Validation Loss: 1.6563479900360107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 404: Training Loss: 1.623863975207011 Validation Loss: 1.6514469385147095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 405: Training Loss: 1.6228105227152507 Validation Loss: 1.6541693210601807\n",
      "Epoch 406: Training Loss: 1.6204686959584553 Validation Loss: 1.651029348373413\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 407: Training Loss: 1.619507908821106 Validation Loss: 1.6443724632263184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 408: Training Loss: 1.617613951365153 Validation Loss: 1.6460283994674683\n",
      "Epoch 409: Training Loss: 1.6156208515167236 Validation Loss: 1.6473675966262817\n",
      "Epoch 410: Training Loss: 1.6143333911895752 Validation Loss: 1.6444625854492188\n",
      "Epoch 411: Training Loss: 1.6125717560450237 Validation Loss: 1.6470081806182861\n",
      "Epoch 412: Training Loss: 1.6113402048746746 Validation Loss: 1.644503116607666\n",
      "Epoch 413: Training Loss: 1.6093868017196655 Validation Loss: 1.6416513919830322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 414: Training Loss: 1.6076160669326782 Validation Loss: 1.6404969692230225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 415: Training Loss: 1.6062995195388794 Validation Loss: 1.640305995941162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 416: Training Loss: 1.6044571002324421 Validation Loss: 1.6366007328033447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 417: Training Loss: 1.6027735074361165 Validation Loss: 1.6351417303085327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 418: Training Loss: 1.601515809694926 Validation Loss: 1.6356828212738037\n",
      "Epoch 419: Training Loss: 1.5997994343439739 Validation Loss: 1.6336392164230347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 420: Training Loss: 1.5980328718821208 Validation Loss: 1.6329374313354492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 421: Training Loss: 1.5966943105061848 Validation Loss: 1.6300017833709717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 422: Training Loss: 1.5947623650232952 Validation Loss: 1.6301720142364502\n",
      "Epoch 423: Training Loss: 1.5936808586120605 Validation Loss: 1.6331586837768555\n",
      "Epoch 424: Training Loss: 1.5918796857198079 Validation Loss: 1.6296942234039307\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 425: Training Loss: 1.590126633644104 Validation Loss: 1.6256917715072632\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 426: Training Loss: 1.5895522038141887 Validation Loss: 1.62370765209198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 427: Training Loss: 1.5876552661259968 Validation Loss: 1.619789958000183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 428: Training Loss: 1.5856078465779622 Validation Loss: 1.6222254037857056\n",
      "Epoch 429: Training Loss: 1.58418075243632 Validation Loss: 1.6265281438827515\n",
      "Epoch 430: Training Loss: 1.5825812021891277 Validation Loss: 1.623266577720642\n",
      "Epoch 431: Training Loss: 1.5810808340708415 Validation Loss: 1.6208446025848389\n",
      "Epoch 432: Training Loss: 1.5792314608891804 Validation Loss: 1.6170649528503418\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433: Training Loss: 1.5777837038040161 Validation Loss: 1.6168043613433838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 434: Training Loss: 1.576205054918925 Validation Loss: 1.6162970066070557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 435: Training Loss: 1.5748799244562786 Validation Loss: 1.616729497909546\n",
      "Epoch 436: Training Loss: 1.5732481082280476 Validation Loss: 1.6111451387405396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 437: Training Loss: 1.5715224345525105 Validation Loss: 1.6096185445785522\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 438: Training Loss: 1.570199688275655 Validation Loss: 1.6087584495544434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 439: Training Loss: 1.5690266688664753 Validation Loss: 1.6137926578521729\n",
      "Epoch 440: Training Loss: 1.5674731731414795 Validation Loss: 1.6100022792816162\n",
      "Epoch 441: Training Loss: 1.5651204188664753 Validation Loss: 1.6064116954803467\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 442: Training Loss: 1.563773234685262 Validation Loss: 1.6037590503692627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 443: Training Loss: 1.5625317096710205 Validation Loss: 1.6033384799957275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 444: Training Loss: 1.5609397490819295 Validation Loss: 1.607436180114746\n",
      "Epoch 445: Training Loss: 1.559430440266927 Validation Loss: 1.6057504415512085\n",
      "Epoch 446: Training Loss: 1.5574739376703899 Validation Loss: 1.6018447875976562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 447: Training Loss: 1.55665123462677 Validation Loss: 1.597554087638855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 448: Training Loss: 1.554779291152954 Validation Loss: 1.5968457460403442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 449: Training Loss: 1.5530585050582886 Validation Loss: 1.598326563835144\n",
      "Epoch 450: Training Loss: 1.551315426826477 Validation Loss: 1.597840428352356\n",
      "Epoch 451: Training Loss: 1.5504244565963745 Validation Loss: 1.595465898513794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 452: Training Loss: 1.5484336217244465 Validation Loss: 1.593237042427063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 453: Training Loss: 1.5476148128509521 Validation Loss: 1.5936861038208008\n",
      "Epoch 454: Training Loss: 1.5460683107376099 Validation Loss: 1.5904911756515503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 455: Training Loss: 1.544053077697754 Validation Loss: 1.5921493768692017\n",
      "Epoch 456: Training Loss: 1.5421905120213826 Validation Loss: 1.5924116373062134\n",
      "Epoch 457: Training Loss: 1.5406946341196697 Validation Loss: 1.5900993347167969\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 458: Training Loss: 1.539403756459554 Validation Loss: 1.5877766609191895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 459: Training Loss: 1.537626028060913 Validation Loss: 1.582319974899292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 460: Training Loss: 1.5365370512008667 Validation Loss: 1.5817474126815796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 461: Training Loss: 1.5346226692199707 Validation Loss: 1.582636833190918\n",
      "Epoch 462: Training Loss: 1.5336166222890217 Validation Loss: 1.582275629043579\n",
      "Epoch 463: Training Loss: 1.5316551129023235 Validation Loss: 1.5832239389419556\n",
      "Epoch 464: Training Loss: 1.5308629671732585 Validation Loss: 1.5811657905578613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 465: Training Loss: 1.5291246175765991 Validation Loss: 1.5810421705245972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 466: Training Loss: 1.5273220936457317 Validation Loss: 1.577789545059204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 467: Training Loss: 1.525781273841858 Validation Loss: 1.574159860610962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 468: Training Loss: 1.5243604977925618 Validation Loss: 1.576578974723816\n",
      "Epoch 469: Training Loss: 1.5224273602167766 Validation Loss: 1.5755456686019897\n",
      "Epoch 470: Training Loss: 1.5217541456222534 Validation Loss: 1.5754567384719849\n",
      "Epoch 471: Training Loss: 1.519508918126424 Validation Loss: 1.569950819015503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 472: Training Loss: 1.5184255043665569 Validation Loss: 1.5681240558624268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 473: Training Loss: 1.5165671904881795 Validation Loss: 1.5680049657821655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 474: Training Loss: 1.5152495702107747 Validation Loss: 1.5677376985549927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 475: Training Loss: 1.5142892996470134 Validation Loss: 1.5707194805145264\n",
      "Epoch 476: Training Loss: 1.5120503107706706 Validation Loss: 1.5666613578796387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 477: Training Loss: 1.5110501050949097 Validation Loss: 1.5655169486999512\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 478: Training Loss: 1.5094117720921834 Validation Loss: 1.5649253129959106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 479: Training Loss: 1.507569392522176 Validation Loss: 1.5631614923477173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 480: Training Loss: 1.5062095721562703 Validation Loss: 1.5612304210662842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 481: Training Loss: 1.5050838788350422 Validation Loss: 1.5595299005508423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 482: Training Loss: 1.50432288646698 Validation Loss: 1.5547226667404175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 483: Training Loss: 1.502111514409383 Validation Loss: 1.5591298341751099\n",
      "Epoch 484: Training Loss: 1.5003968477249146 Validation Loss: 1.5624561309814453\n",
      "Epoch 485: Training Loss: 1.4996074040730794 Validation Loss: 1.5615270137786865\n",
      "Epoch 486: Training Loss: 1.4974214633305867 Validation Loss: 1.5542856454849243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 487: Training Loss: 1.4956052700678508 Validation Loss: 1.550925612449646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 488: Training Loss: 1.494365652402242 Validation Loss: 1.550779104232788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 489: Training Loss: 1.492977778116862 Validation Loss: 1.5517714023590088\n",
      "Epoch 490: Training Loss: 1.4916351636250813 Validation Loss: 1.550642967224121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 491: Training Loss: 1.4902721246083577 Validation Loss: 1.5479782819747925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 492: Training Loss: 1.4884219964345295 Validation Loss: 1.5491389036178589\n",
      "Epoch 493: Training Loss: 1.4878084262212117 Validation Loss: 1.5517619848251343\n",
      "Epoch 494: Training Loss: 1.4857182502746582 Validation Loss: 1.547473669052124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 495: Training Loss: 1.4845017194747925 Validation Loss: 1.5408467054367065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 496: Training Loss: 1.4833152294158936 Validation Loss: 1.542629361152649\n",
      "Epoch 497: Training Loss: 1.481866717338562 Validation Loss: 1.5479530096054077\n",
      "Epoch 498: Training Loss: 1.4798883994420369 Validation Loss: 1.545578956604004\n",
      "Epoch 499: Training Loss: 1.4786110719045003 Validation Loss: 1.5387316942214966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 500: Training Loss: 1.4769576787948608 Validation Loss: 1.5356664657592773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 501: Training Loss: 1.475611646970113 Validation Loss: 1.5383856296539307\n",
      "Epoch 502: Training Loss: 1.4740334351857503 Validation Loss: 1.5388041734695435\n",
      "Epoch 503: Training Loss: 1.4726920922597249 Validation Loss: 1.5411287546157837\n",
      "Epoch 504: Training Loss: 1.4711976846059163 Validation Loss: 1.5358206033706665\n",
      "Epoch 505: Training Loss: 1.4699570337931316 Validation Loss: 1.5357069969177246\n",
      "Epoch 506: Training Loss: 1.4682020743687947 Validation Loss: 1.5313268899917603\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 507: Training Loss: 1.4667398929595947 Validation Loss: 1.5278047323226929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 508: Training Loss: 1.4654951492945354 Validation Loss: 1.5296026468276978\n",
      "Epoch 509: Training Loss: 1.4637007315953572 Validation Loss: 1.5316426753997803\n",
      "Epoch 510: Training Loss: 1.4624773263931274 Validation Loss: 1.532138466835022\n",
      "Epoch 511: Training Loss: 1.4613700310389202 Validation Loss: 1.5263200998306274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 512: Training Loss: 1.459418773651123 Validation Loss: 1.5249717235565186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 513: Training Loss: 1.4584142764409382 Validation Loss: 1.5249648094177246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 514: Training Loss: 1.4564561049143474 Validation Loss: 1.5232566595077515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 515: Training Loss: 1.4556996027628581 Validation Loss: 1.5231300592422485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 516: Training Loss: 1.453949769337972 Validation Loss: 1.5269442796707153\n",
      "Epoch 517: Training Loss: 1.453387935956319 Validation Loss: 1.526431918144226\n",
      "Epoch 518: Training Loss: 1.4528698126475017 Validation Loss: 1.5147287845611572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 519: Training Loss: 1.4504724740982056 Validation Loss: 1.5164825916290283\n",
      "Epoch 520: Training Loss: 1.4483208656311035 Validation Loss: 1.51816987991333\n",
      "Epoch 521: Training Loss: 1.4466164112091064 Validation Loss: 1.5169415473937988\n",
      "Epoch 522: Training Loss: 1.4453706741333008 Validation Loss: 1.5168607234954834\n",
      "Epoch 523: Training Loss: 1.4436871608098347 Validation Loss: 1.5148872137069702\n",
      "Epoch 524: Training Loss: 1.4422993659973145 Validation Loss: 1.5128027200698853\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 525: Training Loss: 1.4412169853846233 Validation Loss: 1.5121811628341675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 526: Training Loss: 1.4404260317484539 Validation Loss: 1.508629560470581\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 527: Training Loss: 1.43905774752299 Validation Loss: 1.5136916637420654\n",
      "Epoch 528: Training Loss: 1.4373126029968262 Validation Loss: 1.5099520683288574\n",
      "Epoch 529: Training Loss: 1.4353099664052327 Validation Loss: 1.5083612203598022\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 530: Training Loss: 1.4343997637430828 Validation Loss: 1.5061993598937988\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 531: Training Loss: 1.4330588181813557 Validation Loss: 1.5051395893096924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 532: Training Loss: 1.4317819277445476 Validation Loss: 1.5116474628448486\n",
      "Epoch 533: Training Loss: 1.4301717281341553 Validation Loss: 1.5070691108703613\n",
      "Epoch 534: Training Loss: 1.4283761183420818 Validation Loss: 1.5017893314361572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 535: Training Loss: 1.426852862040202 Validation Loss: 1.4995282888412476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 536: Training Loss: 1.4258768558502197 Validation Loss: 1.5002700090408325\n",
      "Epoch 537: Training Loss: 1.4247074127197266 Validation Loss: 1.501741886138916\n",
      "Epoch 538: Training Loss: 1.4235095977783203 Validation Loss: 1.497936487197876\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 539: Training Loss: 1.421649734179179 Validation Loss: 1.4981077909469604\n",
      "Epoch 540: Training Loss: 1.420598030090332 Validation Loss: 1.4981971979141235\n",
      "Epoch 541: Training Loss: 1.4187346696853638 Validation Loss: 1.4966990947723389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 542: Training Loss: 1.417930245399475 Validation Loss: 1.4978348016738892\n",
      "Epoch 543: Training Loss: 1.4160126845041912 Validation Loss: 1.4957717657089233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 544: Training Loss: 1.4146989583969116 Validation Loss: 1.4916712045669556\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 545: Training Loss: 1.4139914512634277 Validation Loss: 1.4903435707092285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 546: Training Loss: 1.411983807881673 Validation Loss: 1.48807954788208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 547: Training Loss: 1.4109646479288738 Validation Loss: 1.4861911535263062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 548: Training Loss: 1.409279505411784 Validation Loss: 1.488086223602295\n",
      "Epoch 549: Training Loss: 1.4085866610209148 Validation Loss: 1.4959797859191895\n",
      "Epoch 550: Training Loss: 1.4076889753341675 Validation Loss: 1.4886854887008667\n",
      "Epoch 551: Training Loss: 1.4054800669352214 Validation Loss: 1.485685110092163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 552: Training Loss: 1.4045862754185994 Validation Loss: 1.4794248342514038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 553: Training Loss: 1.4027671813964844 Validation Loss: 1.480170488357544\n",
      "Epoch 554: Training Loss: 1.4007747570673625 Validation Loss: 1.4858022928237915\n",
      "Epoch 555: Training Loss: 1.3997676372528076 Validation Loss: 1.4854650497436523\n",
      "Epoch 556: Training Loss: 1.3986039956410725 Validation Loss: 1.4822497367858887\n",
      "Epoch 557: Training Loss: 1.3966288566589355 Validation Loss: 1.4773997068405151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 558: Training Loss: 1.3954951365788777 Validation Loss: 1.4765539169311523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 559: Training Loss: 1.3938855330149333 Validation Loss: 1.4791312217712402\n",
      "Epoch 560: Training Loss: 1.392837643623352 Validation Loss: 1.4778014421463013\n",
      "Epoch 561: Training Loss: 1.3915726741154988 Validation Loss: 1.4786423444747925\n",
      "Epoch 562: Training Loss: 1.389830470085144 Validation Loss: 1.4739371538162231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 563: Training Loss: 1.3891976277033489 Validation Loss: 1.4704288244247437\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 564: Training Loss: 1.3875150283177693 Validation Loss: 1.4728777408599854\n",
      "Epoch 565: Training Loss: 1.3860714435577393 Validation Loss: 1.4724228382110596\n",
      "Epoch 566: Training Loss: 1.3848737478256226 Validation Loss: 1.4747135639190674\n",
      "Epoch 567: Training Loss: 1.3844330310821533 Validation Loss: 1.4676233530044556\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 568: Training Loss: 1.3819973866144817 Validation Loss: 1.468393087387085\n",
      "Epoch 569: Training Loss: 1.3805794318517048 Validation Loss: 1.4672796726226807\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 570: Training Loss: 1.3795102834701538 Validation Loss: 1.4630515575408936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 571: Training Loss: 1.3778078158696492 Validation Loss: 1.4644806385040283\n",
      "Epoch 572: Training Loss: 1.3764207363128662 Validation Loss: 1.4640989303588867\n",
      "Epoch 573: Training Loss: 1.3752164045969646 Validation Loss: 1.466942310333252\n",
      "Epoch 574: Training Loss: 1.3739680449167888 Validation Loss: 1.4658843278884888\n",
      "Epoch 575: Training Loss: 1.3724675973256428 Validation Loss: 1.4625710248947144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 576: Training Loss: 1.3711922963460286 Validation Loss: 1.4611890316009521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 577: Training Loss: 1.3704469203948975 Validation Loss: 1.4606484174728394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 578: Training Loss: 1.3682609001795452 Validation Loss: 1.458066701889038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 579: Training Loss: 1.367069959640503 Validation Loss: 1.4571863412857056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 580: Training Loss: 1.3658872445424397 Validation Loss: 1.4581416845321655\n",
      "Epoch 581: Training Loss: 1.3648290236790974 Validation Loss: 1.460710883140564\n",
      "Epoch 582: Training Loss: 1.363632361094157 Validation Loss: 1.4534318447113037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 583: Training Loss: 1.3622031609217327 Validation Loss: 1.4532921314239502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 584: Training Loss: 1.3604293664296467 Validation Loss: 1.4509695768356323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 585: Training Loss: 1.3591835896174114 Validation Loss: 1.4484854936599731\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 586: Training Loss: 1.3579148451487224 Validation Loss: 1.450434923171997\n",
      "Epoch 587: Training Loss: 1.356481393178304 Validation Loss: 1.450789213180542\n",
      "Epoch 588: Training Loss: 1.3553851048151653 Validation Loss: 1.45113205909729\n",
      "Epoch 589: Training Loss: 1.353670318921407 Validation Loss: 1.4460448026657104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 590: Training Loss: 1.3523342212041218 Validation Loss: 1.4450408220291138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 591: Training Loss: 1.3512927691141765 Validation Loss: 1.4462065696716309\n",
      "Epoch 592: Training Loss: 1.3498568932215373 Validation Loss: 1.4475892782211304\n",
      "Epoch 593: Training Loss: 1.3494073549906414 Validation Loss: 1.4422764778137207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 594: Training Loss: 1.347944974899292 Validation Loss: 1.4453071355819702\n",
      "Epoch 595: Training Loss: 1.3461472193400066 Validation Loss: 1.441750168800354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 596: Training Loss: 1.3447626829147339 Validation Loss: 1.4407141208648682\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 597: Training Loss: 1.3432551622390747 Validation Loss: 1.4419705867767334\n",
      "Epoch 598: Training Loss: 1.342853347460429 Validation Loss: 1.4450353384017944\n",
      "Epoch 599: Training Loss: 1.3411475817362468 Validation Loss: 1.4401869773864746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 600: Training Loss: 1.339172124862671 Validation Loss: 1.4342538118362427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 601: Training Loss: 1.3385568459828694 Validation Loss: 1.431450605392456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 602: Training Loss: 1.337316632270813 Validation Loss: 1.4363832473754883\n",
      "Epoch 603: Training Loss: 1.3355787992477417 Validation Loss: 1.437288522720337\n",
      "Epoch 604: Training Loss: 1.3341310024261475 Validation Loss: 1.4343836307525635\n",
      "Epoch 605: Training Loss: 1.3326443831125896 Validation Loss: 1.432004451751709\n",
      "Epoch 606: Training Loss: 1.332070787747701 Validation Loss: 1.4316805601119995\n",
      "Epoch 607: Training Loss: 1.3304946422576904 Validation Loss: 1.4321070909500122\n",
      "Epoch 608: Training Loss: 1.3290782769521077 Validation Loss: 1.429441213607788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 609: Training Loss: 1.3280383348464966 Validation Loss: 1.426383137702942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 610: Training Loss: 1.326981544494629 Validation Loss: 1.4297808408737183\n",
      "Epoch 611: Training Loss: 1.3254217704137166 Validation Loss: 1.430972695350647\n",
      "Epoch 612: Training Loss: 1.3241557677586873 Validation Loss: 1.4282256364822388\n",
      "Epoch 613: Training Loss: 1.3231807152430217 Validation Loss: 1.4214366674423218\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 614: Training Loss: 1.322278102238973 Validation Loss: 1.4226304292678833\n",
      "Epoch 615: Training Loss: 1.3199071486790974 Validation Loss: 1.423755168914795\n",
      "Epoch 616: Training Loss: 1.3186725775400798 Validation Loss: 1.4234941005706787\n",
      "Epoch 617: Training Loss: 1.3174323638280232 Validation Loss: 1.4208669662475586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 618: Training Loss: 1.3162612120310466 Validation Loss: 1.4205491542816162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 619: Training Loss: 1.315293272336324 Validation Loss: 1.4176666736602783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 620: Training Loss: 1.3134977420171101 Validation Loss: 1.421512246131897\n",
      "Epoch 621: Training Loss: 1.3127875725428264 Validation Loss: 1.4203234910964966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 622: Training Loss: 1.311316728591919 Validation Loss: 1.415574312210083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 623: Training Loss: 1.3102496067682903 Validation Loss: 1.4163734912872314\n",
      "Epoch 624: Training Loss: 1.3093400796254475 Validation Loss: 1.4172881841659546\n",
      "Epoch 625: Training Loss: 1.3072703282038372 Validation Loss: 1.4131544828414917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 626: Training Loss: 1.3063737551371257 Validation Loss: 1.4105491638183594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 627: Training Loss: 1.3060084184010823 Validation Loss: 1.4133589267730713\n",
      "Epoch 628: Training Loss: 1.303458571434021 Validation Loss: 1.4106868505477905\n",
      "Epoch 629: Training Loss: 1.3023405472437541 Validation Loss: 1.4091914892196655\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 630: Training Loss: 1.3010692596435547 Validation Loss: 1.4086018800735474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 631: Training Loss: 1.3007220427195232 Validation Loss: 1.406356930732727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 632: Training Loss: 1.2985899845759075 Validation Loss: 1.4093044996261597\n",
      "Epoch 633: Training Loss: 1.2980802456537883 Validation Loss: 1.4130038022994995\n",
      "Epoch 634: Training Loss: 1.2961057027180989 Validation Loss: 1.4043595790863037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 635: Training Loss: 1.294862151145935 Validation Loss: 1.401031494140625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 636: Training Loss: 1.2937227884928386 Validation Loss: 1.402579426765442\n",
      "Epoch 637: Training Loss: 1.2927800416946411 Validation Loss: 1.4084162712097168\n",
      "Epoch 638: Training Loss: 1.2913927634557087 Validation Loss: 1.4053577184677124\n",
      "Epoch 639: Training Loss: 1.2915475765864055 Validation Loss: 1.395673394203186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 640: Training Loss: 1.2886018753051758 Validation Loss: 1.399394154548645\n",
      "Epoch 641: Training Loss: 1.288148283958435 Validation Loss: 1.4040621519088745\n",
      "Epoch 642: Training Loss: 1.2864699761072795 Validation Loss: 1.4000972509384155\n",
      "Epoch 643: Training Loss: 1.2849357922871907 Validation Loss: 1.3960129022598267\n",
      "Epoch 644: Training Loss: 1.2838090260823567 Validation Loss: 1.393855094909668\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 645: Training Loss: 1.2826904853185017 Validation Loss: 1.3929672241210938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 646: Training Loss: 1.2822643121083577 Validation Loss: 1.3971526622772217\n",
      "Epoch 647: Training Loss: 1.2801043192545574 Validation Loss: 1.3971748352050781\n",
      "Epoch 648: Training Loss: 1.279152790705363 Validation Loss: 1.3981982469558716\n",
      "Epoch 649: Training Loss: 1.2774149576822917 Validation Loss: 1.3918157815933228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 650: Training Loss: 1.2773348490397136 Validation Loss: 1.3850334882736206\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 651: Training Loss: 1.2752010822296143 Validation Loss: 1.3887299299240112\n",
      "Epoch 652: Training Loss: 1.2735492785771687 Validation Loss: 1.3928263187408447\n",
      "Epoch 653: Training Loss: 1.2728805939356487 Validation Loss: 1.3912675380706787\n",
      "Epoch 654: Training Loss: 1.2714967330296834 Validation Loss: 1.3874386548995972\n",
      "Epoch 655: Training Loss: 1.2700756390889485 Validation Loss: 1.3863871097564697\n",
      "Epoch 656: Training Loss: 1.2687426805496216 Validation Loss: 1.3850315809249878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 657: Training Loss: 1.2686267693837483 Validation Loss: 1.3827275037765503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 658: Training Loss: 1.2672617435455322 Validation Loss: 1.3886332511901855\n",
      "Epoch 659: Training Loss: 1.265237808227539 Validation Loss: 1.3869659900665283\n",
      "Epoch 660: Training Loss: 1.2640109062194824 Validation Loss: 1.383318305015564\n",
      "Epoch 661: Training Loss: 1.262680212656657 Validation Loss: 1.376031517982483\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 662: Training Loss: 1.2621775070826213 Validation Loss: 1.3741225004196167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 663: Training Loss: 1.260113000869751 Validation Loss: 1.3798025846481323\n",
      "Epoch 664: Training Loss: 1.259271780649821 Validation Loss: 1.381742000579834\n",
      "Epoch 665: Training Loss: 1.2583202918370564 Validation Loss: 1.380910038948059\n",
      "Epoch 666: Training Loss: 1.2568241755167644 Validation Loss: 1.3794915676116943\n",
      "Epoch 667: Training Loss: 1.2556227445602417 Validation Loss: 1.3780860900878906\n",
      "Epoch 668: Training Loss: 1.254146138827006 Validation Loss: 1.3753204345703125\n",
      "Epoch 669: Training Loss: 1.2530649105707805 Validation Loss: 1.3744876384735107\n",
      "Epoch 670: Training Loss: 1.2525746027628581 Validation Loss: 1.3698346614837646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 671: Training Loss: 1.250627915064494 Validation Loss: 1.373949408531189\n",
      "Epoch 672: Training Loss: 1.251292626063029 Validation Loss: 1.3798657655715942\n",
      "Epoch 673: Training Loss: 1.2488597234090169 Validation Loss: 1.3698750734329224\n",
      "Epoch 674: Training Loss: 1.2468888362248738 Validation Loss: 1.3663053512573242\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 675: Training Loss: 1.24636971950531 Validation Loss: 1.369581937789917\n",
      "Epoch 676: Training Loss: 1.2441715002059937 Validation Loss: 1.3694967031478882\n",
      "Epoch 677: Training Loss: 1.2435287237167358 Validation Loss: 1.3721519708633423\n",
      "Epoch 678: Training Loss: 1.242180585861206 Validation Loss: 1.3678096532821655\n",
      "Epoch 679: Training Loss: 1.240915020306905 Validation Loss: 1.3627291917800903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 680: Training Loss: 1.2398234605789185 Validation Loss: 1.3633029460906982\n",
      "Epoch 681: Training Loss: 1.238563338915507 Validation Loss: 1.365556001663208\n",
      "Epoch 682: Training Loss: 1.2370683352152507 Validation Loss: 1.365610122680664\n",
      "Epoch 683: Training Loss: 1.2362085183461506 Validation Loss: 1.3630727529525757\n",
      "Epoch 684: Training Loss: 1.2348520358403523 Validation Loss: 1.3613461256027222\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 685: Training Loss: 1.2335864305496216 Validation Loss: 1.3616048097610474\n",
      "Epoch 686: Training Loss: 1.2324845393498738 Validation Loss: 1.362038493156433\n",
      "Epoch 687: Training Loss: 1.2314647436141968 Validation Loss: 1.3577033281326294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 688: Training Loss: 1.2299240032831829 Validation Loss: 1.3567547798156738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 689: Training Loss: 1.2286944389343262 Validation Loss: 1.3554737567901611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 690: Training Loss: 1.228468418121338 Validation Loss: 1.3573150634765625\n",
      "Epoch 691: Training Loss: 1.2268948952356975 Validation Loss: 1.3588393926620483\n",
      "Epoch 692: Training Loss: 1.2255690892537434 Validation Loss: 1.3568670749664307\n",
      "Epoch 693: Training Loss: 1.224220871925354 Validation Loss: 1.3533295392990112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 694: Training Loss: 1.2228795289993286 Validation Loss: 1.3508152961730957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 695: Training Loss: 1.2219698031743367 Validation Loss: 1.352145791053772\n",
      "Epoch 696: Training Loss: 1.2205667893091838 Validation Loss: 1.3520524501800537\n",
      "Epoch 697: Training Loss: 1.2194725275039673 Validation Loss: 1.3527169227600098\n",
      "Epoch 698: Training Loss: 1.2194431622823079 Validation Loss: 1.352783441543579\n",
      "Epoch 699: Training Loss: 1.2174759705861409 Validation Loss: 1.3451764583587646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 700: Training Loss: 1.2169133424758911 Validation Loss: 1.3430877923965454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 701: Training Loss: 1.214661955833435 Validation Loss: 1.3492140769958496\n",
      "Epoch 702: Training Loss: 1.2140541871388753 Validation Loss: 1.3505947589874268\n",
      "Epoch 703: Training Loss: 1.2133445739746094 Validation Loss: 1.3453582525253296\n",
      "Epoch 704: Training Loss: 1.2115866740544636 Validation Loss: 1.3425487279891968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 705: Training Loss: 1.210262656211853 Validation Loss: 1.34200119972229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 706: Training Loss: 1.2087548573811848 Validation Loss: 1.3445419073104858\n",
      "Epoch 707: Training Loss: 1.2082488934199016 Validation Loss: 1.3429100513458252\n",
      "Epoch 708: Training Loss: 1.2065825462341309 Validation Loss: 1.3435713052749634\n",
      "Epoch 709: Training Loss: 1.205397129058838 Validation Loss: 1.3419787883758545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 710: Training Loss: 1.2046743631362915 Validation Loss: 1.3418127298355103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 711: Training Loss: 1.2033198674519856 Validation Loss: 1.3372946977615356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 712: Training Loss: 1.2020339171091716 Validation Loss: 1.335684895515442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 713: Training Loss: 1.2007057269414265 Validation Loss: 1.3377317190170288\n",
      "Epoch 714: Training Loss: 1.1998097499211628 Validation Loss: 1.3385447263717651\n",
      "Epoch 715: Training Loss: 1.198703130086263 Validation Loss: 1.334395170211792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 716: Training Loss: 1.1979732116063435 Validation Loss: 1.334416151046753\n",
      "Epoch 717: Training Loss: 1.1969483296076457 Validation Loss: 1.3371425867080688\n",
      "Epoch 718: Training Loss: 1.1949663162231445 Validation Loss: 1.3342807292938232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 719: Training Loss: 1.1939584414164226 Validation Loss: 1.3289815187454224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 720: Training Loss: 1.1934140523274739 Validation Loss: 1.3298652172088623\n",
      "Epoch 721: Training Loss: 1.19162122408549 Validation Loss: 1.3289523124694824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 722: Training Loss: 1.1902987559636433 Validation Loss: 1.3302558660507202\n",
      "Epoch 723: Training Loss: 1.189518133799235 Validation Loss: 1.334505558013916\n",
      "Epoch 724: Training Loss: 1.1884581645329793 Validation Loss: 1.329254150390625\n",
      "Epoch 725: Training Loss: 1.1871616045633953 Validation Loss: 1.3252676725387573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 726: Training Loss: 1.186476429303487 Validation Loss: 1.3280963897705078\n",
      "Epoch 727: Training Loss: 1.1849261124928792 Validation Loss: 1.3264225721359253\n",
      "Epoch 728: Training Loss: 1.183810551961263 Validation Loss: 1.3258081674575806\n",
      "Epoch 729: Training Loss: 1.1828819513320923 Validation Loss: 1.3238731622695923\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 730: Training Loss: 1.181566556294759 Validation Loss: 1.3211891651153564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 731: Training Loss: 1.180372714996338 Validation Loss: 1.3191230297088623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 732: Training Loss: 1.1791878541310628 Validation Loss: 1.3218915462493896\n",
      "Epoch 733: Training Loss: 1.1780473391215007 Validation Loss: 1.3239880800247192\n",
      "Epoch 734: Training Loss: 1.1765884160995483 Validation Loss: 1.3236593008041382\n",
      "Epoch 735: Training Loss: 1.176121195157369 Validation Loss: 1.3214930295944214\n",
      "Epoch 736: Training Loss: 1.1751564741134644 Validation Loss: 1.3137755393981934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 737: Training Loss: 1.173944075902303 Validation Loss: 1.3152532577514648\n",
      "Epoch 738: Training Loss: 1.1723474264144897 Validation Loss: 1.3195228576660156\n",
      "Epoch 739: Training Loss: 1.171866774559021 Validation Loss: 1.3227462768554688\n",
      "Epoch 740: Training Loss: 1.1704202890396118 Validation Loss: 1.3147326707839966\n",
      "Epoch 741: Training Loss: 1.1701332330703735 Validation Loss: 1.3083254098892212\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 742: Training Loss: 1.1681796312332153 Validation Loss: 1.312095284461975\n",
      "Epoch 743: Training Loss: 1.167350212732951 Validation Loss: 1.320542812347412\n",
      "Epoch 744: Training Loss: 1.1663827498753865 Validation Loss: 1.3157378435134888\n",
      "Epoch 745: Training Loss: 1.1647757689158122 Validation Loss: 1.3092231750488281\n",
      "Epoch 746: Training Loss: 1.1633642117182414 Validation Loss: 1.3074017763137817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 747: Training Loss: 1.1619969209035237 Validation Loss: 1.31011164188385\n",
      "Epoch 748: Training Loss: 1.1612626711527507 Validation Loss: 1.312839388847351\n",
      "Epoch 749: Training Loss: 1.1600497961044312 Validation Loss: 1.3080165386199951\n",
      "Epoch 750: Training Loss: 1.1589577198028564 Validation Loss: 1.305173397064209\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 751: Training Loss: 1.157740632692973 Validation Loss: 1.3052546977996826\n",
      "Epoch 752: Training Loss: 1.1566853523254395 Validation Loss: 1.308817744255066\n",
      "Epoch 753: Training Loss: 1.1553454399108887 Validation Loss: 1.3070883750915527\n",
      "Epoch 754: Training Loss: 1.1547201077143352 Validation Loss: 1.3052096366882324\n",
      "Epoch 755: Training Loss: 1.1536033948262532 Validation Loss: 1.3036311864852905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 756: Training Loss: 1.1523314714431763 Validation Loss: 1.304194688796997\n",
      "Epoch 757: Training Loss: 1.151088039080302 Validation Loss: 1.3011077642440796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 758: Training Loss: 1.1513832807540894 Validation Loss: 1.2961479425430298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 759: Training Loss: 1.1492164134979248 Validation Loss: 1.3023593425750732\n",
      "Epoch 760: Training Loss: 1.1481732527414958 Validation Loss: 1.3077359199523926\n",
      "Epoch 761: Training Loss: 1.1469788551330566 Validation Loss: 1.301701307296753\n",
      "Epoch 762: Training Loss: 1.1456857522328694 Validation Loss: 1.2935481071472168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 763: Training Loss: 1.144953966140747 Validation Loss: 1.2915042638778687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 764: Training Loss: 1.1438117027282715 Validation Loss: 1.2976343631744385\n",
      "Epoch 765: Training Loss: 1.1425625483194988 Validation Loss: 1.2964895963668823\n",
      "Epoch 766: Training Loss: 1.1415741443634033 Validation Loss: 1.297989010810852\n",
      "Epoch 767: Training Loss: 1.1401283740997314 Validation Loss: 1.294732689857483\n",
      "Epoch 768: Training Loss: 1.139521559079488 Validation Loss: 1.2944343090057373\n",
      "Epoch 769: Training Loss: 1.138347824414571 Validation Loss: 1.2909966707229614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 770: Training Loss: 1.136931578318278 Validation Loss: 1.2939895391464233\n",
      "Epoch 771: Training Loss: 1.1364763577779133 Validation Loss: 1.294018030166626\n",
      "Epoch 772: Training Loss: 1.1352907419204712 Validation Loss: 1.287381887435913\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 773: Training Loss: 1.1336657603581746 Validation Loss: 1.2873051166534424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 774: Training Loss: 1.1323519150416057 Validation Loss: 1.2900103330612183\n",
      "Epoch 775: Training Loss: 1.1314120292663574 Validation Loss: 1.2916442155838013\n",
      "Epoch 776: Training Loss: 1.1306721766789753 Validation Loss: 1.2859116792678833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 777: Training Loss: 1.1292558908462524 Validation Loss: 1.2863004207611084\n",
      "Epoch 778: Training Loss: 1.128276030222575 Validation Loss: 1.2845960855484009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 779: Training Loss: 1.1269199053446453 Validation Loss: 1.2857203483581543\n",
      "Epoch 780: Training Loss: 1.1257446606953938 Validation Loss: 1.286953330039978\n",
      "Epoch 781: Training Loss: 1.1259555021921794 Validation Loss: 1.2867146730422974\n",
      "Epoch 782: Training Loss: 1.1241018772125244 Validation Loss: 1.2853566408157349\n",
      "Epoch 783: Training Loss: 1.1230761607487996 Validation Loss: 1.2798504829406738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 784: Training Loss: 1.1222318808237712 Validation Loss: 1.275696039199829\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 785: Training Loss: 1.120729684829712 Validation Loss: 1.2789441347122192\n",
      "Epoch 786: Training Loss: 1.1194062232971191 Validation Loss: 1.2821884155273438\n",
      "Epoch 787: Training Loss: 1.1187607049942017 Validation Loss: 1.2841428518295288\n",
      "Epoch 788: Training Loss: 1.1177490552266438 Validation Loss: 1.2777613401412964\n",
      "Epoch 789: Training Loss: 1.1166364749272664 Validation Loss: 1.2766499519348145\n",
      "Epoch 790: Training Loss: 1.1155323187510173 Validation Loss: 1.2746243476867676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 791: Training Loss: 1.1141064564387004 Validation Loss: 1.27560293674469\n",
      "Epoch 792: Training Loss: 1.114072362581889 Validation Loss: 1.2777684926986694\n",
      "Epoch 793: Training Loss: 1.1121877034505208 Validation Loss: 1.275808334350586\n",
      "Epoch 794: Training Loss: 1.1109398206075032 Validation Loss: 1.2763700485229492\n",
      "Epoch 795: Training Loss: 1.110584815343221 Validation Loss: 1.2727348804473877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 796: Training Loss: 1.1088132460912068 Validation Loss: 1.2690401077270508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 797: Training Loss: 1.1079990863800049 Validation Loss: 1.2678613662719727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 798: Training Loss: 1.1071012020111084 Validation Loss: 1.270779013633728\n",
      "Epoch 799: Training Loss: 1.1058725913365681 Validation Loss: 1.2742366790771484\n",
      "Epoch 800: Training Loss: 1.1052826245625813 Validation Loss: 1.2749247550964355\n",
      "Epoch 801: Training Loss: 1.1039259831110637 Validation Loss: 1.267529845237732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 802: Training Loss: 1.1029990116755168 Validation Loss: 1.2643383741378784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 803: Training Loss: 1.1017355521519978 Validation Loss: 1.2658530473709106\n",
      "Epoch 804: Training Loss: 1.1001029809315999 Validation Loss: 1.268725037574768\n",
      "Epoch 805: Training Loss: 1.0993283192316692 Validation Loss: 1.27089524269104\n",
      "Epoch 806: Training Loss: 1.098492980003357 Validation Loss: 1.2675915956497192\n",
      "Epoch 807: Training Loss: 1.0970906019210815 Validation Loss: 1.2635101079940796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 808: Training Loss: 1.0961318016052246 Validation Loss: 1.26045560836792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 809: Training Loss: 1.0955088535944622 Validation Loss: 1.2651431560516357\n",
      "Epoch 810: Training Loss: 1.0943861802419026 Validation Loss: 1.262680172920227\n",
      "Epoch 811: Training Loss: 1.093279242515564 Validation Loss: 1.2591522932052612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 812: Training Loss: 1.0921493768692017 Validation Loss: 1.2616462707519531\n",
      "Epoch 813: Training Loss: 1.0909337600072224 Validation Loss: 1.260627031326294\n",
      "Epoch 814: Training Loss: 1.089923103650411 Validation Loss: 1.2584384679794312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 815: Training Loss: 1.0887586275736492 Validation Loss: 1.259202480316162\n",
      "Epoch 816: Training Loss: 1.0876015027364094 Validation Loss: 1.257500410079956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 817: Training Loss: 1.0871126254399617 Validation Loss: 1.2549513578414917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 818: Training Loss: 1.0862838824590046 Validation Loss: 1.2612923383712769\n",
      "Epoch 819: Training Loss: 1.084883451461792 Validation Loss: 1.2576173543930054\n",
      "Epoch 820: Training Loss: 1.0838067928949993 Validation Loss: 1.25406014919281\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 821: Training Loss: 1.0825369755427043 Validation Loss: 1.2511736154556274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 822: Training Loss: 1.0815893014272053 Validation Loss: 1.2526180744171143\n",
      "Epoch 823: Training Loss: 1.0804680983225505 Validation Loss: 1.2524627447128296\n",
      "Epoch 824: Training Loss: 1.0794216394424438 Validation Loss: 1.249869704246521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 825: Training Loss: 1.0784785350163777 Validation Loss: 1.2491902112960815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 826: Training Loss: 1.07729971408844 Validation Loss: 1.2504334449768066\n",
      "Epoch 827: Training Loss: 1.076070229212443 Validation Loss: 1.2517144680023193\n",
      "Epoch 828: Training Loss: 1.0755074421564739 Validation Loss: 1.251518964767456\n",
      "Epoch 829: Training Loss: 1.074290156364441 Validation Loss: 1.2515355348587036\n",
      "Epoch 830: Training Loss: 1.0740844408671062 Validation Loss: 1.2470709085464478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 831: Training Loss: 1.0724306106567383 Validation Loss: 1.244504690170288\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 832: Training Loss: 1.0711265007654827 Validation Loss: 1.2455624341964722\n",
      "Epoch 833: Training Loss: 1.0704829295476277 Validation Loss: 1.248925805091858\n",
      "Epoch 834: Training Loss: 1.0700517098108928 Validation Loss: 1.2467217445373535\n",
      "Epoch 835: Training Loss: 1.0685099760691326 Validation Loss: 1.2435948848724365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 836: Training Loss: 1.0670807361602783 Validation Loss: 1.2418537139892578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 837: Training Loss: 1.0661696394284566 Validation Loss: 1.240294098854065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 838: Training Loss: 1.0653047561645508 Validation Loss: 1.2417590618133545\n",
      "Epoch 839: Training Loss: 1.0644218921661377 Validation Loss: 1.2435952425003052\n",
      "Epoch 840: Training Loss: 1.0637816190719604 Validation Loss: 1.2425944805145264\n",
      "Epoch 841: Training Loss: 1.0618627468744914 Validation Loss: 1.2370619773864746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 842: Training Loss: 1.0612436135609944 Validation Loss: 1.2360851764678955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 843: Training Loss: 1.060212254524231 Validation Loss: 1.2394404411315918\n",
      "Epoch 844: Training Loss: 1.0588016510009766 Validation Loss: 1.2399632930755615\n",
      "Epoch 845: Training Loss: 1.0582510034243267 Validation Loss: 1.2385751008987427\n",
      "Epoch 846: Training Loss: 1.0567924976348877 Validation Loss: 1.2338699102401733\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 847: Training Loss: 1.0563050111134846 Validation Loss: 1.2317826747894287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 848: Training Loss: 1.054744561513265 Validation Loss: 1.2353326082229614\n",
      "Epoch 849: Training Loss: 1.0541231234868367 Validation Loss: 1.2400864362716675\n",
      "Epoch 850: Training Loss: 1.0532962878545125 Validation Loss: 1.234158992767334\n",
      "Epoch 851: Training Loss: 1.0523373285929363 Validation Loss: 1.2301597595214844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 852: Training Loss: 1.0510459542274475 Validation Loss: 1.230916142463684\n",
      "Epoch 853: Training Loss: 1.0505021413167317 Validation Loss: 1.232515811920166\n",
      "Epoch 854: Training Loss: 1.0488582849502563 Validation Loss: 1.2317460775375366\n",
      "Epoch 855: Training Loss: 1.0482766230901082 Validation Loss: 1.2294347286224365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 856: Training Loss: 1.0475470622380574 Validation Loss: 1.2329891920089722\n",
      "Epoch 857: Training Loss: 1.0462594032287598 Validation Loss: 1.228551983833313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 858: Training Loss: 1.0446942647298176 Validation Loss: 1.2260305881500244\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 859: Training Loss: 1.0439836978912354 Validation Loss: 1.223976969718933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 860: Training Loss: 1.0431190133094788 Validation Loss: 1.2264996767044067\n",
      "Epoch 861: Training Loss: 1.041819969813029 Validation Loss: 1.2285325527191162\n",
      "Epoch 862: Training Loss: 1.041425069173177 Validation Loss: 1.2271883487701416\n",
      "Epoch 863: Training Loss: 1.0401722590128581 Validation Loss: 1.2245762348175049\n",
      "Epoch 864: Training Loss: 1.0387556354204814 Validation Loss: 1.2216569185256958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 865: Training Loss: 1.0379653573036194 Validation Loss: 1.2209482192993164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 866: Training Loss: 1.0371418197949727 Validation Loss: 1.2205650806427002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 867: Training Loss: 1.036255955696106 Validation Loss: 1.2251955270767212\n",
      "Epoch 868: Training Loss: 1.0352673927942913 Validation Loss: 1.225407361984253\n",
      "Epoch 869: Training Loss: 1.033796787261963 Validation Loss: 1.2198344469070435\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 870: Training Loss: 1.0329790314038594 Validation Loss: 1.2172034978866577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 871: Training Loss: 1.0323107242584229 Validation Loss: 1.2165062427520752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 872: Training Loss: 1.0309803485870361 Validation Loss: 1.2188819646835327\n",
      "Epoch 873: Training Loss: 1.0304515361785889 Validation Loss: 1.2174831628799438\n",
      "Epoch 874: Training Loss: 1.0292296608289082 Validation Loss: 1.220320224761963\n",
      "Epoch 875: Training Loss: 1.0288939873377483 Validation Loss: 1.2152031660079956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 876: Training Loss: 1.0272225141525269 Validation Loss: 1.2140649557113647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 877: Training Loss: 1.025962273279826 Validation Loss: 1.214837908744812\n",
      "Epoch 878: Training Loss: 1.0250360171000164 Validation Loss: 1.2155070304870605\n",
      "Epoch 879: Training Loss: 1.024710734685262 Validation Loss: 1.2133680582046509\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 880: Training Loss: 1.0230669776598613 Validation Loss: 1.2124111652374268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 881: Training Loss: 1.0226239363352458 Validation Loss: 1.211012601852417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 882: Training Loss: 1.0212437113126118 Validation Loss: 1.2154836654663086\n",
      "Epoch 883: Training Loss: 1.0204115708669026 Validation Loss: 1.2143250703811646\n",
      "Epoch 884: Training Loss: 1.0193239847819011 Validation Loss: 1.2087173461914062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 885: Training Loss: 1.0189059972763062 Validation Loss: 1.2043986320495605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 886: Training Loss: 1.017340362071991 Validation Loss: 1.2077292203903198\n",
      "Epoch 887: Training Loss: 1.0160234769185383 Validation Loss: 1.2101690769195557\n",
      "Epoch 888: Training Loss: 1.015248437722524 Validation Loss: 1.2098968029022217\n",
      "Epoch 889: Training Loss: 1.014206846555074 Validation Loss: 1.2072911262512207\n",
      "Epoch 890: Training Loss: 1.013502796490987 Validation Loss: 1.2073460817337036\n",
      "Epoch 891: Training Loss: 1.0122547547022502 Validation Loss: 1.2033032178878784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 892: Training Loss: 1.011806845664978 Validation Loss: 1.2015669345855713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 893: Training Loss: 1.0107725660006206 Validation Loss: 1.205550193786621\n",
      "Epoch 894: Training Loss: 1.0098918080329895 Validation Loss: 1.208835244178772\n",
      "Epoch 895: Training Loss: 1.0092446406682332 Validation Loss: 1.2054111957550049\n",
      "Epoch 896: Training Loss: 1.0076836744944255 Validation Loss: 1.2001224756240845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 897: Training Loss: 1.0064901908238728 Validation Loss: 1.1989673376083374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 898: Training Loss: 1.0055742859840393 Validation Loss: 1.1990317106246948\n",
      "Epoch 899: Training Loss: 1.0050304532051086 Validation Loss: 1.2011867761611938\n",
      "Epoch 900: Training Loss: 1.0037715037663777 Validation Loss: 1.1994242668151855\n",
      "Epoch 901: Training Loss: 1.0028870503107707 Validation Loss: 1.2001553773880005\n",
      "Epoch 902: Training Loss: 1.0019371112187703 Validation Loss: 1.1984822750091553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 903: Training Loss: 1.0012310147285461 Validation Loss: 1.1976964473724365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 904: Training Loss: 1.0000641544659932 Validation Loss: 1.1968306303024292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 905: Training Loss: 0.9989735086758932 Validation Loss: 1.1965183019638062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 906: Training Loss: 0.9981310764948527 Validation Loss: 1.1946090459823608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 907: Training Loss: 0.9971861839294434 Validation Loss: 1.19367516040802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 908: Training Loss: 0.9962990283966064 Validation Loss: 1.1951929330825806\n",
      "Epoch 909: Training Loss: 0.9951328237851461 Validation Loss: 1.193167805671692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 910: Training Loss: 0.9944354097048441 Validation Loss: 1.19426429271698\n",
      "Epoch 911: Training Loss: 0.9936838746070862 Validation Loss: 1.1920506954193115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 912: Training Loss: 0.9925181667009989 Validation Loss: 1.1902644634246826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 913: Training Loss: 0.9916548927625021 Validation Loss: 1.1943641901016235\n",
      "Epoch 914: Training Loss: 0.9906064867973328 Validation Loss: 1.1900534629821777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 915: Training Loss: 0.9898396929105123 Validation Loss: 1.1907575130462646\n",
      "Epoch 916: Training Loss: 0.9888098835945129 Validation Loss: 1.1879022121429443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 917: Training Loss: 0.9880424539248148 Validation Loss: 1.187375545501709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 918: Training Loss: 0.9868355790774027 Validation Loss: 1.1866545677185059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 919: Training Loss: 0.9873583912849426 Validation Loss: 1.1885915994644165\n",
      "Epoch 920: Training Loss: 0.9850023786226908 Validation Loss: 1.1865545511245728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 921: Training Loss: 0.9857234557469686 Validation Loss: 1.1844661235809326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 922: Training Loss: 0.9831969539324442 Validation Loss: 1.187530517578125\n",
      "Epoch 923: Training Loss: 0.9823939402898153 Validation Loss: 1.1889653205871582\n",
      "Epoch 924: Training Loss: 0.9815880656242371 Validation Loss: 1.1822632551193237\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 925: Training Loss: 0.9801552891731262 Validation Loss: 1.1802477836608887\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 926: Training Loss: 0.9795801440874735 Validation Loss: 1.180454969406128\n",
      "Epoch 927: Training Loss: 0.9783689379692078 Validation Loss: 1.1848748922348022\n",
      "Epoch 928: Training Loss: 0.9776477018992106 Validation Loss: 1.1876049041748047\n",
      "Epoch 929: Training Loss: 0.9764116605122884 Validation Loss: 1.1835072040557861\n",
      "Epoch 930: Training Loss: 0.9756568074226379 Validation Loss: 1.1781127452850342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 931: Training Loss: 0.9746072292327881 Validation Loss: 1.1753987073898315\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 932: Training Loss: 0.9740161697069804 Validation Loss: 1.1793681383132935\n",
      "Epoch 933: Training Loss: 0.9734456539154053 Validation Loss: 1.1770331859588623\n",
      "Epoch 934: Training Loss: 0.9723344643910726 Validation Loss: 1.1805343627929688\n",
      "Epoch 935: Training Loss: 0.9710058768590292 Validation Loss: 1.1777855157852173\n",
      "Epoch 936: Training Loss: 0.9697882731755575 Validation Loss: 1.1747775077819824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 937: Training Loss: 0.9690840840339661 Validation Loss: 1.1739119291305542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 938: Training Loss: 0.9681782126426697 Validation Loss: 1.1751623153686523\n",
      "Epoch 939: Training Loss: 0.9671103358268738 Validation Loss: 1.1733424663543701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 940: Training Loss: 0.9662481149037679 Validation Loss: 1.1731230020523071\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 941: Training Loss: 0.9656561017036438 Validation Loss: 1.173171043395996\n",
      "Epoch 942: Training Loss: 0.9644613464673361 Validation Loss: 1.1771162748336792\n",
      "Epoch 943: Training Loss: 0.9637645284334818 Validation Loss: 1.1718230247497559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 944: Training Loss: 0.9624157349268595 Validation Loss: 1.1716917753219604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 945: Training Loss: 0.96149875720342 Validation Loss: 1.16970694065094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 946: Training Loss: 0.9605100949605306 Validation Loss: 1.169329285621643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 947: Training Loss: 0.9598472913106283 Validation Loss: 1.171170711517334\n",
      "Epoch 948: Training Loss: 0.9588933984438578 Validation Loss: 1.1687437295913696\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 949: Training Loss: 0.9580475886662801 Validation Loss: 1.1664037704467773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 950: Training Loss: 0.957531730333964 Validation Loss: 1.1714189052581787\n",
      "Epoch 951: Training Loss: 0.9564550717671713 Validation Loss: 1.1649603843688965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 952: Training Loss: 0.9554211894671122 Validation Loss: 1.16669762134552\n",
      "Epoch 953: Training Loss: 0.954609751701355 Validation Loss: 1.1660922765731812\n",
      "Epoch 954: Training Loss: 0.9536121487617493 Validation Loss: 1.163100004196167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 955: Training Loss: 0.9527385234832764 Validation Loss: 1.1635912656784058\n",
      "Epoch 956: Training Loss: 0.9518279234568278 Validation Loss: 1.1673438549041748\n",
      "Epoch 957: Training Loss: 0.9513495961825053 Validation Loss: 1.1671231985092163\n",
      "Epoch 958: Training Loss: 0.9499958356221517 Validation Loss: 1.1648095846176147\n",
      "Epoch 959: Training Loss: 0.949057916800181 Validation Loss: 1.1585174798965454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 960: Training Loss: 0.9480737249056498 Validation Loss: 1.1599783897399902\n",
      "Epoch 961: Training Loss: 0.947147270043691 Validation Loss: 1.1595053672790527\n",
      "Epoch 962: Training Loss: 0.9469752907752991 Validation Loss: 1.1625292301177979\n",
      "Epoch 963: Training Loss: 0.9452528556187948 Validation Loss: 1.1598830223083496\n",
      "Epoch 964: Training Loss: 0.9441913564999899 Validation Loss: 1.1583497524261475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 965: Training Loss: 0.94371098279953 Validation Loss: 1.1570570468902588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 966: Training Loss: 0.9427947203318278 Validation Loss: 1.1566776037216187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 967: Training Loss: 0.9419642885526022 Validation Loss: 1.159315586090088\n",
      "Epoch 968: Training Loss: 0.9414457281430563 Validation Loss: 1.1603195667266846\n",
      "Epoch 969: Training Loss: 0.9400482773780823 Validation Loss: 1.1543169021606445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 970: Training Loss: 0.9396542906761169 Validation Loss: 1.1514710187911987\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 971: Training Loss: 0.9384232759475708 Validation Loss: 1.1548980474472046\n",
      "Epoch 972: Training Loss: 0.9375837643941244 Validation Loss: 1.15902578830719\n",
      "Epoch 973: Training Loss: 0.936700165271759 Validation Loss: 1.1571199893951416\n",
      "Epoch 974: Training Loss: 0.9358295400937399 Validation Loss: 1.148423671722412\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 975: Training Loss: 0.9354433019955953 Validation Loss: 1.1467608213424683\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 976: Training Loss: 0.9336298704147339 Validation Loss: 1.1526368856430054\n",
      "Epoch 977: Training Loss: 0.9330519437789917 Validation Loss: 1.159406065940857\n",
      "Epoch 978: Training Loss: 0.9326331416765848 Validation Loss: 1.154843807220459\n",
      "Epoch 979: Training Loss: 0.9326358636220297 Validation Loss: 1.145896315574646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 980: Training Loss: 0.9301056067148844 Validation Loss: 1.1487623453140259\n",
      "Epoch 981: Training Loss: 0.9295905033747355 Validation Loss: 1.154162049293518\n",
      "Epoch 982: Training Loss: 0.9284471670786539 Validation Loss: 1.1493496894836426\n",
      "Epoch 983: Training Loss: 0.9274446169535319 Validation Loss: 1.1456552743911743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 984: Training Loss: 0.9265506466229757 Validation Loss: 1.1436821222305298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 985: Training Loss: 0.926056961218516 Validation Loss: 1.1461824178695679\n",
      "Epoch 986: Training Loss: 0.924934983253479 Validation Loss: 1.147120714187622\n",
      "Epoch 987: Training Loss: 0.9242968757947286 Validation Loss: 1.1425381898880005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 988: Training Loss: 0.9231611092885336 Validation Loss: 1.1460973024368286\n",
      "Epoch 989: Training Loss: 0.9222814440727234 Validation Loss: 1.144206166267395\n",
      "Epoch 990: Training Loss: 0.9212165276209513 Validation Loss: 1.1440261602401733\n",
      "Epoch 991: Training Loss: 0.9204650322596232 Validation Loss: 1.1439317464828491\n",
      "Epoch 992: Training Loss: 0.9194815556208292 Validation Loss: 1.1422945261001587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 993: Training Loss: 0.9185418883959452 Validation Loss: 1.140976905822754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 994: Training Loss: 0.9175493717193604 Validation Loss: 1.1403661966323853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 995: Training Loss: 0.9172290960947672 Validation Loss: 1.1394096612930298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 996: Training Loss: 0.9161240259806315 Validation Loss: 1.1400928497314453\n",
      "Epoch 997: Training Loss: 0.9151549736658732 Validation Loss: 1.1427420377731323\n",
      "Epoch 998: Training Loss: 0.9148204127947489 Validation Loss: 1.139242172241211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 999: Training Loss: 0.9133411248524984 Validation Loss: 1.1376148462295532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 1000: Training Loss: 0.9125615159670512 Validation Loss: 1.1358051300048828\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1001: Training Loss: 0.9118361671765646 Validation Loss: 1.1373776197433472\n",
      "Epoch 1002: Training Loss: 0.9108303387959799 Validation Loss: 1.1393229961395264\n",
      "Epoch 1003: Training Loss: 0.9105080167452494 Validation Loss: 1.1374948024749756\n",
      "Epoch 1004: Training Loss: 0.9096144437789917 Validation Loss: 1.1331759691238403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1005: Training Loss: 0.9083037376403809 Validation Loss: 1.1329363584518433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1006: Training Loss: 0.9073657393455505 Validation Loss: 1.1351187229156494\n",
      "Epoch 1007: Training Loss: 0.9072647293408712 Validation Loss: 1.137779712677002\n",
      "Epoch 1008: Training Loss: 0.9056819081306458 Validation Loss: 1.134041428565979\n",
      "Epoch 1009: Training Loss: 0.9049836198488871 Validation Loss: 1.1283328533172607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1010: Training Loss: 0.9041146636009216 Validation Loss: 1.1292550563812256\n",
      "Epoch 1011: Training Loss: 0.9039837121963501 Validation Loss: 1.1346381902694702\n",
      "Epoch 1012: Training Loss: 0.9027672608693441 Validation Loss: 1.1343026161193848\n",
      "Epoch 1013: Training Loss: 0.9018514156341553 Validation Loss: 1.127150297164917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1014: Training Loss: 0.9008540113766988 Validation Loss: 1.126177191734314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1015: Training Loss: 0.8998279372851054 Validation Loss: 1.1276285648345947\n",
      "Epoch 1016: Training Loss: 0.8986256917317709 Validation Loss: 1.1304001808166504\n",
      "Epoch 1017: Training Loss: 0.8983091711997986 Validation Loss: 1.1288700103759766\n",
      "Epoch 1018: Training Loss: 0.8973300258318583 Validation Loss: 1.1254500150680542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1019: Training Loss: 0.8971075415611267 Validation Loss: 1.1277194023132324\n",
      "Epoch 1020: Training Loss: 0.8959659536679586 Validation Loss: 1.1300559043884277\n",
      "Epoch 1021: Training Loss: 0.8947786291440328 Validation Loss: 1.1254783868789673\n",
      "Epoch 1022: Training Loss: 0.894014835357666 Validation Loss: 1.1243219375610352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1023: Training Loss: 0.8932595054308573 Validation Loss: 1.1223019361495972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1024: Training Loss: 0.8920528093973795 Validation Loss: 1.1216833591461182\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1025: Training Loss: 0.891452431678772 Validation Loss: 1.1244330406188965\n",
      "Epoch 1026: Training Loss: 0.8902406692504883 Validation Loss: 1.123387336730957\n",
      "Epoch 1027: Training Loss: 0.8895701169967651 Validation Loss: 1.1224851608276367\n",
      "Epoch 1028: Training Loss: 0.8891955216725668 Validation Loss: 1.1256980895996094\n",
      "Epoch 1029: Training Loss: 0.8879163265228271 Validation Loss: 1.1211212873458862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1030: Training Loss: 0.8869016170501709 Validation Loss: 1.1184817552566528\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1031: Training Loss: 0.8865885138511658 Validation Loss: 1.1181617975234985\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1032: Training Loss: 0.8854132294654846 Validation Loss: 1.1178059577941895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1033: Training Loss: 0.8843543926874796 Validation Loss: 1.1184037923812866\n",
      "Epoch 1034: Training Loss: 0.883607824643453 Validation Loss: 1.1165410280227661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1035: Training Loss: 0.8830748001734415 Validation Loss: 1.1160815954208374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1036: Training Loss: 0.882413367430369 Validation Loss: 1.117069959640503\n",
      "Epoch 1037: Training Loss: 0.881233831246694 Validation Loss: 1.120391845703125\n",
      "Epoch 1038: Training Loss: 0.8809440732002258 Validation Loss: 1.1165533065795898\n",
      "Epoch 1039: Training Loss: 0.8795855244000753 Validation Loss: 1.1169171333312988\n",
      "Epoch 1040: Training Loss: 0.8787769476572672 Validation Loss: 1.1148797273635864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1041: Training Loss: 0.8777709205945333 Validation Loss: 1.1145484447479248\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1042: Training Loss: 0.877020001411438 Validation Loss: 1.1122100353240967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1043: Training Loss: 0.8760785063107809 Validation Loss: 1.1117509603500366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1044: Training Loss: 0.875259538491567 Validation Loss: 1.1117029190063477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1045: Training Loss: 0.8743767142295837 Validation Loss: 1.1159228086471558\n",
      "Epoch 1046: Training Loss: 0.8736792405446371 Validation Loss: 1.1135538816452026\n",
      "Epoch 1047: Training Loss: 0.8726354837417603 Validation Loss: 1.1102614402770996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1048: Training Loss: 0.8718797365824381 Validation Loss: 1.1073781251907349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1049: Training Loss: 0.871207058429718 Validation Loss: 1.1070529222488403\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1050: Training Loss: 0.8704482316970825 Validation Loss: 1.1087489128112793\n",
      "Epoch 1051: Training Loss: 0.8695188760757446 Validation Loss: 1.1097643375396729\n",
      "Epoch 1052: Training Loss: 0.8684982061386108 Validation Loss: 1.1107457876205444\n",
      "Epoch 1053: Training Loss: 0.8679459095001221 Validation Loss: 1.1082674264907837\n",
      "Epoch 1054: Training Loss: 0.8670100172360738 Validation Loss: 1.1061500310897827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1055: Training Loss: 0.866175631682078 Validation Loss: 1.1060056686401367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1056: Training Loss: 0.8654165665308634 Validation Loss: 1.1066073179244995\n",
      "Epoch 1057: Training Loss: 0.8645360668500265 Validation Loss: 1.106717586517334\n",
      "Epoch 1058: Training Loss: 0.8637558817863464 Validation Loss: 1.103481411933899\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1059: Training Loss: 0.8629361192385355 Validation Loss: 1.1036847829818726\n",
      "Epoch 1060: Training Loss: 0.8621694644292196 Validation Loss: 1.1042723655700684\n",
      "Epoch 1061: Training Loss: 0.8616652488708496 Validation Loss: 1.1058738231658936\n",
      "Epoch 1062: Training Loss: 0.8607354362805685 Validation Loss: 1.1011277437210083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1063: Training Loss: 0.8600764671961466 Validation Loss: 1.0996736288070679\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1064: Training Loss: 0.8588617642720541 Validation Loss: 1.1025875806808472\n",
      "Epoch 1065: Training Loss: 0.858712911605835 Validation Loss: 1.105484127998352\n",
      "Epoch 1066: Training Loss: 0.8572405576705933 Validation Loss: 1.100318193435669\n",
      "Epoch 1067: Training Loss: 0.8561851779619852 Validation Loss: 1.0970795154571533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1068: Training Loss: 0.8559461633364359 Validation Loss: 1.0970518589019775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1069: Training Loss: 0.8553406000137329 Validation Loss: 1.1013470888137817\n",
      "Epoch 1070: Training Loss: 0.8541164398193359 Validation Loss: 1.1012552976608276\n",
      "Epoch 1071: Training Loss: 0.8538260062535604 Validation Loss: 1.096814751625061\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1072: Training Loss: 0.8523334066073099 Validation Loss: 1.0980867147445679\n",
      "Epoch 1073: Training Loss: 0.8517448504765829 Validation Loss: 1.0992830991744995\n",
      "Epoch 1074: Training Loss: 0.8506840864817301 Validation Loss: 1.0950870513916016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1075: Training Loss: 0.8499173720677694 Validation Loss: 1.0931521654129028\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1076: Training Loss: 0.8494092226028442 Validation Loss: 1.094760775566101\n",
      "Epoch 1077: Training Loss: 0.8483564456303915 Validation Loss: 1.0932307243347168\n",
      "Epoch 1078: Training Loss: 0.8478056987126669 Validation Loss: 1.0965100526809692\n",
      "Epoch 1079: Training Loss: 0.8466981649398804 Validation Loss: 1.094721794128418\n",
      "Epoch 1080: Training Loss: 0.8463206092516581 Validation Loss: 1.0953495502471924\n",
      "Epoch 1081: Training Loss: 0.8451326092084249 Validation Loss: 1.091341257095337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1082: Training Loss: 0.8445757031440735 Validation Loss: 1.0878928899765015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1083: Training Loss: 0.8442932764689127 Validation Loss: 1.0898059606552124\n",
      "Epoch 1084: Training Loss: 0.8432812094688416 Validation Loss: 1.0924266576766968\n",
      "Epoch 1085: Training Loss: 0.8421394030253092 Validation Loss: 1.0893936157226562\n",
      "Epoch 1086: Training Loss: 0.8414730230967203 Validation Loss: 1.0885765552520752\n",
      "Epoch 1087: Training Loss: 0.8404786388079325 Validation Loss: 1.0927101373672485\n",
      "Epoch 1088: Training Loss: 0.8395678202311198 Validation Loss: 1.0910178422927856\n",
      "Epoch 1089: Training Loss: 0.8389158248901367 Validation Loss: 1.0885134935379028\n",
      "Epoch 1090: Training Loss: 0.8379300236701965 Validation Loss: 1.0873494148254395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1091: Training Loss: 0.8370051383972168 Validation Loss: 1.0859519243240356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1092: Training Loss: 0.8365927735964457 Validation Loss: 1.0875999927520752\n",
      "Epoch 1093: Training Loss: 0.8358737031618754 Validation Loss: 1.0854219198226929\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1094: Training Loss: 0.8349809845288595 Validation Loss: 1.0835721492767334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1095: Training Loss: 0.8342936436335245 Validation Loss: 1.0858311653137207\n",
      "Epoch 1096: Training Loss: 0.8335807124773661 Validation Loss: 1.0892752408981323\n",
      "Epoch 1097: Training Loss: 0.8328520854314169 Validation Loss: 1.0856117010116577\n",
      "Epoch 1098: Training Loss: 0.8315749565760294 Validation Loss: 1.0824785232543945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1099: Training Loss: 0.8316006064414978 Validation Loss: 1.0779744386672974\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1100: Training Loss: 0.8303840756416321 Validation Loss: 1.080495834350586\n",
      "Epoch 1101: Training Loss: 0.8292655348777771 Validation Loss: 1.086266279220581\n",
      "Epoch 1102: Training Loss: 0.8289291858673096 Validation Loss: 1.0867210626602173\n",
      "Epoch 1103: Training Loss: 0.828091581662496 Validation Loss: 1.0799050331115723\n",
      "Epoch 1104: Training Loss: 0.8271653652191162 Validation Loss: 1.0785573720932007\n",
      "Epoch 1105: Training Loss: 0.8264292478561401 Validation Loss: 1.0764596462249756\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1106: Training Loss: 0.8253724376360575 Validation Loss: 1.0785070657730103\n",
      "Epoch 1107: Training Loss: 0.8251561125119528 Validation Loss: 1.0823044776916504\n",
      "Epoch 1108: Training Loss: 0.824028491973877 Validation Loss: 1.0799765586853027\n",
      "Epoch 1109: Training Loss: 0.8232223391532898 Validation Loss: 1.0783509016036987\n",
      "Epoch 1110: Training Loss: 0.822325587272644 Validation Loss: 1.0772500038146973\n",
      "Epoch 1111: Training Loss: 0.8218818505605062 Validation Loss: 1.0782161951065063\n",
      "Epoch 1112: Training Loss: 0.8205766876538595 Validation Loss: 1.0748813152313232\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1113: Training Loss: 0.8199586868286133 Validation Loss: 1.0719373226165771\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1114: Training Loss: 0.8192640344301859 Validation Loss: 1.0730847120285034\n",
      "Epoch 1115: Training Loss: 0.8185642759005228 Validation Loss: 1.0739059448242188\n",
      "Epoch 1116: Training Loss: 0.817823608716329 Validation Loss: 1.075335144996643\n",
      "Epoch 1117: Training Loss: 0.8168518344561259 Validation Loss: 1.0762099027633667\n",
      "Epoch 1118: Training Loss: 0.8161162734031677 Validation Loss: 1.0731004476547241\n",
      "Epoch 1119: Training Loss: 0.8152474363644918 Validation Loss: 1.0718885660171509\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1120: Training Loss: 0.8144040505091349 Validation Loss: 1.070786714553833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1121: Training Loss: 0.8142853577931722 Validation Loss: 1.0728821754455566\n",
      "Epoch 1122: Training Loss: 0.8130741119384766 Validation Loss: 1.0725884437561035\n",
      "Epoch 1123: Training Loss: 0.8124668002128601 Validation Loss: 1.072603464126587\n",
      "Epoch 1124: Training Loss: 0.8117009003957113 Validation Loss: 1.0685375928878784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1125: Training Loss: 0.810969889163971 Validation Loss: 1.0645828247070312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1126: Training Loss: 0.8101290464401245 Validation Loss: 1.0680944919586182\n",
      "Epoch 1127: Training Loss: 0.8095267017682394 Validation Loss: 1.0743110179901123\n",
      "Epoch 1128: Training Loss: 0.8085198998451233 Validation Loss: 1.0730037689208984\n",
      "Epoch 1129: Training Loss: 0.8079028328259786 Validation Loss: 1.067328691482544\n",
      "Epoch 1130: Training Loss: 0.8069515625635783 Validation Loss: 1.0638206005096436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1131: Training Loss: 0.8062895139058431 Validation Loss: 1.0645300149917603\n",
      "Epoch 1132: Training Loss: 0.8057025472323099 Validation Loss: 1.068168044090271\n",
      "Epoch 1133: Training Loss: 0.8047185738881429 Validation Loss: 1.065799355506897\n",
      "Epoch 1134: Training Loss: 0.8039116660753886 Validation Loss: 1.0642050504684448\n",
      "Epoch 1135: Training Loss: 0.8033549984296163 Validation Loss: 1.063234806060791\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1136: Training Loss: 0.8025160829226176 Validation Loss: 1.0652549266815186\n",
      "Epoch 1137: Training Loss: 0.8015620112419128 Validation Loss: 1.0637749433517456\n",
      "Epoch 1138: Training Loss: 0.8008477687835693 Validation Loss: 1.0633697509765625\n",
      "Epoch 1139: Training Loss: 0.8000566562016805 Validation Loss: 1.0613222122192383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1140: Training Loss: 0.7993543744087219 Validation Loss: 1.0608165264129639\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1141: Training Loss: 0.7985234459241232 Validation Loss: 1.0613813400268555\n",
      "Epoch 1142: Training Loss: 0.7977579037348429 Validation Loss: 1.0613127946853638\n",
      "Epoch 1143: Training Loss: 0.7973565459251404 Validation Loss: 1.0598180294036865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1144: Training Loss: 0.7965282201766968 Validation Loss: 1.0586811304092407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1145: Training Loss: 0.795488973458608 Validation Loss: 1.0591102838516235\n",
      "Epoch 1146: Training Loss: 0.7949655453364054 Validation Loss: 1.059428334236145\n",
      "Epoch 1147: Training Loss: 0.7940048972765604 Validation Loss: 1.0614436864852905\n",
      "Epoch 1148: Training Loss: 0.7934808532396952 Validation Loss: 1.060092806816101\n",
      "Epoch 1149: Training Loss: 0.7926551898320516 Validation Loss: 1.0581961870193481\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1150: Training Loss: 0.7919965386390686 Validation Loss: 1.056624412536621\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1151: Training Loss: 0.791779617468516 Validation Loss: 1.0572638511657715\n",
      "Epoch 1152: Training Loss: 0.7910783290863037 Validation Loss: 1.0513761043548584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1153: Training Loss: 0.7898989915847778 Validation Loss: 1.0543302297592163\n",
      "Epoch 1154: Training Loss: 0.7891155481338501 Validation Loss: 1.0586930513381958\n",
      "Epoch 1155: Training Loss: 0.7884514530499777 Validation Loss: 1.0580852031707764\n",
      "Epoch 1156: Training Loss: 0.7874263525009155 Validation Loss: 1.053514838218689\n",
      "Epoch 1157: Training Loss: 0.7865405480066935 Validation Loss: 1.0512588024139404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1158: Training Loss: 0.7861327330271403 Validation Loss: 1.050949215888977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1159: Training Loss: 0.7856146891911825 Validation Loss: 1.0505664348602295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1160: Training Loss: 0.7844759424527487 Validation Loss: 1.0538822412490845\n",
      "Epoch 1161: Training Loss: 0.783751368522644 Validation Loss: 1.0543792247772217\n",
      "Epoch 1162: Training Loss: 0.7830817500750223 Validation Loss: 1.0518265962600708\n",
      "Epoch 1163: Training Loss: 0.7823324004809061 Validation Loss: 1.0528910160064697\n",
      "Epoch 1164: Training Loss: 0.7817250490188599 Validation Loss: 1.0491994619369507\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1165: Training Loss: 0.7809753815333048 Validation Loss: 1.04688560962677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1166: Training Loss: 0.780837893486023 Validation Loss: 1.0519568920135498\n",
      "Epoch 1167: Training Loss: 0.7793551087379456 Validation Loss: 1.0501151084899902\n",
      "Epoch 1168: Training Loss: 0.7784156799316406 Validation Loss: 1.0469136238098145\n",
      "Epoch 1169: Training Loss: 0.7783066630363464 Validation Loss: 1.0443439483642578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1170: Training Loss: 0.7770764827728271 Validation Loss: 1.046769380569458\n",
      "Epoch 1171: Training Loss: 0.7764920790990194 Validation Loss: 1.0506819486618042\n",
      "Epoch 1172: Training Loss: 0.7760304013888041 Validation Loss: 1.0484960079193115\n",
      "Epoch 1173: Training Loss: 0.7752148509025574 Validation Loss: 1.0480540990829468\n",
      "Epoch 1174: Training Loss: 0.7744466463724772 Validation Loss: 1.04198157787323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1175: Training Loss: 0.7740495006243387 Validation Loss: 1.0433704853057861\n",
      "Epoch 1176: Training Loss: 0.7726685007413229 Validation Loss: 1.042562484741211\n",
      "Epoch 1177: Training Loss: 0.7720786333084106 Validation Loss: 1.0428019762039185\n",
      "Epoch 1178: Training Loss: 0.7714194456736246 Validation Loss: 1.043681025505066\n",
      "Epoch 1179: Training Loss: 0.7708416978518168 Validation Loss: 1.0452176332473755\n",
      "Epoch 1180: Training Loss: 0.7704363465309143 Validation Loss: 1.0457736253738403\n",
      "Epoch 1181: Training Loss: 0.7692284981409708 Validation Loss: 1.0399247407913208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1182: Training Loss: 0.7687091827392578 Validation Loss: 1.038671612739563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1183: Training Loss: 0.7678106625874838 Validation Loss: 1.041115164756775\n",
      "Epoch 1184: Training Loss: 0.7672662734985352 Validation Loss: 1.0434694290161133\n",
      "Epoch 1185: Training Loss: 0.7665544549624125 Validation Loss: 1.0402765274047852\n",
      "Epoch 1186: Training Loss: 0.7660165230433146 Validation Loss: 1.037675380706787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1187: Training Loss: 0.7648035685221354 Validation Loss: 1.0397213697433472\n",
      "Epoch 1188: Training Loss: 0.7640871604283651 Validation Loss: 1.0391347408294678\n",
      "Epoch 1189: Training Loss: 0.7635404666264852 Validation Loss: 1.0385671854019165\n",
      "Epoch 1190: Training Loss: 0.7628307342529297 Validation Loss: 1.0363131761550903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1191: Training Loss: 0.7619650761286417 Validation Loss: 1.0361603498458862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1192: Training Loss: 0.7614725629488627 Validation Loss: 1.0413436889648438\n",
      "Epoch 1193: Training Loss: 0.7609665195147196 Validation Loss: 1.037201166152954\n",
      "Epoch 1194: Training Loss: 0.7598731517791748 Validation Loss: 1.036834955215454\n",
      "Epoch 1195: Training Loss: 0.7591269214948019 Validation Loss: 1.035687804222107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1196: Training Loss: 0.7586124539375305 Validation Loss: 1.0375419855117798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1197: Training Loss: 0.7576954364776611 Validation Loss: 1.0347316265106201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1198: Training Loss: 0.7570390502611796 Validation Loss: 1.0318886041641235\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1199: Training Loss: 0.7564426859219869 Validation Loss: 1.0300673246383667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1200: Training Loss: 0.755808969338735 Validation Loss: 1.0327783823013306\n",
      "Epoch 1201: Training Loss: 0.7548969984054565 Validation Loss: 1.0321835279464722\n",
      "Epoch 1202: Training Loss: 0.754307210445404 Validation Loss: 1.0335509777069092\n",
      "Epoch 1203: Training Loss: 0.7535504897435507 Validation Loss: 1.0322765111923218\n",
      "Epoch 1204: Training Loss: 0.7529064615567526 Validation Loss: 1.029442310333252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1205: Training Loss: 0.7521946430206299 Validation Loss: 1.0311110019683838\n",
      "Epoch 1206: Training Loss: 0.7514823079109192 Validation Loss: 1.030855655670166\n",
      "Epoch 1207: Training Loss: 0.7507744828859965 Validation Loss: 1.030173420906067\n",
      "Epoch 1208: Training Loss: 0.7500589688618978 Validation Loss: 1.0328354835510254\n",
      "Epoch 1209: Training Loss: 0.7492560942967733 Validation Loss: 1.030059814453125\n",
      "Epoch 1210: Training Loss: 0.7487174471219381 Validation Loss: 1.0278558731079102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1211: Training Loss: 0.748197078704834 Validation Loss: 1.0238327980041504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1212: Training Loss: 0.747653583685557 Validation Loss: 1.0244323015213013\n",
      "Epoch 1213: Training Loss: 0.7464067339897156 Validation Loss: 1.0287158489227295\n",
      "Epoch 1214: Training Loss: 0.7458831667900085 Validation Loss: 1.0310297012329102\n",
      "Epoch 1215: Training Loss: 0.7451004783312479 Validation Loss: 1.028256893157959\n",
      "Epoch 1216: Training Loss: 0.7444099187850952 Validation Loss: 1.026342749595642\n",
      "Epoch 1217: Training Loss: 0.7437262137730917 Validation Loss: 1.0250486135482788\n",
      "Epoch 1218: Training Loss: 0.7438576022783915 Validation Loss: 1.0271410942077637\n",
      "Epoch 1219: Training Loss: 0.7423800428708395 Validation Loss: 1.0254898071289062\n",
      "Epoch 1220: Training Loss: 0.741666297117869 Validation Loss: 1.0255523920059204\n",
      "Epoch 1221: Training Loss: 0.7412929534912109 Validation Loss: 1.019371509552002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1222: Training Loss: 0.7405720353126526 Validation Loss: 1.0219459533691406\n",
      "Epoch 1223: Training Loss: 0.7394610047340393 Validation Loss: 1.022161602973938\n",
      "Epoch 1224: Training Loss: 0.7392134467760721 Validation Loss: 1.02264404296875\n",
      "Epoch 1225: Training Loss: 0.7380525867144266 Validation Loss: 1.022766351699829\n",
      "Epoch 1226: Training Loss: 0.737522304058075 Validation Loss: 1.02194344997406\n",
      "Epoch 1227: Training Loss: 0.7368190089861552 Validation Loss: 1.0210564136505127\n",
      "Epoch 1228: Training Loss: 0.7366050283114115 Validation Loss: 1.0224498510360718\n",
      "Epoch 1229: Training Loss: 0.735686182975769 Validation Loss: 1.0158640146255493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1230: Training Loss: 0.735233465830485 Validation Loss: 1.0148115158081055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1231: Training Loss: 0.7338764270146688 Validation Loss: 1.0216518640518188\n",
      "Epoch 1232: Training Loss: 0.7334691484769186 Validation Loss: 1.025567889213562\n",
      "Epoch 1233: Training Loss: 0.7331625819206238 Validation Loss: 1.0224666595458984\n",
      "Epoch 1234: Training Loss: 0.7317429979642233 Validation Loss: 1.0164177417755127\n",
      "Epoch 1235: Training Loss: 0.7314420739809672 Validation Loss: 1.013551115989685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1236: Training Loss: 0.7308835387229919 Validation Loss: 1.0151082277297974\n",
      "Epoch 1237: Training Loss: 0.7300981879234314 Validation Loss: 1.0160791873931885\n",
      "Epoch 1238: Training Loss: 0.7291903694470724 Validation Loss: 1.0175678730010986\n",
      "Epoch 1239: Training Loss: 0.7292500138282776 Validation Loss: 1.0185315608978271\n",
      "Epoch 1240: Training Loss: 0.7279970248540243 Validation Loss: 1.0124260187149048\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1241: Training Loss: 0.7280236681302389 Validation Loss: 1.010928988456726\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1242: Training Loss: 0.7268755833307902 Validation Loss: 1.017298698425293\n",
      "Epoch 1243: Training Loss: 0.72608349720637 Validation Loss: 1.0203584432601929\n",
      "Epoch 1244: Training Loss: 0.7254224419593811 Validation Loss: 1.0154719352722168\n",
      "Epoch 1245: Training Loss: 0.7248051762580872 Validation Loss: 1.008806824684143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1246: Training Loss: 0.7240585287412008 Validation Loss: 1.0083235502243042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1247: Training Loss: 0.7231743534406027 Validation Loss: 1.0121803283691406\n",
      "Epoch 1248: Training Loss: 0.722630778948466 Validation Loss: 1.0142825841903687\n",
      "Epoch 1249: Training Loss: 0.7219833731651306 Validation Loss: 1.0104749202728271\n",
      "Epoch 1250: Training Loss: 0.7213886181513468 Validation Loss: 1.0082648992538452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1251: Training Loss: 0.7204546928405762 Validation Loss: 1.0122740268707275\n",
      "Epoch 1252: Training Loss: 0.7197867631912231 Validation Loss: 1.0129848718643188\n",
      "Epoch 1253: Training Loss: 0.7192420363426208 Validation Loss: 1.0137336254119873\n",
      "Epoch 1254: Training Loss: 0.7190743287404379 Validation Loss: 1.0080337524414062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1255: Training Loss: 0.7176646192868551 Validation Loss: 1.007144570350647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1256: Training Loss: 0.7171388864517212 Validation Loss: 1.0069283246994019\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1257: Training Loss: 0.7164620161056519 Validation Loss: 1.0070968866348267\n",
      "Epoch 1258: Training Loss: 0.7159861127535502 Validation Loss: 1.0063050985336304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1259: Training Loss: 0.7154855728149414 Validation Loss: 1.0083190202713013\n",
      "Epoch 1260: Training Loss: 0.7149609327316284 Validation Loss: 1.0104711055755615\n",
      "Epoch 1261: Training Loss: 0.7139516075452169 Validation Loss: 1.0045188665390015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1262: Training Loss: 0.7134639819463094 Validation Loss: 1.0045429468154907\n",
      "Epoch 1263: Training Loss: 0.7132155497868856 Validation Loss: 1.001501202583313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1264: Training Loss: 0.7125906944274902 Validation Loss: 1.0063995122909546\n",
      "Epoch 1265: Training Loss: 0.7114718755086263 Validation Loss: 1.0088374614715576\n",
      "Epoch 1266: Training Loss: 0.710567315419515 Validation Loss: 1.0060794353485107\n",
      "Epoch 1267: Training Loss: 0.7102203170458475 Validation Loss: 1.0011788606643677\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1268: Training Loss: 0.7096347212791443 Validation Loss: 0.9996613264083862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1269: Training Loss: 0.7092839678128561 Validation Loss: 1.007373332977295\n",
      "Epoch 1270: Training Loss: 0.7087719837824503 Validation Loss: 1.0067267417907715\n",
      "Epoch 1271: Training Loss: 0.7072657148043314 Validation Loss: 0.9999105930328369\n",
      "Epoch 1272: Training Loss: 0.7066758275032043 Validation Loss: 0.9984351992607117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1273: Training Loss: 0.7066064476966858 Validation Loss: 1.002244472503662\n",
      "Epoch 1274: Training Loss: 0.7052566409111023 Validation Loss: 0.9997576475143433\n",
      "Epoch 1275: Training Loss: 0.7050607999165853 Validation Loss: 1.0008407831192017\n",
      "Epoch 1276: Training Loss: 0.7041710019111633 Validation Loss: 0.9965408444404602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1277: Training Loss: 0.7035144567489624 Validation Loss: 0.9986579418182373\n",
      "Epoch 1278: Training Loss: 0.7026282548904419 Validation Loss: 0.9992706775665283\n",
      "Epoch 1279: Training Loss: 0.7024983962376913 Validation Loss: 0.9997566938400269\n",
      "Epoch 1280: Training Loss: 0.7014203667640686 Validation Loss: 1.0013269186019897\n",
      "Epoch 1281: Training Loss: 0.7006866534550985 Validation Loss: 1.0021127462387085\n",
      "Epoch 1282: Training Loss: 0.7002678513526917 Validation Loss: 0.9965878129005432\n",
      "Epoch 1283: Training Loss: 0.6993798414866129 Validation Loss: 0.9927938580513\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1284: Training Loss: 0.698743482430776 Validation Loss: 0.9921824336051941\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1285: Training Loss: 0.6981238921483358 Validation Loss: 0.9954772591590881\n",
      "Epoch 1286: Training Loss: 0.6978868246078491 Validation Loss: 0.9939653277397156\n",
      "Epoch 1287: Training Loss: 0.6965983907381693 Validation Loss: 0.9979167580604553\n",
      "Epoch 1288: Training Loss: 0.69610067208608 Validation Loss: 0.9995309710502625\n",
      "Epoch 1289: Training Loss: 0.6955344875653585 Validation Loss: 0.9961273670196533\n",
      "Epoch 1290: Training Loss: 0.6950313846270243 Validation Loss: 0.992195188999176\n",
      "Epoch 1291: Training Loss: 0.6942934989929199 Validation Loss: 0.9914199113845825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1292: Training Loss: 0.6938680410385132 Validation Loss: 0.9907047152519226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1293: Training Loss: 0.6932048598925272 Validation Loss: 0.9899670481681824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1294: Training Loss: 0.6920337478319804 Validation Loss: 0.9940963983535767\n",
      "Epoch 1295: Training Loss: 0.6917503078778585 Validation Loss: 0.9981310963630676\n",
      "Epoch 1296: Training Loss: 0.6914368271827698 Validation Loss: 0.995522677898407\n",
      "Epoch 1297: Training Loss: 0.6908549865086874 Validation Loss: 0.9900127649307251\n",
      "Epoch 1298: Training Loss: 0.6896746754646301 Validation Loss: 0.9859499931335449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1299: Training Loss: 0.6890902916590372 Validation Loss: 0.9864009618759155\n",
      "Epoch 1300: Training Loss: 0.6883990367253622 Validation Loss: 0.9903804063796997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1301: Training Loss: 0.6888953248659769 Validation Loss: 0.9961618781089783\n",
      "Epoch 1302: Training Loss: 0.6873945395151774 Validation Loss: 0.9905768036842346\n",
      "Epoch 1303: Training Loss: 0.6864178578058878 Validation Loss: 0.9866238832473755\n",
      "Epoch 1304: Training Loss: 0.6863524119059244 Validation Loss: 0.9864814281463623\n",
      "Epoch 1305: Training Loss: 0.6852724154790243 Validation Loss: 0.9840243458747864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1306: Training Loss: 0.6850700577100118 Validation Loss: 0.9850420951843262\n",
      "Epoch 1307: Training Loss: 0.6842152078946432 Validation Loss: 0.9885063171386719\n",
      "Epoch 1308: Training Loss: 0.6835280259450277 Validation Loss: 0.9868577718734741\n",
      "Epoch 1309: Training Loss: 0.6828916470209757 Validation Loss: 0.9847517609596252\n",
      "Epoch 1310: Training Loss: 0.6822908719380697 Validation Loss: 0.9872528910636902\n",
      "Epoch 1311: Training Loss: 0.6818101207415262 Validation Loss: 0.9872251749038696\n",
      "Epoch 1312: Training Loss: 0.6812220811843872 Validation Loss: 0.9826831817626953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1313: Training Loss: 0.6801852981249491 Validation Loss: 0.9837017059326172\n",
      "Epoch 1314: Training Loss: 0.6797545353571574 Validation Loss: 0.9839416146278381\n",
      "Epoch 1315: Training Loss: 0.6790860891342163 Validation Loss: 0.9853847026824951\n",
      "Epoch 1316: Training Loss: 0.6785897811253866 Validation Loss: 0.9837871789932251\n",
      "Epoch 1317: Training Loss: 0.6776649753252665 Validation Loss: 0.9841354489326477\n",
      "Epoch 1318: Training Loss: 0.6772579550743103 Validation Loss: 0.9832330346107483\n",
      "Epoch 1319: Training Loss: 0.676684578259786 Validation Loss: 0.9835801720619202\n",
      "Epoch 1320: Training Loss: 0.6759572625160217 Validation Loss: 0.9808558225631714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1321: Training Loss: 0.6755480567614237 Validation Loss: 0.9803827404975891\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1322: Training Loss: 0.6744768222173055 Validation Loss: 0.9792772531509399\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1323: Training Loss: 0.673972467581431 Validation Loss: 0.9785338044166565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1324: Training Loss: 0.6738710403442383 Validation Loss: 0.9824702143669128\n",
      "Epoch 1325: Training Loss: 0.6729369759559631 Validation Loss: 0.9825727939605713\n",
      "Epoch 1326: Training Loss: 0.6725371281305949 Validation Loss: 0.9763554930686951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1327: Training Loss: 0.6716943184534708 Validation Loss: 0.9761687517166138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1328: Training Loss: 0.6709785660107931 Validation Loss: 0.9778492450714111\n",
      "Epoch 1329: Training Loss: 0.6701725920041403 Validation Loss: 0.9804753065109253\n",
      "Epoch 1330: Training Loss: 0.6698789993921915 Validation Loss: 0.9797887206077576\n",
      "Epoch 1331: Training Loss: 0.6693000793457031 Validation Loss: 0.9779849052429199\n",
      "Epoch 1332: Training Loss: 0.6686216791470846 Validation Loss: 0.9751970767974854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1333: Training Loss: 0.6678551832834879 Validation Loss: 0.9791200160980225\n",
      "Epoch 1334: Training Loss: 0.6675905386606852 Validation Loss: 0.978814959526062\n",
      "Epoch 1335: Training Loss: 0.6671455899874369 Validation Loss: 0.973892092704773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1336: Training Loss: 0.6661778092384338 Validation Loss: 0.9726786613464355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1337: Training Loss: 0.6653685967127482 Validation Loss: 0.9753356575965881\n",
      "Epoch 1338: Training Loss: 0.6648326913515726 Validation Loss: 0.976233184337616\n",
      "Epoch 1339: Training Loss: 0.6640679637591044 Validation Loss: 0.9746569395065308\n",
      "Epoch 1340: Training Loss: 0.6635461846987406 Validation Loss: 0.9718082547187805\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1341: Training Loss: 0.6628871560096741 Validation Loss: 0.9740641713142395\n",
      "Epoch 1342: Training Loss: 0.662210742632548 Validation Loss: 0.972877025604248\n",
      "Epoch 1343: Training Loss: 0.6620208621025085 Validation Loss: 0.9721822738647461\n",
      "Epoch 1344: Training Loss: 0.6610151330629984 Validation Loss: 0.9732562899589539\n",
      "Epoch 1345: Training Loss: 0.660573422908783 Validation Loss: 0.9727470874786377\n",
      "Epoch 1346: Training Loss: 0.6599116126696268 Validation Loss: 0.972685694694519\n",
      "Epoch 1347: Training Loss: 0.6595098177591959 Validation Loss: 0.9694374799728394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1348: Training Loss: 0.6586212714513143 Validation Loss: 0.9693775177001953\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1349: Training Loss: 0.6583678126335144 Validation Loss: 0.9732834100723267\n",
      "Epoch 1350: Training Loss: 0.6573466658592224 Validation Loss: 0.9712707996368408\n",
      "Epoch 1351: Training Loss: 0.6570852398872375 Validation Loss: 0.9688736796379089\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1352: Training Loss: 0.6563843886057535 Validation Loss: 0.9677351117134094\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1353: Training Loss: 0.6560313502947489 Validation Loss: 0.969714343547821\n",
      "Epoch 1354: Training Loss: 0.6551398038864136 Validation Loss: 0.9668383002281189\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1355: Training Loss: 0.6545238494873047 Validation Loss: 0.9689749479293823\n",
      "Epoch 1356: Training Loss: 0.6537484725316366 Validation Loss: 0.9690527319908142\n",
      "Epoch 1357: Training Loss: 0.6532466610272726 Validation Loss: 0.96575528383255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1358: Training Loss: 0.6525529424349467 Validation Loss: 0.9658029079437256\n",
      "Epoch 1359: Training Loss: 0.6522651116053263 Validation Loss: 0.9673576354980469\n",
      "Epoch 1360: Training Loss: 0.6516060034434 Validation Loss: 0.9660752415657043\n",
      "Epoch 1361: Training Loss: 0.650822103023529 Validation Loss: 0.9642220139503479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1362: Training Loss: 0.650429904460907 Validation Loss: 0.9672064185142517\n",
      "Epoch 1363: Training Loss: 0.6496303280194601 Validation Loss: 0.9672727584838867\n",
      "Epoch 1364: Training Loss: 0.6492917736371359 Validation Loss: 0.9640806317329407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1365: Training Loss: 0.6487307747205099 Validation Loss: 0.9652801752090454\n",
      "Epoch 1366: Training Loss: 0.6478934486707052 Validation Loss: 0.9634695649147034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1367: Training Loss: 0.6473109324773153 Validation Loss: 0.961614727973938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1368: Training Loss: 0.6467933654785156 Validation Loss: 0.9641404747962952\n",
      "Epoch 1369: Training Loss: 0.6460742751757304 Validation Loss: 0.9632463455200195\n",
      "Epoch 1370: Training Loss: 0.645728588104248 Validation Loss: 0.9639672636985779\n",
      "Epoch 1371: Training Loss: 0.6452569166819254 Validation Loss: 0.9581549167633057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1372: Training Loss: 0.6445085803667704 Validation Loss: 0.9574916362762451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1373: Training Loss: 0.6437641779581705 Validation Loss: 0.9606466293334961\n",
      "Epoch 1374: Training Loss: 0.6431755622227987 Validation Loss: 0.9645863771438599\n",
      "Epoch 1375: Training Loss: 0.6437289714813232 Validation Loss: 0.9630197286605835\n",
      "Epoch 1376: Training Loss: 0.6420688231786092 Validation Loss: 0.9608032703399658\n",
      "Epoch 1377: Training Loss: 0.6415319442749023 Validation Loss: 0.9627726078033447\n",
      "Epoch 1378: Training Loss: 0.6409053405125936 Validation Loss: 0.959690272808075\n",
      "Epoch 1379: Training Loss: 0.6401721437772115 Validation Loss: 0.9552037715911865\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1380: Training Loss: 0.6400829950968424 Validation Loss: 0.9538785219192505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1381: Training Loss: 0.639154851436615 Validation Loss: 0.9584033489227295\n",
      "Epoch 1382: Training Loss: 0.6383361021677653 Validation Loss: 0.961983323097229\n",
      "Epoch 1383: Training Loss: 0.6385494867960612 Validation Loss: 0.9634920358657837\n",
      "Epoch 1384: Training Loss: 0.6373996535936991 Validation Loss: 0.9566749334335327\n",
      "Epoch 1385: Training Loss: 0.6367353200912476 Validation Loss: 0.9546297192573547\n",
      "Epoch 1386: Training Loss: 0.6364874442418417 Validation Loss: 0.9536526203155518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1387: Training Loss: 0.6355379025141398 Validation Loss: 0.957960307598114\n",
      "Epoch 1388: Training Loss: 0.6357998847961426 Validation Loss: 0.9617069959640503\n",
      "Epoch 1389: Training Loss: 0.6348306934038798 Validation Loss: 0.9557771682739258\n",
      "Epoch 1390: Training Loss: 0.6340164740880331 Validation Loss: 0.9515417814254761\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1391: Training Loss: 0.6337554057439169 Validation Loss: 0.9501627087593079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 1392: Training Loss: 0.6327455639839172 Validation Loss: 0.952349066734314\n",
      "Epoch 1393: Training Loss: 0.6323964794476827 Validation Loss: 0.9559973478317261\n",
      "Epoch 1394: Training Loss: 0.6318952639897665 Validation Loss: 0.9560603499412537\n",
      "Epoch 1395: Training Loss: 0.6315357089042664 Validation Loss: 0.9522574543952942\n",
      "Epoch 1396: Training Loss: 0.6305872201919556 Validation Loss: 0.9517867565155029\n",
      "Epoch 1397: Training Loss: 0.6299624641736349 Validation Loss: 0.9538758993148804\n",
      "Epoch 1398: Training Loss: 0.6294514934221903 Validation Loss: 0.9564131498336792\n",
      "Epoch 1399: Training Loss: 0.6287714242935181 Validation Loss: 0.9519083499908447\n",
      "Epoch 1400: Training Loss: 0.6279749870300293 Validation Loss: 0.9496569037437439\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1401: Training Loss: 0.6276085575421652 Validation Loss: 0.9485947489738464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1402: Training Loss: 0.6272280414899191 Validation Loss: 0.9507483839988708\n",
      "Epoch 1403: Training Loss: 0.6267987092336019 Validation Loss: 0.9533460736274719\n",
      "Epoch 1404: Training Loss: 0.6260306437810262 Validation Loss: 0.9490453600883484\n",
      "Epoch 1405: Training Loss: 0.6255204280217489 Validation Loss: 0.9450058937072754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1406: Training Loss: 0.6247422893842062 Validation Loss: 0.9470246434211731\n",
      "Epoch 1407: Training Loss: 0.6239768067995707 Validation Loss: 0.9501394033432007\n",
      "Epoch 1408: Training Loss: 0.623610258102417 Validation Loss: 0.9530349373817444\n",
      "Epoch 1409: Training Loss: 0.6234505772590637 Validation Loss: 0.9526039361953735\n",
      "Epoch 1410: Training Loss: 0.6225970586140951 Validation Loss: 0.9454065561294556\n",
      "Epoch 1411: Training Loss: 0.6219643553098043 Validation Loss: 0.9445160627365112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1412: Training Loss: 0.6217158834139506 Validation Loss: 0.9441503286361694\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1413: Training Loss: 0.6213742097218832 Validation Loss: 0.9479563236236572\n",
      "Epoch 1414: Training Loss: 0.6203112204869589 Validation Loss: 0.9490395188331604\n",
      "Epoch 1415: Training Loss: 0.6195624669392904 Validation Loss: 0.9472620487213135\n",
      "Epoch 1416: Training Loss: 0.6196489532788595 Validation Loss: 0.9420989751815796\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1417: Training Loss: 0.6186402837435404 Validation Loss: 0.9442983865737915\n",
      "Epoch 1418: Training Loss: 0.6177783608436584 Validation Loss: 0.9450099468231201\n",
      "Epoch 1419: Training Loss: 0.6175167560577393 Validation Loss: 0.9467458724975586\n",
      "Epoch 1420: Training Loss: 0.6170199910799662 Validation Loss: 0.9446601271629333\n",
      "Epoch 1421: Training Loss: 0.6163132190704346 Validation Loss: 0.9440664052963257\n",
      "Epoch 1422: Training Loss: 0.6156895558039347 Validation Loss: 0.9421884417533875\n",
      "Epoch 1423: Training Loss: 0.6150684356689453 Validation Loss: 0.9434086084365845\n",
      "Epoch 1424: Training Loss: 0.6146611968676249 Validation Loss: 0.9405797123908997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1425: Training Loss: 0.6140422026316324 Validation Loss: 0.9422528147697449\n",
      "Epoch 1426: Training Loss: 0.613298237323761 Validation Loss: 0.9428539872169495\n",
      "Epoch 1427: Training Loss: 0.6130002538363138 Validation Loss: 0.944197416305542\n",
      "Epoch 1428: Training Loss: 0.6122153401374817 Validation Loss: 0.9429333209991455\n",
      "Epoch 1429: Training Loss: 0.6117311517397562 Validation Loss: 0.9412704706192017\n",
      "Epoch 1430: Training Loss: 0.6118358771006266 Validation Loss: 0.9376733899116516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1431: Training Loss: 0.6108126044273376 Validation Loss: 0.9398442506790161\n",
      "Epoch 1432: Training Loss: 0.6103378732999166 Validation Loss: 0.940336287021637\n",
      "Epoch 1433: Training Loss: 0.6098248759905497 Validation Loss: 0.9391758441925049\n",
      "Epoch 1434: Training Loss: 0.6089499791463217 Validation Loss: 0.9404283165931702\n",
      "Epoch 1435: Training Loss: 0.6084420084953308 Validation Loss: 0.9394931793212891\n",
      "Epoch 1436: Training Loss: 0.6079184810320536 Validation Loss: 0.938057541847229\n",
      "Epoch 1437: Training Loss: 0.6073207457860311 Validation Loss: 0.9372264742851257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1438: Training Loss: 0.6068064173062643 Validation Loss: 0.9388375878334045\n",
      "Epoch 1439: Training Loss: 0.6062063574790955 Validation Loss: 0.9367052912712097\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1440: Training Loss: 0.6056831479072571 Validation Loss: 0.9379124641418457\n",
      "Epoch 1441: Training Loss: 0.6053361296653748 Validation Loss: 0.935945987701416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1442: Training Loss: 0.6052104433377584 Validation Loss: 0.9399034976959229\n",
      "Epoch 1443: Training Loss: 0.604597806930542 Validation Loss: 0.9391065835952759\n",
      "Epoch 1444: Training Loss: 0.6036297281583151 Validation Loss: 0.932991087436676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1445: Training Loss: 0.6032647291819254 Validation Loss: 0.9308335185050964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1446: Training Loss: 0.6027211348215739 Validation Loss: 0.9352796077728271\n",
      "Epoch 1447: Training Loss: 0.6021527449289957 Validation Loss: 0.937995433807373\n",
      "Epoch 1448: Training Loss: 0.6019236246744791 Validation Loss: 0.9388693571090698\n",
      "Epoch 1449: Training Loss: 0.6009271740913391 Validation Loss: 0.9338231086730957\n",
      "Epoch 1450: Training Loss: 0.60077965259552 Validation Loss: 0.9286823868751526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1451: Training Loss: 0.599768877029419 Validation Loss: 0.9319193959236145\n",
      "Epoch 1452: Training Loss: 0.5995869835217794 Validation Loss: 0.9367718696594238\n",
      "Epoch 1453: Training Loss: 0.598767360051473 Validation Loss: 0.9353447556495667\n",
      "Epoch 1454: Training Loss: 0.5980514486630758 Validation Loss: 0.9324393272399902\n",
      "Epoch 1455: Training Loss: 0.5975438157717387 Validation Loss: 0.9312195181846619\n",
      "Epoch 1456: Training Loss: 0.5971219937006632 Validation Loss: 0.9316502213478088\n",
      "Epoch 1457: Training Loss: 0.5966230829556783 Validation Loss: 0.931283712387085\n",
      "Epoch 1458: Training Loss: 0.5961830019950867 Validation Loss: 0.9290493726730347\n",
      "Epoch 1459: Training Loss: 0.595532218615214 Validation Loss: 0.9281403422355652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1460: Training Loss: 0.5948331356048584 Validation Loss: 0.9296945929527283\n",
      "Epoch 1461: Training Loss: 0.5943864981333414 Validation Loss: 0.9325276613235474\n",
      "Epoch 1462: Training Loss: 0.5937774976094564 Validation Loss: 0.9316279888153076\n",
      "Epoch 1463: Training Loss: 0.5933311780293783 Validation Loss: 0.9304009675979614\n",
      "Epoch 1464: Training Loss: 0.5927129586537679 Validation Loss: 0.929220974445343\n",
      "Epoch 1465: Training Loss: 0.5928594072659811 Validation Loss: 0.9294022917747498\n",
      "Epoch 1466: Training Loss: 0.5916401942571005 Validation Loss: 0.9278485178947449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1467: Training Loss: 0.5913814504941305 Validation Loss: 0.9270626306533813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1468: Training Loss: 0.5908651351928711 Validation Loss: 0.9276390075683594\n",
      "Epoch 1469: Training Loss: 0.5902661879857382 Validation Loss: 0.9305281043052673\n",
      "Epoch 1470: Training Loss: 0.5898981094360352 Validation Loss: 0.9280292391777039\n",
      "Epoch 1471: Training Loss: 0.589151918888092 Validation Loss: 0.924495279788971\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1472: Training Loss: 0.5884247819582621 Validation Loss: 0.9227339625358582\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1473: Training Loss: 0.5883649984995524 Validation Loss: 0.9232595562934875\n",
      "Epoch 1474: Training Loss: 0.5876403252283732 Validation Loss: 0.92851322889328\n",
      "Epoch 1475: Training Loss: 0.5870367089907328 Validation Loss: 0.9310064911842346\n",
      "Epoch 1476: Training Loss: 0.5869201421737671 Validation Loss: 0.9286429286003113\n",
      "Epoch 1477: Training Loss: 0.5860101183255514 Validation Loss: 0.9223716855049133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1478: Training Loss: 0.5860340396563212 Validation Loss: 0.9186317324638367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1479: Training Loss: 0.585289200146993 Validation Loss: 0.9221373796463013\n",
      "Epoch 1480: Training Loss: 0.5844316085179647 Validation Loss: 0.9275117516517639\n",
      "Epoch 1481: Training Loss: 0.5841264526049296 Validation Loss: 0.929326593875885\n",
      "Epoch 1482: Training Loss: 0.5839141805966696 Validation Loss: 0.9223732352256775\n",
      "Epoch 1483: Training Loss: 0.583011269569397 Validation Loss: 0.9214394092559814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1484: Training Loss: 0.5824840466181437 Validation Loss: 0.92125004529953\n",
      "Epoch 1485: Training Loss: 0.5818847020467123 Validation Loss: 0.9208159446716309\n",
      "Epoch 1486: Training Loss: 0.5813791950543722 Validation Loss: 0.9200940728187561\n",
      "Epoch 1487: Training Loss: 0.5809472401936849 Validation Loss: 0.925330400466919\n",
      "Epoch 1488: Training Loss: 0.5800992051760355 Validation Loss: 0.9239331483840942\n",
      "Epoch 1489: Training Loss: 0.579759438832601 Validation Loss: 0.9197074174880981\n",
      "Epoch 1490: Training Loss: 0.5791214903195699 Validation Loss: 0.9195231199264526\n",
      "Epoch 1491: Training Loss: 0.5785909493764242 Validation Loss: 0.9196367263793945\n",
      "Epoch 1492: Training Loss: 0.5780707796414694 Validation Loss: 0.9198258519172668\n",
      "Epoch 1493: Training Loss: 0.578140397866567 Validation Loss: 0.9207901358604431\n",
      "Epoch 1494: Training Loss: 0.5770550568898519 Validation Loss: 0.9195646643638611\n",
      "Epoch 1495: Training Loss: 0.5768598516782125 Validation Loss: 0.9202583432197571\n",
      "Epoch 1496: Training Loss: 0.5761552254358927 Validation Loss: 0.9172891974449158\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1497: Training Loss: 0.5759409467379252 Validation Loss: 0.9168826341629028\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1498: Training Loss: 0.5754748384157816 Validation Loss: 0.9193342924118042\n",
      "Epoch 1499: Training Loss: 0.5744618773460388 Validation Loss: 0.9187675714492798\n",
      "Epoch 1500: Training Loss: 0.5741772254308065 Validation Loss: 0.9179707169532776\n",
      "Epoch 1501: Training Loss: 0.5736685792605082 Validation Loss: 0.9147388935089111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1502: Training Loss: 0.5731063286463419 Validation Loss: 0.9145177602767944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1503: Training Loss: 0.5723939538002014 Validation Loss: 0.9164426326751709\n",
      "Epoch 1504: Training Loss: 0.5718896389007568 Validation Loss: 0.9199446439743042\n",
      "Epoch 1505: Training Loss: 0.5715468327204386 Validation Loss: 0.9205513596534729\n",
      "Epoch 1506: Training Loss: 0.5710117220878601 Validation Loss: 0.9165456891059875\n",
      "Epoch 1507: Training Loss: 0.5704095562299093 Validation Loss: 0.9126774668693542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1508: Training Loss: 0.5699401100476583 Validation Loss: 0.9121741056442261\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1509: Training Loss: 0.5695568124453226 Validation Loss: 0.9140198230743408\n",
      "Epoch 1510: Training Loss: 0.5689322551091512 Validation Loss: 0.9168144464492798\n",
      "Epoch 1511: Training Loss: 0.5684710741043091 Validation Loss: 0.9167357683181763\n",
      "Epoch 1512: Training Loss: 0.56800510485967 Validation Loss: 0.9141151905059814\n",
      "Epoch 1513: Training Loss: 0.5673965414365133 Validation Loss: 0.9103277325630188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1514: Training Loss: 0.5670532584190369 Validation Loss: 0.9118707180023193\n",
      "Epoch 1515: Training Loss: 0.5664302905400594 Validation Loss: 0.9131184816360474\n",
      "Epoch 1516: Training Loss: 0.5659083525339762 Validation Loss: 0.9162531495094299\n",
      "Epoch 1517: Training Loss: 0.565804918607076 Validation Loss: 0.9157942533493042\n",
      "Epoch 1518: Training Loss: 0.5648537079493204 Validation Loss: 0.9125701189041138\n",
      "Epoch 1519: Training Loss: 0.5642775098482767 Validation Loss: 0.9078483581542969\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1520: Training Loss: 0.5638819336891174 Validation Loss: 0.9073946475982666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1521: Training Loss: 0.5634196003278097 Validation Loss: 0.9085384011268616\n",
      "Epoch 1522: Training Loss: 0.5633273919423422 Validation Loss: 0.9131925106048584\n",
      "Epoch 1523: Training Loss: 0.5623969634373983 Validation Loss: 0.912153959274292\n",
      "Epoch 1524: Training Loss: 0.5620618859926859 Validation Loss: 0.9105143547058105\n",
      "Epoch 1525: Training Loss: 0.5613778034845988 Validation Loss: 0.9104005694389343\n",
      "Epoch 1526: Training Loss: 0.5607324441274008 Validation Loss: 0.9093726873397827\n",
      "Epoch 1527: Training Loss: 0.5608515739440918 Validation Loss: 0.905066728591919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1528: Training Loss: 0.5597952802975973 Validation Loss: 0.9088676571846008\n",
      "Epoch 1529: Training Loss: 0.5593406756718954 Validation Loss: 0.9104573726654053\n",
      "Epoch 1530: Training Loss: 0.5589666565259298 Validation Loss: 0.9101635217666626\n",
      "Epoch 1531: Training Loss: 0.558435062567393 Validation Loss: 0.9078397750854492\n",
      "Epoch 1532: Training Loss: 0.5579566756884257 Validation Loss: 0.9084348678588867\n",
      "Epoch 1533: Training Loss: 0.5573589305082957 Validation Loss: 0.907427191734314\n",
      "Epoch 1534: Training Loss: 0.5571111639340719 Validation Loss: 0.905188262462616\n",
      "Epoch 1535: Training Loss: 0.5564381082852682 Validation Loss: 0.9062825441360474\n",
      "Epoch 1536: Training Loss: 0.5562605460484823 Validation Loss: 0.9095330834388733\n",
      "Epoch 1537: Training Loss: 0.5553716619809469 Validation Loss: 0.9077633023262024\n",
      "Epoch 1538: Training Loss: 0.5550554394721985 Validation Loss: 0.9042454361915588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1539: Training Loss: 0.5554794470469157 Validation Loss: 0.9000817537307739\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1540: Training Loss: 0.5542437533537546 Validation Loss: 0.9062585234642029\n",
      "Epoch 1541: Training Loss: 0.5537528793017069 Validation Loss: 0.9101600050926208\n",
      "Epoch 1542: Training Loss: 0.5533340771993002 Validation Loss: 0.9072259068489075\n",
      "Epoch 1543: Training Loss: 0.5528852939605713 Validation Loss: 0.9042273163795471\n",
      "Epoch 1544: Training Loss: 0.552193820476532 Validation Loss: 0.9017288088798523\n",
      "Epoch 1545: Training Loss: 0.551535427570343 Validation Loss: 0.9037492871284485\n",
      "Epoch 1546: Training Loss: 0.5512081980705261 Validation Loss: 0.9058352112770081\n",
      "Epoch 1547: Training Loss: 0.5504887104034424 Validation Loss: 0.9043776988983154\n",
      "Epoch 1548: Training Loss: 0.5503947536150614 Validation Loss: 0.9001440405845642\n",
      "Epoch 1549: Training Loss: 0.5496449867884318 Validation Loss: 0.900177001953125\n",
      "Epoch 1550: Training Loss: 0.5490737358729044 Validation Loss: 0.9017273783683777\n",
      "Epoch 1551: Training Loss: 0.5488473971684774 Validation Loss: 0.9046878814697266\n",
      "Epoch 1552: Training Loss: 0.5482744971911112 Validation Loss: 0.9036282300949097\n",
      "Epoch 1553: Training Loss: 0.5476287802060446 Validation Loss: 0.9013135433197021\n",
      "Epoch 1554: Training Loss: 0.5471343994140625 Validation Loss: 0.8989524841308594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1555: Training Loss: 0.5469871958096822 Validation Loss: 0.8995544910430908\n",
      "Epoch 1556: Training Loss: 0.5465328693389893 Validation Loss: 0.8995159268379211\n",
      "Epoch 1557: Training Loss: 0.5457273523012797 Validation Loss: 0.9027809500694275\n",
      "Epoch 1558: Training Loss: 0.5454692741235098 Validation Loss: 0.9031999707221985\n",
      "Epoch 1559: Training Loss: 0.5448494652907053 Validation Loss: 0.9038042426109314\n",
      "Epoch 1560: Training Loss: 0.5442562599976858 Validation Loss: 0.8993354439735413\n",
      "Epoch 1561: Training Loss: 0.5439494450887045 Validation Loss: 0.8957064747810364\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1562: Training Loss: 0.5435175895690918 Validation Loss: 0.894161581993103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1563: Training Loss: 0.5428808331489563 Validation Loss: 0.895158588886261\n",
      "Epoch 1564: Training Loss: 0.5422557095686594 Validation Loss: 0.8986707329750061\n",
      "Epoch 1565: Training Loss: 0.542125403881073 Validation Loss: 0.9007985591888428\n",
      "Epoch 1566: Training Loss: 0.5417117277781168 Validation Loss: 0.8992041945457458\n",
      "Epoch 1567: Training Loss: 0.5415142973264059 Validation Loss: 0.8956825733184814\n",
      "Epoch 1568: Training Loss: 0.540726900100708 Validation Loss: 0.8969007134437561\n",
      "Epoch 1569: Training Loss: 0.5404038230578104 Validation Loss: 0.900205671787262\n",
      "Epoch 1570: Training Loss: 0.5397878487904867 Validation Loss: 0.897580623626709\n",
      "Epoch 1571: Training Loss: 0.538972387711207 Validation Loss: 0.8941041827201843\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1572: Training Loss: 0.538668155670166 Validation Loss: 0.8921705484390259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1573: Training Loss: 0.5380436579386393 Validation Loss: 0.8949449062347412\n",
      "Epoch 1574: Training Loss: 0.5378362635771433 Validation Loss: 0.8980836272239685\n",
      "Epoch 1575: Training Loss: 0.5372262795766195 Validation Loss: 0.8969646692276001\n",
      "Epoch 1576: Training Loss: 0.5365912914276123 Validation Loss: 0.8944039344787598\n",
      "Epoch 1577: Training Loss: 0.5361716945966085 Validation Loss: 0.8930254578590393\n",
      "Epoch 1578: Training Loss: 0.5359496275583903 Validation Loss: 0.8925982713699341\n",
      "Epoch 1579: Training Loss: 0.5352961619695028 Validation Loss: 0.8935675024986267\n",
      "Epoch 1580: Training Loss: 0.5350468357404073 Validation Loss: 0.895073413848877\n",
      "Epoch 1581: Training Loss: 0.5346433818340302 Validation Loss: 0.8917655944824219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1582: Training Loss: 0.5339464743932089 Validation Loss: 0.8937399387359619\n",
      "Epoch 1583: Training Loss: 0.5335362056891123 Validation Loss: 0.8931466341018677\n",
      "Epoch 1584: Training Loss: 0.5329844156901041 Validation Loss: 0.8923072814941406\n",
      "Epoch 1585: Training Loss: 0.5325018763542175 Validation Loss: 0.891454815864563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1586: Training Loss: 0.5324847300847372 Validation Loss: 0.8883854746818542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1587: Training Loss: 0.531875749429067 Validation Loss: 0.8940203785896301\n",
      "Epoch 1588: Training Loss: 0.5314068098862966 Validation Loss: 0.8948891162872314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1589: Training Loss: 0.5306880275408427 Validation Loss: 0.8934320211410522\n",
      "Epoch 1590: Training Loss: 0.5303276479244232 Validation Loss: 0.8898400068283081\n",
      "Epoch 1591: Training Loss: 0.5299196839332581 Validation Loss: 0.8877355456352234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1592: Training Loss: 0.5293443103631338 Validation Loss: 0.8886736631393433\n",
      "Epoch 1593: Training Loss: 0.5287279089291891 Validation Loss: 0.8906651139259338\n",
      "Epoch 1594: Training Loss: 0.5283861458301544 Validation Loss: 0.8914366960525513\n",
      "Epoch 1595: Training Loss: 0.5278610785802206 Validation Loss: 0.8903098702430725\n",
      "Epoch 1596: Training Loss: 0.5275910298029581 Validation Loss: 0.8879520893096924\n",
      "Epoch 1597: Training Loss: 0.5270555516084036 Validation Loss: 0.8894196152687073\n",
      "Epoch 1598: Training Loss: 0.5265958706537882 Validation Loss: 0.889789879322052\n",
      "Epoch 1599: Training Loss: 0.5259186426798502 Validation Loss: 0.8896104097366333\n",
      "Epoch 1600: Training Loss: 0.5256475210189819 Validation Loss: 0.8865523338317871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1601: Training Loss: 0.525246818860372 Validation Loss: 0.8866905570030212\n",
      "Epoch 1602: Training Loss: 0.5247325201829275 Validation Loss: 0.8846784234046936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1603: Training Loss: 0.5246508320172628 Validation Loss: 0.8851984739303589\n",
      "Epoch 1604: Training Loss: 0.5238262216250101 Validation Loss: 0.8886740803718567\n",
      "Epoch 1605: Training Loss: 0.5232357780138651 Validation Loss: 0.8912002444267273\n",
      "Epoch 1606: Training Loss: 0.5228667855262756 Validation Loss: 0.8882054090499878\n",
      "Epoch 1607: Training Loss: 0.5224414865175883 Validation Loss: 0.886981189250946\n",
      "Epoch 1608: Training Loss: 0.5219138065973917 Validation Loss: 0.8854334354400635\n",
      "Epoch 1609: Training Loss: 0.5214282274246216 Validation Loss: 0.883207380771637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1610: Training Loss: 0.5211378236611685 Validation Loss: 0.8847706317901611\n",
      "Epoch 1611: Training Loss: 0.5204811493555704 Validation Loss: 0.8852537870407104\n",
      "Epoch 1612: Training Loss: 0.5200164318084717 Validation Loss: 0.8835689425468445\n",
      "Epoch 1613: Training Loss: 0.51981849471728 Validation Loss: 0.8823401927947998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1614: Training Loss: 0.5194068451722463 Validation Loss: 0.8824654221534729\n",
      "Epoch 1615: Training Loss: 0.519409696261088 Validation Loss: 0.8877758383750916\n",
      "Epoch 1616: Training Loss: 0.5187981526056925 Validation Loss: 0.8861851096153259\n",
      "Epoch 1617: Training Loss: 0.5180257161458334 Validation Loss: 0.8815720677375793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1618: Training Loss: 0.5176492432753245 Validation Loss: 0.8830903768539429\n",
      "Epoch 1619: Training Loss: 0.5170468688011169 Validation Loss: 0.8828513622283936\n",
      "Epoch 1620: Training Loss: 0.5165143807729086 Validation Loss: 0.882133424282074\n",
      "Epoch 1621: Training Loss: 0.5162069102128347 Validation Loss: 0.8833885192871094\n",
      "Epoch 1622: Training Loss: 0.5161418318748474 Validation Loss: 0.8801969885826111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1623: Training Loss: 0.5155336459477743 Validation Loss: 0.8807308077812195\n",
      "Epoch 1624: Training Loss: 0.5150636831919352 Validation Loss: 0.883414089679718\n",
      "Epoch 1625: Training Loss: 0.5142756104469299 Validation Loss: 0.883102536201477\n",
      "Epoch 1626: Training Loss: 0.5139100551605225 Validation Loss: 0.8807589411735535\n",
      "Epoch 1627: Training Loss: 0.5134183565775553 Validation Loss: 0.8802716732025146\n",
      "Epoch 1628: Training Loss: 0.5131062269210815 Validation Loss: 0.8792805075645447\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1629: Training Loss: 0.5126617153485616 Validation Loss: 0.8769164085388184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1630: Training Loss: 0.5121267835299174 Validation Loss: 0.8793607950210571\n",
      "Epoch 1631: Training Loss: 0.5117273430029551 Validation Loss: 0.8819343447685242\n",
      "Epoch 1632: Training Loss: 0.5115047891934713 Validation Loss: 0.8842409253120422\n",
      "Epoch 1633: Training Loss: 0.5114398698012034 Validation Loss: 0.8770120143890381\n",
      "Epoch 1634: Training Loss: 0.510477234919866 Validation Loss: 0.8786690831184387\n",
      "Epoch 1635: Training Loss: 0.5099132855733236 Validation Loss: 0.8780904412269592\n",
      "Epoch 1636: Training Loss: 0.5096178948879242 Validation Loss: 0.8781664371490479\n",
      "Epoch 1637: Training Loss: 0.5091745257377625 Validation Loss: 0.8768093585968018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1638: Training Loss: 0.5085228284200033 Validation Loss: 0.8783571124076843\n",
      "Epoch 1639: Training Loss: 0.5081297556559244 Validation Loss: 0.8802992701530457\n",
      "Epoch 1640: Training Loss: 0.5078321496645609 Validation Loss: 0.8803278803825378\n",
      "Epoch 1641: Training Loss: 0.5073008437951406 Validation Loss: 0.8765424489974976\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1642: Training Loss: 0.5068610509236654 Validation Loss: 0.8743746280670166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1643: Training Loss: 0.5066076318422953 Validation Loss: 0.8729127645492554\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1644: Training Loss: 0.5062089661757151 Validation Loss: 0.8766315579414368\n",
      "Epoch 1645: Training Loss: 0.5054702957471212 Validation Loss: 0.8793283104896545\n",
      "Epoch 1646: Training Loss: 0.5054525236288706 Validation Loss: 0.8773625493049622\n",
      "Epoch 1647: Training Loss: 0.5047012865543365 Validation Loss: 0.8760160207748413\n",
      "Epoch 1648: Training Loss: 0.5047227144241333 Validation Loss: 0.8718208074569702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1649: Training Loss: 0.5038769741853079 Validation Loss: 0.87396240234375\n",
      "Epoch 1650: Training Loss: 0.5036283830801646 Validation Loss: 0.8782574534416199\n",
      "Epoch 1651: Training Loss: 0.5032041271527609 Validation Loss: 0.8786481618881226\n",
      "Epoch 1652: Training Loss: 0.5025069117546082 Validation Loss: 0.87382572889328\n",
      "Epoch 1653: Training Loss: 0.5021611253420512 Validation Loss: 0.8693829774856567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1654: Training Loss: 0.5017049809296926 Validation Loss: 0.8713781237602234\n",
      "Epoch 1655: Training Loss: 0.5012295345465342 Validation Loss: 0.8741673827171326\n",
      "Epoch 1656: Training Loss: 0.5010184248288473 Validation Loss: 0.8774256110191345\n",
      "Epoch 1657: Training Loss: 0.5004209280014038 Validation Loss: 0.8732960820198059\n",
      "Epoch 1658: Training Loss: 0.5000311136245728 Validation Loss: 0.8706868290901184\n",
      "Epoch 1659: Training Loss: 0.499808390935262 Validation Loss: 0.8689599633216858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1660: Training Loss: 0.4997714062531789 Validation Loss: 0.8728868961334229\n",
      "Epoch 1661: Training Loss: 0.4987851679325104 Validation Loss: 0.8714239597320557\n",
      "Epoch 1662: Training Loss: 0.49822773536046344 Validation Loss: 0.8740604519844055\n",
      "Epoch 1663: Training Loss: 0.49789994955062866 Validation Loss: 0.8723096251487732\n",
      "Epoch 1664: Training Loss: 0.49755027890205383 Validation Loss: 0.8687570691108704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1665: Training Loss: 0.49707460403442383 Validation Loss: 0.8714795112609863\n",
      "Epoch 1666: Training Loss: 0.49691272775332135 Validation Loss: 0.8747198581695557\n",
      "Epoch 1667: Training Loss: 0.4960802396138509 Validation Loss: 0.8710685968399048\n",
      "Epoch 1668: Training Loss: 0.49553977449735004 Validation Loss: 0.8681372404098511\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1669: Training Loss: 0.4955219229062398 Validation Loss: 0.8681373596191406\n",
      "Epoch 1670: Training Loss: 0.4948345224062602 Validation Loss: 0.8681027889251709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1671: Training Loss: 0.4945615530014038 Validation Loss: 0.8693910241127014\n",
      "Epoch 1672: Training Loss: 0.4939953287442525 Validation Loss: 0.8710651993751526\n",
      "Epoch 1673: Training Loss: 0.4937184453010559 Validation Loss: 0.8696336150169373\n",
      "Epoch 1674: Training Loss: 0.4933154284954071 Validation Loss: 0.8699660897254944\n",
      "Epoch 1675: Training Loss: 0.49291741847991943 Validation Loss: 0.8671041131019592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1676: Training Loss: 0.4924220144748688 Validation Loss: 0.8649358153343201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1677: Training Loss: 0.4918080270290375 Validation Loss: 0.8672956228256226\n",
      "Epoch 1678: Training Loss: 0.49148300290107727 Validation Loss: 0.8695651292800903\n",
      "Epoch 1679: Training Loss: 0.4913097719351451 Validation Loss: 0.8674262166023254\n",
      "Epoch 1680: Training Loss: 0.49079403281211853 Validation Loss: 0.8693825602531433\n",
      "Epoch 1681: Training Loss: 0.49019065499305725 Validation Loss: 0.8690762519836426\n",
      "Epoch 1682: Training Loss: 0.4897376596927643 Validation Loss: 0.8669902682304382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1683: Training Loss: 0.4895936846733093 Validation Loss: 0.8639494776725769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1684: Training Loss: 0.4891548156738281 Validation Loss: 0.8626136779785156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1685: Training Loss: 0.48872698346773785 Validation Loss: 0.8680692911148071\n",
      "Epoch 1686: Training Loss: 0.48816347122192383 Validation Loss: 0.8696693181991577\n",
      "Epoch 1687: Training Loss: 0.4877859850724538 Validation Loss: 0.8675772547721863\n",
      "Epoch 1688: Training Loss: 0.4873919089635213 Validation Loss: 0.8656802773475647\n",
      "Epoch 1689: Training Loss: 0.4869615137577057 Validation Loss: 0.8642635345458984\n",
      "Epoch 1690: Training Loss: 0.4870845675468445 Validation Loss: 0.8613167405128479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1691: Training Loss: 0.4862135152022044 Validation Loss: 0.864538848400116\n",
      "Epoch 1692: Training Loss: 0.4856545527776082 Validation Loss: 0.864493727684021\n",
      "Epoch 1693: Training Loss: 0.4852308432261149 Validation Loss: 0.864909827709198\n",
      "Epoch 1694: Training Loss: 0.48505086700121564 Validation Loss: 0.8671124577522278\n",
      "Epoch 1695: Training Loss: 0.4844001928965251 Validation Loss: 0.8657817244529724\n",
      "Epoch 1696: Training Loss: 0.4841395914554596 Validation Loss: 0.8621574640274048\n",
      "Epoch 1697: Training Loss: 0.48379212617874146 Validation Loss: 0.858982264995575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1698: Training Loss: 0.4834395945072174 Validation Loss: 0.8599399328231812\n",
      "Epoch 1699: Training Loss: 0.48289499680201214 Validation Loss: 0.8640294075012207\n",
      "Epoch 1700: Training Loss: 0.48241619269053143 Validation Loss: 0.8666355609893799\n",
      "Epoch 1701: Training Loss: 0.4823555052280426 Validation Loss: 0.8671733140945435\n",
      "Epoch 1702: Training Loss: 0.48160938421885174 Validation Loss: 0.8597065806388855\n",
      "Epoch 1703: Training Loss: 0.4811863998572032 Validation Loss: 0.8567756414413452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1704: Training Loss: 0.4806622366110484 Validation Loss: 0.8572571873664856\n",
      "Epoch 1705: Training Loss: 0.48051656285921734 Validation Loss: 0.8607677221298218\n",
      "Epoch 1706: Training Loss: 0.47993611296017963 Validation Loss: 0.8629469871520996\n",
      "Epoch 1707: Training Loss: 0.4794910748799642 Validation Loss: 0.861058235168457\n",
      "Epoch 1708: Training Loss: 0.4791834056377411 Validation Loss: 0.8596360087394714\n",
      "Epoch 1709: Training Loss: 0.4786610007286072 Validation Loss: 0.8609315156936646\n",
      "Epoch 1710: Training Loss: 0.478815237681071 Validation Loss: 0.8638603687286377\n",
      "Epoch 1711: Training Loss: 0.4777707854906718 Validation Loss: 0.859806478023529\n",
      "Epoch 1712: Training Loss: 0.47752679387728375 Validation Loss: 0.8586618900299072\n",
      "Epoch 1713: Training Loss: 0.4771469334761302 Validation Loss: 0.8583961725234985\n",
      "Epoch 1714: Training Loss: 0.47681687275568646 Validation Loss: 0.8574070930480957\n",
      "Epoch 1715: Training Loss: 0.47628504037857056 Validation Loss: 0.8584937453269958\n",
      "Epoch 1716: Training Loss: 0.4761490225791931 Validation Loss: 0.8582814931869507\n",
      "Epoch 1717: Training Loss: 0.47541357080141705 Validation Loss: 0.8586294651031494\n",
      "Epoch 1718: Training Loss: 0.47504358490308124 Validation Loss: 0.8559297323226929\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1719: Training Loss: 0.4749922255674998 Validation Loss: 0.8587742447853088\n",
      "Epoch 1720: Training Loss: 0.47431161999702454 Validation Loss: 0.8581279516220093\n",
      "Epoch 1721: Training Loss: 0.47395922740300495 Validation Loss: 0.8560984134674072\n",
      "Epoch 1722: Training Loss: 0.47363696495691937 Validation Loss: 0.856348991394043\n",
      "Epoch 1723: Training Loss: 0.4735087553660075 Validation Loss: 0.8599400520324707\n",
      "Epoch 1724: Training Loss: 0.4727308948834737 Validation Loss: 0.8583564162254333\n",
      "Epoch 1725: Training Loss: 0.47226661443710327 Validation Loss: 0.8549090027809143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1726: Training Loss: 0.47196997205416363 Validation Loss: 0.8542996644973755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1727: Training Loss: 0.4715257187684377 Validation Loss: 0.8551158905029297\n",
      "Epoch 1728: Training Loss: 0.47114188472429913 Validation Loss: 0.8562501072883606\n",
      "Epoch 1729: Training Loss: 0.47060151894887287 Validation Loss: 0.8560370802879333\n",
      "Epoch 1730: Training Loss: 0.47032081087430316 Validation Loss: 0.8582490682601929\n",
      "Epoch 1731: Training Loss: 0.4698748091856639 Validation Loss: 0.8571153879165649\n",
      "Epoch 1732: Training Loss: 0.46947531898816425 Validation Loss: 0.8542554974555969\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1733: Training Loss: 0.46923614541689557 Validation Loss: 0.8521355986595154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1734: Training Loss: 0.46883554259936017 Validation Loss: 0.8528672456741333\n",
      "Epoch 1735: Training Loss: 0.4682449400424957 Validation Loss: 0.853481650352478\n",
      "Epoch 1736: Training Loss: 0.4679019550482432 Validation Loss: 0.8536180257797241\n",
      "Epoch 1737: Training Loss: 0.4675674835840861 Validation Loss: 0.8529653549194336\n",
      "Epoch 1738: Training Loss: 0.4671889543533325 Validation Loss: 0.8533835411071777\n",
      "Epoch 1739: Training Loss: 0.4667226771513621 Validation Loss: 0.8536056876182556\n",
      "Epoch 1740: Training Loss: 0.4664441645145416 Validation Loss: 0.8547331690788269\n",
      "Epoch 1741: Training Loss: 0.46601518988609314 Validation Loss: 0.852840006351471\n",
      "Epoch 1742: Training Loss: 0.4656416177749634 Validation Loss: 0.8513299822807312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1743: Training Loss: 0.46527260541915894 Validation Loss: 0.8509624600410461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1744: Training Loss: 0.4648269712924957 Validation Loss: 0.8515636324882507\n",
      "Epoch 1745: Training Loss: 0.46451759338378906 Validation Loss: 0.8536025881767273\n",
      "Epoch 1746: Training Loss: 0.46393640836079914 Validation Loss: 0.8549993634223938\n",
      "Epoch 1747: Training Loss: 0.463666170835495 Validation Loss: 0.8528524041175842\n",
      "Epoch 1748: Training Loss: 0.46344637870788574 Validation Loss: 0.8502031564712524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1749: Training Loss: 0.46281986435254413 Validation Loss: 0.8499930500984192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1750: Training Loss: 0.46347639958063763 Validation Loss: 0.8462182283401489\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1751: Training Loss: 0.4621621072292328 Validation Loss: 0.8493899703025818\n",
      "Epoch 1752: Training Loss: 0.4620276391506195 Validation Loss: 0.8563461899757385\n",
      "Epoch 1753: Training Loss: 0.46179765462875366 Validation Loss: 0.8550618290901184\n",
      "Epoch 1754: Training Loss: 0.46106987198193866 Validation Loss: 0.8499603867530823\n",
      "Epoch 1755: Training Loss: 0.4607449173927307 Validation Loss: 0.8451981544494629\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1756: Training Loss: 0.4604684015115102 Validation Loss: 0.8464850187301636\n",
      "Epoch 1757: Training Loss: 0.46024032433827716 Validation Loss: 0.8515409231185913\n",
      "Epoch 1758: Training Loss: 0.45990849534670514 Validation Loss: 0.8499971032142639\n",
      "Epoch 1759: Training Loss: 0.45899925629297894 Validation Loss: 0.8495155572891235\n",
      "Epoch 1760: Training Loss: 0.4587471584479014 Validation Loss: 0.8509123921394348\n",
      "Epoch 1761: Training Loss: 0.45826172828674316 Validation Loss: 0.8492143750190735\n",
      "Epoch 1762: Training Loss: 0.4580220778783162 Validation Loss: 0.8467469811439514\n",
      "Epoch 1763: Training Loss: 0.45755459864934284 Validation Loss: 0.8460902571678162\n",
      "Epoch 1764: Training Loss: 0.4574957986672719 Validation Loss: 0.8502187132835388\n",
      "Epoch 1765: Training Loss: 0.45682724316914874 Validation Loss: 0.8477491736412048\n",
      "Epoch 1766: Training Loss: 0.45646217465400696 Validation Loss: 0.8452993035316467\n",
      "Epoch 1767: Training Loss: 0.4561314682165782 Validation Loss: 0.8457386493682861\n",
      "Epoch 1768: Training Loss: 0.4562341272830963 Validation Loss: 0.8463330268859863\n",
      "Epoch 1769: Training Loss: 0.45532727241516113 Validation Loss: 0.8504434823989868\n",
      "Epoch 1770: Training Loss: 0.45493753751118976 Validation Loss: 0.8494580388069153\n",
      "Epoch 1771: Training Loss: 0.45474569002787274 Validation Loss: 0.8488980531692505\n",
      "Epoch 1772: Training Loss: 0.4540952245394389 Validation Loss: 0.8442961573600769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1773: Training Loss: 0.4540412624677022 Validation Loss: 0.8420184850692749\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1774: Training Loss: 0.45340485374132794 Validation Loss: 0.8446399569511414\n",
      "Epoch 1775: Training Loss: 0.4530077675978343 Validation Loss: 0.8468505144119263\n",
      "Epoch 1776: Training Loss: 0.4525749286015828 Validation Loss: 0.847398042678833\n",
      "Epoch 1777: Training Loss: 0.4522457917531331 Validation Loss: 0.8463312983512878\n",
      "Epoch 1778: Training Loss: 0.4518814484278361 Validation Loss: 0.8438537120819092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1779: Training Loss: 0.4514997402826945 Validation Loss: 0.8440266847610474\n",
      "Epoch 1780: Training Loss: 0.45113590359687805 Validation Loss: 0.8429117202758789\n",
      "Epoch 1781: Training Loss: 0.4508129159609477 Validation Loss: 0.8431065082550049\n",
      "Epoch 1782: Training Loss: 0.4503448208173116 Validation Loss: 0.8460221290588379\n",
      "Epoch 1783: Training Loss: 0.45019041498502094 Validation Loss: 0.8464168310165405\n",
      "Epoch 1784: Training Loss: 0.4496340254942576 Validation Loss: 0.8438313007354736\n",
      "Epoch 1785: Training Loss: 0.44925076762835187 Validation Loss: 0.8411794304847717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1786: Training Loss: 0.44886816541353863 Validation Loss: 0.8417903780937195\n",
      "Epoch 1787: Training Loss: 0.44865943988164264 Validation Loss: 0.8397474884986877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1788: Training Loss: 0.4481765627861023 Validation Loss: 0.8414027690887451\n",
      "Epoch 1789: Training Loss: 0.44772865374883014 Validation Loss: 0.8447606563568115\n",
      "Epoch 1790: Training Loss: 0.44809476534525555 Validation Loss: 0.847372829914093\n",
      "Epoch 1791: Training Loss: 0.447127898534139 Validation Loss: 0.8424493670463562\n",
      "Epoch 1792: Training Loss: 0.4465716481208801 Validation Loss: 0.8404061198234558\n",
      "Epoch 1793: Training Loss: 0.4462864597638448 Validation Loss: 0.8402410745620728\n",
      "Epoch 1794: Training Loss: 0.44584495822588605 Validation Loss: 0.8399766087532043\n",
      "Epoch 1795: Training Loss: 0.44550231099128723 Validation Loss: 0.8419278264045715\n",
      "Epoch 1796: Training Loss: 0.44525737563769024 Validation Loss: 0.8408800959587097\n",
      "Epoch 1797: Training Loss: 0.44481971859931946 Validation Loss: 0.8409111499786377\n",
      "Epoch 1798: Training Loss: 0.44441425800323486 Validation Loss: 0.8399920463562012\n",
      "Epoch 1799: Training Loss: 0.44407644867897034 Validation Loss: 0.8410140872001648\n",
      "Epoch 1800: Training Loss: 0.4437037209669749 Validation Loss: 0.8415660262107849\n",
      "Epoch 1801: Training Loss: 0.4434957305590312 Validation Loss: 0.8418862223625183\n",
      "Epoch 1802: Training Loss: 0.44312115510304767 Validation Loss: 0.839447557926178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1803: Training Loss: 0.44257688522338867 Validation Loss: 0.8384816646575928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1804: Training Loss: 0.4422445595264435 Validation Loss: 0.838200032711029\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1805: Training Loss: 0.4420967996120453 Validation Loss: 0.8378822207450867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1806: Training Loss: 0.4415927728017171 Validation Loss: 0.8374817371368408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1807: Training Loss: 0.44126062591870624 Validation Loss: 0.8414674401283264\n",
      "Epoch 1808: Training Loss: 0.4409322440624237 Validation Loss: 0.8407573103904724\n",
      "Epoch 1809: Training Loss: 0.440565288066864 Validation Loss: 0.8376258015632629\n",
      "Epoch 1810: Training Loss: 0.44001291195551556 Validation Loss: 0.8366950154304504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1811: Training Loss: 0.4398232599099477 Validation Loss: 0.8370477557182312\n",
      "Epoch 1812: Training Loss: 0.43945332368214923 Validation Loss: 0.839728593826294\n",
      "Epoch 1813: Training Loss: 0.4390597144762675 Validation Loss: 0.83777916431427\n",
      "Epoch 1814: Training Loss: 0.4386831025282542 Validation Loss: 0.8357518315315247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1815: Training Loss: 0.43827945987383526 Validation Loss: 0.8361091613769531\n",
      "Epoch 1816: Training Loss: 0.43806583682696026 Validation Loss: 0.8350691795349121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1817: Training Loss: 0.4377322991689046 Validation Loss: 0.8382580280303955\n",
      "Epoch 1818: Training Loss: 0.4372389316558838 Validation Loss: 0.8396655321121216\n",
      "Epoch 1819: Training Loss: 0.43702516953150433 Validation Loss: 0.8393045663833618\n",
      "Epoch 1820: Training Loss: 0.43661537766456604 Validation Loss: 0.8363661766052246\n",
      "Epoch 1821: Training Loss: 0.43604440490404767 Validation Loss: 0.8333437442779541\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1822: Training Loss: 0.43584760030110675 Validation Loss: 0.832466185092926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1823: Training Loss: 0.43542590737342834 Validation Loss: 0.8328190445899963\n",
      "Epoch 1824: Training Loss: 0.4351993799209595 Validation Loss: 0.8360103964805603\n",
      "Epoch 1825: Training Loss: 0.43477651476860046 Validation Loss: 0.836635172367096\n",
      "Epoch 1826: Training Loss: 0.43449199199676514 Validation Loss: 0.8372609615325928\n",
      "Epoch 1827: Training Loss: 0.4342515965302785 Validation Loss: 0.8335630297660828\n",
      "Epoch 1828: Training Loss: 0.4339217146237691 Validation Loss: 0.8329243063926697\n",
      "Epoch 1829: Training Loss: 0.43338582913080853 Validation Loss: 0.8352515697479248\n",
      "Epoch 1830: Training Loss: 0.43298129240671795 Validation Loss: 0.8354989886283875\n",
      "Epoch 1831: Training Loss: 0.43298061688741046 Validation Loss: 0.8329269289970398\n",
      "Epoch 1832: Training Loss: 0.43237874905268353 Validation Loss: 0.8361556529998779\n",
      "Epoch 1833: Training Loss: 0.43198062976201373 Validation Loss: 0.8360356688499451\n",
      "Epoch 1834: Training Loss: 0.43193842967351276 Validation Loss: 0.8358516693115234\n",
      "Epoch 1835: Training Loss: 0.431558758020401 Validation Loss: 0.8297730684280396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1836: Training Loss: 0.43105408549308777 Validation Loss: 0.8289117813110352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1837: Training Loss: 0.4306359092394511 Validation Loss: 0.8322410583496094\n",
      "Epoch 1838: Training Loss: 0.43064043919245404 Validation Loss: 0.8317620754241943\n",
      "Epoch 1839: Training Loss: 0.43018192052841187 Validation Loss: 0.8371464610099792\n",
      "Epoch 1840: Training Loss: 0.42978514234224957 Validation Loss: 0.8358464241027832\n",
      "Epoch 1841: Training Loss: 0.4291283388932546 Validation Loss: 0.830925464630127\n",
      "Epoch 1842: Training Loss: 0.4290354549884796 Validation Loss: 0.8269577622413635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1843: Training Loss: 0.4285879135131836 Validation Loss: 0.8287559151649475\n",
      "Epoch 1844: Training Loss: 0.4281163513660431 Validation Loss: 0.8326914310455322\n",
      "Epoch 1845: Training Loss: 0.42767276366551715 Validation Loss: 0.8346061706542969\n",
      "Epoch 1846: Training Loss: 0.42754021286964417 Validation Loss: 0.8315149545669556\n",
      "Epoch 1847: Training Loss: 0.42726529637972516 Validation Loss: 0.8300600647926331\n",
      "Epoch 1848: Training Loss: 0.42683778206507367 Validation Loss: 0.8325609564781189\n",
      "Epoch 1849: Training Loss: 0.4263351758321126 Validation Loss: 0.8319507241249084\n",
      "Epoch 1850: Training Loss: 0.42600571115811664 Validation Loss: 0.8293634653091431\n",
      "Epoch 1851: Training Loss: 0.4256725112597148 Validation Loss: 0.8296023607254028\n",
      "Epoch 1852: Training Loss: 0.425266553958257 Validation Loss: 0.8287050127983093\n",
      "Epoch 1853: Training Loss: 0.42503835757573444 Validation Loss: 0.8275043368339539\n",
      "Epoch 1854: Training Loss: 0.42471209168434143 Validation Loss: 0.8308190703392029\n",
      "Epoch 1855: Training Loss: 0.42423097292582196 Validation Loss: 0.8315802812576294\n",
      "Epoch 1856: Training Loss: 0.4240340093771617 Validation Loss: 0.8290947079658508\n",
      "Epoch 1857: Training Loss: 0.42361273368199664 Validation Loss: 0.8280719518661499\n",
      "Epoch 1858: Training Loss: 0.4232295751571655 Validation Loss: 0.8270421028137207\n",
      "Epoch 1859: Training Loss: 0.42308907707532245 Validation Loss: 0.827626645565033\n",
      "Epoch 1860: Training Loss: 0.4225548803806305 Validation Loss: 0.8303259015083313\n",
      "Epoch 1861: Training Loss: 0.42224263151486713 Validation Loss: 0.8305459022521973\n",
      "Epoch 1862: Training Loss: 0.42196892698605853 Validation Loss: 0.8302262425422668\n",
      "Epoch 1863: Training Loss: 0.42149125536282855 Validation Loss: 0.8277931213378906\n",
      "Epoch 1864: Training Loss: 0.4212861756483714 Validation Loss: 0.8254391551017761\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1865: Training Loss: 0.4209250013033549 Validation Loss: 0.8265946507453918\n",
      "Epoch 1866: Training Loss: 0.4209528962771098 Validation Loss: 0.8298627138137817\n",
      "Epoch 1867: Training Loss: 0.42034504810969037 Validation Loss: 0.82909095287323\n",
      "Epoch 1868: Training Loss: 0.41994060079256695 Validation Loss: 0.8253799676895142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1869: Training Loss: 0.4195227424303691 Validation Loss: 0.8226088881492615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1870: Training Loss: 0.419414480527242 Validation Loss: 0.8235944509506226\n",
      "Epoch 1871: Training Loss: 0.419136106967926 Validation Loss: 0.8283129930496216\n",
      "Epoch 1872: Training Loss: 0.41886024673779804 Validation Loss: 0.8308534622192383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1873: Training Loss: 0.4183416962623596 Validation Loss: 0.8290777206420898\n",
      "Epoch 1874: Training Loss: 0.41801883776982623 Validation Loss: 0.8241195678710938\n",
      "Epoch 1875: Training Loss: 0.4174729386965434 Validation Loss: 0.8225579261779785\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1876: Training Loss: 0.4173430899779002 Validation Loss: 0.8227080702781677\n",
      "Epoch 1877: Training Loss: 0.4168648024400075 Validation Loss: 0.8255292177200317\n",
      "Epoch 1878: Training Loss: 0.4165329933166504 Validation Loss: 0.8260696530342102\n",
      "Epoch 1879: Training Loss: 0.4163669943809509 Validation Loss: 0.8280442357063293\n",
      "Epoch 1880: Training Loss: 0.41609294215838116 Validation Loss: 0.8266193866729736\n",
      "Epoch 1881: Training Loss: 0.41576773921648663 Validation Loss: 0.8238836526870728\n",
      "Epoch 1882: Training Loss: 0.41537681221961975 Validation Loss: 0.8239910006523132\n",
      "Epoch 1883: Training Loss: 0.4148821036020915 Validation Loss: 0.8236874341964722\n",
      "Epoch 1884: Training Loss: 0.4146595001220703 Validation Loss: 0.8237296938896179\n",
      "Epoch 1885: Training Loss: 0.4144876301288605 Validation Loss: 0.8211690187454224\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1886: Training Loss: 0.41382332642873126 Validation Loss: 0.8226903676986694\n",
      "Epoch 1887: Training Loss: 0.41357411940892536 Validation Loss: 0.8247732520103455\n",
      "Epoch 1888: Training Loss: 0.4137193262577057 Validation Loss: 0.8219630718231201\n",
      "Epoch 1889: Training Loss: 0.41283843914667767 Validation Loss: 0.8234900832176208\n",
      "Epoch 1890: Training Loss: 0.41244789958000183 Validation Loss: 0.8254654407501221\n",
      "Epoch 1891: Training Loss: 0.41255900263786316 Validation Loss: 0.827634334564209\n",
      "Epoch 1892: Training Loss: 0.41210684180259705 Validation Loss: 0.8245176672935486\n",
      "Epoch 1893: Training Loss: 0.41173532605171204 Validation Loss: 0.8187721371650696\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1894: Training Loss: 0.4113054772218068 Validation Loss: 0.8169421553611755\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1895: Training Loss: 0.41110291083653766 Validation Loss: 0.8189744353294373\n",
      "Epoch 1896: Training Loss: 0.41063732902208966 Validation Loss: 0.8232155442237854\n",
      "Epoch 1897: Training Loss: 0.41043617328008014 Validation Loss: 0.8237205147743225\n",
      "Epoch 1898: Training Loss: 0.41000182429949444 Validation Loss: 0.8241042494773865\n",
      "Epoch 1899: Training Loss: 0.4097845256328583 Validation Loss: 0.8215740919113159\n",
      "Epoch 1900: Training Loss: 0.40928234656651813 Validation Loss: 0.8219629526138306\n",
      "Epoch 1901: Training Loss: 0.4089956780274709 Validation Loss: 0.8212568759918213\n",
      "Epoch 1902: Training Loss: 0.40872326493263245 Validation Loss: 0.8191741704940796\n",
      "Epoch 1903: Training Loss: 0.40830379724502563 Validation Loss: 0.8191080093383789\n",
      "Epoch 1904: Training Loss: 0.40808499852816266 Validation Loss: 0.8191386461257935\n",
      "Epoch 1905: Training Loss: 0.4075578451156616 Validation Loss: 0.8205247521400452\n",
      "Epoch 1906: Training Loss: 0.40760395924250287 Validation Loss: 0.8214288949966431\n",
      "Epoch 1907: Training Loss: 0.4072406490643819 Validation Loss: 0.8191171288490295\n",
      "Epoch 1908: Training Loss: 0.4067947566509247 Validation Loss: 0.8187867403030396\n",
      "Epoch 1909: Training Loss: 0.4062840739885966 Validation Loss: 0.8204317688941956\n",
      "Epoch 1910: Training Loss: 0.4059945046901703 Validation Loss: 0.8220657706260681\n",
      "Epoch 1911: Training Loss: 0.4057028790314992 Validation Loss: 0.8201238512992859\n",
      "Epoch 1912: Training Loss: 0.40541725357373554 Validation Loss: 0.8188721537590027\n",
      "Epoch 1913: Training Loss: 0.40550561745961505 Validation Loss: 0.8155902624130249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1914: Training Loss: 0.4049929479757945 Validation Loss: 0.8196197748184204\n",
      "Epoch 1915: Training Loss: 0.4045815070470174 Validation Loss: 0.8193619847297668\n",
      "Epoch 1916: Training Loss: 0.40414270758628845 Validation Loss: 0.8207774758338928\n",
      "Epoch 1917: Training Loss: 0.404070109128952 Validation Loss: 0.8174238801002502\n",
      "Epoch 1918: Training Loss: 0.4034490982691447 Validation Loss: 0.8180254101753235\n",
      "Epoch 1919: Training Loss: 0.4035742183526357 Validation Loss: 0.8156560659408569\n",
      "Epoch 1920: Training Loss: 0.40282947818438214 Validation Loss: 0.8168061375617981\n",
      "Epoch 1921: Training Loss: 0.40240729848543805 Validation Loss: 0.8201806545257568\n",
      "Epoch 1922: Training Loss: 0.40218259890874225 Validation Loss: 0.8205595016479492\n",
      "Epoch 1923: Training Loss: 0.4019162356853485 Validation Loss: 0.8178399801254272\n",
      "Epoch 1924: Training Loss: 0.40147127707799274 Validation Loss: 0.8160853385925293\n",
      "Epoch 1925: Training Loss: 0.40156054496765137 Validation Loss: 0.8171282410621643\n",
      "Epoch 1926: Training Loss: 0.4009356697400411 Validation Loss: 0.8138517141342163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1927: Training Loss: 0.4006777008374532 Validation Loss: 0.8136159181594849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1928: Training Loss: 0.4004484216372172 Validation Loss: 0.8162918090820312\n",
      "Epoch 1929: Training Loss: 0.4003596901893616 Validation Loss: 0.8197436928749084\n",
      "Epoch 1930: Training Loss: 0.3997033139069875 Validation Loss: 0.8178385496139526\n",
      "Epoch 1931: Training Loss: 0.39940815170605976 Validation Loss: 0.8140243291854858\n",
      "Epoch 1932: Training Loss: 0.39904335141181946 Validation Loss: 0.8129728436470032\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1933: Training Loss: 0.3986811439196269 Validation Loss: 0.8133131861686707\n",
      "Epoch 1934: Training Loss: 0.3983067770799001 Validation Loss: 0.8156452775001526\n",
      "Epoch 1935: Training Loss: 0.39807135860125226 Validation Loss: 0.8182145953178406\n",
      "Epoch 1936: Training Loss: 0.39799577991167706 Validation Loss: 0.8187054991722107\n",
      "Epoch 1937: Training Loss: 0.39762136340141296 Validation Loss: 0.8157342076301575\n",
      "Epoch 1938: Training Loss: 0.39720645546913147 Validation Loss: 0.812052309513092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1939: Training Loss: 0.3969326317310333 Validation Loss: 0.810474157333374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1940: Training Loss: 0.3967886070410411 Validation Loss: 0.8139905333518982\n",
      "Epoch 1941: Training Loss: 0.396297683318456 Validation Loss: 0.8142728209495544\n",
      "Epoch 1942: Training Loss: 0.3959425091743469 Validation Loss: 0.8158720135688782\n",
      "Epoch 1943: Training Loss: 0.3958592712879181 Validation Loss: 0.8166623711585999\n",
      "Epoch 1944: Training Loss: 0.395516316095988 Validation Loss: 0.8138729333877563\n",
      "Epoch 1945: Training Loss: 0.3951284388701121 Validation Loss: 0.8119193315505981\n",
      "Epoch 1946: Training Loss: 0.39476914207140607 Validation Loss: 0.8135751485824585\n",
      "Epoch 1947: Training Loss: 0.3945210874080658 Validation Loss: 0.8110436201095581\n",
      "Epoch 1948: Training Loss: 0.394108643134435 Validation Loss: 0.8133206963539124\n",
      "Epoch 1949: Training Loss: 0.3937767545382182 Validation Loss: 0.8142780661582947\n",
      "Epoch 1950: Training Loss: 0.39359845717748004 Validation Loss: 0.8115136027336121\n",
      "Epoch 1951: Training Loss: 0.39318154255549115 Validation Loss: 0.8110441565513611\n",
      "Epoch 1952: Training Loss: 0.392998735109965 Validation Loss: 0.8154683709144592\n",
      "Epoch 1953: Training Loss: 0.39257259170214337 Validation Loss: 0.8134137988090515\n",
      "Epoch 1954: Training Loss: 0.3922440707683563 Validation Loss: 0.8135092854499817\n",
      "Epoch 1955: Training Loss: 0.3923192222913106 Validation Loss: 0.8088375329971313\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1956: Training Loss: 0.392041951417923 Validation Loss: 0.8115852475166321\n",
      "Epoch 1957: Training Loss: 0.3915278712908427 Validation Loss: 0.8117267489433289\n",
      "Epoch 1958: Training Loss: 0.39102743069330853 Validation Loss: 0.8109483122825623\n",
      "Epoch 1959: Training Loss: 0.3907108008861542 Validation Loss: 0.8129130601882935\n",
      "Epoch 1960: Training Loss: 0.3904810845851898 Validation Loss: 0.8131878972053528\n",
      "Epoch 1961: Training Loss: 0.3900890251000722 Validation Loss: 0.8102852702140808\n",
      "Epoch 1962: Training Loss: 0.38985560337702435 Validation Loss: 0.8076531291007996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1963: Training Loss: 0.3897011975447337 Validation Loss: 0.808751106262207\n",
      "Epoch 1964: Training Loss: 0.3891629676024119 Validation Loss: 0.810857892036438\n",
      "Epoch 1965: Training Loss: 0.38881951570510864 Validation Loss: 0.8124556541442871\n",
      "Epoch 1966: Training Loss: 0.38872501254081726 Validation Loss: 0.8106508851051331\n",
      "Epoch 1967: Training Loss: 0.3882485330104828 Validation Loss: 0.8103580474853516\n",
      "Epoch 1968: Training Loss: 0.38792645931243896 Validation Loss: 0.8098669648170471\n",
      "Epoch 1969: Training Loss: 0.387784610191981 Validation Loss: 0.8096809387207031\n",
      "Epoch 1970: Training Loss: 0.38742175698280334 Validation Loss: 0.8091326951980591\n",
      "Epoch 1971: Training Loss: 0.38721505800882977 Validation Loss: 0.8089447021484375\n",
      "Epoch 1972: Training Loss: 0.38695818185806274 Validation Loss: 0.8093934059143066\n",
      "Epoch 1973: Training Loss: 0.387008140484492 Validation Loss: 0.8064206838607788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1974: Training Loss: 0.38622961441675824 Validation Loss: 0.8098046183586121\n",
      "Epoch 1975: Training Loss: 0.3859519958496094 Validation Loss: 0.8108252286911011\n",
      "Epoch 1976: Training Loss: 0.38575872778892517 Validation Loss: 0.8094887137413025\n",
      "Epoch 1977: Training Loss: 0.3854083518187205 Validation Loss: 0.8081443905830383\n",
      "Epoch 1978: Training Loss: 0.3850948214530945 Validation Loss: 0.8076040148735046\n",
      "Epoch 1979: Training Loss: 0.3847673535346985 Validation Loss: 0.8069235682487488\n",
      "Epoch 1980: Training Loss: 0.3844531873861949 Validation Loss: 0.8076478838920593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1981: Training Loss: 0.3840983708699544 Validation Loss: 0.8069779276847839\n",
      "Epoch 1982: Training Loss: 0.38384095827738446 Validation Loss: 0.8086427450180054\n",
      "Epoch 1983: Training Loss: 0.383450190226237 Validation Loss: 0.8081363439559937\n",
      "Epoch 1984: Training Loss: 0.38330863912900287 Validation Loss: 0.806762158870697\n",
      "Epoch 1985: Training Loss: 0.3831574618816376 Validation Loss: 0.8091999292373657\n",
      "Epoch 1986: Training Loss: 0.3826895554860433 Validation Loss: 0.8084615468978882\n",
      "Epoch 1987: Training Loss: 0.38263005018234253 Validation Loss: 0.808363676071167\n",
      "Epoch 1988: Training Loss: 0.38198766112327576 Validation Loss: 0.8052045702934265\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1989: Training Loss: 0.381808340549469 Validation Loss: 0.8030379414558411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1990: Training Loss: 0.3816879491011302 Validation Loss: 0.8039368987083435\n",
      "Epoch 1991: Training Loss: 0.38125185171763104 Validation Loss: 0.8046383261680603\n",
      "Epoch 1992: Training Loss: 0.3808283706506093 Validation Loss: 0.8059849739074707\n",
      "Epoch 1993: Training Loss: 0.38064976533253986 Validation Loss: 0.8073939085006714\n",
      "Epoch 1994: Training Loss: 0.38035715619723004 Validation Loss: 0.8068356513977051\n",
      "Epoch 1995: Training Loss: 0.38034628828366596 Validation Loss: 0.8054072260856628\n",
      "Epoch 1996: Training Loss: 0.37971317768096924 Validation Loss: 0.8071478009223938\n",
      "Epoch 1997: Training Loss: 0.3795100549856822 Validation Loss: 0.8053334951400757\n",
      "Epoch 1998: Training Loss: 0.3791187107563019 Validation Loss: 0.8036679625511169\n",
      "Epoch 1999: Training Loss: 0.37901467084884644 Validation Loss: 0.8011707067489624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2000: Training Loss: 0.3787115315596263 Validation Loss: 0.8020386695861816\n",
      "Epoch 2001: Training Loss: 0.37817448377609253 Validation Loss: 0.8054905533790588\n",
      "Epoch 2002: Training Loss: 0.37789881229400635 Validation Loss: 0.8061899542808533\n",
      "Epoch 2003: Training Loss: 0.37760473291079205 Validation Loss: 0.8058401346206665\n",
      "Epoch 2004: Training Loss: 0.3774529794851939 Validation Loss: 0.8045620322227478\n",
      "Epoch 2005: Training Loss: 0.37713030974070233 Validation Loss: 0.8056188821792603\n",
      "Epoch 2006: Training Loss: 0.3768052359422048 Validation Loss: 0.80487459897995\n",
      "Epoch 2007: Training Loss: 0.37645838658014935 Validation Loss: 0.8028563261032104\n",
      "Epoch 2008: Training Loss: 0.3762885332107544 Validation Loss: 0.8013424873352051\n",
      "Epoch 2009: Training Loss: 0.37595898906389874 Validation Loss: 0.8032709360122681\n",
      "Epoch 2010: Training Loss: 0.3757249613602956 Validation Loss: 0.8043017387390137\n",
      "Epoch 2011: Training Loss: 0.37533921003341675 Validation Loss: 0.8044777512550354\n",
      "Epoch 2012: Training Loss: 0.375346581141154 Validation Loss: 0.8048331141471863\n",
      "Epoch 2013: Training Loss: 0.37512829899787903 Validation Loss: 0.8020784258842468\n",
      "Epoch 2014: Training Loss: 0.37448394298553467 Validation Loss: 0.8030102849006653\n",
      "Epoch 2015: Training Loss: 0.3743151028951009 Validation Loss: 0.8033755421638489\n",
      "Epoch 2016: Training Loss: 0.3740084071954091 Validation Loss: 0.7996844053268433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2017: Training Loss: 0.37387993931770325 Validation Loss: 0.7981842160224915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2018: Training Loss: 0.3737119336922963 Validation Loss: 0.8018270134925842\n",
      "Epoch 2019: Training Loss: 0.3732684552669525 Validation Loss: 0.8068143725395203\n",
      "Epoch 2020: Training Loss: 0.37292346358299255 Validation Loss: 0.8061724305152893\n",
      "Epoch 2021: Training Loss: 0.37260868151982623 Validation Loss: 0.8020920753479004\n",
      "Epoch 2022: Training Loss: 0.3721666733423869 Validation Loss: 0.8001262545585632\n",
      "Epoch 2023: Training Loss: 0.371983140707016 Validation Loss: 0.7983768582344055\n",
      "Epoch 2024: Training Loss: 0.3717454473177592 Validation Loss: 0.7998288869857788\n",
      "Epoch 2025: Training Loss: 0.3716232180595398 Validation Loss: 0.8030403852462769\n",
      "Epoch 2026: Training Loss: 0.3711082835992177 Validation Loss: 0.8029862642288208\n",
      "Epoch 2027: Training Loss: 0.37088871002197266 Validation Loss: 0.7999703884124756\n",
      "Epoch 2028: Training Loss: 0.3707335690657298 Validation Loss: 0.7970178723335266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2029: Training Loss: 0.3703956604003906 Validation Loss: 0.7970640063285828\n",
      "Epoch 2030: Training Loss: 0.37005823850631714 Validation Loss: 0.8029269576072693\n",
      "Epoch 2031: Training Loss: 0.3702248732248942 Validation Loss: 0.8042474389076233\n",
      "Epoch 2032: Training Loss: 0.36959609389305115 Validation Loss: 0.8000515699386597\n",
      "Epoch 2033: Training Loss: 0.3694613079229991 Validation Loss: 0.7971016764640808\n",
      "Epoch 2034: Training Loss: 0.36906498670578003 Validation Loss: 0.7977333068847656\n",
      "Epoch 2035: Training Loss: 0.36851279934247333 Validation Loss: 0.8013280034065247\n",
      "Epoch 2036: Training Loss: 0.36834973096847534 Validation Loss: 0.8034384846687317\n",
      "Epoch 2037: Training Loss: 0.368101567029953 Validation Loss: 0.8019704222679138\n",
      "Epoch 2038: Training Loss: 0.3677005072434743 Validation Loss: 0.8003200888633728\n",
      "Epoch 2039: Training Loss: 0.36744128664334613 Validation Loss: 0.7976022362709045\n",
      "Epoch 2040: Training Loss: 0.36721400419871014 Validation Loss: 0.7970519661903381\n",
      "Epoch 2041: Training Loss: 0.36710434158643085 Validation Loss: 0.797569215297699\n",
      "Epoch 2042: Training Loss: 0.3666468660036723 Validation Loss: 0.7960597276687622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2043: Training Loss: 0.36649346351623535 Validation Loss: 0.7957469820976257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2044: Training Loss: 0.3659658432006836 Validation Loss: 0.799019992351532\n",
      "Epoch 2045: Training Loss: 0.36577108502388 Validation Loss: 0.8015549182891846\n",
      "Epoch 2046: Training Loss: 0.36551109949747723 Validation Loss: 0.8019680380821228\n",
      "Epoch 2047: Training Loss: 0.36528411507606506 Validation Loss: 0.8013243079185486\n",
      "Epoch 2048: Training Loss: 0.36513320604960126 Validation Loss: 0.7963854670524597\n",
      "Epoch 2049: Training Loss: 0.3651538093884786 Validation Loss: 0.7930789589881897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2050: Training Loss: 0.3644263247648875 Validation Loss: 0.7948846220970154\n",
      "Epoch 2051: Training Loss: 0.364668865998586 Validation Loss: 0.8013449311256409\n",
      "Epoch 2052: Training Loss: 0.3640306890010834 Validation Loss: 0.7997655272483826\n",
      "Epoch 2053: Training Loss: 0.3636223574479421 Validation Loss: 0.7979748845100403\n",
      "Epoch 2054: Training Loss: 0.3636391758918762 Validation Loss: 0.7933503985404968\n",
      "Epoch 2055: Training Loss: 0.36336124936739606 Validation Loss: 0.7963107824325562\n",
      "Epoch 2056: Training Loss: 0.36284200350443524 Validation Loss: 0.7986299395561218\n",
      "Epoch 2057: Training Loss: 0.3624507089455922 Validation Loss: 0.7966359853744507\n",
      "Epoch 2058: Training Loss: 0.36219239234924316 Validation Loss: 0.7955825924873352\n",
      "Epoch 2059: Training Loss: 0.3619718750317891 Validation Loss: 0.7959657907485962\n",
      "Epoch 2060: Training Loss: 0.36166009306907654 Validation Loss: 0.796850323677063\n",
      "Epoch 2061: Training Loss: 0.361539363861084 Validation Loss: 0.7964309453964233\n",
      "Epoch 2062: Training Loss: 0.3612576425075531 Validation Loss: 0.7972001433372498\n",
      "Epoch 2063: Training Loss: 0.36083802580833435 Validation Loss: 0.7966008186340332\n",
      "Epoch 2064: Training Loss: 0.3605869909127553 Validation Loss: 0.7957285046577454\n",
      "Epoch 2065: Training Loss: 0.36055224140485126 Validation Loss: 0.795728325843811\n",
      "Epoch 2066: Training Loss: 0.36030641198158264 Validation Loss: 0.7953718304634094\n",
      "Epoch 2067: Training Loss: 0.3596679965655009 Validation Loss: 0.7934793829917908\n",
      "Epoch 2068: Training Loss: 0.3597284257411957 Validation Loss: 0.7910600900650024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2069: Training Loss: 0.3595990637938182 Validation Loss: 0.7935799956321716\n",
      "Epoch 2070: Training Loss: 0.3590657711029053 Validation Loss: 0.7946072220802307\n",
      "Epoch 2071: Training Loss: 0.3587391674518585 Validation Loss: 0.7952979803085327\n",
      "Epoch 2072: Training Loss: 0.3583730359872182 Validation Loss: 0.7969356775283813\n",
      "Epoch 2073: Training Loss: 0.35815752545992535 Validation Loss: 0.7956494092941284\n",
      "Epoch 2074: Training Loss: 0.3579842448234558 Validation Loss: 0.7945682406425476\n",
      "Epoch 2075: Training Loss: 0.3576175769170125 Validation Loss: 0.7937649488449097\n",
      "Epoch 2076: Training Loss: 0.3576427499453227 Validation Loss: 0.7938231229782104\n",
      "Epoch 2077: Training Loss: 0.3574623962243398 Validation Loss: 0.7908151745796204\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2078: Training Loss: 0.3570154706637065 Validation Loss: 0.7931162714958191\n",
      "Epoch 2079: Training Loss: 0.3566303650538127 Validation Loss: 0.7945374846458435\n",
      "Epoch 2080: Training Loss: 0.3564973771572113 Validation Loss: 0.7971556782722473\n",
      "Epoch 2081: Training Loss: 0.35624590516090393 Validation Loss: 0.7950640320777893\n",
      "Epoch 2082: Training Loss: 0.35634809732437134 Validation Loss: 0.7902154922485352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2083: Training Loss: 0.35563422242800397 Validation Loss: 0.7917448282241821\n",
      "Epoch 2084: Training Loss: 0.35518039266268414 Validation Loss: 0.7925287485122681\n",
      "Epoch 2085: Training Loss: 0.3549516598383586 Validation Loss: 0.7925939559936523\n",
      "Epoch 2086: Training Loss: 0.35467488567034405 Validation Loss: 0.7930086851119995\n",
      "Epoch 2087: Training Loss: 0.35444018244743347 Validation Loss: 0.7941064834594727\n",
      "Epoch 2088: Training Loss: 0.35425790150960285 Validation Loss: 0.7935404777526855\n",
      "Epoch 2089: Training Loss: 0.3540457288424174 Validation Loss: 0.7941745519638062\n",
      "Epoch 2090: Training Loss: 0.3536197741826375 Validation Loss: 0.792578935623169\n",
      "Epoch 2091: Training Loss: 0.35332756241162616 Validation Loss: 0.791904866695404\n",
      "Epoch 2092: Training Loss: 0.3532108962535858 Validation Loss: 0.7898406386375427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2093: Training Loss: 0.35295302669207257 Validation Loss: 0.7893935441970825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2094: Training Loss: 0.35273826122283936 Validation Loss: 0.7919979095458984\n",
      "Epoch 2095: Training Loss: 0.3525352378686269 Validation Loss: 0.7956165075302124\n",
      "Epoch 2096: Training Loss: 0.35232582688331604 Validation Loss: 0.7946381568908691\n",
      "Epoch 2097: Training Loss: 0.35175832112630206 Validation Loss: 0.7899030447006226\n",
      "Epoch 2098: Training Loss: 0.3522711197535197 Validation Loss: 0.7848352193832397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2099: Training Loss: 0.3515590727329254 Validation Loss: 0.7881267666816711\n",
      "Epoch 2100: Training Loss: 0.3512476881345113 Validation Loss: 0.7945157289505005\n",
      "Epoch 2101: Training Loss: 0.3511921266714732 Validation Loss: 0.792772114276886\n",
      "Epoch 2102: Training Loss: 0.35057422518730164 Validation Loss: 0.7927095890045166\n",
      "Epoch 2103: Training Loss: 0.35031984249750775 Validation Loss: 0.791214108467102\n",
      "Epoch 2104: Training Loss: 0.35010939836502075 Validation Loss: 0.7899302244186401\n",
      "Epoch 2105: Training Loss: 0.34981002410252887 Validation Loss: 0.7869184613227844\n",
      "Epoch 2106: Training Loss: 0.3496467371781667 Validation Loss: 0.7877277135848999\n",
      "Epoch 2107: Training Loss: 0.3493037521839142 Validation Loss: 0.7903934121131897\n",
      "Epoch 2108: Training Loss: 0.3489770293235779 Validation Loss: 0.792751133441925\n",
      "Epoch 2109: Training Loss: 0.3490537405014038 Validation Loss: 0.790976345539093\n",
      "Epoch 2110: Training Loss: 0.34871633847554523 Validation Loss: 0.7884295582771301\n",
      "Epoch 2111: Training Loss: 0.34830209612846375 Validation Loss: 0.7861626744270325\n",
      "Epoch 2112: Training Loss: 0.34810930490493774 Validation Loss: 0.7876207828521729\n",
      "Epoch 2113: Training Loss: 0.3478521903355916 Validation Loss: 0.790814995765686\n",
      "Epoch 2114: Training Loss: 0.34744102756182355 Validation Loss: 0.7896649837493896\n",
      "Epoch 2115: Training Loss: 0.34718098243077594 Validation Loss: 0.7896021008491516\n",
      "Epoch 2116: Training Loss: 0.3469501535097758 Validation Loss: 0.788279116153717\n",
      "Epoch 2117: Training Loss: 0.34684938192367554 Validation Loss: 0.7887780070304871\n",
      "Epoch 2118: Training Loss: 0.3464040756225586 Validation Loss: 0.7901045680046082\n",
      "Epoch 2119: Training Loss: 0.34668878714243573 Validation Loss: 0.7937822341918945\n",
      "Epoch 2120: Training Loss: 0.3460142910480499 Validation Loss: 0.789685845375061\n",
      "Epoch 2121: Training Loss: 0.3456376890341441 Validation Loss: 0.7851861119270325\n",
      "Epoch 2122: Training Loss: 0.34537772337595624 Validation Loss: 0.784873902797699\n",
      "Epoch 2123: Training Loss: 0.3452862004439036 Validation Loss: 0.7855897545814514\n",
      "Epoch 2124: Training Loss: 0.34491737683614093 Validation Loss: 0.7868632078170776\n",
      "Epoch 2125: Training Loss: 0.34473655621210736 Validation Loss: 0.7898024320602417\n",
      "Epoch 2126: Training Loss: 0.3444828689098358 Validation Loss: 0.7879446744918823\n",
      "Epoch 2127: Training Loss: 0.34409746527671814 Validation Loss: 0.7875990867614746\n",
      "Epoch 2128: Training Loss: 0.3439333240191142 Validation Loss: 0.7888255715370178\n",
      "Epoch 2129: Training Loss: 0.34376702706019086 Validation Loss: 0.787457287311554\n",
      "Epoch 2130: Training Loss: 0.3436667819817861 Validation Loss: 0.7878625392913818\n",
      "Epoch 2131: Training Loss: 0.3431427975495656 Validation Loss: 0.7871006727218628\n",
      "Epoch 2132: Training Loss: 0.3430250287055969 Validation Loss: 0.7844856977462769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2133: Training Loss: 0.3428349296251933 Validation Loss: 0.7845335602760315\n",
      "Epoch 2134: Training Loss: 0.34236906965573627 Validation Loss: 0.7894182205200195\n",
      "Epoch 2135: Training Loss: 0.34217122197151184 Validation Loss: 0.7906599640846252\n",
      "Epoch 2136: Training Loss: 0.3419958750406901 Validation Loss: 0.7893240451812744\n",
      "Epoch 2137: Training Loss: 0.3416029413541158 Validation Loss: 0.7849400639533997\n",
      "Epoch 2138: Training Loss: 0.3415225048859914 Validation Loss: 0.781979501247406\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2139: Training Loss: 0.3413395980993907 Validation Loss: 0.7818204760551453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2140: Training Loss: 0.34085844953854877 Validation Loss: 0.7868504524230957\n",
      "Epoch 2141: Training Loss: 0.3406954805056254 Validation Loss: 0.7890550494194031\n",
      "Epoch 2142: Training Loss: 0.3405471046765645 Validation Loss: 0.7890917062759399\n",
      "Epoch 2143: Training Loss: 0.34018086393674213 Validation Loss: 0.7857308983802795\n",
      "Epoch 2144: Training Loss: 0.3399841288725535 Validation Loss: 0.7839246392250061\n",
      "Epoch 2145: Training Loss: 0.33986468116442364 Validation Loss: 0.7851149439811707\n",
      "Epoch 2146: Training Loss: 0.33941423892974854 Validation Loss: 0.7848355174064636\n",
      "Epoch 2147: Training Loss: 0.3392435113588969 Validation Loss: 0.7842115759849548\n",
      "Epoch 2148: Training Loss: 0.339004784822464 Validation Loss: 0.786067545413971\n",
      "Epoch 2149: Training Loss: 0.33862408995628357 Validation Loss: 0.7859999537467957\n",
      "Epoch 2150: Training Loss: 0.33863288164138794 Validation Loss: 0.7877339720726013\n",
      "Epoch 2151: Training Loss: 0.3383508125940959 Validation Loss: 0.7830680012702942\n",
      "Epoch 2152: Training Loss: 0.33798492948214215 Validation Loss: 0.7832043766975403\n",
      "Epoch 2153: Training Loss: 0.3378667136033376 Validation Loss: 0.7852684855461121\n",
      "Epoch 2154: Training Loss: 0.3374677201112111 Validation Loss: 0.7851870059967041\n",
      "Epoch 2155: Training Loss: 0.33726712067921955 Validation Loss: 0.7829251289367676\n",
      "Epoch 2156: Training Loss: 0.337202250957489 Validation Loss: 0.7815040946006775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2157: Training Loss: 0.33674927552541095 Validation Loss: 0.7843846082687378\n",
      "Epoch 2158: Training Loss: 0.3365847071011861 Validation Loss: 0.785586953163147\n",
      "Epoch 2159: Training Loss: 0.3363065520922343 Validation Loss: 0.785081684589386\n",
      "Epoch 2160: Training Loss: 0.33602331082026166 Validation Loss: 0.7834243178367615\n",
      "Epoch 2161: Training Loss: 0.3357714017232259 Validation Loss: 0.7825223803520203\n",
      "Epoch 2162: Training Loss: 0.335492084423701 Validation Loss: 0.7821651101112366\n",
      "Epoch 2163: Training Loss: 0.3352877199649811 Validation Loss: 0.7834309935569763\n",
      "Epoch 2164: Training Loss: 0.3350025415420532 Validation Loss: 0.7835121154785156\n",
      "Epoch 2165: Training Loss: 0.33486268917719525 Validation Loss: 0.7816015481948853\n",
      "Epoch 2166: Training Loss: 0.3344716529051463 Validation Loss: 0.7828590869903564\n",
      "Epoch 2167: Training Loss: 0.3343740502993266 Validation Loss: 0.7849771976470947\n",
      "Epoch 2168: Training Loss: 0.3343643844127655 Validation Loss: 0.7833679914474487\n",
      "Epoch 2169: Training Loss: 0.33380378286043805 Validation Loss: 0.7822921276092529\n",
      "Epoch 2170: Training Loss: 0.3337179819742839 Validation Loss: 0.7793600559234619\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2171: Training Loss: 0.3337535361448924 Validation Loss: 0.7825160026550293\n",
      "Epoch 2172: Training Loss: 0.3332597513993581 Validation Loss: 0.7813445925712585\n",
      "Epoch 2173: Training Loss: 0.33288158973058063 Validation Loss: 0.7809798121452332\n",
      "Epoch 2174: Training Loss: 0.3326449394226074 Validation Loss: 0.7825574278831482\n",
      "Epoch 2175: Training Loss: 0.3324582775433858 Validation Loss: 0.7810810208320618\n",
      "Epoch 2176: Training Loss: 0.33213624358177185 Validation Loss: 0.7819015383720398\n",
      "Epoch 2177: Training Loss: 0.33233505487442017 Validation Loss: 0.7852235436439514\n",
      "Epoch 2178: Training Loss: 0.3318295975526174 Validation Loss: 0.7816558480262756\n",
      "Epoch 2179: Training Loss: 0.33146337668100995 Validation Loss: 0.7814385294914246\n",
      "Epoch 2180: Training Loss: 0.3312201996644338 Validation Loss: 0.781672477722168\n",
      "Epoch 2181: Training Loss: 0.33087971806526184 Validation Loss: 0.780927836894989\n",
      "Epoch 2182: Training Loss: 0.3306877613067627 Validation Loss: 0.7788649797439575\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2183: Training Loss: 0.33088042338689166 Validation Loss: 0.7823110222816467\n",
      "Epoch 2184: Training Loss: 0.33039100964864093 Validation Loss: 0.7800909876823425\n",
      "Epoch 2185: Training Loss: 0.33002373576164246 Validation Loss: 0.7801668047904968\n",
      "Epoch 2186: Training Loss: 0.3298975427945455 Validation Loss: 0.7801159620285034\n",
      "Epoch 2187: Training Loss: 0.32953016956647235 Validation Loss: 0.7809156775474548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2188: Training Loss: 0.3294810652732849 Validation Loss: 0.7831338047981262\n",
      "Epoch 2189: Training Loss: 0.32917991280555725 Validation Loss: 0.7802817225456238\n",
      "Epoch 2190: Training Loss: 0.32909151911735535 Validation Loss: 0.7774823904037476\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2191: Training Loss: 0.3287007808685303 Validation Loss: 0.7800978422164917\n",
      "Epoch 2192: Training Loss: 0.32833005984624225 Validation Loss: 0.7805401086807251\n",
      "Epoch 2193: Training Loss: 0.3280889590581258 Validation Loss: 0.7803242206573486\n",
      "Epoch 2194: Training Loss: 0.3279985586802165 Validation Loss: 0.7811175584793091\n",
      "Epoch 2195: Training Loss: 0.3278052310148875 Validation Loss: 0.7777183055877686\n",
      "Epoch 2196: Training Loss: 0.3275265693664551 Validation Loss: 0.7770296335220337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2197: Training Loss: 0.32734668254852295 Validation Loss: 0.7794573307037354\n",
      "Epoch 2198: Training Loss: 0.3271083136399587 Validation Loss: 0.7818871140480042\n",
      "Epoch 2199: Training Loss: 0.32669951518376666 Validation Loss: 0.779926598072052\n",
      "Epoch 2200: Training Loss: 0.3267453908920288 Validation Loss: 0.7808270454406738\n",
      "Epoch 2201: Training Loss: 0.32618560393651325 Validation Loss: 0.7778317332267761\n",
      "Epoch 2202: Training Loss: 0.3261413772900899 Validation Loss: 0.7771328091621399\n",
      "Epoch 2203: Training Loss: 0.32577956716219586 Validation Loss: 0.7762055397033691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2204: Training Loss: 0.32575512925783795 Validation Loss: 0.7788663506507874\n",
      "Epoch 2205: Training Loss: 0.32531534632047016 Validation Loss: 0.7781447172164917\n",
      "Epoch 2206: Training Loss: 0.32531823714574176 Validation Loss: 0.7758668065071106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2207: Training Loss: 0.32508047421773273 Validation Loss: 0.7786749005317688\n",
      "Epoch 2208: Training Loss: 0.32461340228716534 Validation Loss: 0.7788766622543335\n",
      "Epoch 2209: Training Loss: 0.32462721069653827 Validation Loss: 0.7811768054962158\n",
      "Epoch 2210: Training Loss: 0.32449740171432495 Validation Loss: 0.7768361568450928\n",
      "Epoch 2211: Training Loss: 0.3239709536234538 Validation Loss: 0.7782901525497437\n",
      "Epoch 2212: Training Loss: 0.32383747895558673 Validation Loss: 0.7783136367797852\n",
      "Epoch 2213: Training Loss: 0.32353590925534564 Validation Loss: 0.7764757871627808\n",
      "Epoch 2214: Training Loss: 0.32320966323216754 Validation Loss: 0.7758477330207825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2215: Training Loss: 0.3234110673268636 Validation Loss: 0.77629154920578\n",
      "Epoch 2216: Training Loss: 0.32279763619105023 Validation Loss: 0.7792608141899109\n",
      "Epoch 2217: Training Loss: 0.3226824899514516 Validation Loss: 0.7791059017181396\n",
      "Epoch 2218: Training Loss: 0.32229718565940857 Validation Loss: 0.7781230807304382\n",
      "Epoch 2219: Training Loss: 0.32202309370040894 Validation Loss: 0.7776660323143005\n",
      "Epoch 2220: Training Loss: 0.3218415876229604 Validation Loss: 0.776014506816864\n",
      "Epoch 2221: Training Loss: 0.32175100843111676 Validation Loss: 0.7759014368057251\n",
      "Epoch 2222: Training Loss: 0.3214674691359202 Validation Loss: 0.7758771777153015\n",
      "Epoch 2223: Training Loss: 0.3214336037635803 Validation Loss: 0.7777798175811768\n",
      "Epoch 2224: Training Loss: 0.32106682658195496 Validation Loss: 0.7760004997253418\n",
      "Epoch 2225: Training Loss: 0.32074280579884845 Validation Loss: 0.7763357758522034\n",
      "Epoch 2226: Training Loss: 0.32058075070381165 Validation Loss: 0.7748023867607117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2227: Training Loss: 0.32036110758781433 Validation Loss: 0.7754086852073669\n",
      "Epoch 2228: Training Loss: 0.32009199261665344 Validation Loss: 0.777225911617279\n",
      "Epoch 2229: Training Loss: 0.3200210630893707 Validation Loss: 0.7757433652877808\n",
      "Epoch 2230: Training Loss: 0.31970415512720746 Validation Loss: 0.7783687710762024\n",
      "Epoch 2231: Training Loss: 0.3195221523443858 Validation Loss: 0.7785536646842957\n",
      "Epoch 2232: Training Loss: 0.31926920016606647 Validation Loss: 0.7754036784172058\n",
      "Epoch 2233: Training Loss: 0.31902555624643963 Validation Loss: 0.772662878036499\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2234: Training Loss: 0.3190036217371623 Validation Loss: 0.7757887244224548\n",
      "Epoch 2235: Training Loss: 0.31851746638615924 Validation Loss: 0.7733383774757385\n",
      "Epoch 2236: Training Loss: 0.31840569774309796 Validation Loss: 0.7763143181800842\n",
      "Epoch 2237: Training Loss: 0.31835949420928955 Validation Loss: 0.7728101015090942\n",
      "Epoch 2238: Training Loss: 0.31796836853027344 Validation Loss: 0.7757188677787781\n",
      "Epoch 2239: Training Loss: 0.3176199495792389 Validation Loss: 0.7760792374610901\n",
      "Epoch 2240: Training Loss: 0.3173784812291463 Validation Loss: 0.7762338519096375\n",
      "Epoch 2241: Training Loss: 0.31719576319058734 Validation Loss: 0.7749186158180237\n",
      "Epoch 2242: Training Loss: 0.3170272211233775 Validation Loss: 0.7741753458976746\n",
      "Epoch 2243: Training Loss: 0.3168390492598216 Validation Loss: 0.7751404047012329\n",
      "Epoch 2244: Training Loss: 0.316969374815623 Validation Loss: 0.7707399725914001\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2245: Training Loss: 0.31642524401346844 Validation Loss: 0.7718876600265503\n",
      "Epoch 2246: Training Loss: 0.31644787391026813 Validation Loss: 0.7773222923278809\n",
      "Epoch 2247: Training Loss: 0.3159198264280955 Validation Loss: 0.7780869007110596\n",
      "Epoch 2248: Training Loss: 0.31574727098147076 Validation Loss: 0.7753169536590576\n",
      "Epoch 2249: Training Loss: 0.31546996037165326 Validation Loss: 0.7733191251754761\n",
      "Epoch 2250: Training Loss: 0.31533952554066974 Validation Loss: 0.7730063199996948\n",
      "Epoch 2251: Training Loss: 0.31500746806462604 Validation Loss: 0.7743226289749146\n",
      "Epoch 2252: Training Loss: 0.31533100207646686 Validation Loss: 0.7759973406791687\n",
      "Epoch 2253: Training Loss: 0.314684917529424 Validation Loss: 0.7726264595985413\n",
      "Epoch 2254: Training Loss: 0.3142990569273631 Validation Loss: 0.7703025341033936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2255: Training Loss: 0.3141408363978068 Validation Loss: 0.7684199213981628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2256: Training Loss: 0.3140003482500712 Validation Loss: 0.7698266506195068\n",
      "Epoch 2257: Training Loss: 0.31373397509257 Validation Loss: 0.7735254764556885\n",
      "Epoch 2258: Training Loss: 0.3134972055753072 Validation Loss: 0.7770695090293884\n",
      "Epoch 2259: Training Loss: 0.31367237369219464 Validation Loss: 0.7784512639045715\n",
      "Epoch 2260: Training Loss: 0.3132283389568329 Validation Loss: 0.7730444073677063\n",
      "Epoch 2261: Training Loss: 0.3127381404240926 Validation Loss: 0.7714269757270813\n",
      "Epoch 2262: Training Loss: 0.3127451340357463 Validation Loss: 0.7688600420951843\n",
      "Epoch 2263: Training Loss: 0.31262049078941345 Validation Loss: 0.771409809589386\n",
      "Epoch 2264: Training Loss: 0.31229036053021747 Validation Loss: 0.7736831903457642\n",
      "Epoch 2265: Training Loss: 0.3121294577916463 Validation Loss: 0.7745979428291321\n",
      "Epoch 2266: Training Loss: 0.3117275834083557 Validation Loss: 0.7707419395446777\n",
      "Epoch 2267: Training Loss: 0.31171122193336487 Validation Loss: 0.7689997553825378\n",
      "Epoch 2268: Training Loss: 0.31157607833544415 Validation Loss: 0.7721303701400757\n",
      "Epoch 2269: Training Loss: 0.3112007677555084 Validation Loss: 0.7728296518325806\n",
      "Epoch 2270: Training Loss: 0.3108867108821869 Validation Loss: 0.7729629278182983\n",
      "Epoch 2271: Training Loss: 0.31062106291453045 Validation Loss: 0.7716389894485474\n",
      "Epoch 2272: Training Loss: 0.3103569944699605 Validation Loss: 0.7713033556938171\n",
      "Epoch 2273: Training Loss: 0.3102927307287852 Validation Loss: 0.7726554870605469\n",
      "Epoch 2274: Training Loss: 0.31006210048993427 Validation Loss: 0.7701879739761353\n",
      "Epoch 2275: Training Loss: 0.30992749333381653 Validation Loss: 0.7723972201347351\n",
      "Epoch 2276: Training Loss: 0.30954771240552265 Validation Loss: 0.7723711133003235\n",
      "Epoch 2277: Training Loss: 0.30934062600135803 Validation Loss: 0.7701624035835266\n",
      "Epoch 2278: Training Loss: 0.309540718793869 Validation Loss: 0.7709813714027405\n",
      "Epoch 2279: Training Loss: 0.3089039822419484 Validation Loss: 0.7696248292922974\n",
      "Epoch 2280: Training Loss: 0.30879483620325726 Validation Loss: 0.7709783911705017\n",
      "Epoch 2281: Training Loss: 0.3085283140341441 Validation Loss: 0.7692298293113708\n",
      "Epoch 2282: Training Loss: 0.3083494206269582 Validation Loss: 0.7711114287376404\n",
      "Epoch 2283: Training Loss: 0.30800963441530865 Validation Loss: 0.770393431186676\n",
      "Epoch 2284: Training Loss: 0.30792444944381714 Validation Loss: 0.7693518996238708\n",
      "Epoch 2285: Training Loss: 0.3077491223812103 Validation Loss: 0.768223762512207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 2286: Training Loss: 0.30739931265513104 Validation Loss: 0.7695934176445007\n",
      "Epoch 2287: Training Loss: 0.30720295508702594 Validation Loss: 0.773128092288971\n",
      "Epoch 2288: Training Loss: 0.30716272195180255 Validation Loss: 0.7719143629074097\n",
      "Epoch 2289: Training Loss: 0.306914081176122 Validation Loss: 0.7689091563224792\n",
      "Epoch 2290: Training Loss: 0.306548148393631 Validation Loss: 0.7678473591804504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2291: Training Loss: 0.3064214587211609 Validation Loss: 0.7702787518501282\n",
      "Epoch 2292: Training Loss: 0.30623336633046466 Validation Loss: 0.7711563110351562\n",
      "Epoch 2293: Training Loss: 0.30628639459609985 Validation Loss: 0.7739410996437073\n",
      "Epoch 2294: Training Loss: 0.3059642016887665 Validation Loss: 0.7698861360549927\n",
      "Epoch 2295: Training Loss: 0.305613249540329 Validation Loss: 0.7666385173797607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2296: Training Loss: 0.30531565348307294 Validation Loss: 0.7667204141616821\n",
      "Epoch 2297: Training Loss: 0.30531371633211773 Validation Loss: 0.7668098211288452\n",
      "Epoch 2298: Training Loss: 0.30490899085998535 Validation Loss: 0.7681410908699036\n",
      "Epoch 2299: Training Loss: 0.30464500188827515 Validation Loss: 0.7713490128517151\n",
      "Epoch 2300: Training Loss: 0.3047573963801066 Validation Loss: 0.7730940580368042\n",
      "Epoch 2301: Training Loss: 0.3044518729050954 Validation Loss: 0.7727104425430298\n",
      "Epoch 2302: Training Loss: 0.3041397035121918 Validation Loss: 0.7683265209197998\n",
      "Epoch 2303: Training Loss: 0.3039805293083191 Validation Loss: 0.7677345871925354\n",
      "Epoch 2304: Training Loss: 0.30368735392888385 Validation Loss: 0.7661950588226318\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2305: Training Loss: 0.30361271897951764 Validation Loss: 0.766029953956604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2306: Training Loss: 0.3032697836558024 Validation Loss: 0.7683788537979126\n",
      "Epoch 2307: Training Loss: 0.30309714873631793 Validation Loss: 0.7695872187614441\n",
      "Epoch 2308: Training Loss: 0.30287472407023114 Validation Loss: 0.7692796587944031\n",
      "Epoch 2309: Training Loss: 0.30273940165837604 Validation Loss: 0.7707690596580505\n",
      "Epoch 2310: Training Loss: 0.3024551471074422 Validation Loss: 0.7672538757324219\n",
      "Epoch 2311: Training Loss: 0.3022902508576711 Validation Loss: 0.767615795135498\n",
      "Epoch 2312: Training Loss: 0.30206185579299927 Validation Loss: 0.7662287950515747\n",
      "Epoch 2313: Training Loss: 0.30191096663475037 Validation Loss: 0.765745997428894\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2314: Training Loss: 0.3016514480113983 Validation Loss: 0.7678220272064209\n",
      "Epoch 2315: Training Loss: 0.30130430062611896 Validation Loss: 0.7698403000831604\n",
      "Epoch 2316: Training Loss: 0.30122796694437665 Validation Loss: 0.7691177725791931\n",
      "Epoch 2317: Training Loss: 0.3010698159535726 Validation Loss: 0.7673448324203491\n",
      "Epoch 2318: Training Loss: 0.30095840493837994 Validation Loss: 0.766189455986023\n",
      "Epoch 2319: Training Loss: 0.3006296257177989 Validation Loss: 0.7666580080986023\n",
      "Epoch 2320: Training Loss: 0.3005170424779256 Validation Loss: 0.7699950933456421\n",
      "Epoch 2321: Training Loss: 0.3001772960027059 Validation Loss: 0.7684609293937683\n",
      "Epoch 2322: Training Loss: 0.3000063399473826 Validation Loss: 0.7666986584663391\n",
      "Epoch 2323: Training Loss: 0.2998228172461192 Validation Loss: 0.7684330344200134\n",
      "Epoch 2324: Training Loss: 0.2998201946417491 Validation Loss: 0.7692210674285889\n",
      "Epoch 2325: Training Loss: 0.2993508478005727 Validation Loss: 0.7644003629684448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2326: Training Loss: 0.29923803607622784 Validation Loss: 0.7624316215515137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2327: Training Loss: 0.2990867793560028 Validation Loss: 0.7640940546989441\n",
      "Epoch 2328: Training Loss: 0.2988033394018809 Validation Loss: 0.7683852314949036\n",
      "Epoch 2329: Training Loss: 0.2988506456216176 Validation Loss: 0.7714561223983765\n",
      "Epoch 2330: Training Loss: 0.2985154390335083 Validation Loss: 0.7685106992721558\n",
      "Epoch 2331: Training Loss: 0.2981150150299072 Validation Loss: 0.766406774520874\n",
      "Epoch 2332: Training Loss: 0.2983623445034027 Validation Loss: 0.761922299861908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2333: Training Loss: 0.29813780387242633 Validation Loss: 0.7662818431854248\n",
      "Epoch 2334: Training Loss: 0.2975106438000997 Validation Loss: 0.7668715119361877\n",
      "Epoch 2335: Training Loss: 0.29737622539202374 Validation Loss: 0.7663096189498901\n",
      "Epoch 2336: Training Loss: 0.29720501104990643 Validation Loss: 0.7655592560768127\n",
      "Epoch 2337: Training Loss: 0.29711751143137616 Validation Loss: 0.764707088470459\n",
      "Epoch 2338: Training Loss: 0.296750341852506 Validation Loss: 0.7665931582450867\n",
      "Epoch 2339: Training Loss: 0.2964989145596822 Validation Loss: 0.7676728367805481\n",
      "Epoch 2340: Training Loss: 0.29636136690775555 Validation Loss: 0.7666077613830566\n",
      "Epoch 2341: Training Loss: 0.29607535401980084 Validation Loss: 0.7656107544898987\n",
      "Epoch 2342: Training Loss: 0.29591478407382965 Validation Loss: 0.7649763822555542\n",
      "Epoch 2343: Training Loss: 0.29576478401819867 Validation Loss: 0.7637410759925842\n",
      "Epoch 2344: Training Loss: 0.2957039773464203 Validation Loss: 0.7653015851974487\n",
      "Epoch 2345: Training Loss: 0.2953757842381795 Validation Loss: 0.7643983364105225\n",
      "Epoch 2346: Training Loss: 0.2952258785565694 Validation Loss: 0.7638612389564514\n",
      "Epoch 2347: Training Loss: 0.294981449842453 Validation Loss: 0.7632438540458679\n",
      "Epoch 2348: Training Loss: 0.29474592208862305 Validation Loss: 0.7641846537590027\n",
      "Epoch 2349: Training Loss: 0.29486913482348126 Validation Loss: 0.7677573561668396\n",
      "Epoch 2350: Training Loss: 0.29440729816754657 Validation Loss: 0.7656509280204773\n",
      "Epoch 2351: Training Loss: 0.29411132136980694 Validation Loss: 0.7649280428886414\n",
      "Epoch 2352: Training Loss: 0.29397472739219666 Validation Loss: 0.7638489007949829\n",
      "Epoch 2353: Training Loss: 0.2938457131385803 Validation Loss: 0.7635226845741272\n",
      "Epoch 2354: Training Loss: 0.29354436695575714 Validation Loss: 0.7618116140365601\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2355: Training Loss: 0.2933988869190216 Validation Loss: 0.7630123496055603\n",
      "Epoch 2356: Training Loss: 0.29331902662913006 Validation Loss: 0.7657844424247742\n",
      "Epoch 2357: Training Loss: 0.2931237618128459 Validation Loss: 0.7646059989929199\n",
      "Epoch 2358: Training Loss: 0.29279768466949463 Validation Loss: 0.7640502452850342\n",
      "Epoch 2359: Training Loss: 0.2925889988740285 Validation Loss: 0.7635525465011597\n",
      "Epoch 2360: Training Loss: 0.29257916410764057 Validation Loss: 0.7621056437492371\n",
      "Epoch 2361: Training Loss: 0.29222134749094647 Validation Loss: 0.7640249729156494\n",
      "Epoch 2362: Training Loss: 0.29200029373168945 Validation Loss: 0.7642616033554077\n",
      "Epoch 2363: Training Loss: 0.2918800910313924 Validation Loss: 0.7640874981880188\n",
      "Epoch 2364: Training Loss: 0.2917180856068929 Validation Loss: 0.764417827129364\n",
      "Epoch 2365: Training Loss: 0.29138962427775067 Validation Loss: 0.7642707824707031\n",
      "Epoch 2366: Training Loss: 0.2912511130174001 Validation Loss: 0.7633149027824402\n",
      "Epoch 2367: Training Loss: 0.2910618185997009 Validation Loss: 0.7636576890945435\n",
      "Epoch 2368: Training Loss: 0.2910355031490326 Validation Loss: 0.7613083124160767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2369: Training Loss: 0.29071319103240967 Validation Loss: 0.7610266208648682\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2370: Training Loss: 0.2904917299747467 Validation Loss: 0.7614076733589172\n",
      "Epoch 2371: Training Loss: 0.2901931206385295 Validation Loss: 0.763481616973877\n",
      "Epoch 2372: Training Loss: 0.2905113399028778 Validation Loss: 0.7678657174110413\n",
      "Epoch 2373: Training Loss: 0.2899657189846039 Validation Loss: 0.7651585340499878\n",
      "Epoch 2374: Training Loss: 0.28963788350423175 Validation Loss: 0.7620329856872559\n",
      "Epoch 2375: Training Loss: 0.28982863823572796 Validation Loss: 0.7578296065330505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2376: Training Loss: 0.2894938389460246 Validation Loss: 0.7612988948822021\n",
      "Epoch 2377: Training Loss: 0.2890864710013072 Validation Loss: 0.7631264328956604\n",
      "Epoch 2378: Training Loss: 0.28897883494695026 Validation Loss: 0.7635883688926697\n",
      "Epoch 2379: Training Loss: 0.28872595230738324 Validation Loss: 0.763939380645752\n",
      "Epoch 2380: Training Loss: 0.2888995409011841 Validation Loss: 0.7649158239364624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2381: Training Loss: 0.2883395353953044 Validation Loss: 0.7610088586807251\n",
      "Epoch 2382: Training Loss: 0.28819137811660767 Validation Loss: 0.7591168284416199\n",
      "Epoch 2383: Training Loss: 0.28799883524576825 Validation Loss: 0.7599336504936218\n",
      "Epoch 2384: Training Loss: 0.28788143396377563 Validation Loss: 0.7627512812614441\n",
      "Epoch 2385: Training Loss: 0.28799917300542194 Validation Loss: 0.7610253095626831\n",
      "Epoch 2386: Training Loss: 0.28742409745852154 Validation Loss: 0.7627018690109253\n",
      "Epoch 2387: Training Loss: 0.28721893827120465 Validation Loss: 0.7626670002937317\n",
      "Epoch 2388: Training Loss: 0.2869715789953868 Validation Loss: 0.7612147331237793\n",
      "Epoch 2389: Training Loss: 0.28691886862119037 Validation Loss: 0.7596494555473328\n",
      "Epoch 2390: Training Loss: 0.28662388523419696 Validation Loss: 0.7599297165870667\n",
      "Epoch 2391: Training Loss: 0.28645551204681396 Validation Loss: 0.760712206363678\n",
      "Epoch 2392: Training Loss: 0.2862800657749176 Validation Loss: 0.7634186148643494\n",
      "Epoch 2393: Training Loss: 0.28642122944196063 Validation Loss: 0.7638441324234009\n",
      "Epoch 2394: Training Loss: 0.28589824835459393 Validation Loss: 0.7604615092277527\n",
      "Epoch 2395: Training Loss: 0.28581444422403973 Validation Loss: 0.7598410248756409\n",
      "Epoch 2396: Training Loss: 0.28551235795021057 Validation Loss: 0.7610974907875061\n",
      "Epoch 2397: Training Loss: 0.2855078975359599 Validation Loss: 0.7625871300697327\n",
      "Epoch 2398: Training Loss: 0.28514252106348675 Validation Loss: 0.7608707547187805\n",
      "Epoch 2399: Training Loss: 0.28495092193285626 Validation Loss: 0.759607195854187\n",
      "Epoch 2400: Training Loss: 0.2848500708738963 Validation Loss: 0.7591840624809265\n",
      "Epoch 2401: Training Loss: 0.28482553362846375 Validation Loss: 0.7605356574058533\n",
      "Epoch 2402: Training Loss: 0.28477975726127625 Validation Loss: 0.7577665448188782\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2403: Training Loss: 0.284141610066096 Validation Loss: 0.7589825987815857\n",
      "Epoch 2404: Training Loss: 0.28410380085309345 Validation Loss: 0.7609978914260864\n",
      "Epoch 2405: Training Loss: 0.28409383694330853 Validation Loss: 0.7646962404251099\n",
      "Epoch 2406: Training Loss: 0.28379475076993305 Validation Loss: 0.7623587846755981\n",
      "Epoch 2407: Training Loss: 0.2833989957968394 Validation Loss: 0.7600391507148743\n",
      "Epoch 2408: Training Loss: 0.28342023491859436 Validation Loss: 0.7570371031761169\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2409: Training Loss: 0.2833177149295807 Validation Loss: 0.7590482234954834\n",
      "Epoch 2410: Training Loss: 0.2829679250717163 Validation Loss: 0.7608776092529297\n",
      "Epoch 2411: Training Loss: 0.28281213839848834 Validation Loss: 0.7591361403465271\n",
      "Epoch 2412: Training Loss: 0.2826942404111226 Validation Loss: 0.7610002756118774\n",
      "Epoch 2413: Training Loss: 0.28249157468477887 Validation Loss: 0.759376049041748\n",
      "Epoch 2414: Training Loss: 0.2821689347426097 Validation Loss: 0.7577141523361206\n",
      "Epoch 2415: Training Loss: 0.28203173478444415 Validation Loss: 0.7584657073020935\n",
      "Epoch 2416: Training Loss: 0.2819516658782959 Validation Loss: 0.7614333033561707\n",
      "Epoch 2417: Training Loss: 0.28182801604270935 Validation Loss: 0.7604578733444214\n",
      "Epoch 2418: Training Loss: 0.2813955694437027 Validation Loss: 0.7583047151565552\n",
      "Epoch 2419: Training Loss: 0.2813471059004466 Validation Loss: 0.7579416632652283\n",
      "Epoch 2420: Training Loss: 0.2811281432708104 Validation Loss: 0.7577747702598572\n",
      "Epoch 2421: Training Loss: 0.28088659048080444 Validation Loss: 0.7579869627952576\n",
      "Epoch 2422: Training Loss: 0.28062761823336285 Validation Loss: 0.759351909160614\n",
      "Epoch 2423: Training Loss: 0.28060993552207947 Validation Loss: 0.7614003419876099\n",
      "Epoch 2424: Training Loss: 0.2803926666577657 Validation Loss: 0.7596170902252197\n",
      "Epoch 2425: Training Loss: 0.2802434414625168 Validation Loss: 0.7589916586875916\n",
      "Epoch 2426: Training Loss: 0.28008370101451874 Validation Loss: 0.7568545937538147\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2427: Training Loss: 0.27973222732543945 Validation Loss: 0.7578868269920349\n",
      "Epoch 2428: Training Loss: 0.2796761691570282 Validation Loss: 0.7596975564956665\n",
      "Epoch 2429: Training Loss: 0.2794797718524933 Validation Loss: 0.7586862444877625\n",
      "Epoch 2430: Training Loss: 0.2792428731918335 Validation Loss: 0.7591070532798767\n",
      "Epoch 2431: Training Loss: 0.27923759321371716 Validation Loss: 0.7606476545333862\n",
      "Epoch 2432: Training Loss: 0.2788440187772115 Validation Loss: 0.7588532567024231\n",
      "Epoch 2433: Training Loss: 0.27865466475486755 Validation Loss: 0.7559897303581238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2434: Training Loss: 0.27856220801671344 Validation Loss: 0.7563811540603638\n",
      "Epoch 2435: Training Loss: 0.27838794390360516 Validation Loss: 0.755866289138794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2436: Training Loss: 0.2782912055651347 Validation Loss: 0.7579729557037354\n",
      "Epoch 2437: Training Loss: 0.27805201212565106 Validation Loss: 0.7603061199188232\n",
      "Epoch 2438: Training Loss: 0.27784795065720874 Validation Loss: 0.75806725025177\n",
      "Epoch 2439: Training Loss: 0.2776243984699249 Validation Loss: 0.7568774223327637\n",
      "Epoch 2440: Training Loss: 0.2774045467376709 Validation Loss: 0.7572039365768433\n",
      "Epoch 2441: Training Loss: 0.2772681812445323 Validation Loss: 0.7581114768981934\n",
      "Epoch 2442: Training Loss: 0.2771920959154765 Validation Loss: 0.757124662399292\n",
      "Epoch 2443: Training Loss: 0.27691881855328876 Validation Loss: 0.7551561594009399\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2444: Training Loss: 0.27677441636721295 Validation Loss: 0.7564666271209717\n",
      "Epoch 2445: Training Loss: 0.2764895061651866 Validation Loss: 0.7556882500648499\n",
      "Epoch 2446: Training Loss: 0.2764952977498372 Validation Loss: 0.7564073801040649\n",
      "Epoch 2447: Training Loss: 0.2761957397063573 Validation Loss: 0.7585945129394531\n",
      "Epoch 2448: Training Loss: 0.27608036001523334 Validation Loss: 0.7575018405914307\n",
      "Epoch 2449: Training Loss: 0.27583425243695575 Validation Loss: 0.7578540444374084\n",
      "Epoch 2450: Training Loss: 0.27588963508605957 Validation Loss: 0.7562076449394226\n",
      "Epoch 2451: Training Loss: 0.27547942598660785 Validation Loss: 0.7572296857833862\n",
      "Epoch 2452: Training Loss: 0.2755623459815979 Validation Loss: 0.7546632289886475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2453: Training Loss: 0.2750603258609772 Validation Loss: 0.7559860348701477\n",
      "Epoch 2454: Training Loss: 0.27486379941304523 Validation Loss: 0.7581060528755188\n",
      "Epoch 2455: Training Loss: 0.2748284538586934 Validation Loss: 0.7578117251396179\n",
      "Epoch 2456: Training Loss: 0.2746021995941798 Validation Loss: 0.7582955956459045\n",
      "Epoch 2457: Training Loss: 0.27459510167439777 Validation Loss: 0.7586117386817932\n",
      "Epoch 2458: Training Loss: 0.2743253906567891 Validation Loss: 0.7575560808181763\n",
      "Epoch 2459: Training Loss: 0.27415217955907184 Validation Loss: 0.753656804561615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2460: Training Loss: 0.2740350862344106 Validation Loss: 0.7545658349990845\n",
      "Epoch 2461: Training Loss: 0.2738000154495239 Validation Loss: 0.7549924850463867\n",
      "Epoch 2462: Training Loss: 0.2736329833666484 Validation Loss: 0.7542851567268372\n",
      "Epoch 2463: Training Loss: 0.27333571513493854 Validation Loss: 0.7555739283561707\n",
      "Epoch 2464: Training Loss: 0.2731357713540395 Validation Loss: 0.7572086453437805\n",
      "Epoch 2465: Training Loss: 0.27313244342803955 Validation Loss: 0.7587571740150452\n",
      "Epoch 2466: Training Loss: 0.27300699551900226 Validation Loss: 0.7574838995933533\n",
      "Epoch 2467: Training Loss: 0.27268314361572266 Validation Loss: 0.7542451024055481\n",
      "Epoch 2468: Training Loss: 0.2725590765476227 Validation Loss: 0.7531603574752808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2469: Training Loss: 0.2724391023317973 Validation Loss: 0.7530158758163452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2470: Training Loss: 0.27224602301915485 Validation Loss: 0.7570174336433411\n",
      "Epoch 2471: Training Loss: 0.27206886808077496 Validation Loss: 0.7592455148696899\n",
      "Epoch 2472: Training Loss: 0.2720390160878499 Validation Loss: 0.7557767033576965\n",
      "Epoch 2473: Training Loss: 0.2717195947964986 Validation Loss: 0.7542515397071838\n",
      "Epoch 2474: Training Loss: 0.27151880661646527 Validation Loss: 0.7535667419433594\n",
      "Epoch 2475: Training Loss: 0.2713061024745305 Validation Loss: 0.7546952962875366\n",
      "Epoch 2476: Training Loss: 0.27139506737391156 Validation Loss: 0.7588176727294922\n",
      "Epoch 2477: Training Loss: 0.27109062174956006 Validation Loss: 0.7583481073379517\n",
      "Epoch 2478: Training Loss: 0.27079933881759644 Validation Loss: 0.7545545101165771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2479: Training Loss: 0.2705788215001424 Validation Loss: 0.7527778744697571\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2480: Training Loss: 0.27052992582321167 Validation Loss: 0.752685010433197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2481: Training Loss: 0.2703568935394287 Validation Loss: 0.7514417171478271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2482: Training Loss: 0.27012621363004047 Validation Loss: 0.7527239322662354\n",
      "Epoch 2483: Training Loss: 0.27007607122262317 Validation Loss: 0.7531833052635193\n",
      "Epoch 2484: Training Loss: 0.2699911892414093 Validation Loss: 0.757462739944458\n",
      "Epoch 2485: Training Loss: 0.2698550472656886 Validation Loss: 0.759281575679779\n",
      "Epoch 2486: Training Loss: 0.2697644035021464 Validation Loss: 0.7538872361183167\n",
      "Epoch 2487: Training Loss: 0.26945404211680096 Validation Loss: 0.754936933517456\n",
      "Epoch 2488: Training Loss: 0.2690403362115224 Validation Loss: 0.753156840801239\n",
      "Epoch 2489: Training Loss: 0.2689356207847595 Validation Loss: 0.7524453401565552\n",
      "Epoch 2490: Training Loss: 0.26886703570683795 Validation Loss: 0.754509687423706\n",
      "Epoch 2491: Training Loss: 0.2685232361157735 Validation Loss: 0.7551454901695251\n",
      "Epoch 2492: Training Loss: 0.268354410926501 Validation Loss: 0.755664050579071\n",
      "Epoch 2493: Training Loss: 0.2681967616081238 Validation Loss: 0.7539244890213013\n",
      "Epoch 2494: Training Loss: 0.26797791322072345 Validation Loss: 0.7530861496925354\n",
      "Epoch 2495: Training Loss: 0.26781125863393146 Validation Loss: 0.7526403665542603\n",
      "Epoch 2496: Training Loss: 0.2677559306224187 Validation Loss: 0.7506787776947021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2497: Training Loss: 0.2679904152949651 Validation Loss: 0.7543495297431946\n",
      "Epoch 2498: Training Loss: 0.2673925906419754 Validation Loss: 0.7550321221351624\n",
      "Epoch 2499: Training Loss: 0.267197718222936 Validation Loss: 0.7527070045471191\n",
      "Epoch 2500: Training Loss: 0.2670057713985443 Validation Loss: 0.7516345381736755\n",
      "Epoch 2501: Training Loss: 0.2670404314994812 Validation Loss: 0.7555763721466064\n",
      "Epoch 2502: Training Loss: 0.2666548738876979 Validation Loss: 0.7547744512557983\n",
      "Epoch 2503: Training Loss: 0.26650673151016235 Validation Loss: 0.7544593811035156\n",
      "Epoch 2504: Training Loss: 0.26638586322466534 Validation Loss: 0.7548947930335999\n",
      "Epoch 2505: Training Loss: 0.26610541343688965 Validation Loss: 0.7522635459899902\n",
      "Epoch 2506: Training Loss: 0.2660653392473857 Validation Loss: 0.7505047917366028\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2507: Training Loss: 0.26605041821797687 Validation Loss: 0.7534784078598022\n",
      "Epoch 2508: Training Loss: 0.26568666100502014 Validation Loss: 0.7519683241844177\n",
      "Epoch 2509: Training Loss: 0.26557735602060956 Validation Loss: 0.7533284425735474\n",
      "Epoch 2510: Training Loss: 0.26544607679049176 Validation Loss: 0.7522779703140259\n",
      "Epoch 2511: Training Loss: 0.2652041216691335 Validation Loss: 0.7526818513870239\n",
      "Epoch 2512: Training Loss: 0.26512787739435834 Validation Loss: 0.7537756562232971\n",
      "Epoch 2513: Training Loss: 0.2649634877840678 Validation Loss: 0.7512954473495483\n",
      "Epoch 2514: Training Loss: 0.2647201716899872 Validation Loss: 0.7498098611831665\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2515: Training Loss: 0.26466014484564465 Validation Loss: 0.7521310448646545\n",
      "Epoch 2516: Training Loss: 0.2643980135520299 Validation Loss: 0.7513507008552551\n",
      "Epoch 2517: Training Loss: 0.2644536594549815 Validation Loss: 0.7555025219917297\n",
      "Epoch 2518: Training Loss: 0.264180322488149 Validation Loss: 0.7530941963195801\n",
      "Epoch 2519: Training Loss: 0.2638801137606303 Validation Loss: 0.7524845600128174\n",
      "Epoch 2520: Training Loss: 0.26366710166136426 Validation Loss: 0.7525655627250671\n",
      "Epoch 2521: Training Loss: 0.26346854865550995 Validation Loss: 0.7523317337036133\n",
      "Epoch 2522: Training Loss: 0.2635491391023 Validation Loss: 0.7527278065681458\n",
      "Epoch 2523: Training Loss: 0.2632249742746353 Validation Loss: 0.7504491806030273\n",
      "Epoch 2524: Training Loss: 0.26300135254859924 Validation Loss: 0.7508456110954285\n",
      "Epoch 2525: Training Loss: 0.2628854811191559 Validation Loss: 0.7512349486351013\n",
      "Epoch 2526: Training Loss: 0.26274851461251575 Validation Loss: 0.750650942325592\n",
      "Epoch 2527: Training Loss: 0.2625741958618164 Validation Loss: 0.753102719783783\n",
      "Epoch 2528: Training Loss: 0.26242325206597644 Validation Loss: 0.7520542144775391\n",
      "Epoch 2529: Training Loss: 0.262259304523468 Validation Loss: 0.753299355506897\n",
      "Epoch 2530: Training Loss: 0.26201923688252765 Validation Loss: 0.7524664402008057\n",
      "Epoch 2531: Training Loss: 0.2618669966856639 Validation Loss: 0.751980721950531\n",
      "Epoch 2532: Training Loss: 0.2620466699202855 Validation Loss: 0.7487587928771973\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2533: Training Loss: 0.2616070657968521 Validation Loss: 0.7515630722045898\n",
      "Epoch 2534: Training Loss: 0.2615862488746643 Validation Loss: 0.7506890892982483\n",
      "Epoch 2535: Training Loss: 0.26143257816632587 Validation Loss: 0.7542380690574646\n",
      "Epoch 2536: Training Loss: 0.26142027974128723 Validation Loss: 0.7553998231887817\n",
      "Epoch 2537: Training Loss: 0.2609219004710515 Validation Loss: 0.7523127794265747\n",
      "Epoch 2538: Training Loss: 0.26084211468696594 Validation Loss: 0.7471581101417542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2539: Training Loss: 0.2607511878013611 Validation Loss: 0.746471107006073\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2540: Training Loss: 0.2604893098274867 Validation Loss: 0.7491170167922974\n",
      "Epoch 2541: Training Loss: 0.2603128105401993 Validation Loss: 0.7518310546875\n",
      "Epoch 2542: Training Loss: 0.26009107132752735 Validation Loss: 0.7536084055900574\n",
      "Epoch 2543: Training Loss: 0.26008766889572144 Validation Loss: 0.7511095404624939\n",
      "Epoch 2544: Training Loss: 0.2597506145636241 Validation Loss: 0.7521818280220032\n",
      "Epoch 2545: Training Loss: 0.2597074906031291 Validation Loss: 0.7494190335273743\n",
      "Epoch 2546: Training Loss: 0.2597963710625966 Validation Loss: 0.7483230233192444\n",
      "Epoch 2547: Training Loss: 0.2593321104844411 Validation Loss: 0.751143753528595\n",
      "Epoch 2548: Training Loss: 0.2593412597974141 Validation Loss: 0.7541880011558533\n",
      "Epoch 2549: Training Loss: 0.2593795458475749 Validation Loss: 0.7544446587562561\n",
      "Epoch 2550: Training Loss: 0.25899363060792285 Validation Loss: 0.7489541172981262\n",
      "Epoch 2551: Training Loss: 0.2586384614308675 Validation Loss: 0.7477515339851379\n",
      "Epoch 2552: Training Loss: 0.25864503781000775 Validation Loss: 0.7465376257896423\n",
      "Epoch 2553: Training Loss: 0.2584092915058136 Validation Loss: 0.7497707009315491\n",
      "Epoch 2554: Training Loss: 0.25844162702560425 Validation Loss: 0.7536560297012329\n",
      "Epoch 2555: Training Loss: 0.25807757178942364 Validation Loss: 0.7517382502555847\n",
      "Epoch 2556: Training Loss: 0.2578113079071045 Validation Loss: 0.7502914667129517\n",
      "Epoch 2557: Training Loss: 0.2577780981858571 Validation Loss: 0.7483341097831726\n",
      "Epoch 2558: Training Loss: 0.2576855768760045 Validation Loss: 0.7472019791603088\n",
      "Epoch 2559: Training Loss: 0.2573951880137126 Validation Loss: 0.7481407523155212\n",
      "Epoch 2560: Training Loss: 0.25722695887088776 Validation Loss: 0.7505450248718262\n",
      "Epoch 2561: Training Loss: 0.2571197599172592 Validation Loss: 0.7516992092132568\n",
      "Epoch 2562: Training Loss: 0.2568682332833608 Validation Loss: 0.7512028217315674\n",
      "Epoch 2563: Training Loss: 0.2569270581007004 Validation Loss: 0.7482561469078064\n",
      "Epoch 2564: Training Loss: 0.25674842794736225 Validation Loss: 0.7510327100753784\n",
      "Epoch 2565: Training Loss: 0.2565259436766307 Validation Loss: 0.7510117292404175\n",
      "Epoch 2566: Training Loss: 0.2563285827636719 Validation Loss: 0.7510480880737305\n",
      "Epoch 2567: Training Loss: 0.25633856654167175 Validation Loss: 0.747153639793396\n",
      "Epoch 2568: Training Loss: 0.2560850878556569 Validation Loss: 0.746364951133728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2569: Training Loss: 0.25590112805366516 Validation Loss: 0.7466235160827637\n",
      "Epoch 2570: Training Loss: 0.2557081878185272 Validation Loss: 0.7497818470001221\n",
      "Epoch 2571: Training Loss: 0.2555027057727178 Validation Loss: 0.7521907091140747\n",
      "Epoch 2572: Training Loss: 0.25545834998289746 Validation Loss: 0.752784788608551\n",
      "Epoch 2573: Training Loss: 0.25527291496594745 Validation Loss: 0.7499247193336487\n",
      "Epoch 2574: Training Loss: 0.255038524667422 Validation Loss: 0.7480998635292053\n",
      "Epoch 2575: Training Loss: 0.2549290955066681 Validation Loss: 0.7474229335784912\n",
      "Epoch 2576: Training Loss: 0.254874790708224 Validation Loss: 0.7491042613983154\n",
      "Epoch 2577: Training Loss: 0.2545636643966039 Validation Loss: 0.7491885423660278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2578: Training Loss: 0.25438327093919116 Validation Loss: 0.7485417723655701\n",
      "Epoch 2579: Training Loss: 0.2545059621334076 Validation Loss: 0.7462621927261353\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2580: Training Loss: 0.254124457637469 Validation Loss: 0.7481959462165833\n",
      "Epoch 2581: Training Loss: 0.25395066539446515 Validation Loss: 0.7509317994117737\n",
      "Epoch 2582: Training Loss: 0.2538159489631653 Validation Loss: 0.7499680519104004\n",
      "Epoch 2583: Training Loss: 0.25362425049146015 Validation Loss: 0.7487244009971619\n",
      "Epoch 2584: Training Loss: 0.25356733798980713 Validation Loss: 0.7500625848770142\n",
      "Epoch 2585: Training Loss: 0.2533120612303416 Validation Loss: 0.7498194575309753\n",
      "Epoch 2586: Training Loss: 0.25318486491839093 Validation Loss: 0.7493277192115784\n",
      "Epoch 2587: Training Loss: 0.25298065940539044 Validation Loss: 0.7463327050209045\n",
      "Epoch 2588: Training Loss: 0.2528449247280757 Validation Loss: 0.7451011538505554\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2589: Training Loss: 0.2529401034116745 Validation Loss: 0.7441859841346741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2590: Training Loss: 0.2525798479715983 Validation Loss: 0.7474308609962463\n",
      "Epoch 2591: Training Loss: 0.2526039232810338 Validation Loss: 0.7514280080795288\n",
      "Epoch 2592: Training Loss: 0.25250819822152454 Validation Loss: 0.7484718561172485\n",
      "Epoch 2593: Training Loss: 0.2520654300848643 Validation Loss: 0.7477681636810303\n",
      "Epoch 2594: Training Loss: 0.25198649366696674 Validation Loss: 0.7470222115516663\n",
      "Epoch 2595: Training Loss: 0.251818522810936 Validation Loss: 0.7488372325897217\n",
      "Epoch 2596: Training Loss: 0.25173438092072803 Validation Loss: 0.750076949596405\n",
      "Epoch 2597: Training Loss: 0.25160714983940125 Validation Loss: 0.7504962682723999\n",
      "Epoch 2598: Training Loss: 0.2514646848042806 Validation Loss: 0.7456026673316956\n",
      "Epoch 2599: Training Loss: 0.25120196243127185 Validation Loss: 0.7450746297836304\n",
      "Epoch 2600: Training Loss: 0.2510514458020528 Validation Loss: 0.7475917339324951\n",
      "Epoch 2601: Training Loss: 0.25086285670598346 Validation Loss: 0.7479901909828186\n",
      "Epoch 2602: Training Loss: 0.25074611604213715 Validation Loss: 0.7488922476768494\n",
      "Epoch 2603: Training Loss: 0.250745286544164 Validation Loss: 0.7489016652107239\n",
      "Epoch 2604: Training Loss: 0.2503627836704254 Validation Loss: 0.747215211391449\n",
      "Epoch 2605: Training Loss: 0.2502141296863556 Validation Loss: 0.7450149655342102\n",
      "Epoch 2606: Training Loss: 0.25022151072820026 Validation Loss: 0.7448230385780334\n",
      "Epoch 2607: Training Loss: 0.2501845608154933 Validation Loss: 0.7483740448951721\n",
      "Epoch 2608: Training Loss: 0.2497987002134323 Validation Loss: 0.7493229508399963\n",
      "Epoch 2609: Training Loss: 0.2500678400198619 Validation Loss: 0.7456570863723755\n",
      "Epoch 2610: Training Loss: 0.2495901882648468 Validation Loss: 0.7465616464614868\n",
      "Epoch 2611: Training Loss: 0.24933037161827087 Validation Loss: 0.746958315372467\n",
      "Epoch 2612: Training Loss: 0.24929606914520264 Validation Loss: 0.7457877993583679\n",
      "Epoch 2613: Training Loss: 0.2491025427977244 Validation Loss: 0.7470420002937317\n",
      "Epoch 2614: Training Loss: 0.24890120327472687 Validation Loss: 0.7483760118484497\n",
      "Epoch 2615: Training Loss: 0.24882535139719644 Validation Loss: 0.7487514615058899\n",
      "Epoch 2616: Training Loss: 0.24862965941429138 Validation Loss: 0.7460400462150574\n",
      "Epoch 2617: Training Loss: 0.24851718544960022 Validation Loss: 0.744133472442627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2618: Training Loss: 0.24870590368906656 Validation Loss: 0.7478229999542236\n",
      "Epoch 2619: Training Loss: 0.24814804891745249 Validation Loss: 0.747191846370697\n",
      "Epoch 2620: Training Loss: 0.24801798164844513 Validation Loss: 0.745012640953064\n",
      "Epoch 2621: Training Loss: 0.2479947159687678 Validation Loss: 0.7450393438339233\n",
      "Epoch 2622: Training Loss: 0.247792919476827 Validation Loss: 0.7450554370880127\n",
      "Epoch 2623: Training Loss: 0.247954860329628 Validation Loss: 0.7498220801353455\n",
      "Epoch 2624: Training Loss: 0.24741014341513315 Validation Loss: 0.7479702234268188\n",
      "Epoch 2625: Training Loss: 0.2472352534532547 Validation Loss: 0.7456041574478149\n",
      "Epoch 2626: Training Loss: 0.24732560416062674 Validation Loss: 0.7429628372192383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2627: Training Loss: 0.24703248341878256 Validation Loss: 0.7441301941871643\n",
      "Epoch 2628: Training Loss: 0.24679580827554068 Validation Loss: 0.7464026212692261\n",
      "Epoch 2629: Training Loss: 0.24663375318050385 Validation Loss: 0.7482087016105652\n",
      "Epoch 2630: Training Loss: 0.24673319856325784 Validation Loss: 0.7496346831321716\n",
      "Epoch 2631: Training Loss: 0.24652849634488425 Validation Loss: 0.7479119300842285\n",
      "Epoch 2632: Training Loss: 0.2461585303147634 Validation Loss: 0.7457103729248047\n",
      "Epoch 2633: Training Loss: 0.24607167144616446 Validation Loss: 0.7442433834075928\n",
      "Epoch 2634: Training Loss: 0.24610515932242075 Validation Loss: 0.7431825399398804\n",
      "Epoch 2635: Training Loss: 0.24578597644964853 Validation Loss: 0.743344783782959\n",
      "Epoch 2636: Training Loss: 0.2456029156843821 Validation Loss: 0.7445095777511597\n",
      "Epoch 2637: Training Loss: 0.245596374074618 Validation Loss: 0.7464362382888794\n",
      "Epoch 2638: Training Loss: 0.24548891683419546 Validation Loss: 0.7475846409797668\n",
      "Epoch 2639: Training Loss: 0.24537846446037292 Validation Loss: 0.7441784739494324\n",
      "Epoch 2640: Training Loss: 0.24534710745016733 Validation Loss: 0.7426260113716125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2641: Training Loss: 0.24491147696971893 Validation Loss: 0.7442517280578613\n",
      "Epoch 2642: Training Loss: 0.24485062062740326 Validation Loss: 0.7483134269714355\n",
      "Epoch 2643: Training Loss: 0.24474650621414185 Validation Loss: 0.7465000748634338\n",
      "Epoch 2644: Training Loss: 0.2444243182738622 Validation Loss: 0.7459271550178528\n",
      "Epoch 2645: Training Loss: 0.2443477213382721 Validation Loss: 0.7442947030067444\n",
      "Epoch 2646: Training Loss: 0.24425912896792093 Validation Loss: 0.7455383539199829\n",
      "Epoch 2647: Training Loss: 0.24402126173178354 Validation Loss: 0.7442820072174072\n",
      "Epoch 2648: Training Loss: 0.24395366509755453 Validation Loss: 0.7431965470314026\n",
      "Epoch 2649: Training Loss: 0.24383765955766043 Validation Loss: 0.7451436519622803\n",
      "Epoch 2650: Training Loss: 0.2435715695222219 Validation Loss: 0.7466965317726135\n",
      "Epoch 2651: Training Loss: 0.24345171451568604 Validation Loss: 0.7460534572601318\n",
      "Epoch 2652: Training Loss: 0.2433757185935974 Validation Loss: 0.7449216246604919\n",
      "Epoch 2653: Training Loss: 0.24323639770348868 Validation Loss: 0.743948221206665\n",
      "Epoch 2654: Training Loss: 0.24302352468172708 Validation Loss: 0.7438678741455078\n",
      "Epoch 2655: Training Loss: 0.24289301534493765 Validation Loss: 0.7445327639579773\n",
      "Epoch 2656: Training Loss: 0.24285211165746054 Validation Loss: 0.7444966435432434\n",
      "Epoch 2657: Training Loss: 0.2428410400946935 Validation Loss: 0.7477286458015442\n",
      "Epoch 2658: Training Loss: 0.24246269961198172 Validation Loss: 0.7451450228691101\n",
      "Epoch 2659: Training Loss: 0.2423740973075231 Validation Loss: 0.7430735230445862\n",
      "Epoch 2660: Training Loss: 0.24234565595785776 Validation Loss: 0.7438235878944397\n",
      "Epoch 2661: Training Loss: 0.24208404123783112 Validation Loss: 0.7451112866401672\n",
      "Epoch 2662: Training Loss: 0.24186290303866068 Validation Loss: 0.7448969483375549\n",
      "Epoch 2663: Training Loss: 0.24173049132029215 Validation Loss: 0.7451691627502441\n",
      "Epoch 2664: Training Loss: 0.24166077375411987 Validation Loss: 0.7423298954963684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2665: Training Loss: 0.24161784847577414 Validation Loss: 0.7444680333137512\n",
      "Epoch 2666: Training Loss: 0.2413000613451004 Validation Loss: 0.7439332008361816\n",
      "Epoch 2667: Training Loss: 0.24111155172189078 Validation Loss: 0.7435118556022644\n",
      "Epoch 2668: Training Loss: 0.24098706245422363 Validation Loss: 0.7442905306816101\n",
      "Epoch 2669: Training Loss: 0.24095834294954935 Validation Loss: 0.7442746758460999\n",
      "Epoch 2670: Training Loss: 0.2412390410900116 Validation Loss: 0.7403478622436523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2671: Training Loss: 0.24070956309636435 Validation Loss: 0.7421691417694092\n",
      "Epoch 2672: Training Loss: 0.240526815255483 Validation Loss: 0.7462966442108154\n",
      "Epoch 2673: Training Loss: 0.24037576715151468 Validation Loss: 0.7461875677108765\n",
      "Epoch 2674: Training Loss: 0.2402357409397761 Validation Loss: 0.7468464970588684\n",
      "Epoch 2675: Training Loss: 0.2402114768822988 Validation Loss: 0.743200421333313\n",
      "Epoch 2676: Training Loss: 0.23989219466845194 Validation Loss: 0.7432962656021118\n",
      "Epoch 2677: Training Loss: 0.239775483806928 Validation Loss: 0.7441855669021606\n",
      "Epoch 2678: Training Loss: 0.23990991711616516 Validation Loss: 0.7463340163230896\n",
      "Epoch 2679: Training Loss: 0.23948067426681519 Validation Loss: 0.743019700050354\n",
      "Epoch 2680: Training Loss: 0.2394286741813024 Validation Loss: 0.7425410747528076\n",
      "Epoch 2681: Training Loss: 0.2392325053612391 Validation Loss: 0.7417337894439697\n",
      "Epoch 2682: Training Loss: 0.23901102940241495 Validation Loss: 0.740645706653595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2683: Training Loss: 0.23899049560228983 Validation Loss: 0.7413358092308044\n",
      "Epoch 2684: Training Loss: 0.23882201810677847 Validation Loss: 0.742053747177124\n",
      "Epoch 2685: Training Loss: 0.23860505719979605 Validation Loss: 0.7441611289978027\n",
      "Epoch 2686: Training Loss: 0.23856395483016968 Validation Loss: 0.7464202642440796\n",
      "Epoch 2687: Training Loss: 0.23845960199832916 Validation Loss: 0.746214747428894\n",
      "Epoch 2688: Training Loss: 0.238414337237676 Validation Loss: 0.7431840896606445\n",
      "Epoch 2689: Training Loss: 0.23844218254089355 Validation Loss: 0.7387135028839111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2690: Training Loss: 0.23813064396381378 Validation Loss: 0.7407746315002441\n",
      "Epoch 2691: Training Loss: 0.2381450037161509 Validation Loss: 0.7464569211006165\n",
      "Epoch 2692: Training Loss: 0.2377529243628184 Validation Loss: 0.7465068101882935\n",
      "Epoch 2693: Training Loss: 0.23763837913672128 Validation Loss: 0.7435098886489868\n",
      "Epoch 2694: Training Loss: 0.2374169925848643 Validation Loss: 0.742130696773529\n",
      "Epoch 2695: Training Loss: 0.23725680510203043 Validation Loss: 0.7399672269821167\n",
      "Epoch 2696: Training Loss: 0.2371276319026947 Validation Loss: 0.7405843138694763\n",
      "Epoch 2697: Training Loss: 0.2371877779563268 Validation Loss: 0.7402312755584717\n",
      "Epoch 2698: Training Loss: 0.23686120907465616 Validation Loss: 0.743348240852356\n",
      "Epoch 2699: Training Loss: 0.23667762676874796 Validation Loss: 0.7448208928108215\n",
      "Epoch 2700: Training Loss: 0.23655528326829275 Validation Loss: 0.7444008588790894\n",
      "Epoch 2701: Training Loss: 0.23642778396606445 Validation Loss: 0.7437630891799927\n",
      "Epoch 2702: Training Loss: 0.2363692025343577 Validation Loss: 0.7414813041687012\n",
      "Epoch 2703: Training Loss: 0.23616341749827066 Validation Loss: 0.7402889728546143\n",
      "Epoch 2704: Training Loss: 0.2361881285905838 Validation Loss: 0.7413073182106018\n",
      "Epoch 2705: Training Loss: 0.2359130084514618 Validation Loss: 0.7416772842407227\n",
      "Epoch 2706: Training Loss: 0.23575436075528464 Validation Loss: 0.742886483669281\n",
      "Epoch 2707: Training Loss: 0.23563067118326822 Validation Loss: 0.7426586151123047\n",
      "Epoch 2708: Training Loss: 0.23544777433077493 Validation Loss: 0.74326491355896\n",
      "Epoch 2709: Training Loss: 0.23534314831097922 Validation Loss: 0.7430635094642639\n",
      "Epoch 2710: Training Loss: 0.23533561329046884 Validation Loss: 0.7433139085769653\n",
      "Epoch 2711: Training Loss: 0.23511170347531637 Validation Loss: 0.7405562400817871\n",
      "Epoch 2712: Training Loss: 0.2350259671608607 Validation Loss: 0.7420620322227478\n",
      "Epoch 2713: Training Loss: 0.23478119572003683 Validation Loss: 0.7412277460098267\n",
      "Epoch 2714: Training Loss: 0.23484262824058533 Validation Loss: 0.7388511300086975\n",
      "Epoch 2715: Training Loss: 0.23490509390830994 Validation Loss: 0.7429008483886719\n",
      "Epoch 2716: Training Loss: 0.23465976119041443 Validation Loss: 0.7413089275360107\n",
      "Epoch 2717: Training Loss: 0.23441175123055777 Validation Loss: 0.7412019371986389\n",
      "Epoch 2718: Training Loss: 0.23415903747081757 Validation Loss: 0.7440963983535767\n",
      "Epoch 2719: Training Loss: 0.23412959277629852 Validation Loss: 0.7439977526664734\n",
      "Epoch 2720: Training Loss: 0.2338944971561432 Validation Loss: 0.7432571053504944\n",
      "Epoch 2721: Training Loss: 0.23377897838751474 Validation Loss: 0.7415246963500977\n",
      "Epoch 2722: Training Loss: 0.23366311689217886 Validation Loss: 0.7388232946395874\n",
      "Epoch 2723: Training Loss: 0.2334596961736679 Validation Loss: 0.7407598495483398\n",
      "Epoch 2724: Training Loss: 0.233348419268926 Validation Loss: 0.7423632740974426\n",
      "Epoch 2725: Training Loss: 0.23314467072486877 Validation Loss: 0.7418379783630371\n",
      "Epoch 2726: Training Loss: 0.23311505218346915 Validation Loss: 0.7428701519966125\n",
      "Epoch 2727: Training Loss: 0.23292366166909537 Validation Loss: 0.7399630546569824\n",
      "Epoch 2728: Training Loss: 0.2328746517499288 Validation Loss: 0.7410610914230347\n",
      "Epoch 2729: Training Loss: 0.23268118997414908 Validation Loss: 0.7422395348548889\n",
      "Epoch 2730: Training Loss: 0.2325111279884974 Validation Loss: 0.7405737638473511\n",
      "Epoch 2731: Training Loss: 0.23240208625793457 Validation Loss: 0.7401875257492065\n",
      "Epoch 2732: Training Loss: 0.23228673140207926 Validation Loss: 0.740527868270874\n",
      "Epoch 2733: Training Loss: 0.23222704231739044 Validation Loss: 0.7405101656913757\n",
      "Epoch 2734: Training Loss: 0.23202507694562277 Validation Loss: 0.7394213676452637\n",
      "Epoch 2735: Training Loss: 0.23204135398070017 Validation Loss: 0.7385994791984558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2736: Training Loss: 0.2317521721124649 Validation Loss: 0.7417308688163757\n",
      "Epoch 2737: Training Loss: 0.2316309760014216 Validation Loss: 0.7412927746772766\n",
      "Epoch 2738: Training Loss: 0.23140078783035278 Validation Loss: 0.7432551383972168\n",
      "Epoch 2739: Training Loss: 0.23138262828191122 Validation Loss: 0.7447227835655212\n",
      "Epoch 2740: Training Loss: 0.23127434651056925 Validation Loss: 0.741817057132721\n",
      "Epoch 2741: Training Loss: 0.23144656419754028 Validation Loss: 0.7384659051895142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2742: Training Loss: 0.230984499057134 Validation Loss: 0.7404282689094543\n",
      "Epoch 2743: Training Loss: 0.23080997169017792 Validation Loss: 0.7399503588676453\n",
      "Epoch 2744: Training Loss: 0.23061807453632355 Validation Loss: 0.7403804063796997\n",
      "Epoch 2745: Training Loss: 0.23067321876684824 Validation Loss: 0.742457389831543\n",
      "Epoch 2746: Training Loss: 0.23045242329438528 Validation Loss: 0.7412776947021484\n",
      "Epoch 2747: Training Loss: 0.2302815020084381 Validation Loss: 0.7406356930732727\n",
      "Epoch 2748: Training Loss: 0.23018617431322733 Validation Loss: 0.7402215600013733\n",
      "Epoch 2749: Training Loss: 0.22993679344654083 Validation Loss: 0.7399129271507263\n",
      "Epoch 2750: Training Loss: 0.2299142132202784 Validation Loss: 0.7372133135795593\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2751: Training Loss: 0.22976558903853098 Validation Loss: 0.7395340204238892\n",
      "Epoch 2752: Training Loss: 0.22962171832720438 Validation Loss: 0.7420023083686829\n",
      "Epoch 2753: Training Loss: 0.22948230306307474 Validation Loss: 0.7421171069145203\n",
      "Epoch 2754: Training Loss: 0.22936821977297464 Validation Loss: 0.7420315742492676\n",
      "Epoch 2755: Training Loss: 0.22930282851060232 Validation Loss: 0.738655149936676\n",
      "Epoch 2756: Training Loss: 0.22912910083929697 Validation Loss: 0.738513708114624\n",
      "Epoch 2757: Training Loss: 0.2289747546116511 Validation Loss: 0.7397217750549316\n",
      "Epoch 2758: Training Loss: 0.22887372970581055 Validation Loss: 0.7407535910606384\n",
      "Epoch 2759: Training Loss: 0.2289604345957438 Validation Loss: 0.7378855347633362\n",
      "Epoch 2760: Training Loss: 0.22891601423422495 Validation Loss: 0.7423553466796875\n",
      "Epoch 2761: Training Loss: 0.2284416804711024 Validation Loss: 0.7405787110328674\n",
      "Epoch 2762: Training Loss: 0.22855807344118753 Validation Loss: 0.7416476011276245\n",
      "Epoch 2763: Training Loss: 0.2284250110387802 Validation Loss: 0.7382197380065918\n",
      "Epoch 2764: Training Loss: 0.22807838519414267 Validation Loss: 0.7376556396484375\n",
      "Epoch 2765: Training Loss: 0.22792703409989676 Validation Loss: 0.7404290437698364\n",
      "Epoch 2766: Training Loss: 0.22786724070707956 Validation Loss: 0.7422240972518921\n",
      "Epoch 2767: Training Loss: 0.22780929505825043 Validation Loss: 0.7385465502738953\n",
      "Epoch 2768: Training Loss: 0.22755571206410727 Validation Loss: 0.7385230660438538\n",
      "Epoch 2769: Training Loss: 0.22739449640115103 Validation Loss: 0.7387849688529968\n",
      "Epoch 2770: Training Loss: 0.22742926081021628 Validation Loss: 0.7371487617492676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2771: Training Loss: 0.22719542682170868 Validation Loss: 0.7406516075134277\n",
      "Epoch 2772: Training Loss: 0.22710123658180237 Validation Loss: 0.7423799633979797\n",
      "Epoch 2773: Training Loss: 0.22691519061724344 Validation Loss: 0.7406896352767944\n",
      "Epoch 2774: Training Loss: 0.22683832049369812 Validation Loss: 0.7375167012214661\n",
      "Epoch 2775: Training Loss: 0.22665157914161682 Validation Loss: 0.7373747825622559\n",
      "Epoch 2776: Training Loss: 0.22647271553675333 Validation Loss: 0.7381705641746521\n",
      "Epoch 2777: Training Loss: 0.22650983929634094 Validation Loss: 0.7417181134223938\n",
      "Epoch 2778: Training Loss: 0.2262978901465734 Validation Loss: 0.7407982349395752\n",
      "Epoch 2779: Training Loss: 0.22615617513656616 Validation Loss: 0.7402639389038086\n",
      "Epoch 2780: Training Loss: 0.22601881126562753 Validation Loss: 0.7403854131698608\n",
      "Epoch 2781: Training Loss: 0.22589637835820517 Validation Loss: 0.7388466596603394\n",
      "Epoch 2782: Training Loss: 0.22581817209720612 Validation Loss: 0.738001823425293\n",
      "Epoch 2783: Training Loss: 0.225627730290095 Validation Loss: 0.7391683459281921\n",
      "Epoch 2784: Training Loss: 0.2255363017320633 Validation Loss: 0.740290105342865\n",
      "Epoch 2785: Training Loss: 0.22562299172083536 Validation Loss: 0.7368898391723633\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2786: Training Loss: 0.22524012625217438 Validation Loss: 0.7375541925430298\n",
      "Epoch 2787: Training Loss: 0.22510854403177896 Validation Loss: 0.739317774772644\n",
      "Epoch 2788: Training Loss: 0.225001389781634 Validation Loss: 0.7377638816833496\n",
      "Epoch 2789: Training Loss: 0.2248542457818985 Validation Loss: 0.7386599183082581\n",
      "Epoch 2790: Training Loss: 0.22471493482589722 Validation Loss: 0.7382217645645142\n",
      "Epoch 2791: Training Loss: 0.22470881541570029 Validation Loss: 0.7402738332748413\n",
      "Epoch 2792: Training Loss: 0.224464217821757 Validation Loss: 0.740185558795929\n",
      "Epoch 2793: Training Loss: 0.22452318171660104 Validation Loss: 0.741445779800415\n",
      "Epoch 2794: Training Loss: 0.22437739372253418 Validation Loss: 0.7379016876220703\n",
      "Epoch 2795: Training Loss: 0.22415457665920258 Validation Loss: 0.7380285263061523\n",
      "Epoch 2796: Training Loss: 0.2240003546079 Validation Loss: 0.7378808856010437\n",
      "Epoch 2797: Training Loss: 0.22378913561503092 Validation Loss: 0.7365378737449646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2798: Training Loss: 0.22378206253051758 Validation Loss: 0.7371285557746887\n",
      "Epoch 2799: Training Loss: 0.2236788272857666 Validation Loss: 0.7384749054908752\n",
      "Epoch 2800: Training Loss: 0.2235002468029658 Validation Loss: 0.7389861345291138\n",
      "Epoch 2801: Training Loss: 0.22338803112506866 Validation Loss: 0.7373595833778381\n",
      "Epoch 2802: Training Loss: 0.2233864814043045 Validation Loss: 0.7405738830566406\n",
      "Epoch 2803: Training Loss: 0.2232533891995748 Validation Loss: 0.7397845983505249\n",
      "Epoch 2804: Training Loss: 0.22298666338125864 Validation Loss: 0.7376387119293213\n",
      "Epoch 2805: Training Loss: 0.22295304636160532 Validation Loss: 0.7351922988891602\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2806: Training Loss: 0.2228500892718633 Validation Loss: 0.7369154095649719\n",
      "Epoch 2807: Training Loss: 0.22270357112089792 Validation Loss: 0.7396001219749451\n",
      "Epoch 2808: Training Loss: 0.22252660989761353 Validation Loss: 0.7402437925338745\n",
      "Epoch 2809: Training Loss: 0.2223840852578481 Validation Loss: 0.7391000986099243\n",
      "Epoch 2810: Training Loss: 0.22223426898320517 Validation Loss: 0.7360833287239075\n",
      "Epoch 2811: Training Loss: 0.2221629520257314 Validation Loss: 0.7345978617668152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2812: Training Loss: 0.2221936732530594 Validation Loss: 0.7354210019111633\n",
      "Epoch 2813: Training Loss: 0.2219793200492859 Validation Loss: 0.7394300699234009\n",
      "Epoch 2814: Training Loss: 0.2218625247478485 Validation Loss: 0.7419766187667847\n",
      "Epoch 2815: Training Loss: 0.22167193392912546 Validation Loss: 0.7394017577171326\n",
      "Epoch 2816: Training Loss: 0.22163276871045431 Validation Loss: 0.7360841631889343\n",
      "Epoch 2817: Training Loss: 0.22148412962754568 Validation Loss: 0.7374757528305054\n",
      "Epoch 2818: Training Loss: 0.2213038057088852 Validation Loss: 0.7398186922073364\n",
      "Epoch 2819: Training Loss: 0.22113045553366342 Validation Loss: 0.7383981347084045\n",
      "Epoch 2820: Training Loss: 0.22119930883248648 Validation Loss: 0.7374869585037231\n",
      "Epoch 2821: Training Loss: 0.22094148894151053 Validation Loss: 0.7367926836013794\n",
      "Epoch 2822: Training Loss: 0.2207572211821874 Validation Loss: 0.737467348575592\n",
      "Epoch 2823: Training Loss: 0.22062679628531137 Validation Loss: 0.7369154095649719\n",
      "Epoch 2824: Training Loss: 0.22050135334332785 Validation Loss: 0.7369393110275269\n",
      "Epoch 2825: Training Loss: 0.22050687670707703 Validation Loss: 0.7347017526626587\n",
      "Epoch 2826: Training Loss: 0.22032479445139566 Validation Loss: 0.7378448843955994\n",
      "Epoch 2827: Training Loss: 0.22026657064755759 Validation Loss: 0.736923098564148\n",
      "Epoch 2828: Training Loss: 0.2199989308913549 Validation Loss: 0.7385696172714233\n",
      "Epoch 2829: Training Loss: 0.22000306844711304 Validation Loss: 0.7413520216941833\n",
      "Epoch 2830: Training Loss: 0.21986513833204904 Validation Loss: 0.7399088144302368\n",
      "Epoch 2831: Training Loss: 0.21992699801921844 Validation Loss: 0.7400715351104736\n",
      "Epoch 2832: Training Loss: 0.21953856945037842 Validation Loss: 0.7366809844970703\n",
      "Epoch 2833: Training Loss: 0.21959890921910605 Validation Loss: 0.7321262955665588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2834: Training Loss: 0.21946154534816742 Validation Loss: 0.7320516705513\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2835: Training Loss: 0.2192221979300181 Validation Loss: 0.7368643283843994\n",
      "Epoch 2836: Training Loss: 0.21902181208133698 Validation Loss: 0.7395966649055481\n",
      "Epoch 2837: Training Loss: 0.2189388871192932 Validation Loss: 0.7403088212013245\n",
      "Epoch 2838: Training Loss: 0.2188931703567505 Validation Loss: 0.7390592098236084\n",
      "Epoch 2839: Training Loss: 0.21879209081331888 Validation Loss: 0.7368965148925781\n",
      "Epoch 2840: Training Loss: 0.21856258809566498 Validation Loss: 0.7359430193901062\n",
      "Epoch 2841: Training Loss: 0.2184741348028183 Validation Loss: 0.7368728518486023\n",
      "Epoch 2842: Training Loss: 0.21854900320370993 Validation Loss: 0.7352613210678101\n",
      "Epoch 2843: Training Loss: 0.2182079404592514 Validation Loss: 0.7361508011817932\n",
      "Epoch 2844: Training Loss: 0.2180612583955129 Validation Loss: 0.7388516664505005\n",
      "Epoch 2845: Training Loss: 0.21806670228640238 Validation Loss: 0.7380499243736267\n",
      "Epoch 2846: Training Loss: 0.2179313749074936 Validation Loss: 0.7373164296150208\n",
      "Epoch 2847: Training Loss: 0.21778578062852225 Validation Loss: 0.7360694408416748\n",
      "Epoch 2848: Training Loss: 0.2176014930009842 Validation Loss: 0.7365691065788269\n",
      "Epoch 2849: Training Loss: 0.21761734783649445 Validation Loss: 0.7394066452980042\n",
      "Epoch 2850: Training Loss: 0.21738098561763763 Validation Loss: 0.7378981113433838\n",
      "Epoch 2851: Training Loss: 0.21742957333723703 Validation Loss: 0.7339503765106201\n",
      "Epoch 2852: Training Loss: 0.21741012235482535 Validation Loss: 0.7367579936981201\n",
      "Epoch 2853: Training Loss: 0.21701335906982422 Validation Loss: 0.7354304790496826\n",
      "Epoch 2854: Training Loss: 0.21688213447729746 Validation Loss: 0.7355362176895142\n",
      "Epoch 2855: Training Loss: 0.21686101456483206 Validation Loss: 0.737210214138031\n",
      "Epoch 2856: Training Loss: 0.21679969131946564 Validation Loss: 0.7379789352416992\n",
      "Epoch 2857: Training Loss: 0.21663006146748862 Validation Loss: 0.737045168876648\n",
      "Epoch 2858: Training Loss: 0.2164580225944519 Validation Loss: 0.7346107959747314\n",
      "Epoch 2859: Training Loss: 0.2163506249586741 Validation Loss: 0.7359323501586914\n",
      "Epoch 2860: Training Loss: 0.21634258329868317 Validation Loss: 0.7369774580001831\n",
      "Epoch 2861: Training Loss: 0.21608814100424448 Validation Loss: 0.7374892830848694\n",
      "Epoch 2862: Training Loss: 0.21609005331993103 Validation Loss: 0.7355586290359497\n",
      "Epoch 2863: Training Loss: 0.21589215099811554 Validation Loss: 0.7368812561035156\n",
      "Epoch 2864: Training Loss: 0.21582004924615225 Validation Loss: 0.7374070882797241\n",
      "Epoch 2865: Training Loss: 0.21562844018141428 Validation Loss: 0.7367867827415466\n",
      "Epoch 2866: Training Loss: 0.2154932220776876 Validation Loss: 0.7350670099258423\n",
      "Epoch 2867: Training Loss: 0.21547863880793253 Validation Loss: 0.7331504821777344\n",
      "Epoch 2868: Training Loss: 0.2153032124042511 Validation Loss: 0.7337744832038879\n",
      "Epoch 2869: Training Loss: 0.21512515346209207 Validation Loss: 0.7354899644851685\n",
      "Epoch 2870: Training Loss: 0.2153101513783137 Validation Loss: 0.7395238876342773\n",
      "Epoch 2871: Training Loss: 0.21498743196328482 Validation Loss: 0.738852858543396\n",
      "Epoch 2872: Training Loss: 0.2148295889298121 Validation Loss: 0.7383743524551392\n",
      "Epoch 2873: Training Loss: 0.21465653677781424 Validation Loss: 0.7359256148338318\n",
      "Epoch 2874: Training Loss: 0.21459915240605673 Validation Loss: 0.732154369354248\n",
      "Epoch 2875: Training Loss: 0.21452565987904867 Validation Loss: 0.7316471934318542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2876: Training Loss: 0.21445241073767343 Validation Loss: 0.733231246471405\n",
      "Epoch 2877: Training Loss: 0.2141714096069336 Validation Loss: 0.7364238500595093\n",
      "Epoch 2878: Training Loss: 0.21398756404717764 Validation Loss: 0.7387643456459045\n",
      "Epoch 2879: Training Loss: 0.21421162287394205 Validation Loss: 0.7412387132644653\n",
      "Epoch 2880: Training Loss: 0.2140077998240789 Validation Loss: 0.7380008697509766\n",
      "Epoch 2881: Training Loss: 0.21374214192231497 Validation Loss: 0.7359044551849365\n",
      "Epoch 2882: Training Loss: 0.21367555359999338 Validation Loss: 0.7329904437065125\n",
      "Epoch 2883: Training Loss: 0.21351118385791779 Validation Loss: 0.7329007983207703\n",
      "Epoch 2884: Training Loss: 0.2135417362054189 Validation Loss: 0.7345190644264221\n",
      "Epoch 2885: Training Loss: 0.21328939497470856 Validation Loss: 0.7348573207855225\n",
      "Epoch 2886: Training Loss: 0.2132082680861155 Validation Loss: 0.7362536191940308\n",
      "Epoch 2887: Training Loss: 0.21313648422559103 Validation Loss: 0.7342981100082397\n",
      "Epoch 2888: Training Loss: 0.21309176087379456 Validation Loss: 0.7338690757751465\n",
      "Epoch 2889: Training Loss: 0.21284128725528717 Validation Loss: 0.7360531687736511\n",
      "Epoch 2890: Training Loss: 0.2127405901749929 Validation Loss: 0.7348880171775818\n",
      "Epoch 2891: Training Loss: 0.21279186010360718 Validation Loss: 0.7380080819129944\n",
      "Epoch 2892: Training Loss: 0.2125721275806427 Validation Loss: 0.7358689308166504\n",
      "Epoch 2893: Training Loss: 0.21254253884156546 Validation Loss: 0.7346603870391846\n",
      "Epoch 2894: Training Loss: 0.21231930951277414 Validation Loss: 0.7362604737281799\n",
      "Epoch 2895: Training Loss: 0.21220052738984427 Validation Loss: 0.7374973893165588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2896: Training Loss: 0.21209492286046347 Validation Loss: 0.7359879016876221\n",
      "Epoch 2897: Training Loss: 0.21184411644935608 Validation Loss: 0.7339982390403748\n",
      "Epoch 2898: Training Loss: 0.21174962321917215 Validation Loss: 0.7330446243286133\n",
      "Epoch 2899: Training Loss: 0.21175124247868857 Validation Loss: 0.7336985468864441\n",
      "Epoch 2900: Training Loss: 0.2116037756204605 Validation Loss: 0.7324702143669128\n",
      "Epoch 2901: Training Loss: 0.21150020758310953 Validation Loss: 0.7352412939071655\n",
      "Epoch 2902: Training Loss: 0.21132901310920715 Validation Loss: 0.7350587248802185\n",
      "Epoch 2903: Training Loss: 0.21154441436131796 Validation Loss: 0.7368243336677551\n",
      "Epoch 2904: Training Loss: 0.21115545431772867 Validation Loss: 0.7343688011169434\n",
      "Epoch 2905: Training Loss: 0.21099155644575754 Validation Loss: 0.73353511095047\n",
      "Epoch 2906: Training Loss: 0.21093580623467764 Validation Loss: 0.7329373359680176\n",
      "Epoch 2907: Training Loss: 0.21115060150623322 Validation Loss: 0.7318430542945862\n",
      "Epoch 2908: Training Loss: 0.21090171734491983 Validation Loss: 0.7364501953125\n",
      "Epoch 2909: Training Loss: 0.2105784366528193 Validation Loss: 0.7371240854263306\n",
      "Epoch 2910: Training Loss: 0.21051089962323508 Validation Loss: 0.7369633913040161\n",
      "Epoch 2911: Training Loss: 0.21039240062236786 Validation Loss: 0.7348507642745972\n",
      "Epoch 2912: Training Loss: 0.2102072536945343 Validation Loss: 0.7337632179260254\n",
      "Epoch 2913: Training Loss: 0.21010054151217142 Validation Loss: 0.7338846325874329\n",
      "Epoch 2914: Training Loss: 0.21000096201896667 Validation Loss: 0.7337402105331421\n",
      "Epoch 2915: Training Loss: 0.20993567009766897 Validation Loss: 0.7345243096351624\n",
      "Epoch 2916: Training Loss: 0.20980549355347952 Validation Loss: 0.7346771955490112\n",
      "Epoch 2917: Training Loss: 0.20978506902853647 Validation Loss: 0.7344911694526672\n",
      "Epoch 2918: Training Loss: 0.20956607162952423 Validation Loss: 0.7356868982315063\n",
      "Epoch 2919: Training Loss: 0.2094707489013672 Validation Loss: 0.7350895404815674\n",
      "Epoch 2920: Training Loss: 0.20938714345296225 Validation Loss: 0.7355837225914001\n",
      "Epoch 2921: Training Loss: 0.20925959448019663 Validation Loss: 0.7338401675224304\n",
      "Epoch 2922: Training Loss: 0.20925249656041464 Validation Loss: 0.7324740290641785\n",
      "Epoch 2923: Training Loss: 0.20900260408719382 Validation Loss: 0.7342001795768738\n",
      "Epoch 2924: Training Loss: 0.20888623098532358 Validation Loss: 0.7347350716590881\n",
      "Epoch 2925: Training Loss: 0.2089868833621343 Validation Loss: 0.7369294166564941\n",
      "Epoch 2926: Training Loss: 0.20875675479571024 Validation Loss: 0.7362052202224731\n",
      "Epoch 2927: Training Loss: 0.2086117962996165 Validation Loss: 0.7350646257400513\n",
      "Epoch 2928: Training Loss: 0.20857405165831247 Validation Loss: 0.732418954372406\n",
      "Epoch 2929: Training Loss: 0.20845008393128714 Validation Loss: 0.7342283129692078\n",
      "Epoch 2930: Training Loss: 0.20839320123195648 Validation Loss: 0.731890082359314\n",
      "Epoch 2931: Training Loss: 0.20815087854862213 Validation Loss: 0.7333584427833557\n",
      "Epoch 2932: Training Loss: 0.20813759167989096 Validation Loss: 0.7354329824447632\n",
      "Epoch 2933: Training Loss: 0.2078982392946879 Validation Loss: 0.7344400882720947\n",
      "Epoch 2934: Training Loss: 0.20775690178076425 Validation Loss: 0.7324762344360352\n",
      "Epoch 2935: Training Loss: 0.2076956033706665 Validation Loss: 0.7334940433502197\n",
      "Epoch 2936: Training Loss: 0.20756465196609497 Validation Loss: 0.7346288561820984\n",
      "Epoch 2937: Training Loss: 0.2074871559937795 Validation Loss: 0.7339620590209961\n",
      "Epoch 2938: Training Loss: 0.2074215660492579 Validation Loss: 0.7337680459022522\n",
      "Epoch 2939: Training Loss: 0.20733319222927094 Validation Loss: 0.7361211776733398\n",
      "Epoch 2940: Training Loss: 0.20710209012031555 Validation Loss: 0.734481155872345\n",
      "Epoch 2941: Training Loss: 0.20703652004400888 Validation Loss: 0.7335694432258606\n",
      "Epoch 2942: Training Loss: 0.20701136688391367 Validation Loss: 0.7303920984268188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2943: Training Loss: 0.20681300262610117 Validation Loss: 0.7321789264678955\n",
      "Epoch 2944: Training Loss: 0.20670773088932037 Validation Loss: 0.7327329516410828\n",
      "Epoch 2945: Training Loss: 0.20666358371575674 Validation Loss: 0.7341878414154053\n",
      "Epoch 2946: Training Loss: 0.2068935583035151 Validation Loss: 0.7384946942329407\n",
      "Epoch 2947: Training Loss: 0.20645479361216226 Validation Loss: 0.7360627055168152\n",
      "Epoch 2948: Training Loss: 0.20647298296292624 Validation Loss: 0.7312953472137451\n",
      "Epoch 2949: Training Loss: 0.20620194574197134 Validation Loss: 0.732761800289154\n",
      "Epoch 2950: Training Loss: 0.2060132473707199 Validation Loss: 0.7334188222885132\n",
      "Epoch 2951: Training Loss: 0.20613626142342886 Validation Loss: 0.7341241240501404\n",
      "Epoch 2952: Training Loss: 0.20589945216973624 Validation Loss: 0.7310004830360413\n",
      "Epoch 2953: Training Loss: 0.20573254923025766 Validation Loss: 0.7311912178993225\n",
      "Epoch 2954: Training Loss: 0.20575782656669617 Validation Loss: 0.7343124151229858\n",
      "Epoch 2955: Training Loss: 0.20559427638848624 Validation Loss: 0.735713005065918\n",
      "Epoch 2956: Training Loss: 0.20538479089736938 Validation Loss: 0.7338147759437561\n",
      "Epoch 2957: Training Loss: 0.2053048014640808 Validation Loss: 0.7321648597717285\n",
      "Epoch 2958: Training Loss: 0.2052037020524343 Validation Loss: 0.7315599322319031\n",
      "Epoch 2959: Training Loss: 0.20516575376192728 Validation Loss: 0.7346401214599609\n",
      "Epoch 2960: Training Loss: 0.2050623744726181 Validation Loss: 0.734878420829773\n",
      "Epoch 2961: Training Loss: 0.2048360308011373 Validation Loss: 0.733954131603241\n",
      "Epoch 2962: Training Loss: 0.20469972491264343 Validation Loss: 0.7323899269104004\n",
      "Epoch 2963: Training Loss: 0.20469256738821665 Validation Loss: 0.7323125600814819\n",
      "Epoch 2964: Training Loss: 0.20453091462453207 Validation Loss: 0.7305057644844055\n",
      "Epoch 2965: Training Loss: 0.20453760027885437 Validation Loss: 0.7322552800178528\n",
      "Epoch 2966: Training Loss: 0.20456285774707794 Validation Loss: 0.7349629998207092\n",
      "Epoch 2967: Training Loss: 0.20428191622098288 Validation Loss: 0.7353578805923462\n",
      "Epoch 2968: Training Loss: 0.2041710764169693 Validation Loss: 0.7331713438034058\n",
      "Epoch 2969: Training Loss: 0.2039364626010259 Validation Loss: 0.7314099669456482\n",
      "Epoch 2970: Training Loss: 0.2043372243642807 Validation Loss: 0.7281858325004578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2971: Training Loss: 0.2039447178443273 Validation Loss: 0.7291232943534851\n",
      "Epoch 2972: Training Loss: 0.20382014413674673 Validation Loss: 0.7337250113487244\n",
      "Epoch 2973: Training Loss: 0.2036406397819519 Validation Loss: 0.7355421781539917\n",
      "Epoch 2974: Training Loss: 0.20380311210950217 Validation Loss: 0.7377356290817261\n",
      "Epoch 2975: Training Loss: 0.2034050722916921 Validation Loss: 0.7347061634063721\n",
      "Epoch 2976: Training Loss: 0.20320862034956613 Validation Loss: 0.7314159870147705\n",
      "Epoch 2977: Training Loss: 0.20338181654612222 Validation Loss: 0.7295780777931213\n",
      "Epoch 2978: Training Loss: 0.20345242321491241 Validation Loss: 0.72788006067276\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2979: Training Loss: 0.2031366229057312 Validation Loss: 0.7335628867149353\n",
      "Epoch 2980: Training Loss: 0.20292878150939941 Validation Loss: 0.7356883883476257\n",
      "Epoch 2981: Training Loss: 0.20307762424151102 Validation Loss: 0.7372801303863525\n",
      "Epoch 2982: Training Loss: 0.20271466175715128 Validation Loss: 0.7338964343070984\n",
      "Epoch 2983: Training Loss: 0.20251747965812683 Validation Loss: 0.7294727563858032\n",
      "Epoch 2984: Training Loss: 0.20268213748931885 Validation Loss: 0.7274484038352966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2985: Training Loss: 0.2025193323691686 Validation Loss: 0.7290920615196228\n",
      "Epoch 2986: Training Loss: 0.20239696403344473 Validation Loss: 0.7343593239784241\n",
      "Epoch 2987: Training Loss: 0.20211229721705118 Validation Loss: 0.7352640628814697\n",
      "Epoch 2988: Training Loss: 0.20208758612473807 Validation Loss: 0.7365950345993042\n",
      "Epoch 2989: Training Loss: 0.2020030419031779 Validation Loss: 0.735292911529541\n",
      "Epoch 2990: Training Loss: 0.20182797809441885 Validation Loss: 0.7318016290664673\n",
      "Epoch 2991: Training Loss: 0.20214322209358215 Validation Loss: 0.7279648184776306\n",
      "Epoch 2992: Training Loss: 0.20160870750745138 Validation Loss: 0.7310603857040405\n",
      "Epoch 2993: Training Loss: 0.20150648057460785 Validation Loss: 0.734674870967865\n",
      "Epoch 2994: Training Loss: 0.2014588862657547 Validation Loss: 0.7334152460098267\n",
      "Epoch 2995: Training Loss: 0.2014740208784739 Validation Loss: 0.7315853834152222\n",
      "Epoch 2996: Training Loss: 0.20113680263360342 Validation Loss: 0.7342284917831421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2997: Training Loss: 0.2010663002729416 Validation Loss: 0.7342386245727539\n",
      "Epoch 2998: Training Loss: 0.20104284087816873 Validation Loss: 0.7352961301803589\n",
      "Epoch 2999: Training Loss: 0.20105023682117462 Validation Loss: 0.7312421202659607\n",
      "Epoch 3000: Training Loss: 0.20084692537784576 Validation Loss: 0.7302793264389038\n",
      "Epoch 3001: Training Loss: 0.2007294495900472 Validation Loss: 0.731911838054657\n",
      "Epoch 3002: Training Loss: 0.2006746381521225 Validation Loss: 0.7342194318771362\n",
      "Epoch 3003: Training Loss: 0.20049130419890085 Validation Loss: 0.733995795249939\n",
      "Epoch 3004: Training Loss: 0.2003675252199173 Validation Loss: 0.7318450808525085\n",
      "Epoch 3005: Training Loss: 0.20023125410079956 Validation Loss: 0.7305748462677002\n",
      "Epoch 3006: Training Loss: 0.2006242722272873 Validation Loss: 0.7324998378753662\n",
      "Epoch 3007: Training Loss: 0.20004436870416006 Validation Loss: 0.7314561009407043\n",
      "Epoch 3008: Training Loss: 0.1999151756366094 Validation Loss: 0.7310377955436707\n",
      "Epoch 3009: Training Loss: 0.20005536079406738 Validation Loss: 0.7286955714225769\n",
      "Epoch 3010: Training Loss: 0.19979552924633026 Validation Loss: 0.7313456535339355\n",
      "Epoch 3011: Training Loss: 0.19978934526443481 Validation Loss: 0.7341901063919067\n",
      "Epoch 3012: Training Loss: 0.19971982141335806 Validation Loss: 0.7354494333267212\n",
      "Epoch 3013: Training Loss: 0.19939546287059784 Validation Loss: 0.7326143383979797\n",
      "Epoch 3014: Training Loss: 0.19924658040205637 Validation Loss: 0.7305591702461243\n",
      "Epoch 3015: Training Loss: 0.1992236723502477 Validation Loss: 0.7304169535636902\n",
      "Epoch 3016: Training Loss: 0.19908701380093893 Validation Loss: 0.729846179485321\n",
      "Epoch 3017: Training Loss: 0.19919544955094656 Validation Loss: 0.7327656745910645\n",
      "Epoch 3018: Training Loss: 0.19904832045237222 Validation Loss: 0.7310197353363037\n",
      "Epoch 3019: Training Loss: 0.19882212579250336 Validation Loss: 0.7310767769813538\n",
      "Epoch 3020: Training Loss: 0.1986977607011795 Validation Loss: 0.7309439182281494\n",
      "Epoch 3021: Training Loss: 0.1985986828804016 Validation Loss: 0.7316471338272095\n",
      "Epoch 3022: Training Loss: 0.19850479066371918 Validation Loss: 0.7307314872741699\n",
      "Epoch 3023: Training Loss: 0.1985308180252711 Validation Loss: 0.7333850860595703\n",
      "Epoch 3024: Training Loss: 0.19832918047904968 Validation Loss: 0.732246994972229\n",
      "Epoch 3025: Training Loss: 0.19824554026126862 Validation Loss: 0.731559693813324\n",
      "Epoch 3026: Training Loss: 0.19820141792297363 Validation Loss: 0.7301504015922546\n",
      "Epoch 3027: Training Loss: 0.1980243076880773 Validation Loss: 0.7317919135093689\n",
      "Epoch 3028: Training Loss: 0.19801530738671622 Validation Loss: 0.7306095957756042\n",
      "Epoch 3029: Training Loss: 0.19788531959056854 Validation Loss: 0.7326434850692749\n",
      "Epoch 3030: Training Loss: 0.19775333007176718 Validation Loss: 0.7310724854469299\n",
      "Epoch 3031: Training Loss: 0.19764128824075064 Validation Loss: 0.7323538064956665\n",
      "Epoch 3032: Training Loss: 0.19752322137355804 Validation Loss: 0.7304911613464355\n",
      "Epoch 3033: Training Loss: 0.19743305444717407 Validation Loss: 0.7309374809265137\n",
      "Epoch 3034: Training Loss: 0.19726195434729257 Validation Loss: 0.7309593558311462\n",
      "Epoch 3035: Training Loss: 0.19731317460536957 Validation Loss: 0.730530858039856\n",
      "Epoch 3036: Training Loss: 0.1970688800017039 Validation Loss: 0.7316749691963196\n",
      "Epoch 3037: Training Loss: 0.19699675341447195 Validation Loss: 0.73407381772995\n",
      "Epoch 3038: Training Loss: 0.19689189394315085 Validation Loss: 0.7345451712608337\n",
      "Epoch 3039: Training Loss: 0.19682512680689493 Validation Loss: 0.733945906162262\n",
      "Epoch 3040: Training Loss: 0.1970991442600886 Validation Loss: 0.7277752757072449\n",
      "Epoch 3041: Training Loss: 0.19675292571385702 Validation Loss: 0.7298535704612732\n",
      "Epoch 3042: Training Loss: 0.19645902017752329 Validation Loss: 0.7303963303565979\n",
      "Epoch 3043: Training Loss: 0.1964897116025289 Validation Loss: 0.7320976853370667\n",
      "Epoch 3044: Training Loss: 0.19641903539498648 Validation Loss: 0.7295169234275818\n",
      "Epoch 3045: Training Loss: 0.1962218383948008 Validation Loss: 0.7286834120750427\n",
      "Epoch 3046: Training Loss: 0.19608068962891897 Validation Loss: 0.7310553193092346\n",
      "Epoch 3047: Training Loss: 0.19594706098238626 Validation Loss: 0.7315568923950195\n",
      "Epoch 3048: Training Loss: 0.1959996074438095 Validation Loss: 0.7311939001083374\n",
      "Epoch 3049: Training Loss: 0.19591318567593893 Validation Loss: 0.7331818342208862\n",
      "Epoch 3050: Training Loss: 0.19573442141215006 Validation Loss: 0.7318512797355652\n",
      "Epoch 3051: Training Loss: 0.19556004305680594 Validation Loss: 0.7310178279876709\n",
      "Epoch 3052: Training Loss: 0.19555906454722086 Validation Loss: 0.7294411063194275\n",
      "Epoch 3053: Training Loss: 0.19545618693033853 Validation Loss: 0.731015145778656\n",
      "Epoch 3054: Training Loss: 0.19525176286697388 Validation Loss: 0.7307396531105042\n",
      "Epoch 3055: Training Loss: 0.19523770610491434 Validation Loss: 0.7305585741996765\n",
      "Epoch 3056: Training Loss: 0.19508635997772217 Validation Loss: 0.7310243844985962\n",
      "Epoch 3057: Training Loss: 0.19521163403987885 Validation Loss: 0.7324041128158569\n",
      "Epoch 3058: Training Loss: 0.19488967955112457 Validation Loss: 0.7310168743133545\n",
      "Epoch 3059: Training Loss: 0.19488875567913055 Validation Loss: 0.7304986715316772\n",
      "Epoch 3060: Training Loss: 0.19477065900961557 Validation Loss: 0.7316179871559143\n",
      "Epoch 3061: Training Loss: 0.19477813442548117 Validation Loss: 0.7292843461036682\n",
      "Epoch 3062: Training Loss: 0.19457393884658813 Validation Loss: 0.7291988730430603\n",
      "Epoch 3063: Training Loss: 0.19459323088328043 Validation Loss: 0.7281600832939148\n",
      "Epoch 3064: Training Loss: 0.1943773478269577 Validation Loss: 0.7316805124282837\n",
      "Epoch 3065: Training Loss: 0.19423426687717438 Validation Loss: 0.7326449155807495\n",
      "Epoch 3066: Training Loss: 0.19427457451820374 Validation Loss: 0.7340618968009949\n",
      "Epoch 3067: Training Loss: 0.19437913099924722 Validation Loss: 0.7294919490814209\n",
      "Epoch 3068: Training Loss: 0.19398088256518045 Validation Loss: 0.7285954356193542\n",
      "Epoch 3069: Training Loss: 0.19385557373364767 Validation Loss: 0.7287943959236145\n",
      "Epoch 3070: Training Loss: 0.19374952216943106 Validation Loss: 0.7319495677947998\n",
      "Epoch 3071: Training Loss: 0.19367304941018423 Validation Loss: 0.7317165732383728\n",
      "Epoch 3072: Training Loss: 0.19355781376361847 Validation Loss: 0.7324796319007874\n",
      "Epoch 3073: Training Loss: 0.19355469445387521 Validation Loss: 0.7303343415260315\n",
      "Epoch 3074: Training Loss: 0.1933831423521042 Validation Loss: 0.7298676371574402\n",
      "Epoch 3075: Training Loss: 0.19323020180066428 Validation Loss: 0.7305376529693604\n",
      "Epoch 3076: Training Loss: 0.19335739314556122 Validation Loss: 0.7331540584564209\n",
      "Epoch 3077: Training Loss: 0.19319106141726175 Validation Loss: 0.7328457832336426\n",
      "Epoch 3078: Training Loss: 0.1931852400302887 Validation Loss: 0.7277273535728455\n",
      "Epoch 3079: Training Loss: 0.19290110965569815 Validation Loss: 0.727092444896698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3080: Training Loss: 0.19281133512655893 Validation Loss: 0.7278290390968323\n",
      "Epoch 3081: Training Loss: 0.19270759324232736 Validation Loss: 0.7286446690559387\n",
      "Epoch 3082: Training Loss: 0.19259647031625113 Validation Loss: 0.732029139995575\n",
      "Epoch 3083: Training Loss: 0.19246191283067068 Validation Loss: 0.7327073812484741\n",
      "Epoch 3084: Training Loss: 0.1924041211605072 Validation Loss: 0.732475996017456\n",
      "Epoch 3085: Training Loss: 0.19239982962608337 Validation Loss: 0.7315807938575745\n",
      "Epoch 3086: Training Loss: 0.19215932488441467 Validation Loss: 0.7305215001106262\n",
      "Epoch 3087: Training Loss: 0.1922121892372767 Validation Loss: 0.7284635901451111\n",
      "Epoch 3088: Training Loss: 0.1919547269741694 Validation Loss: 0.7295913100242615\n",
      "Epoch 3089: Training Loss: 0.19188776115576425 Validation Loss: 0.7294204831123352\n",
      "Epoch 3090: Training Loss: 0.19189206262429556 Validation Loss: 0.7322416305541992\n",
      "Epoch 3091: Training Loss: 0.1917064686616262 Validation Loss: 0.7314093708992004\n",
      "Epoch 3092: Training Loss: 0.19158825278282166 Validation Loss: 0.7297346591949463\n",
      "Epoch 3093: Training Loss: 0.1917328635851542 Validation Loss: 0.7276009321212769\n",
      "Epoch 3094: Training Loss: 0.1914801150560379 Validation Loss: 0.7282674312591553\n",
      "Epoch 3095: Training Loss: 0.19135781129201254 Validation Loss: 0.7314152121543884\n",
      "Epoch 3096: Training Loss: 0.19141019880771637 Validation Loss: 0.7342480421066284\n",
      "Epoch 3097: Training Loss: 0.19141826530297598 Validation Loss: 0.729570746421814\n",
      "Epoch 3098: Training Loss: 0.1911422312259674 Validation Loss: 0.7297402620315552\n",
      "Epoch 3099: Training Loss: 0.19095690051714578 Validation Loss: 0.7288487553596497\n",
      "Epoch 3100: Training Loss: 0.19092336297035217 Validation Loss: 0.729500949382782\n",
      "Epoch 3101: Training Loss: 0.19088377058506012 Validation Loss: 0.7308915257453918\n",
      "Epoch 3102: Training Loss: 0.19087442258993784 Validation Loss: 0.7286235690116882\n",
      "Epoch 3103: Training Loss: 0.19059141973654428 Validation Loss: 0.7297935485839844\n",
      "Epoch 3104: Training Loss: 0.19051098823547363 Validation Loss: 0.729985237121582\n",
      "Epoch 3105: Training Loss: 0.1912843237320582 Validation Loss: 0.7353857159614563\n",
      "Epoch 3106: Training Loss: 0.19041301310062408 Validation Loss: 0.7318705916404724\n",
      "Epoch 3107: Training Loss: 0.1902663360039393 Validation Loss: 0.7272161245346069\n",
      "Epoch 3108: Training Loss: 0.19021340211232504 Validation Loss: 0.7281982898712158\n",
      "Epoch 3109: Training Loss: 0.19029571612675986 Validation Loss: 0.7260289192199707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3110: Training Loss: 0.18998974561691284 Validation Loss: 0.7294812798500061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3111: Training Loss: 0.18976288040479025 Validation Loss: 0.7311321496963501\n",
      "Epoch 3112: Training Loss: 0.1903421034415563 Validation Loss: 0.7354740500450134\n",
      "Epoch 3113: Training Loss: 0.18981059392293295 Validation Loss: 0.7315611243247986\n",
      "Epoch 3114: Training Loss: 0.18969900409380594 Validation Loss: 0.727023184299469\n",
      "Epoch 3115: Training Loss: 0.18947339554627737 Validation Loss: 0.7267917990684509\n",
      "Epoch 3116: Training Loss: 0.18957801659901938 Validation Loss: 0.7256674766540527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3117: Training Loss: 0.18929106493790945 Validation Loss: 0.7289121747016907\n",
      "Epoch 3118: Training Loss: 0.18918639421463013 Validation Loss: 0.7318679094314575\n",
      "Epoch 3119: Training Loss: 0.1891180177529653 Validation Loss: 0.7337403297424316\n",
      "Epoch 3120: Training Loss: 0.18905600408713022 Validation Loss: 0.7315809726715088\n",
      "Epoch 3121: Training Loss: 0.18892324964205423 Validation Loss: 0.7300949692726135\n",
      "Epoch 3122: Training Loss: 0.1888797531525294 Validation Loss: 0.7306391596794128\n",
      "Epoch 3123: Training Loss: 0.18866926431655884 Validation Loss: 0.7283201813697815\n",
      "Epoch 3124: Training Loss: 0.18870707352956137 Validation Loss: 0.7257918119430542\n",
      "Epoch 3125: Training Loss: 0.1886878858009974 Validation Loss: 0.7287270426750183\n",
      "Epoch 3126: Training Loss: 0.18851342300573984 Validation Loss: 0.7280886173248291\n",
      "Epoch 3127: Training Loss: 0.18845387796560922 Validation Loss: 0.7303522825241089\n",
      "Epoch 3128: Training Loss: 0.18831278383731842 Validation Loss: 0.7316566705703735\n",
      "Epoch 3129: Training Loss: 0.1881635586420695 Validation Loss: 0.7302103042602539\n",
      "Epoch 3130: Training Loss: 0.18844944735368094 Validation Loss: 0.726037323474884\n",
      "Epoch 3131: Training Loss: 0.1880346139272054 Validation Loss: 0.7281394004821777\n",
      "Epoch 3132: Training Loss: 0.18785178661346436 Validation Loss: 0.730490505695343\n",
      "Epoch 3133: Training Loss: 0.1881119360526403 Validation Loss: 0.7332131266593933\n",
      "Epoch 3134: Training Loss: 0.18795115749041238 Validation Loss: 0.7326734662055969\n",
      "Epoch 3135: Training Loss: 0.18775798380374908 Validation Loss: 0.7274644374847412\n",
      "Epoch 3136: Training Loss: 0.187523086865743 Validation Loss: 0.7267820239067078\n",
      "Epoch 3137: Training Loss: 0.18741708000500998 Validation Loss: 0.7270926237106323\n",
      "Epoch 3138: Training Loss: 0.18738498290379843 Validation Loss: 0.7286556363105774\n",
      "Epoch 3139: Training Loss: 0.18733211855093637 Validation Loss: 0.7277737259864807\n",
      "Epoch 3140: Training Loss: 0.18713633716106415 Validation Loss: 0.730389416217804\n",
      "Epoch 3141: Training Loss: 0.18714962899684906 Validation Loss: 0.729336678981781\n",
      "Epoch 3142: Training Loss: 0.18699138859907785 Validation Loss: 0.7302068471908569\n",
      "Epoch 3143: Training Loss: 0.18692956368128458 Validation Loss: 0.731557309627533\n",
      "Epoch 3144: Training Loss: 0.1869440277417501 Validation Loss: 0.7306137084960938\n",
      "Epoch 3145: Training Loss: 0.18665088713169098 Validation Loss: 0.7278335690498352\n",
      "Epoch 3146: Training Loss: 0.1865922063589096 Validation Loss: 0.7276290655136108\n",
      "Epoch 3147: Training Loss: 0.18657116095225015 Validation Loss: 0.7260282039642334\n",
      "Epoch 3148: Training Loss: 0.18643958866596222 Validation Loss: 0.727832019329071\n",
      "Epoch 3149: Training Loss: 0.18639922638734183 Validation Loss: 0.7303412556648254\n",
      "Epoch 3150: Training Loss: 0.1862280567487081 Validation Loss: 0.7297592759132385\n",
      "Epoch 3151: Training Loss: 0.18623057504494986 Validation Loss: 0.7315509915351868\n",
      "Epoch 3152: Training Loss: 0.18607723216215769 Validation Loss: 0.7305394411087036\n",
      "Epoch 3153: Training Loss: 0.18608009815216064 Validation Loss: 0.7273507118225098\n",
      "Epoch 3154: Training Loss: 0.18605879445870718 Validation Loss: 0.7257087826728821\n",
      "Epoch 3155: Training Loss: 0.18595915536085764 Validation Loss: 0.7298896908760071\n",
      "Epoch 3156: Training Loss: 0.1857114384571711 Validation Loss: 0.7298402786254883\n",
      "Epoch 3157: Training Loss: 0.18562265733877817 Validation Loss: 0.7286590337753296\n",
      "Epoch 3158: Training Loss: 0.18560134867827097 Validation Loss: 0.7272322773933411\n",
      "Epoch 3159: Training Loss: 0.18565533061822256 Validation Loss: 0.7298274636268616\n",
      "Epoch 3160: Training Loss: 0.18539990981419882 Validation Loss: 0.7287527322769165\n",
      "Epoch 3161: Training Loss: 0.18534566958745322 Validation Loss: 0.7286211848258972\n",
      "Epoch 3162: Training Loss: 0.1854762335618337 Validation Loss: 0.7274013757705688\n",
      "Epoch 3163: Training Loss: 0.1852326194445292 Validation Loss: 0.7310123443603516\n",
      "Epoch 3164: Training Loss: 0.18496051927407584 Validation Loss: 0.7310929894447327\n",
      "Epoch 3165: Training Loss: 0.18505281706651053 Validation Loss: 0.7302740812301636\n",
      "Epoch 3166: Training Loss: 0.18483155965805054 Validation Loss: 0.7278842329978943\n",
      "Epoch 3167: Training Loss: 0.18473240236441293 Validation Loss: 0.7259172797203064\n",
      "Epoch 3168: Training Loss: 0.18488827347755432 Validation Loss: 0.7243288159370422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3169: Training Loss: 0.18505285680294037 Validation Loss: 0.7312473654747009\n",
      "Epoch 3170: Training Loss: 0.18448854486147562 Validation Loss: 0.7326958179473877\n",
      "Epoch 3171: Training Loss: 0.1844213306903839 Validation Loss: 0.7312610745429993\n",
      "Epoch 3172: Training Loss: 0.1845167577266693 Validation Loss: 0.7270652055740356\n",
      "Epoch 3173: Training Loss: 0.1842258870601654 Validation Loss: 0.7271890640258789\n",
      "Epoch 3174: Training Loss: 0.18412413199742636 Validation Loss: 0.7269492149353027\n",
      "Epoch 3175: Training Loss: 0.1841705689827601 Validation Loss: 0.7279874086380005\n",
      "Epoch 3176: Training Loss: 0.18392611543337503 Validation Loss: 0.7272300720214844\n",
      "Epoch 3177: Training Loss: 0.1839451640844345 Validation Loss: 0.7294589877128601\n",
      "Epoch 3178: Training Loss: 0.18376551071802774 Validation Loss: 0.7300069332122803\n",
      "Epoch 3179: Training Loss: 0.18365995089213052 Validation Loss: 0.728059709072113\n",
      "Epoch 3180: Training Loss: 0.18393774330615997 Validation Loss: 0.725481390953064\n",
      "Epoch 3181: Training Loss: 0.18360582490762076 Validation Loss: 0.7275238037109375\n",
      "Epoch 3182: Training Loss: 0.18350139260292053 Validation Loss: 0.7293029427528381\n",
      "Epoch 3183: Training Loss: 0.18347808718681335 Validation Loss: 0.7325178384780884\n",
      "Epoch 3184: Training Loss: 0.1837917019923528 Validation Loss: 0.734754204750061\n",
      "Epoch 3185: Training Loss: 0.18379706144332886 Validation Loss: 0.7279677987098694\n",
      "Epoch 3186: Training Loss: 0.18311424056688944 Validation Loss: 0.7255100607872009\n",
      "Epoch 3187: Training Loss: 0.18302519619464874 Validation Loss: 0.7258913516998291\n",
      "Epoch 3188: Training Loss: 0.18300180633862814 Validation Loss: 0.7284887433052063\n",
      "Epoch 3189: Training Loss: 0.18287179370721182 Validation Loss: 0.7273926734924316\n",
      "Epoch 3190: Training Loss: 0.18273726602395376 Validation Loss: 0.7295440435409546\n",
      "Epoch 3191: Training Loss: 0.18266552686691284 Validation Loss: 0.7305248379707336\n",
      "Epoch 3192: Training Loss: 0.18262864649295807 Validation Loss: 0.7275636792182922\n",
      "Epoch 3193: Training Loss: 0.18242391447226206 Validation Loss: 0.7265892028808594\n",
      "Epoch 3194: Training Loss: 0.18237016598383585 Validation Loss: 0.7280216217041016\n",
      "Epoch 3195: Training Loss: 0.1822874496380488 Validation Loss: 0.7274216413497925\n",
      "Epoch 3196: Training Loss: 0.182191401720047 Validation Loss: 0.7284047603607178\n",
      "Epoch 3197: Training Loss: 0.1820429563522339 Validation Loss: 0.7298682928085327\n",
      "Epoch 3198: Training Loss: 0.18203396598498026 Validation Loss: 0.7283744812011719\n",
      "Epoch 3199: Training Loss: 0.1818725715080897 Validation Loss: 0.7281448841094971\n",
      "Epoch 3200: Training Loss: 0.18180415530999502 Validation Loss: 0.7284760475158691\n",
      "Epoch 3201: Training Loss: 0.18179128070672354 Validation Loss: 0.7302067279815674\n",
      "Epoch 3202: Training Loss: 0.1818269044160843 Validation Loss: 0.7310730218887329\n",
      "Epoch 3203: Training Loss: 0.18160796165466309 Validation Loss: 0.7285877466201782\n",
      "Epoch 3204: Training Loss: 0.181608776251475 Validation Loss: 0.7251960635185242\n",
      "Epoch 3205: Training Loss: 0.18166565895080566 Validation Loss: 0.7232527732849121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3206: Training Loss: 0.18151884774367014 Validation Loss: 0.7239622473716736\n",
      "Epoch 3207: Training Loss: 0.181313489874204 Validation Loss: 0.7300013303756714\n",
      "Epoch 3208: Training Loss: 0.18146370351314545 Validation Loss: 0.7341766357421875\n",
      "Epoch 3209: Training Loss: 0.1813273330529531 Validation Loss: 0.7319570183753967\n",
      "Epoch 3210: Training Loss: 0.18093645572662354 Validation Loss: 0.7292824387550354\n",
      "Epoch 3211: Training Loss: 0.180968776345253 Validation Loss: 0.7256565093994141\n",
      "Epoch 3212: Training Loss: 0.18093754351139069 Validation Loss: 0.7253521084785461\n",
      "Epoch 3213: Training Loss: 0.18084890643755594 Validation Loss: 0.7282636165618896\n",
      "Epoch 3214: Training Loss: 0.1806753029425939 Validation Loss: 0.7292723655700684\n",
      "Epoch 3215: Training Loss: 0.18054346243540445 Validation Loss: 0.7284787893295288\n",
      "Epoch 3216: Training Loss: 0.18045538663864136 Validation Loss: 0.7280208468437195\n",
      "Epoch 3217: Training Loss: 0.18067562083403269 Validation Loss: 0.7251660823822021\n",
      "Epoch 3218: Training Loss: 0.18034441272417703 Validation Loss: 0.7254956960678101\n",
      "Epoch 3219: Training Loss: 0.180190180738767 Validation Loss: 0.7263596057891846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3220: Training Loss: 0.18012308577696481 Validation Loss: 0.7297133207321167\n",
      "Epoch 3221: Training Loss: 0.18020827074845633 Validation Loss: 0.7326607704162598\n",
      "Epoch 3222: Training Loss: 0.18001814683278403 Validation Loss: 0.7301570773124695\n",
      "Epoch 3223: Training Loss: 0.17983477811018625 Validation Loss: 0.7287638187408447\n",
      "Epoch 3224: Training Loss: 0.18021835386753082 Validation Loss: 0.7238964438438416\n",
      "Epoch 3225: Training Loss: 0.17982635895411173 Validation Loss: 0.7248346209526062\n",
      "Epoch 3226: Training Loss: 0.179670512676239 Validation Loss: 0.7262771725654602\n",
      "Epoch 3227: Training Loss: 0.17964259286721548 Validation Loss: 0.7307304739952087\n",
      "Epoch 3228: Training Loss: 0.17956656217575073 Validation Loss: 0.732069730758667\n",
      "Epoch 3229: Training Loss: 0.17944190402825674 Validation Loss: 0.7298916578292847\n",
      "Epoch 3230: Training Loss: 0.17923598984877268 Validation Loss: 0.727850615978241\n",
      "Epoch 3231: Training Loss: 0.17908447484175363 Validation Loss: 0.7254037857055664\n",
      "Epoch 3232: Training Loss: 0.17923169831434885 Validation Loss: 0.7261645197868347\n",
      "Epoch 3233: Training Loss: 0.17906699081261954 Validation Loss: 0.7250397205352783\n",
      "Epoch 3234: Training Loss: 0.17900433639685312 Validation Loss: 0.7258992791175842\n",
      "Epoch 3235: Training Loss: 0.17886638641357422 Validation Loss: 0.727134644985199\n",
      "Epoch 3236: Training Loss: 0.17873250941435495 Validation Loss: 0.7289815545082092\n",
      "Epoch 3237: Training Loss: 0.17870386938254038 Validation Loss: 0.729604959487915\n",
      "Epoch 3238: Training Loss: 0.17866605520248413 Validation Loss: 0.7299594283103943\n",
      "Epoch 3239: Training Loss: 0.1786088744799296 Validation Loss: 0.7293709516525269\n",
      "Epoch 3240: Training Loss: 0.17848415672779083 Validation Loss: 0.7277250289916992\n",
      "Epoch 3241: Training Loss: 0.17839448153972626 Validation Loss: 0.7248655557632446\n",
      "Epoch 3242: Training Loss: 0.17854556938012442 Validation Loss: 0.7238733172416687\n",
      "Epoch 3243: Training Loss: 0.17822960515817007 Validation Loss: 0.7266444563865662\n",
      "Epoch 3244: Training Loss: 0.17806855340798697 Validation Loss: 0.7300987243652344\n",
      "Epoch 3245: Training Loss: 0.17805803815523782 Validation Loss: 0.7302269339561462\n",
      "Epoch 3246: Training Loss: 0.17797764639059702 Validation Loss: 0.728471040725708\n",
      "Epoch 3247: Training Loss: 0.17792501052220663 Validation Loss: 0.7259301543235779\n",
      "Epoch 3248: Training Loss: 0.17773108184337616 Validation Loss: 0.7254558801651001\n",
      "Epoch 3249: Training Loss: 0.17776988446712494 Validation Loss: 0.7271517515182495\n",
      "Epoch 3250: Training Loss: 0.1775733083486557 Validation Loss: 0.7281471490859985\n",
      "Epoch 3251: Training Loss: 0.17781537274519602 Validation Loss: 0.7263107895851135\n",
      "Epoch 3252: Training Loss: 0.17762581010659537 Validation Loss: 0.7301315069198608\n",
      "Epoch 3253: Training Loss: 0.1774009813865026 Validation Loss: 0.7286112308502197\n",
      "Epoch 3254: Training Loss: 0.17725766201814017 Validation Loss: 0.72838294506073\n",
      "Epoch 3255: Training Loss: 0.17726594706376395 Validation Loss: 0.728323221206665\n",
      "Epoch 3256: Training Loss: 0.17725438872973123 Validation Loss: 0.7266126871109009\n",
      "Epoch 3257: Training Loss: 0.1770633061726888 Validation Loss: 0.7259166240692139\n",
      "Epoch 3258: Training Loss: 0.17693274716536203 Validation Loss: 0.7258009314537048\n",
      "Epoch 3259: Training Loss: 0.17683548231919607 Validation Loss: 0.7263979315757751\n",
      "Epoch 3260: Training Loss: 0.17673629025618234 Validation Loss: 0.7281307578086853\n",
      "Epoch 3261: Training Loss: 0.17676508923371634 Validation Loss: 0.7271406054496765\n",
      "Epoch 3262: Training Loss: 0.1766348977883657 Validation Loss: 0.7271970510482788\n",
      "Epoch 3263: Training Loss: 0.17652723689874014 Validation Loss: 0.7276206612586975\n",
      "Epoch 3264: Training Loss: 0.1766524463891983 Validation Loss: 0.7291114330291748\n",
      "Epoch 3265: Training Loss: 0.1764127860466639 Validation Loss: 0.7276569604873657\n",
      "Epoch 3266: Training Loss: 0.17654121418793997 Validation Loss: 0.7241598963737488\n",
      "Epoch 3267: Training Loss: 0.1764813462893168 Validation Loss: 0.7223143577575684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3268: Training Loss: 0.17616766194502512 Validation Loss: 0.7267372012138367\n",
      "Epoch 3269: Training Loss: 0.17599293092886606 Validation Loss: 0.7293145060539246\n",
      "Epoch 3270: Training Loss: 0.1761734535296758 Validation Loss: 0.7333283424377441\n",
      "Epoch 3271: Training Loss: 0.17629838486512503 Validation Loss: 0.7289233207702637\n",
      "Epoch 3272: Training Loss: 0.17576778928438822 Validation Loss: 0.7276620864868164\n",
      "Epoch 3273: Training Loss: 0.1759615739186605 Validation Loss: 0.725871205329895\n",
      "Epoch 3274: Training Loss: 0.1756062755982081 Validation Loss: 0.7266745567321777\n",
      "Epoch 3275: Training Loss: 0.17552300790945688 Validation Loss: 0.7274191975593567\n",
      "Epoch 3276: Training Loss: 0.1757870316505432 Validation Loss: 0.7296439409255981\n",
      "Epoch 3277: Training Loss: 0.17540190617243448 Validation Loss: 0.7283973693847656\n",
      "Epoch 3278: Training Loss: 0.17545509338378906 Validation Loss: 0.7240035533905029\n",
      "Epoch 3279: Training Loss: 0.17522601783275604 Validation Loss: 0.7232807874679565\n",
      "Epoch 3280: Training Loss: 0.1752060055732727 Validation Loss: 0.7264077067375183\n",
      "Epoch 3281: Training Loss: 0.17512213190396628 Validation Loss: 0.726054310798645\n",
      "Epoch 3282: Training Loss: 0.17505464951197305 Validation Loss: 0.7290988564491272\n",
      "Epoch 3283: Training Loss: 0.17495798071225485 Validation Loss: 0.7285089492797852\n",
      "Epoch 3284: Training Loss: 0.17489400506019592 Validation Loss: 0.7279771566390991\n",
      "Epoch 3285: Training Loss: 0.17484180629253387 Validation Loss: 0.7282698750495911\n",
      "Epoch 3286: Training Loss: 0.17466160158316293 Validation Loss: 0.7272504568099976\n",
      "Epoch 3287: Training Loss: 0.17455832660198212 Validation Loss: 0.7248790860176086\n",
      "Epoch 3288: Training Loss: 0.17455474535624185 Validation Loss: 0.724445641040802\n",
      "Epoch 3289: Training Loss: 0.1744699329137802 Validation Loss: 0.7252779006958008\n",
      "Epoch 3290: Training Loss: 0.1746311088403066 Validation Loss: 0.7287510633468628\n",
      "Epoch 3291: Training Loss: 0.17436514794826508 Validation Loss: 0.72947758436203\n",
      "Epoch 3292: Training Loss: 0.17418870329856873 Validation Loss: 0.7277646064758301\n",
      "Epoch 3293: Training Loss: 0.17413901289304098 Validation Loss: 0.7254419922828674\n",
      "Epoch 3294: Training Loss: 0.1740702341000239 Validation Loss: 0.7250911593437195\n",
      "Epoch 3295: Training Loss: 0.1740851253271103 Validation Loss: 0.7244551181793213\n",
      "Epoch 3296: Training Loss: 0.1738854149977366 Validation Loss: 0.7254149913787842\n",
      "Epoch 3297: Training Loss: 0.17378162344296774 Validation Loss: 0.7280485033988953\n",
      "Epoch 3298: Training Loss: 0.17368543148040771 Validation Loss: 0.7287610173225403\n",
      "Epoch 3299: Training Loss: 0.17363299429416656 Validation Loss: 0.7298898696899414\n",
      "Epoch 3300: Training Loss: 0.1736470659573873 Validation Loss: 0.730156421661377\n",
      "Epoch 3301: Training Loss: 0.1734908471504847 Validation Loss: 0.7265914678573608\n",
      "Epoch 3302: Training Loss: 0.1735618511835734 Validation Loss: 0.7228564620018005\n",
      "Epoch 3303: Training Loss: 0.17335262397925058 Validation Loss: 0.7243388295173645\n",
      "Epoch 3304: Training Loss: 0.1732763002316157 Validation Loss: 0.7242948412895203\n",
      "Epoch 3305: Training Loss: 0.1732294112443924 Validation Loss: 0.7266924977302551\n",
      "Epoch 3306: Training Loss: 0.17303747435410818 Validation Loss: 0.7277613878250122\n",
      "Epoch 3307: Training Loss: 0.17312062780062357 Validation Loss: 0.7302566766738892\n",
      "Epoch 3308: Training Loss: 0.17303763329982758 Validation Loss: 0.7285834550857544\n",
      "Epoch 3309: Training Loss: 0.17286707957585654 Validation Loss: 0.7276245355606079\n",
      "Epoch 3310: Training Loss: 0.1728045791387558 Validation Loss: 0.7248024940490723\n",
      "Epoch 3311: Training Loss: 0.17277931670347849 Validation Loss: 0.7269381284713745\n",
      "Epoch 3312: Training Loss: 0.17263440291086832 Validation Loss: 0.7267364859580994\n",
      "Epoch 3313: Training Loss: 0.17250471313794455 Validation Loss: 0.7260278463363647\n",
      "Epoch 3314: Training Loss: 0.17247972885767618 Validation Loss: 0.7279209494590759\n",
      "Epoch 3315: Training Loss: 0.17238080501556396 Validation Loss: 0.7267197966575623\n",
      "Epoch 3316: Training Loss: 0.1723901778459549 Validation Loss: 0.7282095551490784\n",
      "Epoch 3317: Training Loss: 0.17236988743146262 Validation Loss: 0.7248092889785767\n",
      "Epoch 3318: Training Loss: 0.172157550851504 Validation Loss: 0.7258267998695374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3319: Training Loss: 0.172017902135849 Validation Loss: 0.7260705232620239\n",
      "Epoch 3320: Training Loss: 0.17201885084311166 Validation Loss: 0.7277278900146484\n",
      "Epoch 3321: Training Loss: 0.17195030053456625 Validation Loss: 0.7280204892158508\n",
      "Epoch 3322: Training Loss: 0.17185358703136444 Validation Loss: 0.7284585237503052\n",
      "Epoch 3323: Training Loss: 0.17173229157924652 Validation Loss: 0.7272098064422607\n",
      "Epoch 3324: Training Loss: 0.17163669069608053 Validation Loss: 0.7254146933555603\n",
      "Epoch 3325: Training Loss: 0.1720376362403234 Validation Loss: 0.7219893336296082\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3326: Training Loss: 0.17160119116306305 Validation Loss: 0.723253607749939\n",
      "Epoch 3327: Training Loss: 0.17140425244967142 Validation Loss: 0.7256098389625549\n",
      "Epoch 3328: Training Loss: 0.17130881547927856 Validation Loss: 0.7284040451049805\n",
      "Epoch 3329: Training Loss: 0.1712862898906072 Validation Loss: 0.7299783825874329\n",
      "Epoch 3330: Training Loss: 0.17128051817417145 Validation Loss: 0.727668821811676\n",
      "Epoch 3331: Training Loss: 0.17119807998339334 Validation Loss: 0.7286930084228516\n",
      "Epoch 3332: Training Loss: 0.17114998400211334 Validation Loss: 0.7249372601509094\n",
      "Epoch 3333: Training Loss: 0.1710595041513443 Validation Loss: 0.7265231013298035\n",
      "Epoch 3334: Training Loss: 0.17091917991638184 Validation Loss: 0.7263983488082886\n",
      "Epoch 3335: Training Loss: 0.17086008191108704 Validation Loss: 0.7250429391860962\n",
      "Epoch 3336: Training Loss: 0.17100858191649118 Validation Loss: 0.7282943725585938\n",
      "Epoch 3337: Training Loss: 0.1710105985403061 Validation Loss: 0.7241179943084717\n",
      "Epoch 3338: Training Loss: 0.17060675720373789 Validation Loss: 0.7244554758071899\n",
      "Epoch 3339: Training Loss: 0.1704892764488856 Validation Loss: 0.72665935754776\n",
      "Epoch 3340: Training Loss: 0.1704174131155014 Validation Loss: 0.7271038293838501\n",
      "Epoch 3341: Training Loss: 0.17031648258368173 Validation Loss: 0.7267363667488098\n",
      "Epoch 3342: Training Loss: 0.17025242249170938 Validation Loss: 0.7259711027145386\n",
      "Epoch 3343: Training Loss: 0.1701455960671107 Validation Loss: 0.7270006537437439\n",
      "Epoch 3344: Training Loss: 0.17010926206906637 Validation Loss: 0.726685643196106\n",
      "Epoch 3345: Training Loss: 0.17002783715724945 Validation Loss: 0.7268193364143372\n",
      "Epoch 3346: Training Loss: 0.16994319359461466 Validation Loss: 0.7275292277336121\n",
      "Epoch 3347: Training Loss: 0.16988298296928406 Validation Loss: 0.7272403240203857\n",
      "Epoch 3348: Training Loss: 0.16978608071804047 Validation Loss: 0.7256488800048828\n",
      "Epoch 3349: Training Loss: 0.16985593239466348 Validation Loss: 0.7248451113700867\n",
      "Epoch 3350: Training Loss: 0.16977845629056296 Validation Loss: 0.7255982756614685\n",
      "Epoch 3351: Training Loss: 0.16979309419790903 Validation Loss: 0.7232155203819275\n",
      "Epoch 3352: Training Loss: 0.1694749891757965 Validation Loss: 0.7247936725616455\n",
      "Epoch 3353: Training Loss: 0.16966780523459116 Validation Loss: 0.7290078401565552\n",
      "Epoch 3354: Training Loss: 0.16943895320097604 Validation Loss: 0.729393482208252\n",
      "Epoch 3355: Training Loss: 0.16931824386119843 Validation Loss: 0.7271233797073364\n",
      "Epoch 3356: Training Loss: 0.16921662787596384 Validation Loss: 0.725355863571167\n",
      "Epoch 3357: Training Loss: 0.16919499138991037 Validation Loss: 0.7247430682182312\n",
      "Epoch 3358: Training Loss: 0.1690861483414968 Validation Loss: 0.7259844541549683\n",
      "Epoch 3359: Training Loss: 0.168995534380277 Validation Loss: 0.7253113985061646\n",
      "Epoch 3360: Training Loss: 0.16894359389940897 Validation Loss: 0.7276836037635803\n",
      "Epoch 3361: Training Loss: 0.16885987420876822 Validation Loss: 0.7281317710876465\n",
      "Epoch 3362: Training Loss: 0.1688457429409027 Validation Loss: 0.7268491983413696\n",
      "Epoch 3363: Training Loss: 0.16897056500116983 Validation Loss: 0.7238456010818481\n",
      "Epoch 3364: Training Loss: 0.16857187946637472 Validation Loss: 0.7247586846351624\n",
      "Epoch 3365: Training Loss: 0.16868024071057638 Validation Loss: 0.7276872396469116\n",
      "Epoch 3366: Training Loss: 0.16850667198499045 Validation Loss: 0.7264374494552612\n",
      "Epoch 3367: Training Loss: 0.16838720440864563 Validation Loss: 0.7249248027801514\n",
      "Epoch 3368: Training Loss: 0.1682939132054647 Validation Loss: 0.7268751263618469\n",
      "Epoch 3369: Training Loss: 0.16818868120511374 Validation Loss: 0.7267018556594849\n",
      "Epoch 3370: Training Loss: 0.16822457313537598 Validation Loss: 0.7255632877349854\n",
      "Epoch 3371: Training Loss: 0.16809975107510886 Validation Loss: 0.7262714505195618\n",
      "Epoch 3372: Training Loss: 0.16803604861100516 Validation Loss: 0.7258457541465759\n",
      "Epoch 3373: Training Loss: 0.16811507940292358 Validation Loss: 0.7268145084381104\n",
      "Epoch 3374: Training Loss: 0.16793015599250793 Validation Loss: 0.725109338760376\n",
      "Epoch 3375: Training Loss: 0.16774697601795197 Validation Loss: 0.7252829074859619\n",
      "Epoch 3376: Training Loss: 0.16779185831546783 Validation Loss: 0.7250586748123169\n",
      "Epoch 3377: Training Loss: 0.1675716241200765 Validation Loss: 0.7261976003646851\n",
      "Epoch 3378: Training Loss: 0.16770810385545096 Validation Loss: 0.7283377051353455\n",
      "Epoch 3379: Training Loss: 0.16770866513252258 Validation Loss: 0.7243595123291016\n",
      "Epoch 3380: Training Loss: 0.16739101707935333 Validation Loss: 0.723848819732666\n",
      "Epoch 3381: Training Loss: 0.16752459605534872 Validation Loss: 0.7235486507415771\n",
      "Epoch 3382: Training Loss: 0.1672761191924413 Validation Loss: 0.7267533540725708\n",
      "Epoch 3383: Training Loss: 0.16721762220064798 Validation Loss: 0.726809024810791\n",
      "Epoch 3384: Training Loss: 0.1672201951344808 Validation Loss: 0.7305401563644409\n",
      "Epoch 3385: Training Loss: 0.1672993004322052 Validation Loss: 0.7311316728591919\n",
      "Epoch 3386: Training Loss: 0.16723918914794922 Validation Loss: 0.7251856923103333\n",
      "Epoch 3387: Training Loss: 0.166922926902771 Validation Loss: 0.7238502502441406\n",
      "Epoch 3388: Training Loss: 0.1674309273560842 Validation Loss: 0.7261584401130676\n",
      "Epoch 3389: Training Loss: 0.1669213225444158 Validation Loss: 0.7237187623977661\n",
      "Epoch 3390: Training Loss: 0.16685213148593903 Validation Loss: 0.7219461798667908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3391: Training Loss: 0.1666294733683268 Validation Loss: 0.7243478298187256\n",
      "Epoch 3392: Training Loss: 0.16650108993053436 Validation Loss: 0.7260139584541321\n",
      "Epoch 3393: Training Loss: 0.1665214796861013 Validation Loss: 0.7250876426696777\n",
      "Epoch 3394: Training Loss: 0.16647436718146005 Validation Loss: 0.728418231010437\n",
      "Epoch 3395: Training Loss: 0.16636490325133005 Validation Loss: 0.7282689213752747\n",
      "Epoch 3396: Training Loss: 0.1662211169799169 Validation Loss: 0.7277578711509705\n",
      "Epoch 3397: Training Loss: 0.166161780556043 Validation Loss: 0.7270859479904175\n",
      "Epoch 3398: Training Loss: 0.16614157458146414 Validation Loss: 0.7245792150497437\n",
      "Epoch 3399: Training Loss: 0.16606765488783518 Validation Loss: 0.7230936884880066\n",
      "Epoch 3400: Training Loss: 0.1661366472641627 Validation Loss: 0.7256237268447876\n",
      "Epoch 3401: Training Loss: 0.16591447095076242 Validation Loss: 0.7231424450874329\n",
      "Epoch 3402: Training Loss: 0.165869211157163 Validation Loss: 0.7255926728248596\n",
      "Epoch 3403: Training Loss: 0.16588189701239267 Validation Loss: 0.7271904349327087\n",
      "Epoch 3404: Training Loss: 0.16561724742253622 Validation Loss: 0.7261261343955994\n",
      "Epoch 3405: Training Loss: 0.16556158165136972 Validation Loss: 0.724041223526001\n",
      "Epoch 3406: Training Loss: 0.16573691368103027 Validation Loss: 0.7222656607627869\n",
      "Epoch 3407: Training Loss: 0.16553946336110434 Validation Loss: 0.7240108847618103\n",
      "Epoch 3408: Training Loss: 0.16531023383140564 Validation Loss: 0.7277325987815857\n",
      "Epoch 3409: Training Loss: 0.16530726353327432 Validation Loss: 0.7282567620277405\n",
      "Epoch 3410: Training Loss: 0.16525795062383017 Validation Loss: 0.7299400568008423\n",
      "Epoch 3411: Training Loss: 0.1651411453882853 Validation Loss: 0.7285407781600952\n",
      "Epoch 3412: Training Loss: 0.16506814459959665 Validation Loss: 0.7263440489768982\n",
      "Epoch 3413: Training Loss: 0.16551174720128378 Validation Loss: 0.7202693223953247\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3414: Training Loss: 0.1650387148062388 Validation Loss: 0.7227439284324646\n",
      "Epoch 3415: Training Loss: 0.16501930356025696 Validation Loss: 0.7270599603652954\n",
      "Epoch 3416: Training Loss: 0.16478362182776132 Validation Loss: 0.7280794382095337\n",
      "Epoch 3417: Training Loss: 0.1648921718200048 Validation Loss: 0.7254800200462341\n",
      "Epoch 3418: Training Loss: 0.16507287323474884 Validation Loss: 0.7278386950492859\n",
      "Epoch 3419: Training Loss: 0.16530557970205942 Validation Loss: 0.7217077016830444\n",
      "Epoch 3420: Training Loss: 0.16474148631095886 Validation Loss: 0.7209750413894653\n",
      "Epoch 3421: Training Loss: 0.16456519067287445 Validation Loss: 0.7260290384292603\n",
      "Epoch 3422: Training Loss: 0.16432959834734598 Validation Loss: 0.728509783744812\n",
      "Epoch 3423: Training Loss: 0.1645042449235916 Validation Loss: 0.730918288230896\n",
      "Epoch 3424: Training Loss: 0.16428744296232858 Validation Loss: 0.7295210361480713\n",
      "Epoch 3425: Training Loss: 0.1642856995264689 Validation Loss: 0.7239239811897278\n",
      "Epoch 3426: Training Loss: 0.16420689225196838 Validation Loss: 0.7207126021385193\n",
      "Epoch 3427: Training Loss: 0.16416259606679282 Validation Loss: 0.7205715775489807\n",
      "Epoch 3428: Training Loss: 0.1640204886595408 Validation Loss: 0.7218559384346008\n",
      "Epoch 3429: Training Loss: 0.16374566157658896 Validation Loss: 0.7266562581062317\n",
      "Epoch 3430: Training Loss: 0.1639411300420761 Validation Loss: 0.7302207946777344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3431: Training Loss: 0.1638837605714798 Validation Loss: 0.7305358052253723\n",
      "Epoch 3432: Training Loss: 0.1636838217576345 Validation Loss: 0.7281301021575928\n",
      "Epoch 3433: Training Loss: 0.16361371676127115 Validation Loss: 0.7235514521598816\n",
      "Epoch 3434: Training Loss: 0.1636663426955541 Validation Loss: 0.7207641005516052\n",
      "Epoch 3435: Training Loss: 0.16362588107585907 Validation Loss: 0.7238582372665405\n",
      "Epoch 3436: Training Loss: 0.16330846647421518 Validation Loss: 0.7259295582771301\n",
      "Epoch 3437: Training Loss: 0.16323127845923105 Validation Loss: 0.7273704409599304\n",
      "Epoch 3438: Training Loss: 0.16328395903110504 Validation Loss: 0.7277685403823853\n",
      "Epoch 3439: Training Loss: 0.1631196786959966 Validation Loss: 0.7260758280754089\n",
      "Epoch 3440: Training Loss: 0.16313949724038443 Validation Loss: 0.7233842015266418\n",
      "Epoch 3441: Training Loss: 0.16310624778270721 Validation Loss: 0.7255390882492065\n",
      "Epoch 3442: Training Loss: 0.1629256308078766 Validation Loss: 0.724377453327179\n",
      "Epoch 3443: Training Loss: 0.1628860135873159 Validation Loss: 0.7237058281898499\n",
      "Epoch 3444: Training Loss: 0.16278329491615295 Validation Loss: 0.7258368134498596\n",
      "Epoch 3445: Training Loss: 0.16268523037433624 Validation Loss: 0.7266073226928711\n",
      "Epoch 3446: Training Loss: 0.16264558831850687 Validation Loss: 0.72654128074646\n",
      "Epoch 3447: Training Loss: 0.16261541346708933 Validation Loss: 0.7272248268127441\n",
      "Epoch 3448: Training Loss: 0.16251559555530548 Validation Loss: 0.724854052066803\n",
      "Epoch 3449: Training Loss: 0.16246238350868225 Validation Loss: 0.72425377368927\n",
      "Epoch 3450: Training Loss: 0.16240880390008292 Validation Loss: 0.7251099348068237\n",
      "Epoch 3451: Training Loss: 0.16242244839668274 Validation Loss: 0.7265211343765259\n",
      "Epoch 3452: Training Loss: 0.16219308972358704 Validation Loss: 0.7261841893196106\n",
      "Epoch 3453: Training Loss: 0.16219920416673025 Validation Loss: 0.7246721982955933\n",
      "Epoch 3454: Training Loss: 0.1627658704916636 Validation Loss: 0.7210486531257629\n",
      "Epoch 3455: Training Loss: 0.1621930549542109 Validation Loss: 0.7246170043945312\n",
      "Epoch 3456: Training Loss: 0.16190657019615173 Validation Loss: 0.7254306674003601\n",
      "Epoch 3457: Training Loss: 0.16186354557673135 Validation Loss: 0.7278302311897278\n",
      "Epoch 3458: Training Loss: 0.1618263175090154 Validation Loss: 0.7281313538551331\n",
      "Epoch 3459: Training Loss: 0.16178692877292633 Validation Loss: 0.7274065613746643\n",
      "Epoch 3460: Training Loss: 0.16188507278760275 Validation Loss: 0.7228911519050598\n",
      "Epoch 3461: Training Loss: 0.16163902481396994 Validation Loss: 0.722648561000824\n",
      "Epoch 3462: Training Loss: 0.16157247622807822 Validation Loss: 0.7236506342887878\n",
      "Epoch 3463: Training Loss: 0.1614758570988973 Validation Loss: 0.7250192761421204\n",
      "Epoch 3464: Training Loss: 0.16135090589523315 Validation Loss: 0.7266371846199036\n",
      "Epoch 3465: Training Loss: 0.1613613764444987 Validation Loss: 0.7255174517631531\n",
      "Epoch 3466: Training Loss: 0.16124639411767325 Validation Loss: 0.7246079444885254\n",
      "Epoch 3467: Training Loss: 0.16122968991597494 Validation Loss: 0.7260773181915283\n",
      "Epoch 3468: Training Loss: 0.16134160260359445 Validation Loss: 0.7275195121765137\n",
      "Epoch 3469: Training Loss: 0.16102810204029083 Validation Loss: 0.7244250774383545\n",
      "Epoch 3470: Training Loss: 0.16091573735078177 Validation Loss: 0.7237611413002014\n",
      "Epoch 3471: Training Loss: 0.16097467641035715 Validation Loss: 0.7226410508155823\n",
      "Epoch 3472: Training Loss: 0.16085802018642426 Validation Loss: 0.7245566248893738\n",
      "Epoch 3473: Training Loss: 0.16080169876416525 Validation Loss: 0.7256962656974792\n",
      "Epoch 3474: Training Loss: 0.1607534239689509 Validation Loss: 0.7252814769744873\n",
      "Epoch 3475: Training Loss: 0.16072172919909158 Validation Loss: 0.7236672639846802\n",
      "Epoch 3476: Training Loss: 0.16090130309263864 Validation Loss: 0.728708028793335\n",
      "Epoch 3477: Training Loss: 0.16056489447752634 Validation Loss: 0.727073073387146\n",
      "Epoch 3478: Training Loss: 0.16036754846572876 Validation Loss: 0.72557532787323\n",
      "Epoch 3479: Training Loss: 0.16053911546866098 Validation Loss: 0.7222988605499268\n",
      "Epoch 3480: Training Loss: 0.16030112405618033 Validation Loss: 0.7239853739738464\n",
      "Epoch 3481: Training Loss: 0.16020227471987405 Validation Loss: 0.7246964573860168\n",
      "Epoch 3482: Training Loss: 0.16019821166992188 Validation Loss: 0.7254207730293274\n",
      "Epoch 3483: Training Loss: 0.16008900105953217 Validation Loss: 0.7250258922576904\n",
      "Epoch 3484: Training Loss: 0.1602969616651535 Validation Loss: 0.7288152575492859\n",
      "Epoch 3485: Training Loss: 0.15997809171676636 Validation Loss: 0.7269063591957092\n",
      "Epoch 3486: Training Loss: 0.15991411606470743 Validation Loss: 0.7244748473167419\n",
      "Epoch 3487: Training Loss: 0.15980264544487 Validation Loss: 0.7234185338020325\n",
      "Epoch 3488: Training Loss: 0.15973580380280814 Validation Loss: 0.7229692935943604\n",
      "Epoch 3489: Training Loss: 0.15964993834495544 Validation Loss: 0.7246215343475342\n",
      "Epoch 3490: Training Loss: 0.1596612185239792 Validation Loss: 0.7264959812164307\n",
      "Epoch 3491: Training Loss: 0.1595862607161204 Validation Loss: 0.7257335186004639\n",
      "Epoch 3492: Training Loss: 0.1594872126976649 Validation Loss: 0.7246318459510803\n",
      "Epoch 3493: Training Loss: 0.15939168135325113 Validation Loss: 0.7235100865364075\n",
      "Epoch 3494: Training Loss: 0.15938518941402435 Validation Loss: 0.7232869267463684\n",
      "Epoch 3495: Training Loss: 0.1593488504489263 Validation Loss: 0.7259472608566284\n",
      "Epoch 3496: Training Loss: 0.15942803025245667 Validation Loss: 0.7238873839378357\n",
      "Epoch 3497: Training Loss: 0.15908639132976532 Validation Loss: 0.7249359488487244\n",
      "Epoch 3498: Training Loss: 0.1590229074160258 Validation Loss: 0.7260982990264893\n",
      "Epoch 3499: Training Loss: 0.1589893897374471 Validation Loss: 0.7259838581085205\n",
      "Epoch 3500: Training Loss: 0.15893800556659698 Validation Loss: 0.7245277762413025\n",
      "Epoch 3501: Training Loss: 0.15896406769752502 Validation Loss: 0.7236397862434387\n",
      "Epoch 3502: Training Loss: 0.15903357168038687 Validation Loss: 0.727384626865387\n",
      "Epoch 3503: Training Loss: 0.1587869624296824 Validation Loss: 0.7254437208175659\n",
      "Epoch 3504: Training Loss: 0.15868671735127768 Validation Loss: 0.7239407300949097\n",
      "Epoch 3505: Training Loss: 0.15861327449480692 Validation Loss: 0.7242984175682068\n",
      "Epoch 3506: Training Loss: 0.1586356262365977 Validation Loss: 0.7256262302398682\n",
      "Epoch 3507: Training Loss: 0.1586188425620397 Validation Loss: 0.7230035066604614\n",
      "Epoch 3508: Training Loss: 0.15869481364885965 Validation Loss: 0.7220795154571533\n",
      "Epoch 3509: Training Loss: 0.1584100772937139 Validation Loss: 0.7261945009231567\n",
      "Epoch 3510: Training Loss: 0.15828965604305267 Validation Loss: 0.7269005179405212\n",
      "Epoch 3511: Training Loss: 0.15816331406434378 Validation Loss: 0.7267348170280457\n",
      "Epoch 3512: Training Loss: 0.15850294629732767 Validation Loss: 0.7303987741470337\n",
      "Epoch 3513: Training Loss: 0.15813368062178293 Validation Loss: 0.7260613441467285\n",
      "Epoch 3514: Training Loss: 0.15792672336101532 Validation Loss: 0.7230410575866699\n",
      "Epoch 3515: Training Loss: 0.15845958391825357 Validation Loss: 0.7258874773979187\n",
      "Epoch 3516: Training Loss: 0.15782401462395987 Validation Loss: 0.7252793908119202\n",
      "Epoch 3517: Training Loss: 0.15796291828155518 Validation Loss: 0.7243133783340454\n",
      "Epoch 3518: Training Loss: 0.15774835646152496 Validation Loss: 0.7203825116157532\n",
      "Epoch 3519: Training Loss: 0.15771302580833435 Validation Loss: 0.7213848233222961\n",
      "Epoch 3520: Training Loss: 0.15778652330239615 Validation Loss: 0.7207883596420288\n",
      "Epoch 3521: Training Loss: 0.15756721794605255 Validation Loss: 0.7256706357002258\n",
      "Epoch 3522: Training Loss: 0.15750729044278464 Validation Loss: 0.7270734906196594\n",
      "Epoch 3523: Training Loss: 0.15734424690405527 Validation Loss: 0.7280502915382385\n",
      "Epoch 3524: Training Loss: 0.15746499101320902 Validation Loss: 0.7288578152656555\n",
      "Epoch 3525: Training Loss: 0.15739392737547556 Validation Loss: 0.726424515247345\n",
      "Epoch 3526: Training Loss: 0.1571597084403038 Validation Loss: 0.7245173454284668\n",
      "Epoch 3527: Training Loss: 0.1572241485118866 Validation Loss: 0.7208285927772522\n",
      "Epoch 3528: Training Loss: 0.1571034143368403 Validation Loss: 0.7210438251495361\n",
      "Epoch 3529: Training Loss: 0.1570865362882614 Validation Loss: 0.7244715690612793\n",
      "Epoch 3530: Training Loss: 0.15689760446548462 Validation Loss: 0.7256788611412048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3531: Training Loss: 0.1570085088411967 Validation Loss: 0.7242818474769592\n",
      "Epoch 3532: Training Loss: 0.15674041708310446 Validation Loss: 0.7250267863273621\n",
      "Epoch 3533: Training Loss: 0.15675785640875498 Validation Loss: 0.7250436544418335\n",
      "Epoch 3534: Training Loss: 0.1566875527302424 Validation Loss: 0.7269855737686157\n",
      "Epoch 3535: Training Loss: 0.1566056360801061 Validation Loss: 0.7268704175949097\n",
      "Epoch 3536: Training Loss: 0.1566085418065389 Validation Loss: 0.7249622344970703\n",
      "Epoch 3537: Training Loss: 0.1568127622207006 Validation Loss: 0.7216830849647522\n",
      "Epoch 3538: Training Loss: 0.15657907724380493 Validation Loss: 0.7209358215332031\n",
      "Epoch 3539: Training Loss: 0.15629449983437857 Validation Loss: 0.7243837714195251\n",
      "Epoch 3540: Training Loss: 0.1562556028366089 Validation Loss: 0.7271324396133423\n",
      "Epoch 3541: Training Loss: 0.15621898074944815 Validation Loss: 0.7275527715682983\n",
      "Epoch 3542: Training Loss: 0.15612641473611197 Validation Loss: 0.7271603345870972\n",
      "Epoch 3543: Training Loss: 0.15611997743447623 Validation Loss: 0.7253784537315369\n",
      "Epoch 3544: Training Loss: 0.156129519144694 Validation Loss: 0.7234249711036682\n",
      "Epoch 3545: Training Loss: 0.1560229311386744 Validation Loss: 0.7225993275642395\n",
      "Epoch 3546: Training Loss: 0.1559772491455078 Validation Loss: 0.7253926396369934\n",
      "Epoch 3547: Training Loss: 0.15591839452584585 Validation Loss: 0.7275428771972656\n",
      "Epoch 3548: Training Loss: 0.15584649642308554 Validation Loss: 0.7276164889335632\n",
      "Epoch 3549: Training Loss: 0.15563248097896576 Validation Loss: 0.7242483496665955\n",
      "Epoch 3550: Training Loss: 0.15560351560513178 Validation Loss: 0.722175121307373\n",
      "Epoch 3551: Training Loss: 0.1556205948193868 Validation Loss: 0.7232815027236938\n",
      "Epoch 3552: Training Loss: 0.1556809345881144 Validation Loss: 0.7217662334442139\n",
      "Epoch 3553: Training Loss: 0.15548169116179147 Validation Loss: 0.7245143055915833\n",
      "Epoch 3554: Training Loss: 0.15586863458156586 Validation Loss: 0.7281086444854736\n",
      "Epoch 3555: Training Loss: 0.15539555251598358 Validation Loss: 0.7263849377632141\n",
      "Epoch 3556: Training Loss: 0.15518829226493835 Validation Loss: 0.7247125506401062\n",
      "Epoch 3557: Training Loss: 0.15523838500181833 Validation Loss: 0.7219394445419312\n",
      "Epoch 3558: Training Loss: 0.15552244087060293 Validation Loss: 0.7192754149436951\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3559: Training Loss: 0.15521411101023355 Validation Loss: 0.7235205173492432\n",
      "Epoch 3560: Training Loss: 0.15494051575660706 Validation Loss: 0.7253147959709167\n",
      "Epoch 3561: Training Loss: 0.15492738783359528 Validation Loss: 0.7267822623252869\n",
      "Epoch 3562: Training Loss: 0.15489905575911203 Validation Loss: 0.7282833456993103\n",
      "Epoch 3563: Training Loss: 0.15508781870206198 Validation Loss: 0.7296179533004761\n",
      "Epoch 3564: Training Loss: 0.15503676732381186 Validation Loss: 0.7235718965530396\n",
      "Epoch 3565: Training Loss: 0.15493758022785187 Validation Loss: 0.7249726057052612\n",
      "Epoch 3566: Training Loss: 0.15453439950942993 Validation Loss: 0.7234193086624146\n",
      "Epoch 3567: Training Loss: 0.15455781171719232 Validation Loss: 0.7207638621330261\n",
      "Epoch 3568: Training Loss: 0.15446266531944275 Validation Loss: 0.7219381332397461\n",
      "Epoch 3569: Training Loss: 0.15442404647668204 Validation Loss: 0.7219436764717102\n",
      "Epoch 3570: Training Loss: 0.1543074051539103 Validation Loss: 0.7245216369628906\n",
      "Epoch 3571: Training Loss: 0.15423023204008737 Validation Loss: 0.725530207157135\n",
      "Epoch 3572: Training Loss: 0.15417842070261636 Validation Loss: 0.7263755798339844\n",
      "Epoch 3573: Training Loss: 0.15417413413524628 Validation Loss: 0.7269852757453918\n",
      "Epoch 3574: Training Loss: 0.15411533415317535 Validation Loss: 0.7244628667831421\n",
      "Epoch 3575: Training Loss: 0.1540097345908483 Validation Loss: 0.7241091132164001\n",
      "Epoch 3576: Training Loss: 0.1541325350602468 Validation Loss: 0.7255184650421143\n",
      "Epoch 3577: Training Loss: 0.15394523739814758 Validation Loss: 0.7250262498855591\n",
      "Epoch 3578: Training Loss: 0.15400908390680948 Validation Loss: 0.7256587743759155\n",
      "Epoch 3579: Training Loss: 0.15381311376889548 Validation Loss: 0.7251386046409607\n",
      "Epoch 3580: Training Loss: 0.15365900099277496 Validation Loss: 0.7227602005004883\n",
      "Epoch 3581: Training Loss: 0.15368960301081339 Validation Loss: 0.7229399681091309\n",
      "Epoch 3582: Training Loss: 0.15359801054000854 Validation Loss: 0.7235010862350464\n",
      "Epoch 3583: Training Loss: 0.15398680667082468 Validation Loss: 0.7194355130195618\n",
      "Epoch 3584: Training Loss: 0.1535499095916748 Validation Loss: 0.722030758857727\n",
      "Epoch 3585: Training Loss: 0.15345372259616852 Validation Loss: 0.727223813533783\n",
      "Epoch 3586: Training Loss: 0.1533214251200358 Validation Loss: 0.7275768518447876\n",
      "Epoch 3587: Training Loss: 0.15347625811894736 Validation Loss: 0.7284795045852661\n",
      "Epoch 3588: Training Loss: 0.15325510005156198 Validation Loss: 0.7245243787765503\n",
      "Epoch 3589: Training Loss: 0.15321122109889984 Validation Loss: 0.7237159013748169\n",
      "Epoch 3590: Training Loss: 0.15318325658639273 Validation Loss: 0.7215636372566223\n",
      "Epoch 3591: Training Loss: 0.15303802490234375 Validation Loss: 0.7225754857063293\n",
      "Epoch 3592: Training Loss: 0.15302366018295288 Validation Loss: 0.7255922555923462\n",
      "Epoch 3593: Training Loss: 0.1530994027853012 Validation Loss: 0.7239164113998413\n",
      "Epoch 3594: Training Loss: 0.15290948251883188 Validation Loss: 0.7258933782577515\n",
      "Epoch 3595: Training Loss: 0.1528091530005137 Validation Loss: 0.7263022661209106\n",
      "Epoch 3596: Training Loss: 0.15271337827046713 Validation Loss: 0.7244110107421875\n",
      "Epoch 3597: Training Loss: 0.15260655184586844 Validation Loss: 0.7248055338859558\n",
      "Epoch 3598: Training Loss: 0.15257558723290762 Validation Loss: 0.7252745628356934\n",
      "Epoch 3599: Training Loss: 0.1524915893872579 Validation Loss: 0.724075436592102\n",
      "Epoch 3600: Training Loss: 0.1525732527176539 Validation Loss: 0.722549319267273\n",
      "Epoch 3601: Training Loss: 0.1525071312983831 Validation Loss: 0.7246956825256348\n",
      "Epoch 3602: Training Loss: 0.15238992869853973 Validation Loss: 0.7262120246887207\n",
      "Epoch 3603: Training Loss: 0.1524287462234497 Validation Loss: 0.72278892993927\n",
      "Epoch 3604: Training Loss: 0.15228834499915442 Validation Loss: 0.7218837738037109\n",
      "Epoch 3605: Training Loss: 0.15230380495389303 Validation Loss: 0.7223976850509644\n",
      "Epoch 3606: Training Loss: 0.15222337345282236 Validation Loss: 0.7267434597015381\n",
      "Epoch 3607: Training Loss: 0.1521210918823878 Validation Loss: 0.7266425490379333\n",
      "Epoch 3608: Training Loss: 0.1519471357266108 Validation Loss: 0.7263705730438232\n",
      "Epoch 3609: Training Loss: 0.1519279678662618 Validation Loss: 0.7236974239349365\n",
      "Epoch 3610: Training Loss: 0.1518263171116511 Validation Loss: 0.7230441570281982\n",
      "Epoch 3611: Training Loss: 0.15179307758808136 Validation Loss: 0.7224761843681335\n",
      "Epoch 3612: Training Loss: 0.1517775058746338 Validation Loss: 0.7228662371635437\n",
      "Epoch 3613: Training Loss: 0.15159231424331665 Validation Loss: 0.7252550721168518\n",
      "Epoch 3614: Training Loss: 0.15181395908196768 Validation Loss: 0.7281417846679688\n",
      "Epoch 3615: Training Loss: 0.1516118049621582 Validation Loss: 0.7267705798149109\n",
      "Epoch 3616: Training Loss: 0.15148132046063742 Validation Loss: 0.7241355180740356\n",
      "Epoch 3617: Training Loss: 0.15142656366030374 Validation Loss: 0.7228034734725952\n",
      "Epoch 3618: Training Loss: 0.1515603611866633 Validation Loss: 0.7204437851905823\n",
      "Epoch 3619: Training Loss: 0.15127185980478922 Validation Loss: 0.722049355506897\n",
      "Epoch 3620: Training Loss: 0.1512597252925237 Validation Loss: 0.7264566421508789\n",
      "Epoch 3621: Training Loss: 0.15139959255854288 Validation Loss: 0.7293590307235718\n",
      "Epoch 3622: Training Loss: 0.15131228417158127 Validation Loss: 0.7257494330406189\n",
      "Epoch 3623: Training Loss: 0.1510761876900991 Validation Loss: 0.7230431437492371\n",
      "Epoch 3624: Training Loss: 0.15098131199677786 Validation Loss: 0.7220922112464905\n",
      "Epoch 3625: Training Loss: 0.151004691918691 Validation Loss: 0.721123993396759\n",
      "Epoch 3626: Training Loss: 0.15100290377934775 Validation Loss: 0.7236931324005127\n",
      "Epoch 3627: Training Loss: 0.1507398635149002 Validation Loss: 0.725566565990448\n",
      "Epoch 3628: Training Loss: 0.1507144719362259 Validation Loss: 0.7268519997596741\n",
      "Epoch 3629: Training Loss: 0.1506748596827189 Validation Loss: 0.7256351709365845\n",
      "Epoch 3630: Training Loss: 0.15063134829203287 Validation Loss: 0.7265064120292664\n",
      "Epoch 3631: Training Loss: 0.1505457411209742 Validation Loss: 0.7255343198776245\n",
      "Epoch 3632: Training Loss: 0.15051182607809702 Validation Loss: 0.7253894805908203\n",
      "Epoch 3633: Training Loss: 0.15043140947818756 Validation Loss: 0.7220280766487122\n",
      "Epoch 3634: Training Loss: 0.1504136323928833 Validation Loss: 0.7221903204917908\n",
      "Epoch 3635: Training Loss: 0.150321493546168 Validation Loss: 0.7219371199607849\n",
      "Epoch 3636: Training Loss: 0.15029213825861612 Validation Loss: 0.7244994044303894\n",
      "Epoch 3637: Training Loss: 0.1501775085926056 Validation Loss: 0.7245368957519531\n",
      "Epoch 3638: Training Loss: 0.1501255730787913 Validation Loss: 0.7239223718643188\n",
      "Epoch 3639: Training Loss: 0.15033918619155884 Validation Loss: 0.7223896980285645\n",
      "Epoch 3640: Training Loss: 0.15003321568171182 Validation Loss: 0.7249123454093933\n",
      "Epoch 3641: Training Loss: 0.14991260568300882 Validation Loss: 0.7270834445953369\n",
      "Epoch 3642: Training Loss: 0.15000150601069132 Validation Loss: 0.7260290384292603\n",
      "Epoch 3643: Training Loss: 0.1497874061266581 Validation Loss: 0.7261987328529358\n",
      "Epoch 3644: Training Loss: 0.15000330408414206 Validation Loss: 0.7279523611068726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3645: Training Loss: 0.1496827354033788 Validation Loss: 0.7248856425285339\n",
      "Epoch 3646: Training Loss: 0.14962958296140036 Validation Loss: 0.7219511270523071\n",
      "Epoch 3647: Training Loss: 0.14956206580003104 Validation Loss: 0.7221909165382385\n",
      "Epoch 3648: Training Loss: 0.1495109349489212 Validation Loss: 0.7221644520759583\n",
      "Epoch 3649: Training Loss: 0.14979601403077444 Validation Loss: 0.7209287285804749\n",
      "Epoch 3650: Training Loss: 0.14939765135447183 Validation Loss: 0.7226675152778625\n",
      "Epoch 3651: Training Loss: 0.14936211705207825 Validation Loss: 0.7268644571304321\n",
      "Epoch 3652: Training Loss: 0.149464949965477 Validation Loss: 0.7261314392089844\n",
      "Epoch 3653: Training Loss: 0.14922895034154257 Validation Loss: 0.7260324358940125\n",
      "Epoch 3654: Training Loss: 0.1492016762495041 Validation Loss: 0.7257767915725708\n",
      "Epoch 3655: Training Loss: 0.14927939573923746 Validation Loss: 0.7263889908790588\n",
      "Epoch 3656: Training Loss: 0.1493176966905594 Validation Loss: 0.7271406054496765\n",
      "Epoch 3657: Training Loss: 0.14911376933256784 Validation Loss: 0.7227696180343628\n",
      "Epoch 3658: Training Loss: 0.14928529659907022 Validation Loss: 0.7194836139678955\n",
      "Epoch 3659: Training Loss: 0.1489949623743693 Validation Loss: 0.7216771841049194\n",
      "Epoch 3660: Training Loss: 0.1489003598690033 Validation Loss: 0.7220807075500488\n",
      "Epoch 3661: Training Loss: 0.1487315148115158 Validation Loss: 0.7263293266296387\n",
      "Epoch 3662: Training Loss: 0.14873014638821283 Validation Loss: 0.7264001369476318\n",
      "Epoch 3663: Training Loss: 0.148646742105484 Validation Loss: 0.7263150811195374\n",
      "Epoch 3664: Training Loss: 0.14854570229848227 Validation Loss: 0.7249996662139893\n",
      "Epoch 3665: Training Loss: 0.1484790196021398 Validation Loss: 0.7240609526634216\n",
      "Epoch 3666: Training Loss: 0.14845609664916992 Validation Loss: 0.7241218090057373\n",
      "Epoch 3667: Training Loss: 0.14843811094760895 Validation Loss: 0.7246076464653015\n",
      "Epoch 3668: Training Loss: 0.1484207957983017 Validation Loss: 0.7236002087593079\n",
      "Epoch 3669: Training Loss: 0.14826403061548868 Validation Loss: 0.7253720164299011\n",
      "Epoch 3670: Training Loss: 0.14831649760405222 Validation Loss: 0.7268578410148621\n",
      "Epoch 3671: Training Loss: 0.14825948079427084 Validation Loss: 0.724267303943634\n",
      "Epoch 3672: Training Loss: 0.1481308490037918 Validation Loss: 0.7231709957122803\n",
      "Epoch 3673: Training Loss: 0.14799592643976212 Validation Loss: 0.7228320240974426\n",
      "Epoch 3674: Training Loss: 0.1479324003060659 Validation Loss: 0.7228396534919739\n",
      "Epoch 3675: Training Loss: 0.1479050268729528 Validation Loss: 0.7244187593460083\n",
      "Epoch 3676: Training Loss: 0.1480453908443451 Validation Loss: 0.7275323867797852\n",
      "Epoch 3677: Training Loss: 0.147853950659434 Validation Loss: 0.7254157066345215\n",
      "Epoch 3678: Training Loss: 0.14777145783106485 Validation Loss: 0.7220801711082458\n",
      "Epoch 3679: Training Loss: 0.14771199723084769 Validation Loss: 0.7220978736877441\n",
      "Epoch 3680: Training Loss: 0.14771037797133127 Validation Loss: 0.723242998123169\n",
      "Epoch 3681: Training Loss: 0.1475809762875239 Validation Loss: 0.7243183255195618\n",
      "Epoch 3682: Training Loss: 0.14772920310497284 Validation Loss: 0.7268708348274231\n",
      "Epoch 3683: Training Loss: 0.1475190967321396 Validation Loss: 0.7245108485221863\n",
      "Epoch 3684: Training Loss: 0.14742158353328705 Validation Loss: 0.7235571146011353\n",
      "Epoch 3685: Training Loss: 0.14731422563393912 Validation Loss: 0.7223665118217468\n",
      "Epoch 3686: Training Loss: 0.14733981092770895 Validation Loss: 0.723560094833374\n",
      "Epoch 3687: Training Loss: 0.14734251300493875 Validation Loss: 0.7219940423965454\n",
      "Epoch 3688: Training Loss: 0.14740650355815887 Validation Loss: 0.725135326385498\n",
      "Epoch 3689: Training Loss: 0.14746232827504477 Validation Loss: 0.7274508476257324\n",
      "Epoch 3690: Training Loss: 0.1470820059378942 Validation Loss: 0.7243137359619141\n",
      "Epoch 3691: Training Loss: 0.1470631112655004 Validation Loss: 0.7222100496292114\n",
      "Epoch 3692: Training Loss: 0.1469524453083674 Validation Loss: 0.7220824360847473\n",
      "Epoch 3693: Training Loss: 0.14696122209231058 Validation Loss: 0.7225674986839294\n",
      "Epoch 3694: Training Loss: 0.14697368939717612 Validation Loss: 0.7261041402816772\n",
      "Epoch 3695: Training Loss: 0.14675297339757284 Validation Loss: 0.7265139222145081\n",
      "Epoch 3696: Training Loss: 0.14668330550193787 Validation Loss: 0.7256801128387451\n",
      "Epoch 3697: Training Loss: 0.14685340722401938 Validation Loss: 0.7234076261520386\n",
      "Epoch 3698: Training Loss: 0.14655335247516632 Validation Loss: 0.7233982086181641\n",
      "Epoch 3699: Training Loss: 0.1465605099995931 Validation Loss: 0.7248061895370483\n",
      "Epoch 3700: Training Loss: 0.14650284747282663 Validation Loss: 0.7247672080993652\n",
      "Epoch 3701: Training Loss: 0.1464499831199646 Validation Loss: 0.7248638272285461\n",
      "Epoch 3702: Training Loss: 0.1463408718506495 Validation Loss: 0.7236688733100891\n",
      "Epoch 3703: Training Loss: 0.14626471201578775 Validation Loss: 0.722417414188385\n",
      "Epoch 3704: Training Loss: 0.14637437959512076 Validation Loss: 0.7207669615745544\n",
      "Epoch 3705: Training Loss: 0.14632112781206766 Validation Loss: 0.7238594889640808\n",
      "Epoch 3706: Training Loss: 0.14613809188206991 Validation Loss: 0.724807620048523\n",
      "Epoch 3707: Training Loss: 0.14607317745685577 Validation Loss: 0.7253724932670593\n",
      "Epoch 3708: Training Loss: 0.14596418042977652 Validation Loss: 0.7250350117683411\n",
      "Epoch 3709: Training Loss: 0.14597482482592264 Validation Loss: 0.7238172292709351\n",
      "Epoch 3710: Training Loss: 0.14591885109742483 Validation Loss: 0.7226848006248474\n",
      "Epoch 3711: Training Loss: 0.14617944260438284 Validation Loss: 0.7259143590927124\n",
      "Epoch 3712: Training Loss: 0.14581195513407388 Validation Loss: 0.7255407571792603\n",
      "Epoch 3713: Training Loss: 0.14603158334891 Validation Loss: 0.7213954329490662\n",
      "Epoch 3714: Training Loss: 0.14575166503588358 Validation Loss: 0.722556471824646\n",
      "Epoch 3715: Training Loss: 0.14562528828779855 Validation Loss: 0.7244569659233093\n",
      "Epoch 3716: Training Loss: 0.14557799696922302 Validation Loss: 0.7249965667724609\n",
      "Epoch 3717: Training Loss: 0.14551412562529245 Validation Loss: 0.7256047129631042\n",
      "Epoch 3718: Training Loss: 0.14541264871756235 Validation Loss: 0.7248140573501587\n",
      "Epoch 3719: Training Loss: 0.14542694886525473 Validation Loss: 0.7240524888038635\n",
      "Epoch 3720: Training Loss: 0.14534084995587668 Validation Loss: 0.7232478260993958\n",
      "Epoch 3721: Training Loss: 0.14535164336363474 Validation Loss: 0.7224004864692688\n",
      "Epoch 3722: Training Loss: 0.14521448810895285 Validation Loss: 0.724741518497467\n",
      "Epoch 3723: Training Loss: 0.14527024825414023 Validation Loss: 0.7268739938735962\n",
      "Epoch 3724: Training Loss: 0.14523309469223022 Validation Loss: 0.7245393395423889\n",
      "Epoch 3725: Training Loss: 0.14508979519208273 Validation Loss: 0.723872721195221\n",
      "Epoch 3726: Training Loss: 0.14499108990033469 Validation Loss: 0.7217523455619812\n",
      "Epoch 3727: Training Loss: 0.14495360354582468 Validation Loss: 0.7216425538063049\n",
      "Epoch 3728: Training Loss: 0.14485624680916467 Validation Loss: 0.7224727869033813\n",
      "Epoch 3729: Training Loss: 0.1448466827472051 Validation Loss: 0.7240366339683533\n",
      "Epoch 3730: Training Loss: 0.14474535981814066 Validation Loss: 0.7244452238082886\n",
      "Epoch 3731: Training Loss: 0.14470366140206656 Validation Loss: 0.7262306809425354\n",
      "Epoch 3732: Training Loss: 0.1447329968214035 Validation Loss: 0.7268521785736084\n",
      "Epoch 3733: Training Loss: 0.14463125665982565 Validation Loss: 0.7267710566520691\n",
      "Epoch 3734: Training Loss: 0.14451651771863303 Validation Loss: 0.7243118286132812\n",
      "Epoch 3735: Training Loss: 0.14453383286794028 Validation Loss: 0.7234933376312256\n",
      "Epoch 3736: Training Loss: 0.1444652775923411 Validation Loss: 0.7207949161529541\n",
      "Epoch 3737: Training Loss: 0.14450699587663016 Validation Loss: 0.7234665751457214\n",
      "Epoch 3738: Training Loss: 0.144329234957695 Validation Loss: 0.7238662242889404\n",
      "Epoch 3739: Training Loss: 0.1442475269238154 Validation Loss: 0.7245246171951294\n",
      "Epoch 3740: Training Loss: 0.1445029079914093 Validation Loss: 0.7216020822525024\n",
      "Epoch 3741: Training Loss: 0.14412929117679596 Validation Loss: 0.7224540710449219\n",
      "Epoch 3742: Training Loss: 0.14405140777428946 Validation Loss: 0.7236160635948181\n",
      "Epoch 3743: Training Loss: 0.14401585360368094 Validation Loss: 0.7258158326148987\n",
      "Epoch 3744: Training Loss: 0.14400758097569147 Validation Loss: 0.7253596782684326\n",
      "Epoch 3745: Training Loss: 0.14397421230872473 Validation Loss: 0.7266131043434143\n",
      "Epoch 3746: Training Loss: 0.14387009541193643 Validation Loss: 0.7248877882957458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3747: Training Loss: 0.14384346703688303 Validation Loss: 0.7235249876976013\n",
      "Epoch 3748: Training Loss: 0.14379578332106271 Validation Loss: 0.7223134636878967\n",
      "Epoch 3749: Training Loss: 0.14369472861289978 Validation Loss: 0.7236653566360474\n",
      "Epoch 3750: Training Loss: 0.14372265338897705 Validation Loss: 0.7255722880363464\n",
      "Epoch 3751: Training Loss: 0.14359328399101892 Validation Loss: 0.7256084680557251\n",
      "Epoch 3752: Training Loss: 0.14356389145056406 Validation Loss: 0.7241279482841492\n",
      "Epoch 3753: Training Loss: 0.14346352716286978 Validation Loss: 0.7228975296020508\n",
      "Epoch 3754: Training Loss: 0.1434886654218038 Validation Loss: 0.7221543192863464\n",
      "Epoch 3755: Training Loss: 0.14337358375390372 Validation Loss: 0.7235993146896362\n",
      "Epoch 3756: Training Loss: 0.14334474503993988 Validation Loss: 0.7223774194717407\n",
      "Epoch 3757: Training Loss: 0.14339903990427652 Validation Loss: 0.7252885699272156\n",
      "Epoch 3758: Training Loss: 0.1432245820760727 Validation Loss: 0.7256330251693726\n",
      "Epoch 3759: Training Loss: 0.14317569633324942 Validation Loss: 0.7237502336502075\n",
      "Epoch 3760: Training Loss: 0.14308849473794302 Validation Loss: 0.7238336205482483\n",
      "Epoch 3761: Training Loss: 0.14313031236330667 Validation Loss: 0.7254867553710938\n",
      "Epoch 3762: Training Loss: 0.14302038153012595 Validation Loss: 0.7233418226242065\n",
      "Epoch 3763: Training Loss: 0.14289240539073944 Validation Loss: 0.7233713269233704\n",
      "Epoch 3764: Training Loss: 0.14295212924480438 Validation Loss: 0.7231236100196838\n",
      "Epoch 3765: Training Loss: 0.1428231249252955 Validation Loss: 0.7245535850524902\n",
      "Epoch 3766: Training Loss: 0.1427395741144816 Validation Loss: 0.7244444489479065\n",
      "Epoch 3767: Training Loss: 0.14273637036482492 Validation Loss: 0.7235642671585083\n",
      "Epoch 3768: Training Loss: 0.14267358928918839 Validation Loss: 0.7246204614639282\n",
      "Epoch 3769: Training Loss: 0.1425659880042076 Validation Loss: 0.7241181135177612\n",
      "Epoch 3770: Training Loss: 0.14268170297145844 Validation Loss: 0.7258495688438416\n",
      "Epoch 3771: Training Loss: 0.14249304930369058 Validation Loss: 0.7237465381622314\n",
      "Epoch 3772: Training Loss: 0.14280316730340323 Validation Loss: 0.7200919389724731\n",
      "Epoch 3773: Training Loss: 0.14251220722993216 Validation Loss: 0.7226130366325378\n",
      "Epoch 3774: Training Loss: 0.14246482898791632 Validation Loss: 0.7245858311653137\n",
      "Epoch 3775: Training Loss: 0.14223259687423706 Validation Loss: 0.7255668640136719\n",
      "Epoch 3776: Training Loss: 0.14218940834204355 Validation Loss: 0.7265238761901855\n",
      "Epoch 3777: Training Loss: 0.14216724783182144 Validation Loss: 0.7254095673561096\n",
      "Epoch 3778: Training Loss: 0.14211424191792807 Validation Loss: 0.725799560546875\n",
      "Epoch 3779: Training Loss: 0.14209852615992227 Validation Loss: 0.7243962287902832\n",
      "Epoch 3780: Training Loss: 0.14216222862402597 Validation Loss: 0.7246513366699219\n",
      "Epoch 3781: Training Loss: 0.14188740650812784 Validation Loss: 0.7215074300765991\n",
      "Epoch 3782: Training Loss: 0.14189110696315765 Validation Loss: 0.7198241353034973\n",
      "Epoch 3783: Training Loss: 0.14219006399313608 Validation Loss: 0.7188010811805725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3784: Training Loss: 0.14200110733509064 Validation Loss: 0.7239987254142761\n",
      "Epoch 3785: Training Loss: 0.14169220378001532 Validation Loss: 0.7266504764556885\n",
      "Epoch 3786: Training Loss: 0.14173452307780585 Validation Loss: 0.7280446290969849\n",
      "Epoch 3787: Training Loss: 0.14162976294755936 Validation Loss: 0.7262358665466309\n",
      "Epoch 3788: Training Loss: 0.1416468769311905 Validation Loss: 0.7237141728401184\n",
      "Epoch 3789: Training Loss: 0.14156347016493478 Validation Loss: 0.7224082946777344\n",
      "Epoch 3790: Training Loss: 0.14141258100668588 Validation Loss: 0.7236241698265076\n",
      "Epoch 3791: Training Loss: 0.14135719587405524 Validation Loss: 0.7247480154037476\n",
      "Epoch 3792: Training Loss: 0.14160795013109842 Validation Loss: 0.7278014421463013\n",
      "Epoch 3793: Training Loss: 0.14137754837671915 Validation Loss: 0.7253847122192383\n",
      "Epoch 3794: Training Loss: 0.14128642280896506 Validation Loss: 0.7227353453636169\n",
      "Epoch 3795: Training Loss: 0.14121371507644653 Validation Loss: 0.7225637435913086\n",
      "Epoch 3796: Training Loss: 0.14112409949302673 Validation Loss: 0.7218924164772034\n",
      "Epoch 3797: Training Loss: 0.1410695513089498 Validation Loss: 0.7225744724273682\n",
      "Epoch 3798: Training Loss: 0.1410577048858007 Validation Loss: 0.7215096950531006\n",
      "Epoch 3799: Training Loss: 0.14109246929486594 Validation Loss: 0.7238160967826843\n",
      "Epoch 3800: Training Loss: 0.14093398551146188 Validation Loss: 0.7241555452346802\n",
      "Epoch 3801: Training Loss: 0.14102817823489508 Validation Loss: 0.726521372795105\n",
      "Epoch 3802: Training Loss: 0.14084748923778534 Validation Loss: 0.7254623770713806\n",
      "Epoch 3803: Training Loss: 0.14078939457734427 Validation Loss: 0.724193274974823\n",
      "Epoch 3804: Training Loss: 0.1407012144724528 Validation Loss: 0.7250191569328308\n",
      "Epoch 3805: Training Loss: 0.14072910447915396 Validation Loss: 0.7248330116271973\n",
      "Epoch 3806: Training Loss: 0.14061442762613297 Validation Loss: 0.7246032953262329\n",
      "Epoch 3807: Training Loss: 0.14066332578659058 Validation Loss: 0.7221210598945618\n",
      "Epoch 3808: Training Loss: 0.14054525146881738 Validation Loss: 0.7227819561958313\n",
      "Epoch 3809: Training Loss: 0.14051988472541174 Validation Loss: 0.7241535782814026\n",
      "Epoch 3810: Training Loss: 0.14033598701159158 Validation Loss: 0.7243495583534241\n",
      "Epoch 3811: Training Loss: 0.14032396177450815 Validation Loss: 0.7240244150161743\n",
      "Epoch 3812: Training Loss: 0.14042108257611594 Validation Loss: 0.7216796875\n",
      "Epoch 3813: Training Loss: 0.1403543303410212 Validation Loss: 0.7249459028244019\n",
      "Epoch 3814: Training Loss: 0.1402395119269689 Validation Loss: 0.72432941198349\n",
      "Epoch 3815: Training Loss: 0.14009725550810495 Validation Loss: 0.7251356840133667\n",
      "Epoch 3816: Training Loss: 0.14019557336966196 Validation Loss: 0.7232538461685181\n",
      "Epoch 3817: Training Loss: 0.140125073492527 Validation Loss: 0.722632110118866\n",
      "Epoch 3818: Training Loss: 0.1400445873538653 Validation Loss: 0.7232548594474792\n",
      "Epoch 3819: Training Loss: 0.13988740742206573 Validation Loss: 0.7258573174476624\n",
      "Epoch 3820: Training Loss: 0.13992859919865927 Validation Loss: 0.7270590662956238\n",
      "Epoch 3821: Training Loss: 0.14006774624188742 Validation Loss: 0.7277693152427673\n",
      "Epoch 3822: Training Loss: 0.13989108552535376 Validation Loss: 0.72357177734375\n",
      "Epoch 3823: Training Loss: 0.1396672526995341 Validation Loss: 0.7226117253303528\n",
      "Epoch 3824: Training Loss: 0.13965588808059692 Validation Loss: 0.7220935821533203\n",
      "Epoch 3825: Training Loss: 0.13961241642634073 Validation Loss: 0.7211394309997559\n",
      "Epoch 3826: Training Loss: 0.13967403521140417 Validation Loss: 0.721950888633728\n",
      "Epoch 3827: Training Loss: 0.140157679716746 Validation Loss: 0.7199378609657288\n",
      "Epoch 3828: Training Loss: 0.1394586314757665 Validation Loss: 0.7256323099136353\n",
      "Epoch 3829: Training Loss: 0.1393471658229828 Validation Loss: 0.7292807102203369\n",
      "Epoch 3830: Training Loss: 0.13945461312929788 Validation Loss: 0.7301162481307983\n",
      "Epoch 3831: Training Loss: 0.13944514095783234 Validation Loss: 0.7273432016372681\n",
      "Epoch 3832: Training Loss: 0.13928029934565225 Validation Loss: 0.7233261466026306\n",
      "Epoch 3833: Training Loss: 0.139199232061704 Validation Loss: 0.7218530774116516\n",
      "Epoch 3834: Training Loss: 0.13950693607330322 Validation Loss: 0.7199211120605469\n",
      "Epoch 3835: Training Loss: 0.13923348983128866 Validation Loss: 0.7214058041572571\n",
      "Epoch 3836: Training Loss: 0.13929165403048197 Validation Loss: 0.7263283133506775\n",
      "Epoch 3837: Training Loss: 0.1394884636004766 Validation Loss: 0.7288968563079834\n",
      "Epoch 3838: Training Loss: 0.13899600009123483 Validation Loss: 0.7251212000846863\n",
      "Epoch 3839: Training Loss: 0.13892598698536554 Validation Loss: 0.7212887406349182\n",
      "Epoch 3840: Training Loss: 0.1389041543006897 Validation Loss: 0.719582736492157\n",
      "Epoch 3841: Training Loss: 0.13893807182709375 Validation Loss: 0.7209784984588623\n",
      "Epoch 3842: Training Loss: 0.1388003428777059 Validation Loss: 0.7254266142845154\n",
      "Epoch 3843: Training Loss: 0.13861526548862457 Validation Loss: 0.7263638377189636\n",
      "Epoch 3844: Training Loss: 0.13900581995646158 Validation Loss: 0.7293517589569092\n",
      "Epoch 3845: Training Loss: 0.1386582851409912 Validation Loss: 0.7269415259361267\n",
      "Epoch 3846: Training Loss: 0.1386240174372991 Validation Loss: 0.7224107980728149\n",
      "Epoch 3847: Training Loss: 0.13845245043436685 Validation Loss: 0.7220353484153748\n",
      "Epoch 3848: Training Loss: 0.138410285115242 Validation Loss: 0.7215496301651001\n",
      "Epoch 3849: Training Loss: 0.1383377363284429 Validation Loss: 0.7223626375198364\n",
      "Epoch 3850: Training Loss: 0.13831916451454163 Validation Loss: 0.7221009731292725\n",
      "Epoch 3851: Training Loss: 0.13825217386086783 Validation Loss: 0.7245431542396545\n",
      "Epoch 3852: Training Loss: 0.1382322609424591 Validation Loss: 0.7256924510002136\n",
      "Epoch 3853: Training Loss: 0.13816659897565842 Validation Loss: 0.725377082824707\n",
      "Epoch 3854: Training Loss: 0.13808449606100717 Validation Loss: 0.7240235805511475\n",
      "Epoch 3855: Training Loss: 0.13818841675917307 Validation Loss: 0.722332239151001\n",
      "Epoch 3856: Training Loss: 0.13801953693230948 Validation Loss: 0.7238340377807617\n",
      "Epoch 3857: Training Loss: 0.13794007152318954 Validation Loss: 0.7248157262802124\n",
      "Epoch 3858: Training Loss: 0.1378715286652247 Validation Loss: 0.7256696224212646\n",
      "Epoch 3859: Training Loss: 0.13810553650061289 Validation Loss: 0.7271672487258911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3860: Training Loss: 0.13790296763181686 Validation Loss: 0.7234988212585449\n",
      "Epoch 3861: Training Loss: 0.1377139687538147 Validation Loss: 0.7224212884902954\n",
      "Epoch 3862: Training Loss: 0.13769206156333288 Validation Loss: 0.7220022082328796\n",
      "Epoch 3863: Training Loss: 0.1377407064040502 Validation Loss: 0.72330242395401\n",
      "Epoch 3864: Training Loss: 0.13789852956930795 Validation Loss: 0.7213623523712158\n",
      "Epoch 3865: Training Loss: 0.1375260204076767 Validation Loss: 0.7237940430641174\n",
      "Epoch 3866: Training Loss: 0.13751435528198877 Validation Loss: 0.7268216609954834\n",
      "Epoch 3867: Training Loss: 0.13759167244036993 Validation Loss: 0.726895272731781\n",
      "Epoch 3868: Training Loss: 0.1374352624018987 Validation Loss: 0.7256606221199036\n",
      "Epoch 3869: Training Loss: 0.13750212887922922 Validation Loss: 0.7213887572288513\n",
      "Epoch 3870: Training Loss: 0.13744877527157465 Validation Loss: 0.7232546806335449\n",
      "Epoch 3871: Training Loss: 0.1373480831583341 Validation Loss: 0.7211953997612\n",
      "Epoch 3872: Training Loss: 0.13741171608368555 Validation Loss: 0.7246943712234497\n",
      "Epoch 3873: Training Loss: 0.1372142086426417 Validation Loss: 0.7249972224235535\n",
      "Epoch 3874: Training Loss: 0.13708873589833578 Validation Loss: 0.7240248322486877\n",
      "Epoch 3875: Training Loss: 0.13710642606019974 Validation Loss: 0.7258885502815247\n",
      "Epoch 3876: Training Loss: 0.13708517948786417 Validation Loss: 0.72340989112854\n",
      "Epoch 3877: Training Loss: 0.13693190614382425 Validation Loss: 0.7225034236907959\n",
      "Epoch 3878: Training Loss: 0.13699427247047424 Validation Loss: 0.7224408984184265\n",
      "Epoch 3879: Training Loss: 0.1368822529911995 Validation Loss: 0.7227802276611328\n",
      "Epoch 3880: Training Loss: 0.13674693802992502 Validation Loss: 0.7246919274330139\n",
      "Epoch 3881: Training Loss: 0.13673224548498789 Validation Loss: 0.7258742451667786\n",
      "Epoch 3882: Training Loss: 0.13679935286442438 Validation Loss: 0.7273298501968384\n",
      "Epoch 3883: Training Loss: 0.13679696867863336 Validation Loss: 0.7287315130233765\n",
      "Epoch 3884: Training Loss: 0.13665597389141718 Validation Loss: 0.7258092164993286\n",
      "Epoch 3885: Training Loss: 0.13669241468111673 Validation Loss: 0.7225281000137329\n",
      "Epoch 3886: Training Loss: 0.1365968113144239 Validation Loss: 0.7212860584259033\n",
      "Epoch 3887: Training Loss: 0.1364439775546392 Validation Loss: 0.722239077091217\n",
      "Epoch 3888: Training Loss: 0.13647407790025076 Validation Loss: 0.725610613822937\n",
      "Epoch 3889: Training Loss: 0.13638127346833548 Validation Loss: 0.7260552048683167\n",
      "Epoch 3890: Training Loss: 0.1363558272520701 Validation Loss: 0.725993812084198\n",
      "Epoch 3891: Training Loss: 0.1363493800163269 Validation Loss: 0.7228063344955444\n",
      "Epoch 3892: Training Loss: 0.13613183548053107 Validation Loss: 0.7223570942878723\n",
      "Epoch 3893: Training Loss: 0.13613426685333252 Validation Loss: 0.7226864099502563\n",
      "Epoch 3894: Training Loss: 0.13620492319266 Validation Loss: 0.7215777039527893\n",
      "Epoch 3895: Training Loss: 0.13626381009817123 Validation Loss: 0.7255365252494812\n",
      "Epoch 3896: Training Loss: 0.1362078661719958 Validation Loss: 0.7280171513557434\n",
      "Epoch 3897: Training Loss: 0.13606543093919754 Validation Loss: 0.7266323566436768\n",
      "Epoch 3898: Training Loss: 0.13620069374640784 Validation Loss: 0.7216942310333252\n",
      "Epoch 3899: Training Loss: 0.13604027281204858 Validation Loss: 0.7190337777137756\n",
      "Epoch 3900: Training Loss: 0.13589325547218323 Validation Loss: 0.7217652797698975\n",
      "Epoch 3901: Training Loss: 0.13572250803311667 Validation Loss: 0.7240821123123169\n",
      "Epoch 3902: Training Loss: 0.13577318688233694 Validation Loss: 0.7261191010475159\n",
      "Epoch 3903: Training Loss: 0.13577838987112045 Validation Loss: 0.7245180606842041\n",
      "Epoch 3904: Training Loss: 0.1358002225557963 Validation Loss: 0.727629542350769\n",
      "Epoch 3905: Training Loss: 0.13556019961833954 Validation Loss: 0.7267420291900635\n",
      "Epoch 3906: Training Loss: 0.1354693571726481 Validation Loss: 0.7240130305290222\n",
      "Epoch 3907: Training Loss: 0.1354649911324183 Validation Loss: 0.7226519584655762\n",
      "Epoch 3908: Training Loss: 0.13541863858699799 Validation Loss: 0.7229561805725098\n",
      "Epoch 3909: Training Loss: 0.1353660449385643 Validation Loss: 0.7222795486450195\n",
      "Epoch 3910: Training Loss: 0.13529719412326813 Validation Loss: 0.7245115041732788\n",
      "Epoch 3911: Training Loss: 0.1355616251627604 Validation Loss: 0.7275400161743164\n",
      "Epoch 3912: Training Loss: 0.13523254295190176 Validation Loss: 0.724574625492096\n",
      "Epoch 3913: Training Loss: 0.13531984637180963 Validation Loss: 0.7233746647834778\n",
      "Epoch 3914: Training Loss: 0.13540326058864594 Validation Loss: 0.7189844846725464\n",
      "Epoch 3915: Training Loss: 0.135144313176473 Validation Loss: 0.7210986018180847\n",
      "Epoch 3916: Training Loss: 0.1350384553273519 Validation Loss: 0.7239395380020142\n",
      "Epoch 3917: Training Loss: 0.13507219652334848 Validation Loss: 0.7274232506752014\n",
      "Epoch 3918: Training Loss: 0.1349338839451472 Validation Loss: 0.7280192375183105\n",
      "Epoch 3919: Training Loss: 0.13500693440437317 Validation Loss: 0.7274778485298157\n",
      "Epoch 3920: Training Loss: 0.13482859482367834 Validation Loss: 0.7241429686546326\n",
      "Epoch 3921: Training Loss: 0.1349219928185145 Validation Loss: 0.7200583219528198\n",
      "Epoch 3922: Training Loss: 0.13484222690264383 Validation Loss: 0.7194139957427979\n",
      "Epoch 3923: Training Loss: 0.1347395951549212 Validation Loss: 0.7231230735778809\n",
      "Epoch 3924: Training Loss: 0.13489132622877756 Validation Loss: 0.7280261516571045\n",
      "Epoch 3925: Training Loss: 0.13487703601519266 Validation Loss: 0.7249622344970703\n",
      "Epoch 3926: Training Loss: 0.13458737979332605 Validation Loss: 0.7263447642326355\n",
      "Epoch 3927: Training Loss: 0.1344501574834188 Validation Loss: 0.7246655225753784\n",
      "Epoch 3928: Training Loss: 0.13437486688296 Validation Loss: 0.7235205769538879\n",
      "Epoch 3929: Training Loss: 0.13444982220729193 Validation Loss: 0.7244930267333984\n",
      "Epoch 3930: Training Loss: 0.13429901003837585 Validation Loss: 0.7225497364997864\n",
      "Epoch 3931: Training Loss: 0.13436323404312134 Validation Loss: 0.7237812280654907\n",
      "Epoch 3932: Training Loss: 0.1343019207318624 Validation Loss: 0.7214838266372681\n",
      "Epoch 3933: Training Loss: 0.13422875354687372 Validation Loss: 0.7224767804145813\n",
      "Epoch 3934: Training Loss: 0.13409535338481268 Validation Loss: 0.7231878042221069\n",
      "Epoch 3935: Training Loss: 0.1340294728676478 Validation Loss: 0.7233119010925293\n",
      "Epoch 3936: Training Loss: 0.13407829403877258 Validation Loss: 0.7239772081375122\n",
      "Epoch 3937: Training Loss: 0.13405410200357437 Validation Loss: 0.724338710308075\n",
      "Epoch 3938: Training Loss: 0.13398089756568274 Validation Loss: 0.727222740650177\n",
      "Epoch 3939: Training Loss: 0.13404220094283423 Validation Loss: 0.7248507738113403\n",
      "Epoch 3940: Training Loss: 0.13395535200834274 Validation Loss: 0.7273781299591064\n",
      "Epoch 3941: Training Loss: 0.13379268099864325 Validation Loss: 0.7257044315338135\n",
      "Epoch 3942: Training Loss: 0.13372314472993216 Validation Loss: 0.7243487238883972\n",
      "Epoch 3943: Training Loss: 0.13401190439860025 Validation Loss: 0.7254076600074768\n",
      "Epoch 3944: Training Loss: 0.1338524396220843 Validation Loss: 0.7205280065536499\n",
      "Epoch 3945: Training Loss: 0.13370639830827713 Validation Loss: 0.7215697765350342\n",
      "Epoch 3946: Training Loss: 0.1336962729692459 Validation Loss: 0.7234821319580078\n",
      "Epoch 3947: Training Loss: 0.13374254355827966 Validation Loss: 0.7206235527992249\n",
      "Epoch 3948: Training Loss: 0.13351917763551077 Validation Loss: 0.7237003445625305\n",
      "Epoch 3949: Training Loss: 0.133381021519502 Validation Loss: 0.7242078185081482\n",
      "Epoch 3950: Training Loss: 0.133330337703228 Validation Loss: 0.724588930606842\n",
      "Epoch 3951: Training Loss: 0.13336996734142303 Validation Loss: 0.7268418669700623\n",
      "Epoch 3952: Training Loss: 0.1334445079167684 Validation Loss: 0.7240334153175354\n",
      "Epoch 3953: Training Loss: 0.13331250846385956 Validation Loss: 0.7241581082344055\n",
      "Epoch 3954: Training Loss: 0.1332040180762609 Validation Loss: 0.725881814956665\n",
      "Epoch 3955: Training Loss: 0.13313424835602441 Validation Loss: 0.7236617803573608\n",
      "Epoch 3956: Training Loss: 0.13304449121157327 Validation Loss: 0.7230278253555298\n",
      "Epoch 3957: Training Loss: 0.13309211283922195 Validation Loss: 0.7244863510131836\n",
      "Epoch 3958: Training Loss: 0.13304033875465393 Validation Loss: 0.7220565676689148\n",
      "Epoch 3959: Training Loss: 0.13313431044419607 Validation Loss: 0.7255212664604187\n",
      "Epoch 3960: Training Loss: 0.1328692063689232 Validation Loss: 0.7253984808921814\n",
      "Epoch 3961: Training Loss: 0.132806658744812 Validation Loss: 0.7238594889640808\n",
      "Epoch 3962: Training Loss: 0.1329058756430944 Validation Loss: 0.7253751754760742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3963: Training Loss: 0.13274562855561575 Validation Loss: 0.7233046293258667\n",
      "Epoch 3964: Training Loss: 0.1329356680313746 Validation Loss: 0.7256824970245361\n",
      "Epoch 3965: Training Loss: 0.13263899832963943 Validation Loss: 0.7233222126960754\n",
      "Epoch 3966: Training Loss: 0.1325707584619522 Validation Loss: 0.7231931686401367\n",
      "Epoch 3967: Training Loss: 0.13255595664183298 Validation Loss: 0.7225368618965149\n",
      "Epoch 3968: Training Loss: 0.1327329178651174 Validation Loss: 0.7246941924095154\n",
      "Epoch 3969: Training Loss: 0.13250140845775604 Validation Loss: 0.7235428690910339\n",
      "Epoch 3970: Training Loss: 0.13237273693084717 Validation Loss: 0.7226855754852295\n",
      "Epoch 3971: Training Loss: 0.1323982228835424 Validation Loss: 0.7238155007362366\n",
      "Epoch 3972: Training Loss: 0.13232599198818207 Validation Loss: 0.7235183119773865\n",
      "Epoch 3973: Training Loss: 0.1324739878376325 Validation Loss: 0.7221861481666565\n",
      "Epoch 3974: Training Loss: 0.13225477437178293 Validation Loss: 0.7235351800918579\n",
      "Epoch 3975: Training Loss: 0.13223032156626383 Validation Loss: 0.7248600721359253\n",
      "Epoch 3976: Training Loss: 0.13214227308829626 Validation Loss: 0.7264696955680847\n",
      "Epoch 3977: Training Loss: 0.13207409530878067 Validation Loss: 0.7265426516532898\n",
      "Epoch 3978: Training Loss: 0.13206546505292258 Validation Loss: 0.7242535948753357\n",
      "Epoch 3979: Training Loss: 0.1322089284658432 Validation Loss: 0.721598744392395\n",
      "Epoch 3980: Training Loss: 0.13200595726569495 Validation Loss: 0.7235803008079529\n",
      "Epoch 3981: Training Loss: 0.13189679880936941 Validation Loss: 0.7248802185058594\n",
      "Epoch 3982: Training Loss: 0.13185599446296692 Validation Loss: 0.7262645959854126\n",
      "Epoch 3983: Training Loss: 0.13184356689453125 Validation Loss: 0.7259779572486877\n",
      "Epoch 3984: Training Loss: 0.13179970035950342 Validation Loss: 0.724181592464447\n",
      "Epoch 3985: Training Loss: 0.13173172374566397 Validation Loss: 0.7250022292137146\n",
      "Epoch 3986: Training Loss: 0.13180078814427057 Validation Loss: 0.7232123613357544\n",
      "Epoch 3987: Training Loss: 0.1319702665011088 Validation Loss: 0.7268288731575012\n",
      "Epoch 3988: Training Loss: 0.13157816976308823 Validation Loss: 0.725242018699646\n",
      "Epoch 3989: Training Loss: 0.1317258725563685 Validation Loss: 0.722602128982544\n",
      "Epoch 3990: Training Loss: 0.13161452611287436 Validation Loss: 0.7251970767974854\n",
      "Epoch 3991: Training Loss: 0.1313920021057129 Validation Loss: 0.7248503565788269\n",
      "Epoch 3992: Training Loss: 0.1313583254814148 Validation Loss: 0.7238832712173462\n",
      "Epoch 3993: Training Loss: 0.13128791004419327 Validation Loss: 0.7235686779022217\n",
      "Epoch 3994: Training Loss: 0.13130140056212744 Validation Loss: 0.7222839593887329\n",
      "Epoch 3995: Training Loss: 0.13121269394954047 Validation Loss: 0.7229400873184204\n",
      "Epoch 3996: Training Loss: 0.13123740504185358 Validation Loss: 0.7243141531944275\n",
      "Epoch 3997: Training Loss: 0.13117427130540213 Validation Loss: 0.7249072790145874\n",
      "Epoch 3998: Training Loss: 0.13112347573041916 Validation Loss: 0.7238006591796875\n",
      "Epoch 3999: Training Loss: 0.13124222060044607 Validation Loss: 0.7218764424324036\n",
      "Epoch 4000: Training Loss: 0.1311068112651507 Validation Loss: 0.7241814136505127\n",
      "Epoch 4001: Training Loss: 0.13096892337004343 Validation Loss: 0.7240017056465149\n",
      "Epoch 4002: Training Loss: 0.13089108963807425 Validation Loss: 0.7251662611961365\n",
      "Epoch 4003: Training Loss: 0.1310067450006803 Validation Loss: 0.7275862693786621\n",
      "Epoch 4004: Training Loss: 0.13089358806610107 Validation Loss: 0.7268180847167969\n",
      "Epoch 4005: Training Loss: 0.1308367202679316 Validation Loss: 0.7250086069107056\n",
      "Epoch 4006: Training Loss: 0.13072108229001364 Validation Loss: 0.7240088582038879\n",
      "Epoch 4007: Training Loss: 0.13073049982388815 Validation Loss: 0.7229620218276978\n",
      "Epoch 4008: Training Loss: 0.1306663453578949 Validation Loss: 0.7237315773963928\n",
      "Epoch 4009: Training Loss: 0.13066332042217255 Validation Loss: 0.7258285284042358\n",
      "Epoch 4010: Training Loss: 0.1305909901857376 Validation Loss: 0.7264119386672974\n",
      "Epoch 4011: Training Loss: 0.13063051303227743 Validation Loss: 0.7237173914909363\n",
      "Epoch 4012: Training Loss: 0.13065563639005026 Validation Loss: 0.7224138975143433\n",
      "Epoch 4013: Training Loss: 0.1305370902021726 Validation Loss: 0.7209972143173218\n",
      "Epoch 4014: Training Loss: 0.13043814897537231 Validation Loss: 0.7212347388267517\n",
      "Epoch 4015: Training Loss: 0.13030963142712912 Validation Loss: 0.7252788543701172\n",
      "Epoch 4016: Training Loss: 0.13030187537272772 Validation Loss: 0.7277581095695496\n",
      "Epoch 4017: Training Loss: 0.1303334335486094 Validation Loss: 0.728964626789093\n",
      "Epoch 4018: Training Loss: 0.13029344628254572 Validation Loss: 0.7254601120948792\n",
      "Epoch 4019: Training Loss: 0.13019050657749176 Validation Loss: 0.7253828644752502\n",
      "Epoch 4020: Training Loss: 0.1301172897219658 Validation Loss: 0.7227212190628052\n",
      "Epoch 4021: Training Loss: 0.13021066536506018 Validation Loss: 0.7203686237335205\n",
      "Epoch 4022: Training Loss: 0.13017933815717697 Validation Loss: 0.7204990983009338\n",
      "Epoch 4023: Training Loss: 0.13022667666276297 Validation Loss: 0.7257266044616699\n",
      "Epoch 4024: Training Loss: 0.1299425239364306 Validation Loss: 0.7263145446777344\n",
      "Epoch 4025: Training Loss: 0.12991600235303244 Validation Loss: 0.7251261472702026\n",
      "Epoch 4026: Training Loss: 0.12983484814564386 Validation Loss: 0.7264721989631653\n",
      "Epoch 4027: Training Loss: 0.12986751894156137 Validation Loss: 0.7248053550720215\n",
      "Epoch 4028: Training Loss: 0.12985920906066895 Validation Loss: 0.7230621576309204\n",
      "Epoch 4029: Training Loss: 0.12985526025295258 Validation Loss: 0.7255513072013855\n",
      "Epoch 4030: Training Loss: 0.12971940139929453 Validation Loss: 0.7254362106323242\n",
      "Epoch 4031: Training Loss: 0.12962228804826736 Validation Loss: 0.7238938808441162\n",
      "Epoch 4032: Training Loss: 0.12962098916371664 Validation Loss: 0.7231327891349792\n",
      "Epoch 4033: Training Loss: 0.12968003749847412 Validation Loss: 0.7240067720413208\n",
      "Epoch 4034: Training Loss: 0.12947012732426325 Validation Loss: 0.7238746285438538\n",
      "Epoch 4035: Training Loss: 0.1294011945525805 Validation Loss: 0.7234746813774109\n",
      "Epoch 4036: Training Loss: 0.12938721974690756 Validation Loss: 0.7237142324447632\n",
      "Epoch 4037: Training Loss: 0.12933888286352158 Validation Loss: 0.7249418497085571\n",
      "Epoch 4038: Training Loss: 0.1293334017197291 Validation Loss: 0.7245832085609436\n",
      "Epoch 4039: Training Loss: 0.1292522375782331 Validation Loss: 0.7235790491104126\n",
      "Epoch 4040: Training Loss: 0.1292902206381162 Validation Loss: 0.7259984612464905\n",
      "Epoch 4041: Training Loss: 0.1293691024184227 Validation Loss: 0.7229973673820496\n",
      "Epoch 4042: Training Loss: 0.1291011373202006 Validation Loss: 0.7239256501197815\n",
      "Epoch 4043: Training Loss: 0.12907013297080994 Validation Loss: 0.7249643802642822\n",
      "Epoch 4044: Training Loss: 0.12913785874843597 Validation Loss: 0.7235561609268188\n",
      "Epoch 4045: Training Loss: 0.1289638727903366 Validation Loss: 0.7231751084327698\n",
      "Epoch 4046: Training Loss: 0.12895986686150232 Validation Loss: 0.7232961058616638\n",
      "Epoch 4047: Training Loss: 0.1290715585152308 Validation Loss: 0.7276418209075928\n",
      "Epoch 4048: Training Loss: 0.12890198330084482 Validation Loss: 0.7281119227409363\n",
      "Epoch 4049: Training Loss: 0.12889833003282547 Validation Loss: 0.7281007766723633\n",
      "Epoch 4050: Training Loss: 0.12888907392819723 Validation Loss: 0.7234963178634644\n",
      "Epoch 4051: Training Loss: 0.12875171502431235 Validation Loss: 0.7220306396484375\n",
      "Epoch 4052: Training Loss: 0.1286894455552101 Validation Loss: 0.7220053672790527\n",
      "Epoch 4053: Training Loss: 0.12858962019284567 Validation Loss: 0.7226443886756897\n",
      "Epoch 4054: Training Loss: 0.128565343717734 Validation Loss: 0.7236146926879883\n",
      "Epoch 4055: Training Loss: 0.12856176992257437 Validation Loss: 0.7261253595352173\n",
      "Epoch 4056: Training Loss: 0.12854153166214624 Validation Loss: 0.7248542904853821\n",
      "Epoch 4057: Training Loss: 0.1285340835650762 Validation Loss: 0.7241719961166382\n",
      "Epoch 4058: Training Loss: 0.12840110063552856 Validation Loss: 0.7238079905509949\n",
      "Epoch 4059: Training Loss: 0.128438929716746 Validation Loss: 0.7244817018508911\n",
      "Epoch 4060: Training Loss: 0.12831129133701324 Validation Loss: 0.7241292595863342\n",
      "Epoch 4061: Training Loss: 0.12828168272972107 Validation Loss: 0.7247621417045593\n",
      "Epoch 4062: Training Loss: 0.12819792330265045 Validation Loss: 0.7253263592720032\n",
      "Epoch 4063: Training Loss: 0.12821509192387262 Validation Loss: 0.7245742678642273\n",
      "Epoch 4064: Training Loss: 0.128087284664313 Validation Loss: 0.7247887253761292\n",
      "Epoch 4065: Training Loss: 0.12826162576675415 Validation Loss: 0.7272107601165771\n",
      "Epoch 4066: Training Loss: 0.12859426935513815 Validation Loss: 0.7219474911689758\n",
      "Epoch 4067: Training Loss: 0.1280572141210238 Validation Loss: 0.7217525243759155\n",
      "Epoch 4068: Training Loss: 0.12806702156861624 Validation Loss: 0.722691535949707\n",
      "Epoch 4069: Training Loss: 0.1278925413886706 Validation Loss: 0.724555253982544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4070: Training Loss: 0.12783595422903696 Validation Loss: 0.7274598479270935\n",
      "Epoch 4071: Training Loss: 0.12788903216520944 Validation Loss: 0.7266494035720825\n",
      "Epoch 4072: Training Loss: 0.12788321574529013 Validation Loss: 0.7254806160926819\n",
      "Epoch 4073: Training Loss: 0.12807427843411764 Validation Loss: 0.7221589684486389\n",
      "Epoch 4074: Training Loss: 0.12779458860556284 Validation Loss: 0.7259699702262878\n",
      "Epoch 4075: Training Loss: 0.1276814565062523 Validation Loss: 0.7266618013381958\n",
      "Epoch 4076: Training Loss: 0.12758720169464746 Validation Loss: 0.7268255949020386\n",
      "Epoch 4077: Training Loss: 0.12772895147403082 Validation Loss: 0.7247635126113892\n",
      "Epoch 4078: Training Loss: 0.12754988918701807 Validation Loss: 0.7227409482002258\n",
      "Epoch 4079: Training Loss: 0.1276633863647779 Validation Loss: 0.7256340980529785\n",
      "Epoch 4080: Training Loss: 0.12745076417922974 Validation Loss: 0.7256020307540894\n",
      "Epoch 4081: Training Loss: 0.12747063984473547 Validation Loss: 0.7230278849601746\n",
      "Epoch 4082: Training Loss: 0.1274232566356659 Validation Loss: 0.7239475250244141\n",
      "Epoch 4083: Training Loss: 0.12732929488023123 Validation Loss: 0.7252535223960876\n",
      "Epoch 4084: Training Loss: 0.12725785871346793 Validation Loss: 0.724835216999054\n",
      "Epoch 4085: Training Loss: 0.1274526963631312 Validation Loss: 0.7251930832862854\n",
      "Epoch 4086: Training Loss: 0.127173642317454 Validation Loss: 0.7231274247169495\n",
      "Epoch 4087: Training Loss: 0.12719657023747763 Validation Loss: 0.7216483354568481\n",
      "Epoch 4088: Training Loss: 0.12740774204333624 Validation Loss: 0.7250741124153137\n",
      "Epoch 4089: Training Loss: 0.12717999269564947 Validation Loss: 0.7271938920021057\n",
      "Epoch 4090: Training Loss: 0.1269662951429685 Validation Loss: 0.7249253988265991\n",
      "Epoch 4091: Training Loss: 0.1269630566239357 Validation Loss: 0.7227956056594849\n",
      "Epoch 4092: Training Loss: 0.1269347369670868 Validation Loss: 0.722599446773529\n",
      "Epoch 4093: Training Loss: 0.1268500586350759 Validation Loss: 0.7230299711227417\n",
      "Epoch 4094: Training Loss: 0.12695362667242685 Validation Loss: 0.7256829142570496\n",
      "Epoch 4095: Training Loss: 0.1267962952454885 Validation Loss: 0.7258442640304565\n",
      "Epoch 4096: Training Loss: 0.12686597059170404 Validation Loss: 0.7249273061752319\n",
      "Epoch 4097: Training Loss: 0.1267271265387535 Validation Loss: 0.72321617603302\n",
      "Epoch 4098: Training Loss: 0.1268161634604136 Validation Loss: 0.7213213443756104\n",
      "Epoch 4099: Training Loss: 0.12666398535172144 Validation Loss: 0.7243717312812805\n",
      "Epoch 4100: Training Loss: 0.1265935699144999 Validation Loss: 0.7263184785842896\n",
      "Epoch 4101: Training Loss: 0.12660598258177438 Validation Loss: 0.7245296835899353\n",
      "Epoch 4102: Training Loss: 0.12648002554972967 Validation Loss: 0.7256315350532532\n",
      "Epoch 4103: Training Loss: 0.12647159894307455 Validation Loss: 0.7259073257446289\n",
      "Epoch 4104: Training Loss: 0.1264655664563179 Validation Loss: 0.7241041660308838\n",
      "Epoch 4105: Training Loss: 0.12646049757798514 Validation Loss: 0.7239713072776794\n",
      "Epoch 4106: Training Loss: 0.1264385258158048 Validation Loss: 0.722524106502533\n",
      "Epoch 4107: Training Loss: 0.12628348420063654 Validation Loss: 0.7249516248703003\n",
      "Epoch 4108: Training Loss: 0.12630725155274072 Validation Loss: 0.7273054718971252\n",
      "Epoch 4109: Training Loss: 0.12632839133342108 Validation Loss: 0.7250226736068726\n",
      "Epoch 4110: Training Loss: 0.12614998718102774 Validation Loss: 0.7251341342926025\n",
      "Epoch 4111: Training Loss: 0.12637339532375336 Validation Loss: 0.7269089818000793\n",
      "Epoch 4112: Training Loss: 0.1262005940079689 Validation Loss: 0.7240031361579895\n",
      "Epoch 4113: Training Loss: 0.12602430085341135 Validation Loss: 0.7232083678245544\n",
      "Epoch 4114: Training Loss: 0.12602296471595764 Validation Loss: 0.7248639464378357\n",
      "Epoch 4115: Training Loss: 0.125932348271211 Validation Loss: 0.723456621170044\n",
      "Epoch 4116: Training Loss: 0.1259391208489736 Validation Loss: 0.7241446375846863\n",
      "Epoch 4117: Training Loss: 0.12600673486789069 Validation Loss: 0.7244907021522522\n",
      "Epoch 4118: Training Loss: 0.12590303272008896 Validation Loss: 0.7270331978797913\n",
      "Epoch 4119: Training Loss: 0.12591438740491867 Validation Loss: 0.7240008115768433\n",
      "Epoch 4120: Training Loss: 0.12574163327614465 Validation Loss: 0.7243930101394653\n",
      "Epoch 4121: Training Loss: 0.12569363166888556 Validation Loss: 0.7240855097770691\n",
      "Epoch 4122: Training Loss: 0.12564709782600403 Validation Loss: 0.72301185131073\n",
      "Epoch 4123: Training Loss: 0.12568728625774384 Validation Loss: 0.725749671459198\n",
      "Epoch 4124: Training Loss: 0.125670425593853 Validation Loss: 0.7272939085960388\n",
      "Epoch 4125: Training Loss: 0.125522310535113 Validation Loss: 0.7260444164276123\n",
      "Epoch 4126: Training Loss: 0.12541872262954712 Validation Loss: 0.7241166830062866\n",
      "Epoch 4127: Training Loss: 0.12551232675711313 Validation Loss: 0.7211597561836243\n",
      "Epoch 4128: Training Loss: 0.12540317326784134 Validation Loss: 0.7222669720649719\n",
      "Epoch 4129: Training Loss: 0.12543324877818426 Validation Loss: 0.7244151830673218\n",
      "Epoch 4130: Training Loss: 0.12531289954980215 Validation Loss: 0.7247478365898132\n",
      "Epoch 4131: Training Loss: 0.1253031368056933 Validation Loss: 0.7238725423812866\n",
      "Epoch 4132: Training Loss: 0.12526272237300873 Validation Loss: 0.7242054343223572\n",
      "Epoch 4133: Training Loss: 0.1253798852364222 Validation Loss: 0.7231186628341675\n",
      "Epoch 4134: Training Loss: 0.1250943864385287 Validation Loss: 0.726192057132721\n",
      "Epoch 4135: Training Loss: 0.12523100276788077 Validation Loss: 0.7267135977745056\n",
      "Epoch 4136: Training Loss: 0.12507256617148718 Validation Loss: 0.7283662557601929\n",
      "Epoch 4137: Training Loss: 0.12510541081428528 Validation Loss: 0.7275382876396179\n",
      "Epoch 4138: Training Loss: 0.12506775806347528 Validation Loss: 0.7245979309082031\n",
      "Epoch 4139: Training Loss: 0.12492054949204127 Validation Loss: 0.7249402403831482\n",
      "Epoch 4140: Training Loss: 0.12506897002458572 Validation Loss: 0.7266652584075928\n",
      "Epoch 4141: Training Loss: 0.12484050790468852 Validation Loss: 0.725638210773468\n",
      "Epoch 4142: Training Loss: 0.1248355582356453 Validation Loss: 0.7246371507644653\n",
      "Epoch 4143: Training Loss: 0.12472323328256607 Validation Loss: 0.7225344777107239\n",
      "Epoch 4144: Training Loss: 0.12503885726133981 Validation Loss: 0.719469428062439\n",
      "Epoch 4145: Training Loss: 0.12474287301301956 Validation Loss: 0.7215617895126343\n",
      "Epoch 4146: Training Loss: 0.12477302302916844 Validation Loss: 0.7258480787277222\n",
      "Epoch 4147: Training Loss: 0.12474229435125987 Validation Loss: 0.7292754650115967\n",
      "Epoch 4148: Training Loss: 0.12465839833021164 Validation Loss: 0.729427695274353\n",
      "Epoch 4149: Training Loss: 0.12457208832105 Validation Loss: 0.7266068458557129\n",
      "Epoch 4150: Training Loss: 0.12450317045052846 Validation Loss: 0.7249684929847717\n",
      "Epoch 4151: Training Loss: 0.12464125702778499 Validation Loss: 0.7200060486793518\n",
      "Epoch 4152: Training Loss: 0.1245695302883784 Validation Loss: 0.7219597697257996\n",
      "Epoch 4153: Training Loss: 0.12451305240392685 Validation Loss: 0.7216491103172302\n",
      "Epoch 4154: Training Loss: 0.12444975972175598 Validation Loss: 0.7209736704826355\n",
      "Epoch 4155: Training Loss: 0.12424331158399582 Validation Loss: 0.7252711653709412\n",
      "Epoch 4156: Training Loss: 0.12434937804937363 Validation Loss: 0.7258179187774658\n",
      "Epoch 4157: Training Loss: 0.12418210258086522 Validation Loss: 0.7284486293792725\n",
      "Epoch 4158: Training Loss: 0.12432002524534862 Validation Loss: 0.7301051616668701\n",
      "Epoch 4159: Training Loss: 0.12423501412073772 Validation Loss: 0.7296246886253357\n",
      "Epoch 4160: Training Loss: 0.12421165406703949 Validation Loss: 0.7242469191551208\n",
      "Epoch 4161: Training Loss: 0.12440738081932068 Validation Loss: 0.7199332118034363\n",
      "Epoch 4162: Training Loss: 0.12428067872921626 Validation Loss: 0.7237783074378967\n",
      "Epoch 4163: Training Loss: 0.12389499694108963 Validation Loss: 0.7244088649749756\n",
      "Epoch 4164: Training Loss: 0.12384986132383347 Validation Loss: 0.7247878909111023\n",
      "Epoch 4165: Training Loss: 0.12388608107964198 Validation Loss: 0.7262343168258667\n",
      "Epoch 4166: Training Loss: 0.12390273561080296 Validation Loss: 0.7246792912483215\n",
      "Epoch 4167: Training Loss: 0.12376088400681813 Validation Loss: 0.7246019244194031\n",
      "Epoch 4168: Training Loss: 0.1237986187140147 Validation Loss: 0.7237560153007507\n",
      "Epoch 4169: Training Loss: 0.12377393990755081 Validation Loss: 0.7250625491142273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4170: Training Loss: 0.12388101716836293 Validation Loss: 0.7243496179580688\n",
      "Epoch 4171: Training Loss: 0.12356547762950261 Validation Loss: 0.7254348397254944\n",
      "Epoch 4172: Training Loss: 0.12363667786121368 Validation Loss: 0.7275874018669128\n",
      "Epoch 4173: Training Loss: 0.12363148232301076 Validation Loss: 0.7262322902679443\n",
      "Epoch 4174: Training Loss: 0.12356298665205638 Validation Loss: 0.7254073023796082\n",
      "Epoch 4175: Training Loss: 0.1234595129887263 Validation Loss: 0.7247549295425415\n",
      "Epoch 4176: Training Loss: 0.12359044204155605 Validation Loss: 0.725670337677002\n",
      "Epoch 4177: Training Loss: 0.12337344884872437 Validation Loss: 0.7257335186004639\n",
      "Epoch 4178: Training Loss: 0.12335419903198878 Validation Loss: 0.7257733345031738\n",
      "Epoch 4179: Training Loss: 0.1232477401693662 Validation Loss: 0.7234306335449219\n",
      "Epoch 4180: Training Loss: 0.12320312360922496 Validation Loss: 0.7220799922943115\n",
      "Epoch 4181: Training Loss: 0.12328139692544937 Validation Loss: 0.7218716740608215\n",
      "Epoch 4182: Training Loss: 0.12319611757993698 Validation Loss: 0.7219887971878052\n",
      "Epoch 4183: Training Loss: 0.12359033028284709 Validation Loss: 0.7278355956077576\n",
      "Epoch 4184: Training Loss: 0.1231163963675499 Validation Loss: 0.7272681593894958\n",
      "Epoch 4185: Training Loss: 0.12306203941504161 Validation Loss: 0.7253374457359314\n",
      "Epoch 4186: Training Loss: 0.12301015108823776 Validation Loss: 0.725239634513855\n",
      "Epoch 4187: Training Loss: 0.12303841610749562 Validation Loss: 0.7266106009483337\n",
      "Epoch 4188: Training Loss: 0.12294681370258331 Validation Loss: 0.7266129851341248\n",
      "Epoch 4189: Training Loss: 0.12283114343881607 Validation Loss: 0.7250937223434448\n",
      "Epoch 4190: Training Loss: 0.12293686469395955 Validation Loss: 0.7245422005653381\n",
      "Epoch 4191: Training Loss: 0.1227325772245725 Validation Loss: 0.723779559135437\n",
      "Epoch 4192: Training Loss: 0.12271937231222789 Validation Loss: 0.723388135433197\n",
      "Epoch 4193: Training Loss: 0.12268061935901642 Validation Loss: 0.7234166860580444\n",
      "Epoch 4194: Training Loss: 0.12264942874511083 Validation Loss: 0.7233277559280396\n",
      "Epoch 4195: Training Loss: 0.12277011573314667 Validation Loss: 0.7258349061012268\n",
      "Epoch 4196: Training Loss: 0.1226327195763588 Validation Loss: 0.7269759178161621\n",
      "Epoch 4197: Training Loss: 0.1227051814397176 Validation Loss: 0.7239514589309692\n",
      "Epoch 4198: Training Loss: 0.12256082147359848 Validation Loss: 0.7234100103378296\n",
      "Epoch 4199: Training Loss: 0.12259809176127116 Validation Loss: 0.7224462628364563\n",
      "Epoch 4200: Training Loss: 0.12247216949860255 Validation Loss: 0.7258219718933105\n",
      "Epoch 4201: Training Loss: 0.12285057703653972 Validation Loss: 0.7297717332839966\n",
      "Epoch 4202: Training Loss: 0.12258343895276387 Validation Loss: 0.7291275858879089\n",
      "Epoch 4203: Training Loss: 0.12231747061014175 Validation Loss: 0.7253868579864502\n",
      "Epoch 4204: Training Loss: 0.12225955228010814 Validation Loss: 0.7219652533531189\n",
      "Epoch 4205: Training Loss: 0.12231043974558513 Validation Loss: 0.7215883135795593\n",
      "Epoch 4206: Training Loss: 0.12225708117087682 Validation Loss: 0.7211328744888306\n",
      "Epoch 4207: Training Loss: 0.12221475193897884 Validation Loss: 0.7225276231765747\n",
      "Epoch 4208: Training Loss: 0.12211601436138153 Validation Loss: 0.7240734696388245\n",
      "Epoch 4209: Training Loss: 0.12215990573167801 Validation Loss: 0.7237207293510437\n",
      "Epoch 4210: Training Loss: 0.12201620638370514 Validation Loss: 0.7264485359191895\n",
      "Epoch 4211: Training Loss: 0.12214660396178563 Validation Loss: 0.7297585010528564\n",
      "Epoch 4212: Training Loss: 0.12201613932847977 Validation Loss: 0.7290175557136536\n",
      "Epoch 4213: Training Loss: 0.12233415991067886 Validation Loss: 0.723461389541626\n",
      "Epoch 4214: Training Loss: 0.12205244849125545 Validation Loss: 0.7217418551445007\n",
      "Epoch 4215: Training Loss: 0.12198033432165782 Validation Loss: 0.7220270037651062\n",
      "Epoch 4216: Training Loss: 0.121889462073644 Validation Loss: 0.7268185019493103\n",
      "Epoch 4217: Training Loss: 0.12176071107387543 Validation Loss: 0.7275574803352356\n",
      "Epoch 4218: Training Loss: 0.12177210052808125 Validation Loss: 0.7283400893211365\n",
      "Epoch 4219: Training Loss: 0.12199066330989201 Validation Loss: 0.7288508415222168\n",
      "Epoch 4220: Training Loss: 0.12177268167336781 Validation Loss: 0.7255318760871887\n",
      "Epoch 4221: Training Loss: 0.12157693753639857 Validation Loss: 0.7233099937438965\n",
      "Epoch 4222: Training Loss: 0.12156313409407933 Validation Loss: 0.7222333550453186\n",
      "Epoch 4223: Training Loss: 0.12163865814606349 Validation Loss: 0.723168671131134\n",
      "Epoch 4224: Training Loss: 0.12149198601643245 Validation Loss: 0.7257643938064575\n",
      "Epoch 4225: Training Loss: 0.1214850942293803 Validation Loss: 0.7271155714988708\n",
      "Epoch 4226: Training Loss: 0.12142914285262425 Validation Loss: 0.7271315455436707\n",
      "Epoch 4227: Training Loss: 0.1215620090564092 Validation Loss: 0.7281115651130676\n",
      "Epoch 4228: Training Loss: 0.12176820387442906 Validation Loss: 0.7297052145004272\n",
      "Epoch 4229: Training Loss: 0.12146425743897755 Validation Loss: 0.7232407927513123\n",
      "Epoch 4230: Training Loss: 0.12121801823377609 Validation Loss: 0.7209960222244263\n",
      "Epoch 4231: Training Loss: 0.12129478653271993 Validation Loss: 0.7208889126777649\n",
      "Epoch 4232: Training Loss: 0.12120143324136734 Validation Loss: 0.7226352691650391\n",
      "Epoch 4233: Training Loss: 0.12123339871565501 Validation Loss: 0.7256128787994385\n",
      "Epoch 4234: Training Loss: 0.12137310951948166 Validation Loss: 0.729319155216217\n",
      "Epoch 4235: Training Loss: 0.12114062408606212 Validation Loss: 0.7290756106376648\n",
      "Epoch 4236: Training Loss: 0.12133070826530457 Validation Loss: 0.7237339019775391\n",
      "Epoch 4237: Training Loss: 0.12123297403256099 Validation Loss: 0.7207798361778259\n",
      "Epoch 4238: Training Loss: 0.121299942334493 Validation Loss: 0.7250289916992188\n",
      "Epoch 4239: Training Loss: 0.12112245460351308 Validation Loss: 0.7246147394180298\n",
      "Epoch 4240: Training Loss: 0.12149260193109512 Validation Loss: 0.7294406294822693\n",
      "Epoch 4241: Training Loss: 0.12100555251042049 Validation Loss: 0.7270057201385498\n",
      "Epoch 4242: Training Loss: 0.12071023384730022 Validation Loss: 0.7252852916717529\n",
      "Epoch 4243: Training Loss: 0.12097802261511485 Validation Loss: 0.7225407361984253\n",
      "Epoch 4244: Training Loss: 0.12080618490775426 Validation Loss: 0.7243553400039673\n",
      "Epoch 4245: Training Loss: 0.12068828692038854 Validation Loss: 0.725383996963501\n",
      "Epoch 4246: Training Loss: 0.12063852945963542 Validation Loss: 0.7260910868644714\n",
      "Epoch 4247: Training Loss: 0.12068495651086171 Validation Loss: 0.7236893177032471\n",
      "Epoch 4248: Training Loss: 0.12068299452463786 Validation Loss: 0.7224396467208862\n",
      "Epoch 4249: Training Loss: 0.12060088664293289 Validation Loss: 0.7256398797035217\n",
      "Epoch 4250: Training Loss: 0.12065258622169495 Validation Loss: 0.728333055973053\n",
      "Epoch 4251: Training Loss: 0.1206138829390208 Validation Loss: 0.7248833775520325\n",
      "Epoch 4252: Training Loss: 0.12041935821374257 Validation Loss: 0.7237756252288818\n",
      "Epoch 4253: Training Loss: 0.12032850831747055 Validation Loss: 0.725225031375885\n",
      "Epoch 4254: Training Loss: 0.12026921411355336 Validation Loss: 0.7256689667701721\n",
      "Epoch 4255: Training Loss: 0.12039723992347717 Validation Loss: 0.7247092127799988\n",
      "Epoch 4256: Training Loss: 0.12032767136891682 Validation Loss: 0.727182924747467\n",
      "Epoch 4257: Training Loss: 0.12023457139730453 Validation Loss: 0.7274460792541504\n",
      "Epoch 4258: Training Loss: 0.12031486382087071 Validation Loss: 0.7237600088119507\n",
      "Epoch 4259: Training Loss: 0.1201815331975619 Validation Loss: 0.7250897884368896\n",
      "Epoch 4260: Training Loss: 0.12012510001659393 Validation Loss: 0.7238137722015381\n",
      "Epoch 4261: Training Loss: 0.12024121979872386 Validation Loss: 0.7219683527946472\n",
      "Epoch 4262: Training Loss: 0.12026053915421168 Validation Loss: 0.7265324592590332\n",
      "Epoch 4263: Training Loss: 0.12000559270381927 Validation Loss: 0.7262758612632751\n",
      "Epoch 4264: Training Loss: 0.12013593564430873 Validation Loss: 0.7290087342262268\n",
      "Epoch 4265: Training Loss: 0.1199447438120842 Validation Loss: 0.7287254929542542\n",
      "Epoch 4266: Training Loss: 0.11982988814512889 Validation Loss: 0.7262336015701294\n",
      "Epoch 4267: Training Loss: 0.11977226535479228 Validation Loss: 0.7235808372497559\n",
      "Epoch 4268: Training Loss: 0.1197846531867981 Validation Loss: 0.7225300669670105\n",
      "Epoch 4269: Training Loss: 0.11989182730515797 Validation Loss: 0.7203329801559448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4270: Training Loss: 0.1197407195965449 Validation Loss: 0.7235147953033447\n",
      "Epoch 4271: Training Loss: 0.11976689845323563 Validation Loss: 0.7275383472442627\n",
      "Epoch 4272: Training Loss: 0.1196994533141454 Validation Loss: 0.7270933389663696\n",
      "Epoch 4273: Training Loss: 0.11974609394868214 Validation Loss: 0.7272594571113586\n",
      "Epoch 4274: Training Loss: 0.1195249433318774 Validation Loss: 0.7269675135612488\n",
      "Epoch 4275: Training Loss: 0.11983879903952281 Validation Loss: 0.7234777212142944\n",
      "Epoch 4276: Training Loss: 0.11958475162585576 Validation Loss: 0.7229139804840088\n",
      "Epoch 4277: Training Loss: 0.11968456953763962 Validation Loss: 0.7266393899917603\n",
      "Epoch 4278: Training Loss: 0.11965678880612056 Validation Loss: 0.7287808656692505\n",
      "Epoch 4279: Training Loss: 0.11949843913316727 Validation Loss: 0.7256444096565247\n",
      "Epoch 4280: Training Loss: 0.11940332253774007 Validation Loss: 0.7252775430679321\n",
      "Epoch 4281: Training Loss: 0.11941183855136235 Validation Loss: 0.7232237458229065\n",
      "Epoch 4282: Training Loss: 0.11937813212474187 Validation Loss: 0.722026526927948\n",
      "Epoch 4283: Training Loss: 0.11921410510937373 Validation Loss: 0.7241400480270386\n",
      "Epoch 4284: Training Loss: 0.11915601293245952 Validation Loss: 0.7278141975402832\n",
      "Epoch 4285: Training Loss: 0.1192149172226588 Validation Loss: 0.7268373370170593\n",
      "Epoch 4286: Training Loss: 0.11906683444976807 Validation Loss: 0.7265001535415649\n",
      "Epoch 4287: Training Loss: 0.11909518142541249 Validation Loss: 0.7277498245239258\n",
      "Epoch 4288: Training Loss: 0.1190738429625829 Validation Loss: 0.7257989048957825\n",
      "Epoch 4289: Training Loss: 0.11895668258269627 Validation Loss: 0.7263118028640747\n",
      "Epoch 4290: Training Loss: 0.11909302324056625 Validation Loss: 0.7279313802719116\n",
      "Epoch 4291: Training Loss: 0.11891465882460277 Validation Loss: 0.7255470752716064\n",
      "Epoch 4292: Training Loss: 0.11888404190540314 Validation Loss: 0.7243845462799072\n",
      "Epoch 4293: Training Loss: 0.11924764017264049 Validation Loss: 0.7203783392906189\n",
      "Epoch 4294: Training Loss: 0.11891719450553258 Validation Loss: 0.7233917713165283\n",
      "Epoch 4295: Training Loss: 0.11882769813140233 Validation Loss: 0.7256112694740295\n",
      "Epoch 4296: Training Loss: 0.11865407228469849 Validation Loss: 0.7269737720489502\n",
      "Epoch 4297: Training Loss: 0.11870531737804413 Validation Loss: 0.7284592986106873\n",
      "Epoch 4298: Training Loss: 0.11863647649685542 Validation Loss: 0.727764904499054\n",
      "Epoch 4299: Training Loss: 0.11859653393427531 Validation Loss: 0.7281577587127686\n",
      "Epoch 4300: Training Loss: 0.1186291053891182 Validation Loss: 0.7278065085411072\n",
      "Epoch 4301: Training Loss: 0.11857292552789052 Validation Loss: 0.7245803475379944\n",
      "Epoch 4302: Training Loss: 0.11857634037733078 Validation Loss: 0.722443163394928\n",
      "Epoch 4303: Training Loss: 0.11845840762058894 Validation Loss: 0.7226359844207764\n",
      "Epoch 4304: Training Loss: 0.1185384343067805 Validation Loss: 0.724417507648468\n",
      "Epoch 4305: Training Loss: 0.1184617852171262 Validation Loss: 0.7274965047836304\n",
      "Epoch 4306: Training Loss: 0.11849598586559296 Validation Loss: 0.7280237674713135\n",
      "Epoch 4307: Training Loss: 0.11832647770643234 Validation Loss: 0.7256628274917603\n",
      "Epoch 4308: Training Loss: 0.11827522764603297 Validation Loss: 0.7250609993934631\n",
      "Epoch 4309: Training Loss: 0.11824678132931392 Validation Loss: 0.7252005934715271\n",
      "Epoch 4310: Training Loss: 0.11819260319073994 Validation Loss: 0.7249051928520203\n",
      "Epoch 4311: Training Loss: 0.11811713874340057 Validation Loss: 0.7260769009590149\n",
      "Epoch 4312: Training Loss: 0.11812069515387218 Validation Loss: 0.7252286672592163\n",
      "Epoch 4313: Training Loss: 0.11811158309380214 Validation Loss: 0.7266619801521301\n",
      "Epoch 4314: Training Loss: 0.11802644282579422 Validation Loss: 0.726170539855957\n",
      "Epoch 4315: Training Loss: 0.11827558775742848 Validation Loss: 0.7282651662826538\n",
      "Epoch 4316: Training Loss: 0.11797431359688441 Validation Loss: 0.7265968918800354\n",
      "Epoch 4317: Training Loss: 0.11791797478993733 Validation Loss: 0.7248576283454895\n",
      "Epoch 4318: Training Loss: 0.1179228921731313 Validation Loss: 0.7250630855560303\n",
      "Epoch 4319: Training Loss: 0.11785058428843816 Validation Loss: 0.7246055603027344\n",
      "Epoch 4320: Training Loss: 0.11796869585911433 Validation Loss: 0.7219711542129517\n",
      "Epoch 4321: Training Loss: 0.11788416405518849 Validation Loss: 0.7251111268997192\n",
      "Epoch 4322: Training Loss: 0.1179359182715416 Validation Loss: 0.7272081971168518\n",
      "Epoch 4323: Training Loss: 0.11806583156188329 Validation Loss: 0.7297528982162476\n",
      "Epoch 4324: Training Loss: 0.11789456009864807 Validation Loss: 0.7254554629325867\n",
      "Epoch 4325: Training Loss: 0.11772632598876953 Validation Loss: 0.7234348058700562\n",
      "Epoch 4326: Training Loss: 0.11781336118777593 Validation Loss: 0.7258387207984924\n",
      "Epoch 4327: Training Loss: 0.11776681244373322 Validation Loss: 0.7267311215400696\n",
      "Epoch 4328: Training Loss: 0.11759813129901886 Validation Loss: 0.7270890474319458\n",
      "Epoch 4329: Training Loss: 0.11742150286833446 Validation Loss: 0.7248570919036865\n",
      "Epoch 4330: Training Loss: 0.11749735722939174 Validation Loss: 0.7232254147529602\n",
      "Epoch 4331: Training Loss: 0.11752355347077052 Validation Loss: 0.7245161533355713\n",
      "Epoch 4332: Training Loss: 0.11741865923007329 Validation Loss: 0.7263351678848267\n",
      "Epoch 4333: Training Loss: 0.11735333998998006 Validation Loss: 0.7256984114646912\n",
      "Epoch 4334: Training Loss: 0.11730418105920155 Validation Loss: 0.7261378765106201\n",
      "Epoch 4335: Training Loss: 0.11727667103211085 Validation Loss: 0.7258394956588745\n",
      "Epoch 4336: Training Loss: 0.11726061751445134 Validation Loss: 0.724956214427948\n",
      "Epoch 4337: Training Loss: 0.11736669143040974 Validation Loss: 0.7254976034164429\n",
      "Epoch 4338: Training Loss: 0.11721866577863693 Validation Loss: 0.7270480990409851\n",
      "Epoch 4339: Training Loss: 0.11710880696773529 Validation Loss: 0.7267354130744934\n",
      "Epoch 4340: Training Loss: 0.11720834175745647 Validation Loss: 0.7279636859893799\n",
      "Epoch 4341: Training Loss: 0.11702744414409001 Validation Loss: 0.7258955836296082\n",
      "Epoch 4342: Training Loss: 0.11724441001812617 Validation Loss: 0.7221786975860596\n",
      "Epoch 4343: Training Loss: 0.11713199565807979 Validation Loss: 0.7256656289100647\n",
      "Epoch 4344: Training Loss: 0.1169259746869405 Validation Loss: 0.7257498502731323\n",
      "Epoch 4345: Training Loss: 0.11690308153629303 Validation Loss: 0.7254222631454468\n",
      "Epoch 4346: Training Loss: 0.11686619867881139 Validation Loss: 0.7262961268424988\n",
      "Epoch 4347: Training Loss: 0.11690514783064525 Validation Loss: 0.7267134189605713\n",
      "Epoch 4348: Training Loss: 0.11688140034675598 Validation Loss: 0.7252501249313354\n",
      "Epoch 4349: Training Loss: 0.11674168705940247 Validation Loss: 0.7264857292175293\n",
      "Epoch 4350: Training Loss: 0.11687669654687245 Validation Loss: 0.7253790497779846\n",
      "Epoch 4351: Training Loss: 0.11667930086453755 Validation Loss: 0.7249093055725098\n",
      "Epoch 4352: Training Loss: 0.11678341279427211 Validation Loss: 0.7235455513000488\n",
      "Epoch 4353: Training Loss: 0.11678299556175868 Validation Loss: 0.7274194359779358\n",
      "Epoch 4354: Training Loss: 0.11662025501330693 Validation Loss: 0.7265080809593201\n",
      "Epoch 4355: Training Loss: 0.11659551660219829 Validation Loss: 0.7259708642959595\n",
      "Epoch 4356: Training Loss: 0.11650817096233368 Validation Loss: 0.7277541160583496\n",
      "Epoch 4357: Training Loss: 0.11649861931800842 Validation Loss: 0.7276758551597595\n",
      "Epoch 4358: Training Loss: 0.11656869202852249 Validation Loss: 0.725850522518158\n",
      "Epoch 4359: Training Loss: 0.11651972432931264 Validation Loss: 0.7276302576065063\n",
      "Epoch 4360: Training Loss: 0.11635302752256393 Validation Loss: 0.7271472215652466\n",
      "Epoch 4361: Training Loss: 0.11628404259681702 Validation Loss: 0.7257646322250366\n",
      "Epoch 4362: Training Loss: 0.11626957356929779 Validation Loss: 0.7246964573860168\n",
      "Epoch 4363: Training Loss: 0.11637973537047704 Validation Loss: 0.7223920226097107\n",
      "Epoch 4364: Training Loss: 0.1164421687523524 Validation Loss: 0.7219552397727966\n",
      "Epoch 4365: Training Loss: 0.116229514280955 Validation Loss: 0.7243022918701172\n",
      "Epoch 4366: Training Loss: 0.11609048147996266 Validation Loss: 0.7272220253944397\n",
      "Epoch 4367: Training Loss: 0.11609521508216858 Validation Loss: 0.7289639711380005\n",
      "Epoch 4368: Training Loss: 0.11621905614932378 Validation Loss: 0.7306511998176575\n",
      "Epoch 4369: Training Loss: 0.1160574381550153 Validation Loss: 0.7287168502807617\n",
      "Epoch 4370: Training Loss: 0.1160262127717336 Validation Loss: 0.7265756130218506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4371: Training Loss: 0.11606125285228093 Validation Loss: 0.7255924344062805\n",
      "Epoch 4372: Training Loss: 0.11594615876674652 Validation Loss: 0.7243292331695557\n",
      "Epoch 4373: Training Loss: 0.11607944717009862 Validation Loss: 0.7252922058105469\n",
      "Epoch 4374: Training Loss: 0.11589270581801732 Validation Loss: 0.7243497967720032\n",
      "Epoch 4375: Training Loss: 0.11584207167228062 Validation Loss: 0.7254746556282043\n",
      "Epoch 4376: Training Loss: 0.11578137675921123 Validation Loss: 0.7264536619186401\n",
      "Epoch 4377: Training Loss: 0.11592922111352284 Validation Loss: 0.7285991311073303\n",
      "Epoch 4378: Training Loss: 0.1157655914624532 Validation Loss: 0.727236270904541\n",
      "Epoch 4379: Training Loss: 0.11569478114446004 Validation Loss: 0.7261275053024292\n",
      "Epoch 4380: Training Loss: 0.11587415883938472 Validation Loss: 0.7232062220573425\n",
      "Epoch 4381: Training Loss: 0.1156821499268214 Validation Loss: 0.723990797996521\n",
      "Epoch 4382: Training Loss: 0.11563121527433395 Validation Loss: 0.7247758507728577\n",
      "Epoch 4383: Training Loss: 0.11554696907599767 Validation Loss: 0.7272449135780334\n",
      "Epoch 4384: Training Loss: 0.11593829343716304 Validation Loss: 0.7315661907196045\n",
      "Epoch 4385: Training Loss: 0.11552268763383229 Validation Loss: 0.7293664813041687\n",
      "Epoch 4386: Training Loss: 0.11561470727125804 Validation Loss: 0.7246399521827698\n",
      "Epoch 4387: Training Loss: 0.11559415856997173 Validation Loss: 0.7263497710227966\n",
      "Epoch 4388: Training Loss: 0.11533572276433308 Validation Loss: 0.7245534658432007\n",
      "Epoch 4389: Training Loss: 0.11535489062468211 Validation Loss: 0.7237376570701599\n",
      "Epoch 4390: Training Loss: 0.11551124354203542 Validation Loss: 0.7252806425094604\n",
      "Epoch 4391: Training Loss: 0.11550336082776387 Validation Loss: 0.7226266860961914\n",
      "Epoch 4392: Training Loss: 0.11564382165670395 Validation Loss: 0.727665901184082\n",
      "Epoch 4393: Training Loss: 0.11522610982259114 Validation Loss: 0.7264776229858398\n",
      "Epoch 4394: Training Loss: 0.115216759343942 Validation Loss: 0.7268630266189575\n",
      "Epoch 4395: Training Loss: 0.11519306649764378 Validation Loss: 0.7288549542427063\n",
      "Epoch 4396: Training Loss: 0.11520932863156001 Validation Loss: 0.7261189222335815\n",
      "Epoch 4397: Training Loss: 0.11510345836480458 Validation Loss: 0.7250235080718994\n",
      "Epoch 4398: Training Loss: 0.11510021239519119 Validation Loss: 0.726402997970581\n",
      "Epoch 4399: Training Loss: 0.11506490657726924 Validation Loss: 0.726531982421875\n",
      "Epoch 4400: Training Loss: 0.11493215461572011 Validation Loss: 0.725231409072876\n",
      "Epoch 4401: Training Loss: 0.11489066233237584 Validation Loss: 0.7249305248260498\n",
      "Epoch 4402: Training Loss: 0.11485939969619115 Validation Loss: 0.7248696088790894\n",
      "Epoch 4403: Training Loss: 0.11484864602486293 Validation Loss: 0.7252153158187866\n",
      "Epoch 4404: Training Loss: 0.11479186018308003 Validation Loss: 0.7266738414764404\n",
      "Epoch 4405: Training Loss: 0.1148906076947848 Validation Loss: 0.7282955050468445\n",
      "Epoch 4406: Training Loss: 0.11488174398740132 Validation Loss: 0.72780841588974\n",
      "Epoch 4407: Training Loss: 0.11499642829100291 Validation Loss: 0.7301166653633118\n",
      "Epoch 4408: Training Loss: 0.11468775322039922 Validation Loss: 0.7264254689216614\n",
      "Epoch 4409: Training Loss: 0.11475613216559093 Validation Loss: 0.7267662882804871\n",
      "Epoch 4410: Training Loss: 0.11461525162061055 Validation Loss: 0.7235093712806702\n",
      "Epoch 4411: Training Loss: 0.11459749688704808 Validation Loss: 0.7240991592407227\n",
      "Epoch 4412: Training Loss: 0.114541361729304 Validation Loss: 0.7248331904411316\n",
      "Epoch 4413: Training Loss: 0.11455536633729935 Validation Loss: 0.7234465479850769\n",
      "Epoch 4414: Training Loss: 0.11456706871589024 Validation Loss: 0.7226077318191528\n",
      "Epoch 4415: Training Loss: 0.11442730824152629 Validation Loss: 0.7243098020553589\n",
      "Epoch 4416: Training Loss: 0.11489618569612503 Validation Loss: 0.729927122592926\n",
      "Epoch 4417: Training Loss: 0.11442769318819046 Validation Loss: 0.7292545437812805\n",
      "Epoch 4418: Training Loss: 0.11430038760105769 Validation Loss: 0.7284232378005981\n",
      "Epoch 4419: Training Loss: 0.11444590489069621 Validation Loss: 0.7250230312347412\n",
      "Epoch 4420: Training Loss: 0.1143788496653239 Validation Loss: 0.7262079119682312\n",
      "Epoch 4421: Training Loss: 0.11419293036063512 Validation Loss: 0.7265337109565735\n",
      "Epoch 4422: Training Loss: 0.11416136473417282 Validation Loss: 0.7266989350318909\n",
      "Epoch 4423: Training Loss: 0.11435888707637787 Validation Loss: 0.724716305732727\n",
      "Epoch 4424: Training Loss: 0.11424439152081807 Validation Loss: 0.7272675633430481\n",
      "Epoch 4425: Training Loss: 0.11423866699139278 Validation Loss: 0.7284938097000122\n",
      "Epoch 4426: Training Loss: 0.11403177678585052 Validation Loss: 0.7275446057319641\n",
      "Epoch 4427: Training Loss: 0.11404163142045338 Validation Loss: 0.7259705662727356\n",
      "Epoch 4428: Training Loss: 0.11400309205055237 Validation Loss: 0.7248512506484985\n",
      "Epoch 4429: Training Loss: 0.11412364492813747 Validation Loss: 0.722735583782196\n",
      "Epoch 4430: Training Loss: 0.11402037739753723 Validation Loss: 0.7248595356941223\n",
      "Epoch 4431: Training Loss: 0.11396364122629166 Validation Loss: 0.7271902561187744\n",
      "Epoch 4432: Training Loss: 0.1138781209786733 Validation Loss: 0.7291080355644226\n",
      "Epoch 4433: Training Loss: 0.11380580067634583 Validation Loss: 0.7290087938308716\n",
      "Epoch 4434: Training Loss: 0.1139115293820699 Validation Loss: 0.7262026071548462\n",
      "Epoch 4435: Training Loss: 0.11403703689575195 Validation Loss: 0.7282096743583679\n",
      "Epoch 4436: Training Loss: 0.11409120510021846 Validation Loss: 0.7241393327713013\n",
      "Epoch 4437: Training Loss: 0.11385286102692287 Validation Loss: 0.7275487780570984\n",
      "Epoch 4438: Training Loss: 0.11385456969340642 Validation Loss: 0.7295452952384949\n",
      "Epoch 4439: Training Loss: 0.11360270778338115 Validation Loss: 0.7283238768577576\n",
      "Epoch 4440: Training Loss: 0.11354582011699677 Validation Loss: 0.7260292172431946\n",
      "Epoch 4441: Training Loss: 0.11368585626284282 Validation Loss: 0.7232652306556702\n",
      "Epoch 4442: Training Loss: 0.11359511564175288 Validation Loss: 0.7250897288322449\n",
      "Epoch 4443: Training Loss: 0.1136242945988973 Validation Loss: 0.7239523530006409\n",
      "Epoch 4444: Training Loss: 0.11344922333955765 Validation Loss: 0.7261002063751221\n",
      "Epoch 4445: Training Loss: 0.11336412032445271 Validation Loss: 0.7269008755683899\n",
      "Epoch 4446: Training Loss: 0.11349393179019292 Validation Loss: 0.7254775762557983\n",
      "Epoch 4447: Training Loss: 0.11331604421138763 Validation Loss: 0.7266935110092163\n",
      "Epoch 4448: Training Loss: 0.11329606920480728 Validation Loss: 0.7289987206459045\n",
      "Epoch 4449: Training Loss: 0.11330100148916245 Validation Loss: 0.729667603969574\n",
      "Epoch 4450: Training Loss: 0.11339575052261353 Validation Loss: 0.726385772228241\n",
      "Epoch 4451: Training Loss: 0.1131837194164594 Validation Loss: 0.7264768481254578\n",
      "Epoch 4452: Training Loss: 0.1131293773651123 Validation Loss: 0.7263659238815308\n",
      "Epoch 4453: Training Loss: 0.11314806838830312 Validation Loss: 0.7270980477333069\n",
      "Epoch 4454: Training Loss: 0.11310925831397374 Validation Loss: 0.7275123596191406\n",
      "Epoch 4455: Training Loss: 0.11310560752948125 Validation Loss: 0.7260724902153015\n",
      "Epoch 4456: Training Loss: 0.1129805992046992 Validation Loss: 0.725631833076477\n",
      "Epoch 4457: Training Loss: 0.11307525138060252 Validation Loss: 0.7283745408058167\n",
      "Epoch 4458: Training Loss: 0.11294266829888026 Validation Loss: 0.7284202575683594\n",
      "Epoch 4459: Training Loss: 0.11312925070524216 Validation Loss: 0.729691743850708\n",
      "Epoch 4460: Training Loss: 0.11291439086198807 Validation Loss: 0.7268988490104675\n",
      "Epoch 4461: Training Loss: 0.11288907627264659 Validation Loss: 0.7235066890716553\n",
      "Epoch 4462: Training Loss: 0.11345133185386658 Validation Loss: 0.7193403244018555\n",
      "Epoch 4463: Training Loss: 0.11291518807411194 Validation Loss: 0.7231283187866211\n",
      "Epoch 4464: Training Loss: 0.11275244255860646 Validation Loss: 0.7271696925163269\n",
      "Epoch 4465: Training Loss: 0.11265593270460765 Validation Loss: 0.7296121716499329\n",
      "Epoch 4466: Training Loss: 0.11275430520375569 Validation Loss: 0.7313535809516907\n",
      "Epoch 4467: Training Loss: 0.1127767339348793 Validation Loss: 0.7319860458374023\n",
      "Epoch 4468: Training Loss: 0.11274795730908711 Validation Loss: 0.72728431224823\n",
      "Epoch 4469: Training Loss: 0.11266890913248062 Validation Loss: 0.7250298261642456\n",
      "Epoch 4470: Training Loss: 0.11292179177204768 Validation Loss: 0.7219388484954834\n",
      "Epoch 4471: Training Loss: 0.11268072326978047 Validation Loss: 0.7261848449707031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4472: Training Loss: 0.11269788444042206 Validation Loss: 0.7302740216255188\n",
      "Epoch 4473: Training Loss: 0.11257124940554301 Validation Loss: 0.7283299565315247\n",
      "Epoch 4474: Training Loss: 0.11246362576882045 Validation Loss: 0.7270531058311462\n",
      "Epoch 4475: Training Loss: 0.11248188217480977 Validation Loss: 0.7261407971382141\n",
      "Epoch 4476: Training Loss: 0.11240056902170181 Validation Loss: 0.7249383926391602\n",
      "Epoch 4477: Training Loss: 0.11233412226041158 Validation Loss: 0.7274994850158691\n",
      "Epoch 4478: Training Loss: 0.11228139698505402 Validation Loss: 0.7277572154998779\n",
      "Epoch 4479: Training Loss: 0.112260602414608 Validation Loss: 0.7285032272338867\n",
      "Epoch 4480: Training Loss: 0.11220455914735794 Validation Loss: 0.7277833819389343\n",
      "Epoch 4481: Training Loss: 0.11219984789689381 Validation Loss: 0.7273449301719666\n",
      "Epoch 4482: Training Loss: 0.11211916555960973 Validation Loss: 0.7257288694381714\n",
      "Epoch 4483: Training Loss: 0.11226191868384679 Validation Loss: 0.7233094573020935\n",
      "Epoch 4484: Training Loss: 0.11249547203381856 Validation Loss: 0.7281676530838013\n",
      "Epoch 4485: Training Loss: 0.11204424997170766 Validation Loss: 0.7286801934242249\n",
      "Epoch 4486: Training Loss: 0.11201281100511551 Validation Loss: 0.727225124835968\n",
      "Epoch 4487: Training Loss: 0.11212138086557388 Validation Loss: 0.724565863609314\n",
      "Epoch 4488: Training Loss: 0.11193297306696574 Validation Loss: 0.7255493402481079\n",
      "Epoch 4489: Training Loss: 0.11205341170231502 Validation Loss: 0.7284547686576843\n",
      "Epoch 4490: Training Loss: 0.11189795037110646 Validation Loss: 0.7268420457839966\n",
      "Epoch 4491: Training Loss: 0.11203363289435704 Validation Loss: 0.7246667742729187\n",
      "Epoch 4492: Training Loss: 0.1119779422879219 Validation Loss: 0.727508544921875\n",
      "Epoch 4493: Training Loss: 0.11182934294144313 Validation Loss: 0.726331889629364\n",
      "Epoch 4494: Training Loss: 0.11172037074963252 Validation Loss: 0.7271363139152527\n",
      "Epoch 4495: Training Loss: 0.11174461245536804 Validation Loss: 0.7284198999404907\n",
      "Epoch 4496: Training Loss: 0.11190598458051682 Validation Loss: 0.7254275679588318\n",
      "Epoch 4497: Training Loss: 0.11184260745843251 Validation Loss: 0.7285329699516296\n",
      "Epoch 4498: Training Loss: 0.11194767554601033 Validation Loss: 0.730726957321167\n",
      "Epoch 4499: Training Loss: 0.11160210768381755 Validation Loss: 0.7282571196556091\n",
      "Epoch 4500: Training Loss: 0.11160802841186523 Validation Loss: 0.7251226305961609\n",
      "Epoch 4501: Training Loss: 0.1114985669652621 Validation Loss: 0.7239882946014404\n",
      "Epoch 4502: Training Loss: 0.11152070760726929 Validation Loss: 0.7232148051261902\n",
      "Epoch 4503: Training Loss: 0.1115264321366946 Validation Loss: 0.724853515625\n",
      "Epoch 4504: Training Loss: 0.11143714686234792 Validation Loss: 0.7274873852729797\n",
      "Epoch 4505: Training Loss: 0.11136806507905324 Validation Loss: 0.728154182434082\n",
      "Epoch 4506: Training Loss: 0.11143229901790619 Validation Loss: 0.7286703586578369\n",
      "Epoch 4507: Training Loss: 0.11140326410531998 Validation Loss: 0.7288108468055725\n",
      "Epoch 4508: Training Loss: 0.1112929458419482 Validation Loss: 0.7282335162162781\n",
      "Epoch 4509: Training Loss: 0.1112498864531517 Validation Loss: 0.7278733253479004\n",
      "Epoch 4510: Training Loss: 0.11135375748078029 Validation Loss: 0.7290261387825012\n",
      "Epoch 4511: Training Loss: 0.11132932702700298 Validation Loss: 0.7253485918045044\n",
      "Epoch 4512: Training Loss: 0.11113517731428146 Validation Loss: 0.7247825860977173\n",
      "Epoch 4513: Training Loss: 0.11116822808980942 Validation Loss: 0.7243366241455078\n",
      "Epoch 4514: Training Loss: 0.11107073972622554 Validation Loss: 0.726391077041626\n",
      "Epoch 4515: Training Loss: 0.11102902889251709 Validation Loss: 0.7270119786262512\n",
      "Epoch 4516: Training Loss: 0.1113255297144254 Validation Loss: 0.7305463552474976\n",
      "Epoch 4517: Training Loss: 0.11129381507635117 Validation Loss: 0.7268035411834717\n",
      "Epoch 4518: Training Loss: 0.11093786110480626 Validation Loss: 0.7264567017555237\n",
      "Epoch 4519: Training Loss: 0.11099076271057129 Validation Loss: 0.7278878092765808\n",
      "Epoch 4520: Training Loss: 0.11093055705229442 Validation Loss: 0.7265773415565491\n",
      "Epoch 4521: Training Loss: 0.1108494574824969 Validation Loss: 0.7269853353500366\n",
      "Epoch 4522: Training Loss: 0.11090029776096344 Validation Loss: 0.7274506688117981\n",
      "Epoch 4523: Training Loss: 0.11080523083607356 Validation Loss: 0.7283944487571716\n",
      "Epoch 4524: Training Loss: 0.11075051873922348 Validation Loss: 0.7277962565422058\n",
      "Epoch 4525: Training Loss: 0.11086071779330571 Validation Loss: 0.7252508401870728\n",
      "Epoch 4526: Training Loss: 0.11072568595409393 Validation Loss: 0.7255673408508301\n",
      "Epoch 4527: Training Loss: 0.11078430463870366 Validation Loss: 0.7255398035049438\n",
      "Epoch 4528: Training Loss: 0.11070764809846878 Validation Loss: 0.7254033088684082\n",
      "Epoch 4529: Training Loss: 0.11061898370583852 Validation Loss: 0.7269507050514221\n",
      "Epoch 4530: Training Loss: 0.11093877255916595 Validation Loss: 0.7309654355049133\n",
      "Epoch 4531: Training Loss: 0.11057706425587337 Validation Loss: 0.7292881011962891\n",
      "Epoch 4532: Training Loss: 0.11047042657931645 Validation Loss: 0.7283907532691956\n",
      "Epoch 4533: Training Loss: 0.11046742151180904 Validation Loss: 0.7259988784790039\n",
      "Epoch 4534: Training Loss: 0.11061493555704753 Validation Loss: 0.7234266996383667\n",
      "Epoch 4535: Training Loss: 0.11056810120741527 Validation Loss: 0.7270047664642334\n",
      "Epoch 4536: Training Loss: 0.11041524509588878 Validation Loss: 0.7279826402664185\n",
      "Epoch 4537: Training Loss: 0.11082518100738525 Validation Loss: 0.7320271134376526\n",
      "Epoch 4538: Training Loss: 0.11044067144393921 Validation Loss: 0.73081374168396\n",
      "Epoch 4539: Training Loss: 0.11053088804086049 Validation Loss: 0.724910318851471\n",
      "Epoch 4540: Training Loss: 0.11025457580884297 Validation Loss: 0.7241082191467285\n",
      "Epoch 4541: Training Loss: 0.11042169729868571 Validation Loss: 0.7224383354187012\n",
      "Epoch 4542: Training Loss: 0.11035015682379405 Validation Loss: 0.7231374382972717\n",
      "Epoch 4543: Training Loss: 0.1102462833126386 Validation Loss: 0.7274534106254578\n",
      "Epoch 4544: Training Loss: 0.11017369478940964 Validation Loss: 0.731716513633728\n",
      "Epoch 4545: Training Loss: 0.11035092175006866 Validation Loss: 0.7324634790420532\n",
      "Epoch 4546: Training Loss: 0.11081096529960632 Validation Loss: 0.7263967394828796\n",
      "Epoch 4547: Training Loss: 0.11001476645469666 Validation Loss: 0.7265799045562744\n",
      "Epoch 4548: Training Loss: 0.11036187907059987 Validation Loss: 0.7242806553840637\n",
      "Epoch 4549: Training Loss: 0.11008058736721675 Validation Loss: 0.728314220905304\n",
      "Epoch 4550: Training Loss: 0.10999389986197154 Validation Loss: 0.7302548289299011\n",
      "Epoch 4551: Training Loss: 0.10992912948131561 Validation Loss: 0.7306767702102661\n",
      "Epoch 4552: Training Loss: 0.10992287347714107 Validation Loss: 0.7295034527778625\n",
      "Epoch 4553: Training Loss: 0.10987670471270879 Validation Loss: 0.7289272546768188\n",
      "Epoch 4554: Training Loss: 0.10977979997793834 Validation Loss: 0.7256147861480713\n",
      "Epoch 4555: Training Loss: 0.10978250453869502 Validation Loss: 0.7238745093345642\n",
      "Epoch 4556: Training Loss: 0.10977796465158463 Validation Loss: 0.7245112657546997\n",
      "Epoch 4557: Training Loss: 0.10996326059103012 Validation Loss: 0.7246033549308777\n",
      "Epoch 4558: Training Loss: 0.10971416532993317 Validation Loss: 0.7285220623016357\n",
      "Epoch 4559: Training Loss: 0.10963856180508931 Validation Loss: 0.7292166948318481\n",
      "Epoch 4560: Training Loss: 0.1096396545569102 Validation Loss: 0.7300138473510742\n",
      "Epoch 4561: Training Loss: 0.10975975294907887 Validation Loss: 0.7303799986839294\n",
      "Epoch 4562: Training Loss: 0.10968664288520813 Validation Loss: 0.7302914261817932\n",
      "Epoch 4563: Training Loss: 0.1096106544137001 Validation Loss: 0.7265814542770386\n",
      "Epoch 4564: Training Loss: 0.10949615637461345 Validation Loss: 0.7248541712760925\n",
      "Epoch 4565: Training Loss: 0.10966900239388148 Validation Loss: 0.7251321077346802\n",
      "Epoch 4566: Training Loss: 0.10941766450802486 Validation Loss: 0.7253572940826416\n",
      "Epoch 4567: Training Loss: 0.10942625999450684 Validation Loss: 0.7275852560997009\n",
      "Epoch 4568: Training Loss: 0.1093236431479454 Validation Loss: 0.7281486988067627\n",
      "Epoch 4569: Training Loss: 0.10938672721385956 Validation Loss: 0.7289419174194336\n",
      "Epoch 4570: Training Loss: 0.10928907990455627 Validation Loss: 0.7283312082290649\n",
      "Epoch 4571: Training Loss: 0.10939664642016093 Validation Loss: 0.726555585861206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4572: Training Loss: 0.10926574220259984 Validation Loss: 0.7276245951652527\n",
      "Epoch 4573: Training Loss: 0.10927523175875346 Validation Loss: 0.7255372405052185\n",
      "Epoch 4574: Training Loss: 0.10924237966537476 Validation Loss: 0.7259137034416199\n",
      "Epoch 4575: Training Loss: 0.10915148506561916 Validation Loss: 0.7281144261360168\n",
      "Epoch 4576: Training Loss: 0.1091489518682162 Validation Loss: 0.7288703918457031\n",
      "Epoch 4577: Training Loss: 0.10911320646603902 Validation Loss: 0.7287971377372742\n",
      "Epoch 4578: Training Loss: 0.10915752748648326 Validation Loss: 0.7288799285888672\n",
      "Epoch 4579: Training Loss: 0.10927413403987885 Validation Loss: 0.7303301692008972\n",
      "Epoch 4580: Training Loss: 0.10907515386740367 Validation Loss: 0.7266495823860168\n",
      "Epoch 4581: Training Loss: 0.10898646463950475 Validation Loss: 0.7251651287078857\n",
      "Epoch 4582: Training Loss: 0.10892862578233083 Validation Loss: 0.7256038188934326\n",
      "Epoch 4583: Training Loss: 0.10892013957103093 Validation Loss: 0.7270454168319702\n",
      "Epoch 4584: Training Loss: 0.10892569025357564 Validation Loss: 0.7261900901794434\n",
      "Epoch 4585: Training Loss: 0.10883571455876033 Validation Loss: 0.7266119718551636\n",
      "Epoch 4586: Training Loss: 0.10886108378569286 Validation Loss: 0.7261874079704285\n",
      "Epoch 4587: Training Loss: 0.10876157879829407 Validation Loss: 0.7286304831504822\n",
      "Epoch 4588: Training Loss: 0.10883187254269917 Validation Loss: 0.7285131216049194\n",
      "Epoch 4589: Training Loss: 0.1086844156185786 Validation Loss: 0.7291669249534607\n",
      "Epoch 4590: Training Loss: 0.10871195048093796 Validation Loss: 0.7299264669418335\n",
      "Epoch 4591: Training Loss: 0.1088583692908287 Validation Loss: 0.7300784587860107\n",
      "Epoch 4592: Training Loss: 0.1086869090795517 Validation Loss: 0.7282708883285522\n",
      "Epoch 4593: Training Loss: 0.1085550586382548 Validation Loss: 0.7276979088783264\n",
      "Epoch 4594: Training Loss: 0.10909548898537953 Validation Loss: 0.7236342430114746\n",
      "Epoch 4595: Training Loss: 0.10862597823143005 Validation Loss: 0.7251351475715637\n",
      "Epoch 4596: Training Loss: 0.10855261236429214 Validation Loss: 0.7287625074386597\n",
      "Epoch 4597: Training Loss: 0.10856204479932785 Validation Loss: 0.7285824418067932\n",
      "Epoch 4598: Training Loss: 0.10845049719015758 Validation Loss: 0.729540228843689\n",
      "Epoch 4599: Training Loss: 0.10858044276634853 Validation Loss: 0.7310524582862854\n",
      "Epoch 4600: Training Loss: 0.10847983757654826 Validation Loss: 0.7303364276885986\n",
      "Epoch 4601: Training Loss: 0.10859936227401097 Validation Loss: 0.725079357624054\n",
      "Epoch 4602: Training Loss: 0.10837292671203613 Validation Loss: 0.7245475053787231\n",
      "Epoch 4603: Training Loss: 0.10829807817935944 Validation Loss: 0.7255462408065796\n",
      "Epoch 4604: Training Loss: 0.10827221969763438 Validation Loss: 0.726181149482727\n",
      "Epoch 4605: Training Loss: 0.10823295265436172 Validation Loss: 0.7283452749252319\n",
      "Epoch 4606: Training Loss: 0.10823749999205272 Validation Loss: 0.7305327653884888\n",
      "Epoch 4607: Training Loss: 0.10833029448986053 Validation Loss: 0.7322567105293274\n",
      "Epoch 4608: Training Loss: 0.10829464842875798 Validation Loss: 0.7316264510154724\n",
      "Epoch 4609: Training Loss: 0.10808514555295308 Validation Loss: 0.7281166911125183\n",
      "Epoch 4610: Training Loss: 0.10806980232397716 Validation Loss: 0.7241320013999939\n",
      "Epoch 4611: Training Loss: 0.10814854751030605 Validation Loss: 0.7253132462501526\n",
      "Epoch 4612: Training Loss: 0.10808151960372925 Validation Loss: 0.7251879572868347\n",
      "Epoch 4613: Training Loss: 0.10801560680071513 Validation Loss: 0.7267277836799622\n",
      "Epoch 4614: Training Loss: 0.10799046357472737 Validation Loss: 0.7267053723335266\n",
      "Epoch 4615: Training Loss: 0.10789120694001515 Validation Loss: 0.7284709811210632\n",
      "Epoch 4616: Training Loss: 0.10793691873550415 Validation Loss: 0.7308493256568909\n",
      "Epoch 4617: Training Loss: 0.10787637035051982 Validation Loss: 0.7305361032485962\n",
      "Epoch 4618: Training Loss: 0.10788797090450923 Validation Loss: 0.7301116585731506\n",
      "Epoch 4619: Training Loss: 0.10815804948409398 Validation Loss: 0.724453866481781\n",
      "Epoch 4620: Training Loss: 0.10785998155673344 Validation Loss: 0.7230825424194336\n",
      "Epoch 4621: Training Loss: 0.10785364111264546 Validation Loss: 0.7237412333488464\n",
      "Epoch 4622: Training Loss: 0.10774563004573186 Validation Loss: 0.7261977791786194\n",
      "Epoch 4623: Training Loss: 0.10765840361515681 Validation Loss: 0.730291485786438\n",
      "Epoch 4624: Training Loss: 0.10762982815504074 Validation Loss: 0.7308564782142639\n",
      "Epoch 4625: Training Loss: 0.10760524620612462 Validation Loss: 0.730707585811615\n",
      "Epoch 4626: Training Loss: 0.10757385691006978 Validation Loss: 0.7300348281860352\n",
      "Epoch 4627: Training Loss: 0.10754367460807164 Validation Loss: 0.7286247611045837\n",
      "Epoch 4628: Training Loss: 0.10749597102403641 Validation Loss: 0.7273291349411011\n",
      "Epoch 4629: Training Loss: 0.10752605646848679 Validation Loss: 0.7277363538742065\n",
      "Epoch 4630: Training Loss: 0.10742156704266866 Validation Loss: 0.7269036173820496\n",
      "Epoch 4631: Training Loss: 0.10738216092189153 Validation Loss: 0.7270200252532959\n",
      "Epoch 4632: Training Loss: 0.10741996516784032 Validation Loss: 0.7280565500259399\n",
      "Epoch 4633: Training Loss: 0.10742003470659256 Validation Loss: 0.7270018458366394\n",
      "Epoch 4634: Training Loss: 0.10730939855178197 Validation Loss: 0.7270206809043884\n",
      "Epoch 4635: Training Loss: 0.10735814521710078 Validation Loss: 0.7266358137130737\n",
      "Epoch 4636: Training Loss: 0.10741883267958958 Validation Loss: 0.728339672088623\n",
      "Epoch 4637: Training Loss: 0.10731506099303563 Validation Loss: 0.7275139093399048\n",
      "Epoch 4638: Training Loss: 0.10741684834162395 Validation Loss: 0.7308050394058228\n",
      "Epoch 4639: Training Loss: 0.10733494162559509 Validation Loss: 0.7287008762359619\n",
      "Epoch 4640: Training Loss: 0.1072065681219101 Validation Loss: 0.7295086979866028\n",
      "Epoch 4641: Training Loss: 0.10711415360371272 Validation Loss: 0.7286586761474609\n",
      "Epoch 4642: Training Loss: 0.10709820687770844 Validation Loss: 0.728186845779419\n",
      "Epoch 4643: Training Loss: 0.10706299543380737 Validation Loss: 0.7266390323638916\n",
      "Epoch 4644: Training Loss: 0.10714034487803777 Validation Loss: 0.728328287601471\n",
      "Epoch 4645: Training Loss: 0.10711341351270676 Validation Loss: 0.7260303497314453\n",
      "Epoch 4646: Training Loss: 0.10704353948434193 Validation Loss: 0.72690349817276\n",
      "Epoch 4647: Training Loss: 0.10707306861877441 Validation Loss: 0.7291892170906067\n",
      "Epoch 4648: Training Loss: 0.10708220551411311 Validation Loss: 0.7260667681694031\n",
      "Epoch 4649: Training Loss: 0.10701801131169002 Validation Loss: 0.7292534112930298\n",
      "Epoch 4650: Training Loss: 0.10691759983698527 Validation Loss: 0.7298188209533691\n",
      "Epoch 4651: Training Loss: 0.10690235098203023 Validation Loss: 0.7303475737571716\n",
      "Epoch 4652: Training Loss: 0.10690197596947353 Validation Loss: 0.7304496765136719\n",
      "Epoch 4653: Training Loss: 0.10675257196029027 Validation Loss: 0.7271844744682312\n",
      "Epoch 4654: Training Loss: 0.10688471297423045 Validation Loss: 0.7237607836723328\n",
      "Epoch 4655: Training Loss: 0.10680177062749863 Validation Loss: 0.7237564325332642\n",
      "Epoch 4656: Training Loss: 0.10688783973455429 Validation Loss: 0.7254298329353333\n",
      "Epoch 4657: Training Loss: 0.10687907785177231 Validation Loss: 0.731475830078125\n",
      "Epoch 4658: Training Loss: 0.1067442496617635 Validation Loss: 0.7321413159370422\n",
      "Epoch 4659: Training Loss: 0.10667456934849422 Validation Loss: 0.7299225330352783\n",
      "Epoch 4660: Training Loss: 0.10667218267917633 Validation Loss: 0.7304224967956543\n",
      "Epoch 4661: Training Loss: 0.10658293962478638 Validation Loss: 0.7272384762763977\n",
      "Epoch 4662: Training Loss: 0.10658119122187297 Validation Loss: 0.7274309992790222\n",
      "Epoch 4663: Training Loss: 0.10652414709329605 Validation Loss: 0.7255880832672119\n",
      "Epoch 4664: Training Loss: 0.10648317138353984 Validation Loss: 0.726206362247467\n",
      "Epoch 4665: Training Loss: 0.10648016631603241 Validation Loss: 0.7279872298240662\n",
      "Epoch 4666: Training Loss: 0.10661006718873978 Validation Loss: 0.731161892414093\n",
      "Epoch 4667: Training Loss: 0.10639999558528264 Validation Loss: 0.731241762638092\n",
      "Epoch 4668: Training Loss: 0.10662247985601425 Validation Loss: 0.7263453006744385\n",
      "Epoch 4669: Training Loss: 0.10639691352844238 Validation Loss: 0.7249112725257874\n",
      "Epoch 4670: Training Loss: 0.10655789573987325 Validation Loss: 0.729141354560852\n",
      "Epoch 4671: Training Loss: 0.10621371865272522 Validation Loss: 0.729282557964325\n",
      "Epoch 4672: Training Loss: 0.10618095596631368 Validation Loss: 0.7286855578422546\n",
      "Epoch 4673: Training Loss: 0.10625677804152171 Validation Loss: 0.7267866134643555\n",
      "Epoch 4674: Training Loss: 0.10613855967919032 Validation Loss: 0.726778507232666\n",
      "Epoch 4675: Training Loss: 0.10610589881738026 Validation Loss: 0.7271395921707153\n",
      "Epoch 4676: Training Loss: 0.10610268761714299 Validation Loss: 0.727900505065918\n",
      "Epoch 4677: Training Loss: 0.10635268191496532 Validation Loss: 0.7317480444908142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4678: Training Loss: 0.10605484743913014 Validation Loss: 0.7310718297958374\n",
      "Epoch 4679: Training Loss: 0.10608784854412079 Validation Loss: 0.7282059788703918\n",
      "Epoch 4680: Training Loss: 0.10604001830021541 Validation Loss: 0.7268335819244385\n",
      "Epoch 4681: Training Loss: 0.10605809837579727 Validation Loss: 0.7256619930267334\n",
      "Epoch 4682: Training Loss: 0.10606162250041962 Validation Loss: 0.7274076342582703\n",
      "Epoch 4683: Training Loss: 0.10589280227820079 Validation Loss: 0.7297690510749817\n",
      "Epoch 4684: Training Loss: 0.10591459025939305 Validation Loss: 0.7302756309509277\n",
      "Epoch 4685: Training Loss: 0.10595748325188954 Validation Loss: 0.7313328981399536\n",
      "Epoch 4686: Training Loss: 0.10585086047649384 Validation Loss: 0.7302111387252808\n",
      "Epoch 4687: Training Loss: 0.10585870097080867 Validation Loss: 0.7266273498535156\n",
      "Epoch 4688: Training Loss: 0.10575614372889201 Validation Loss: 0.7254064679145813\n",
      "Epoch 4689: Training Loss: 0.1059458355108897 Validation Loss: 0.727577805519104\n",
      "Epoch 4690: Training Loss: 0.105854831635952 Validation Loss: 0.7303053736686707\n",
      "Epoch 4691: Training Loss: 0.10566193113724391 Validation Loss: 0.7280508875846863\n",
      "Epoch 4692: Training Loss: 0.10558919608592987 Validation Loss: 0.7268929481506348\n",
      "Epoch 4693: Training Loss: 0.1057053655385971 Validation Loss: 0.7282925248146057\n",
      "Epoch 4694: Training Loss: 0.10554851094881694 Validation Loss: 0.728278636932373\n",
      "Epoch 4695: Training Loss: 0.10555634399255116 Validation Loss: 0.7281597256660461\n",
      "Epoch 4696: Training Loss: 0.10550115505854289 Validation Loss: 0.72869873046875\n",
      "Epoch 4697: Training Loss: 0.1057325154542923 Validation Loss: 0.7304924726486206\n",
      "Epoch 4698: Training Loss: 0.10560042907794316 Validation Loss: 0.726497232913971\n",
      "Epoch 4699: Training Loss: 0.10540885974963506 Validation Loss: 0.7258881330490112\n",
      "Epoch 4700: Training Loss: 0.10544256865978241 Validation Loss: 0.7278639674186707\n",
      "Epoch 4701: Training Loss: 0.10534090052048366 Validation Loss: 0.7287854552268982\n",
      "Epoch 4702: Training Loss: 0.10573311150074005 Validation Loss: 0.7323188185691833\n",
      "Epoch 4703: Training Loss: 0.10546125719944636 Validation Loss: 0.7320629358291626\n",
      "Epoch 4704: Training Loss: 0.10528206080198288 Validation Loss: 0.7275717854499817\n",
      "Epoch 4705: Training Loss: 0.1053444892168045 Validation Loss: 0.727443277835846\n",
      "Epoch 4706: Training Loss: 0.10525908321142197 Validation Loss: 0.7247512340545654\n",
      "Epoch 4707: Training Loss: 0.10555080324411392 Validation Loss: 0.7285240888595581\n",
      "Epoch 4708: Training Loss: 0.10538332164287567 Validation Loss: 0.7257955074310303\n",
      "Epoch 4709: Training Loss: 0.10512087494134903 Validation Loss: 0.7275936007499695\n",
      "Epoch 4710: Training Loss: 0.10514172663291295 Validation Loss: 0.727419912815094\n",
      "Epoch 4711: Training Loss: 0.10533687720696132 Validation Loss: 0.7317437529563904\n",
      "Epoch 4712: Training Loss: 0.10518575708071391 Validation Loss: 0.7326671481132507\n",
      "Epoch 4713: Training Loss: 0.1051311840613683 Validation Loss: 0.7283442616462708\n",
      "Epoch 4714: Training Loss: 0.10498647391796112 Validation Loss: 0.7284061312675476\n",
      "Epoch 4715: Training Loss: 0.10525149603684743 Validation Loss: 0.7248529195785522\n",
      "Epoch 4716: Training Loss: 0.10509954392910004 Validation Loss: 0.7248725891113281\n",
      "Epoch 4717: Training Loss: 0.10502807547648747 Validation Loss: 0.7278019189834595\n",
      "Epoch 4718: Training Loss: 0.10483652849992116 Validation Loss: 0.7306240200996399\n",
      "Epoch 4719: Training Loss: 0.10487096259991328 Validation Loss: 0.7321189045906067\n",
      "Epoch 4720: Training Loss: 0.10489130765199661 Validation Loss: 0.7304059863090515\n",
      "Epoch 4721: Training Loss: 0.10489000876744588 Validation Loss: 0.731689453125\n",
      "Epoch 4722: Training Loss: 0.10476763049761455 Validation Loss: 0.7290037870407104\n",
      "Epoch 4723: Training Loss: 0.10492527484893799 Validation Loss: 0.7303211688995361\n",
      "Epoch 4724: Training Loss: 0.10468571136395137 Validation Loss: 0.7276862263679504\n",
      "Epoch 4725: Training Loss: 0.10468678673108418 Validation Loss: 0.7257790565490723\n",
      "Epoch 4726: Training Loss: 0.10474007576704025 Validation Loss: 0.7247074842453003\n",
      "Epoch 4727: Training Loss: 0.10513851543267567 Validation Loss: 0.729923665523529\n",
      "Epoch 4728: Training Loss: 0.10463932156562805 Validation Loss: 0.7285439968109131\n",
      "Epoch 4729: Training Loss: 0.10477924346923828 Validation Loss: 0.7313930988311768\n",
      "Epoch 4730: Training Loss: 0.10460667312145233 Validation Loss: 0.7310137748718262\n",
      "Epoch 4731: Training Loss: 0.10451412945985794 Validation Loss: 0.7287218570709229\n",
      "Epoch 4732: Training Loss: 0.10480455060799916 Validation Loss: 0.7253818511962891\n",
      "Epoch 4733: Training Loss: 0.10464037706454594 Validation Loss: 0.7252509593963623\n",
      "Epoch 4734: Training Loss: 0.10451572636763255 Validation Loss: 0.7278609275817871\n",
      "Epoch 4735: Training Loss: 0.10481155912081401 Validation Loss: 0.7338391542434692\n",
      "Epoch 4736: Training Loss: 0.10448355724414189 Validation Loss: 0.7334247827529907\n",
      "Epoch 4737: Training Loss: 0.1044773335258166 Validation Loss: 0.7307413816452026\n",
      "Epoch 4738: Training Loss: 0.10429389774799347 Validation Loss: 0.728394091129303\n",
      "Epoch 4739: Training Loss: 0.10435863087574641 Validation Loss: 0.7280592322349548\n",
      "Epoch 4740: Training Loss: 0.10432007908821106 Validation Loss: 0.72850501537323\n",
      "Epoch 4741: Training Loss: 0.10433829079071681 Validation Loss: 0.7263022065162659\n",
      "Epoch 4742: Training Loss: 0.10425849755605061 Validation Loss: 0.725418746471405\n",
      "Epoch 4743: Training Loss: 0.10421316077311833 Validation Loss: 0.7268909811973572\n",
      "Epoch 4744: Training Loss: 0.10425066947937012 Validation Loss: 0.7306342720985413\n",
      "Epoch 4745: Training Loss: 0.10422553370396297 Validation Loss: 0.7327893376350403\n",
      "Epoch 4746: Training Loss: 0.1041590894261996 Validation Loss: 0.7313117980957031\n",
      "Epoch 4747: Training Loss: 0.10400141775608063 Validation Loss: 0.7286093235015869\n",
      "Epoch 4748: Training Loss: 0.10400152703126271 Validation Loss: 0.7278098464012146\n",
      "Epoch 4749: Training Loss: 0.10411988943815231 Validation Loss: 0.7286633849143982\n",
      "Epoch 4750: Training Loss: 0.10399308303991954 Validation Loss: 0.7285467386245728\n",
      "Epoch 4751: Training Loss: 0.10402014354864757 Validation Loss: 0.7265894412994385\n",
      "Epoch 4752: Training Loss: 0.10396495213111241 Validation Loss: 0.7261173725128174\n",
      "Epoch 4753: Training Loss: 0.1039428636431694 Validation Loss: 0.7283461689949036\n",
      "Epoch 4754: Training Loss: 0.10385177532831828 Validation Loss: 0.729310929775238\n",
      "Epoch 4755: Training Loss: 0.10381554067134857 Validation Loss: 0.7303592562675476\n",
      "Epoch 4756: Training Loss: 0.10383200148741405 Validation Loss: 0.7293749451637268\n",
      "Epoch 4757: Training Loss: 0.10380930950244267 Validation Loss: 0.7297395467758179\n",
      "Epoch 4758: Training Loss: 0.10378094762563705 Validation Loss: 0.7281178832054138\n",
      "Epoch 4759: Training Loss: 0.1037574013074239 Validation Loss: 0.7296973466873169\n",
      "Epoch 4760: Training Loss: 0.10367493828137715 Validation Loss: 0.7304087281227112\n",
      "Epoch 4761: Training Loss: 0.10370481262604396 Validation Loss: 0.7300642728805542\n",
      "Epoch 4762: Training Loss: 0.10363802562157313 Validation Loss: 0.7306099534034729\n",
      "Epoch 4763: Training Loss: 0.103680024544398 Validation Loss: 0.73068767786026\n",
      "Epoch 4764: Training Loss: 0.10355780273675919 Validation Loss: 0.7285503149032593\n",
      "Epoch 4765: Training Loss: 0.10377463201681773 Validation Loss: 0.7262282371520996\n",
      "Epoch 4766: Training Loss: 0.10368732859690984 Validation Loss: 0.7290892004966736\n",
      "Epoch 4767: Training Loss: 0.10364232709010442 Validation Loss: 0.7299996614456177\n",
      "Epoch 4768: Training Loss: 0.10346166292826335 Validation Loss: 0.7282701134681702\n",
      "Epoch 4769: Training Loss: 0.1034895231326421 Validation Loss: 0.7270714640617371\n",
      "Epoch 4770: Training Loss: 0.10344429065783818 Validation Loss: 0.7266021966934204\n",
      "Epoch 4771: Training Loss: 0.10343452543020248 Validation Loss: 0.727577269077301\n",
      "Epoch 4772: Training Loss: 0.10334759205579758 Validation Loss: 0.7291041016578674\n",
      "Epoch 4773: Training Loss: 0.10331793874502182 Validation Loss: 0.7309644818305969\n",
      "Epoch 4774: Training Loss: 0.10333446164925893 Validation Loss: 0.7305528521537781\n",
      "Epoch 4775: Training Loss: 0.10329185426235199 Validation Loss: 0.7290827631950378\n",
      "Epoch 4776: Training Loss: 0.10330935815970103 Validation Loss: 0.7303675413131714\n",
      "Epoch 4777: Training Loss: 0.10324682046969731 Validation Loss: 0.7295644879341125\n",
      "Epoch 4778: Training Loss: 0.10323768357435863 Validation Loss: 0.7271146774291992\n",
      "Epoch 4779: Training Loss: 0.10326933364073436 Validation Loss: 0.7261619567871094\n",
      "Epoch 4780: Training Loss: 0.10320654511451721 Validation Loss: 0.7286544442176819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4781: Training Loss: 0.10315375278393428 Validation Loss: 0.7288386225700378\n",
      "Epoch 4782: Training Loss: 0.10306233416001002 Validation Loss: 0.7311559915542603\n",
      "Epoch 4783: Training Loss: 0.10331279784440994 Validation Loss: 0.7293421030044556\n",
      "Epoch 4784: Training Loss: 0.10299551486968994 Validation Loss: 0.7311263084411621\n",
      "Epoch 4785: Training Loss: 0.10302392393350601 Validation Loss: 0.7305797338485718\n",
      "Epoch 4786: Training Loss: 0.10303403933842976 Validation Loss: 0.7294767498970032\n",
      "Epoch 4787: Training Loss: 0.10295804589986801 Validation Loss: 0.7295292019844055\n",
      "Epoch 4788: Training Loss: 0.10291239370902379 Validation Loss: 0.7289048433303833\n",
      "Epoch 4789: Training Loss: 0.10287563999493916 Validation Loss: 0.7287126183509827\n",
      "Epoch 4790: Training Loss: 0.10288339853286743 Validation Loss: 0.7290148735046387\n",
      "Epoch 4791: Training Loss: 0.10283102840185165 Validation Loss: 0.7291906476020813\n",
      "Epoch 4792: Training Loss: 0.10281271735827129 Validation Loss: 0.7291886210441589\n",
      "Epoch 4793: Training Loss: 0.10275909056266148 Validation Loss: 0.7296349406242371\n",
      "Epoch 4794: Training Loss: 0.10290130227804184 Validation Loss: 0.7282233238220215\n",
      "Epoch 4795: Training Loss: 0.10278921326001485 Validation Loss: 0.7306097149848938\n",
      "Epoch 4796: Training Loss: 0.10274600982666016 Validation Loss: 0.7319051623344421\n",
      "Epoch 4797: Training Loss: 0.10285911212364833 Validation Loss: 0.7332090139389038\n",
      "Epoch 4798: Training Loss: 0.10282858212788899 Validation Loss: 0.7330436706542969\n",
      "Epoch 4799: Training Loss: 0.10257352391878764 Validation Loss: 0.7296577095985413\n",
      "Epoch 4800: Training Loss: 0.1028944527109464 Validation Loss: 0.7243067622184753\n",
      "Epoch 4801: Training Loss: 0.10275606562693913 Validation Loss: 0.72432541847229\n",
      "Epoch 4802: Training Loss: 0.10269755373398463 Validation Loss: 0.7260817289352417\n",
      "Epoch 4803: Training Loss: 0.10251593838135402 Validation Loss: 0.7285154461860657\n",
      "Epoch 4804: Training Loss: 0.10243180890878041 Validation Loss: 0.7304491400718689\n",
      "Epoch 4805: Training Loss: 0.1025156502922376 Validation Loss: 0.7303686738014221\n",
      "Epoch 4806: Training Loss: 0.10289335747559865 Validation Loss: 0.7349241375923157\n",
      "Epoch 4807: Training Loss: 0.10270228733619054 Validation Loss: 0.730005145072937\n",
      "Epoch 4808: Training Loss: 0.10243229568004608 Validation Loss: 0.7282484173774719\n",
      "Epoch 4809: Training Loss: 0.10255037248134613 Validation Loss: 0.7260529398918152\n",
      "Epoch 4810: Training Loss: 0.10245240479707718 Validation Loss: 0.7275342345237732\n",
      "Epoch 4811: Training Loss: 0.10225408524274826 Validation Loss: 0.7294723391532898\n",
      "Epoch 4812: Training Loss: 0.10226801037788391 Validation Loss: 0.7308499217033386\n",
      "Epoch 4813: Training Loss: 0.10237374156713486 Validation Loss: 0.7325820922851562\n",
      "Epoch 4814: Training Loss: 0.10239038864771526 Validation Loss: 0.7350342273712158\n",
      "Epoch 4815: Training Loss: 0.10233801106611888 Validation Loss: 0.7334361672401428\n",
      "Epoch 4816: Training Loss: 0.10286668439706166 Validation Loss: 0.7268582582473755\n",
      "Epoch 4817: Training Loss: 0.10232866803805034 Validation Loss: 0.7282313108444214\n",
      "Epoch 4818: Training Loss: 0.10232507437467575 Validation Loss: 0.7305111289024353\n",
      "Epoch 4819: Training Loss: 0.1021793782711029 Validation Loss: 0.7302375435829163\n",
      "Epoch 4820: Training Loss: 0.10205131272474925 Validation Loss: 0.7275212407112122\n",
      "Epoch 4821: Training Loss: 0.10238219300905864 Validation Loss: 0.723953366279602\n",
      "Epoch 4822: Training Loss: 0.10208592812220256 Validation Loss: 0.7262906432151794\n",
      "Epoch 4823: Training Loss: 0.10198749353488286 Validation Loss: 0.7282658219337463\n",
      "Epoch 4824: Training Loss: 0.10185189048449199 Validation Loss: 0.73161780834198\n",
      "Epoch 4825: Training Loss: 0.10188773274421692 Validation Loss: 0.732998788356781\n",
      "Epoch 4826: Training Loss: 0.10197918862104416 Validation Loss: 0.7331588864326477\n",
      "Epoch 4827: Training Loss: 0.10190412650505702 Validation Loss: 0.7330190539360046\n",
      "Epoch 4828: Training Loss: 0.10187240938345592 Validation Loss: 0.7301826477050781\n",
      "Epoch 4829: Training Loss: 0.10188804070154826 Validation Loss: 0.7275421619415283\n",
      "Epoch 4830: Training Loss: 0.10190791388352712 Validation Loss: 0.7278212904930115\n",
      "Epoch 4831: Training Loss: 0.10173814495404561 Validation Loss: 0.728458821773529\n",
      "Epoch 4832: Training Loss: 0.10172722985347112 Validation Loss: 0.7310623526573181\n",
      "Epoch 4833: Training Loss: 0.10184182475010554 Validation Loss: 0.7288716435432434\n",
      "Epoch 4834: Training Loss: 0.10163872440656026 Validation Loss: 0.7303877472877502\n",
      "Epoch 4835: Training Loss: 0.10167150944471359 Validation Loss: 0.7308285236358643\n",
      "Epoch 4836: Training Loss: 0.10160203278064728 Validation Loss: 0.73099684715271\n",
      "Epoch 4837: Training Loss: 0.10165061056613922 Validation Loss: 0.7315901517868042\n",
      "Epoch 4838: Training Loss: 0.10187886903683345 Validation Loss: 0.7277623414993286\n",
      "Epoch 4839: Training Loss: 0.10168792059024175 Validation Loss: 0.7271609902381897\n",
      "Epoch 4840: Training Loss: 0.10149424026409785 Validation Loss: 0.7281810641288757\n",
      "Epoch 4841: Training Loss: 0.10153743624687195 Validation Loss: 0.7300171852111816\n",
      "Epoch 4842: Training Loss: 0.10148615638415019 Validation Loss: 0.730539083480835\n",
      "Epoch 4843: Training Loss: 0.10147063185771306 Validation Loss: 0.7299638986587524\n",
      "Epoch 4844: Training Loss: 0.10150331507126491 Validation Loss: 0.730553150177002\n",
      "Epoch 4845: Training Loss: 0.10184609393278758 Validation Loss: 0.7273848056793213\n",
      "Epoch 4846: Training Loss: 0.10131511092185974 Validation Loss: 0.7290311455726624\n",
      "Epoch 4847: Training Loss: 0.10148121416568756 Validation Loss: 0.7318016290664673\n",
      "Epoch 4848: Training Loss: 0.10130833089351654 Validation Loss: 0.7328320741653442\n",
      "Epoch 4849: Training Loss: 0.10139112671216328 Validation Loss: 0.7303047180175781\n",
      "Epoch 4850: Training Loss: 0.10128655781348546 Validation Loss: 0.7318443059921265\n",
      "Epoch 4851: Training Loss: 0.10123721758524577 Validation Loss: 0.7302650809288025\n",
      "Epoch 4852: Training Loss: 0.10152180989583333 Validation Loss: 0.7271875143051147\n",
      "Epoch 4853: Training Loss: 0.10122443487246831 Validation Loss: 0.7277707457542419\n",
      "Epoch 4854: Training Loss: 0.101182426015536 Validation Loss: 0.731744110584259\n",
      "Epoch 4855: Training Loss: 0.10122065742810567 Validation Loss: 0.7337257862091064\n",
      "Epoch 4856: Training Loss: 0.10116699089606603 Validation Loss: 0.7330293655395508\n",
      "Epoch 4857: Training Loss: 0.10128364711999893 Validation Loss: 0.7284908294677734\n",
      "Epoch 4858: Training Loss: 0.10105313112338384 Validation Loss: 0.7270140647888184\n",
      "Epoch 4859: Training Loss: 0.10122319062550862 Validation Loss: 0.7305217385292053\n",
      "Epoch 4860: Training Loss: 0.10096945365269978 Validation Loss: 0.7300131916999817\n",
      "Epoch 4861: Training Loss: 0.10115274290243785 Validation Loss: 0.7270445823669434\n",
      "Epoch 4862: Training Loss: 0.10133388638496399 Validation Loss: 0.7259631156921387\n",
      "Epoch 4863: Training Loss: 0.10102057208617528 Validation Loss: 0.7308754920959473\n",
      "Epoch 4864: Training Loss: 0.10097540418306987 Validation Loss: 0.734388530254364\n",
      "Epoch 4865: Training Loss: 0.1009620726108551 Validation Loss: 0.7329413294792175\n",
      "Epoch 4866: Training Loss: 0.10156525671482086 Validation Loss: 0.7364162802696228\n",
      "Epoch 4867: Training Loss: 0.1008579855163892 Validation Loss: 0.7326556444168091\n",
      "Epoch 4868: Training Loss: 0.10081353535254796 Validation Loss: 0.7299354076385498\n",
      "Epoch 4869: Training Loss: 0.10076617201169331 Validation Loss: 0.7257141470909119\n",
      "Epoch 4870: Training Loss: 0.10127061108748119 Validation Loss: 0.7225812673568726\n",
      "Epoch 4871: Training Loss: 0.10089463740587234 Validation Loss: 0.7259702086448669\n",
      "Epoch 4872: Training Loss: 0.10083137452602386 Validation Loss: 0.7317414879798889\n",
      "Epoch 4873: Training Loss: 0.10074736674626668 Validation Loss: 0.7347500920295715\n",
      "Epoch 4874: Training Loss: 0.10086627552906673 Validation Loss: 0.7358218431472778\n",
      "Epoch 4875: Training Loss: 0.10068123787641525 Validation Loss: 0.7314696907997131\n",
      "Epoch 4876: Training Loss: 0.10067114482323329 Validation Loss: 0.727101743221283\n",
      "Epoch 4877: Training Loss: 0.10075432062149048 Validation Loss: 0.7244154810905457\n",
      "Epoch 4878: Training Loss: 0.1006910428404808 Validation Loss: 0.7254235148429871\n",
      "Epoch 4879: Training Loss: 0.10069849093755086 Validation Loss: 0.7315526008605957\n",
      "Epoch 4880: Training Loss: 0.10075314591328303 Validation Loss: 0.7355902791023254\n",
      "Epoch 4881: Training Loss: 0.10057743142048518 Validation Loss: 0.7346723079681396\n",
      "Epoch 4882: Training Loss: 0.10040859133005142 Validation Loss: 0.7315748333930969\n",
      "Epoch 4883: Training Loss: 0.10036332656939824 Validation Loss: 0.7284991145133972\n",
      "Epoch 4884: Training Loss: 0.10043385128180186 Validation Loss: 0.7279564738273621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4885: Training Loss: 0.10035982728004456 Validation Loss: 0.7264055013656616\n",
      "Epoch 4886: Training Loss: 0.10038753847281139 Validation Loss: 0.7281895875930786\n",
      "Epoch 4887: Training Loss: 0.10027257601420085 Validation Loss: 0.728691577911377\n",
      "Epoch 4888: Training Loss: 0.10031290352344513 Validation Loss: 0.7318710684776306\n",
      "Epoch 4889: Training Loss: 0.10031724224487941 Validation Loss: 0.7304883599281311\n",
      "Epoch 4890: Training Loss: 0.10031421234210332 Validation Loss: 0.7316942811012268\n",
      "Epoch 4891: Training Loss: 0.10023475935061772 Validation Loss: 0.7307280898094177\n",
      "Epoch 4892: Training Loss: 0.10014468679825465 Validation Loss: 0.7293034195899963\n",
      "Epoch 4893: Training Loss: 0.1000984584291776 Validation Loss: 0.7286221385002136\n",
      "Epoch 4894: Training Loss: 0.10014184564352036 Validation Loss: 0.7300143241882324\n",
      "Epoch 4895: Training Loss: 0.10008456806341808 Validation Loss: 0.7305365800857544\n",
      "Epoch 4896: Training Loss: 0.1000370979309082 Validation Loss: 0.7307382225990295\n",
      "Epoch 4897: Training Loss: 0.10004955530166626 Validation Loss: 0.7294343113899231\n",
      "Epoch 4898: Training Loss: 0.09998967746893565 Validation Loss: 0.7300135493278503\n",
      "Epoch 4899: Training Loss: 0.10010344286759694 Validation Loss: 0.7321540117263794\n",
      "Epoch 4900: Training Loss: 0.10020385434230168 Validation Loss: 0.7281308174133301\n",
      "Epoch 4901: Training Loss: 0.10000317047039668 Validation Loss: 0.7297472357749939\n",
      "Epoch 4902: Training Loss: 0.09993694722652435 Validation Loss: 0.7300119996070862\n",
      "Epoch 4903: Training Loss: 0.09986293812592824 Validation Loss: 0.7302610278129578\n",
      "Epoch 4904: Training Loss: 0.09990046421686809 Validation Loss: 0.7316792011260986\n",
      "Epoch 4905: Training Loss: 0.09991692006587982 Validation Loss: 0.7332382798194885\n",
      "Epoch 4906: Training Loss: 0.09986115743716557 Validation Loss: 0.7328330874443054\n",
      "Epoch 4907: Training Loss: 0.09974336127440135 Validation Loss: 0.7312013506889343\n",
      "Epoch 4908: Training Loss: 0.09980690479278564 Validation Loss: 0.7276961803436279\n",
      "Epoch 4909: Training Loss: 0.09979296227296193 Validation Loss: 0.7269802689552307\n",
      "Epoch 4910: Training Loss: 0.09982972592115402 Validation Loss: 0.7265383005142212\n",
      "Epoch 4911: Training Loss: 0.09967568268378575 Validation Loss: 0.7292781472206116\n",
      "Epoch 4912: Training Loss: 0.09974480420351028 Validation Loss: 0.7338820099830627\n",
      "Epoch 4913: Training Loss: 0.09985681623220444 Validation Loss: 0.7361785769462585\n",
      "Epoch 4914: Training Loss: 0.09973173836867015 Validation Loss: 0.7318528890609741\n",
      "Epoch 4915: Training Loss: 0.09963346521059673 Validation Loss: 0.7304022312164307\n",
      "Epoch 4916: Training Loss: 0.0995673934618632 Validation Loss: 0.7271096110343933\n",
      "Epoch 4917: Training Loss: 0.09957012782494228 Validation Loss: 0.7285481095314026\n",
      "Epoch 4918: Training Loss: 0.10000238070885341 Validation Loss: 0.7257160544395447\n",
      "Epoch 4919: Training Loss: 0.10005843639373779 Validation Loss: 0.7323873043060303\n",
      "Epoch 4920: Training Loss: 0.09947919597228368 Validation Loss: 0.7330363988876343\n",
      "Epoch 4921: Training Loss: 0.09947449713945389 Validation Loss: 0.7320346832275391\n",
      "Epoch 4922: Training Loss: 0.09944145878156026 Validation Loss: 0.7301135659217834\n",
      "Epoch 4923: Training Loss: 0.09964808573325475 Validation Loss: 0.7272815108299255\n",
      "Epoch 4924: Training Loss: 0.09944356977939606 Validation Loss: 0.7312188744544983\n",
      "Epoch 4925: Training Loss: 0.0993523250023524 Validation Loss: 0.7333934903144836\n",
      "Epoch 4926: Training Loss: 0.09933002044757207 Validation Loss: 0.731869101524353\n",
      "Epoch 4927: Training Loss: 0.09926816076040268 Validation Loss: 0.7326855063438416\n",
      "Epoch 4928: Training Loss: 0.09924574941396713 Validation Loss: 0.7306950092315674\n",
      "Epoch 4929: Training Loss: 0.09940876066684723 Validation Loss: 0.7308883666992188\n",
      "Epoch 4930: Training Loss: 0.0991441657145818 Validation Loss: 0.7295215129852295\n",
      "Epoch 4931: Training Loss: 0.09923163553078969 Validation Loss: 0.7307161688804626\n",
      "Epoch 4932: Training Loss: 0.09935557593901952 Validation Loss: 0.732756495475769\n",
      "Epoch 4933: Training Loss: 0.09918519854545593 Validation Loss: 0.7317380905151367\n",
      "Epoch 4934: Training Loss: 0.09908337394396464 Validation Loss: 0.728405773639679\n",
      "Epoch 4935: Training Loss: 0.09914729495843251 Validation Loss: 0.7289270758628845\n",
      "Epoch 4936: Training Loss: 0.09902706742286682 Validation Loss: 0.7287523746490479\n",
      "Epoch 4937: Training Loss: 0.09904147684574127 Validation Loss: 0.7283504009246826\n",
      "Epoch 4938: Training Loss: 0.09911497433980306 Validation Loss: 0.7307692170143127\n",
      "Epoch 4939: Training Loss: 0.09902798632780711 Validation Loss: 0.7312172651290894\n",
      "Epoch 4940: Training Loss: 0.09895547479391098 Validation Loss: 0.7288939952850342\n",
      "Epoch 4941: Training Loss: 0.09908083577950795 Validation Loss: 0.7271589040756226\n",
      "Epoch 4942: Training Loss: 0.09895715117454529 Validation Loss: 0.7291588187217712\n",
      "Epoch 4943: Training Loss: 0.09886820117632548 Validation Loss: 0.7302550673484802\n",
      "Epoch 4944: Training Loss: 0.09882422536611557 Validation Loss: 0.7327297925949097\n",
      "Epoch 4945: Training Loss: 0.09880317250887553 Validation Loss: 0.7321053147315979\n",
      "Epoch 4946: Training Loss: 0.0988511194785436 Validation Loss: 0.7304626703262329\n",
      "Epoch 4947: Training Loss: 0.09895286957422893 Validation Loss: 0.7335681319236755\n",
      "Epoch 4948: Training Loss: 0.09911230951547623 Validation Loss: 0.7303661108016968\n",
      "Epoch 4949: Training Loss: 0.09874334434668224 Validation Loss: 0.7307710647583008\n",
      "Epoch 4950: Training Loss: 0.09875667343537013 Validation Loss: 0.7327784895896912\n",
      "Epoch 4951: Training Loss: 0.09864436089992523 Validation Loss: 0.7309143543243408\n",
      "Epoch 4952: Training Loss: 0.09869624674320221 Validation Loss: 0.7289554476737976\n",
      "Epoch 4953: Training Loss: 0.09861741711695989 Validation Loss: 0.7297478914260864\n",
      "Epoch 4954: Training Loss: 0.09857659538586934 Validation Loss: 0.730157732963562\n",
      "Epoch 4955: Training Loss: 0.09880658239126205 Validation Loss: 0.7336174845695496\n",
      "Epoch 4956: Training Loss: 0.09880902866522472 Validation Loss: 0.7343733906745911\n",
      "Epoch 4957: Training Loss: 0.0990031510591507 Validation Loss: 0.7278141379356384\n",
      "Epoch 4958: Training Loss: 0.09860380987326305 Validation Loss: 0.7267224788665771\n",
      "Epoch 4959: Training Loss: 0.09850628922382991 Validation Loss: 0.7286154627799988\n",
      "Epoch 4960: Training Loss: 0.09843263278404872 Validation Loss: 0.7297303080558777\n",
      "Epoch 4961: Training Loss: 0.09842689583698909 Validation Loss: 0.732139527797699\n",
      "Epoch 4962: Training Loss: 0.0984738419453303 Validation Loss: 0.7308617234230042\n",
      "Epoch 4963: Training Loss: 0.09860924631357193 Validation Loss: 0.7345322370529175\n",
      "Epoch 4964: Training Loss: 0.09842824687560399 Validation Loss: 0.7318832874298096\n",
      "Epoch 4965: Training Loss: 0.09828820576270421 Validation Loss: 0.7306734323501587\n",
      "Epoch 4966: Training Loss: 0.09823821981747945 Validation Loss: 0.7303377985954285\n",
      "Epoch 4967: Training Loss: 0.09848437209924062 Validation Loss: 0.7284958958625793\n",
      "Epoch 4968: Training Loss: 0.0982404425740242 Validation Loss: 0.7296450138092041\n",
      "Epoch 4969: Training Loss: 0.09823321799437205 Validation Loss: 0.7321586608886719\n",
      "Epoch 4970: Training Loss: 0.09830599278211594 Validation Loss: 0.7348291277885437\n",
      "Epoch 4971: Training Loss: 0.09826898574829102 Validation Loss: 0.7329439520835876\n",
      "Epoch 4972: Training Loss: 0.09813143809636433 Validation Loss: 0.7322384715080261\n",
      "Epoch 4973: Training Loss: 0.09822497020165126 Validation Loss: 0.731590747833252\n",
      "Epoch 4974: Training Loss: 0.09805476665496826 Validation Loss: 0.7290508151054382\n",
      "Epoch 4975: Training Loss: 0.09839860598246257 Validation Loss: 0.7254475951194763\n",
      "Epoch 4976: Training Loss: 0.09812479217847188 Validation Loss: 0.7268112897872925\n",
      "Epoch 4977: Training Loss: 0.09822553147872289 Validation Loss: 0.7326017618179321\n",
      "Epoch 4978: Training Loss: 0.09797043353319168 Validation Loss: 0.7338458895683289\n",
      "Epoch 4979: Training Loss: 0.09816377361615498 Validation Loss: 0.7351763844490051\n",
      "Epoch 4980: Training Loss: 0.09800599018732707 Validation Loss: 0.7313527464866638\n",
      "Epoch 4981: Training Loss: 0.0979182521502177 Validation Loss: 0.7302404642105103\n",
      "Epoch 4982: Training Loss: 0.09814828634262085 Validation Loss: 0.7323204278945923\n",
      "Epoch 4983: Training Loss: 0.09787773340940475 Validation Loss: 0.732007622718811\n",
      "Epoch 4984: Training Loss: 0.09821726381778717 Validation Loss: 0.7269128561019897\n",
      "Epoch 4985: Training Loss: 0.09807982047398885 Validation Loss: 0.7285968661308289\n",
      "Epoch 4986: Training Loss: 0.09783482799927394 Validation Loss: 0.7311874628067017\n",
      "Epoch 4987: Training Loss: 0.09774783750375111 Validation Loss: 0.7320518493652344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4988: Training Loss: 0.09777584671974182 Validation Loss: 0.7336953282356262\n",
      "Epoch 4989: Training Loss: 0.09776794662078221 Validation Loss: 0.7333354949951172\n",
      "Epoch 4990: Training Loss: 0.0976986934741338 Validation Loss: 0.7322834730148315\n",
      "Epoch 4991: Training Loss: 0.09763203561306 Validation Loss: 0.7302043437957764\n",
      "Epoch 4992: Training Loss: 0.09774184475342433 Validation Loss: 0.7281869053840637\n",
      "Epoch 4993: Training Loss: 0.09769310802221298 Validation Loss: 0.7279810905456543\n",
      "Epoch 4994: Training Loss: 0.09775133679310481 Validation Loss: 0.7280999422073364\n",
      "Epoch 4995: Training Loss: 0.09754525870084763 Validation Loss: 0.7320595383644104\n",
      "Epoch 4996: Training Loss: 0.09756297618150711 Validation Loss: 0.7354215979576111\n",
      "Epoch 4997: Training Loss: 0.09777361402908961 Validation Loss: 0.7372674942016602\n",
      "Epoch 4998: Training Loss: 0.09756942590077718 Validation Loss: 0.7348155379295349\n",
      "Epoch 4999: Training Loss: 0.09742144246896108 Validation Loss: 0.7315545082092285\n",
      "Epoch 5000: Training Loss: 0.0974454755584399 Validation Loss: 0.7306166291236877\n",
      "Epoch 5001: Training Loss: 0.0974570984641711 Validation Loss: 0.7284331321716309\n",
      "Epoch 5002: Training Loss: 0.09750464061896007 Validation Loss: 0.7287423610687256\n",
      "Epoch 5003: Training Loss: 0.09747820347547531 Validation Loss: 0.7301768660545349\n",
      "Epoch 5004: Training Loss: 0.09735100467999776 Validation Loss: 0.7298596501350403\n",
      "Epoch 5005: Training Loss: 0.0973249798019727 Validation Loss: 0.7297590374946594\n",
      "Epoch 5006: Training Loss: 0.09733284016450246 Validation Loss: 0.7301689386367798\n",
      "Epoch 5007: Training Loss: 0.09750265131394069 Validation Loss: 0.7333572506904602\n",
      "Epoch 5008: Training Loss: 0.09727230419715245 Validation Loss: 0.7322346568107605\n",
      "Epoch 5009: Training Loss: 0.09726704905430476 Validation Loss: 0.7299788594245911\n",
      "Epoch 5010: Training Loss: 0.09730003277460735 Validation Loss: 0.7286592721939087\n",
      "Epoch 5011: Training Loss: 0.09737181415160497 Validation Loss: 0.7325186133384705\n",
      "Epoch 5012: Training Loss: 0.09739932169516881 Validation Loss: 0.7303072214126587\n",
      "Epoch 5013: Training Loss: 0.09710746755202611 Validation Loss: 0.7313960790634155\n",
      "Epoch 5014: Training Loss: 0.09714724371830623 Validation Loss: 0.7329568862915039\n",
      "Epoch 5015: Training Loss: 0.0972290833791097 Validation Loss: 0.7307944297790527\n",
      "Epoch 5016: Training Loss: 0.09704920649528503 Validation Loss: 0.7308452129364014\n",
      "Epoch 5017: Training Loss: 0.09743562340736389 Validation Loss: 0.7349793314933777\n",
      "Epoch 5018: Training Loss: 0.09721732884645462 Validation Loss: 0.7310159802436829\n",
      "Epoch 5019: Training Loss: 0.09701173255840938 Validation Loss: 0.7313249111175537\n",
      "Epoch 5020: Training Loss: 0.09696106612682343 Validation Loss: 0.7305577397346497\n",
      "Epoch 5021: Training Loss: 0.09694194048643112 Validation Loss: 0.7305845618247986\n",
      "Epoch 5022: Training Loss: 0.096958393851916 Validation Loss: 0.7320125102996826\n",
      "Epoch 5023: Training Loss: 0.0968903402487437 Validation Loss: 0.7332761883735657\n",
      "Epoch 5024: Training Loss: 0.09694952021042506 Validation Loss: 0.7315015196800232\n",
      "Epoch 5025: Training Loss: 0.0970026006301244 Validation Loss: 0.73386549949646\n",
      "Epoch 5026: Training Loss: 0.09692616015672684 Validation Loss: 0.7342311143875122\n",
      "Epoch 5027: Training Loss: 0.0968255673845609 Validation Loss: 0.7311789393424988\n",
      "Epoch 5028: Training Loss: 0.09673862159252167 Validation Loss: 0.7298440337181091\n",
      "Epoch 5029: Training Loss: 0.09679494549830754 Validation Loss: 0.7291567921638489\n",
      "Epoch 5030: Training Loss: 0.09705643852551778 Validation Loss: 0.7326700687408447\n",
      "Epoch 5031: Training Loss: 0.09691015630960464 Validation Loss: 0.7300481200218201\n",
      "Epoch 5032: Training Loss: 0.09672874460617702 Validation Loss: 0.7301417589187622\n",
      "Epoch 5033: Training Loss: 0.09676260749499004 Validation Loss: 0.7293687462806702\n",
      "Epoch 5034: Training Loss: 0.09684247026840846 Validation Loss: 0.7326884865760803\n",
      "Epoch 5035: Training Loss: 0.09671064217885335 Validation Loss: 0.7333817481994629\n",
      "Epoch 5036: Training Loss: 0.09676368286212285 Validation Loss: 0.7336527705192566\n",
      "Epoch 5037: Training Loss: 0.09682155400514603 Validation Loss: 0.7289084792137146\n",
      "Epoch 5038: Training Loss: 0.09654713173707326 Validation Loss: 0.7284101247787476\n",
      "Epoch 5039: Training Loss: 0.09655326356490453 Validation Loss: 0.7294859290122986\n",
      "Epoch 5040: Training Loss: 0.09652241071065266 Validation Loss: 0.730163037776947\n",
      "Epoch 5041: Training Loss: 0.09658307830492656 Validation Loss: 0.7337803244590759\n",
      "Epoch 5042: Training Loss: 0.096515158812205 Validation Loss: 0.7354318499565125\n",
      "Epoch 5043: Training Loss: 0.09663890053828557 Validation Loss: 0.7327550649642944\n",
      "Epoch 5044: Training Loss: 0.09655897319316864 Validation Loss: 0.7295353412628174\n",
      "Epoch 5045: Training Loss: 0.09645559638738632 Validation Loss: 0.73185795545578\n",
      "Epoch 5046: Training Loss: 0.09659885118405025 Validation Loss: 0.7345422506332397\n",
      "Epoch 5047: Training Loss: 0.09634170432885487 Validation Loss: 0.732478141784668\n",
      "Epoch 5048: Training Loss: 0.09632952759663264 Validation Loss: 0.7305577397346497\n",
      "Epoch 5049: Training Loss: 0.09635562201340993 Validation Loss: 0.7281195521354675\n",
      "Epoch 5050: Training Loss: 0.09628501037756602 Validation Loss: 0.7293034195899963\n",
      "Epoch 5051: Training Loss: 0.09647191067536671 Validation Loss: 0.7318770289421082\n",
      "Epoch 5052: Training Loss: 0.09626539051532745 Validation Loss: 0.7306563258171082\n",
      "Epoch 5053: Training Loss: 0.09616963813702266 Validation Loss: 0.7311474084854126\n",
      "Epoch 5054: Training Loss: 0.09618198374907176 Validation Loss: 0.7309672236442566\n",
      "Epoch 5055: Training Loss: 0.09612542142470677 Validation Loss: 0.7324665188789368\n",
      "Epoch 5056: Training Loss: 0.096255657573541 Validation Loss: 0.7315620183944702\n",
      "Epoch 5057: Training Loss: 0.09606603036324184 Validation Loss: 0.7328174114227295\n",
      "Epoch 5058: Training Loss: 0.09617248674233754 Validation Loss: 0.7352306842803955\n",
      "Epoch 5059: Training Loss: 0.09633221973975499 Validation Loss: 0.7321762442588806\n",
      "Epoch 5060: Training Loss: 0.09605202575524648 Validation Loss: 0.7317230701446533\n",
      "Epoch 5061: Training Loss: 0.09611467023690541 Validation Loss: 0.7343795895576477\n",
      "Epoch 5062: Training Loss: 0.09607421110073726 Validation Loss: 0.735397219657898\n",
      "Epoch 5063: Training Loss: 0.09605588018894196 Validation Loss: 0.7318583130836487\n",
      "Epoch 5064: Training Loss: 0.09590947876373927 Validation Loss: 0.7310342192649841\n",
      "Epoch 5065: Training Loss: 0.09606908758481343 Validation Loss: 0.7327153086662292\n",
      "Epoch 5066: Training Loss: 0.09611766288677852 Validation Loss: 0.7283234000205994\n",
      "Epoch 5067: Training Loss: 0.0961579903960228 Validation Loss: 0.7268185019493103\n",
      "Epoch 5068: Training Loss: 0.09596532086531322 Validation Loss: 0.7312579154968262\n",
      "Epoch 5069: Training Loss: 0.09591648230950038 Validation Loss: 0.7332982420921326\n",
      "Epoch 5070: Training Loss: 0.09581382821003596 Validation Loss: 0.7326957583427429\n",
      "Epoch 5071: Training Loss: 0.09590785950422287 Validation Loss: 0.7318654656410217\n",
      "Epoch 5072: Training Loss: 0.09577994545300801 Validation Loss: 0.7311907410621643\n",
      "Epoch 5073: Training Loss: 0.09572543452183406 Validation Loss: 0.732451856136322\n",
      "Epoch 5074: Training Loss: 0.09569434821605682 Validation Loss: 0.7329277992248535\n",
      "Epoch 5075: Training Loss: 0.09565979242324829 Validation Loss: 0.732487142086029\n",
      "Epoch 5076: Training Loss: 0.09566618253787358 Validation Loss: 0.7326670289039612\n",
      "Epoch 5077: Training Loss: 0.09569337218999863 Validation Loss: 0.7329211235046387\n",
      "Epoch 5078: Training Loss: 0.09576504677534103 Validation Loss: 0.7338353991508484\n",
      "Epoch 5079: Training Loss: 0.09559729943672816 Validation Loss: 0.7310339212417603\n",
      "Epoch 5080: Training Loss: 0.09580972542365392 Validation Loss: 0.7291948795318604\n",
      "Epoch 5081: Training Loss: 0.09564748654762904 Validation Loss: 0.7315473556518555\n",
      "Epoch 5082: Training Loss: 0.09586727370818456 Validation Loss: 0.7350592613220215\n",
      "Epoch 5083: Training Loss: 0.09550427397092183 Validation Loss: 0.7339911460876465\n",
      "Epoch 5084: Training Loss: 0.09569897999366124 Validation Loss: 0.7348670363426208\n",
      "Epoch 5085: Training Loss: 0.09555766483147939 Validation Loss: 0.7303341031074524\n",
      "Epoch 5086: Training Loss: 0.09547899911801021 Validation Loss: 0.7276244759559631\n",
      "Epoch 5087: Training Loss: 0.09551995992660522 Validation Loss: 0.7290123701095581\n",
      "Epoch 5088: Training Loss: 0.09544557829697926 Validation Loss: 0.7307674288749695\n",
      "Epoch 5089: Training Loss: 0.0957617238163948 Validation Loss: 0.7354409694671631\n",
      "Epoch 5090: Training Loss: 0.09534246971209843 Validation Loss: 0.7341576814651489\n",
      "Epoch 5091: Training Loss: 0.09551673630873363 Validation Loss: 0.7353830933570862\n",
      "Epoch 5092: Training Loss: 0.09574943780899048 Validation Loss: 0.7293422222137451\n",
      "Epoch 5093: Training Loss: 0.09534591436386108 Validation Loss: 0.7300344705581665\n",
      "Epoch 5094: Training Loss: 0.09537771592537563 Validation Loss: 0.729509711265564\n",
      "Epoch 5095: Training Loss: 0.09523899853229523 Validation Loss: 0.7325572371482849\n",
      "Epoch 5096: Training Loss: 0.09517781933148702 Validation Loss: 0.7332314252853394\n",
      "Epoch 5097: Training Loss: 0.09542088210582733 Validation Loss: 0.7360333204269409\n",
      "Epoch 5098: Training Loss: 0.095278466741244 Validation Loss: 0.7351517677307129\n",
      "Epoch 5099: Training Loss: 0.09511837114890416 Validation Loss: 0.7334561347961426\n",
      "Epoch 5100: Training Loss: 0.09509243567784627 Validation Loss: 0.7305386662483215\n",
      "Epoch 5101: Training Loss: 0.09508526076873143 Validation Loss: 0.7300436496734619\n",
      "Epoch 5102: Training Loss: 0.09507468342781067 Validation Loss: 0.729313850402832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5103: Training Loss: 0.09508076806863149 Validation Loss: 0.730160117149353\n",
      "Epoch 5104: Training Loss: 0.09517855445543925 Validation Loss: 0.7330771684646606\n",
      "Epoch 5105: Training Loss: 0.09516370544830959 Validation Loss: 0.7308424115180969\n",
      "Epoch 5106: Training Loss: 0.09497675796349843 Validation Loss: 0.7325954437255859\n",
      "Epoch 5107: Training Loss: 0.09494605660438538 Validation Loss: 0.7330996990203857\n",
      "Epoch 5108: Training Loss: 0.09495609750350316 Validation Loss: 0.7341614365577698\n",
      "Epoch 5109: Training Loss: 0.09506205221017201 Validation Loss: 0.7347403168678284\n",
      "Epoch 5110: Training Loss: 0.09488974263270696 Validation Loss: 0.7329272031784058\n",
      "Epoch 5111: Training Loss: 0.09522022555271785 Validation Loss: 0.7281580567359924\n",
      "Epoch 5112: Training Loss: 0.09489735960960388 Validation Loss: 0.7287405133247375\n",
      "Epoch 5113: Training Loss: 0.0948973720272382 Validation Loss: 0.7311989068984985\n",
      "Epoch 5114: Training Loss: 0.09496679902076721 Validation Loss: 0.7340254783630371\n",
      "Epoch 5115: Training Loss: 0.0949220930536588 Validation Loss: 0.7360010743141174\n",
      "Epoch 5116: Training Loss: 0.09501676013072331 Validation Loss: 0.7314897179603577\n",
      "Epoch 5117: Training Loss: 0.09476929654677708 Validation Loss: 0.7323409914970398\n",
      "Epoch 5118: Training Loss: 0.09473649909098943 Validation Loss: 0.7306596636772156\n",
      "Epoch 5119: Training Loss: 0.09473458429177602 Validation Loss: 0.7317770719528198\n",
      "Epoch 5120: Training Loss: 0.09492841859658559 Validation Loss: 0.734158992767334\n",
      "Epoch 5121: Training Loss: 0.09480358411868413 Validation Loss: 0.734697699546814\n",
      "Epoch 5122: Training Loss: 0.09468960762023926 Validation Loss: 0.7318585515022278\n",
      "Epoch 5123: Training Loss: 0.09467607488234837 Validation Loss: 0.7315347194671631\n",
      "Epoch 5124: Training Loss: 0.0945742204785347 Validation Loss: 0.73183274269104\n",
      "Epoch 5125: Training Loss: 0.09454669803380966 Validation Loss: 0.7312532067298889\n",
      "Epoch 5126: Training Loss: 0.09455517927805583 Validation Loss: 0.7328411340713501\n",
      "Epoch 5127: Training Loss: 0.09459725022315979 Validation Loss: 0.7344801425933838\n",
      "Epoch 5128: Training Loss: 0.09448590129613876 Validation Loss: 0.7331711649894714\n",
      "Epoch 5129: Training Loss: 0.0946006253361702 Validation Loss: 0.7299339771270752\n",
      "Epoch 5130: Training Loss: 0.09442584713300069 Validation Loss: 0.7302643060684204\n",
      "Epoch 5131: Training Loss: 0.09457295884688695 Validation Loss: 0.733328640460968\n",
      "Epoch 5132: Training Loss: 0.09458600481351216 Validation Loss: 0.7354964017868042\n",
      "Epoch 5133: Training Loss: 0.09438018004099528 Validation Loss: 0.7328640818595886\n",
      "Epoch 5134: Training Loss: 0.09431686997413635 Validation Loss: 0.7309908866882324\n",
      "Epoch 5135: Training Loss: 0.09443125625451405 Validation Loss: 0.7290263772010803\n",
      "Epoch 5136: Training Loss: 0.0943412979443868 Validation Loss: 0.730198323726654\n",
      "Epoch 5137: Training Loss: 0.09456453720728557 Validation Loss: 0.7338621020317078\n",
      "Epoch 5138: Training Loss: 0.09439585109551747 Validation Loss: 0.731785237789154\n",
      "Epoch 5139: Training Loss: 0.09430461873610814 Validation Loss: 0.7338244318962097\n",
      "Epoch 5140: Training Loss: 0.09423789878686269 Validation Loss: 0.7327525019645691\n",
      "Epoch 5141: Training Loss: 0.0941965679327647 Validation Loss: 0.7336512804031372\n",
      "Epoch 5142: Training Loss: 0.09421207755804062 Validation Loss: 0.7328636050224304\n",
      "Epoch 5143: Training Loss: 0.09426891058683395 Validation Loss: 0.7340204119682312\n",
      "Epoch 5144: Training Loss: 0.0941958948969841 Validation Loss: 0.7335308790206909\n",
      "Epoch 5145: Training Loss: 0.09425420065720876 Validation Loss: 0.7342159152030945\n",
      "Epoch 5146: Training Loss: 0.09413267175356548 Validation Loss: 0.7346019148826599\n",
      "Epoch 5147: Training Loss: 0.09413972745339076 Validation Loss: 0.7332205772399902\n",
      "Epoch 5148: Training Loss: 0.0940294936299324 Validation Loss: 0.7295692563056946\n",
      "Epoch 5149: Training Loss: 0.09403196225563686 Validation Loss: 0.7292467951774597\n",
      "Epoch 5150: Training Loss: 0.09408588955799739 Validation Loss: 0.7307708263397217\n",
      "Epoch 5151: Training Loss: 0.09420856088399887 Validation Loss: 0.73350590467453\n",
      "Epoch 5152: Training Loss: 0.0939504827062289 Validation Loss: 0.7335354685783386\n",
      "Epoch 5153: Training Loss: 0.09388892352581024 Validation Loss: 0.7327185273170471\n",
      "Epoch 5154: Training Loss: 0.09394247581561406 Validation Loss: 0.7339687347412109\n",
      "Epoch 5155: Training Loss: 0.09386857599020004 Validation Loss: 0.7335789799690247\n",
      "Epoch 5156: Training Loss: 0.0939986805121104 Validation Loss: 0.731317400932312\n",
      "Epoch 5157: Training Loss: 0.09381457418203354 Validation Loss: 0.7311165928840637\n",
      "Epoch 5158: Training Loss: 0.09419804066419601 Validation Loss: 0.7288656234741211\n",
      "Epoch 5159: Training Loss: 0.09378935396671295 Validation Loss: 0.730659008026123\n",
      "Epoch 5160: Training Loss: 0.09417601674795151 Validation Loss: 0.736420750617981\n",
      "Epoch 5161: Training Loss: 0.09385250508785248 Validation Loss: 0.7354728579521179\n",
      "Epoch 5162: Training Loss: 0.09389092773199081 Validation Loss: 0.7360364198684692\n",
      "Epoch 5163: Training Loss: 0.09372898439566295 Validation Loss: 0.7352489233016968\n",
      "Epoch 5164: Training Loss: 0.09396804124116898 Validation Loss: 0.732075035572052\n",
      "Epoch 5165: Training Loss: 0.09391614298025767 Validation Loss: 0.7341797947883606\n",
      "Epoch 5166: Training Loss: 0.0937368596593539 Validation Loss: 0.7339640259742737\n",
      "Epoch 5167: Training Loss: 0.09358305484056473 Validation Loss: 0.7325882911682129\n",
      "Epoch 5168: Training Loss: 0.09365370869636536 Validation Loss: 0.7310051918029785\n",
      "Epoch 5169: Training Loss: 0.09380899618069331 Validation Loss: 0.731776773929596\n",
      "Epoch 5170: Training Loss: 0.09362634519735973 Validation Loss: 0.7302103638648987\n",
      "Epoch 5171: Training Loss: 0.09350275993347168 Validation Loss: 0.7312285900115967\n",
      "Epoch 5172: Training Loss: 0.09349675476551056 Validation Loss: 0.7323223948478699\n",
      "Epoch 5173: Training Loss: 0.09344630688428879 Validation Loss: 0.7343330383300781\n",
      "Epoch 5174: Training Loss: 0.09347777316967647 Validation Loss: 0.7347432971000671\n",
      "Epoch 5175: Training Loss: 0.09356727202733357 Validation Loss: 0.7364940643310547\n",
      "Epoch 5176: Training Loss: 0.09345389157533646 Validation Loss: 0.7341117262840271\n",
      "Epoch 5177: Training Loss: 0.09339029341936111 Validation Loss: 0.7320326566696167\n",
      "Epoch 5178: Training Loss: 0.09356838713089626 Validation Loss: 0.7344352602958679\n",
      "Epoch 5179: Training Loss: 0.09344976892073949 Validation Loss: 0.7310773730278015\n",
      "Epoch 5180: Training Loss: 0.09341179082791011 Validation Loss: 0.731522798538208\n",
      "Epoch 5181: Training Loss: 0.0933127651611964 Validation Loss: 0.7321627736091614\n",
      "Epoch 5182: Training Loss: 0.09334326287110646 Validation Loss: 0.7303977608680725\n",
      "Epoch 5183: Training Loss: 0.09324866284926732 Validation Loss: 0.7315752506256104\n",
      "Epoch 5184: Training Loss: 0.09322921683390935 Validation Loss: 0.7332531213760376\n",
      "Epoch 5185: Training Loss: 0.09343341489632924 Validation Loss: 0.7363067865371704\n",
      "Epoch 5186: Training Loss: 0.09325599173704784 Validation Loss: 0.7360821962356567\n",
      "Epoch 5187: Training Loss: 0.09312547991673152 Validation Loss: 0.7339335083961487\n",
      "Epoch 5188: Training Loss: 0.09313276161750157 Validation Loss: 0.7320995330810547\n",
      "Epoch 5189: Training Loss: 0.09316754341125488 Validation Loss: 0.7325455546379089\n",
      "Epoch 5190: Training Loss: 0.09309252351522446 Validation Loss: 0.7313442230224609\n",
      "Epoch 5191: Training Loss: 0.09324091424544652 Validation Loss: 0.7311093211174011\n",
      "Epoch 5192: Training Loss: 0.0930889497200648 Validation Loss: 0.7312217950820923\n",
      "Epoch 5193: Training Loss: 0.09300752232472102 Validation Loss: 0.7333981990814209\n",
      "Epoch 5194: Training Loss: 0.09316341578960419 Validation Loss: 0.7367950081825256\n",
      "Epoch 5195: Training Loss: 0.09314077844222386 Validation Loss: 0.7377196550369263\n",
      "Epoch 5196: Training Loss: 0.0930559312303861 Validation Loss: 0.7334452867507935\n",
      "Epoch 5197: Training Loss: 0.09295427550872166 Validation Loss: 0.7312281131744385\n",
      "Epoch 5198: Training Loss: 0.09292663633823395 Validation Loss: 0.7308624982833862\n",
      "Epoch 5199: Training Loss: 0.09307618687550227 Validation Loss: 0.730033814907074\n",
      "Epoch 5200: Training Loss: 0.09290446092685063 Validation Loss: 0.730800449848175\n",
      "Epoch 5201: Training Loss: 0.09287550797065099 Validation Loss: 0.7318644523620605\n",
      "Epoch 5202: Training Loss: 0.0927929828564326 Validation Loss: 0.7343694567680359\n",
      "Epoch 5203: Training Loss: 0.09290346751610438 Validation Loss: 0.7343712449073792\n",
      "Epoch 5204: Training Loss: 0.09321743249893188 Validation Loss: 0.7379132509231567\n",
      "Epoch 5205: Training Loss: 0.0929015005628268 Validation Loss: 0.73736572265625\n",
      "Epoch 5206: Training Loss: 0.09283004452784856 Validation Loss: 0.7325620651245117\n",
      "Epoch 5207: Training Loss: 0.09266971796751022 Validation Loss: 0.730337381362915\n",
      "Epoch 5208: Training Loss: 0.09279856085777283 Validation Loss: 0.7316311597824097\n",
      "Epoch 5209: Training Loss: 0.09272890041271846 Validation Loss: 0.7322824001312256\n",
      "Epoch 5210: Training Loss: 0.09268096337715785 Validation Loss: 0.7311347126960754\n",
      "Epoch 5211: Training Loss: 0.09302317599455516 Validation Loss: 0.7292207479476929\n",
      "Epoch 5212: Training Loss: 0.09264601270357768 Validation Loss: 0.7316223978996277\n",
      "Epoch 5213: Training Loss: 0.09276007364193599 Validation Loss: 0.7366438508033752\n",
      "Epoch 5214: Training Loss: 0.09263043850660324 Validation Loss: 0.7369720339775085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5215: Training Loss: 0.09256917238235474 Validation Loss: 0.7364373207092285\n",
      "Epoch 5216: Training Loss: 0.09254752844572067 Validation Loss: 0.7358404994010925\n",
      "Epoch 5217: Training Loss: 0.09255192925532658 Validation Loss: 0.7337734699249268\n",
      "Epoch 5218: Training Loss: 0.09258319934209187 Validation Loss: 0.7316910624504089\n",
      "Epoch 5219: Training Loss: 0.09255846093098323 Validation Loss: 0.7335515022277832\n",
      "Epoch 5220: Training Loss: 0.09252732247114182 Validation Loss: 0.734682023525238\n",
      "Epoch 5221: Training Loss: 0.09245401620864868 Validation Loss: 0.7325172424316406\n",
      "Epoch 5222: Training Loss: 0.09240888804197311 Validation Loss: 0.733058512210846\n",
      "Epoch 5223: Training Loss: 0.09241993228594463 Validation Loss: 0.732901930809021\n",
      "Epoch 5224: Training Loss: 0.09253615885972977 Validation Loss: 0.7353401184082031\n",
      "Epoch 5225: Training Loss: 0.09234718233346939 Validation Loss: 0.7343671917915344\n",
      "Epoch 5226: Training Loss: 0.09240996092557907 Validation Loss: 0.7323347330093384\n",
      "Epoch 5227: Training Loss: 0.092381052672863 Validation Loss: 0.730826199054718\n",
      "Epoch 5228: Training Loss: 0.09255678455034892 Validation Loss: 0.7300402522087097\n",
      "Epoch 5229: Training Loss: 0.09225988139708836 Validation Loss: 0.7323892116546631\n",
      "Epoch 5230: Training Loss: 0.09220446397860844 Validation Loss: 0.7343482971191406\n",
      "Epoch 5231: Training Loss: 0.09240791449944179 Validation Loss: 0.7355577349662781\n",
      "Epoch 5232: Training Loss: 0.09231892228126526 Validation Loss: 0.7375606894493103\n",
      "Epoch 5233: Training Loss: 0.09223592032988866 Validation Loss: 0.7364629507064819\n",
      "Epoch 5234: Training Loss: 0.09219605227311452 Validation Loss: 0.73321932554245\n",
      "Epoch 5235: Training Loss: 0.0923305054505666 Validation Loss: 0.7303634881973267\n",
      "Epoch 5236: Training Loss: 0.09224788099527359 Validation Loss: 0.7320417165756226\n",
      "Epoch 5237: Training Loss: 0.09213981529076894 Validation Loss: 0.7325627207756042\n",
      "Epoch 5238: Training Loss: 0.09207072108983994 Validation Loss: 0.7337580323219299\n",
      "Epoch 5239: Training Loss: 0.09203613052765529 Validation Loss: 0.7344561219215393\n",
      "Epoch 5240: Training Loss: 0.09217300762732823 Validation Loss: 0.7372400164604187\n",
      "Epoch 5241: Training Loss: 0.0921068216363589 Validation Loss: 0.7363797426223755\n",
      "Epoch 5242: Training Loss: 0.09202065815528233 Validation Loss: 0.7340344786643982\n",
      "Epoch 5243: Training Loss: 0.09200625866651535 Validation Loss: 0.7338888049125671\n",
      "Epoch 5244: Training Loss: 0.09195513278245926 Validation Loss: 0.7323813438415527\n",
      "Epoch 5245: Training Loss: 0.09192591905593872 Validation Loss: 0.7322586178779602\n",
      "Epoch 5246: Training Loss: 0.09211378792921703 Validation Loss: 0.730707049369812\n",
      "Epoch 5247: Training Loss: 0.0919990489880244 Validation Loss: 0.7331047058105469\n",
      "Epoch 5248: Training Loss: 0.09198645999034245 Validation Loss: 0.7325199246406555\n",
      "Epoch 5249: Training Loss: 0.09201516211032867 Validation Loss: 0.7366060614585876\n",
      "Epoch 5250: Training Loss: 0.0918721208969752 Validation Loss: 0.7370249629020691\n",
      "Epoch 5251: Training Loss: 0.0919144277771314 Validation Loss: 0.7367404103279114\n",
      "Epoch 5252: Training Loss: 0.0917872463663419 Validation Loss: 0.73575359582901\n",
      "Epoch 5253: Training Loss: 0.09187193711598714 Validation Loss: 0.7311848998069763\n",
      "Epoch 5254: Training Loss: 0.0918977086742719 Validation Loss: 0.7303476333618164\n",
      "Epoch 5255: Training Loss: 0.09201120088497798 Validation Loss: 0.7338303327560425\n",
      "Epoch 5256: Training Loss: 0.09168557574351628 Validation Loss: 0.7338634729385376\n",
      "Epoch 5257: Training Loss: 0.09166199713945389 Validation Loss: 0.734348714351654\n",
      "Epoch 5258: Training Loss: 0.09183249870936076 Validation Loss: 0.7343736290931702\n",
      "Epoch 5259: Training Loss: 0.09174979229768117 Validation Loss: 0.7354379892349243\n",
      "Epoch 5260: Training Loss: 0.09164141615231831 Validation Loss: 0.7343809008598328\n",
      "Epoch 5261: Training Loss: 0.09158985316753387 Validation Loss: 0.7320157885551453\n",
      "Epoch 5262: Training Loss: 0.09165036926666896 Validation Loss: 0.7305368781089783\n",
      "Epoch 5263: Training Loss: 0.0918896918495496 Validation Loss: 0.7283716797828674\n",
      "Epoch 5264: Training Loss: 0.09156594177087148 Validation Loss: 0.7315574288368225\n",
      "Epoch 5265: Training Loss: 0.09147870540618896 Validation Loss: 0.7342863082885742\n",
      "Epoch 5266: Training Loss: 0.0915373091896375 Validation Loss: 0.7376930713653564\n",
      "Epoch 5267: Training Loss: 0.09160229812065761 Validation Loss: 0.7395505905151367\n",
      "Epoch 5268: Training Loss: 0.09149322162071864 Validation Loss: 0.7371954917907715\n",
      "Epoch 5269: Training Loss: 0.09153666098912557 Validation Loss: 0.7357337474822998\n",
      "Epoch 5270: Training Loss: 0.09140024334192276 Validation Loss: 0.7329601049423218\n",
      "Epoch 5271: Training Loss: 0.09171086301406224 Validation Loss: 0.7286068797111511\n",
      "Epoch 5272: Training Loss: 0.09149130682150523 Validation Loss: 0.7300301194190979\n",
      "Epoch 5273: Training Loss: 0.09143800288438797 Validation Loss: 0.7314828038215637\n",
      "Epoch 5274: Training Loss: 0.09138785550991695 Validation Loss: 0.7358021140098572\n",
      "Epoch 5275: Training Loss: 0.0912880003452301 Validation Loss: 0.737267017364502\n",
      "Epoch 5276: Training Loss: 0.09150143961111705 Validation Loss: 0.7351241111755371\n",
      "Epoch 5277: Training Loss: 0.091289222240448 Validation Loss: 0.734445333480835\n",
      "Epoch 5278: Training Loss: 0.09154967466990153 Validation Loss: 0.7321047782897949\n",
      "Epoch 5279: Training Loss: 0.091403067111969 Validation Loss: 0.7316272854804993\n",
      "Epoch 5280: Training Loss: 0.0914312054713567 Validation Loss: 0.7368149757385254\n",
      "Epoch 5281: Training Loss: 0.09120279798905055 Validation Loss: 0.7376358509063721\n",
      "Epoch 5282: Training Loss: 0.09126760810613632 Validation Loss: 0.7378799915313721\n",
      "Epoch 5283: Training Loss: 0.09122367203235626 Validation Loss: 0.7350595593452454\n",
      "Epoch 5284: Training Loss: 0.0912003293633461 Validation Loss: 0.7343859076499939\n",
      "Epoch 5285: Training Loss: 0.09108273684978485 Validation Loss: 0.732100784778595\n",
      "Epoch 5286: Training Loss: 0.09113650023937225 Validation Loss: 0.7326589226722717\n",
      "Epoch 5287: Training Loss: 0.09129367023706436 Validation Loss: 0.7343247532844543\n",
      "Epoch 5288: Training Loss: 0.0910704309741656 Validation Loss: 0.7314692139625549\n",
      "Epoch 5289: Training Loss: 0.09111298869053523 Validation Loss: 0.7298246622085571\n",
      "Epoch 5290: Training Loss: 0.09129614631334941 Validation Loss: 0.7345630526542664\n",
      "Epoch 5291: Training Loss: 0.09098686029513676 Validation Loss: 0.7358378171920776\n",
      "Epoch 5292: Training Loss: 0.09105797857046127 Validation Loss: 0.7336256504058838\n",
      "Epoch 5293: Training Loss: 0.09104578197002411 Validation Loss: 0.7364262342453003\n",
      "Epoch 5294: Training Loss: 0.09088730315367381 Validation Loss: 0.7354410290718079\n",
      "Epoch 5295: Training Loss: 0.0908953920006752 Validation Loss: 0.7343060374259949\n",
      "Epoch 5296: Training Loss: 0.09086941927671432 Validation Loss: 0.7353606224060059\n",
      "Epoch 5297: Training Loss: 0.09086316327253978 Validation Loss: 0.7333332300186157\n",
      "Epoch 5298: Training Loss: 0.09085537989934285 Validation Loss: 0.7345560193061829\n",
      "Epoch 5299: Training Loss: 0.09093483537435532 Validation Loss: 0.7318459749221802\n",
      "Epoch 5300: Training Loss: 0.09080441544453303 Validation Loss: 0.7334579229354858\n",
      "Epoch 5301: Training Loss: 0.09090107679367065 Validation Loss: 0.732818603515625\n",
      "Epoch 5302: Training Loss: 0.0907616913318634 Validation Loss: 0.7352437973022461\n",
      "Epoch 5303: Training Loss: 0.09072908014059067 Validation Loss: 0.7356495261192322\n",
      "Epoch 5304: Training Loss: 0.09075450152158737 Validation Loss: 0.736034631729126\n",
      "Epoch 5305: Training Loss: 0.09107206265131633 Validation Loss: 0.7388702630996704\n",
      "Epoch 5306: Training Loss: 0.0907629703481992 Validation Loss: 0.7350839972496033\n",
      "Epoch 5307: Training Loss: 0.09064613531033199 Validation Loss: 0.733576774597168\n",
      "Epoch 5308: Training Loss: 0.09068645040194194 Validation Loss: 0.7339386343955994\n",
      "Epoch 5309: Training Loss: 0.09062834332386653 Validation Loss: 0.7319998741149902\n",
      "Epoch 5310: Training Loss: 0.09064756830533345 Validation Loss: 0.7312169671058655\n",
      "Epoch 5311: Training Loss: 0.0905641218026479 Validation Loss: 0.7324708104133606\n",
      "Epoch 5312: Training Loss: 0.09051824112733205 Validation Loss: 0.7336102724075317\n",
      "Epoch 5313: Training Loss: 0.09062563876310985 Validation Loss: 0.7365175485610962\n",
      "Epoch 5314: Training Loss: 0.09100299328565598 Validation Loss: 0.7406407594680786\n",
      "Epoch 5315: Training Loss: 0.0905437817176183 Validation Loss: 0.7376962304115295\n",
      "Epoch 5316: Training Loss: 0.0905935565630595 Validation Loss: 0.7323358058929443\n",
      "Epoch 5317: Training Loss: 0.09062994519869487 Validation Loss: 0.7296269536018372\n",
      "Epoch 5318: Training Loss: 0.09058904647827148 Validation Loss: 0.7291930317878723\n",
      "Epoch 5319: Training Loss: 0.09045450141032536 Validation Loss: 0.7324037551879883\n",
      "Epoch 5320: Training Loss: 0.09033732612927754 Validation Loss: 0.7353686690330505\n",
      "Epoch 5321: Training Loss: 0.0904061496257782 Validation Loss: 0.7365753054618835\n",
      "Epoch 5322: Training Loss: 0.09040544182062149 Validation Loss: 0.7389516830444336\n",
      "Epoch 5323: Training Loss: 0.09050547331571579 Validation Loss: 0.7358089685440063\n",
      "Epoch 5324: Training Loss: 0.09028041859467824 Validation Loss: 0.735191822052002\n",
      "Epoch 5325: Training Loss: 0.09027054905891418 Validation Loss: 0.7344163060188293\n",
      "Epoch 5326: Training Loss: 0.09027623136838277 Validation Loss: 0.7346113324165344\n",
      "Epoch 5327: Training Loss: 0.09035675475994746 Validation Loss: 0.7317584753036499\n",
      "Epoch 5328: Training Loss: 0.09025399386882782 Validation Loss: 0.7332905530929565\n",
      "Epoch 5329: Training Loss: 0.09023135900497437 Validation Loss: 0.7341877818107605\n",
      "Epoch 5330: Training Loss: 0.09030014276504517 Validation Loss: 0.7346723675727844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5331: Training Loss: 0.09021038313706715 Validation Loss: 0.7339322566986084\n",
      "Epoch 5332: Training Loss: 0.09012053658564885 Validation Loss: 0.7349231839179993\n",
      "Epoch 5333: Training Loss: 0.09010126441717148 Validation Loss: 0.7357866764068604\n",
      "Epoch 5334: Training Loss: 0.09042537957429886 Validation Loss: 0.7331638336181641\n",
      "Epoch 5335: Training Loss: 0.09010331084330876 Validation Loss: 0.7354727387428284\n",
      "Epoch 5336: Training Loss: 0.09040201952060063 Validation Loss: 0.7382474541664124\n",
      "Epoch 5337: Training Loss: 0.09008122483889262 Validation Loss: 0.7363128662109375\n",
      "Epoch 5338: Training Loss: 0.08999600261449814 Validation Loss: 0.7332130670547485\n",
      "Epoch 5339: Training Loss: 0.09001777817805608 Validation Loss: 0.7307154536247253\n",
      "Epoch 5340: Training Loss: 0.09001665314038594 Validation Loss: 0.7311655879020691\n",
      "Epoch 5341: Training Loss: 0.090180737276872 Validation Loss: 0.7302035093307495\n",
      "Epoch 5342: Training Loss: 0.09038202961285909 Validation Loss: 0.7366103529930115\n",
      "Epoch 5343: Training Loss: 0.0901092067360878 Validation Loss: 0.7397331595420837\n",
      "Epoch 5344: Training Loss: 0.09005040675401688 Validation Loss: 0.7399815320968628\n",
      "Epoch 5345: Training Loss: 0.09004504233598709 Validation Loss: 0.7349120378494263\n",
      "Epoch 5346: Training Loss: 0.08985014756520589 Validation Loss: 0.7330841422080994\n",
      "Epoch 5347: Training Loss: 0.0898863027493159 Validation Loss: 0.7342680096626282\n",
      "Epoch 5348: Training Loss: 0.08992233375708263 Validation Loss: 0.733056902885437\n",
      "Epoch 5349: Training Loss: 0.08982554078102112 Validation Loss: 0.7344722151756287\n",
      "Epoch 5350: Training Loss: 0.08986954639355342 Validation Loss: 0.7369849681854248\n",
      "Epoch 5351: Training Loss: 0.08975570648908615 Validation Loss: 0.7369763255119324\n",
      "Epoch 5352: Training Loss: 0.08986934771140416 Validation Loss: 0.7358348965644836\n",
      "Epoch 5353: Training Loss: 0.08970608065525691 Validation Loss: 0.7343952655792236\n",
      "Epoch 5354: Training Loss: 0.08971757690111797 Validation Loss: 0.7338908910751343\n",
      "Epoch 5355: Training Loss: 0.08995105822881062 Validation Loss: 0.7316665053367615\n",
      "Epoch 5356: Training Loss: 0.08971136311690013 Validation Loss: 0.7329607605934143\n",
      "Epoch 5357: Training Loss: 0.08992023517688115 Validation Loss: 0.7358049750328064\n",
      "Epoch 5358: Training Loss: 0.0897804821530978 Validation Loss: 0.7340960502624512\n",
      "Epoch 5359: Training Loss: 0.08957682798306148 Validation Loss: 0.7350367903709412\n",
      "Epoch 5360: Training Loss: 0.08967054386933644 Validation Loss: 0.7374459505081177\n",
      "Epoch 5361: Training Loss: 0.08966747671365738 Validation Loss: 0.7351033687591553\n",
      "Epoch 5362: Training Loss: 0.0896395891904831 Validation Loss: 0.7346700429916382\n",
      "Epoch 5363: Training Loss: 0.08949058502912521 Validation Loss: 0.7344710230827332\n",
      "Epoch 5364: Training Loss: 0.0896244967977206 Validation Loss: 0.7327972054481506\n",
      "Epoch 5365: Training Loss: 0.0897274340192477 Validation Loss: 0.7357563376426697\n",
      "Epoch 5366: Training Loss: 0.0895051434636116 Validation Loss: 0.7360254526138306\n",
      "Epoch 5367: Training Loss: 0.08951166520516078 Validation Loss: 0.7365691661834717\n",
      "Epoch 5368: Training Loss: 0.08970282723506291 Validation Loss: 0.7326341867446899\n",
      "Epoch 5369: Training Loss: 0.08949604382117589 Validation Loss: 0.7336892485618591\n",
      "Epoch 5370: Training Loss: 0.089396633207798 Validation Loss: 0.7337505221366882\n",
      "Epoch 5371: Training Loss: 0.08938200026750565 Validation Loss: 0.7345984578132629\n",
      "Epoch 5372: Training Loss: 0.08932353307803471 Validation Loss: 0.7362847328186035\n",
      "Epoch 5373: Training Loss: 0.08943679928779602 Validation Loss: 0.7378324270248413\n",
      "Epoch 5374: Training Loss: 0.08932159841060638 Validation Loss: 0.7359841465950012\n",
      "Epoch 5375: Training Loss: 0.08927418291568756 Validation Loss: 0.7347936034202576\n",
      "Epoch 5376: Training Loss: 0.08947295447190602 Validation Loss: 0.7356439232826233\n",
      "Epoch 5377: Training Loss: 0.08951550722122192 Validation Loss: 0.7377573251724243\n",
      "Epoch 5378: Training Loss: 0.08918057133754094 Validation Loss: 0.734552800655365\n",
      "Epoch 5379: Training Loss: 0.08921959499518077 Validation Loss: 0.7324814200401306\n",
      "Epoch 5380: Training Loss: 0.08924578378597896 Validation Loss: 0.7323578000068665\n",
      "Epoch 5381: Training Loss: 0.08918419728676479 Validation Loss: 0.7324159145355225\n",
      "Epoch 5382: Training Loss: 0.08917578061421712 Validation Loss: 0.734700083732605\n",
      "Epoch 5383: Training Loss: 0.0891778568426768 Validation Loss: 0.7343773245811462\n",
      "Epoch 5384: Training Loss: 0.08916885902484258 Validation Loss: 0.7374756336212158\n",
      "Epoch 5385: Training Loss: 0.08911389609177907 Validation Loss: 0.7383565902709961\n",
      "Epoch 5386: Training Loss: 0.08907843629519145 Validation Loss: 0.7362638711929321\n",
      "Epoch 5387: Training Loss: 0.08908875286579132 Validation Loss: 0.7344954013824463\n",
      "Epoch 5388: Training Loss: 0.08899738887945811 Validation Loss: 0.733771026134491\n",
      "Epoch 5389: Training Loss: 0.0892484684785207 Validation Loss: 0.735877275466919\n",
      "Epoch 5390: Training Loss: 0.08900414903958638 Validation Loss: 0.7359705567359924\n",
      "Epoch 5391: Training Loss: 0.08904483914375305 Validation Loss: 0.7349344491958618\n",
      "Epoch 5392: Training Loss: 0.08895186583201091 Validation Loss: 0.7356824278831482\n",
      "Epoch 5393: Training Loss: 0.08903623123963673 Validation Loss: 0.7335942983627319\n",
      "Epoch 5394: Training Loss: 0.08912792553504308 Validation Loss: 0.7359116673469543\n",
      "Epoch 5395: Training Loss: 0.08906726787487666 Validation Loss: 0.7375147938728333\n",
      "Epoch 5396: Training Loss: 0.08904960254828136 Validation Loss: 0.7333297729492188\n",
      "Epoch 5397: Training Loss: 0.08888864268859227 Validation Loss: 0.7321634888648987\n",
      "Epoch 5398: Training Loss: 0.08892358591159184 Validation Loss: 0.7337837219238281\n",
      "Epoch 5399: Training Loss: 0.08882778882980347 Validation Loss: 0.73598313331604\n",
      "Epoch 5400: Training Loss: 0.08884601294994354 Validation Loss: 0.735103189945221\n",
      "Epoch 5401: Training Loss: 0.08883785208066304 Validation Loss: 0.7374224662780762\n",
      "Epoch 5402: Training Loss: 0.08878465990225475 Validation Loss: 0.7366541028022766\n",
      "Epoch 5403: Training Loss: 0.08874500542879105 Validation Loss: 0.737375795841217\n",
      "Epoch 5404: Training Loss: 0.0887435997525851 Validation Loss: 0.7371408343315125\n",
      "Epoch 5405: Training Loss: 0.08870299408833186 Validation Loss: 0.7350584268569946\n",
      "Epoch 5406: Training Loss: 0.0886970857779185 Validation Loss: 0.7346675992012024\n",
      "Epoch 5407: Training Loss: 0.08875580877065659 Validation Loss: 0.7341520190238953\n",
      "Epoch 5408: Training Loss: 0.08868786195913951 Validation Loss: 0.7340456247329712\n",
      "Epoch 5409: Training Loss: 0.0886347343524297 Validation Loss: 0.7347730398178101\n",
      "Epoch 5410: Training Loss: 0.0887221892674764 Validation Loss: 0.7326932549476624\n",
      "Epoch 5411: Training Loss: 0.0890930915872256 Validation Loss: 0.7302432656288147\n",
      "Epoch 5412: Training Loss: 0.08858410269021988 Validation Loss: 0.7342130541801453\n",
      "Epoch 5413: Training Loss: 0.08852733423312505 Validation Loss: 0.7376125454902649\n",
      "Epoch 5414: Training Loss: 0.08850920697053273 Validation Loss: 0.739562451839447\n",
      "Epoch 5415: Training Loss: 0.08859801292419434 Validation Loss: 0.740544855594635\n",
      "Epoch 5416: Training Loss: 0.08862009147802989 Validation Loss: 0.7379820346832275\n",
      "Epoch 5417: Training Loss: 0.08849224696556728 Validation Loss: 0.7349168658256531\n",
      "Epoch 5418: Training Loss: 0.08845737079779308 Validation Loss: 0.7345499992370605\n",
      "Epoch 5419: Training Loss: 0.08853837847709656 Validation Loss: 0.7356676459312439\n",
      "Epoch 5420: Training Loss: 0.08848411589860916 Validation Loss: 0.7333579659461975\n",
      "Epoch 5421: Training Loss: 0.08861829837163289 Validation Loss: 0.7346687912940979\n",
      "Epoch 5422: Training Loss: 0.0884011040131251 Validation Loss: 0.7348072528839111\n",
      "Epoch 5423: Training Loss: 0.08848925928274791 Validation Loss: 0.7352024912834167\n",
      "Epoch 5424: Training Loss: 0.08831129471460979 Validation Loss: 0.7339992523193359\n",
      "Epoch 5425: Training Loss: 0.08830593278010686 Validation Loss: 0.7339641451835632\n",
      "Epoch 5426: Training Loss: 0.08846886952718098 Validation Loss: 0.7326157093048096\n",
      "Epoch 5427: Training Loss: 0.08827793349822362 Validation Loss: 0.7356827855110168\n",
      "Epoch 5428: Training Loss: 0.08822366346915562 Validation Loss: 0.7371773719787598\n",
      "Epoch 5429: Training Loss: 0.0883129636446635 Validation Loss: 0.7392431497573853\n",
      "Epoch 5430: Training Loss: 0.08834478755791982 Validation Loss: 0.7401624917984009\n",
      "Epoch 5431: Training Loss: 0.08823834856351216 Validation Loss: 0.7382922172546387\n",
      "Epoch 5432: Training Loss: 0.08821093291044235 Validation Loss: 0.735374391078949\n",
      "Epoch 5433: Training Loss: 0.08826174090305965 Validation Loss: 0.7330024242401123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5434: Training Loss: 0.08817122628291447 Validation Loss: 0.733352780342102\n",
      "Epoch 5435: Training Loss: 0.08820972839991252 Validation Loss: 0.7361958622932434\n",
      "Epoch 5436: Training Loss: 0.08832720667123795 Validation Loss: 0.7386835217475891\n",
      "Epoch 5437: Training Loss: 0.08814725776513417 Validation Loss: 0.7381208539009094\n",
      "Epoch 5438: Training Loss: 0.08821565409501393 Validation Loss: 0.7342928051948547\n",
      "Epoch 5439: Training Loss: 0.08820389707883199 Validation Loss: 0.7320757508277893\n",
      "Epoch 5440: Training Loss: 0.08810988068580627 Validation Loss: 0.7333765625953674\n",
      "Epoch 5441: Training Loss: 0.08817414442698161 Validation Loss: 0.7371250987052917\n",
      "Epoch 5442: Training Loss: 0.08802750209967296 Validation Loss: 0.7374987602233887\n",
      "Epoch 5443: Training Loss: 0.08800823489824931 Validation Loss: 0.7355732917785645\n",
      "Epoch 5444: Training Loss: 0.08798824002345403 Validation Loss: 0.7355934977531433\n",
      "Epoch 5445: Training Loss: 0.0880550667643547 Validation Loss: 0.7371217608451843\n",
      "Epoch 5446: Training Loss: 0.08802368740240733 Validation Loss: 0.7375108599662781\n",
      "Epoch 5447: Training Loss: 0.08795735985040665 Validation Loss: 0.7371653318405151\n",
      "Epoch 5448: Training Loss: 0.08784205466508865 Validation Loss: 0.734717607498169\n",
      "Epoch 5449: Training Loss: 0.08827914297580719 Validation Loss: 0.7303673028945923\n",
      "Epoch 5450: Training Loss: 0.08800435562928517 Validation Loss: 0.7307925820350647\n",
      "Epoch 5451: Training Loss: 0.08798220256964366 Validation Loss: 0.7356426119804382\n",
      "Epoch 5452: Training Loss: 0.08776819705963135 Validation Loss: 0.7373546957969666\n",
      "Epoch 5453: Training Loss: 0.0880923221508662 Validation Loss: 0.7410511374473572\n",
      "Epoch 5454: Training Loss: 0.08792756746212642 Validation Loss: 0.7403826713562012\n",
      "Epoch 5455: Training Loss: 0.08788647999366124 Validation Loss: 0.7358317971229553\n",
      "Epoch 5456: Training Loss: 0.0877122034629186 Validation Loss: 0.7338431477546692\n",
      "Epoch 5457: Training Loss: 0.0879603698849678 Validation Loss: 0.7304890751838684\n",
      "Epoch 5458: Training Loss: 0.0878089889883995 Validation Loss: 0.733600378036499\n",
      "Epoch 5459: Training Loss: 0.08768298228581746 Validation Loss: 0.7362412810325623\n",
      "Epoch 5460: Training Loss: 0.08787810305754344 Validation Loss: 0.7353891134262085\n",
      "Epoch 5461: Training Loss: 0.0876963809132576 Validation Loss: 0.7385307550430298\n",
      "Epoch 5462: Training Loss: 0.08765842765569687 Validation Loss: 0.740253210067749\n",
      "Epoch 5463: Training Loss: 0.08782432973384857 Validation Loss: 0.7374005913734436\n",
      "Epoch 5464: Training Loss: 0.08769424259662628 Validation Loss: 0.7385992407798767\n",
      "Epoch 5465: Training Loss: 0.08756168186664581 Validation Loss: 0.7375364899635315\n",
      "Epoch 5466: Training Loss: 0.08755282312631607 Validation Loss: 0.7366098165512085\n",
      "Epoch 5467: Training Loss: 0.08750445644060771 Validation Loss: 0.7360286712646484\n",
      "Epoch 5468: Training Loss: 0.08752852926651637 Validation Loss: 0.7362601161003113\n",
      "Epoch 5469: Training Loss: 0.0876193717122078 Validation Loss: 0.7365456819534302\n",
      "Epoch 5470: Training Loss: 0.08746140201886494 Validation Loss: 0.7348248958587646\n",
      "Epoch 5471: Training Loss: 0.08745841681957245 Validation Loss: 0.7339620590209961\n",
      "Epoch 5472: Training Loss: 0.08745221048593521 Validation Loss: 0.7328177094459534\n",
      "Epoch 5473: Training Loss: 0.08754898856083553 Validation Loss: 0.7353355288505554\n",
      "Epoch 5474: Training Loss: 0.08754439900318782 Validation Loss: 0.7373036742210388\n",
      "Epoch 5475: Training Loss: 0.0877068464954694 Validation Loss: 0.7334849834442139\n",
      "Epoch 5476: Training Loss: 0.08737568805615108 Validation Loss: 0.7343805432319641\n",
      "Epoch 5477: Training Loss: 0.08733063687880833 Validation Loss: 0.7358112931251526\n",
      "Epoch 5478: Training Loss: 0.08742266893386841 Validation Loss: 0.7345094680786133\n",
      "Epoch 5479: Training Loss: 0.08754974852005641 Validation Loss: 0.7332581281661987\n",
      "Epoch 5480: Training Loss: 0.08737812315424283 Validation Loss: 0.7373430132865906\n",
      "Epoch 5481: Training Loss: 0.08726867039998372 Validation Loss: 0.7384431958198547\n",
      "Epoch 5482: Training Loss: 0.08736831198136012 Validation Loss: 0.7404806017875671\n",
      "Epoch 5483: Training Loss: 0.08755867679913838 Validation Loss: 0.7360573410987854\n",
      "Epoch 5484: Training Loss: 0.08724695195754369 Validation Loss: 0.737244725227356\n",
      "Epoch 5485: Training Loss: 0.08718612045049667 Validation Loss: 0.7362099885940552\n",
      "Epoch 5486: Training Loss: 0.08725521465142567 Validation Loss: 0.73625648021698\n",
      "Epoch 5487: Training Loss: 0.08728449294964473 Validation Loss: 0.7347851395606995\n",
      "Epoch 5488: Training Loss: 0.08718037853638332 Validation Loss: 0.736153781414032\n",
      "Epoch 5489: Training Loss: 0.0871557245651881 Validation Loss: 0.7369094491004944\n",
      "Epoch 5490: Training Loss: 0.08717728406190872 Validation Loss: 0.7377745509147644\n",
      "Epoch 5491: Training Loss: 0.08729968716700871 Validation Loss: 0.7350441217422485\n",
      "Epoch 5492: Training Loss: 0.08732468138138454 Validation Loss: 0.7373898029327393\n",
      "Epoch 5493: Training Loss: 0.08708509306112926 Validation Loss: 0.7375567555427551\n",
      "Epoch 5494: Training Loss: 0.08755305409431458 Validation Loss: 0.7325977683067322\n",
      "Epoch 5495: Training Loss: 0.0871056616306305 Validation Loss: 0.7351605892181396\n",
      "Epoch 5496: Training Loss: 0.08726440618435542 Validation Loss: 0.7390536665916443\n",
      "Epoch 5497: Training Loss: 0.08708058794339497 Validation Loss: 0.7399097681045532\n",
      "Epoch 5498: Training Loss: 0.08698135862747829 Validation Loss: 0.7382054924964905\n",
      "Epoch 5499: Training Loss: 0.08759992569684982 Validation Loss: 0.7321150302886963\n",
      "Epoch 5500: Training Loss: 0.08703885227441788 Validation Loss: 0.7331299781799316\n",
      "Epoch 5501: Training Loss: 0.08700268467267354 Validation Loss: 0.7362990379333496\n",
      "Epoch 5502: Training Loss: 0.08689419428507487 Validation Loss: 0.7384538054466248\n",
      "Epoch 5503: Training Loss: 0.08702706297238667 Validation Loss: 0.740240752696991\n",
      "Epoch 5504: Training Loss: 0.08695955326159795 Validation Loss: 0.7401120066642761\n",
      "Epoch 5505: Training Loss: 0.08692860106627147 Validation Loss: 0.737040638923645\n",
      "Epoch 5506: Training Loss: 0.08679976811011632 Validation Loss: 0.7340519428253174\n",
      "Epoch 5507: Training Loss: 0.08686290433009465 Validation Loss: 0.7350911498069763\n",
      "Epoch 5508: Training Loss: 0.08680398762226105 Validation Loss: 0.7336953282356262\n",
      "Epoch 5509: Training Loss: 0.08679865549008052 Validation Loss: 0.7349228858947754\n",
      "Epoch 5510: Training Loss: 0.08674691865841548 Validation Loss: 0.7370692491531372\n",
      "Epoch 5511: Training Loss: 0.08674893031517665 Validation Loss: 0.7370400428771973\n",
      "Epoch 5512: Training Loss: 0.08675171434879303 Validation Loss: 0.7361481189727783\n",
      "Epoch 5513: Training Loss: 0.08715444058179855 Validation Loss: 0.7401077747344971\n",
      "Epoch 5514: Training Loss: 0.0868440642952919 Validation Loss: 0.7413474917411804\n",
      "Epoch 5515: Training Loss: 0.0866339976588885 Validation Loss: 0.7377277612686157\n",
      "Epoch 5516: Training Loss: 0.08689945191144943 Validation Loss: 0.732435405254364\n",
      "Epoch 5517: Training Loss: 0.08670895298322041 Validation Loss: 0.7337044477462769\n",
      "Epoch 5518: Training Loss: 0.0866711437702179 Validation Loss: 0.7348164319992065\n",
      "Epoch 5519: Training Loss: 0.08672148485978444 Validation Loss: 0.7360587120056152\n",
      "Epoch 5520: Training Loss: 0.08653062582015991 Validation Loss: 0.7370253801345825\n",
      "Epoch 5521: Training Loss: 0.08659365276495616 Validation Loss: 0.7366316318511963\n",
      "Epoch 5522: Training Loss: 0.08670215557018916 Validation Loss: 0.7392244338989258\n",
      "Epoch 5523: Training Loss: 0.08658478409051895 Validation Loss: 0.736239492893219\n",
      "Epoch 5524: Training Loss: 0.08651739607254665 Validation Loss: 0.7363843321800232\n",
      "Epoch 5525: Training Loss: 0.08647220581769943 Validation Loss: 0.7355517148971558\n",
      "Epoch 5526: Training Loss: 0.08653566241264343 Validation Loss: 0.7337074279785156\n",
      "Epoch 5527: Training Loss: 0.08657145748535792 Validation Loss: 0.7348281741142273\n",
      "Epoch 5528: Training Loss: 0.08666225771109264 Validation Loss: 0.7333717346191406\n",
      "Epoch 5529: Training Loss: 0.08677302797635396 Validation Loss: 0.7380980849266052\n",
      "Epoch 5530: Training Loss: 0.0865543857216835 Validation Loss: 0.7411430478096008\n",
      "Epoch 5531: Training Loss: 0.08650483936071396 Validation Loss: 0.7400732636451721\n",
      "Epoch 5532: Training Loss: 0.0864082599679629 Validation Loss: 0.7377402186393738\n",
      "Epoch 5533: Training Loss: 0.0863714466492335 Validation Loss: 0.7345858812332153\n",
      "Epoch 5534: Training Loss: 0.08634340266386668 Validation Loss: 0.7352506518363953\n",
      "Epoch 5535: Training Loss: 0.08631972968578339 Validation Loss: 0.7359714508056641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5536: Training Loss: 0.08632103105386098 Validation Loss: 0.7382981777191162\n",
      "Epoch 5537: Training Loss: 0.0862574428319931 Validation Loss: 0.7379049062728882\n",
      "Epoch 5538: Training Loss: 0.08635476231575012 Validation Loss: 0.7399903535842896\n",
      "Epoch 5539: Training Loss: 0.0862312838435173 Validation Loss: 0.7386361956596375\n",
      "Epoch 5540: Training Loss: 0.08630657941102982 Validation Loss: 0.7353223562240601\n",
      "Epoch 5541: Training Loss: 0.08648025244474411 Validation Loss: 0.737969696521759\n",
      "Epoch 5542: Training Loss: 0.08634304751952489 Validation Loss: 0.7344242334365845\n",
      "Epoch 5543: Training Loss: 0.08617471158504486 Validation Loss: 0.7339016795158386\n",
      "Epoch 5544: Training Loss: 0.08632406840721767 Validation Loss: 0.7370740175247192\n",
      "Epoch 5545: Training Loss: 0.08615664144357045 Validation Loss: 0.7361273169517517\n",
      "Epoch 5546: Training Loss: 0.08612265934546788 Validation Loss: 0.7371541261672974\n",
      "Epoch 5547: Training Loss: 0.08626544723908107 Validation Loss: 0.734374463558197\n",
      "Epoch 5548: Training Loss: 0.08610421667496364 Validation Loss: 0.7364940047264099\n",
      "Epoch 5549: Training Loss: 0.0862791861097018 Validation Loss: 0.7348544597625732\n",
      "Epoch 5550: Training Loss: 0.08619842678308487 Validation Loss: 0.7399985194206238\n",
      "Epoch 5551: Training Loss: 0.08601848905285199 Validation Loss: 0.7408714294433594\n",
      "Epoch 5552: Training Loss: 0.08603103955586751 Validation Loss: 0.7387256026268005\n",
      "Epoch 5553: Training Loss: 0.08598411083221436 Validation Loss: 0.7378888726234436\n",
      "Epoch 5554: Training Loss: 0.08593269934256871 Validation Loss: 0.7366814017295837\n",
      "Epoch 5555: Training Loss: 0.08591990669568379 Validation Loss: 0.7366387248039246\n",
      "Epoch 5556: Training Loss: 0.08592141419649124 Validation Loss: 0.735684871673584\n",
      "Epoch 5557: Training Loss: 0.08597674469153087 Validation Loss: 0.7348479628562927\n",
      "Epoch 5558: Training Loss: 0.0859020600716273 Validation Loss: 0.7370976805686951\n",
      "Epoch 5559: Training Loss: 0.08586032936970393 Validation Loss: 0.7388640642166138\n",
      "Epoch 5560: Training Loss: 0.08587144315242767 Validation Loss: 0.7376614212989807\n",
      "Epoch 5561: Training Loss: 0.085820771753788 Validation Loss: 0.7381131052970886\n",
      "Epoch 5562: Training Loss: 0.0858878344297409 Validation Loss: 0.7357012629508972\n",
      "Epoch 5563: Training Loss: 0.08584344138701756 Validation Loss: 0.7367303371429443\n",
      "Epoch 5564: Training Loss: 0.08576985448598862 Validation Loss: 0.7375631928443909\n",
      "Epoch 5565: Training Loss: 0.08574868490298589 Validation Loss: 0.7378337383270264\n",
      "Epoch 5566: Training Loss: 0.08570362130800883 Validation Loss: 0.7371473908424377\n",
      "Epoch 5567: Training Loss: 0.08575363208850224 Validation Loss: 0.7383615374565125\n",
      "Epoch 5568: Training Loss: 0.08572712292273839 Validation Loss: 0.7370159029960632\n",
      "Epoch 5569: Training Loss: 0.08585448066393535 Validation Loss: 0.735105574131012\n",
      "Epoch 5570: Training Loss: 0.08571916818618774 Validation Loss: 0.7365812659263611\n",
      "Epoch 5571: Training Loss: 0.0856711541612943 Validation Loss: 0.7372077107429504\n",
      "Epoch 5572: Training Loss: 0.08572181314229965 Validation Loss: 0.7399377822875977\n",
      "Epoch 5573: Training Loss: 0.08561221758524577 Validation Loss: 0.7386339902877808\n",
      "Epoch 5574: Training Loss: 0.08555134510000546 Validation Loss: 0.7374648451805115\n",
      "Epoch 5575: Training Loss: 0.08566569536924362 Validation Loss: 0.7379503846168518\n",
      "Epoch 5576: Training Loss: 0.08552227169275284 Validation Loss: 0.7362136840820312\n",
      "Epoch 5577: Training Loss: 0.08552284042040507 Validation Loss: 0.7352400422096252\n",
      "Epoch 5578: Training Loss: 0.08553039034207661 Validation Loss: 0.7347821593284607\n",
      "Epoch 5579: Training Loss: 0.08557143062353134 Validation Loss: 0.7341359257698059\n",
      "Epoch 5580: Training Loss: 0.08551687250534694 Validation Loss: 0.735575258731842\n",
      "Epoch 5581: Training Loss: 0.08566454549630483 Validation Loss: 0.7402825951576233\n",
      "Epoch 5582: Training Loss: 0.08546738078196843 Validation Loss: 0.7398229837417603\n",
      "Epoch 5583: Training Loss: 0.08544699599345525 Validation Loss: 0.7391059398651123\n",
      "Epoch 5584: Training Loss: 0.08545373131831487 Validation Loss: 0.7391240000724792\n",
      "Epoch 5585: Training Loss: 0.08548411975304286 Validation Loss: 0.7396615743637085\n",
      "Epoch 5586: Training Loss: 0.08538458247979482 Validation Loss: 0.7370890974998474\n",
      "Epoch 5587: Training Loss: 0.08534385512272517 Validation Loss: 0.7359864115715027\n",
      "Epoch 5588: Training Loss: 0.08534921705722809 Validation Loss: 0.7352781295776367\n",
      "Epoch 5589: Training Loss: 0.0853516012430191 Validation Loss: 0.7356082201004028\n",
      "Epoch 5590: Training Loss: 0.08532428244749705 Validation Loss: 0.7368481755256653\n",
      "Epoch 5591: Training Loss: 0.0855476086338361 Validation Loss: 0.7401953339576721\n",
      "Epoch 5592: Training Loss: 0.08547992259263992 Validation Loss: 0.7363929748535156\n",
      "Epoch 5593: Training Loss: 0.0852210596203804 Validation Loss: 0.7363903522491455\n",
      "Epoch 5594: Training Loss: 0.0852167730530103 Validation Loss: 0.736375093460083\n",
      "Epoch 5595: Training Loss: 0.0852703998486201 Validation Loss: 0.7380505204200745\n",
      "Epoch 5596: Training Loss: 0.08530149857203166 Validation Loss: 0.7395549416542053\n",
      "Epoch 5597: Training Loss: 0.08521102865537007 Validation Loss: 0.7375832796096802\n",
      "Epoch 5598: Training Loss: 0.08522199094295502 Validation Loss: 0.7359133958816528\n",
      "Epoch 5599: Training Loss: 0.08530054241418839 Validation Loss: 0.7347874045372009\n",
      "Epoch 5600: Training Loss: 0.08513091504573822 Validation Loss: 0.7355260252952576\n",
      "Epoch 5601: Training Loss: 0.08521277209122975 Validation Loss: 0.7364599108695984\n",
      "Epoch 5602: Training Loss: 0.08523927628993988 Validation Loss: 0.7411395311355591\n",
      "Epoch 5603: Training Loss: 0.08511769274870555 Validation Loss: 0.7413948178291321\n",
      "Epoch 5604: Training Loss: 0.08511549482742946 Validation Loss: 0.7398536801338196\n",
      "Epoch 5605: Training Loss: 0.0850978617866834 Validation Loss: 0.7372361421585083\n",
      "Epoch 5606: Training Loss: 0.08511540045340855 Validation Loss: 0.7369912266731262\n",
      "Epoch 5607: Training Loss: 0.08513667682806651 Validation Loss: 0.7386216521263123\n",
      "Epoch 5608: Training Loss: 0.08523392180601756 Validation Loss: 0.735072910785675\n",
      "Epoch 5609: Training Loss: 0.08511243263880412 Validation Loss: 0.736967146396637\n",
      "Epoch 5610: Training Loss: 0.08497545123100281 Validation Loss: 0.7366930246353149\n",
      "Epoch 5611: Training Loss: 0.08504599084456761 Validation Loss: 0.7377181053161621\n",
      "Epoch 5612: Training Loss: 0.08498067408800125 Validation Loss: 0.7391010522842407\n",
      "Epoch 5613: Training Loss: 0.08493248869975407 Validation Loss: 0.7392979860305786\n",
      "Epoch 5614: Training Loss: 0.08485175172487895 Validation Loss: 0.7374063730239868\n",
      "Epoch 5615: Training Loss: 0.08491567770640056 Validation Loss: 0.7355056405067444\n",
      "Epoch 5616: Training Loss: 0.0848860926926136 Validation Loss: 0.7363181710243225\n",
      "Epoch 5617: Training Loss: 0.08489483098189037 Validation Loss: 0.7353085279464722\n",
      "Epoch 5618: Training Loss: 0.08508697897195816 Validation Loss: 0.7347391247749329\n",
      "Epoch 5619: Training Loss: 0.08482787509759267 Validation Loss: 0.7360270023345947\n",
      "Epoch 5620: Training Loss: 0.0847409615914027 Validation Loss: 0.7388501167297363\n",
      "Epoch 5621: Training Loss: 0.0848798577984174 Validation Loss: 0.7430477142333984\n",
      "Epoch 5622: Training Loss: 0.08494078616301219 Validation Loss: 0.7417839169502258\n",
      "Epoch 5623: Training Loss: 0.08483296384414037 Validation Loss: 0.7402636408805847\n",
      "Epoch 5624: Training Loss: 0.0847335656483968 Validation Loss: 0.7387645840644836\n",
      "Epoch 5625: Training Loss: 0.08500657727320989 Validation Loss: 0.7401356101036072\n",
      "Epoch 5626: Training Loss: 0.08462763826052348 Validation Loss: 0.7383304834365845\n",
      "Epoch 5627: Training Loss: 0.0846911296248436 Validation Loss: 0.7342965006828308\n",
      "Epoch 5628: Training Loss: 0.08468679587046306 Validation Loss: 0.7340763211250305\n",
      "Epoch 5629: Training Loss: 0.08464346826076508 Validation Loss: 0.7353312969207764\n",
      "Epoch 5630: Training Loss: 0.08458036929368973 Validation Loss: 0.7361878156661987\n",
      "Epoch 5631: Training Loss: 0.08464370916287105 Validation Loss: 0.7362400889396667\n",
      "Epoch 5632: Training Loss: 0.08454182992378871 Validation Loss: 0.7373567223548889\n",
      "Epoch 5633: Training Loss: 0.08455051481723785 Validation Loss: 0.7382028102874756\n",
      "Epoch 5634: Training Loss: 0.08463174353043239 Validation Loss: 0.738523542881012\n",
      "Epoch 5635: Training Loss: 0.08467006186644237 Validation Loss: 0.7395639419555664\n",
      "Epoch 5636: Training Loss: 0.08458466827869415 Validation Loss: 0.7370994091033936\n",
      "Epoch 5637: Training Loss: 0.08447972685098648 Validation Loss: 0.7366141676902771\n",
      "Epoch 5638: Training Loss: 0.08455366144577663 Validation Loss: 0.738236665725708\n",
      "Epoch 5639: Training Loss: 0.08451315512259801 Validation Loss: 0.7390255928039551\n",
      "Epoch 5640: Training Loss: 0.08447776238123576 Validation Loss: 0.7386078834533691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5641: Training Loss: 0.08440688500801723 Validation Loss: 0.7386952638626099\n",
      "Epoch 5642: Training Loss: 0.08441295971473058 Validation Loss: 0.7385894060134888\n",
      "Epoch 5643: Training Loss: 0.08451027671496074 Validation Loss: 0.7403635382652283\n",
      "Epoch 5644: Training Loss: 0.08437776938080788 Validation Loss: 0.7386886477470398\n",
      "Epoch 5645: Training Loss: 0.08434482167164485 Validation Loss: 0.73691326379776\n",
      "Epoch 5646: Training Loss: 0.08432142188151677 Validation Loss: 0.7371135354042053\n",
      "Epoch 5647: Training Loss: 0.08433099836111069 Validation Loss: 0.7373020648956299\n",
      "Epoch 5648: Training Loss: 0.0842984989285469 Validation Loss: 0.7368936538696289\n",
      "Epoch 5649: Training Loss: 0.0843006322781245 Validation Loss: 0.7364997863769531\n",
      "Epoch 5650: Training Loss: 0.08436717092990875 Validation Loss: 0.7358250617980957\n",
      "Epoch 5651: Training Loss: 0.08427001784245174 Validation Loss: 0.7372421026229858\n",
      "Epoch 5652: Training Loss: 0.08448398113250732 Validation Loss: 0.7358465194702148\n",
      "Epoch 5653: Training Loss: 0.08450577159722646 Validation Loss: 0.7419278621673584\n",
      "Epoch 5654: Training Loss: 0.08424616605043411 Validation Loss: 0.7409390211105347\n",
      "Epoch 5655: Training Loss: 0.08427763481934865 Validation Loss: 0.7387911677360535\n",
      "Epoch 5656: Training Loss: 0.08414747814337413 Validation Loss: 0.7388818264007568\n",
      "Epoch 5657: Training Loss: 0.08417364954948425 Validation Loss: 0.7391262650489807\n",
      "Epoch 5658: Training Loss: 0.0841424564520518 Validation Loss: 0.7392592430114746\n",
      "Epoch 5659: Training Loss: 0.08415139218171437 Validation Loss: 0.7386357188224792\n",
      "Epoch 5660: Training Loss: 0.08412295828262965 Validation Loss: 0.7370542883872986\n",
      "Epoch 5661: Training Loss: 0.08411705245574315 Validation Loss: 0.7383993268013\n",
      "Epoch 5662: Training Loss: 0.08406207710504532 Validation Loss: 0.7386255860328674\n",
      "Epoch 5663: Training Loss: 0.08404046545426051 Validation Loss: 0.7389549016952515\n",
      "Epoch 5664: Training Loss: 0.08408695956071217 Validation Loss: 0.7371212840080261\n",
      "Epoch 5665: Training Loss: 0.08416002492109935 Validation Loss: 0.7383769750595093\n",
      "Epoch 5666: Training Loss: 0.08400237808624904 Validation Loss: 0.7379850745201111\n",
      "Epoch 5667: Training Loss: 0.08447980880737305 Validation Loss: 0.7338641285896301\n",
      "Epoch 5668: Training Loss: 0.08410254369179408 Validation Loss: 0.7368338704109192\n",
      "Epoch 5669: Training Loss: 0.08413695295651753 Validation Loss: 0.7364641427993774\n",
      "Epoch 5670: Training Loss: 0.08446614195903142 Validation Loss: 0.742083728313446\n",
      "Epoch 5671: Training Loss: 0.08402696251869202 Validation Loss: 0.7398567199707031\n",
      "Epoch 5672: Training Loss: 0.08398834615945816 Validation Loss: 0.7377133965492249\n",
      "Epoch 5673: Training Loss: 0.08403447767098744 Validation Loss: 0.7385480403900146\n",
      "Epoch 5674: Training Loss: 0.08392995595932007 Validation Loss: 0.7372978329658508\n",
      "Epoch 5675: Training Loss: 0.08387827376524608 Validation Loss: 0.7370713353157043\n",
      "Epoch 5676: Training Loss: 0.0838749036192894 Validation Loss: 0.7399606704711914\n",
      "Epoch 5677: Training Loss: 0.0838307390610377 Validation Loss: 0.7396586537361145\n",
      "Epoch 5678: Training Loss: 0.08376529812812805 Validation Loss: 0.7396190166473389\n",
      "Epoch 5679: Training Loss: 0.0839887410402298 Validation Loss: 0.7403481602668762\n",
      "Epoch 5680: Training Loss: 0.08381719638903935 Validation Loss: 0.7384122014045715\n",
      "Epoch 5681: Training Loss: 0.0837203785777092 Validation Loss: 0.7373402714729309\n",
      "Epoch 5682: Training Loss: 0.083804356555144 Validation Loss: 0.7384797930717468\n",
      "Epoch 5683: Training Loss: 0.08397480100393295 Validation Loss: 0.7344579696655273\n",
      "Epoch 5684: Training Loss: 0.08379078656435013 Validation Loss: 0.7357181310653687\n",
      "Epoch 5685: Training Loss: 0.08374942094087601 Validation Loss: 0.7383376359939575\n",
      "Epoch 5686: Training Loss: 0.08367305745681126 Validation Loss: 0.7384119033813477\n",
      "Epoch 5687: Training Loss: 0.08367801954348882 Validation Loss: 0.7393290400505066\n",
      "Epoch 5688: Training Loss: 0.08372870584328969 Validation Loss: 0.7414621710777283\n",
      "Epoch 5689: Training Loss: 0.08365236222743988 Validation Loss: 0.7402803301811218\n",
      "Epoch 5690: Training Loss: 0.08369402090708415 Validation Loss: 0.7387551069259644\n",
      "Epoch 5691: Training Loss: 0.08367165674765904 Validation Loss: 0.7369665503501892\n",
      "Epoch 5692: Training Loss: 0.0837215210000674 Validation Loss: 0.7351836562156677\n",
      "Epoch 5693: Training Loss: 0.08357026427984238 Validation Loss: 0.7372463345527649\n",
      "Epoch 5694: Training Loss: 0.08359069128831227 Validation Loss: 0.7403669357299805\n",
      "Epoch 5695: Training Loss: 0.08363983655969302 Validation Loss: 0.7428327798843384\n",
      "Epoch 5696: Training Loss: 0.08362605422735214 Validation Loss: 0.7411845326423645\n",
      "Epoch 5697: Training Loss: 0.08402211219072342 Validation Loss: 0.7358854413032532\n",
      "Epoch 5698: Training Loss: 0.08346483359734218 Validation Loss: 0.7360206842422485\n",
      "Epoch 5699: Training Loss: 0.08363432933886845 Validation Loss: 0.7386037111282349\n",
      "Epoch 5700: Training Loss: 0.08369727681080501 Validation Loss: 0.7413769960403442\n",
      "Epoch 5701: Training Loss: 0.08397652953863144 Validation Loss: 0.7358984351158142\n",
      "Epoch 5702: Training Loss: 0.0834534540772438 Validation Loss: 0.7368409037590027\n",
      "Epoch 5703: Training Loss: 0.08347442994515102 Validation Loss: 0.7374367713928223\n",
      "Epoch 5704: Training Loss: 0.08352132389942805 Validation Loss: 0.7387727499008179\n",
      "Epoch 5705: Training Loss: 0.08357702195644379 Validation Loss: 0.7421256303787231\n",
      "Epoch 5706: Training Loss: 0.08338351299365361 Validation Loss: 0.7416868209838867\n",
      "Epoch 5707: Training Loss: 0.08367176602284114 Validation Loss: 0.7434268593788147\n",
      "Epoch 5708: Training Loss: 0.08335185299317042 Validation Loss: 0.7401298880577087\n",
      "Epoch 5709: Training Loss: 0.08338979383309682 Validation Loss: 0.7358708381652832\n",
      "Epoch 5710: Training Loss: 0.08334204306205113 Validation Loss: 0.734138548374176\n",
      "Epoch 5711: Training Loss: 0.08336683611075084 Validation Loss: 0.7351414561271667\n",
      "Epoch 5712: Training Loss: 0.0833442434668541 Validation Loss: 0.7380608320236206\n",
      "Epoch 5713: Training Loss: 0.0833226889371872 Validation Loss: 0.7395279407501221\n",
      "Epoch 5714: Training Loss: 0.08329109102487564 Validation Loss: 0.7386853694915771\n",
      "Epoch 5715: Training Loss: 0.08356742809216182 Validation Loss: 0.7426899075508118\n",
      "Epoch 5716: Training Loss: 0.08326452970504761 Validation Loss: 0.7394852638244629\n",
      "Epoch 5717: Training Loss: 0.08317512025435765 Validation Loss: 0.739194393157959\n",
      "Epoch 5718: Training Loss: 0.0831352174282074 Validation Loss: 0.737259566783905\n",
      "Epoch 5719: Training Loss: 0.08365455518166225 Validation Loss: 0.7338899374008179\n",
      "Epoch 5720: Training Loss: 0.08327621221542358 Validation Loss: 0.7381617426872253\n",
      "Epoch 5721: Training Loss: 0.08335600545008977 Validation Loss: 0.7429158687591553\n",
      "Epoch 5722: Training Loss: 0.0834968239068985 Validation Loss: 0.7453396320343018\n",
      "Epoch 5723: Training Loss: 0.083127960562706 Validation Loss: 0.7407552599906921\n",
      "Epoch 5724: Training Loss: 0.08314011494318645 Validation Loss: 0.7363701462745667\n",
      "Epoch 5725: Training Loss: 0.0831134170293808 Validation Loss: 0.7357613444328308\n",
      "Epoch 5726: Training Loss: 0.08306347827116649 Validation Loss: 0.7370681166648865\n",
      "Epoch 5727: Training Loss: 0.08302702754735947 Validation Loss: 0.739419162273407\n",
      "Epoch 5728: Training Loss: 0.08301351591944695 Validation Loss: 0.7403601408004761\n",
      "Epoch 5729: Training Loss: 0.08311915521820386 Validation Loss: 0.7419890761375427\n",
      "Epoch 5730: Training Loss: 0.08312434703111649 Validation Loss: 0.7385021448135376\n",
      "Epoch 5731: Training Loss: 0.08311253041028976 Validation Loss: 0.7390459179878235\n",
      "Epoch 5732: Training Loss: 0.08292806148529053 Validation Loss: 0.7384018898010254\n",
      "Epoch 5733: Training Loss: 0.08293970177570979 Validation Loss: 0.7363132834434509\n",
      "Epoch 5734: Training Loss: 0.08291387806336085 Validation Loss: 0.7375805974006653\n",
      "Epoch 5735: Training Loss: 0.08315900713205338 Validation Loss: 0.7346322536468506\n",
      "Epoch 5736: Training Loss: 0.08281222234169643 Validation Loss: 0.7370195984840393\n",
      "Epoch 5737: Training Loss: 0.08279417206843694 Validation Loss: 0.7388713359832764\n",
      "Epoch 5738: Training Loss: 0.08279025306304295 Validation Loss: 0.7406353950500488\n",
      "Epoch 5739: Training Loss: 0.08282641818126042 Validation Loss: 0.7423007488250732\n",
      "Epoch 5740: Training Loss: 0.08301965147256851 Validation Loss: 0.7392582297325134\n",
      "Epoch 5741: Training Loss: 0.08286697169144948 Validation Loss: 0.7384871244430542\n",
      "Epoch 5742: Training Loss: 0.08291126290957133 Validation Loss: 0.7417191863059998\n",
      "Epoch 5743: Training Loss: 0.083157184223334 Validation Loss: 0.7381245493888855\n",
      "Epoch 5744: Training Loss: 0.08268748472134273 Validation Loss: 0.7395046949386597\n",
      "Epoch 5745: Training Loss: 0.08284100890159607 Validation Loss: 0.7383931875228882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5746: Training Loss: 0.08301563064257304 Validation Loss: 0.7431570887565613\n",
      "Epoch 5747: Training Loss: 0.0827225682636102 Validation Loss: 0.7410818338394165\n",
      "Epoch 5748: Training Loss: 0.08279787252346675 Validation Loss: 0.7380228042602539\n",
      "Epoch 5749: Training Loss: 0.08272107938925426 Validation Loss: 0.73639976978302\n",
      "Epoch 5750: Training Loss: 0.08274303376674652 Validation Loss: 0.7352737188339233\n",
      "Epoch 5751: Training Loss: 0.08258640766143799 Validation Loss: 0.7387215495109558\n",
      "Epoch 5752: Training Loss: 0.08278115093708038 Validation Loss: 0.7433080673217773\n",
      "Epoch 5753: Training Loss: 0.08288845171531041 Validation Loss: 0.7450069785118103\n",
      "Epoch 5754: Training Loss: 0.08277604232231776 Validation Loss: 0.7393916845321655\n",
      "Epoch 5755: Training Loss: 0.08280955503384273 Validation Loss: 0.7354558110237122\n",
      "Epoch 5756: Training Loss: 0.08254209409157436 Validation Loss: 0.7365939021110535\n",
      "Epoch 5757: Training Loss: 0.08252747356891632 Validation Loss: 0.7389659881591797\n",
      "Epoch 5758: Training Loss: 0.08261056244373322 Validation Loss: 0.7419793009757996\n",
      "Epoch 5759: Training Loss: 0.0825358380873998 Validation Loss: 0.7414761185646057\n",
      "Epoch 5760: Training Loss: 0.08251423388719559 Validation Loss: 0.739467203617096\n",
      "Epoch 5761: Training Loss: 0.08248101423184077 Validation Loss: 0.7388355135917664\n",
      "Epoch 5762: Training Loss: 0.08261104176441829 Validation Loss: 0.7357937097549438\n",
      "Epoch 5763: Training Loss: 0.08253093560536702 Validation Loss: 0.7374619245529175\n",
      "Epoch 5764: Training Loss: 0.0824549173315366 Validation Loss: 0.7374926805496216\n",
      "Epoch 5765: Training Loss: 0.08233468731244405 Validation Loss: 0.7394514679908752\n",
      "Epoch 5766: Training Loss: 0.08241947491963704 Validation Loss: 0.7422075867652893\n",
      "Epoch 5767: Training Loss: 0.0824239378174146 Validation Loss: 0.7422256469726562\n",
      "Epoch 5768: Training Loss: 0.08238221953312556 Validation Loss: 0.7405198216438293\n",
      "Epoch 5769: Training Loss: 0.08236419906218846 Validation Loss: 0.7394649386405945\n",
      "Epoch 5770: Training Loss: 0.08242217202981313 Validation Loss: 0.7378135323524475\n",
      "Epoch 5771: Training Loss: 0.08236079663038254 Validation Loss: 0.7388214468955994\n",
      "Epoch 5772: Training Loss: 0.08228396872679393 Validation Loss: 0.7407404780387878\n",
      "Epoch 5773: Training Loss: 0.08236139267683029 Validation Loss: 0.7428329586982727\n",
      "Epoch 5774: Training Loss: 0.08251779526472092 Validation Loss: 0.7385116815567017\n",
      "Epoch 5775: Training Loss: 0.08220783869425456 Validation Loss: 0.7384007573127747\n",
      "Epoch 5776: Training Loss: 0.08225977420806885 Validation Loss: 0.7393204569816589\n",
      "Epoch 5777: Training Loss: 0.08234296490748723 Validation Loss: 0.740064263343811\n",
      "Epoch 5778: Training Loss: 0.08235197762648265 Validation Loss: 0.7409544587135315\n",
      "Epoch 5779: Training Loss: 0.08220284680525462 Validation Loss: 0.7393985986709595\n",
      "Epoch 5780: Training Loss: 0.08220197508732478 Validation Loss: 0.7381336092948914\n",
      "Epoch 5781: Training Loss: 0.08211257805426915 Validation Loss: 0.73692786693573\n",
      "Epoch 5782: Training Loss: 0.08214163531859715 Validation Loss: 0.7369928956031799\n",
      "Epoch 5783: Training Loss: 0.08219316725929578 Validation Loss: 0.7353458404541016\n",
      "Epoch 5784: Training Loss: 0.08215274413426717 Validation Loss: 0.7372022271156311\n",
      "Epoch 5785: Training Loss: 0.08205398668845494 Validation Loss: 0.7399725914001465\n",
      "Epoch 5786: Training Loss: 0.08217223733663559 Validation Loss: 0.7387921214103699\n",
      "Epoch 5787: Training Loss: 0.08201667418082555 Validation Loss: 0.740060031414032\n",
      "Epoch 5788: Training Loss: 0.08201093226671219 Validation Loss: 0.7414942979812622\n",
      "Epoch 5789: Training Loss: 0.08204501122236252 Validation Loss: 0.7427391409873962\n",
      "Epoch 5790: Training Loss: 0.082053375740846 Validation Loss: 0.7431865930557251\n",
      "Epoch 5791: Training Loss: 0.08198379228512447 Validation Loss: 0.7396478056907654\n",
      "Epoch 5792: Training Loss: 0.08207277208566666 Validation Loss: 0.7392805814743042\n",
      "Epoch 5793: Training Loss: 0.08209264278411865 Validation Loss: 0.7370761036872864\n",
      "Epoch 5794: Training Loss: 0.08195682366689046 Validation Loss: 0.736192524433136\n",
      "Epoch 5795: Training Loss: 0.08206589023272197 Validation Loss: 0.7390455007553101\n",
      "Epoch 5796: Training Loss: 0.08213132123152415 Validation Loss: 0.7418410181999207\n",
      "Epoch 5797: Training Loss: 0.08200828979412715 Validation Loss: 0.7395434975624084\n",
      "Epoch 5798: Training Loss: 0.08197182416915894 Validation Loss: 0.7375392317771912\n",
      "Epoch 5799: Training Loss: 0.08192668110132217 Validation Loss: 0.7400493621826172\n",
      "Epoch 5800: Training Loss: 0.08196545640627544 Validation Loss: 0.7415351867675781\n",
      "Epoch 5801: Training Loss: 0.08187993864218394 Validation Loss: 0.7412303686141968\n",
      "Epoch 5802: Training Loss: 0.08188647280136745 Validation Loss: 0.7409656643867493\n",
      "Epoch 5803: Training Loss: 0.08187734832366307 Validation Loss: 0.7379515171051025\n",
      "Epoch 5804: Training Loss: 0.08188450833161671 Validation Loss: 0.7397943735122681\n",
      "Epoch 5805: Training Loss: 0.08191682398319244 Validation Loss: 0.7368714809417725\n",
      "Epoch 5806: Training Loss: 0.08183796207110088 Validation Loss: 0.736565351486206\n",
      "Epoch 5807: Training Loss: 0.08204388121763866 Validation Loss: 0.7408953309059143\n",
      "Epoch 5808: Training Loss: 0.08170277625322342 Validation Loss: 0.7417895793914795\n",
      "Epoch 5809: Training Loss: 0.08176896969477336 Validation Loss: 0.7403067946434021\n",
      "Epoch 5810: Training Loss: 0.08179822688301404 Validation Loss: 0.740142285823822\n",
      "Epoch 5811: Training Loss: 0.08176202823718388 Validation Loss: 0.7397915124893188\n",
      "Epoch 5812: Training Loss: 0.08172063777844112 Validation Loss: 0.7386496067047119\n",
      "Epoch 5813: Training Loss: 0.08170665303866069 Validation Loss: 0.7411888837814331\n",
      "Epoch 5814: Training Loss: 0.08168691396713257 Validation Loss: 0.7401317358016968\n",
      "Epoch 5815: Training Loss: 0.08181677261988322 Validation Loss: 0.7375635504722595\n",
      "Epoch 5816: Training Loss: 0.08160028358300526 Validation Loss: 0.7379834055900574\n",
      "Epoch 5817: Training Loss: 0.08170597503582637 Validation Loss: 0.7423096895217896\n",
      "Epoch 5818: Training Loss: 0.08158057555556297 Validation Loss: 0.7428113222122192\n",
      "Epoch 5819: Training Loss: 0.08173805226882298 Validation Loss: 0.7394493818283081\n",
      "Epoch 5820: Training Loss: 0.08156577746073405 Validation Loss: 0.7387682199478149\n",
      "Epoch 5821: Training Loss: 0.08157554268836975 Validation Loss: 0.737551212310791\n",
      "Epoch 5822: Training Loss: 0.08150943120320638 Validation Loss: 0.7392076253890991\n",
      "Epoch 5823: Training Loss: 0.08147977540890376 Validation Loss: 0.7420073747634888\n",
      "Epoch 5824: Training Loss: 0.08143298824628194 Validation Loss: 0.7423389554023743\n",
      "Epoch 5825: Training Loss: 0.08140866458415985 Validation Loss: 0.7428601980209351\n",
      "Epoch 5826: Training Loss: 0.08142157644033432 Validation Loss: 0.7420251965522766\n",
      "Epoch 5827: Training Loss: 0.08139653379718463 Validation Loss: 0.7399729490280151\n",
      "Epoch 5828: Training Loss: 0.08137785891691844 Validation Loss: 0.7389318943023682\n",
      "Epoch 5829: Training Loss: 0.08139166235923767 Validation Loss: 0.7392321228981018\n",
      "Epoch 5830: Training Loss: 0.08137333889802296 Validation Loss: 0.7394159436225891\n",
      "Epoch 5831: Training Loss: 0.08156942576169968 Validation Loss: 0.7420209050178528\n",
      "Epoch 5832: Training Loss: 0.08141763259967168 Validation Loss: 0.7388148307800293\n",
      "Epoch 5833: Training Loss: 0.08129209031661351 Validation Loss: 0.7381335496902466\n",
      "Epoch 5834: Training Loss: 0.08158695697784424 Validation Loss: 0.7405779361724854\n",
      "Epoch 5835: Training Loss: 0.08155633012453715 Validation Loss: 0.7374889850616455\n",
      "Epoch 5836: Training Loss: 0.08128302544355392 Validation Loss: 0.7396893501281738\n",
      "Epoch 5837: Training Loss: 0.08137534807125728 Validation Loss: 0.7419183254241943\n",
      "Epoch 5838: Training Loss: 0.08156345287958781 Validation Loss: 0.7393186688423157\n",
      "Epoch 5839: Training Loss: 0.08138140539328258 Validation Loss: 0.7408633232116699\n",
      "Epoch 5840: Training Loss: 0.08125158647696178 Validation Loss: 0.7421935796737671\n",
      "Epoch 5841: Training Loss: 0.08127082635958989 Validation Loss: 0.7426472306251526\n",
      "Epoch 5842: Training Loss: 0.08126808702945709 Validation Loss: 0.7426493763923645\n",
      "Epoch 5843: Training Loss: 0.08144563436508179 Validation Loss: 0.7379794120788574\n",
      "Epoch 5844: Training Loss: 0.08117960145076115 Validation Loss: 0.7370770573616028\n",
      "Epoch 5845: Training Loss: 0.08119276662667592 Validation Loss: 0.7397318482398987\n",
      "Epoch 5846: Training Loss: 0.08118367691834767 Validation Loss: 0.7401462197303772\n",
      "Epoch 5847: Training Loss: 0.08107571552197139 Validation Loss: 0.7413727641105652\n",
      "Epoch 5848: Training Loss: 0.08134076495965321 Validation Loss: 0.7437828779220581\n",
      "Epoch 5849: Training Loss: 0.0814913734793663 Validation Loss: 0.7387992143630981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5850: Training Loss: 0.08104977880915006 Validation Loss: 0.7388375401496887\n",
      "Epoch 5851: Training Loss: 0.08100920418898265 Validation Loss: 0.738886833190918\n",
      "Epoch 5852: Training Loss: 0.08107781658569972 Validation Loss: 0.7400460839271545\n",
      "Epoch 5853: Training Loss: 0.08105426281690598 Validation Loss: 0.7403526306152344\n",
      "Epoch 5854: Training Loss: 0.08096113552649815 Validation Loss: 0.7405259609222412\n",
      "Epoch 5855: Training Loss: 0.081174631913503 Validation Loss: 0.7416367530822754\n",
      "Epoch 5856: Training Loss: 0.0810849020878474 Validation Loss: 0.7418982982635498\n",
      "Epoch 5857: Training Loss: 0.08093352367480595 Validation Loss: 0.7396875023841858\n",
      "Epoch 5858: Training Loss: 0.08093484491109848 Validation Loss: 0.7394298911094666\n",
      "Epoch 5859: Training Loss: 0.08099501083294551 Validation Loss: 0.7378438115119934\n",
      "Epoch 5860: Training Loss: 0.08131417135397594 Validation Loss: 0.742401659488678\n",
      "Epoch 5861: Training Loss: 0.080953282614549 Validation Loss: 0.7426664233207703\n",
      "Epoch 5862: Training Loss: 0.08090520650148392 Validation Loss: 0.741313636302948\n",
      "Epoch 5863: Training Loss: 0.08082507799069087 Validation Loss: 0.7398837804794312\n",
      "Epoch 5864: Training Loss: 0.08100846906503041 Validation Loss: 0.7417020797729492\n",
      "Epoch 5865: Training Loss: 0.08086124062538147 Validation Loss: 0.7399321794509888\n",
      "Epoch 5866: Training Loss: 0.08093295743068059 Validation Loss: 0.7405636310577393\n",
      "Epoch 5867: Training Loss: 0.08081187059481938 Validation Loss: 0.7403099536895752\n",
      "Epoch 5868: Training Loss: 0.0807822694381078 Validation Loss: 0.7403071522712708\n",
      "Epoch 5869: Training Loss: 0.08078282574812572 Validation Loss: 0.7379323244094849\n",
      "Epoch 5870: Training Loss: 0.08078732341527939 Validation Loss: 0.7398077249526978\n",
      "Epoch 5871: Training Loss: 0.08070517579714458 Validation Loss: 0.7396467328071594\n",
      "Epoch 5872: Training Loss: 0.08073705186446507 Validation Loss: 0.7403647303581238\n",
      "Epoch 5873: Training Loss: 0.08086729298035304 Validation Loss: 0.7419968843460083\n",
      "Epoch 5874: Training Loss: 0.08065936714410782 Validation Loss: 0.7407580614089966\n",
      "Epoch 5875: Training Loss: 0.0808170313636462 Validation Loss: 0.7377398610115051\n",
      "Epoch 5876: Training Loss: 0.08087473114331563 Validation Loss: 0.7358701825141907\n",
      "Epoch 5877: Training Loss: 0.08070046206315358 Validation Loss: 0.7385292053222656\n",
      "Epoch 5878: Training Loss: 0.08068096389373143 Validation Loss: 0.7423895001411438\n",
      "Epoch 5879: Training Loss: 0.08081792294979095 Validation Loss: 0.7466875910758972\n",
      "Epoch 5880: Training Loss: 0.08066690216461818 Validation Loss: 0.7441226243972778\n",
      "Epoch 5881: Training Loss: 0.08058914045492808 Validation Loss: 0.7420695424079895\n",
      "Epoch 5882: Training Loss: 0.08054037888844807 Validation Loss: 0.7410343885421753\n",
      "Epoch 5883: Training Loss: 0.08059746772050858 Validation Loss: 0.7413890957832336\n",
      "Epoch 5884: Training Loss: 0.0805895949403445 Validation Loss: 0.738589346408844\n",
      "Epoch 5885: Training Loss: 0.08053228259086609 Validation Loss: 0.7379264831542969\n",
      "Epoch 5886: Training Loss: 0.08056738475958507 Validation Loss: 0.7369194030761719\n",
      "Epoch 5887: Training Loss: 0.08064163972934087 Validation Loss: 0.740645706653595\n",
      "Epoch 5888: Training Loss: 0.08062360932429631 Validation Loss: 0.7395988702774048\n",
      "Epoch 5889: Training Loss: 0.08048659563064575 Validation Loss: 0.7410501837730408\n",
      "Epoch 5890: Training Loss: 0.08059682448705037 Validation Loss: 0.740023672580719\n",
      "Epoch 5891: Training Loss: 0.08040913939476013 Validation Loss: 0.7408504486083984\n",
      "Epoch 5892: Training Loss: 0.08060523867607117 Validation Loss: 0.7448585629463196\n",
      "Epoch 5893: Training Loss: 0.08046652128299077 Validation Loss: 0.7442523837089539\n",
      "Epoch 5894: Training Loss: 0.0804317444562912 Validation Loss: 0.7429220676422119\n",
      "Epoch 5895: Training Loss: 0.08067530890305837 Validation Loss: 0.7389668822288513\n",
      "Epoch 5896: Training Loss: 0.0805756002664566 Validation Loss: 0.7363916039466858\n",
      "Epoch 5897: Training Loss: 0.0807344416777293 Validation Loss: 0.7420613169670105\n",
      "Epoch 5898: Training Loss: 0.08043014009793599 Validation Loss: 0.7434287071228027\n",
      "Epoch 5899: Training Loss: 0.08028832077980042 Validation Loss: 0.7425861954689026\n",
      "Epoch 5900: Training Loss: 0.08029053608576457 Validation Loss: 0.7412465214729309\n",
      "Epoch 5901: Training Loss: 0.08039470762014389 Validation Loss: 0.7406099438667297\n",
      "Epoch 5902: Training Loss: 0.08024387806653976 Validation Loss: 0.74055016040802\n",
      "Epoch 5903: Training Loss: 0.08022753397623698 Validation Loss: 0.7397441864013672\n",
      "Epoch 5904: Training Loss: 0.08040640751520793 Validation Loss: 0.7386286854743958\n",
      "Epoch 5905: Training Loss: 0.08021879941225052 Validation Loss: 0.7384805083274841\n",
      "Epoch 5906: Training Loss: 0.0803586095571518 Validation Loss: 0.741115152835846\n",
      "Epoch 5907: Training Loss: 0.0801862006386121 Validation Loss: 0.7409030795097351\n",
      "Epoch 5908: Training Loss: 0.0802070324619611 Validation Loss: 0.7413065433502197\n",
      "Epoch 5909: Training Loss: 0.08029507845640182 Validation Loss: 0.7393989562988281\n",
      "Epoch 5910: Training Loss: 0.0802726149559021 Validation Loss: 0.7387425899505615\n",
      "Epoch 5911: Training Loss: 0.08017266790072124 Validation Loss: 0.7429198026657104\n",
      "Epoch 5912: Training Loss: 0.08035574853420258 Validation Loss: 0.7463844418525696\n",
      "Epoch 5913: Training Loss: 0.0802719493707021 Validation Loss: 0.7442476749420166\n",
      "Epoch 5914: Training Loss: 0.0800764337182045 Validation Loss: 0.7413144111633301\n",
      "Epoch 5915: Training Loss: 0.08012791971365611 Validation Loss: 0.740817129611969\n",
      "Epoch 5916: Training Loss: 0.0800787980357806 Validation Loss: 0.7410274744033813\n",
      "Epoch 5917: Training Loss: 0.08016922821601231 Validation Loss: 0.7385138869285583\n",
      "Epoch 5918: Training Loss: 0.08020631472269694 Validation Loss: 0.7372008562088013\n",
      "Epoch 5919: Training Loss: 0.08000865081946056 Validation Loss: 0.7401215434074402\n",
      "Epoch 5920: Training Loss: 0.08001578350861867 Validation Loss: 0.7436095476150513\n",
      "Epoch 5921: Training Loss: 0.07998100916544597 Validation Loss: 0.7433225512504578\n",
      "Epoch 5922: Training Loss: 0.08010432620843251 Validation Loss: 0.7443640828132629\n",
      "Epoch 5923: Training Loss: 0.08013088256120682 Validation Loss: 0.7408502697944641\n",
      "Epoch 5924: Training Loss: 0.0800428291161855 Validation Loss: 0.7392365336418152\n",
      "Epoch 5925: Training Loss: 0.07988979170719783 Validation Loss: 0.7383164167404175\n",
      "Epoch 5926: Training Loss: 0.07998913774887721 Validation Loss: 0.7405214905738831\n",
      "Epoch 5927: Training Loss: 0.0798938622077306 Validation Loss: 0.7410860657691956\n",
      "Epoch 5928: Training Loss: 0.07990530878305435 Validation Loss: 0.7395657896995544\n",
      "Epoch 5929: Training Loss: 0.07988588760296504 Validation Loss: 0.7388173341751099\n",
      "Epoch 5930: Training Loss: 0.07984319577614467 Validation Loss: 0.739502489566803\n",
      "Epoch 5931: Training Loss: 0.07985215137402217 Validation Loss: 0.7404271364212036\n",
      "Epoch 5932: Training Loss: 0.07989755272865295 Validation Loss: 0.7436829805374146\n",
      "Epoch 5933: Training Loss: 0.07981857160727183 Validation Loss: 0.7427352070808411\n",
      "Epoch 5934: Training Loss: 0.08001506328582764 Validation Loss: 0.7411822080612183\n",
      "Epoch 5935: Training Loss: 0.07976299772659938 Validation Loss: 0.7420616149902344\n",
      "Epoch 5936: Training Loss: 0.07994452615578969 Validation Loss: 0.7444254159927368\n",
      "Epoch 5937: Training Loss: 0.07978151986996333 Validation Loss: 0.7427446246147156\n",
      "Epoch 5938: Training Loss: 0.07972568025191624 Validation Loss: 0.7410967946052551\n",
      "Epoch 5939: Training Loss: 0.08041711896657944 Validation Loss: 0.7447527647018433\n",
      "Epoch 5940: Training Loss: 0.08000388493140538 Validation Loss: 0.7402486801147461\n",
      "Epoch 5941: Training Loss: 0.07971714188655217 Validation Loss: 0.7393966913223267\n",
      "Epoch 5942: Training Loss: 0.07978120446205139 Validation Loss: 0.7409175634384155\n",
      "Epoch 5943: Training Loss: 0.07964161535104115 Validation Loss: 0.7407281398773193\n",
      "Epoch 5944: Training Loss: 0.07968189070622127 Validation Loss: 0.7388521432876587\n",
      "Epoch 5945: Training Loss: 0.07963228225708008 Validation Loss: 0.7390397787094116\n",
      "Epoch 5946: Training Loss: 0.07977451384067535 Validation Loss: 0.7434606552124023\n",
      "Epoch 5947: Training Loss: 0.07965941975514094 Validation Loss: 0.7442949414253235\n",
      "Epoch 5948: Training Loss: 0.07963634033997853 Validation Loss: 0.7429743409156799\n",
      "Epoch 5949: Training Loss: 0.07964587956666946 Validation Loss: 0.7409252524375916\n",
      "Epoch 5950: Training Loss: 0.07956248770157497 Validation Loss: 0.7399006485939026\n",
      "Epoch 5951: Training Loss: 0.07956923792759578 Validation Loss: 0.7390839457511902\n",
      "Epoch 5952: Training Loss: 0.07968407620986302 Validation Loss: 0.742234468460083\n",
      "Epoch 5953: Training Loss: 0.07954480250676473 Validation Loss: 0.7426172494888306\n",
      "Epoch 5954: Training Loss: 0.0795476461450259 Validation Loss: 0.7433729767799377\n",
      "Epoch 5955: Training Loss: 0.07975569367408752 Validation Loss: 0.7448922991752625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5956: Training Loss: 0.07956501096487045 Validation Loss: 0.7400607466697693\n",
      "Epoch 5957: Training Loss: 0.07941632096966107 Validation Loss: 0.7384285926818848\n",
      "Epoch 5958: Training Loss: 0.07947931190331776 Validation Loss: 0.7376629710197449\n",
      "Epoch 5959: Training Loss: 0.07946149756511052 Validation Loss: 0.7395128607749939\n",
      "Epoch 5960: Training Loss: 0.07939044386148453 Validation Loss: 0.7411710619926453\n",
      "Epoch 5961: Training Loss: 0.07942003756761551 Validation Loss: 0.7404918670654297\n",
      "Epoch 5962: Training Loss: 0.07936828583478928 Validation Loss: 0.7407832145690918\n",
      "Epoch 5963: Training Loss: 0.07943369696537654 Validation Loss: 0.7417782545089722\n",
      "Epoch 5964: Training Loss: 0.0794098104039828 Validation Loss: 0.7416459321975708\n",
      "Epoch 5965: Training Loss: 0.0793611208597819 Validation Loss: 0.7443450093269348\n",
      "Epoch 5966: Training Loss: 0.07936721295118332 Validation Loss: 0.744458019733429\n",
      "Epoch 5967: Training Loss: 0.07935209820667903 Validation Loss: 0.7431735396385193\n",
      "Epoch 5968: Training Loss: 0.07927439610163371 Validation Loss: 0.7414246797561646\n",
      "Epoch 5969: Training Loss: 0.07939306398232777 Validation Loss: 0.7390149831771851\n",
      "Epoch 5970: Training Loss: 0.07952721913655598 Validation Loss: 0.7417249083518982\n",
      "Epoch 5971: Training Loss: 0.0793318251768748 Validation Loss: 0.7407175302505493\n",
      "Epoch 5972: Training Loss: 0.07959569742282231 Validation Loss: 0.7451574802398682\n",
      "Epoch 5973: Training Loss: 0.07923139383395512 Validation Loss: 0.7442116141319275\n",
      "Epoch 5974: Training Loss: 0.07921876758337021 Validation Loss: 0.7412465810775757\n",
      "Epoch 5975: Training Loss: 0.07937281082073848 Validation Loss: 0.738249659538269\n",
      "Epoch 5976: Training Loss: 0.07921914259592693 Validation Loss: 0.738406777381897\n",
      "Epoch 5977: Training Loss: 0.07916419953107834 Validation Loss: 0.7402335405349731\n",
      "Epoch 5978: Training Loss: 0.07934933279951413 Validation Loss: 0.7449005246162415\n",
      "Epoch 5979: Training Loss: 0.0792599469423294 Validation Loss: 0.7445347309112549\n",
      "Epoch 5980: Training Loss: 0.07929054647684097 Validation Loss: 0.7451067566871643\n",
      "Epoch 5981: Training Loss: 0.0792537455757459 Validation Loss: 0.7411678433418274\n",
      "Epoch 5982: Training Loss: 0.07916931807994843 Validation Loss: 0.7401195764541626\n",
      "Epoch 5983: Training Loss: 0.07908382763465245 Validation Loss: 0.7397496700286865\n",
      "Epoch 5984: Training Loss: 0.07906938840945561 Validation Loss: 0.7403174638748169\n",
      "Epoch 5985: Training Loss: 0.07904256631930669 Validation Loss: 0.74242103099823\n",
      "Epoch 5986: Training Loss: 0.07924028237660725 Validation Loss: 0.7446345686912537\n",
      "Epoch 5987: Training Loss: 0.07910340527693431 Validation Loss: 0.7443966865539551\n",
      "Epoch 5988: Training Loss: 0.07917797317107518 Validation Loss: 0.7401129007339478\n",
      "Epoch 5989: Training Loss: 0.0790681963165601 Validation Loss: 0.7381378412246704\n",
      "Epoch 5990: Training Loss: 0.07907407979170482 Validation Loss: 0.7385212182998657\n",
      "Epoch 5991: Training Loss: 0.07900964220364888 Validation Loss: 0.7416653037071228\n",
      "Epoch 5992: Training Loss: 0.07895943770805995 Validation Loss: 0.741344690322876\n",
      "Epoch 5993: Training Loss: 0.07893202950557072 Validation Loss: 0.7431074380874634\n",
      "Epoch 5994: Training Loss: 0.07893370340267818 Validation Loss: 0.744069516658783\n",
      "Epoch 5995: Training Loss: 0.07893102864424388 Validation Loss: 0.7435569763183594\n",
      "Epoch 5996: Training Loss: 0.07900104175011317 Validation Loss: 0.7440822720527649\n",
      "Epoch 5997: Training Loss: 0.07918848097324371 Validation Loss: 0.7420132756233215\n",
      "Epoch 5998: Training Loss: 0.07890463123718898 Validation Loss: 0.7426325082778931\n",
      "Epoch 5999: Training Loss: 0.07900370409091313 Validation Loss: 0.7442612051963806\n",
      "Epoch 6000: Training Loss: 0.07894789179166158 Validation Loss: 0.74359130859375\n",
      "Epoch 6001: Training Loss: 0.07882905006408691 Validation Loss: 0.7401471734046936\n",
      "Epoch 6002: Training Loss: 0.07893273731072743 Validation Loss: 0.7383849620819092\n",
      "Epoch 6003: Training Loss: 0.07892517000436783 Validation Loss: 0.7372297644615173\n",
      "Epoch 6004: Training Loss: 0.07880488286415736 Validation Loss: 0.7389821410179138\n",
      "Epoch 6005: Training Loss: 0.07875709732373555 Validation Loss: 0.7414919137954712\n",
      "Epoch 6006: Training Loss: 0.07871539394060771 Validation Loss: 0.7434937357902527\n",
      "Epoch 6007: Training Loss: 0.07875503847996394 Validation Loss: 0.7434428334236145\n",
      "Epoch 6008: Training Loss: 0.07872008035580318 Validation Loss: 0.744364857673645\n",
      "Epoch 6009: Training Loss: 0.07883303612470627 Validation Loss: 0.7428211569786072\n",
      "Epoch 6010: Training Loss: 0.07868162045876186 Validation Loss: 0.7434343695640564\n",
      "Epoch 6011: Training Loss: 0.07870038350423177 Validation Loss: 0.7419466376304626\n",
      "Epoch 6012: Training Loss: 0.07896474003791809 Validation Loss: 0.7453328967094421\n",
      "Epoch 6013: Training Loss: 0.07871384173631668 Validation Loss: 0.7452094554901123\n",
      "Epoch 6014: Training Loss: 0.07887277379631996 Validation Loss: 0.739900529384613\n",
      "Epoch 6015: Training Loss: 0.07894353320201238 Validation Loss: 0.7422986626625061\n",
      "Epoch 6016: Training Loss: 0.0786169171333313 Validation Loss: 0.7410644888877869\n",
      "Epoch 6017: Training Loss: 0.07861824085315068 Validation Loss: 0.7413002848625183\n",
      "Epoch 6018: Training Loss: 0.07858128845691681 Validation Loss: 0.7409953474998474\n",
      "Epoch 6019: Training Loss: 0.07872416079044342 Validation Loss: 0.7430038452148438\n",
      "Epoch 6020: Training Loss: 0.07859198500712712 Validation Loss: 0.7432935833930969\n",
      "Epoch 6021: Training Loss: 0.078528826435407 Validation Loss: 0.7404698133468628\n",
      "Epoch 6022: Training Loss: 0.07863564789295197 Validation Loss: 0.7397127151489258\n",
      "Epoch 6023: Training Loss: 0.07851722339789073 Validation Loss: 0.7403455972671509\n",
      "Epoch 6024: Training Loss: 0.07863373061021169 Validation Loss: 0.7390623688697815\n",
      "Epoch 6025: Training Loss: 0.0788029134273529 Validation Loss: 0.7441392540931702\n",
      "Epoch 6026: Training Loss: 0.07854210833708446 Validation Loss: 0.7462435960769653\n",
      "Epoch 6027: Training Loss: 0.07849686841169994 Validation Loss: 0.7441124320030212\n",
      "Epoch 6028: Training Loss: 0.0787770797808965 Validation Loss: 0.7405834794044495\n",
      "Epoch 6029: Training Loss: 0.07848672817150752 Validation Loss: 0.7418490052223206\n",
      "Epoch 6030: Training Loss: 0.07844381034374237 Validation Loss: 0.740999698638916\n",
      "Epoch 6031: Training Loss: 0.07842569053173065 Validation Loss: 0.7406960129737854\n",
      "Epoch 6032: Training Loss: 0.07867920150359471 Validation Loss: 0.7442305684089661\n",
      "Epoch 6033: Training Loss: 0.07846235980590184 Validation Loss: 0.7458899617195129\n",
      "Epoch 6034: Training Loss: 0.07838599383831024 Validation Loss: 0.7452066540718079\n",
      "Epoch 6035: Training Loss: 0.0783138448993365 Validation Loss: 0.7433509230613708\n",
      "Epoch 6036: Training Loss: 0.07850926369428635 Validation Loss: 0.7445910573005676\n",
      "Epoch 6037: Training Loss: 0.0783283847073714 Validation Loss: 0.7401275634765625\n",
      "Epoch 6038: Training Loss: 0.07838815947373708 Validation Loss: 0.7391756772994995\n",
      "Epoch 6039: Training Loss: 0.07843760152657826 Validation Loss: 0.7375029921531677\n",
      "Epoch 6040: Training Loss: 0.07833339522282283 Validation Loss: 0.740313708782196\n",
      "Epoch 6041: Training Loss: 0.07852179060379665 Validation Loss: 0.7395949959754944\n",
      "Epoch 6042: Training Loss: 0.0783347338438034 Validation Loss: 0.7412981986999512\n",
      "Epoch 6043: Training Loss: 0.07856954137484233 Validation Loss: 0.7483079433441162\n",
      "Epoch 6044: Training Loss: 0.07842199504375458 Validation Loss: 0.7501795887947083\n",
      "Epoch 6045: Training Loss: 0.07831612726052602 Validation Loss: 0.7464384436607361\n",
      "Epoch 6046: Training Loss: 0.07816632091999054 Validation Loss: 0.7438534498214722\n",
      "Epoch 6047: Training Loss: 0.0781405046582222 Validation Loss: 0.7412564158439636\n",
      "Epoch 6048: Training Loss: 0.07811742772658666 Validation Loss: 0.7398888468742371\n",
      "Epoch 6049: Training Loss: 0.07823901623487473 Validation Loss: 0.7377060651779175\n",
      "Epoch 6050: Training Loss: 0.07825179398059845 Validation Loss: 0.7388931512832642\n",
      "Epoch 6051: Training Loss: 0.07812122752269109 Validation Loss: 0.7400684952735901\n",
      "Epoch 6052: Training Loss: 0.0784811998407046 Validation Loss: 0.7391251921653748\n",
      "Epoch 6053: Training Loss: 0.07807008425394694 Validation Loss: 0.7437698841094971\n",
      "Epoch 6054: Training Loss: 0.07850361118714015 Validation Loss: 0.7490286827087402\n",
      "Epoch 6055: Training Loss: 0.07814103613297145 Validation Loss: 0.7489946484565735\n",
      "Epoch 6056: Training Loss: 0.07813939700524013 Validation Loss: 0.7477100491523743\n",
      "Epoch 6057: Training Loss: 0.07805011669794719 Validation Loss: 0.742708146572113\n",
      "Epoch 6058: Training Loss: 0.07800614709655444 Validation Loss: 0.7389116287231445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6059: Training Loss: 0.07812925676504771 Validation Loss: 0.7368860244750977\n",
      "Epoch 6060: Training Loss: 0.07813191910584767 Validation Loss: 0.7388578057289124\n",
      "Epoch 6061: Training Loss: 0.07814303785562515 Validation Loss: 0.743075966835022\n",
      "Epoch 6062: Training Loss: 0.07808811962604523 Validation Loss: 0.744962751865387\n",
      "Epoch 6063: Training Loss: 0.07807690401871999 Validation Loss: 0.7428160905838013\n",
      "Epoch 6064: Training Loss: 0.07793143639961879 Validation Loss: 0.7417995929718018\n",
      "Epoch 6065: Training Loss: 0.07794697086016338 Validation Loss: 0.7429065108299255\n",
      "Epoch 6066: Training Loss: 0.07792268693447113 Validation Loss: 0.741462767124176\n",
      "Epoch 6067: Training Loss: 0.07788312435150146 Validation Loss: 0.743108868598938\n",
      "Epoch 6068: Training Loss: 0.07789181917905807 Validation Loss: 0.7431300282478333\n",
      "Epoch 6069: Training Loss: 0.07783459375301997 Validation Loss: 0.7433056235313416\n",
      "Epoch 6070: Training Loss: 0.07784927884737651 Validation Loss: 0.7424600720405579\n",
      "Epoch 6071: Training Loss: 0.07784975568453471 Validation Loss: 0.7433586120605469\n",
      "Epoch 6072: Training Loss: 0.07805535197257996 Validation Loss: 0.7451356053352356\n",
      "Epoch 6073: Training Loss: 0.07783366491397221 Validation Loss: 0.7439404726028442\n",
      "Epoch 6074: Training Loss: 0.07784239451090495 Validation Loss: 0.741802453994751\n",
      "Epoch 6075: Training Loss: 0.0778663878639539 Validation Loss: 0.7433832883834839\n",
      "Epoch 6076: Training Loss: 0.07773224264383316 Validation Loss: 0.7427508234977722\n",
      "Epoch 6077: Training Loss: 0.07772885511318843 Validation Loss: 0.7416306138038635\n",
      "Epoch 6078: Training Loss: 0.07793330649534862 Validation Loss: 0.7402896881103516\n",
      "Epoch 6079: Training Loss: 0.07791078339020412 Validation Loss: 0.7438691854476929\n",
      "Epoch 6080: Training Loss: 0.07767381022373836 Validation Loss: 0.7434145212173462\n",
      "Epoch 6081: Training Loss: 0.07776812588175137 Validation Loss: 0.7443016767501831\n",
      "Epoch 6082: Training Loss: 0.07769034554560979 Validation Loss: 0.7437155246734619\n",
      "Epoch 6083: Training Loss: 0.07780613253513972 Validation Loss: 0.7405078411102295\n",
      "Epoch 6084: Training Loss: 0.07782759269078572 Validation Loss: 0.7393538355827332\n",
      "Epoch 6085: Training Loss: 0.07769948616623878 Validation Loss: 0.7421727180480957\n",
      "Epoch 6086: Training Loss: 0.07764568428198497 Validation Loss: 0.7431971430778503\n",
      "Epoch 6087: Training Loss: 0.07759720087051392 Validation Loss: 0.7448289394378662\n",
      "Epoch 6088: Training Loss: 0.07770327602823575 Validation Loss: 0.7472405433654785\n",
      "Epoch 6089: Training Loss: 0.07767206306258838 Validation Loss: 0.7462239861488342\n",
      "Epoch 6090: Training Loss: 0.07768817742665608 Validation Loss: 0.7433304786682129\n",
      "Epoch 6091: Training Loss: 0.07781559725602467 Validation Loss: 0.7455417513847351\n",
      "Epoch 6092: Training Loss: 0.07751981044809024 Validation Loss: 0.7437297105789185\n",
      "Epoch 6093: Training Loss: 0.07762111226717631 Validation Loss: 0.7402665615081787\n",
      "Epoch 6094: Training Loss: 0.07776667922735214 Validation Loss: 0.7387524843215942\n",
      "Epoch 6095: Training Loss: 0.07751463850339253 Validation Loss: 0.7415381669998169\n",
      "Epoch 6096: Training Loss: 0.07781677320599556 Validation Loss: 0.746315598487854\n",
      "Epoch 6097: Training Loss: 0.07763796299695969 Validation Loss: 0.7478500604629517\n",
      "Epoch 6098: Training Loss: 0.07751309995849927 Validation Loss: 0.7444603443145752\n",
      "Epoch 6099: Training Loss: 0.07746638357639313 Validation Loss: 0.7430785298347473\n",
      "Epoch 6100: Training Loss: 0.07760373751322429 Validation Loss: 0.7407824397087097\n",
      "Epoch 6101: Training Loss: 0.07753795385360718 Validation Loss: 0.739338755607605\n",
      "Epoch 6102: Training Loss: 0.07746784140666325 Validation Loss: 0.7411080002784729\n",
      "Epoch 6103: Training Loss: 0.07738591482241948 Validation Loss: 0.7426276206970215\n",
      "Epoch 6104: Training Loss: 0.07756209125121434 Validation Loss: 0.7451470494270325\n",
      "Epoch 6105: Training Loss: 0.07742941876252492 Validation Loss: 0.7442599534988403\n",
      "Epoch 6106: Training Loss: 0.07747595012187958 Validation Loss: 0.7426267266273499\n",
      "Epoch 6107: Training Loss: 0.07748507211605708 Validation Loss: 0.7415505647659302\n",
      "Epoch 6108: Training Loss: 0.07737373560667038 Validation Loss: 0.7435798645019531\n",
      "Epoch 6109: Training Loss: 0.07731938610474269 Validation Loss: 0.7437018752098083\n",
      "Epoch 6110: Training Loss: 0.07737161964178085 Validation Loss: 0.7422502636909485\n",
      "Epoch 6111: Training Loss: 0.07748506218194962 Validation Loss: 0.7445095181465149\n",
      "Epoch 6112: Training Loss: 0.07724438856045406 Validation Loss: 0.7437312006950378\n",
      "Epoch 6113: Training Loss: 0.07725135733683904 Validation Loss: 0.7434139251708984\n",
      "Epoch 6114: Training Loss: 0.07725631197293599 Validation Loss: 0.7440536022186279\n",
      "Epoch 6115: Training Loss: 0.07727910329898198 Validation Loss: 0.7426153421401978\n",
      "Epoch 6116: Training Loss: 0.07740723590056102 Validation Loss: 0.745463490486145\n",
      "Epoch 6117: Training Loss: 0.07726756234963734 Validation Loss: 0.7438945174217224\n",
      "Epoch 6118: Training Loss: 0.07716668024659157 Validation Loss: 0.7429093718528748\n",
      "Epoch 6119: Training Loss: 0.07731837655107181 Validation Loss: 0.7432355284690857\n",
      "Epoch 6120: Training Loss: 0.07728724430004756 Validation Loss: 0.7418259978294373\n",
      "Epoch 6121: Training Loss: 0.07724018767476082 Validation Loss: 0.7400425672531128\n",
      "Epoch 6122: Training Loss: 0.07721668233474095 Validation Loss: 0.7416427731513977\n",
      "Epoch 6123: Training Loss: 0.07713231941064198 Validation Loss: 0.7443746328353882\n",
      "Epoch 6124: Training Loss: 0.07709707319736481 Validation Loss: 0.7458586096763611\n",
      "Epoch 6125: Training Loss: 0.0772020493944486 Validation Loss: 0.7446434497833252\n",
      "Epoch 6126: Training Loss: 0.077133409678936 Validation Loss: 0.7445717453956604\n",
      "Epoch 6127: Training Loss: 0.07706766948103905 Validation Loss: 0.7444756031036377\n",
      "Epoch 6128: Training Loss: 0.07732035840551059 Validation Loss: 0.7414056062698364\n",
      "Epoch 6129: Training Loss: 0.0772234673301379 Validation Loss: 0.7442299723625183\n",
      "Epoch 6130: Training Loss: 0.07705077528953552 Validation Loss: 0.7437075972557068\n",
      "Epoch 6131: Training Loss: 0.07708469778299332 Validation Loss: 0.7423011064529419\n",
      "Epoch 6132: Training Loss: 0.0771913081407547 Validation Loss: 0.745043933391571\n",
      "Epoch 6133: Training Loss: 0.07734303921461105 Validation Loss: 0.7416719198226929\n",
      "Epoch 6134: Training Loss: 0.07704679171244304 Validation Loss: 0.7422643899917603\n",
      "Epoch 6135: Training Loss: 0.07700641701618831 Validation Loss: 0.7420729398727417\n",
      "Epoch 6136: Training Loss: 0.07697564611832301 Validation Loss: 0.7427643537521362\n",
      "Epoch 6137: Training Loss: 0.07688587158918381 Validation Loss: 0.7448383569717407\n",
      "Epoch 6138: Training Loss: 0.0769442617893219 Validation Loss: 0.7463477253913879\n",
      "Epoch 6139: Training Loss: 0.07698888580004375 Validation Loss: 0.7447968125343323\n",
      "Epoch 6140: Training Loss: 0.07709942758083344 Validation Loss: 0.7472348213195801\n",
      "Epoch 6141: Training Loss: 0.0768909715116024 Validation Loss: 0.7450284957885742\n",
      "Epoch 6142: Training Loss: 0.07697320232788722 Validation Loss: 0.7456883788108826\n",
      "Epoch 6143: Training Loss: 0.07699932530522346 Validation Loss: 0.7420588731765747\n",
      "Epoch 6144: Training Loss: 0.07696925103664398 Validation Loss: 0.743326723575592\n",
      "Epoch 6145: Training Loss: 0.07684637109438579 Validation Loss: 0.742267906665802\n",
      "Epoch 6146: Training Loss: 0.07680042833089828 Validation Loss: 0.7428656816482544\n",
      "Epoch 6147: Training Loss: 0.07694439093271892 Validation Loss: 0.7405858039855957\n",
      "Epoch 6148: Training Loss: 0.07685799151659012 Validation Loss: 0.7421509027481079\n",
      "Epoch 6149: Training Loss: 0.07693371176719666 Validation Loss: 0.7451350092887878\n",
      "Epoch 6150: Training Loss: 0.07678287476301193 Validation Loss: 0.7449834942817688\n",
      "Epoch 6151: Training Loss: 0.07682035615046819 Validation Loss: 0.7458685636520386\n",
      "Epoch 6152: Training Loss: 0.07680178930362065 Validation Loss: 0.7450003623962402\n",
      "Epoch 6153: Training Loss: 0.07682304456830025 Validation Loss: 0.741213321685791\n",
      "Epoch 6154: Training Loss: 0.07681591808795929 Validation Loss: 0.7396757006645203\n",
      "Epoch 6155: Training Loss: 0.07673605531454086 Validation Loss: 0.7421493530273438\n",
      "Epoch 6156: Training Loss: 0.0768512561917305 Validation Loss: 0.741189181804657\n",
      "Epoch 6157: Training Loss: 0.07678565134604771 Validation Loss: 0.7417422533035278\n",
      "Epoch 6158: Training Loss: 0.0766819640994072 Validation Loss: 0.7468735575675964\n",
      "Epoch 6159: Training Loss: 0.07664654776453972 Validation Loss: 0.7480770349502563\n",
      "Epoch 6160: Training Loss: 0.07679150253534317 Validation Loss: 0.749390184879303\n",
      "Epoch 6161: Training Loss: 0.076663372417291 Validation Loss: 0.7478023767471313\n",
      "Epoch 6162: Training Loss: 0.07669791827599208 Validation Loss: 0.7444896101951599\n",
      "Epoch 6163: Training Loss: 0.07677549123764038 Validation Loss: 0.7396834492683411\n",
      "Epoch 6164: Training Loss: 0.07662149270375569 Validation Loss: 0.7394187450408936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6165: Training Loss: 0.07675840457280476 Validation Loss: 0.7429822683334351\n",
      "Epoch 6166: Training Loss: 0.07657280564308167 Validation Loss: 0.7440637350082397\n",
      "Epoch 6167: Training Loss: 0.0765266294280688 Validation Loss: 0.7443069219589233\n",
      "Epoch 6168: Training Loss: 0.07659939924875896 Validation Loss: 0.7446803450584412\n",
      "Epoch 6169: Training Loss: 0.07662242154280345 Validation Loss: 0.7445063591003418\n",
      "Epoch 6170: Training Loss: 0.07658926397562027 Validation Loss: 0.7444111108779907\n",
      "Epoch 6171: Training Loss: 0.0765352373321851 Validation Loss: 0.7418392300605774\n",
      "Epoch 6172: Training Loss: 0.07648504773775737 Validation Loss: 0.741313636302948\n",
      "Epoch 6173: Training Loss: 0.07646458595991135 Validation Loss: 0.7429057955741882\n",
      "Epoch 6174: Training Loss: 0.07642767081658046 Validation Loss: 0.7444369792938232\n",
      "Epoch 6175: Training Loss: 0.07641846934954326 Validation Loss: 0.7449815273284912\n",
      "Epoch 6176: Training Loss: 0.07638910412788391 Validation Loss: 0.7454714179039001\n",
      "Epoch 6177: Training Loss: 0.07638461391131084 Validation Loss: 0.7459021806716919\n",
      "Epoch 6178: Training Loss: 0.07639342546463013 Validation Loss: 0.7462416887283325\n",
      "Epoch 6179: Training Loss: 0.07647040113806725 Validation Loss: 0.743498682975769\n",
      "Epoch 6180: Training Loss: 0.07660378515720367 Validation Loss: 0.7456517815589905\n",
      "Epoch 6181: Training Loss: 0.07643258571624756 Validation Loss: 0.7452477216720581\n",
      "Epoch 6182: Training Loss: 0.0764895925919215 Validation Loss: 0.7418135404586792\n",
      "Epoch 6183: Training Loss: 0.07652009775241216 Validation Loss: 0.7420157194137573\n",
      "Epoch 6184: Training Loss: 0.07659647862116496 Validation Loss: 0.7396868467330933\n",
      "Epoch 6185: Training Loss: 0.07647696385780971 Validation Loss: 0.744460940361023\n",
      "Epoch 6186: Training Loss: 0.07631107419729233 Validation Loss: 0.7448067665100098\n",
      "Epoch 6187: Training Loss: 0.0764049490292867 Validation Loss: 0.7438534498214722\n",
      "Epoch 6188: Training Loss: 0.07622078185280164 Validation Loss: 0.7452622056007385\n",
      "Epoch 6189: Training Loss: 0.07631990561882655 Validation Loss: 0.744415819644928\n",
      "Epoch 6190: Training Loss: 0.0762921820084254 Validation Loss: 0.7450743317604065\n",
      "Epoch 6191: Training Loss: 0.07622080544630687 Validation Loss: 0.74552983045578\n",
      "Epoch 6192: Training Loss: 0.07623684778809547 Validation Loss: 0.7462476491928101\n",
      "Epoch 6193: Training Loss: 0.07619974762201309 Validation Loss: 0.7465400695800781\n",
      "Epoch 6194: Training Loss: 0.07636929551760356 Validation Loss: 0.7430520057678223\n",
      "Epoch 6195: Training Loss: 0.07624990244706471 Validation Loss: 0.7435734868049622\n",
      "Epoch 6196: Training Loss: 0.07616502170761426 Validation Loss: 0.7443851232528687\n",
      "Epoch 6197: Training Loss: 0.07621471832195918 Validation Loss: 0.7444784045219421\n",
      "Epoch 6198: Training Loss: 0.07620010276635487 Validation Loss: 0.7445551753044128\n",
      "Epoch 6199: Training Loss: 0.07631595805287361 Validation Loss: 0.7415211796760559\n",
      "Epoch 6200: Training Loss: 0.07615012302994728 Validation Loss: 0.7410213351249695\n",
      "Epoch 6201: Training Loss: 0.07608909532427788 Validation Loss: 0.7432512640953064\n",
      "Epoch 6202: Training Loss: 0.07633272310098012 Validation Loss: 0.7477011680603027\n",
      "Epoch 6203: Training Loss: 0.07613563040892284 Validation Loss: 0.7459896802902222\n",
      "Epoch 6204: Training Loss: 0.07607367013891538 Validation Loss: 0.7466492056846619\n",
      "Epoch 6205: Training Loss: 0.07612149169047673 Validation Loss: 0.7458663582801819\n",
      "Epoch 6206: Training Loss: 0.07611187919974327 Validation Loss: 0.7446539998054504\n",
      "Epoch 6207: Training Loss: 0.0759580892821153 Validation Loss: 0.7424426674842834\n",
      "Epoch 6208: Training Loss: 0.07624830305576324 Validation Loss: 0.7392798662185669\n",
      "Epoch 6209: Training Loss: 0.07608837882677714 Validation Loss: 0.7399894595146179\n",
      "Epoch 6210: Training Loss: 0.07608772565921147 Validation Loss: 0.74116051197052\n",
      "Epoch 6211: Training Loss: 0.0759665531416734 Validation Loss: 0.743874192237854\n",
      "Epoch 6212: Training Loss: 0.07603764782349269 Validation Loss: 0.7473576068878174\n",
      "Epoch 6213: Training Loss: 0.07639155288537343 Validation Loss: 0.7507036328315735\n",
      "Epoch 6214: Training Loss: 0.07618409146865208 Validation Loss: 0.7489302158355713\n",
      "Epoch 6215: Training Loss: 0.07600556810696919 Validation Loss: 0.7470141649246216\n",
      "Epoch 6216: Training Loss: 0.07588276515404384 Validation Loss: 0.7412051558494568\n",
      "Epoch 6217: Training Loss: 0.07592158143719037 Validation Loss: 0.7387211322784424\n",
      "Epoch 6218: Training Loss: 0.07616171116630237 Validation Loss: 0.7413344383239746\n",
      "Epoch 6219: Training Loss: 0.07620587696631749 Validation Loss: 0.7392687201499939\n",
      "Epoch 6220: Training Loss: 0.07602325081825256 Validation Loss: 0.7438510060310364\n",
      "Epoch 6221: Training Loss: 0.07588377098242442 Validation Loss: 0.7478719353675842\n",
      "Epoch 6222: Training Loss: 0.07653501878182094 Validation Loss: 0.7524235248565674\n",
      "Epoch 6223: Training Loss: 0.07594770441452663 Validation Loss: 0.7486978769302368\n",
      "Epoch 6224: Training Loss: 0.07609166204929352 Validation Loss: 0.7423530220985413\n",
      "Epoch 6225: Training Loss: 0.075880811860164 Validation Loss: 0.7411915063858032\n",
      "Epoch 6226: Training Loss: 0.07584481686353683 Validation Loss: 0.7417830228805542\n",
      "Epoch 6227: Training Loss: 0.07589033991098404 Validation Loss: 0.7438492774963379\n",
      "Epoch 6228: Training Loss: 0.07593870411316554 Validation Loss: 0.7478215098381042\n",
      "Epoch 6229: Training Loss: 0.0757317120830218 Validation Loss: 0.7477440237998962\n",
      "Epoch 6230: Training Loss: 0.07601725558439891 Validation Loss: 0.7487949132919312\n",
      "Epoch 6231: Training Loss: 0.07578993091980617 Validation Loss: 0.7455245852470398\n",
      "Epoch 6232: Training Loss: 0.07563435286283493 Validation Loss: 0.7424921989440918\n",
      "Epoch 6233: Training Loss: 0.07595576345920563 Validation Loss: 0.7382957935333252\n",
      "Epoch 6234: Training Loss: 0.07576391597588857 Validation Loss: 0.74016934633255\n",
      "Epoch 6235: Training Loss: 0.07585559288660686 Validation Loss: 0.7450821399688721\n",
      "Epoch 6236: Training Loss: 0.07572477062543233 Validation Loss: 0.7442940473556519\n",
      "Epoch 6237: Training Loss: 0.07561337947845459 Validation Loss: 0.7458876967430115\n",
      "Epoch 6238: Training Loss: 0.07580936948458354 Validation Loss: 0.7438548803329468\n",
      "Epoch 6239: Training Loss: 0.07584195087353389 Validation Loss: 0.7478191256523132\n",
      "Epoch 6240: Training Loss: 0.07591596618294716 Validation Loss: 0.749219536781311\n",
      "Epoch 6241: Training Loss: 0.07577391465504964 Validation Loss: 0.7443830370903015\n",
      "Epoch 6242: Training Loss: 0.07556923230489095 Validation Loss: 0.7434384226799011\n",
      "Epoch 6243: Training Loss: 0.07555823773145676 Validation Loss: 0.7433545589447021\n",
      "Epoch 6244: Training Loss: 0.07555261005957921 Validation Loss: 0.7426469326019287\n",
      "Epoch 6245: Training Loss: 0.07618638873100281 Validation Loss: 0.7399073243141174\n",
      "Epoch 6246: Training Loss: 0.07562415301799774 Validation Loss: 0.7452555298805237\n",
      "Epoch 6247: Training Loss: 0.07555882384379704 Validation Loss: 0.7487415075302124\n",
      "Epoch 6248: Training Loss: 0.0756660948197047 Validation Loss: 0.7517145872116089\n",
      "Epoch 6249: Training Loss: 0.07572755714257558 Validation Loss: 0.7479010820388794\n",
      "Epoch 6250: Training Loss: 0.07561516016721725 Validation Loss: 0.7449039220809937\n",
      "Epoch 6251: Training Loss: 0.0758436347047488 Validation Loss: 0.7470173239707947\n",
      "Epoch 6252: Training Loss: 0.07540377726157506 Validation Loss: 0.7434090971946716\n",
      "Epoch 6253: Training Loss: 0.07542749245961507 Validation Loss: 0.7411034107208252\n",
      "Epoch 6254: Training Loss: 0.07543662066260974 Validation Loss: 0.7416168451309204\n",
      "Epoch 6255: Training Loss: 0.0754510685801506 Validation Loss: 0.7420493364334106\n",
      "Epoch 6256: Training Loss: 0.07539510726928711 Validation Loss: 0.7436338663101196\n",
      "Epoch 6257: Training Loss: 0.07535804311434428 Validation Loss: 0.7449014782905579\n",
      "Epoch 6258: Training Loss: 0.07537375514705975 Validation Loss: 0.7463297247886658\n",
      "Epoch 6259: Training Loss: 0.07552621016899745 Validation Loss: 0.7483773231506348\n",
      "Epoch 6260: Training Loss: 0.07546426355838776 Validation Loss: 0.7461040019989014\n",
      "Epoch 6261: Training Loss: 0.07528839260339737 Validation Loss: 0.7447793483734131\n",
      "Epoch 6262: Training Loss: 0.07532600189248721 Validation Loss: 0.7437275648117065\n",
      "Epoch 6263: Training Loss: 0.07529288654526074 Validation Loss: 0.743577241897583\n",
      "Epoch 6264: Training Loss: 0.07541773716608684 Validation Loss: 0.745002031326294\n",
      "Epoch 6265: Training Loss: 0.07571255415678024 Validation Loss: 0.7411421537399292\n",
      "Epoch 6266: Training Loss: 0.07527933393915494 Validation Loss: 0.7428717017173767\n",
      "Epoch 6267: Training Loss: 0.07521229982376099 Validation Loss: 0.7454054951667786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6268: Training Loss: 0.07519543170928955 Validation Loss: 0.7464355826377869\n",
      "Epoch 6269: Training Loss: 0.07534147923191388 Validation Loss: 0.7471917867660522\n",
      "Epoch 6270: Training Loss: 0.07548569142818451 Validation Loss: 0.7446295619010925\n",
      "Epoch 6271: Training Loss: 0.07520583768685658 Validation Loss: 0.7446807026863098\n",
      "Epoch 6272: Training Loss: 0.07515589147806168 Validation Loss: 0.7463062405586243\n",
      "Epoch 6273: Training Loss: 0.0751586544016997 Validation Loss: 0.7470949292182922\n",
      "Epoch 6274: Training Loss: 0.07514229913552602 Validation Loss: 0.7475012540817261\n",
      "Epoch 6275: Training Loss: 0.07518910616636276 Validation Loss: 0.7471712231636047\n",
      "Epoch 6276: Training Loss: 0.07515914862354596 Validation Loss: 0.7462705373764038\n",
      "Epoch 6277: Training Loss: 0.07515888661146164 Validation Loss: 0.7432693839073181\n",
      "Epoch 6278: Training Loss: 0.07513794551293056 Validation Loss: 0.7429297566413879\n",
      "Epoch 6279: Training Loss: 0.07516847799221675 Validation Loss: 0.7419621348381042\n",
      "Epoch 6280: Training Loss: 0.07538545628388722 Validation Loss: 0.7453052401542664\n",
      "Epoch 6281: Training Loss: 0.07518407702445984 Validation Loss: 0.7438456416130066\n",
      "Epoch 6282: Training Loss: 0.07513552407423656 Validation Loss: 0.7467271685600281\n",
      "Epoch 6283: Training Loss: 0.07533995931347211 Validation Loss: 0.7495575547218323\n",
      "Epoch 6284: Training Loss: 0.07526201258103053 Validation Loss: 0.7492548823356628\n",
      "Epoch 6285: Training Loss: 0.07529714703559875 Validation Loss: 0.7441679835319519\n",
      "Epoch 6286: Training Loss: 0.07502476871013641 Validation Loss: 0.7420740127563477\n",
      "Epoch 6287: Training Loss: 0.07530338565508525 Validation Loss: 0.7400079369544983\n",
      "Epoch 6288: Training Loss: 0.07517571747303009 Validation Loss: 0.7443046569824219\n",
      "Epoch 6289: Training Loss: 0.0751352459192276 Validation Loss: 0.7443209290504456\n",
      "Epoch 6290: Training Loss: 0.07540394862492879 Validation Loss: 0.7495861649513245\n",
      "Epoch 6291: Training Loss: 0.07543792327245076 Validation Loss: 0.7467064261436462\n",
      "Epoch 6292: Training Loss: 0.07495031381646793 Validation Loss: 0.7470096945762634\n",
      "Epoch 6293: Training Loss: 0.07504051923751831 Validation Loss: 0.7483272552490234\n",
      "Epoch 6294: Training Loss: 0.0749427005648613 Validation Loss: 0.7474964261054993\n",
      "Epoch 6295: Training Loss: 0.07497461636861165 Validation Loss: 0.7436000108718872\n",
      "Epoch 6296: Training Loss: 0.07505952318509419 Validation Loss: 0.7404068112373352\n",
      "Epoch 6297: Training Loss: 0.0749199849863847 Validation Loss: 0.7420064210891724\n",
      "Epoch 6298: Training Loss: 0.07498679930965106 Validation Loss: 0.742685854434967\n",
      "Epoch 6299: Training Loss: 0.07490221162637074 Validation Loss: 0.7473526000976562\n",
      "Epoch 6300: Training Loss: 0.07524362951517105 Validation Loss: 0.7516824007034302\n",
      "Epoch 6301: Training Loss: 0.07521999627351761 Validation Loss: 0.7467840909957886\n",
      "Epoch 6302: Training Loss: 0.07478062932689984 Validation Loss: 0.7456203103065491\n",
      "Epoch 6303: Training Loss: 0.07481662680705388 Validation Loss: 0.7454676032066345\n",
      "Epoch 6304: Training Loss: 0.0748243381579717 Validation Loss: 0.7442643642425537\n",
      "Epoch 6305: Training Loss: 0.07490777721007665 Validation Loss: 0.746597409248352\n",
      "Epoch 6306: Training Loss: 0.07488276064395905 Validation Loss: 0.745042085647583\n",
      "Epoch 6307: Training Loss: 0.0748854490617911 Validation Loss: 0.7474557161331177\n",
      "Epoch 6308: Training Loss: 0.07484643037120502 Validation Loss: 0.7482370734214783\n",
      "Epoch 6309: Training Loss: 0.07472837592164676 Validation Loss: 0.7467406988143921\n",
      "Epoch 6310: Training Loss: 0.07481720298528671 Validation Loss: 0.7425658702850342\n",
      "Epoch 6311: Training Loss: 0.07480453948179881 Validation Loss: 0.7421466708183289\n",
      "Epoch 6312: Training Loss: 0.074699101348718 Validation Loss: 0.7429560422897339\n",
      "Epoch 6313: Training Loss: 0.07471088816722234 Validation Loss: 0.7445214986801147\n",
      "Epoch 6314: Training Loss: 0.07488903651634853 Validation Loss: 0.7431907653808594\n",
      "Epoch 6315: Training Loss: 0.07465249796708424 Validation Loss: 0.746624767780304\n",
      "Epoch 6316: Training Loss: 0.07473467538754146 Validation Loss: 0.7495691776275635\n",
      "Epoch 6317: Training Loss: 0.07469133163491885 Validation Loss: 0.7484356760978699\n",
      "Epoch 6318: Training Loss: 0.07467957337697347 Validation Loss: 0.7464046478271484\n",
      "Epoch 6319: Training Loss: 0.07457720736662547 Validation Loss: 0.745229184627533\n",
      "Epoch 6320: Training Loss: 0.0747124378879865 Validation Loss: 0.7438477873802185\n",
      "Epoch 6321: Training Loss: 0.0747700606783231 Validation Loss: 0.7431914806365967\n",
      "Epoch 6322: Training Loss: 0.07456051309903462 Validation Loss: 0.7451149225234985\n",
      "Epoch 6323: Training Loss: 0.07457295556863149 Validation Loss: 0.7478269934654236\n",
      "Epoch 6324: Training Loss: 0.0746612586081028 Validation Loss: 0.7493233680725098\n",
      "Epoch 6325: Training Loss: 0.07467483729124069 Validation Loss: 0.7483505606651306\n",
      "Epoch 6326: Training Loss: 0.0745781088868777 Validation Loss: 0.745598316192627\n",
      "Epoch 6327: Training Loss: 0.0745169868071874 Validation Loss: 0.743871808052063\n",
      "Epoch 6328: Training Loss: 0.07455438002943993 Validation Loss: 0.7423536777496338\n",
      "Epoch 6329: Training Loss: 0.0745050956805547 Validation Loss: 0.7441694736480713\n",
      "Epoch 6330: Training Loss: 0.07448352873325348 Validation Loss: 0.7465328574180603\n",
      "Epoch 6331: Training Loss: 0.07460970555742581 Validation Loss: 0.7448201179504395\n",
      "Epoch 6332: Training Loss: 0.07468839238087337 Validation Loss: 0.7491689324378967\n",
      "Epoch 6333: Training Loss: 0.07445801546176274 Validation Loss: 0.748624861240387\n",
      "Epoch 6334: Training Loss: 0.07441932459672292 Validation Loss: 0.747079610824585\n",
      "Epoch 6335: Training Loss: 0.07443127532800038 Validation Loss: 0.7468839287757874\n",
      "Epoch 6336: Training Loss: 0.07446777572234471 Validation Loss: 0.7473642826080322\n",
      "Epoch 6337: Training Loss: 0.07441682368516922 Validation Loss: 0.7453893423080444\n",
      "Epoch 6338: Training Loss: 0.07435828695694606 Validation Loss: 0.7447426915168762\n",
      "Epoch 6339: Training Loss: 0.07454292724529903 Validation Loss: 0.7421503663063049\n",
      "Epoch 6340: Training Loss: 0.07452698300282161 Validation Loss: 0.745079755783081\n",
      "Epoch 6341: Training Loss: 0.0743275818725427 Validation Loss: 0.7460866570472717\n",
      "Epoch 6342: Training Loss: 0.07448573037981987 Validation Loss: 0.7443896532058716\n",
      "Epoch 6343: Training Loss: 0.07433271408081055 Validation Loss: 0.7462893724441528\n",
      "Epoch 6344: Training Loss: 0.07431566591064136 Validation Loss: 0.7462034821510315\n",
      "Epoch 6345: Training Loss: 0.07446455582976341 Validation Loss: 0.7497137188911438\n",
      "Epoch 6346: Training Loss: 0.07431413481632869 Validation Loss: 0.7491315603256226\n",
      "Epoch 6347: Training Loss: 0.07428773244222005 Validation Loss: 0.7475062012672424\n",
      "Epoch 6348: Training Loss: 0.07479410991072655 Validation Loss: 0.7419310808181763\n",
      "Epoch 6349: Training Loss: 0.0744211549560229 Validation Loss: 0.7445499897003174\n",
      "Epoch 6350: Training Loss: 0.07448426634073257 Validation Loss: 0.7439566850662231\n",
      "Epoch 6351: Training Loss: 0.07424440855781238 Validation Loss: 0.7463605403900146\n",
      "Epoch 6352: Training Loss: 0.07424728572368622 Validation Loss: 0.7463057637214661\n",
      "Epoch 6353: Training Loss: 0.07441202799479167 Validation Loss: 0.7498185634613037\n",
      "Epoch 6354: Training Loss: 0.07420560469230016 Validation Loss: 0.7481213212013245\n",
      "Epoch 6355: Training Loss: 0.07421950995922089 Validation Loss: 0.744766116142273\n",
      "Epoch 6356: Training Loss: 0.0741902490456899 Validation Loss: 0.745907187461853\n",
      "Epoch 6357: Training Loss: 0.07416025673349698 Validation Loss: 0.7467836737632751\n",
      "Epoch 6358: Training Loss: 0.0741780623793602 Validation Loss: 0.7464393973350525\n",
      "Epoch 6359: Training Loss: 0.07436206688483556 Validation Loss: 0.7431380152702332\n",
      "Epoch 6360: Training Loss: 0.07413490861654282 Validation Loss: 0.7446671724319458\n",
      "Epoch 6361: Training Loss: 0.07406743491689365 Validation Loss: 0.7456254363059998\n",
      "Epoch 6362: Training Loss: 0.07417479902505875 Validation Loss: 0.7474035024642944\n",
      "Epoch 6363: Training Loss: 0.07413890212774277 Validation Loss: 0.7477057576179504\n",
      "Epoch 6364: Training Loss: 0.07410941521326701 Validation Loss: 0.7457600235939026\n",
      "Epoch 6365: Training Loss: 0.07407705734173457 Validation Loss: 0.7466022372245789\n",
      "Epoch 6366: Training Loss: 0.074173870186011 Validation Loss: 0.7432698607444763\n",
      "Epoch 6367: Training Loss: 0.07403548061847687 Validation Loss: 0.7435563802719116\n",
      "Epoch 6368: Training Loss: 0.07436985770861308 Validation Loss: 0.7478122711181641\n",
      "Epoch 6369: Training Loss: 0.07404190550247829 Validation Loss: 0.7463931441307068\n",
      "Epoch 6370: Training Loss: 0.07407065232594807 Validation Loss: 0.7451817989349365\n",
      "Epoch 6371: Training Loss: 0.07404133180777232 Validation Loss: 0.7444168925285339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6372: Training Loss: 0.07422778010368347 Validation Loss: 0.748117208480835\n",
      "Epoch 6373: Training Loss: 0.07397431135177612 Validation Loss: 0.7488989233970642\n",
      "Epoch 6374: Training Loss: 0.07394818216562271 Validation Loss: 0.7474402189254761\n",
      "Epoch 6375: Training Loss: 0.07416630908846855 Validation Loss: 0.7439993619918823\n",
      "Epoch 6376: Training Loss: 0.07398255417744319 Validation Loss: 0.7458558082580566\n",
      "Epoch 6377: Training Loss: 0.07397112498680751 Validation Loss: 0.747412919998169\n",
      "Epoch 6378: Training Loss: 0.07388486216465633 Validation Loss: 0.7473865747451782\n",
      "Epoch 6379: Training Loss: 0.0738750696182251 Validation Loss: 0.7470374703407288\n",
      "Epoch 6380: Training Loss: 0.0738766019543012 Validation Loss: 0.7466210126876831\n",
      "Epoch 6381: Training Loss: 0.0740225041906039 Validation Loss: 0.744163453578949\n",
      "Epoch 6382: Training Loss: 0.07384427388509114 Validation Loss: 0.7444934248924255\n",
      "Epoch 6383: Training Loss: 0.07383203878998756 Validation Loss: 0.7464864253997803\n",
      "Epoch 6384: Training Loss: 0.07400539020697276 Validation Loss: 0.7499541640281677\n",
      "Epoch 6385: Training Loss: 0.07387310514847438 Validation Loss: 0.7475318312644958\n",
      "Epoch 6386: Training Loss: 0.07388809074958165 Validation Loss: 0.7454501986503601\n",
      "Epoch 6387: Training Loss: 0.07383723805348079 Validation Loss: 0.7462112903594971\n",
      "Epoch 6388: Training Loss: 0.07389893010258675 Validation Loss: 0.744627058506012\n",
      "Epoch 6389: Training Loss: 0.07377883791923523 Validation Loss: 0.7473275065422058\n",
      "Epoch 6390: Training Loss: 0.07395015408595403 Validation Loss: 0.7502336502075195\n",
      "Epoch 6391: Training Loss: 0.0738565797607104 Validation Loss: 0.7481536865234375\n",
      "Epoch 6392: Training Loss: 0.07386589422821999 Validation Loss: 0.7441259622573853\n",
      "Epoch 6393: Training Loss: 0.07375663270552953 Validation Loss: 0.7445974946022034\n",
      "Epoch 6394: Training Loss: 0.07374728098511696 Validation Loss: 0.7438350319862366\n",
      "Epoch 6395: Training Loss: 0.07366791119178136 Validation Loss: 0.745419442653656\n",
      "Epoch 6396: Training Loss: 0.07372475415468216 Validation Loss: 0.747856855392456\n",
      "Epoch 6397: Training Loss: 0.07362396270036697 Validation Loss: 0.7477972507476807\n",
      "Epoch 6398: Training Loss: 0.0737095723549525 Validation Loss: 0.7476498484611511\n",
      "Epoch 6399: Training Loss: 0.07365946471691132 Validation Loss: 0.7457593679428101\n",
      "Epoch 6400: Training Loss: 0.07362781713406245 Validation Loss: 0.7465434670448303\n",
      "Epoch 6401: Training Loss: 0.07371554523706436 Validation Loss: 0.7479769587516785\n",
      "Epoch 6402: Training Loss: 0.0736145203312238 Validation Loss: 0.7465742826461792\n",
      "Epoch 6403: Training Loss: 0.07355836406350136 Validation Loss: 0.7461560368537903\n",
      "Epoch 6404: Training Loss: 0.07362572103738785 Validation Loss: 0.7445812821388245\n",
      "Epoch 6405: Training Loss: 0.07361394415299098 Validation Loss: 0.7460213303565979\n",
      "Epoch 6406: Training Loss: 0.07361674557129542 Validation Loss: 0.7454420924186707\n",
      "Epoch 6407: Training Loss: 0.07354933395981789 Validation Loss: 0.7467641830444336\n",
      "Epoch 6408: Training Loss: 0.07351579641302426 Validation Loss: 0.7468442320823669\n",
      "Epoch 6409: Training Loss: 0.07370549440383911 Validation Loss: 0.7461476922035217\n",
      "Epoch 6410: Training Loss: 0.07362680633862813 Validation Loss: 0.7475956678390503\n",
      "Epoch 6411: Training Loss: 0.07376110057036082 Validation Loss: 0.7444268465042114\n",
      "Epoch 6412: Training Loss: 0.07415090749661128 Validation Loss: 0.7495012879371643\n",
      "Epoch 6413: Training Loss: 0.07370660702387492 Validation Loss: 0.7453606128692627\n",
      "Epoch 6414: Training Loss: 0.07350751012563705 Validation Loss: 0.745389997959137\n",
      "Epoch 6415: Training Loss: 0.07346219817797343 Validation Loss: 0.7466035485267639\n",
      "Epoch 6416: Training Loss: 0.07342507317662239 Validation Loss: 0.7465498447418213\n",
      "Epoch 6417: Training Loss: 0.07340895508726437 Validation Loss: 0.7476486563682556\n",
      "Epoch 6418: Training Loss: 0.07339879373709361 Validation Loss: 0.746800422668457\n",
      "Epoch 6419: Training Loss: 0.07341383397579193 Validation Loss: 0.7467676997184753\n",
      "Epoch 6420: Training Loss: 0.0733965386946996 Validation Loss: 0.7463244795799255\n",
      "Epoch 6421: Training Loss: 0.07344702631235123 Validation Loss: 0.7484523057937622\n",
      "Epoch 6422: Training Loss: 0.07338203365604083 Validation Loss: 0.7463529706001282\n",
      "Epoch 6423: Training Loss: 0.07345614582300186 Validation Loss: 0.7451955080032349\n",
      "Epoch 6424: Training Loss: 0.07335313657919566 Validation Loss: 0.7455832958221436\n",
      "Epoch 6425: Training Loss: 0.07335727661848068 Validation Loss: 0.7476979494094849\n",
      "Epoch 6426: Training Loss: 0.07330248256524403 Validation Loss: 0.7474451661109924\n",
      "Epoch 6427: Training Loss: 0.07333014657100041 Validation Loss: 0.7470758557319641\n",
      "Epoch 6428: Training Loss: 0.07337145879864693 Validation Loss: 0.7455547451972961\n",
      "Epoch 6429: Training Loss: 0.07330579310655594 Validation Loss: 0.7470842003822327\n",
      "Epoch 6430: Training Loss: 0.07327597339948018 Validation Loss: 0.7479475140571594\n",
      "Epoch 6431: Training Loss: 0.073499479641517 Validation Loss: 0.7451921105384827\n",
      "Epoch 6432: Training Loss: 0.0734909897049268 Validation Loss: 0.7438384890556335\n",
      "Epoch 6433: Training Loss: 0.07320014387369156 Validation Loss: 0.7460872530937195\n",
      "Epoch 6434: Training Loss: 0.07339626178145409 Validation Loss: 0.7515565752983093\n",
      "Epoch 6435: Training Loss: 0.0732860838373502 Validation Loss: 0.7528574466705322\n",
      "Epoch 6436: Training Loss: 0.07357415060202281 Validation Loss: 0.748059868812561\n",
      "Epoch 6437: Training Loss: 0.07325510183970134 Validation Loss: 0.7455949187278748\n",
      "Epoch 6438: Training Loss: 0.07317918787399928 Validation Loss: 0.7450181841850281\n",
      "Epoch 6439: Training Loss: 0.07327383011579514 Validation Loss: 0.7444123029708862\n",
      "Epoch 6440: Training Loss: 0.07320440808931987 Validation Loss: 0.7443844079971313\n",
      "Epoch 6441: Training Loss: 0.07311441873510678 Validation Loss: 0.7473317980766296\n",
      "Epoch 6442: Training Loss: 0.07330063730478287 Validation Loss: 0.7509543895721436\n",
      "Epoch 6443: Training Loss: 0.0731742928425471 Validation Loss: 0.7491515278816223\n",
      "Epoch 6444: Training Loss: 0.07329459985097249 Validation Loss: 0.7488846182823181\n",
      "Epoch 6445: Training Loss: 0.07311676690975825 Validation Loss: 0.7458693981170654\n",
      "Epoch 6446: Training Loss: 0.07307139287392299 Validation Loss: 0.7454098463058472\n",
      "Epoch 6447: Training Loss: 0.07314817855755489 Validation Loss: 0.7462319731712341\n",
      "Epoch 6448: Training Loss: 0.07312731941541036 Validation Loss: 0.7452804446220398\n",
      "Epoch 6449: Training Loss: 0.07356144487857819 Validation Loss: 0.742965042591095\n",
      "Epoch 6450: Training Loss: 0.07306985432902972 Validation Loss: 0.7470535039901733\n",
      "Epoch 6451: Training Loss: 0.072988527516524 Validation Loss: 0.7497008442878723\n",
      "Epoch 6452: Training Loss: 0.07317199433843295 Validation Loss: 0.7517921924591064\n",
      "Epoch 6453: Training Loss: 0.0732934648791949 Validation Loss: 0.7529729008674622\n",
      "Epoch 6454: Training Loss: 0.07310597101847331 Validation Loss: 0.7491424083709717\n",
      "Epoch 6455: Training Loss: 0.072958139081796 Validation Loss: 0.7471346855163574\n",
      "Epoch 6456: Training Loss: 0.07303960373004277 Validation Loss: 0.7436292767524719\n",
      "Epoch 6457: Training Loss: 0.07303618391354878 Validation Loss: 0.7427765130996704\n",
      "Epoch 6458: Training Loss: 0.07309961567322414 Validation Loss: 0.745437502861023\n",
      "Epoch 6459: Training Loss: 0.07296668986479442 Validation Loss: 0.7455967664718628\n",
      "Epoch 6460: Training Loss: 0.07292432337999344 Validation Loss: 0.7475720047950745\n",
      "Epoch 6461: Training Loss: 0.07305784026781718 Validation Loss: 0.7509294748306274\n",
      "Epoch 6462: Training Loss: 0.07310609271128972 Validation Loss: 0.7525137662887573\n",
      "Epoch 6463: Training Loss: 0.07330184678236644 Validation Loss: 0.7462605237960815\n",
      "Epoch 6464: Training Loss: 0.07286630819241206 Validation Loss: 0.7441984415054321\n",
      "Epoch 6465: Training Loss: 0.07291248440742493 Validation Loss: 0.7432245016098022\n",
      "Epoch 6466: Training Loss: 0.07313955202698708 Validation Loss: 0.7420305013656616\n",
      "Epoch 6467: Training Loss: 0.07286024590333302 Validation Loss: 0.7472051978111267\n",
      "Epoch 6468: Training Loss: 0.07284880181153615 Validation Loss: 0.7506250739097595\n",
      "Epoch 6469: Training Loss: 0.0729424109061559 Validation Loss: 0.75239497423172\n",
      "Epoch 6470: Training Loss: 0.07284635429581006 Validation Loss: 0.7506941556930542\n",
      "Epoch 6471: Training Loss: 0.0728920226295789 Validation Loss: 0.7492409348487854\n",
      "Epoch 6472: Training Loss: 0.07304211209217708 Validation Loss: 0.744509220123291\n",
      "Epoch 6473: Training Loss: 0.07282361139853795 Validation Loss: 0.7450104355812073\n",
      "Epoch 6474: Training Loss: 0.07301084697246552 Validation Loss: 0.7428719401359558\n",
      "Epoch 6475: Training Loss: 0.07284632325172424 Validation Loss: 0.7444199323654175\n",
      "Epoch 6476: Training Loss: 0.07276520878076553 Validation Loss: 0.7488955855369568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6477: Training Loss: 0.07270918413996696 Validation Loss: 0.7507051229476929\n",
      "Epoch 6478: Training Loss: 0.07342010363936424 Validation Loss: 0.7465825080871582\n",
      "Epoch 6479: Training Loss: 0.07279859979947408 Validation Loss: 0.7467972636222839\n",
      "Epoch 6480: Training Loss: 0.07266343384981155 Validation Loss: 0.7488552331924438\n",
      "Epoch 6481: Training Loss: 0.07270630200703938 Validation Loss: 0.749789834022522\n",
      "Epoch 6482: Training Loss: 0.07265103856722514 Validation Loss: 0.7511351108551025\n",
      "Epoch 6483: Training Loss: 0.07279174154003461 Validation Loss: 0.7522468566894531\n",
      "Epoch 6484: Training Loss: 0.07299705843130748 Validation Loss: 0.7474839091300964\n",
      "Epoch 6485: Training Loss: 0.07274657860398293 Validation Loss: 0.745681881904602\n",
      "Epoch 6486: Training Loss: 0.07264776279528935 Validation Loss: 0.7467542290687561\n",
      "Epoch 6487: Training Loss: 0.07260556022326152 Validation Loss: 0.7466353178024292\n",
      "Epoch 6488: Training Loss: 0.07269429787993431 Validation Loss: 0.7483603954315186\n",
      "Epoch 6489: Training Loss: 0.07263383269309998 Validation Loss: 0.74836665391922\n",
      "Epoch 6490: Training Loss: 0.07258519530296326 Validation Loss: 0.748790442943573\n",
      "Epoch 6491: Training Loss: 0.07290715724229813 Validation Loss: 0.7451090812683105\n",
      "Epoch 6492: Training Loss: 0.07257008800903957 Validation Loss: 0.7458637952804565\n",
      "Epoch 6493: Training Loss: 0.0727174108227094 Validation Loss: 0.7494142651557922\n",
      "Epoch 6494: Training Loss: 0.07251005868117015 Validation Loss: 0.7491446137428284\n",
      "Epoch 6495: Training Loss: 0.07274446139732997 Validation Loss: 0.7457996606826782\n",
      "Epoch 6496: Training Loss: 0.07275411486625671 Validation Loss: 0.7446978092193604\n",
      "Epoch 6497: Training Loss: 0.0727840115626653 Validation Loss: 0.7499869465827942\n",
      "Epoch 6498: Training Loss: 0.07252682248751323 Validation Loss: 0.7504974603652954\n",
      "Epoch 6499: Training Loss: 0.07259479040900867 Validation Loss: 0.7506353855133057\n",
      "Epoch 6500: Training Loss: 0.07273887346188228 Validation Loss: 0.7466608881950378\n",
      "Epoch 6501: Training Loss: 0.07245015601317088 Validation Loss: 0.7458859086036682\n",
      "Epoch 6502: Training Loss: 0.07258562743663788 Validation Loss: 0.7485952973365784\n",
      "Epoch 6503: Training Loss: 0.07251777375737826 Validation Loss: 0.7466133236885071\n",
      "Epoch 6504: Training Loss: 0.07246081779400508 Validation Loss: 0.7463048696517944\n",
      "Epoch 6505: Training Loss: 0.07259480282664299 Validation Loss: 0.7495948672294617\n",
      "Epoch 6506: Training Loss: 0.07248338560263316 Validation Loss: 0.750795304775238\n",
      "Epoch 6507: Training Loss: 0.07241456458965938 Validation Loss: 0.7489319443702698\n",
      "Epoch 6508: Training Loss: 0.07256849606831868 Validation Loss: 0.7450752258300781\n",
      "Epoch 6509: Training Loss: 0.07235697408517201 Validation Loss: 0.7450825572013855\n",
      "Epoch 6510: Training Loss: 0.07243077456951141 Validation Loss: 0.7445863485336304\n",
      "Epoch 6511: Training Loss: 0.07258628308773041 Validation Loss: 0.7486653923988342\n",
      "Epoch 6512: Training Loss: 0.07253604630629222 Validation Loss: 0.7461128830909729\n",
      "Epoch 6513: Training Loss: 0.07231362164020538 Validation Loss: 0.7467628121376038\n",
      "Epoch 6514: Training Loss: 0.07226195062200229 Validation Loss: 0.7486361861228943\n",
      "Epoch 6515: Training Loss: 0.07234754165013631 Validation Loss: 0.7499539852142334\n",
      "Epoch 6516: Training Loss: 0.07230504353841145 Validation Loss: 0.7501190900802612\n",
      "Epoch 6517: Training Loss: 0.07227213432391484 Validation Loss: 0.7500792741775513\n",
      "Epoch 6518: Training Loss: 0.07224242140849431 Validation Loss: 0.7484334707260132\n",
      "Epoch 6519: Training Loss: 0.07233534504969914 Validation Loss: 0.7475100159645081\n",
      "Epoch 6520: Training Loss: 0.0722648911178112 Validation Loss: 0.7458692193031311\n",
      "Epoch 6521: Training Loss: 0.07246129711469014 Validation Loss: 0.7481999397277832\n",
      "Epoch 6522: Training Loss: 0.07221705342332523 Validation Loss: 0.7475597858428955\n",
      "Epoch 6523: Training Loss: 0.0722779209415118 Validation Loss: 0.748689591884613\n",
      "Epoch 6524: Training Loss: 0.0722581719358762 Validation Loss: 0.7463391423225403\n",
      "Epoch 6525: Training Loss: 0.07247641682624817 Validation Loss: 0.7439978718757629\n",
      "Epoch 6526: Training Loss: 0.07225821912288666 Validation Loss: 0.7473181486129761\n",
      "Epoch 6527: Training Loss: 0.07220818847417831 Validation Loss: 0.7489598989486694\n",
      "Epoch 6528: Training Loss: 0.07214036459724109 Validation Loss: 0.7488045692443848\n",
      "Epoch 6529: Training Loss: 0.07217888906598091 Validation Loss: 0.7495990991592407\n",
      "Epoch 6530: Training Loss: 0.07213809341192245 Validation Loss: 0.7500810027122498\n",
      "Epoch 6531: Training Loss: 0.07215698932607968 Validation Loss: 0.7491649985313416\n",
      "Epoch 6532: Training Loss: 0.07213257004817326 Validation Loss: 0.7480468153953552\n",
      "Epoch 6533: Training Loss: 0.07220552116632462 Validation Loss: 0.7493686676025391\n",
      "Epoch 6534: Training Loss: 0.07208644350369771 Validation Loss: 0.7476258873939514\n",
      "Epoch 6535: Training Loss: 0.07227634141842525 Validation Loss: 0.7455355525016785\n",
      "Epoch 6536: Training Loss: 0.07216047743956248 Validation Loss: 0.7451339960098267\n",
      "Epoch 6537: Training Loss: 0.07210995505253474 Validation Loss: 0.748482346534729\n",
      "Epoch 6538: Training Loss: 0.07212628424167633 Validation Loss: 0.7511695623397827\n",
      "Epoch 6539: Training Loss: 0.07219551503658295 Validation Loss: 0.7496370077133179\n",
      "Epoch 6540: Training Loss: 0.07209572692712148 Validation Loss: 0.74835205078125\n",
      "Epoch 6541: Training Loss: 0.07202090074618657 Validation Loss: 0.7499946355819702\n",
      "Epoch 6542: Training Loss: 0.07211324075857799 Validation Loss: 0.7489343285560608\n",
      "Epoch 6543: Training Loss: 0.07200155531366666 Validation Loss: 0.748603105545044\n",
      "Epoch 6544: Training Loss: 0.07209984833995502 Validation Loss: 0.7465906143188477\n",
      "Epoch 6545: Training Loss: 0.07200805718700092 Validation Loss: 0.7480896711349487\n",
      "Epoch 6546: Training Loss: 0.07193133980035782 Validation Loss: 0.7474391460418701\n",
      "Epoch 6547: Training Loss: 0.07200177510579427 Validation Loss: 0.747050940990448\n",
      "Epoch 6548: Training Loss: 0.07190634682774544 Validation Loss: 0.7481242418289185\n",
      "Epoch 6549: Training Loss: 0.07192835584282875 Validation Loss: 0.7495353817939758\n",
      "Epoch 6550: Training Loss: 0.07199454059203465 Validation Loss: 0.7491385340690613\n",
      "Epoch 6551: Training Loss: 0.07190313438574474 Validation Loss: 0.7483212947845459\n",
      "Epoch 6552: Training Loss: 0.07184851293762524 Validation Loss: 0.7493912577629089\n",
      "Epoch 6553: Training Loss: 0.07197081918517749 Validation Loss: 0.7502325773239136\n",
      "Epoch 6554: Training Loss: 0.07191263387600581 Validation Loss: 0.7505456805229187\n",
      "Epoch 6555: Training Loss: 0.07197121530771255 Validation Loss: 0.7501356601715088\n",
      "Epoch 6556: Training Loss: 0.07186105350653331 Validation Loss: 0.746898889541626\n",
      "Epoch 6557: Training Loss: 0.07202654207746188 Validation Loss: 0.7435808777809143\n",
      "Epoch 6558: Training Loss: 0.07188993568221728 Validation Loss: 0.7453417181968689\n",
      "Epoch 6559: Training Loss: 0.07189757873614629 Validation Loss: 0.7454016804695129\n",
      "Epoch 6560: Training Loss: 0.0718195028603077 Validation Loss: 0.7471707463264465\n",
      "Epoch 6561: Training Loss: 0.07178391267855962 Validation Loss: 0.7506904006004333\n",
      "Epoch 6562: Training Loss: 0.07191815475622813 Validation Loss: 0.7498249411582947\n",
      "Epoch 6563: Training Loss: 0.0718058670560519 Validation Loss: 0.7510314583778381\n",
      "Epoch 6564: Training Loss: 0.07178277149796486 Validation Loss: 0.7499987483024597\n",
      "Epoch 6565: Training Loss: 0.07179144149025281 Validation Loss: 0.7482920289039612\n",
      "Epoch 6566: Training Loss: 0.07182955493529637 Validation Loss: 0.7465141415596008\n",
      "Epoch 6567: Training Loss: 0.07175576562682788 Validation Loss: 0.7482418417930603\n",
      "Epoch 6568: Training Loss: 0.07175756618380547 Validation Loss: 0.7483245134353638\n",
      "Epoch 6569: Training Loss: 0.07173457493384679 Validation Loss: 0.7477970719337463\n",
      "Epoch 6570: Training Loss: 0.07197099054853122 Validation Loss: 0.7463350296020508\n",
      "Epoch 6571: Training Loss: 0.07177592193086942 Validation Loss: 0.7508971691131592\n",
      "Epoch 6572: Training Loss: 0.07187525928020477 Validation Loss: 0.7521510720252991\n",
      "Epoch 6573: Training Loss: 0.07172359277804692 Validation Loss: 0.75096595287323\n",
      "Epoch 6574: Training Loss: 0.07181933770577113 Validation Loss: 0.7518190145492554\n",
      "Epoch 6575: Training Loss: 0.07163251067201297 Validation Loss: 0.7506252527236938\n",
      "Epoch 6576: Training Loss: 0.07175266742706299 Validation Loss: 0.7454018592834473\n",
      "Epoch 6577: Training Loss: 0.07200352350870769 Validation Loss: 0.7419520616531372\n",
      "Epoch 6578: Training Loss: 0.07183696577946345 Validation Loss: 0.7429888844490051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6579: Training Loss: 0.071558378636837 Validation Loss: 0.746341347694397\n",
      "Epoch 6580: Training Loss: 0.07155034070213635 Validation Loss: 0.750673770904541\n",
      "Epoch 6581: Training Loss: 0.07153243819872539 Validation Loss: 0.7525548934936523\n",
      "Epoch 6582: Training Loss: 0.0717159869770209 Validation Loss: 0.7521718144416809\n",
      "Epoch 6583: Training Loss: 0.07204647113879521 Validation Loss: 0.748325526714325\n",
      "Epoch 6584: Training Loss: 0.07154352466265361 Validation Loss: 0.7489923238754272\n",
      "Epoch 6585: Training Loss: 0.07196387276053429 Validation Loss: 0.7536832094192505\n",
      "Epoch 6586: Training Loss: 0.07154539227485657 Validation Loss: 0.7520371079444885\n",
      "Epoch 6587: Training Loss: 0.0717486006518205 Validation Loss: 0.7473642230033875\n",
      "Epoch 6588: Training Loss: 0.07157171765963237 Validation Loss: 0.7463226914405823\n",
      "Epoch 6589: Training Loss: 0.07152554392814636 Validation Loss: 0.7472858428955078\n",
      "Epoch 6590: Training Loss: 0.07154325892527898 Validation Loss: 0.7496805787086487\n",
      "Epoch 6591: Training Loss: 0.0714664397140344 Validation Loss: 0.7504734992980957\n",
      "Epoch 6592: Training Loss: 0.07145674029986064 Validation Loss: 0.7507254481315613\n",
      "Epoch 6593: Training Loss: 0.07150850196679433 Validation Loss: 0.7473723888397217\n",
      "Epoch 6594: Training Loss: 0.07175257056951523 Validation Loss: 0.7501278519630432\n",
      "Epoch 6595: Training Loss: 0.07143326848745346 Validation Loss: 0.7479599118232727\n",
      "Epoch 6596: Training Loss: 0.07154290874799092 Validation Loss: 0.7448658347129822\n",
      "Epoch 6597: Training Loss: 0.07167188574870427 Validation Loss: 0.7439299821853638\n",
      "Epoch 6598: Training Loss: 0.07144563893477122 Validation Loss: 0.7483594417572021\n",
      "Epoch 6599: Training Loss: 0.07134801646073659 Validation Loss: 0.7498694062232971\n",
      "Epoch 6600: Training Loss: 0.07147850841283798 Validation Loss: 0.7504667639732361\n",
      "Epoch 6601: Training Loss: 0.07132527977228165 Validation Loss: 0.7513948678970337\n",
      "Epoch 6602: Training Loss: 0.07132840280731519 Validation Loss: 0.7519909739494324\n",
      "Epoch 6603: Training Loss: 0.07133640100558598 Validation Loss: 0.7512449026107788\n",
      "Epoch 6604: Training Loss: 0.07132819915811221 Validation Loss: 0.7509162425994873\n",
      "Epoch 6605: Training Loss: 0.07133456567923228 Validation Loss: 0.749866783618927\n",
      "Epoch 6606: Training Loss: 0.07134050627549489 Validation Loss: 0.7466567754745483\n",
      "Epoch 6607: Training Loss: 0.07131507868568103 Validation Loss: 0.7473656535148621\n",
      "Epoch 6608: Training Loss: 0.07145513594150543 Validation Loss: 0.7448045611381531\n",
      "Epoch 6609: Training Loss: 0.07135116805632909 Validation Loss: 0.7453749775886536\n",
      "Epoch 6610: Training Loss: 0.07133918007214864 Validation Loss: 0.7465663552284241\n",
      "Epoch 6611: Training Loss: 0.07121811558802922 Validation Loss: 0.7485669851303101\n",
      "Epoch 6612: Training Loss: 0.07128871232271194 Validation Loss: 0.7517063021659851\n",
      "Epoch 6613: Training Loss: 0.07146729280551274 Validation Loss: 0.7494338154792786\n",
      "Epoch 6614: Training Loss: 0.07120138530929883 Validation Loss: 0.7510043382644653\n",
      "Epoch 6615: Training Loss: 0.07143071914712588 Validation Loss: 0.7522333860397339\n",
      "Epoch 6616: Training Loss: 0.0713404081761837 Validation Loss: 0.7538016438484192\n",
      "Epoch 6617: Training Loss: 0.07123640427986781 Validation Loss: 0.7499675154685974\n",
      "Epoch 6618: Training Loss: 0.07122230157256126 Validation Loss: 0.7479875087738037\n",
      "Epoch 6619: Training Loss: 0.07114181915918986 Validation Loss: 0.7476428747177124\n",
      "Epoch 6620: Training Loss: 0.07116966197888057 Validation Loss: 0.7484695315361023\n",
      "Epoch 6621: Training Loss: 0.07112825041015942 Validation Loss: 0.7477279901504517\n",
      "Epoch 6622: Training Loss: 0.07122533768415451 Validation Loss: 0.7495257258415222\n",
      "Epoch 6623: Training Loss: 0.07106292247772217 Validation Loss: 0.7496501803398132\n",
      "Epoch 6624: Training Loss: 0.07122972359259923 Validation Loss: 0.7520059943199158\n",
      "Epoch 6625: Training Loss: 0.07121104498704274 Validation Loss: 0.7523419260978699\n",
      "Epoch 6626: Training Loss: 0.07147278388341267 Validation Loss: 0.7465207576751709\n",
      "Epoch 6627: Training Loss: 0.07117752730846405 Validation Loss: 0.7452874183654785\n",
      "Epoch 6628: Training Loss: 0.07121225198109944 Validation Loss: 0.7477337718009949\n",
      "Epoch 6629: Training Loss: 0.071049469212691 Validation Loss: 0.7486858367919922\n",
      "Epoch 6630: Training Loss: 0.0710478350520134 Validation Loss: 0.7493981719017029\n",
      "Epoch 6631: Training Loss: 0.07096961885690689 Validation Loss: 0.7503712177276611\n",
      "Epoch 6632: Training Loss: 0.0711384651561578 Validation Loss: 0.7531642317771912\n",
      "Epoch 6633: Training Loss: 0.0710496890048186 Validation Loss: 0.7514638304710388\n",
      "Epoch 6634: Training Loss: 0.07108171780904134 Validation Loss: 0.7519270181655884\n",
      "Epoch 6635: Training Loss: 0.07165233915050824 Validation Loss: 0.7461032867431641\n",
      "Epoch 6636: Training Loss: 0.07115019237001736 Validation Loss: 0.7447429299354553\n",
      "Epoch 6637: Training Loss: 0.07101791352033615 Validation Loss: 0.7484195232391357\n",
      "Epoch 6638: Training Loss: 0.07102656240264575 Validation Loss: 0.7500733733177185\n",
      "Epoch 6639: Training Loss: 0.0709137072165807 Validation Loss: 0.7510620951652527\n",
      "Epoch 6640: Training Loss: 0.07096648092071216 Validation Loss: 0.7500311136245728\n",
      "Epoch 6641: Training Loss: 0.07115507746736209 Validation Loss: 0.7529541850090027\n",
      "Epoch 6642: Training Loss: 0.07100626205404599 Validation Loss: 0.750436007976532\n",
      "Epoch 6643: Training Loss: 0.07099140683809917 Validation Loss: 0.7480865716934204\n",
      "Epoch 6644: Training Loss: 0.07129361356298129 Validation Loss: 0.7445405721664429\n",
      "Epoch 6645: Training Loss: 0.07123413557807605 Validation Loss: 0.7500244975090027\n",
      "Epoch 6646: Training Loss: 0.07091976205507915 Validation Loss: 0.7493176460266113\n",
      "Epoch 6647: Training Loss: 0.07087800155083339 Validation Loss: 0.7497247457504272\n",
      "Epoch 6648: Training Loss: 0.0708150677382946 Validation Loss: 0.750471830368042\n",
      "Epoch 6649: Training Loss: 0.07085114220778148 Validation Loss: 0.7507633566856384\n",
      "Epoch 6650: Training Loss: 0.07080640892187755 Validation Loss: 0.7502522468566895\n",
      "Epoch 6651: Training Loss: 0.07082364956537883 Validation Loss: 0.7492669224739075\n",
      "Epoch 6652: Training Loss: 0.07087277993559837 Validation Loss: 0.7482797503471375\n",
      "Epoch 6653: Training Loss: 0.07074236621459325 Validation Loss: 0.7492967247962952\n",
      "Epoch 6654: Training Loss: 0.07075296590725581 Validation Loss: 0.7497956156730652\n",
      "Epoch 6655: Training Loss: 0.07083205630381902 Validation Loss: 0.752400815486908\n",
      "Epoch 6656: Training Loss: 0.07082407052318256 Validation Loss: 0.7502474188804626\n",
      "Epoch 6657: Training Loss: 0.07075562328100204 Validation Loss: 0.7511919736862183\n",
      "Epoch 6658: Training Loss: 0.07076920444766681 Validation Loss: 0.75017911195755\n",
      "Epoch 6659: Training Loss: 0.07069367791215579 Validation Loss: 0.748993456363678\n",
      "Epoch 6660: Training Loss: 0.07084962477286656 Validation Loss: 0.7472859621047974\n",
      "Epoch 6661: Training Loss: 0.07071033741037051 Validation Loss: 0.7492138743400574\n",
      "Epoch 6662: Training Loss: 0.07074707001447678 Validation Loss: 0.7491483688354492\n",
      "Epoch 6663: Training Loss: 0.07068220277627309 Validation Loss: 0.7492795586585999\n",
      "Epoch 6664: Training Loss: 0.0710040715833505 Validation Loss: 0.7528550028800964\n",
      "Epoch 6665: Training Loss: 0.0707377480963866 Validation Loss: 0.7498836517333984\n",
      "Epoch 6666: Training Loss: 0.07066165407498677 Validation Loss: 0.7479459643363953\n",
      "Epoch 6667: Training Loss: 0.07072594513495763 Validation Loss: 0.7492318749427795\n",
      "Epoch 6668: Training Loss: 0.07064258058865865 Validation Loss: 0.7472837567329407\n",
      "Epoch 6669: Training Loss: 0.07064438983798027 Validation Loss: 0.7476841807365417\n",
      "Epoch 6670: Training Loss: 0.07062856356302898 Validation Loss: 0.7490427494049072\n",
      "Epoch 6671: Training Loss: 0.07067552208900452 Validation Loss: 0.7486487627029419\n",
      "Epoch 6672: Training Loss: 0.07079667846361797 Validation Loss: 0.7529750466346741\n",
      "Epoch 6673: Training Loss: 0.07061123227079709 Validation Loss: 0.7527743577957153\n",
      "Epoch 6674: Training Loss: 0.07064200316866238 Validation Loss: 0.7503570914268494\n",
      "Epoch 6675: Training Loss: 0.07066823542118073 Validation Loss: 0.751424252986908\n",
      "Epoch 6676: Training Loss: 0.07056367273132007 Validation Loss: 0.7504557371139526\n",
      "Epoch 6677: Training Loss: 0.07056529571612676 Validation Loss: 0.7481675744056702\n",
      "Epoch 6678: Training Loss: 0.07052637139956157 Validation Loss: 0.7474793195724487\n",
      "Epoch 6679: Training Loss: 0.07051364208261172 Validation Loss: 0.747941255569458\n",
      "Epoch 6680: Training Loss: 0.07049447298049927 Validation Loss: 0.7481799721717834\n",
      "Epoch 6681: Training Loss: 0.07057605187098186 Validation Loss: 0.7505457997322083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6682: Training Loss: 0.07060025508205096 Validation Loss: 0.7486395835876465\n",
      "Epoch 6683: Training Loss: 0.0704702412088712 Validation Loss: 0.7501704692840576\n",
      "Epoch 6684: Training Loss: 0.07047278806567192 Validation Loss: 0.7489814162254333\n",
      "Epoch 6685: Training Loss: 0.07064298788706462 Validation Loss: 0.7520332336425781\n",
      "Epoch 6686: Training Loss: 0.07048585638403893 Validation Loss: 0.7503508925437927\n",
      "Epoch 6687: Training Loss: 0.07047337666153908 Validation Loss: 0.748491644859314\n",
      "Epoch 6688: Training Loss: 0.07039556776483853 Validation Loss: 0.7487877011299133\n",
      "Epoch 6689: Training Loss: 0.07064854229489963 Validation Loss: 0.7500042915344238\n",
      "Epoch 6690: Training Loss: 0.07052092750867207 Validation Loss: 0.7483882308006287\n",
      "Epoch 6691: Training Loss: 0.07042964299519856 Validation Loss: 0.7503563761711121\n",
      "Epoch 6692: Training Loss: 0.07037506873408954 Validation Loss: 0.7510435581207275\n",
      "Epoch 6693: Training Loss: 0.0703650563955307 Validation Loss: 0.750378429889679\n",
      "Epoch 6694: Training Loss: 0.07036331420143445 Validation Loss: 0.7501367926597595\n",
      "Epoch 6695: Training Loss: 0.07033694411317508 Validation Loss: 0.7506043910980225\n",
      "Epoch 6696: Training Loss: 0.0704251949985822 Validation Loss: 0.7514877319335938\n",
      "Epoch 6697: Training Loss: 0.07034815102815628 Validation Loss: 0.7509872913360596\n",
      "Epoch 6698: Training Loss: 0.07040303697188695 Validation Loss: 0.7519857287406921\n",
      "Epoch 6699: Training Loss: 0.07036207243800163 Validation Loss: 0.7526665925979614\n",
      "Epoch 6700: Training Loss: 0.07031713053584099 Validation Loss: 0.7504145503044128\n",
      "Epoch 6701: Training Loss: 0.07030078768730164 Validation Loss: 0.7509241104125977\n",
      "Epoch 6702: Training Loss: 0.07027849555015564 Validation Loss: 0.7491083741188049\n",
      "Epoch 6703: Training Loss: 0.07024887949228287 Validation Loss: 0.747628927230835\n",
      "Epoch 6704: Training Loss: 0.07056469718615214 Validation Loss: 0.7507390379905701\n",
      "Epoch 6705: Training Loss: 0.07030398026108742 Validation Loss: 0.7510536909103394\n",
      "Epoch 6706: Training Loss: 0.07052247226238251 Validation Loss: 0.7468603253364563\n",
      "Epoch 6707: Training Loss: 0.07022252678871155 Validation Loss: 0.7472264766693115\n",
      "Epoch 6708: Training Loss: 0.07025762895743053 Validation Loss: 0.7491328716278076\n",
      "Epoch 6709: Training Loss: 0.07019945606589317 Validation Loss: 0.749924898147583\n",
      "Epoch 6710: Training Loss: 0.07025518144170444 Validation Loss: 0.7488634586334229\n",
      "Epoch 6711: Training Loss: 0.07019160687923431 Validation Loss: 0.7508423328399658\n",
      "Epoch 6712: Training Loss: 0.07016261915365855 Validation Loss: 0.752109706401825\n",
      "Epoch 6713: Training Loss: 0.07029274602731068 Validation Loss: 0.7527026534080505\n",
      "Epoch 6714: Training Loss: 0.07014542321364085 Validation Loss: 0.7505655884742737\n",
      "Epoch 6715: Training Loss: 0.0702448512117068 Validation Loss: 0.7502877116203308\n",
      "Epoch 6716: Training Loss: 0.07017855594555537 Validation Loss: 0.748389482498169\n",
      "Epoch 6717: Training Loss: 0.07019164909919103 Validation Loss: 0.7482855319976807\n",
      "Epoch 6718: Training Loss: 0.0701456827421983 Validation Loss: 0.7488800883293152\n",
      "Epoch 6719: Training Loss: 0.07019688189029694 Validation Loss: 0.7526246905326843\n",
      "Epoch 6720: Training Loss: 0.07016272842884064 Validation Loss: 0.7518923282623291\n",
      "Epoch 6721: Training Loss: 0.07005734741687775 Validation Loss: 0.7511177062988281\n",
      "Epoch 6722: Training Loss: 0.0700916846593221 Validation Loss: 0.7497916221618652\n",
      "Epoch 6723: Training Loss: 0.07009698326388995 Validation Loss: 0.7500424385070801\n",
      "Epoch 6724: Training Loss: 0.0701398216187954 Validation Loss: 0.7518178224563599\n",
      "Epoch 6725: Training Loss: 0.0700107179582119 Validation Loss: 0.7501455545425415\n",
      "Epoch 6726: Training Loss: 0.07002924879391988 Validation Loss: 0.7495354413986206\n",
      "Epoch 6727: Training Loss: 0.07010994603236516 Validation Loss: 0.7470740675926208\n",
      "Epoch 6728: Training Loss: 0.07012719164292018 Validation Loss: 0.7467106580734253\n",
      "Epoch 6729: Training Loss: 0.07002612575888634 Validation Loss: 0.7499160170555115\n",
      "Epoch 6730: Training Loss: 0.06998289997378986 Validation Loss: 0.7525447010993958\n",
      "Epoch 6731: Training Loss: 0.06999339163303375 Validation Loss: 0.7539605498313904\n",
      "Epoch 6732: Training Loss: 0.07009730984767278 Validation Loss: 0.7513384819030762\n",
      "Epoch 6733: Training Loss: 0.0700033816198508 Validation Loss: 0.7507186532020569\n",
      "Epoch 6734: Training Loss: 0.07013887415329616 Validation Loss: 0.7519122362136841\n",
      "Epoch 6735: Training Loss: 0.07022236039241155 Validation Loss: 0.753662109375\n",
      "Epoch 6736: Training Loss: 0.06998971352974574 Validation Loss: 0.7503522038459778\n",
      "Epoch 6737: Training Loss: 0.06993533422549565 Validation Loss: 0.7476422190666199\n",
      "Epoch 6738: Training Loss: 0.06998231386144955 Validation Loss: 0.7485119700431824\n",
      "Epoch 6739: Training Loss: 0.07004135598738988 Validation Loss: 0.7464714050292969\n",
      "Epoch 6740: Training Loss: 0.06996878484884898 Validation Loss: 0.746645987033844\n",
      "Epoch 6741: Training Loss: 0.06987094009915988 Validation Loss: 0.7500734329223633\n",
      "Epoch 6742: Training Loss: 0.06984268749753635 Validation Loss: 0.7530075311660767\n",
      "Epoch 6743: Training Loss: 0.06993509083986282 Validation Loss: 0.7544439435005188\n",
      "Epoch 6744: Training Loss: 0.0698787197470665 Validation Loss: 0.753312349319458\n",
      "Epoch 6745: Training Loss: 0.06994011128942172 Validation Loss: 0.753399133682251\n",
      "Epoch 6746: Training Loss: 0.07003547127048175 Validation Loss: 0.7491863965988159\n",
      "Epoch 6747: Training Loss: 0.06976611663897832 Validation Loss: 0.7481899857521057\n",
      "Epoch 6748: Training Loss: 0.0701630488038063 Validation Loss: 0.745598316192627\n",
      "Epoch 6749: Training Loss: 0.06991446514924367 Validation Loss: 0.7498935461044312\n",
      "Epoch 6750: Training Loss: 0.06983596086502075 Validation Loss: 0.7522765398025513\n",
      "Epoch 6751: Training Loss: 0.06976716717084248 Validation Loss: 0.7521789073944092\n",
      "Epoch 6752: Training Loss: 0.06971761708458264 Validation Loss: 0.752751886844635\n",
      "Epoch 6753: Training Loss: 0.06973106910785039 Validation Loss: 0.7518407106399536\n",
      "Epoch 6754: Training Loss: 0.0700016828874747 Validation Loss: 0.7482728362083435\n",
      "Epoch 6755: Training Loss: 0.07001747439304988 Validation Loss: 0.7522774338722229\n",
      "Epoch 6756: Training Loss: 0.06979077309370041 Validation Loss: 0.7501845955848694\n",
      "Epoch 6757: Training Loss: 0.06972608839472134 Validation Loss: 0.7496597170829773\n",
      "Epoch 6758: Training Loss: 0.06969096759955089 Validation Loss: 0.7500874400138855\n",
      "Epoch 6759: Training Loss: 0.06978797540068626 Validation Loss: 0.7523465752601624\n",
      "Epoch 6760: Training Loss: 0.06971849376956622 Validation Loss: 0.7524563670158386\n",
      "Epoch 6761: Training Loss: 0.06966078529755275 Validation Loss: 0.7515836358070374\n",
      "Epoch 6762: Training Loss: 0.0696746123333772 Validation Loss: 0.7496439814567566\n",
      "Epoch 6763: Training Loss: 0.06987123439709346 Validation Loss: 0.7469135522842407\n",
      "Epoch 6764: Training Loss: 0.06972134485840797 Validation Loss: 0.7472175359725952\n",
      "Epoch 6765: Training Loss: 0.06969424337148666 Validation Loss: 0.7508329749107361\n",
      "Epoch 6766: Training Loss: 0.06977405771613121 Validation Loss: 0.7542818188667297\n",
      "Epoch 6767: Training Loss: 0.06981740146875381 Validation Loss: 0.7512296438217163\n",
      "Epoch 6768: Training Loss: 0.06961572170257568 Validation Loss: 0.750217616558075\n",
      "Epoch 6769: Training Loss: 0.06970547139644623 Validation Loss: 0.7521910071372986\n",
      "Epoch 6770: Training Loss: 0.06987449154257774 Validation Loss: 0.7491878867149353\n",
      "Epoch 6771: Training Loss: 0.07016441722710927 Validation Loss: 0.7543033361434937\n",
      "Epoch 6772: Training Loss: 0.0695782204469045 Validation Loss: 0.7517640590667725\n",
      "Epoch 6773: Training Loss: 0.06963105375568072 Validation Loss: 0.7489890456199646\n",
      "Epoch 6774: Training Loss: 0.06974197303255399 Validation Loss: 0.7462040185928345\n",
      "Epoch 6775: Training Loss: 0.06964292625586192 Validation Loss: 0.7487568259239197\n",
      "Epoch 6776: Training Loss: 0.0695351945857207 Validation Loss: 0.7492964267730713\n",
      "Epoch 6777: Training Loss: 0.06948340559999149 Validation Loss: 0.7508187890052795\n",
      "Epoch 6778: Training Loss: 0.06959699218471845 Validation Loss: 0.7535828351974487\n",
      "Epoch 6779: Training Loss: 0.06955212603012721 Validation Loss: 0.7520864009857178\n",
      "Epoch 6780: Training Loss: 0.06949848433335622 Validation Loss: 0.7517132759094238\n",
      "Epoch 6781: Training Loss: 0.06951880206664403 Validation Loss: 0.7529414296150208\n",
      "Epoch 6782: Training Loss: 0.06944996739427249 Validation Loss: 0.7519541382789612\n",
      "Epoch 6783: Training Loss: 0.06944744288921356 Validation Loss: 0.7523744106292725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6784: Training Loss: 0.06947485978404681 Validation Loss: 0.7496834397315979\n",
      "Epoch 6785: Training Loss: 0.06982424110174179 Validation Loss: 0.7466049790382385\n",
      "Epoch 6786: Training Loss: 0.06943980728586514 Validation Loss: 0.7491862773895264\n",
      "Epoch 6787: Training Loss: 0.06946862488985062 Validation Loss: 0.7496110796928406\n",
      "Epoch 6788: Training Loss: 0.06961128984888394 Validation Loss: 0.7545313835144043\n",
      "Epoch 6789: Training Loss: 0.06947437673807144 Validation Loss: 0.7543394565582275\n",
      "Epoch 6790: Training Loss: 0.06953237081567447 Validation Loss: 0.7513922452926636\n",
      "Epoch 6791: Training Loss: 0.06956228241324425 Validation Loss: 0.7488774061203003\n",
      "Epoch 6792: Training Loss: 0.06958988681435585 Validation Loss: 0.7521421909332275\n",
      "Epoch 6793: Training Loss: 0.06939252590139706 Validation Loss: 0.7520247101783752\n",
      "Epoch 6794: Training Loss: 0.06962436934312184 Validation Loss: 0.754170835018158\n",
      "Epoch 6795: Training Loss: 0.0694695549706618 Validation Loss: 0.7502933144569397\n",
      "Epoch 6796: Training Loss: 0.06952199588219325 Validation Loss: 0.74753338098526\n",
      "Epoch 6797: Training Loss: 0.06942746664086978 Validation Loss: 0.7497023344039917\n",
      "Epoch 6798: Training Loss: 0.06942753493785858 Validation Loss: 0.7524766325950623\n",
      "Epoch 6799: Training Loss: 0.06927113855878513 Validation Loss: 0.7526019811630249\n",
      "Epoch 6800: Training Loss: 0.06946685289343198 Validation Loss: 0.7549076080322266\n",
      "Epoch 6801: Training Loss: 0.06927769134442012 Validation Loss: 0.7533019185066223\n",
      "Epoch 6802: Training Loss: 0.0699852705001831 Validation Loss: 0.7467131018638611\n",
      "Epoch 6803: Training Loss: 0.06963435436288516 Validation Loss: 0.7445334196090698\n",
      "Epoch 6804: Training Loss: 0.06946321328481038 Validation Loss: 0.7490304112434387\n",
      "Epoch 6805: Training Loss: 0.06916553527116776 Validation Loss: 0.7520701885223389\n",
      "Epoch 6806: Training Loss: 0.06937605390946071 Validation Loss: 0.7524675130844116\n",
      "Epoch 6807: Training Loss: 0.06973803912599881 Validation Loss: 0.7582833170890808\n",
      "Epoch 6808: Training Loss: 0.06940369059642156 Validation Loss: 0.7565016150474548\n",
      "Epoch 6809: Training Loss: 0.06962569306294124 Validation Loss: 0.750816822052002\n",
      "Epoch 6810: Training Loss: 0.06958820546666782 Validation Loss: 0.7469242215156555\n",
      "Epoch 6811: Training Loss: 0.06922351072231929 Validation Loss: 0.7492221593856812\n",
      "Epoch 6812: Training Loss: 0.0692047302921613 Validation Loss: 0.7518444061279297\n",
      "Epoch 6813: Training Loss: 0.06940760215123494 Validation Loss: 0.7503367066383362\n",
      "Epoch 6814: Training Loss: 0.06937178845206897 Validation Loss: 0.7491786479949951\n",
      "Epoch 6815: Training Loss: 0.06908221915364265 Validation Loss: 0.7521380186080933\n",
      "Epoch 6816: Training Loss: 0.06910091762741406 Validation Loss: 0.7546578049659729\n",
      "Epoch 6817: Training Loss: 0.06916819761196773 Validation Loss: 0.7560353875160217\n",
      "Epoch 6818: Training Loss: 0.06914617866277695 Validation Loss: 0.7551555037498474\n",
      "Epoch 6819: Training Loss: 0.06905929123361905 Validation Loss: 0.7521007061004639\n",
      "Epoch 6820: Training Loss: 0.06920475264390309 Validation Loss: 0.7519761919975281\n",
      "Epoch 6821: Training Loss: 0.06948762387037277 Validation Loss: 0.7464901804924011\n",
      "Epoch 6822: Training Loss: 0.06926154096921285 Validation Loss: 0.747561514377594\n",
      "Epoch 6823: Training Loss: 0.06914035851756732 Validation Loss: 0.747665286064148\n",
      "Epoch 6824: Training Loss: 0.06903549407919247 Validation Loss: 0.7508722543716431\n",
      "Epoch 6825: Training Loss: 0.06908377011617024 Validation Loss: 0.753949761390686\n",
      "Epoch 6826: Training Loss: 0.06941386808951695 Validation Loss: 0.7571665644645691\n",
      "Epoch 6827: Training Loss: 0.06910804162422816 Validation Loss: 0.7556619644165039\n",
      "Epoch 6828: Training Loss: 0.06899580111106236 Validation Loss: 0.7533770799636841\n",
      "Epoch 6829: Training Loss: 0.06894324471553166 Validation Loss: 0.7491754293441772\n",
      "Epoch 6830: Training Loss: 0.06902065128087997 Validation Loss: 0.7484251856803894\n",
      "Epoch 6831: Training Loss: 0.06909130762020747 Validation Loss: 0.7497296929359436\n",
      "Epoch 6832: Training Loss: 0.06903300061821938 Validation Loss: 0.747795045375824\n",
      "Epoch 6833: Training Loss: 0.06912912552555402 Validation Loss: 0.7470533847808838\n",
      "Epoch 6834: Training Loss: 0.06901675711075465 Validation Loss: 0.7484339475631714\n",
      "Epoch 6835: Training Loss: 0.06889313211043675 Validation Loss: 0.7516624331474304\n",
      "Epoch 6836: Training Loss: 0.06899000828464825 Validation Loss: 0.7570796012878418\n",
      "Epoch 6837: Training Loss: 0.06903303911288579 Validation Loss: 0.7558969855308533\n",
      "Epoch 6838: Training Loss: 0.06934400896231334 Validation Loss: 0.7525051236152649\n",
      "Epoch 6839: Training Loss: 0.06889113162954648 Validation Loss: 0.7531293034553528\n",
      "Epoch 6840: Training Loss: 0.06886614362398784 Validation Loss: 0.7525515556335449\n",
      "Epoch 6841: Training Loss: 0.06921379640698433 Validation Loss: 0.7556717395782471\n",
      "Epoch 6842: Training Loss: 0.06911926468213399 Validation Loss: 0.751327633857727\n",
      "Epoch 6843: Training Loss: 0.0688154362142086 Validation Loss: 0.7508216500282288\n",
      "Epoch 6844: Training Loss: 0.068885937333107 Validation Loss: 0.751617431640625\n",
      "Epoch 6845: Training Loss: 0.06887053698301315 Validation Loss: 0.7519965171813965\n",
      "Epoch 6846: Training Loss: 0.06902311618129413 Validation Loss: 0.7538941502571106\n",
      "Epoch 6847: Training Loss: 0.06888964523871739 Validation Loss: 0.7534197568893433\n",
      "Epoch 6848: Training Loss: 0.0687674308816592 Validation Loss: 0.7516415119171143\n",
      "Epoch 6849: Training Loss: 0.0688443419833978 Validation Loss: 0.749792218208313\n",
      "Epoch 6850: Training Loss: 0.06881083051363628 Validation Loss: 0.7474965453147888\n",
      "Epoch 6851: Training Loss: 0.06884180878599484 Validation Loss: 0.7483781576156616\n",
      "Epoch 6852: Training Loss: 0.06889558956027031 Validation Loss: 0.7480390667915344\n",
      "Epoch 6853: Training Loss: 0.06900662928819656 Validation Loss: 0.7474644780158997\n",
      "Epoch 6854: Training Loss: 0.06876694907744725 Validation Loss: 0.7511950731277466\n",
      "Epoch 6855: Training Loss: 0.06875476861993472 Validation Loss: 0.7519829273223877\n",
      "Epoch 6856: Training Loss: 0.068820521235466 Validation Loss: 0.7563884854316711\n",
      "Epoch 6857: Training Loss: 0.06893676022688548 Validation Loss: 0.7576469779014587\n",
      "Epoch 6858: Training Loss: 0.06882051875193913 Validation Loss: 0.7539681196212769\n",
      "Epoch 6859: Training Loss: 0.06866743043065071 Validation Loss: 0.7523281574249268\n",
      "Epoch 6860: Training Loss: 0.0687462476392587 Validation Loss: 0.7497401237487793\n",
      "Epoch 6861: Training Loss: 0.06892862419287364 Validation Loss: 0.7481451630592346\n",
      "Epoch 6862: Training Loss: 0.06908650944630305 Validation Loss: 0.7533658742904663\n",
      "Epoch 6863: Training Loss: 0.06875773519277573 Validation Loss: 0.7552938461303711\n",
      "Epoch 6864: Training Loss: 0.06882231682538986 Validation Loss: 0.7515955567359924\n",
      "Epoch 6865: Training Loss: 0.06864305833975475 Validation Loss: 0.7514962553977966\n",
      "Epoch 6866: Training Loss: 0.06868291522065799 Validation Loss: 0.7527462840080261\n",
      "Epoch 6867: Training Loss: 0.06903995325167973 Validation Loss: 0.7479016780853271\n",
      "Epoch 6868: Training Loss: 0.06879461308320363 Validation Loss: 0.7513200044631958\n",
      "Epoch 6869: Training Loss: 0.06866099933783214 Validation Loss: 0.7502164244651794\n",
      "Epoch 6870: Training Loss: 0.06856637448072433 Validation Loss: 0.751419186592102\n",
      "Epoch 6871: Training Loss: 0.06868307416637738 Validation Loss: 0.7510517239570618\n",
      "Epoch 6872: Training Loss: 0.06886168817679088 Validation Loss: 0.7545422315597534\n",
      "Epoch 6873: Training Loss: 0.06863841414451599 Validation Loss: 0.7530638575553894\n",
      "Epoch 6874: Training Loss: 0.06864997992912929 Validation Loss: 0.7511560916900635\n",
      "Epoch 6875: Training Loss: 0.0687140313287576 Validation Loss: 0.7539905905723572\n",
      "Epoch 6876: Training Loss: 0.06861515591541927 Validation Loss: 0.7542040348052979\n",
      "Epoch 6877: Training Loss: 0.06855233386158943 Validation Loss: 0.7511785626411438\n",
      "Epoch 6878: Training Loss: 0.06852669517199199 Validation Loss: 0.751621663570404\n",
      "Epoch 6879: Training Loss: 0.06865738580624263 Validation Loss: 0.7531610131263733\n",
      "Epoch 6880: Training Loss: 0.06851866220434506 Validation Loss: 0.7503578662872314\n",
      "Epoch 6881: Training Loss: 0.0684544990460078 Validation Loss: 0.7500423789024353\n",
      "Epoch 6882: Training Loss: 0.06849696983893712 Validation Loss: 0.74931401014328\n",
      "Epoch 6883: Training Loss: 0.06860462576150894 Validation Loss: 0.7521712183952332\n",
      "Epoch 6884: Training Loss: 0.0684096763531367 Validation Loss: 0.7525938153266907\n",
      "Epoch 6885: Training Loss: 0.06850060075521469 Validation Loss: 0.7539548277854919\n",
      "Epoch 6886: Training Loss: 0.06862850983937581 Validation Loss: 0.7514652609825134\n",
      "Epoch 6887: Training Loss: 0.0684021587173144 Validation Loss: 0.7524623274803162\n",
      "Epoch 6888: Training Loss: 0.06846346830328305 Validation Loss: 0.7530673146247864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6889: Training Loss: 0.06850217282772064 Validation Loss: 0.7512816190719604\n",
      "Epoch 6890: Training Loss: 0.06836331263184547 Validation Loss: 0.7524039149284363\n",
      "Epoch 6891: Training Loss: 0.06871713449557622 Validation Loss: 0.7559037804603577\n",
      "Epoch 6892: Training Loss: 0.06840972478191058 Validation Loss: 0.754592776298523\n",
      "Epoch 6893: Training Loss: 0.06838580220937729 Validation Loss: 0.7526659369468689\n",
      "Epoch 6894: Training Loss: 0.06834342579046886 Validation Loss: 0.749595582485199\n",
      "Epoch 6895: Training Loss: 0.06840770443280537 Validation Loss: 0.747947633266449\n",
      "Epoch 6896: Training Loss: 0.06838766361276309 Validation Loss: 0.7487156987190247\n",
      "Epoch 6897: Training Loss: 0.06835082297523816 Validation Loss: 0.7502302527427673\n",
      "Epoch 6898: Training Loss: 0.06834812959035237 Validation Loss: 0.7512655258178711\n",
      "Epoch 6899: Training Loss: 0.06873456637064616 Validation Loss: 0.7561899423599243\n",
      "Epoch 6900: Training Loss: 0.06835273901621501 Validation Loss: 0.7550743222236633\n",
      "Epoch 6901: Training Loss: 0.06834570318460464 Validation Loss: 0.7534284591674805\n",
      "Epoch 6902: Training Loss: 0.0683823215464751 Validation Loss: 0.7533261775970459\n",
      "Epoch 6903: Training Loss: 0.06831870848933856 Validation Loss: 0.7515044808387756\n",
      "Epoch 6904: Training Loss: 0.0682698351641496 Validation Loss: 0.7508929967880249\n",
      "Epoch 6905: Training Loss: 0.06857127820452054 Validation Loss: 0.7531368136405945\n",
      "Epoch 6906: Training Loss: 0.06828653315703075 Validation Loss: 0.7526332139968872\n",
      "Epoch 6907: Training Loss: 0.0683400643368562 Validation Loss: 0.7492390275001526\n",
      "Epoch 6908: Training Loss: 0.06825809180736542 Validation Loss: 0.7497589588165283\n",
      "Epoch 6909: Training Loss: 0.06829904764890671 Validation Loss: 0.7520995736122131\n",
      "Epoch 6910: Training Loss: 0.06821510940790176 Validation Loss: 0.7528467774391174\n",
      "Epoch 6911: Training Loss: 0.0681787592669328 Validation Loss: 0.7526252865791321\n",
      "Epoch 6912: Training Loss: 0.06818429877360661 Validation Loss: 0.7531126141548157\n",
      "Epoch 6913: Training Loss: 0.06830555821458499 Validation Loss: 0.7546748518943787\n",
      "Epoch 6914: Training Loss: 0.06832505017518997 Validation Loss: 0.7546641230583191\n",
      "Epoch 6915: Training Loss: 0.06822558989127477 Validation Loss: 0.7512499690055847\n",
      "Epoch 6916: Training Loss: 0.0683748833835125 Validation Loss: 0.7479901313781738\n",
      "Epoch 6917: Training Loss: 0.06822336216767629 Validation Loss: 0.7485325336456299\n",
      "Epoch 6918: Training Loss: 0.06823551903168361 Validation Loss: 0.7514864802360535\n",
      "Epoch 6919: Training Loss: 0.06811243668198586 Validation Loss: 0.7544259428977966\n",
      "Epoch 6920: Training Loss: 0.06814965481559436 Validation Loss: 0.7552079558372498\n",
      "Epoch 6921: Training Loss: 0.06837482998768489 Validation Loss: 0.7568737268447876\n",
      "Epoch 6922: Training Loss: 0.06853791822989781 Validation Loss: 0.7524075508117676\n",
      "Epoch 6923: Training Loss: 0.06811256458361943 Validation Loss: 0.7530572414398193\n",
      "Epoch 6924: Training Loss: 0.06829168771704038 Validation Loss: 0.7501481771469116\n",
      "Epoch 6925: Training Loss: 0.06813893715540568 Validation Loss: 0.7509501576423645\n",
      "Epoch 6926: Training Loss: 0.06801519294579823 Validation Loss: 0.7531364560127258\n",
      "Epoch 6927: Training Loss: 0.06817685564359029 Validation Loss: 0.7525511384010315\n",
      "Epoch 6928: Training Loss: 0.06810657183329265 Validation Loss: 0.7550662159919739\n",
      "Epoch 6929: Training Loss: 0.06831795846422513 Validation Loss: 0.7577435374259949\n",
      "Epoch 6930: Training Loss: 0.06807265554865201 Validation Loss: 0.7546586990356445\n",
      "Epoch 6931: Training Loss: 0.06798163056373596 Validation Loss: 0.7515973448753357\n",
      "Epoch 6932: Training Loss: 0.06824660673737526 Validation Loss: 0.7469483613967896\n",
      "Epoch 6933: Training Loss: 0.06807993228236835 Validation Loss: 0.7486697435379028\n",
      "Epoch 6934: Training Loss: 0.0680762951572736 Validation Loss: 0.7517344355583191\n",
      "Epoch 6935: Training Loss: 0.06805881361166637 Validation Loss: 0.7530496716499329\n",
      "Epoch 6936: Training Loss: 0.06808089216550191 Validation Loss: 0.7552819848060608\n",
      "Epoch 6937: Training Loss: 0.06808799256881078 Validation Loss: 0.7562201619148254\n",
      "Epoch 6938: Training Loss: 0.06794576346874237 Validation Loss: 0.754403293132782\n",
      "Epoch 6939: Training Loss: 0.06803707654277484 Validation Loss: 0.7500998377799988\n",
      "Epoch 6940: Training Loss: 0.06793200224637985 Validation Loss: 0.7504957318305969\n",
      "Epoch 6941: Training Loss: 0.06810304149985313 Validation Loss: 0.7532488107681274\n",
      "Epoch 6942: Training Loss: 0.06790384898583095 Validation Loss: 0.7513278722763062\n",
      "Epoch 6943: Training Loss: 0.06795671582221985 Validation Loss: 0.7502593994140625\n",
      "Epoch 6944: Training Loss: 0.06801106159885724 Validation Loss: 0.7528844475746155\n",
      "Epoch 6945: Training Loss: 0.06811180338263512 Validation Loss: 0.750609815120697\n",
      "Epoch 6946: Training Loss: 0.06785417844851811 Validation Loss: 0.7516260147094727\n",
      "Epoch 6947: Training Loss: 0.06786862015724182 Validation Loss: 0.7540897130966187\n",
      "Epoch 6948: Training Loss: 0.0679104911784331 Validation Loss: 0.7544987201690674\n",
      "Epoch 6949: Training Loss: 0.06791623433430989 Validation Loss: 0.756630539894104\n",
      "Epoch 6950: Training Loss: 0.06824293235937755 Validation Loss: 0.7515822052955627\n",
      "Epoch 6951: Training Loss: 0.06798763448993365 Validation Loss: 0.750510573387146\n",
      "Epoch 6952: Training Loss: 0.06784132743875186 Validation Loss: 0.7535037994384766\n",
      "Epoch 6953: Training Loss: 0.06780085836847623 Validation Loss: 0.7559679746627808\n",
      "Epoch 6954: Training Loss: 0.06788166115681331 Validation Loss: 0.7546498775482178\n",
      "Epoch 6955: Training Loss: 0.06830457970499992 Validation Loss: 0.7579218149185181\n",
      "Epoch 6956: Training Loss: 0.06782581905523936 Validation Loss: 0.7559143304824829\n",
      "Epoch 6957: Training Loss: 0.06793881331880887 Validation Loss: 0.7507980465888977\n",
      "Epoch 6958: Training Loss: 0.06794420753916104 Validation Loss: 0.746720552444458\n",
      "Epoch 6959: Training Loss: 0.06798192858695984 Validation Loss: 0.7499114274978638\n",
      "Epoch 6960: Training Loss: 0.06773632888992627 Validation Loss: 0.7507050037384033\n",
      "Epoch 6961: Training Loss: 0.0678153211871783 Validation Loss: 0.7542968392372131\n",
      "Epoch 6962: Training Loss: 0.0677386944492658 Validation Loss: 0.7552879452705383\n",
      "Epoch 6963: Training Loss: 0.06776237984498341 Validation Loss: 0.7540473341941833\n",
      "Epoch 6964: Training Loss: 0.06767331063747406 Validation Loss: 0.7532386779785156\n",
      "Epoch 6965: Training Loss: 0.06782813370227814 Validation Loss: 0.7512262463569641\n",
      "Epoch 6966: Training Loss: 0.06775240351756413 Validation Loss: 0.752712607383728\n",
      "Epoch 6967: Training Loss: 0.06773781155546506 Validation Loss: 0.7540927529335022\n",
      "Epoch 6968: Training Loss: 0.06779069577654202 Validation Loss: 0.7545166015625\n",
      "Epoch 6969: Training Loss: 0.06766574953993161 Validation Loss: 0.7537278532981873\n",
      "Epoch 6970: Training Loss: 0.06790987029671669 Validation Loss: 0.7492515444755554\n",
      "Epoch 6971: Training Loss: 0.06775067498286565 Validation Loss: 0.7486163377761841\n",
      "Epoch 6972: Training Loss: 0.06774409612019856 Validation Loss: 0.7525869011878967\n",
      "Epoch 6973: Training Loss: 0.06760793800155322 Validation Loss: 0.7542691826820374\n",
      "Epoch 6974: Training Loss: 0.06764338289697965 Validation Loss: 0.7553337216377258\n",
      "Epoch 6975: Training Loss: 0.06765138978759448 Validation Loss: 0.7537106275558472\n",
      "Epoch 6976: Training Loss: 0.06785948077837627 Validation Loss: 0.7509514093399048\n",
      "Epoch 6977: Training Loss: 0.06764195362726848 Validation Loss: 0.7536171674728394\n",
      "Epoch 6978: Training Loss: 0.06769464537501335 Validation Loss: 0.7525771260261536\n",
      "Epoch 6979: Training Loss: 0.0675647035241127 Validation Loss: 0.7547890543937683\n",
      "Epoch 6980: Training Loss: 0.06758716329932213 Validation Loss: 0.7551141977310181\n",
      "Epoch 6981: Training Loss: 0.0678039938211441 Validation Loss: 0.7578698396682739\n",
      "Epoch 6982: Training Loss: 0.06763748700420062 Validation Loss: 0.7547550797462463\n",
      "Epoch 6983: Training Loss: 0.06759573643406232 Validation Loss: 0.7546151280403137\n",
      "Epoch 6984: Training Loss: 0.06756869703531265 Validation Loss: 0.7526755928993225\n",
      "Epoch 6985: Training Loss: 0.067508681366841 Validation Loss: 0.7514684796333313\n",
      "Epoch 6986: Training Loss: 0.0676942951977253 Validation Loss: 0.7491387128829956\n",
      "Epoch 6987: Training Loss: 0.06748204429944356 Validation Loss: 0.7515212893486023\n",
      "Epoch 6988: Training Loss: 0.0675209475060304 Validation Loss: 0.7525513172149658\n",
      "Epoch 6989: Training Loss: 0.06742554778854053 Validation Loss: 0.7554786801338196\n",
      "Epoch 6990: Training Loss: 0.06756391127904256 Validation Loss: 0.7583810687065125\n",
      "Epoch 6991: Training Loss: 0.06752480566501617 Validation Loss: 0.7559051513671875\n",
      "Epoch 6992: Training Loss: 0.067636805276076 Validation Loss: 0.7527866959571838\n",
      "Epoch 6993: Training Loss: 0.06745118647813797 Validation Loss: 0.753779411315918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6994: Training Loss: 0.06757644439737003 Validation Loss: 0.7514842748641968\n",
      "Epoch 6995: Training Loss: 0.06757181386152904 Validation Loss: 0.7524710297584534\n",
      "Epoch 6996: Training Loss: 0.06740610922376315 Validation Loss: 0.7545104622840881\n",
      "Epoch 6997: Training Loss: 0.06742961953083675 Validation Loss: 0.755523145198822\n",
      "Epoch 6998: Training Loss: 0.0673539936542511 Validation Loss: 0.7543517351150513\n",
      "Epoch 6999: Training Loss: 0.0676416444281737 Validation Loss: 0.7563071250915527\n",
      "Epoch 7000: Training Loss: 0.06751338765025139 Validation Loss: 0.7545080780982971\n",
      "Epoch 7001: Training Loss: 0.06730574493606885 Validation Loss: 0.7525942325592041\n",
      "Epoch 7002: Training Loss: 0.06735116988420486 Validation Loss: 0.7511517405509949\n",
      "Epoch 7003: Training Loss: 0.06732180466254552 Validation Loss: 0.7519323825836182\n",
      "Epoch 7004: Training Loss: 0.06734744583566983 Validation Loss: 0.752178430557251\n",
      "Epoch 7005: Training Loss: 0.06729531412323315 Validation Loss: 0.7528405785560608\n",
      "Epoch 7006: Training Loss: 0.06728461136420567 Validation Loss: 0.7537923455238342\n",
      "Epoch 7007: Training Loss: 0.06728765865166982 Validation Loss: 0.7547430992126465\n",
      "Epoch 7008: Training Loss: 0.06726007411877315 Validation Loss: 0.7542213201522827\n",
      "Epoch 7009: Training Loss: 0.06741215412815411 Validation Loss: 0.7553108930587769\n",
      "Epoch 7010: Training Loss: 0.06724477310975392 Validation Loss: 0.7538875937461853\n",
      "Epoch 7011: Training Loss: 0.06725154196222623 Validation Loss: 0.7514367699623108\n",
      "Epoch 7012: Training Loss: 0.06721290200948715 Validation Loss: 0.751430332660675\n",
      "Epoch 7013: Training Loss: 0.06728318706154823 Validation Loss: 0.7507352232933044\n",
      "Epoch 7014: Training Loss: 0.06752149139841397 Validation Loss: 0.7496222257614136\n",
      "Epoch 7015: Training Loss: 0.06792662913600604 Validation Loss: 0.756698489189148\n",
      "Epoch 7016: Training Loss: 0.06729087233543396 Validation Loss: 0.7553147673606873\n",
      "Epoch 7017: Training Loss: 0.06751622259616852 Validation Loss: 0.7586759328842163\n",
      "Epoch 7018: Training Loss: 0.06720953434705734 Validation Loss: 0.7568233013153076\n",
      "Epoch 7019: Training Loss: 0.06717226405938466 Validation Loss: 0.7544263601303101\n",
      "Epoch 7020: Training Loss: 0.06714131310582161 Validation Loss: 0.7517604827880859\n",
      "Epoch 7021: Training Loss: 0.0672729139526685 Validation Loss: 0.75201416015625\n",
      "Epoch 7022: Training Loss: 0.06718579183022182 Validation Loss: 0.7527416944503784\n",
      "Epoch 7023: Training Loss: 0.06721176703770955 Validation Loss: 0.7531504034996033\n",
      "Epoch 7024: Training Loss: 0.06715841218829155 Validation Loss: 0.7542689442634583\n",
      "Epoch 7025: Training Loss: 0.06726612274845441 Validation Loss: 0.7521199584007263\n",
      "Epoch 7026: Training Loss: 0.06717287997404735 Validation Loss: 0.7526469826698303\n",
      "Epoch 7027: Training Loss: 0.06720008701086044 Validation Loss: 0.7527089715003967\n",
      "Epoch 7028: Training Loss: 0.06719414393107097 Validation Loss: 0.7554060816764832\n",
      "Epoch 7029: Training Loss: 0.06717551251252492 Validation Loss: 0.7569433450698853\n",
      "Epoch 7030: Training Loss: 0.06725526849428813 Validation Loss: 0.7545870542526245\n",
      "Epoch 7031: Training Loss: 0.06718542178471883 Validation Loss: 0.7518122792243958\n",
      "Epoch 7032: Training Loss: 0.06708746900161107 Validation Loss: 0.7517416477203369\n",
      "Epoch 7033: Training Loss: 0.06722785532474518 Validation Loss: 0.7555713653564453\n",
      "Epoch 7034: Training Loss: 0.06704358756542206 Validation Loss: 0.7545846700668335\n",
      "Epoch 7035: Training Loss: 0.06709834312399228 Validation Loss: 0.7552641034126282\n",
      "Epoch 7036: Training Loss: 0.0670502744615078 Validation Loss: 0.7547000646591187\n",
      "Epoch 7037: Training Loss: 0.06724077463150024 Validation Loss: 0.7506896257400513\n",
      "Epoch 7038: Training Loss: 0.06716777756810188 Validation Loss: 0.7527896761894226\n",
      "Epoch 7039: Training Loss: 0.06717164441943169 Validation Loss: 0.7547431588172913\n",
      "Epoch 7040: Training Loss: 0.06758026654521625 Validation Loss: 0.7501572370529175\n",
      "Epoch 7041: Training Loss: 0.06709881871938705 Validation Loss: 0.7502536773681641\n",
      "Epoch 7042: Training Loss: 0.06694465378920238 Validation Loss: 0.753794252872467\n",
      "Epoch 7043: Training Loss: 0.06697131941715877 Validation Loss: 0.7552877068519592\n",
      "Epoch 7044: Training Loss: 0.06700581560532252 Validation Loss: 0.7557426691055298\n",
      "Epoch 7045: Training Loss: 0.06698941563566525 Validation Loss: 0.7578521966934204\n",
      "Epoch 7046: Training Loss: 0.06708548466364543 Validation Loss: 0.7577858567237854\n",
      "Epoch 7047: Training Loss: 0.06712608908613522 Validation Loss: 0.7579190135002136\n",
      "Epoch 7048: Training Loss: 0.06747849037249883 Validation Loss: 0.7511809468269348\n",
      "Epoch 7049: Training Loss: 0.06711006040374438 Validation Loss: 0.7488827705383301\n",
      "Epoch 7050: Training Loss: 0.06712242340048154 Validation Loss: 0.7521640658378601\n",
      "Epoch 7051: Training Loss: 0.06728999068339665 Validation Loss: 0.7566180229187012\n",
      "Epoch 7052: Training Loss: 0.06689339751998584 Validation Loss: 0.7565581202507019\n",
      "Epoch 7053: Training Loss: 0.06685976435740788 Validation Loss: 0.7544859647750854\n",
      "Epoch 7054: Training Loss: 0.06690042093396187 Validation Loss: 0.7521592378616333\n",
      "Epoch 7055: Training Loss: 0.06698402017354965 Validation Loss: 0.7529553771018982\n",
      "Epoch 7056: Training Loss: 0.06684955085317294 Validation Loss: 0.7526432871818542\n",
      "Epoch 7057: Training Loss: 0.06684196119507153 Validation Loss: 0.7536808252334595\n",
      "Epoch 7058: Training Loss: 0.06687941650549571 Validation Loss: 0.7527692914009094\n",
      "Epoch 7059: Training Loss: 0.06689842542012532 Validation Loss: 0.7525733113288879\n",
      "Epoch 7060: Training Loss: 0.06682480499148369 Validation Loss: 0.7540611624717712\n",
      "Epoch 7061: Training Loss: 0.0670786624153455 Validation Loss: 0.7579011917114258\n",
      "Epoch 7062: Training Loss: 0.06685500591993332 Validation Loss: 0.7561352252960205\n",
      "Epoch 7063: Training Loss: 0.06683595975240071 Validation Loss: 0.753742516040802\n",
      "Epoch 7064: Training Loss: 0.06703273952007294 Validation Loss: 0.7564269304275513\n",
      "Epoch 7065: Training Loss: 0.06697679683566093 Validation Loss: 0.7516595125198364\n",
      "Epoch 7066: Training Loss: 0.06681317836046219 Validation Loss: 0.7515367269515991\n",
      "Epoch 7067: Training Loss: 0.06679386769731839 Validation Loss: 0.7533979415893555\n",
      "Epoch 7068: Training Loss: 0.0667488103111585 Validation Loss: 0.7545689344406128\n",
      "Epoch 7069: Training Loss: 0.06679647788405418 Validation Loss: 0.7539479732513428\n",
      "Epoch 7070: Training Loss: 0.06670376906792323 Validation Loss: 0.75480717420578\n",
      "Epoch 7071: Training Loss: 0.06671757871905963 Validation Loss: 0.7554954886436462\n",
      "Epoch 7072: Training Loss: 0.06723012526830037 Validation Loss: 0.7508435845375061\n",
      "Epoch 7073: Training Loss: 0.0666764664153258 Validation Loss: 0.7532180547714233\n",
      "Epoch 7074: Training Loss: 0.06676731631159782 Validation Loss: 0.7551203370094299\n",
      "Epoch 7075: Training Loss: 0.0666885847846667 Validation Loss: 0.7572234272956848\n",
      "Epoch 7076: Training Loss: 0.0667242668569088 Validation Loss: 0.7568365335464478\n",
      "Epoch 7077: Training Loss: 0.06675011788805325 Validation Loss: 0.75701904296875\n",
      "Epoch 7078: Training Loss: 0.06685410936673482 Validation Loss: 0.7523935437202454\n",
      "Epoch 7079: Training Loss: 0.06668305893739064 Validation Loss: 0.7530694007873535\n",
      "Epoch 7080: Training Loss: 0.06662781909108162 Validation Loss: 0.7525508999824524\n",
      "Epoch 7081: Training Loss: 0.06686944141983986 Validation Loss: 0.7545109391212463\n",
      "Epoch 7082: Training Loss: 0.06686558326085408 Validation Loss: 0.7508688569068909\n",
      "Epoch 7083: Training Loss: 0.06664112955331802 Validation Loss: 0.7525316476821899\n",
      "Epoch 7084: Training Loss: 0.0667351062099139 Validation Loss: 0.7561776638031006\n",
      "Epoch 7085: Training Loss: 0.06660804773370425 Validation Loss: 0.7569495439529419\n",
      "Epoch 7086: Training Loss: 0.06668598825732867 Validation Loss: 0.7578359246253967\n",
      "Epoch 7087: Training Loss: 0.0666697124640147 Validation Loss: 0.754361093044281\n",
      "Epoch 7088: Training Loss: 0.06673151875535648 Validation Loss: 0.7508335709571838\n",
      "Epoch 7089: Training Loss: 0.06668514013290405 Validation Loss: 0.7502853870391846\n",
      "Epoch 7090: Training Loss: 0.06654157241185506 Validation Loss: 0.7530291080474854\n",
      "Epoch 7091: Training Loss: 0.06695489957928658 Validation Loss: 0.7590641975402832\n",
      "Epoch 7092: Training Loss: 0.06660910447438557 Validation Loss: 0.7570995688438416\n",
      "Epoch 7093: Training Loss: 0.0665585957467556 Validation Loss: 0.7568432688713074\n",
      "Epoch 7094: Training Loss: 0.06658791253964107 Validation Loss: 0.7529087662696838\n",
      "Epoch 7095: Training Loss: 0.06656330451369286 Validation Loss: 0.7517164945602417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7096: Training Loss: 0.06657269845406215 Validation Loss: 0.7522878646850586\n",
      "Epoch 7097: Training Loss: 0.0665433220565319 Validation Loss: 0.7532301545143127\n",
      "Epoch 7098: Training Loss: 0.06659506882230441 Validation Loss: 0.7527449131011963\n",
      "Epoch 7099: Training Loss: 0.06654727458953857 Validation Loss: 0.7559775710105896\n",
      "Epoch 7100: Training Loss: 0.06655001143614452 Validation Loss: 0.7566335797309875\n",
      "Epoch 7101: Training Loss: 0.06667509178320567 Validation Loss: 0.7553690075874329\n",
      "Epoch 7102: Training Loss: 0.06645252307256062 Validation Loss: 0.7554911971092224\n",
      "Epoch 7103: Training Loss: 0.06645258764425914 Validation Loss: 0.7542862296104431\n",
      "Epoch 7104: Training Loss: 0.06640628104408582 Validation Loss: 0.7545164227485657\n",
      "Epoch 7105: Training Loss: 0.06641112516323726 Validation Loss: 0.7557483315467834\n",
      "Epoch 7106: Training Loss: 0.06651779885093372 Validation Loss: 0.7541830539703369\n",
      "Epoch 7107: Training Loss: 0.06644011661410332 Validation Loss: 0.7566924095153809\n",
      "Epoch 7108: Training Loss: 0.06647106384237607 Validation Loss: 0.7574779987335205\n",
      "Epoch 7109: Training Loss: 0.06685243298610051 Validation Loss: 0.7594126462936401\n",
      "Epoch 7110: Training Loss: 0.0663911613325278 Validation Loss: 0.7565903067588806\n",
      "Epoch 7111: Training Loss: 0.06643091514706612 Validation Loss: 0.7509473562240601\n",
      "Epoch 7112: Training Loss: 0.06664356340964635 Validation Loss: 0.7476194500923157\n",
      "Epoch 7113: Training Loss: 0.06659074500203133 Validation Loss: 0.7503776550292969\n",
      "Epoch 7114: Training Loss: 0.0663338874777158 Validation Loss: 0.7533903121948242\n",
      "Epoch 7115: Training Loss: 0.0663869281609853 Validation Loss: 0.7566068768501282\n",
      "Epoch 7116: Training Loss: 0.06637948627273242 Validation Loss: 0.7569326758384705\n",
      "Epoch 7117: Training Loss: 0.06632806360721588 Validation Loss: 0.7574663162231445\n",
      "Epoch 7118: Training Loss: 0.0663291501502196 Validation Loss: 0.7583046555519104\n",
      "Epoch 7119: Training Loss: 0.0662905549009641 Validation Loss: 0.7566946148872375\n",
      "Epoch 7120: Training Loss: 0.06636947269241016 Validation Loss: 0.7533016204833984\n",
      "Epoch 7121: Training Loss: 0.06667908405264218 Validation Loss: 0.7498283386230469\n",
      "Epoch 7122: Training Loss: 0.06628625094890594 Validation Loss: 0.7522885799407959\n",
      "Epoch 7123: Training Loss: 0.06630705793698628 Validation Loss: 0.7562775611877441\n",
      "Epoch 7124: Training Loss: 0.06654757882157962 Validation Loss: 0.7606820464134216\n",
      "Epoch 7125: Training Loss: 0.06635195886095364 Validation Loss: 0.7590481638908386\n",
      "Epoch 7126: Training Loss: 0.06625869870185852 Validation Loss: 0.756209135055542\n",
      "Epoch 7127: Training Loss: 0.0664280317723751 Validation Loss: 0.7516826391220093\n",
      "Epoch 7128: Training Loss: 0.06644389902551968 Validation Loss: 0.749912440776825\n",
      "Epoch 7129: Training Loss: 0.06630651404460271 Validation Loss: 0.7518778443336487\n",
      "Epoch 7130: Training Loss: 0.0664276306827863 Validation Loss: 0.7578428983688354\n",
      "Epoch 7131: Training Loss: 0.06629011655847232 Validation Loss: 0.758878231048584\n",
      "Epoch 7132: Training Loss: 0.06619695449868838 Validation Loss: 0.7578157782554626\n",
      "Epoch 7133: Training Loss: 0.06615816056728363 Validation Loss: 0.7561780214309692\n",
      "Epoch 7134: Training Loss: 0.06615088259180386 Validation Loss: 0.7549331188201904\n",
      "Epoch 7135: Training Loss: 0.06613395735621452 Validation Loss: 0.7547733187675476\n",
      "Epoch 7136: Training Loss: 0.06642770146330197 Validation Loss: 0.7514628767967224\n",
      "Epoch 7137: Training Loss: 0.06617174794276555 Validation Loss: 0.7527160048484802\n",
      "Epoch 7138: Training Loss: 0.06613671034574509 Validation Loss: 0.753618061542511\n",
      "Epoch 7139: Training Loss: 0.06625935932000478 Validation Loss: 0.7578113079071045\n",
      "Epoch 7140: Training Loss: 0.06613940248886745 Validation Loss: 0.7591082453727722\n",
      "Epoch 7141: Training Loss: 0.06629504139224689 Validation Loss: 0.7564394474029541\n",
      "Epoch 7142: Training Loss: 0.0660811501244704 Validation Loss: 0.7553701400756836\n",
      "Epoch 7143: Training Loss: 0.06616111472249031 Validation Loss: 0.7560003995895386\n",
      "Epoch 7144: Training Loss: 0.06604317824045818 Validation Loss: 0.7543250918388367\n",
      "Epoch 7145: Training Loss: 0.06612129385272662 Validation Loss: 0.7538924217224121\n",
      "Epoch 7146: Training Loss: 0.06622351954380672 Validation Loss: 0.7525478601455688\n",
      "Epoch 7147: Training Loss: 0.06615754341085751 Validation Loss: 0.752202033996582\n",
      "Epoch 7148: Training Loss: 0.06604170054197311 Validation Loss: 0.7537939548492432\n",
      "Epoch 7149: Training Loss: 0.06610923757155736 Validation Loss: 0.7540001273155212\n",
      "Epoch 7150: Training Loss: 0.06595111017425855 Validation Loss: 0.7570114731788635\n",
      "Epoch 7151: Training Loss: 0.06640420854091644 Validation Loss: 0.7614636421203613\n",
      "Epoch 7152: Training Loss: 0.06626630201935768 Validation Loss: 0.7618285417556763\n",
      "Epoch 7153: Training Loss: 0.06615590800841649 Validation Loss: 0.7559759020805359\n",
      "Epoch 7154: Training Loss: 0.06597140183051427 Validation Loss: 0.7546880841255188\n",
      "Epoch 7155: Training Loss: 0.06607935080925624 Validation Loss: 0.7515566349029541\n",
      "Epoch 7156: Training Loss: 0.06610256433486938 Validation Loss: 0.7514382600784302\n",
      "Epoch 7157: Training Loss: 0.06599504748980205 Validation Loss: 0.7527446746826172\n",
      "Epoch 7158: Training Loss: 0.06592860445380211 Validation Loss: 0.7548743486404419\n",
      "Epoch 7159: Training Loss: 0.06596067175269127 Validation Loss: 0.7583709359169006\n",
      "Epoch 7160: Training Loss: 0.06603559603293736 Validation Loss: 0.7572160959243774\n",
      "Epoch 7161: Training Loss: 0.06597301239768665 Validation Loss: 0.7583897709846497\n",
      "Epoch 7162: Training Loss: 0.06600919490059216 Validation Loss: 0.7582790851593018\n",
      "Epoch 7163: Training Loss: 0.06610434378186862 Validation Loss: 0.7535185813903809\n",
      "Epoch 7164: Training Loss: 0.06590624650319417 Validation Loss: 0.7537479400634766\n",
      "Epoch 7165: Training Loss: 0.06590900694330533 Validation Loss: 0.7544743418693542\n",
      "Epoch 7166: Training Loss: 0.06597040841976802 Validation Loss: 0.753071665763855\n",
      "Epoch 7167: Training Loss: 0.06595409785707791 Validation Loss: 0.7551069259643555\n",
      "Epoch 7168: Training Loss: 0.0659168151517709 Validation Loss: 0.7568750381469727\n",
      "Epoch 7169: Training Loss: 0.06588895867268245 Validation Loss: 0.7574832439422607\n",
      "Epoch 7170: Training Loss: 0.06583874921003978 Validation Loss: 0.7564629316329956\n",
      "Epoch 7171: Training Loss: 0.06586607297261556 Validation Loss: 0.755519688129425\n",
      "Epoch 7172: Training Loss: 0.06586866949995358 Validation Loss: 0.7530442476272583\n",
      "Epoch 7173: Training Loss: 0.06597380836804707 Validation Loss: 0.7541260123252869\n",
      "Epoch 7174: Training Loss: 0.06585149466991425 Validation Loss: 0.7533072233200073\n",
      "Epoch 7175: Training Loss: 0.06603979816039403 Validation Loss: 0.7533128261566162\n",
      "Epoch 7176: Training Loss: 0.06597581754128139 Validation Loss: 0.7579160332679749\n",
      "Epoch 7177: Training Loss: 0.06583856170376141 Validation Loss: 0.7572569251060486\n",
      "Epoch 7178: Training Loss: 0.06583534056941669 Validation Loss: 0.7576937675476074\n",
      "Epoch 7179: Training Loss: 0.06583881129821141 Validation Loss: 0.7583226561546326\n",
      "Epoch 7180: Training Loss: 0.06590296824773152 Validation Loss: 0.754325807094574\n",
      "Epoch 7181: Training Loss: 0.06595265120267868 Validation Loss: 0.7570146918296814\n",
      "Epoch 7182: Training Loss: 0.06575590620438258 Validation Loss: 0.75486820936203\n",
      "Epoch 7183: Training Loss: 0.06579928720990817 Validation Loss: 0.755390465259552\n",
      "Epoch 7184: Training Loss: 0.06578429664174716 Validation Loss: 0.753613293170929\n",
      "Epoch 7185: Training Loss: 0.06589474777380626 Validation Loss: 0.7550793290138245\n",
      "Epoch 7186: Training Loss: 0.06614281982183456 Validation Loss: 0.7585595846176147\n",
      "Epoch 7187: Training Loss: 0.06573062514265378 Validation Loss: 0.7568089365959167\n",
      "Epoch 7188: Training Loss: 0.06568460166454315 Validation Loss: 0.7560306787490845\n",
      "Epoch 7189: Training Loss: 0.06570804367462794 Validation Loss: 0.7542920708656311\n",
      "Epoch 7190: Training Loss: 0.06565501044193904 Validation Loss: 0.753149688243866\n",
      "Epoch 7191: Training Loss: 0.06570594881971677 Validation Loss: 0.753368079662323\n",
      "Epoch 7192: Training Loss: 0.06576240435242653 Validation Loss: 0.7558630108833313\n",
      "Epoch 7193: Training Loss: 0.06570136298735936 Validation Loss: 0.7550259828567505\n",
      "Epoch 7194: Training Loss: 0.06564425056179364 Validation Loss: 0.756558895111084\n",
      "Epoch 7195: Training Loss: 0.0656283088028431 Validation Loss: 0.7574970722198486\n",
      "Epoch 7196: Training Loss: 0.06584054852525394 Validation Loss: 0.7599547505378723\n",
      "Epoch 7197: Training Loss: 0.06566281989216805 Validation Loss: 0.7573026418685913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7198: Training Loss: 0.0655918021996816 Validation Loss: 0.7546635866165161\n",
      "Epoch 7199: Training Loss: 0.06559543559948604 Validation Loss: 0.7529743909835815\n",
      "Epoch 7200: Training Loss: 0.06559192637602489 Validation Loss: 0.7529399991035461\n",
      "Epoch 7201: Training Loss: 0.0656416229903698 Validation Loss: 0.7524941563606262\n",
      "Epoch 7202: Training Loss: 0.06560567393898964 Validation Loss: 0.7544607520103455\n",
      "Epoch 7203: Training Loss: 0.06586235264937083 Validation Loss: 0.7587239146232605\n",
      "Epoch 7204: Training Loss: 0.06557816515366237 Validation Loss: 0.7577025890350342\n",
      "Epoch 7205: Training Loss: 0.06577061613400777 Validation Loss: 0.7591345906257629\n",
      "Epoch 7206: Training Loss: 0.06552805130680402 Validation Loss: 0.7576590776443481\n",
      "Epoch 7207: Training Loss: 0.06550163154800732 Validation Loss: 0.7542757987976074\n",
      "Epoch 7208: Training Loss: 0.06556297714511554 Validation Loss: 0.7534083724021912\n",
      "Epoch 7209: Training Loss: 0.06554742902517319 Validation Loss: 0.7517198324203491\n",
      "Epoch 7210: Training Loss: 0.0655218002696832 Validation Loss: 0.7535443902015686\n",
      "Epoch 7211: Training Loss: 0.0655510847767194 Validation Loss: 0.7557335495948792\n",
      "Epoch 7212: Training Loss: 0.06573923801382382 Validation Loss: 0.7585549354553223\n",
      "Epoch 7213: Training Loss: 0.06547802314162254 Validation Loss: 0.7574770450592041\n",
      "Epoch 7214: Training Loss: 0.06552229449152946 Validation Loss: 0.756692111492157\n",
      "Epoch 7215: Training Loss: 0.0655195564031601 Validation Loss: 0.7566915154457092\n",
      "Epoch 7216: Training Loss: 0.06585440908869107 Validation Loss: 0.7522197365760803\n",
      "Epoch 7217: Training Loss: 0.06561580548683803 Validation Loss: 0.754170835018158\n",
      "Epoch 7218: Training Loss: 0.06543323894341786 Validation Loss: 0.7549818158149719\n",
      "Epoch 7219: Training Loss: 0.06546994919578235 Validation Loss: 0.7575753331184387\n",
      "Epoch 7220: Training Loss: 0.06543031955758731 Validation Loss: 0.7563297152519226\n",
      "Epoch 7221: Training Loss: 0.0653980125983556 Validation Loss: 0.7567523121833801\n",
      "Epoch 7222: Training Loss: 0.06548679744203885 Validation Loss: 0.7551959156990051\n",
      "Epoch 7223: Training Loss: 0.06551747148235638 Validation Loss: 0.7544474005699158\n",
      "Epoch 7224: Training Loss: 0.06540726001063983 Validation Loss: 0.756596028804779\n",
      "Epoch 7225: Training Loss: 0.06537601475914319 Validation Loss: 0.7573007345199585\n",
      "Epoch 7226: Training Loss: 0.06548476343353589 Validation Loss: 0.75919109582901\n",
      "Epoch 7227: Training Loss: 0.0655240739385287 Validation Loss: 0.7596333622932434\n",
      "Epoch 7228: Training Loss: 0.06534375250339508 Validation Loss: 0.7576346397399902\n",
      "Epoch 7229: Training Loss: 0.06529247388243675 Validation Loss: 0.7556626796722412\n",
      "Epoch 7230: Training Loss: 0.06567185247937839 Validation Loss: 0.7513208985328674\n",
      "Epoch 7231: Training Loss: 0.06553452461957932 Validation Loss: 0.7497938871383667\n",
      "Epoch 7232: Training Loss: 0.06559043874343236 Validation Loss: 0.7550963163375854\n",
      "Epoch 7233: Training Loss: 0.06542537237207095 Validation Loss: 0.757979154586792\n",
      "Epoch 7234: Training Loss: 0.06546495233972867 Validation Loss: 0.7598622441291809\n",
      "Epoch 7235: Training Loss: 0.0653287706275781 Validation Loss: 0.7581058144569397\n",
      "Epoch 7236: Training Loss: 0.06574699655175209 Validation Loss: 0.752959668636322\n",
      "Epoch 7237: Training Loss: 0.06532751023769379 Validation Loss: 0.752731204032898\n",
      "Epoch 7238: Training Loss: 0.06531607483824094 Validation Loss: 0.7552104592323303\n",
      "Epoch 7239: Training Loss: 0.0653995784620444 Validation Loss: 0.7540438175201416\n",
      "Epoch 7240: Training Loss: 0.06531256685654323 Validation Loss: 0.7556563019752502\n",
      "Epoch 7241: Training Loss: 0.06536634018023808 Validation Loss: 0.7554854154586792\n",
      "Epoch 7242: Training Loss: 0.06525563821196556 Validation Loss: 0.7572200298309326\n",
      "Epoch 7243: Training Loss: 0.06532913198073705 Validation Loss: 0.7609254717826843\n",
      "Epoch 7244: Training Loss: 0.06535205245018005 Validation Loss: 0.7613974809646606\n",
      "Epoch 7245: Training Loss: 0.0654213863114516 Validation Loss: 0.7606072425842285\n",
      "Epoch 7246: Training Loss: 0.06518396859367688 Validation Loss: 0.7571197748184204\n",
      "Epoch 7247: Training Loss: 0.06514175608754158 Validation Loss: 0.7540434002876282\n",
      "Epoch 7248: Training Loss: 0.06532412146528561 Validation Loss: 0.751086413860321\n",
      "Epoch 7249: Training Loss: 0.06527300179004669 Validation Loss: 0.7535711526870728\n",
      "Epoch 7250: Training Loss: 0.0653714972237746 Validation Loss: 0.7540072202682495\n",
      "Epoch 7251: Training Loss: 0.06514790902535121 Validation Loss: 0.7568484544754028\n",
      "Epoch 7252: Training Loss: 0.06516080349683762 Validation Loss: 0.7589837908744812\n",
      "Epoch 7253: Training Loss: 0.06538265198469162 Validation Loss: 0.757005512714386\n",
      "Epoch 7254: Training Loss: 0.06515433639287949 Validation Loss: 0.7575637698173523\n",
      "Epoch 7255: Training Loss: 0.06517502168814342 Validation Loss: 0.7590703964233398\n",
      "Epoch 7256: Training Loss: 0.06512754783034325 Validation Loss: 0.7570974826812744\n",
      "Epoch 7257: Training Loss: 0.06513731678326924 Validation Loss: 0.7554770708084106\n",
      "Epoch 7258: Training Loss: 0.06516948093970616 Validation Loss: 0.7562157511711121\n",
      "Epoch 7259: Training Loss: 0.06512889514366786 Validation Loss: 0.7565324902534485\n",
      "Epoch 7260: Training Loss: 0.06511827061573665 Validation Loss: 0.7568218111991882\n",
      "Epoch 7261: Training Loss: 0.06525269150733948 Validation Loss: 0.7545272707939148\n",
      "Epoch 7262: Training Loss: 0.0651717372238636 Validation Loss: 0.7537251710891724\n",
      "Epoch 7263: Training Loss: 0.0651486466328303 Validation Loss: 0.7566644549369812\n",
      "Epoch 7264: Training Loss: 0.06505911548932393 Validation Loss: 0.7566694021224976\n",
      "Epoch 7265: Training Loss: 0.06512707968552907 Validation Loss: 0.7582972049713135\n",
      "Epoch 7266: Training Loss: 0.06554186840852101 Validation Loss: 0.7540987133979797\n",
      "Epoch 7267: Training Loss: 0.0651046521961689 Validation Loss: 0.7553437352180481\n",
      "Epoch 7268: Training Loss: 0.0650399128595988 Validation Loss: 0.7564115524291992\n",
      "Epoch 7269: Training Loss: 0.06503412003318469 Validation Loss: 0.7589665651321411\n",
      "Epoch 7270: Training Loss: 0.06501150752107303 Validation Loss: 0.7600514888763428\n",
      "Epoch 7271: Training Loss: 0.06514239311218262 Validation Loss: 0.7608106732368469\n",
      "Epoch 7272: Training Loss: 0.06522696216901143 Validation Loss: 0.7603791952133179\n",
      "Epoch 7273: Training Loss: 0.06516073023279507 Validation Loss: 0.7547695636749268\n",
      "Epoch 7274: Training Loss: 0.06519647936026256 Validation Loss: 0.7498083710670471\n",
      "Epoch 7275: Training Loss: 0.06511774534980456 Validation Loss: 0.7519038319587708\n",
      "Epoch 7276: Training Loss: 0.06516870359579723 Validation Loss: 0.7525107860565186\n",
      "Epoch 7277: Training Loss: 0.06501418848832448 Validation Loss: 0.7576104402542114\n",
      "Epoch 7278: Training Loss: 0.06508009508252144 Validation Loss: 0.7609245777130127\n",
      "Epoch 7279: Training Loss: 0.06506275261441867 Validation Loss: 0.7615394592285156\n",
      "Epoch 7280: Training Loss: 0.06496708219250043 Validation Loss: 0.7597920298576355\n",
      "Epoch 7281: Training Loss: 0.06493368248144786 Validation Loss: 0.7577029466629028\n",
      "Epoch 7282: Training Loss: 0.06524785608053207 Validation Loss: 0.7533524036407471\n",
      "Epoch 7283: Training Loss: 0.06495812783638637 Validation Loss: 0.7543979287147522\n",
      "Epoch 7284: Training Loss: 0.06510241453846295 Validation Loss: 0.7582916021347046\n",
      "Epoch 7285: Training Loss: 0.06499768048524857 Validation Loss: 0.7563912272453308\n",
      "Epoch 7286: Training Loss: 0.06521186729272206 Validation Loss: 0.7537127137184143\n",
      "Epoch 7287: Training Loss: 0.06500033165017764 Validation Loss: 0.7582597136497498\n",
      "Epoch 7288: Training Loss: 0.06486052523056667 Validation Loss: 0.75904381275177\n",
      "Epoch 7289: Training Loss: 0.06499311824639638 Validation Loss: 0.7581209540367126\n",
      "Epoch 7290: Training Loss: 0.0649321936070919 Validation Loss: 0.7561716437339783\n",
      "Epoch 7291: Training Loss: 0.06494845325748126 Validation Loss: 0.7588675618171692\n",
      "Epoch 7292: Training Loss: 0.0648260513941447 Validation Loss: 0.7587936520576477\n",
      "Epoch 7293: Training Loss: 0.06481072182456653 Validation Loss: 0.7583930492401123\n",
      "Epoch 7294: Training Loss: 0.06512634456157684 Validation Loss: 0.7539849877357483\n",
      "Epoch 7295: Training Loss: 0.0649745911359787 Validation Loss: 0.7541484236717224\n",
      "Epoch 7296: Training Loss: 0.06493584314982097 Validation Loss: 0.7575968503952026\n",
      "Epoch 7297: Training Loss: 0.06484659637014072 Validation Loss: 0.7592343091964722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7298: Training Loss: 0.06488092243671417 Validation Loss: 0.760470986366272\n",
      "Epoch 7299: Training Loss: 0.06501789391040802 Validation Loss: 0.7556211948394775\n",
      "Epoch 7300: Training Loss: 0.06479004894693692 Validation Loss: 0.7543430328369141\n",
      "Epoch 7301: Training Loss: 0.06492714087168376 Validation Loss: 0.7545244097709656\n",
      "Epoch 7302: Training Loss: 0.06479017684857051 Validation Loss: 0.754984974861145\n",
      "Epoch 7303: Training Loss: 0.06475437680880229 Validation Loss: 0.7577769160270691\n",
      "Epoch 7304: Training Loss: 0.06479782362778981 Validation Loss: 0.7564361691474915\n",
      "Epoch 7305: Training Loss: 0.06469490999976794 Validation Loss: 0.757793664932251\n",
      "Epoch 7306: Training Loss: 0.06487155457337697 Validation Loss: 0.756085991859436\n",
      "Epoch 7307: Training Loss: 0.0647080031534036 Validation Loss: 0.7585168480873108\n",
      "Epoch 7308: Training Loss: 0.0648262823621432 Validation Loss: 0.7603985667228699\n",
      "Epoch 7309: Training Loss: 0.06489378213882446 Validation Loss: 0.757447361946106\n",
      "Epoch 7310: Training Loss: 0.0647481307387352 Validation Loss: 0.7592357993125916\n",
      "Epoch 7311: Training Loss: 0.06472965081532796 Validation Loss: 0.7571473121643066\n",
      "Epoch 7312: Training Loss: 0.06473587701718013 Validation Loss: 0.7556917667388916\n",
      "Epoch 7313: Training Loss: 0.06479471549391747 Validation Loss: 0.7585890889167786\n",
      "Epoch 7314: Training Loss: 0.06462398047248523 Validation Loss: 0.7581509351730347\n",
      "Epoch 7315: Training Loss: 0.06474595765272777 Validation Loss: 0.7551954388618469\n",
      "Epoch 7316: Training Loss: 0.06472200403610866 Validation Loss: 0.7541847229003906\n",
      "Epoch 7317: Training Loss: 0.06489648794134457 Validation Loss: 0.7588350176811218\n",
      "Epoch 7318: Training Loss: 0.06478800748785336 Validation Loss: 0.7610102295875549\n",
      "Epoch 7319: Training Loss: 0.06463207056125005 Validation Loss: 0.7592683434486389\n",
      "Epoch 7320: Training Loss: 0.06464080388347308 Validation Loss: 0.7565934658050537\n",
      "Epoch 7321: Training Loss: 0.06473637620608012 Validation Loss: 0.7539408802986145\n",
      "Epoch 7322: Training Loss: 0.06482405960559845 Validation Loss: 0.756014347076416\n",
      "Epoch 7323: Training Loss: 0.06474251672625542 Validation Loss: 0.7587134838104248\n",
      "Epoch 7324: Training Loss: 0.06464006503423055 Validation Loss: 0.7592145204544067\n",
      "Epoch 7325: Training Loss: 0.06482518836855888 Validation Loss: 0.7546409368515015\n",
      "Epoch 7326: Training Loss: 0.06469221661488216 Validation Loss: 0.756821870803833\n",
      "Epoch 7327: Training Loss: 0.06456337620814641 Validation Loss: 0.7564886212348938\n",
      "Epoch 7328: Training Loss: 0.06462118898828824 Validation Loss: 0.7577430009841919\n",
      "Epoch 7329: Training Loss: 0.06455176944533984 Validation Loss: 0.7579762935638428\n",
      "Epoch 7330: Training Loss: 0.0644867258767287 Validation Loss: 0.7578412890434265\n",
      "Epoch 7331: Training Loss: 0.06454554075996081 Validation Loss: 0.7573568224906921\n",
      "Epoch 7332: Training Loss: 0.06459078565239906 Validation Loss: 0.7558222413063049\n",
      "Epoch 7333: Training Loss: 0.0645257905125618 Validation Loss: 0.7581063508987427\n",
      "Epoch 7334: Training Loss: 0.06450635194778442 Validation Loss: 0.7577446699142456\n",
      "Epoch 7335: Training Loss: 0.06452036276459694 Validation Loss: 0.7576919794082642\n",
      "Epoch 7336: Training Loss: 0.06461962560812633 Validation Loss: 0.7589395046234131\n",
      "Epoch 7337: Training Loss: 0.06450452283024788 Validation Loss: 0.7595802545547485\n",
      "Epoch 7338: Training Loss: 0.06451594208677609 Validation Loss: 0.759229838848114\n",
      "Epoch 7339: Training Loss: 0.06519831717014313 Validation Loss: 0.7520580291748047\n",
      "Epoch 7340: Training Loss: 0.06458313887317975 Validation Loss: 0.7549830675125122\n",
      "Epoch 7341: Training Loss: 0.0644639494518439 Validation Loss: 0.7565280795097351\n",
      "Epoch 7342: Training Loss: 0.06445445741216342 Validation Loss: 0.7589554190635681\n",
      "Epoch 7343: Training Loss: 0.0644029217461745 Validation Loss: 0.7595131993293762\n",
      "Epoch 7344: Training Loss: 0.0646010972559452 Validation Loss: 0.7561572790145874\n",
      "Epoch 7345: Training Loss: 0.06446002672115962 Validation Loss: 0.7578838467597961\n",
      "Epoch 7346: Training Loss: 0.06441336746017139 Validation Loss: 0.7580099701881409\n",
      "Epoch 7347: Training Loss: 0.06473649914065997 Validation Loss: 0.7547412514686584\n",
      "Epoch 7348: Training Loss: 0.06453054025769234 Validation Loss: 0.7573301792144775\n",
      "Epoch 7349: Training Loss: 0.06440320611000061 Validation Loss: 0.7584660053253174\n",
      "Epoch 7350: Training Loss: 0.06438197940587997 Validation Loss: 0.7590875029563904\n",
      "Epoch 7351: Training Loss: 0.06439805527528127 Validation Loss: 0.7594662308692932\n",
      "Epoch 7352: Training Loss: 0.06437969828645389 Validation Loss: 0.7568997740745544\n",
      "Epoch 7353: Training Loss: 0.06429802502195041 Validation Loss: 0.7568873763084412\n",
      "Epoch 7354: Training Loss: 0.06431667382518451 Validation Loss: 0.7570315599441528\n",
      "Epoch 7355: Training Loss: 0.06450586393475533 Validation Loss: 0.7543382048606873\n",
      "Epoch 7356: Training Loss: 0.06446419035394986 Validation Loss: 0.7575000524520874\n",
      "Epoch 7357: Training Loss: 0.06448277706901233 Validation Loss: 0.7594718933105469\n",
      "Epoch 7358: Training Loss: 0.06430113315582275 Validation Loss: 0.7585158944129944\n",
      "Epoch 7359: Training Loss: 0.06432800491650899 Validation Loss: 0.7574127912521362\n",
      "Epoch 7360: Training Loss: 0.06452985232075055 Validation Loss: 0.7548686861991882\n",
      "Epoch 7361: Training Loss: 0.06433260565002759 Validation Loss: 0.7579447627067566\n",
      "Epoch 7362: Training Loss: 0.06427926446000735 Validation Loss: 0.7581249475479126\n",
      "Epoch 7363: Training Loss: 0.0642751691242059 Validation Loss: 0.7596979737281799\n",
      "Epoch 7364: Training Loss: 0.064248892168204 Validation Loss: 0.7598941922187805\n",
      "Epoch 7365: Training Loss: 0.06425795083244641 Validation Loss: 0.7597718834877014\n",
      "Epoch 7366: Training Loss: 0.06431445603569348 Validation Loss: 0.7567063570022583\n",
      "Epoch 7367: Training Loss: 0.06425215179721515 Validation Loss: 0.7547975778579712\n",
      "Epoch 7368: Training Loss: 0.06422050793965657 Validation Loss: 0.7553980946540833\n",
      "Epoch 7369: Training Loss: 0.06419045602281888 Validation Loss: 0.7568400502204895\n",
      "Epoch 7370: Training Loss: 0.06441106895605724 Validation Loss: 0.7556890249252319\n",
      "Epoch 7371: Training Loss: 0.06451546649138133 Validation Loss: 0.7605075836181641\n",
      "Epoch 7372: Training Loss: 0.06429446488618851 Validation Loss: 0.7596753835678101\n",
      "Epoch 7373: Training Loss: 0.06426282599568367 Validation Loss: 0.7605891227722168\n",
      "Epoch 7374: Training Loss: 0.06419903039932251 Validation Loss: 0.7594159245491028\n",
      "Epoch 7375: Training Loss: 0.06459100047747295 Validation Loss: 0.7537433505058289\n",
      "Epoch 7376: Training Loss: 0.06417751188079517 Validation Loss: 0.755344808101654\n",
      "Epoch 7377: Training Loss: 0.06419289608796437 Validation Loss: 0.7572798132896423\n",
      "Epoch 7378: Training Loss: 0.06412062297264735 Validation Loss: 0.7584547400474548\n",
      "Epoch 7379: Training Loss: 0.06423781191309293 Validation Loss: 0.7605109214782715\n",
      "Epoch 7380: Training Loss: 0.06423396741350491 Validation Loss: 0.7609496116638184\n",
      "Epoch 7381: Training Loss: 0.06405925378203392 Validation Loss: 0.7587200999259949\n",
      "Epoch 7382: Training Loss: 0.06464282795786858 Validation Loss: 0.7529336214065552\n",
      "Epoch 7383: Training Loss: 0.06419083972771962 Validation Loss: 0.7536607384681702\n",
      "Epoch 7384: Training Loss: 0.06409780060251553 Validation Loss: 0.7562646269798279\n",
      "Epoch 7385: Training Loss: 0.06405353173613548 Validation Loss: 0.7594306468963623\n",
      "Epoch 7386: Training Loss: 0.0642825501660506 Validation Loss: 0.7632073760032654\n",
      "Epoch 7387: Training Loss: 0.06421565636992455 Validation Loss: 0.7622684240341187\n",
      "Epoch 7388: Training Loss: 0.06431640436251958 Validation Loss: 0.7563853859901428\n",
      "Epoch 7389: Training Loss: 0.06420819585522015 Validation Loss: 0.7561339735984802\n",
      "Epoch 7390: Training Loss: 0.06412361934781075 Validation Loss: 0.7574362754821777\n",
      "Epoch 7391: Training Loss: 0.06416341041525205 Validation Loss: 0.7554658055305481\n",
      "Epoch 7392: Training Loss: 0.06416202709078789 Validation Loss: 0.7551060914993286\n",
      "Epoch 7393: Training Loss: 0.06419350579380989 Validation Loss: 0.7599206566810608\n",
      "Epoch 7394: Training Loss: 0.06407865633567174 Validation Loss: 0.7618671655654907\n",
      "Epoch 7395: Training Loss: 0.06407569845517476 Validation Loss: 0.7621275782585144\n",
      "Epoch 7396: Training Loss: 0.06402014568448067 Validation Loss: 0.760465681552887\n",
      "Epoch 7397: Training Loss: 0.06425479426980019 Validation Loss: 0.7608750462532043\n",
      "Epoch 7398: Training Loss: 0.06393524259328842 Validation Loss: 0.75538170337677\n",
      "Epoch 7399: Training Loss: 0.06405652686953545 Validation Loss: 0.7522087693214417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7400: Training Loss: 0.0641004629433155 Validation Loss: 0.7529451847076416\n",
      "Epoch 7401: Training Loss: 0.0640465219815572 Validation Loss: 0.7553622126579285\n",
      "Epoch 7402: Training Loss: 0.06395626440644264 Validation Loss: 0.7561613321304321\n",
      "Epoch 7403: Training Loss: 0.06392577290534973 Validation Loss: 0.758163332939148\n",
      "Epoch 7404: Training Loss: 0.06391418973604839 Validation Loss: 0.7604365944862366\n",
      "Epoch 7405: Training Loss: 0.06414320568243663 Validation Loss: 0.7634698152542114\n",
      "Epoch 7406: Training Loss: 0.06406096369028091 Validation Loss: 0.7610531449317932\n",
      "Epoch 7407: Training Loss: 0.0641597496966521 Validation Loss: 0.7565630674362183\n",
      "Epoch 7408: Training Loss: 0.06418461725115776 Validation Loss: 0.7539369463920593\n",
      "Epoch 7409: Training Loss: 0.06430186207095782 Validation Loss: 0.7590278387069702\n",
      "Epoch 7410: Training Loss: 0.06389583150545756 Validation Loss: 0.7586163878440857\n",
      "Epoch 7411: Training Loss: 0.0639060450096925 Validation Loss: 0.7599320411682129\n",
      "Epoch 7412: Training Loss: 0.06421640391151111 Validation Loss: 0.762715220451355\n",
      "Epoch 7413: Training Loss: 0.06400423372785251 Validation Loss: 0.7588098049163818\n",
      "Epoch 7414: Training Loss: 0.06397806604703267 Validation Loss: 0.755450963973999\n",
      "Epoch 7415: Training Loss: 0.06392016261816025 Validation Loss: 0.7549248337745667\n",
      "Epoch 7416: Training Loss: 0.0639468381802241 Validation Loss: 0.7573708891868591\n",
      "Epoch 7417: Training Loss: 0.06387353191773097 Validation Loss: 0.7589162588119507\n",
      "Epoch 7418: Training Loss: 0.06387868026892345 Validation Loss: 0.7582393288612366\n",
      "Epoch 7419: Training Loss: 0.06381169458230336 Validation Loss: 0.7581297755241394\n",
      "Epoch 7420: Training Loss: 0.06391534085075061 Validation Loss: 0.7566013932228088\n",
      "Epoch 7421: Training Loss: 0.06381031995018323 Validation Loss: 0.7571427822113037\n",
      "Epoch 7422: Training Loss: 0.06418218960364659 Validation Loss: 0.7610412240028381\n",
      "Epoch 7423: Training Loss: 0.06382684285442035 Validation Loss: 0.7616176009178162\n",
      "Epoch 7424: Training Loss: 0.06409364814559619 Validation Loss: 0.7569266557693481\n",
      "Epoch 7425: Training Loss: 0.06388821825385094 Validation Loss: 0.7592035531997681\n",
      "Epoch 7426: Training Loss: 0.06401872634887695 Validation Loss: 0.7560659646987915\n",
      "Epoch 7427: Training Loss: 0.0637979507446289 Validation Loss: 0.7577939629554749\n",
      "Epoch 7428: Training Loss: 0.06374042977889378 Validation Loss: 0.7582044005393982\n",
      "Epoch 7429: Training Loss: 0.06380765015880267 Validation Loss: 0.7569523453712463\n",
      "Epoch 7430: Training Loss: 0.0638419712583224 Validation Loss: 0.7562206387519836\n",
      "Epoch 7431: Training Loss: 0.06373205905159314 Validation Loss: 0.7592206001281738\n",
      "Epoch 7432: Training Loss: 0.0637793888648351 Validation Loss: 0.7615045309066772\n",
      "Epoch 7433: Training Loss: 0.0637416032453378 Validation Loss: 0.760650098323822\n",
      "Epoch 7434: Training Loss: 0.06373063971598943 Validation Loss: 0.758505642414093\n",
      "Epoch 7435: Training Loss: 0.063710102190574 Validation Loss: 0.758561372756958\n",
      "Epoch 7436: Training Loss: 0.06376050040125847 Validation Loss: 0.7565363049507141\n",
      "Epoch 7437: Training Loss: 0.06373999391992886 Validation Loss: 0.7558808326721191\n",
      "Epoch 7438: Training Loss: 0.06404971952239673 Validation Loss: 0.7611994743347168\n",
      "Epoch 7439: Training Loss: 0.0637620637814204 Validation Loss: 0.7618560194969177\n",
      "Epoch 7440: Training Loss: 0.06383100897073746 Validation Loss: 0.761854350566864\n",
      "Epoch 7441: Training Loss: 0.06363286326328914 Validation Loss: 0.7588039636611938\n",
      "Epoch 7442: Training Loss: 0.06367360055446625 Validation Loss: 0.7555854320526123\n",
      "Epoch 7443: Training Loss: 0.06382704898715019 Validation Loss: 0.7540780305862427\n",
      "Epoch 7444: Training Loss: 0.06367865577340126 Validation Loss: 0.7564437985420227\n",
      "Epoch 7445: Training Loss: 0.06360080341498058 Validation Loss: 0.7576748132705688\n",
      "Epoch 7446: Training Loss: 0.06362894425789516 Validation Loss: 0.758542001247406\n",
      "Epoch 7447: Training Loss: 0.06363528842727344 Validation Loss: 0.7610985040664673\n",
      "Epoch 7448: Training Loss: 0.0636773370206356 Validation Loss: 0.762086033821106\n",
      "Epoch 7449: Training Loss: 0.063653614372015 Validation Loss: 0.7589150667190552\n",
      "Epoch 7450: Training Loss: 0.06354880705475807 Validation Loss: 0.7579922676086426\n",
      "Epoch 7451: Training Loss: 0.06354938820004463 Validation Loss: 0.7579736709594727\n",
      "Epoch 7452: Training Loss: 0.06369970738887787 Validation Loss: 0.756549060344696\n",
      "Epoch 7453: Training Loss: 0.06360592444737752 Validation Loss: 0.7585601210594177\n",
      "Epoch 7454: Training Loss: 0.06358238309621811 Validation Loss: 0.7596085071563721\n",
      "Epoch 7455: Training Loss: 0.0635838694870472 Validation Loss: 0.758022665977478\n",
      "Epoch 7456: Training Loss: 0.06353101506829262 Validation Loss: 0.7583080530166626\n",
      "Epoch 7457: Training Loss: 0.06370007619261742 Validation Loss: 0.7616507411003113\n",
      "Epoch 7458: Training Loss: 0.06361741448442142 Validation Loss: 0.761396586894989\n",
      "Epoch 7459: Training Loss: 0.06365845228234927 Validation Loss: 0.7617555856704712\n",
      "Epoch 7460: Training Loss: 0.06351347391804059 Validation Loss: 0.7570555806159973\n",
      "Epoch 7461: Training Loss: 0.06362609937787056 Validation Loss: 0.754124104976654\n",
      "Epoch 7462: Training Loss: 0.06386047850052516 Validation Loss: 0.7522850632667542\n",
      "Epoch 7463: Training Loss: 0.06368343656261762 Validation Loss: 0.756902813911438\n",
      "Epoch 7464: Training Loss: 0.06353746851285298 Validation Loss: 0.7606913447380066\n",
      "Epoch 7465: Training Loss: 0.06367315724492073 Validation Loss: 0.7591506838798523\n",
      "Epoch 7466: Training Loss: 0.0634674181540807 Validation Loss: 0.7610635757446289\n",
      "Epoch 7467: Training Loss: 0.06348704174160957 Validation Loss: 0.7626675963401794\n",
      "Epoch 7468: Training Loss: 0.0638794315358003 Validation Loss: 0.7585373520851135\n",
      "Epoch 7469: Training Loss: 0.0635979026556015 Validation Loss: 0.7557880878448486\n",
      "Epoch 7470: Training Loss: 0.06344219793876012 Validation Loss: 0.7581254243850708\n",
      "Epoch 7471: Training Loss: 0.063679705063502 Validation Loss: 0.7596969604492188\n",
      "Epoch 7472: Training Loss: 0.06343269472320874 Validation Loss: 0.7614079117774963\n",
      "Epoch 7473: Training Loss: 0.06339653953909874 Validation Loss: 0.7612649202346802\n",
      "Epoch 7474: Training Loss: 0.06341586013634999 Validation Loss: 0.7593492865562439\n",
      "Epoch 7475: Training Loss: 0.06344250837961833 Validation Loss: 0.7572548985481262\n",
      "Epoch 7476: Training Loss: 0.06370429322123528 Validation Loss: 0.7555045485496521\n",
      "Epoch 7477: Training Loss: 0.06345450505614281 Validation Loss: 0.7589617967605591\n",
      "Epoch 7478: Training Loss: 0.06334213415781657 Validation Loss: 0.7598046064376831\n",
      "Epoch 7479: Training Loss: 0.06370174263914426 Validation Loss: 0.7645727396011353\n",
      "Epoch 7480: Training Loss: 0.06373985980947812 Validation Loss: 0.7600109577178955\n",
      "Epoch 7481: Training Loss: 0.06359401096900304 Validation Loss: 0.7566166520118713\n",
      "Epoch 7482: Training Loss: 0.06347368781765302 Validation Loss: 0.7604170441627502\n",
      "Epoch 7483: Training Loss: 0.06348515798648198 Validation Loss: 0.7616044282913208\n",
      "Epoch 7484: Training Loss: 0.06347205117344856 Validation Loss: 0.7579029202461243\n",
      "Epoch 7485: Training Loss: 0.06328668569525082 Validation Loss: 0.7569002509117126\n",
      "Epoch 7486: Training Loss: 0.06338829919695854 Validation Loss: 0.7557588219642639\n",
      "Epoch 7487: Training Loss: 0.06332256893316905 Validation Loss: 0.7587480545043945\n",
      "Epoch 7488: Training Loss: 0.06333335985740025 Validation Loss: 0.7608736157417297\n",
      "Epoch 7489: Training Loss: 0.06332850580414136 Validation Loss: 0.7622902393341064\n",
      "Epoch 7490: Training Loss: 0.06336263567209244 Validation Loss: 0.7606315612792969\n",
      "Epoch 7491: Training Loss: 0.06321388979752858 Validation Loss: 0.7599897980690002\n",
      "Epoch 7492: Training Loss: 0.06337070092558861 Validation Loss: 0.757133424282074\n",
      "Epoch 7493: Training Loss: 0.06324686110019684 Validation Loss: 0.7575177550315857\n",
      "Epoch 7494: Training Loss: 0.0634153683980306 Validation Loss: 0.7603788375854492\n",
      "Epoch 7495: Training Loss: 0.06330349047978719 Validation Loss: 0.759492039680481\n",
      "Epoch 7496: Training Loss: 0.06323232005039851 Validation Loss: 0.7582144141197205\n",
      "Epoch 7497: Training Loss: 0.0631955328087012 Validation Loss: 0.7585873007774353\n",
      "Epoch 7498: Training Loss: 0.06357087939977646 Validation Loss: 0.7562720775604248\n",
      "Epoch 7499: Training Loss: 0.06320060292879741 Validation Loss: 0.7586024403572083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7500: Training Loss: 0.06327147036790848 Validation Loss: 0.7613998651504517\n",
      "Epoch 7501: Training Loss: 0.06330067167679469 Validation Loss: 0.7598643898963928\n",
      "Epoch 7502: Training Loss: 0.0632304884493351 Validation Loss: 0.7611832618713379\n",
      "Epoch 7503: Training Loss: 0.06342390552163124 Validation Loss: 0.7633227109909058\n",
      "Epoch 7504: Training Loss: 0.06334179763992627 Validation Loss: 0.7589378356933594\n",
      "Epoch 7505: Training Loss: 0.06315729891260465 Validation Loss: 0.7567859292030334\n",
      "Epoch 7506: Training Loss: 0.06318217515945435 Validation Loss: 0.7561675906181335\n",
      "Epoch 7507: Training Loss: 0.06318180014689763 Validation Loss: 0.7573028206825256\n",
      "Epoch 7508: Training Loss: 0.06325157110889752 Validation Loss: 0.7568916082382202\n",
      "Epoch 7509: Training Loss: 0.0633963868021965 Validation Loss: 0.7617303729057312\n",
      "Epoch 7510: Training Loss: 0.06311901410420735 Validation Loss: 0.7622884511947632\n",
      "Epoch 7511: Training Loss: 0.06314243624607722 Validation Loss: 0.7604121565818787\n",
      "Epoch 7512: Training Loss: 0.06314680725336075 Validation Loss: 0.7584167122840881\n",
      "Epoch 7513: Training Loss: 0.06307189539074898 Validation Loss: 0.7584308981895447\n",
      "Epoch 7514: Training Loss: 0.06311360994974773 Validation Loss: 0.7581762075424194\n",
      "Epoch 7515: Training Loss: 0.06304706757267316 Validation Loss: 0.7582189440727234\n",
      "Epoch 7516: Training Loss: 0.06330887600779533 Validation Loss: 0.7614828944206238\n",
      "Epoch 7517: Training Loss: 0.06305136904120445 Validation Loss: 0.7607229948043823\n",
      "Epoch 7518: Training Loss: 0.06308077399929364 Validation Loss: 0.7607817649841309\n",
      "Epoch 7519: Training Loss: 0.06308087582389514 Validation Loss: 0.7587637901306152\n",
      "Epoch 7520: Training Loss: 0.06302068134148915 Validation Loss: 0.75806725025177\n",
      "Epoch 7521: Training Loss: 0.06309713174899419 Validation Loss: 0.7592010498046875\n",
      "Epoch 7522: Training Loss: 0.06342516466975212 Validation Loss: 0.7624877691268921\n",
      "Epoch 7523: Training Loss: 0.06307339171568553 Validation Loss: 0.7592790722846985\n",
      "Epoch 7524: Training Loss: 0.06313278277715047 Validation Loss: 0.7570295333862305\n",
      "Epoch 7525: Training Loss: 0.06302112589279811 Validation Loss: 0.7567177414894104\n",
      "Epoch 7526: Training Loss: 0.06308294584353764 Validation Loss: 0.7581103444099426\n",
      "Epoch 7527: Training Loss: 0.06298494090636571 Validation Loss: 0.7589156031608582\n",
      "Epoch 7528: Training Loss: 0.06307843948403995 Validation Loss: 0.7586957812309265\n",
      "Epoch 7529: Training Loss: 0.06296543901165326 Validation Loss: 0.7619584798812866\n",
      "Epoch 7530: Training Loss: 0.06305943801999092 Validation Loss: 0.7639620900154114\n",
      "Epoch 7531: Training Loss: 0.06319122513135274 Validation Loss: 0.7642792463302612\n",
      "Epoch 7532: Training Loss: 0.06298989057540894 Validation Loss: 0.7610194683074951\n",
      "Epoch 7533: Training Loss: 0.063102920850118 Validation Loss: 0.7559301257133484\n",
      "Epoch 7534: Training Loss: 0.06296083082755406 Validation Loss: 0.7553690671920776\n",
      "Epoch 7535: Training Loss: 0.06307366242011388 Validation Loss: 0.7545844912528992\n",
      "Epoch 7536: Training Loss: 0.06297179559866588 Validation Loss: 0.7578570246696472\n",
      "Epoch 7537: Training Loss: 0.06291115159789722 Validation Loss: 0.7609517574310303\n",
      "Epoch 7538: Training Loss: 0.06306332970658939 Validation Loss: 0.7654253840446472\n",
      "Epoch 7539: Training Loss: 0.06316416834791501 Validation Loss: 0.7620285749435425\n",
      "Epoch 7540: Training Loss: 0.06297559539477031 Validation Loss: 0.7605219483375549\n",
      "Epoch 7541: Training Loss: 0.0630435881515344 Validation Loss: 0.7621505856513977\n",
      "Epoch 7542: Training Loss: 0.06297782311836879 Validation Loss: 0.7621673345565796\n",
      "Epoch 7543: Training Loss: 0.06291629001498222 Validation Loss: 0.758743405342102\n",
      "Epoch 7544: Training Loss: 0.06314946586887042 Validation Loss: 0.7545661926269531\n",
      "Epoch 7545: Training Loss: 0.06292239452401797 Validation Loss: 0.7562403678894043\n",
      "Epoch 7546: Training Loss: 0.06291693076491356 Validation Loss: 0.7589609622955322\n",
      "Epoch 7547: Training Loss: 0.06282369171579678 Validation Loss: 0.7606035470962524\n",
      "Epoch 7548: Training Loss: 0.06295571352044742 Validation Loss: 0.7620084285736084\n",
      "Epoch 7549: Training Loss: 0.06291570514440536 Validation Loss: 0.7623257040977478\n",
      "Epoch 7550: Training Loss: 0.0628653069337209 Validation Loss: 0.7633956074714661\n",
      "Epoch 7551: Training Loss: 0.06283861771225929 Validation Loss: 0.7604045867919922\n",
      "Epoch 7552: Training Loss: 0.06281683097283046 Validation Loss: 0.7584445476531982\n",
      "Epoch 7553: Training Loss: 0.06283926591277122 Validation Loss: 0.7591487169265747\n",
      "Epoch 7554: Training Loss: 0.06276381760835648 Validation Loss: 0.7585779428482056\n",
      "Epoch 7555: Training Loss: 0.06281114245454471 Validation Loss: 0.7603636384010315\n",
      "Epoch 7556: Training Loss: 0.06295196091135342 Validation Loss: 0.7580350637435913\n",
      "Epoch 7557: Training Loss: 0.06277276078859965 Validation Loss: 0.7593562006950378\n",
      "Epoch 7558: Training Loss: 0.06272110342979431 Validation Loss: 0.7597497701644897\n",
      "Epoch 7559: Training Loss: 0.06274731581409772 Validation Loss: 0.7598430514335632\n",
      "Epoch 7560: Training Loss: 0.06280219927430153 Validation Loss: 0.762549877166748\n",
      "Epoch 7561: Training Loss: 0.06282082696755727 Validation Loss: 0.7627706527709961\n",
      "Epoch 7562: Training Loss: 0.06278803820411365 Validation Loss: 0.7594300508499146\n",
      "Epoch 7563: Training Loss: 0.06270822634299596 Validation Loss: 0.7586824297904968\n",
      "Epoch 7564: Training Loss: 0.06269692877928416 Validation Loss: 0.7582589983940125\n",
      "Epoch 7565: Training Loss: 0.0627462553481261 Validation Loss: 0.7596080899238586\n",
      "Epoch 7566: Training Loss: 0.06280731658140819 Validation Loss: 0.7579078078269958\n",
      "Epoch 7567: Training Loss: 0.06270118181904157 Validation Loss: 0.7592076659202576\n",
      "Epoch 7568: Training Loss: 0.06277020896474521 Validation Loss: 0.758293092250824\n",
      "Epoch 7569: Training Loss: 0.06273111080129941 Validation Loss: 0.7605046033859253\n",
      "Epoch 7570: Training Loss: 0.06264170507589976 Validation Loss: 0.7606591582298279\n",
      "Epoch 7571: Training Loss: 0.06290712455908458 Validation Loss: 0.763085126876831\n",
      "Epoch 7572: Training Loss: 0.06287839636206627 Validation Loss: 0.7590160965919495\n",
      "Epoch 7573: Training Loss: 0.06265141690770785 Validation Loss: 0.7577869296073914\n",
      "Epoch 7574: Training Loss: 0.06266730651259422 Validation Loss: 0.757434606552124\n",
      "Epoch 7575: Training Loss: 0.0626541996995608 Validation Loss: 0.7580035328865051\n",
      "Epoch 7576: Training Loss: 0.06282977635661761 Validation Loss: 0.7618345618247986\n",
      "Epoch 7577: Training Loss: 0.06271020695567131 Validation Loss: 0.7611668109893799\n",
      "Epoch 7578: Training Loss: 0.06274459883570671 Validation Loss: 0.7633090019226074\n",
      "Epoch 7579: Training Loss: 0.06258927906552951 Validation Loss: 0.7622271180152893\n",
      "Epoch 7580: Training Loss: 0.06266152362028758 Validation Loss: 0.7592877745628357\n",
      "Epoch 7581: Training Loss: 0.06259212767084439 Validation Loss: 0.7585321664810181\n",
      "Epoch 7582: Training Loss: 0.06275749454895656 Validation Loss: 0.7607668042182922\n",
      "Epoch 7583: Training Loss: 0.06257366885741551 Validation Loss: 0.7614562511444092\n",
      "Epoch 7584: Training Loss: 0.06259842341144879 Validation Loss: 0.7604831457138062\n",
      "Epoch 7585: Training Loss: 0.06257461756467819 Validation Loss: 0.7610782384872437\n",
      "Epoch 7586: Training Loss: 0.06270620971918106 Validation Loss: 0.7577269077301025\n",
      "Epoch 7587: Training Loss: 0.0625264048576355 Validation Loss: 0.7577744722366333\n",
      "Epoch 7588: Training Loss: 0.06259712825218837 Validation Loss: 0.758037269115448\n",
      "Epoch 7589: Training Loss: 0.06272709990541141 Validation Loss: 0.7614505290985107\n",
      "Epoch 7590: Training Loss: 0.06281691665450732 Validation Loss: 0.7644672989845276\n",
      "Epoch 7591: Training Loss: 0.06260824203491211 Validation Loss: 0.7613325715065002\n",
      "Epoch 7592: Training Loss: 0.06261434033513069 Validation Loss: 0.7618980407714844\n",
      "Epoch 7593: Training Loss: 0.06249855582912763 Validation Loss: 0.759152352809906\n",
      "Epoch 7594: Training Loss: 0.06258476649721463 Validation Loss: 0.7601852416992188\n",
      "Epoch 7595: Training Loss: 0.06256674354275067 Validation Loss: 0.7595497369766235\n",
      "Epoch 7596: Training Loss: 0.06273762633403142 Validation Loss: 0.7566813826560974\n",
      "Epoch 7597: Training Loss: 0.0624847412109375 Validation Loss: 0.758098304271698\n",
      "Epoch 7598: Training Loss: 0.06264647468924522 Validation Loss: 0.7620288133621216\n",
      "Epoch 7599: Training Loss: 0.06243746603528658 Validation Loss: 0.7628586888313293\n",
      "Epoch 7600: Training Loss: 0.06242381160457929 Validation Loss: 0.7625497579574585\n",
      "Epoch 7601: Training Loss: 0.06249394888679186 Validation Loss: 0.7625846266746521\n",
      "Epoch 7602: Training Loss: 0.0624437170724074 Validation Loss: 0.7606130242347717\n",
      "Epoch 7603: Training Loss: 0.06251807510852814 Validation Loss: 0.7574145793914795\n",
      "Epoch 7604: Training Loss: 0.06259575362006824 Validation Loss: 0.7565572261810303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7605: Training Loss: 0.06243781248728434 Validation Loss: 0.757470428943634\n",
      "Epoch 7606: Training Loss: 0.0624208798011144 Validation Loss: 0.7613092660903931\n",
      "Epoch 7607: Training Loss: 0.062408171594142914 Validation Loss: 0.7630959749221802\n",
      "Epoch 7608: Training Loss: 0.06276997923851013 Validation Loss: 0.7666090130805969\n",
      "Epoch 7609: Training Loss: 0.06246100986997286 Validation Loss: 0.7647697329521179\n",
      "Epoch 7610: Training Loss: 0.062392544001340866 Validation Loss: 0.7614933252334595\n",
      "Epoch 7611: Training Loss: 0.06246664250890414 Validation Loss: 0.7576479315757751\n",
      "Epoch 7612: Training Loss: 0.06242170060674349 Validation Loss: 0.7574202418327332\n",
      "Epoch 7613: Training Loss: 0.062388346840937935 Validation Loss: 0.7561034560203552\n",
      "Epoch 7614: Training Loss: 0.062381708373626076 Validation Loss: 0.7562098503112793\n",
      "Epoch 7615: Training Loss: 0.06253121669093768 Validation Loss: 0.7612767219543457\n",
      "Epoch 7616: Training Loss: 0.06234097480773926 Validation Loss: 0.7611909508705139\n",
      "Epoch 7617: Training Loss: 0.06234486401081085 Validation Loss: 0.7606950998306274\n",
      "Epoch 7618: Training Loss: 0.06275790184736252 Validation Loss: 0.7644277215003967\n",
      "Epoch 7619: Training Loss: 0.06253036980827649 Validation Loss: 0.7602118849754333\n",
      "Epoch 7620: Training Loss: 0.062278807163238525 Validation Loss: 0.7603092789649963\n",
      "Epoch 7621: Training Loss: 0.062393984446922936 Validation Loss: 0.7603047490119934\n",
      "Epoch 7622: Training Loss: 0.06233165537317594 Validation Loss: 0.7593421936035156\n",
      "Epoch 7623: Training Loss: 0.062291560073693596 Validation Loss: 0.7580783367156982\n",
      "Epoch 7624: Training Loss: 0.06254131098588307 Validation Loss: 0.7616836428642273\n",
      "Epoch 7625: Training Loss: 0.06224652503927549 Validation Loss: 0.7618510723114014\n",
      "Epoch 7626: Training Loss: 0.062331926077604294 Validation Loss: 0.7597540616989136\n",
      "Epoch 7627: Training Loss: 0.06261516238252322 Validation Loss: 0.7633020877838135\n",
      "Epoch 7628: Training Loss: 0.062308828035990395 Validation Loss: 0.7603369951248169\n",
      "Epoch 7629: Training Loss: 0.06273006151119868 Validation Loss: 0.7558838725090027\n",
      "Epoch 7630: Training Loss: 0.06258292620380719 Validation Loss: 0.7549646496772766\n",
      "Epoch 7631: Training Loss: 0.062444935242335 Validation Loss: 0.7617934942245483\n",
      "Epoch 7632: Training Loss: 0.062178721030553184 Validation Loss: 0.765035092830658\n",
      "Epoch 7633: Training Loss: 0.06235964596271515 Validation Loss: 0.7670936584472656\n",
      "Epoch 7634: Training Loss: 0.062498548378547035 Validation Loss: 0.7625910639762878\n",
      "Epoch 7635: Training Loss: 0.06232206771771113 Validation Loss: 0.7605279684066772\n",
      "Epoch 7636: Training Loss: 0.062236676613489784 Validation Loss: 0.7594162821769714\n",
      "Epoch 7637: Training Loss: 0.062201538433631264 Validation Loss: 0.7596643567085266\n",
      "Epoch 7638: Training Loss: 0.06221494823694229 Validation Loss: 0.7613967657089233\n",
      "Epoch 7639: Training Loss: 0.0621918278435866 Validation Loss: 0.7622416019439697\n",
      "Epoch 7640: Training Loss: 0.062419517586628594 Validation Loss: 0.7580334544181824\n",
      "Epoch 7641: Training Loss: 0.062397781759500504 Validation Loss: 0.7610393762588501\n",
      "Epoch 7642: Training Loss: 0.06231019149223963 Validation Loss: 0.76152503490448\n",
      "Epoch 7643: Training Loss: 0.062098428606987 Validation Loss: 0.7609646320343018\n",
      "Epoch 7644: Training Loss: 0.062090897311766945 Validation Loss: 0.7598706483840942\n",
      "Epoch 7645: Training Loss: 0.062098524222771324 Validation Loss: 0.7599976658821106\n",
      "Epoch 7646: Training Loss: 0.06217109908660253 Validation Loss: 0.7583532333374023\n",
      "Epoch 7647: Training Loss: 0.06222230071822802 Validation Loss: 0.7588794827461243\n",
      "Epoch 7648: Training Loss: 0.06219693894187609 Validation Loss: 0.762339174747467\n",
      "Epoch 7649: Training Loss: 0.06219755237301191 Validation Loss: 0.7628113627433777\n",
      "Epoch 7650: Training Loss: 0.062069730212291084 Validation Loss: 0.7634280323982239\n",
      "Epoch 7651: Training Loss: 0.06207001333435377 Validation Loss: 0.7617713212966919\n",
      "Epoch 7652: Training Loss: 0.062308695167303085 Validation Loss: 0.7601395845413208\n",
      "Epoch 7653: Training Loss: 0.06204411263267199 Validation Loss: 0.760502278804779\n",
      "Epoch 7654: Training Loss: 0.06221631790200869 Validation Loss: 0.7593318819999695\n",
      "Epoch 7655: Training Loss: 0.062037998189528785 Validation Loss: 0.7598894834518433\n",
      "Epoch 7656: Training Loss: 0.06210532411932945 Validation Loss: 0.7594959735870361\n",
      "Epoch 7657: Training Loss: 0.06211350361506144 Validation Loss: 0.7602519989013672\n",
      "Epoch 7658: Training Loss: 0.062392293165127434 Validation Loss: 0.7648684978485107\n",
      "Epoch 7659: Training Loss: 0.062065575271844864 Validation Loss: 0.7644023895263672\n",
      "Epoch 7660: Training Loss: 0.06216471269726753 Validation Loss: 0.7644825577735901\n",
      "Epoch 7661: Training Loss: 0.06197826688488325 Validation Loss: 0.761107325553894\n",
      "Epoch 7662: Training Loss: 0.06202499195933342 Validation Loss: 0.7568193674087524\n",
      "Epoch 7663: Training Loss: 0.06207188218832016 Validation Loss: 0.7562116980552673\n",
      "Epoch 7664: Training Loss: 0.06207639972368876 Validation Loss: 0.7574736475944519\n",
      "Epoch 7665: Training Loss: 0.06224801763892174 Validation Loss: 0.756743848323822\n",
      "Epoch 7666: Training Loss: 0.062013321866591774 Validation Loss: 0.762387752532959\n",
      "Epoch 7667: Training Loss: 0.061939630657434464 Validation Loss: 0.7636013031005859\n",
      "Epoch 7668: Training Loss: 0.06213012586037318 Validation Loss: 0.7667259573936462\n",
      "Epoch 7669: Training Loss: 0.06206245223681132 Validation Loss: 0.7642315030097961\n",
      "Epoch 7670: Training Loss: 0.06211064135034879 Validation Loss: 0.7643537521362305\n",
      "Epoch 7671: Training Loss: 0.06199078013499578 Validation Loss: 0.7598875761032104\n",
      "Epoch 7672: Training Loss: 0.06190609186887741 Validation Loss: 0.7575218677520752\n",
      "Epoch 7673: Training Loss: 0.06198675309618314 Validation Loss: 0.7566555738449097\n",
      "Epoch 7674: Training Loss: 0.061971262097358704 Validation Loss: 0.7579271793365479\n",
      "Epoch 7675: Training Loss: 0.06194415564338366 Validation Loss: 0.7589579820632935\n",
      "Epoch 7676: Training Loss: 0.061968920131524406 Validation Loss: 0.762662410736084\n",
      "Epoch 7677: Training Loss: 0.0619154746333758 Validation Loss: 0.7644903063774109\n",
      "Epoch 7678: Training Loss: 0.062244207908709846 Validation Loss: 0.7675831317901611\n",
      "Epoch 7679: Training Loss: 0.06196341539422671 Validation Loss: 0.7661677002906799\n",
      "Epoch 7680: Training Loss: 0.06236802786588669 Validation Loss: 0.7585700750350952\n",
      "Epoch 7681: Training Loss: 0.06220695500572523 Validation Loss: 0.7550660371780396\n",
      "Epoch 7682: Training Loss: 0.062049744029839836 Validation Loss: 0.7569751739501953\n",
      "Epoch 7683: Training Loss: 0.06189324830969175 Validation Loss: 0.7608890533447266\n",
      "Epoch 7684: Training Loss: 0.061797590305407844 Validation Loss: 0.762846052646637\n",
      "Epoch 7685: Training Loss: 0.061927810311317444 Validation Loss: 0.7619013786315918\n",
      "Epoch 7686: Training Loss: 0.061823335786660515 Validation Loss: 0.763719379901886\n",
      "Epoch 7687: Training Loss: 0.061909809708595276 Validation Loss: 0.7627764940261841\n",
      "Epoch 7688: Training Loss: 0.06188258156180382 Validation Loss: 0.7622537612915039\n",
      "Epoch 7689: Training Loss: 0.061803835133711495 Validation Loss: 0.7616791725158691\n",
      "Epoch 7690: Training Loss: 0.06181184947490692 Validation Loss: 0.7622650265693665\n",
      "Epoch 7691: Training Loss: 0.06179464980959892 Validation Loss: 0.7605286836624146\n",
      "Epoch 7692: Training Loss: 0.061817785104115806 Validation Loss: 0.7604700326919556\n",
      "Epoch 7693: Training Loss: 0.061869931717713676 Validation Loss: 0.7620605826377869\n",
      "Epoch 7694: Training Loss: 0.06191879759232203 Validation Loss: 0.7644400000572205\n",
      "Epoch 7695: Training Loss: 0.06187246615688006 Validation Loss: 0.7617718577384949\n",
      "Epoch 7696: Training Loss: 0.0618189200758934 Validation Loss: 0.7610123753547668\n",
      "Epoch 7697: Training Loss: 0.061771163096030555 Validation Loss: 0.7605881094932556\n",
      "Epoch 7698: Training Loss: 0.06172889098525047 Validation Loss: 0.7612553834915161\n",
      "Epoch 7699: Training Loss: 0.061820552994807564 Validation Loss: 0.7606644630432129\n",
      "Epoch 7700: Training Loss: 0.061783219377199806 Validation Loss: 0.7621713280677795\n",
      "Epoch 7701: Training Loss: 0.061735800156990685 Validation Loss: 0.7607886791229248\n",
      "Epoch 7702: Training Loss: 0.06170030186573664 Validation Loss: 0.7601044774055481\n",
      "Epoch 7703: Training Loss: 0.06178802500168482 Validation Loss: 0.7602912187576294\n",
      "Epoch 7704: Training Loss: 0.06181041275461515 Validation Loss: 0.7631374001502991\n",
      "Epoch 7705: Training Loss: 0.061837409933408104 Validation Loss: 0.7626565098762512\n",
      "Epoch 7706: Training Loss: 0.06199344868461291 Validation Loss: 0.7594050168991089\n",
      "Epoch 7707: Training Loss: 0.06168336421251297 Validation Loss: 0.7595818638801575\n",
      "Epoch 7708: Training Loss: 0.06176107128461202 Validation Loss: 0.7628158926963806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7709: Training Loss: 0.06163748726248741 Validation Loss: 0.7631567120552063\n",
      "Epoch 7710: Training Loss: 0.06187089035908381 Validation Loss: 0.7607537508010864\n",
      "Epoch 7711: Training Loss: 0.06169745698571205 Validation Loss: 0.7621039152145386\n",
      "Epoch 7712: Training Loss: 0.06179940576354662 Validation Loss: 0.7632128596305847\n",
      "Epoch 7713: Training Loss: 0.061760105192661285 Validation Loss: 0.7626586556434631\n",
      "Epoch 7714: Training Loss: 0.06168047214547793 Validation Loss: 0.761677622795105\n",
      "Epoch 7715: Training Loss: 0.061647252490123115 Validation Loss: 0.760589599609375\n",
      "Epoch 7716: Training Loss: 0.06164435545603434 Validation Loss: 0.7615710496902466\n",
      "Epoch 7717: Training Loss: 0.06175759807229042 Validation Loss: 0.760254979133606\n",
      "Epoch 7718: Training Loss: 0.0618609848121802 Validation Loss: 0.7632829546928406\n",
      "Epoch 7719: Training Loss: 0.061621442437171936 Validation Loss: 0.7611212134361267\n",
      "Epoch 7720: Training Loss: 0.06160653382539749 Validation Loss: 0.7606170773506165\n",
      "Epoch 7721: Training Loss: 0.06179806093374888 Validation Loss: 0.7624139189720154\n",
      "Epoch 7722: Training Loss: 0.06160197158654531 Validation Loss: 0.7598665952682495\n",
      "Epoch 7723: Training Loss: 0.06165569648146629 Validation Loss: 0.7586333751678467\n",
      "Epoch 7724: Training Loss: 0.06158948938051859 Validation Loss: 0.7607396245002747\n",
      "Epoch 7725: Training Loss: 0.061610423028469086 Validation Loss: 0.7629514336585999\n",
      "Epoch 7726: Training Loss: 0.061755226304133735 Validation Loss: 0.7653949856758118\n",
      "Epoch 7727: Training Loss: 0.061578930666049324 Validation Loss: 0.7640333771705627\n",
      "Epoch 7728: Training Loss: 0.061538136253754296 Validation Loss: 0.7611146569252014\n",
      "Epoch 7729: Training Loss: 0.06199163074294726 Validation Loss: 0.757516086101532\n",
      "Epoch 7730: Training Loss: 0.06171452874938647 Validation Loss: 0.7607592940330505\n",
      "Epoch 7731: Training Loss: 0.06156762937704722 Validation Loss: 0.7611156702041626\n",
      "Epoch 7732: Training Loss: 0.06160885592301687 Validation Loss: 0.764339029788971\n",
      "Epoch 7733: Training Loss: 0.06155739103754362 Validation Loss: 0.7636938095092773\n",
      "Epoch 7734: Training Loss: 0.06172300254305204 Validation Loss: 0.7653899788856506\n",
      "Epoch 7735: Training Loss: 0.06156978756189346 Validation Loss: 0.7648389935493469\n",
      "Epoch 7736: Training Loss: 0.06157369166612625 Validation Loss: 0.7605382204055786\n",
      "Epoch 7737: Training Loss: 0.06148177261153857 Validation Loss: 0.7590716481208801\n",
      "Epoch 7738: Training Loss: 0.06149741758902868 Validation Loss: 0.7585562467575073\n",
      "Epoch 7739: Training Loss: 0.06148156523704529 Validation Loss: 0.7606664896011353\n",
      "Epoch 7740: Training Loss: 0.06145075708627701 Validation Loss: 0.7628510594367981\n",
      "Epoch 7741: Training Loss: 0.06159797931710879 Validation Loss: 0.7605445384979248\n",
      "Epoch 7742: Training Loss: 0.061471198995908104 Validation Loss: 0.7626593708992004\n",
      "Epoch 7743: Training Loss: 0.06140555813908577 Validation Loss: 0.7631511688232422\n",
      "Epoch 7744: Training Loss: 0.06145118921995163 Validation Loss: 0.7636538147926331\n",
      "Epoch 7745: Training Loss: 0.06146738057335218 Validation Loss: 0.7622246146202087\n",
      "Epoch 7746: Training Loss: 0.06154390921195348 Validation Loss: 0.7616979479789734\n",
      "Epoch 7747: Training Loss: 0.061520054936409 Validation Loss: 0.7595828175544739\n",
      "Epoch 7748: Training Loss: 0.06145540873209635 Validation Loss: 0.7608562111854553\n",
      "Epoch 7749: Training Loss: 0.06152492264906565 Validation Loss: 0.7637181878089905\n",
      "Epoch 7750: Training Loss: 0.061356292416652046 Validation Loss: 0.7630239129066467\n",
      "Epoch 7751: Training Loss: 0.06153758615255356 Validation Loss: 0.760202944278717\n",
      "Epoch 7752: Training Loss: 0.06144129360715548 Validation Loss: 0.7619209289550781\n",
      "Epoch 7753: Training Loss: 0.06139502674341202 Validation Loss: 0.7634825110435486\n",
      "Epoch 7754: Training Loss: 0.061503188063700996 Validation Loss: 0.7609580755233765\n",
      "Epoch 7755: Training Loss: 0.06134830042719841 Validation Loss: 0.7622593641281128\n",
      "Epoch 7756: Training Loss: 0.061298713088035583 Validation Loss: 0.762377917766571\n",
      "Epoch 7757: Training Loss: 0.061301372945308685 Validation Loss: 0.7626855373382568\n",
      "Epoch 7758: Training Loss: 0.06157049536705017 Validation Loss: 0.7645573616027832\n",
      "Epoch 7759: Training Loss: 0.06146358698606491 Validation Loss: 0.7612372636795044\n",
      "Epoch 7760: Training Loss: 0.061418451368808746 Validation Loss: 0.7589514255523682\n",
      "Epoch 7761: Training Loss: 0.06132957215110461 Validation Loss: 0.7605257630348206\n",
      "Epoch 7762: Training Loss: 0.061578319718440376 Validation Loss: 0.7639603614807129\n",
      "Epoch 7763: Training Loss: 0.06139329945047697 Validation Loss: 0.7639874219894409\n",
      "Epoch 7764: Training Loss: 0.06130651260415713 Validation Loss: 0.7623312473297119\n",
      "Epoch 7765: Training Loss: 0.06127398336927096 Validation Loss: 0.7621843814849854\n",
      "Epoch 7766: Training Loss: 0.06201744948824247 Validation Loss: 0.7574910521507263\n",
      "Epoch 7767: Training Loss: 0.06133147453268369 Validation Loss: 0.7610900402069092\n",
      "Epoch 7768: Training Loss: 0.06135302037000656 Validation Loss: 0.7647078037261963\n",
      "Epoch 7769: Training Loss: 0.061243110646804176 Validation Loss: 0.7657267451286316\n",
      "Epoch 7770: Training Loss: 0.06126889089743296 Validation Loss: 0.7650565505027771\n",
      "Epoch 7771: Training Loss: 0.061261922121047974 Validation Loss: 0.762790322303772\n",
      "Epoch 7772: Training Loss: 0.06128043681383133 Validation Loss: 0.7611211538314819\n",
      "Epoch 7773: Training Loss: 0.0612187422811985 Validation Loss: 0.7616438269615173\n",
      "Epoch 7774: Training Loss: 0.06128750493129095 Validation Loss: 0.7609460353851318\n",
      "Epoch 7775: Training Loss: 0.06118802229563395 Validation Loss: 0.7622947692871094\n",
      "Epoch 7776: Training Loss: 0.061352238059043884 Validation Loss: 0.7600379586219788\n",
      "Epoch 7777: Training Loss: 0.06121085832516352 Validation Loss: 0.7603386640548706\n",
      "Epoch 7778: Training Loss: 0.061628665775060654 Validation Loss: 0.7662730813026428\n",
      "Epoch 7779: Training Loss: 0.061230303098758064 Validation Loss: 0.7656447887420654\n",
      "Epoch 7780: Training Loss: 0.06138746812939644 Validation Loss: 0.7622388601303101\n",
      "Epoch 7781: Training Loss: 0.06114643563826879 Validation Loss: 0.761568009853363\n",
      "Epoch 7782: Training Loss: 0.06134264046947161 Validation Loss: 0.7608339786529541\n",
      "Epoch 7783: Training Loss: 0.06123084823290507 Validation Loss: 0.7599579691886902\n",
      "Epoch 7784: Training Loss: 0.06142079954346021 Validation Loss: 0.7618304491043091\n",
      "Epoch 7785: Training Loss: 0.061545116206010185 Validation Loss: 0.7666520476341248\n",
      "Epoch 7786: Training Loss: 0.06139985968669256 Validation Loss: 0.7626121044158936\n",
      "Epoch 7787: Training Loss: 0.0612262487411499 Validation Loss: 0.7644137740135193\n",
      "Epoch 7788: Training Loss: 0.06112401063243548 Validation Loss: 0.7620493173599243\n",
      "Epoch 7789: Training Loss: 0.06116228178143501 Validation Loss: 0.7604078650474548\n",
      "Epoch 7790: Training Loss: 0.061165296783049904 Validation Loss: 0.759443461894989\n",
      "Epoch 7791: Training Loss: 0.061226386576890945 Validation Loss: 0.7603558897972107\n",
      "Epoch 7792: Training Loss: 0.061063170433044434 Validation Loss: 0.762752890586853\n",
      "Epoch 7793: Training Loss: 0.06110156451662382 Validation Loss: 0.7622820734977722\n",
      "Epoch 7794: Training Loss: 0.06131273011366526 Validation Loss: 0.7612555027008057\n",
      "Epoch 7795: Training Loss: 0.06122241790095965 Validation Loss: 0.7647852897644043\n",
      "Epoch 7796: Training Loss: 0.06122787669301033 Validation Loss: 0.7667374610900879\n",
      "Epoch 7797: Training Loss: 0.061094955851634346 Validation Loss: 0.765865683555603\n",
      "Epoch 7798: Training Loss: 0.061052106320858 Validation Loss: 0.7635663151741028\n",
      "Epoch 7799: Training Loss: 0.06111600870887438 Validation Loss: 0.7591067552566528\n",
      "Epoch 7800: Training Loss: 0.06112300232052803 Validation Loss: 0.7579111456871033\n",
      "Epoch 7801: Training Loss: 0.06121528521180153 Validation Loss: 0.7603413462638855\n",
      "Epoch 7802: Training Loss: 0.06104495997230212 Validation Loss: 0.7622268199920654\n",
      "Epoch 7803: Training Loss: 0.061436803390582405 Validation Loss: 0.7667875289916992\n",
      "Epoch 7804: Training Loss: 0.06115116427342097 Validation Loss: 0.7651899456977844\n",
      "Epoch 7805: Training Loss: 0.06121178468068441 Validation Loss: 0.7663858532905579\n",
      "Epoch 7806: Training Loss: 0.061199004451433815 Validation Loss: 0.7607162594795227\n",
      "Epoch 7807: Training Loss: 0.061207069704929985 Validation Loss: 0.7577516436576843\n",
      "Epoch 7808: Training Loss: 0.06132908413807551 Validation Loss: 0.7617365121841431\n",
      "Epoch 7809: Training Loss: 0.0610058568418026 Validation Loss: 0.7629075646400452\n",
      "Epoch 7810: Training Loss: 0.06094437092542648 Validation Loss: 0.7636217474937439\n",
      "Epoch 7811: Training Loss: 0.0610901415348053 Validation Loss: 0.7614535689353943\n",
      "Epoch 7812: Training Loss: 0.06104939430952072 Validation Loss: 0.7640911936759949\n",
      "Epoch 7813: Training Loss: 0.06095770249764124 Validation Loss: 0.764305591583252\n",
      "Epoch 7814: Training Loss: 0.06100473180413246 Validation Loss: 0.765388548374176\n",
      "Epoch 7815: Training Loss: 0.061005186289548874 Validation Loss: 0.7641718983650208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7816: Training Loss: 0.06117581451932589 Validation Loss: 0.759947657585144\n",
      "Epoch 7817: Training Loss: 0.06099999447663625 Validation Loss: 0.759995698928833\n",
      "Epoch 7818: Training Loss: 0.060906545569499336 Validation Loss: 0.7619408965110779\n",
      "Epoch 7819: Training Loss: 0.060942752907673516 Validation Loss: 0.7645667195320129\n",
      "Epoch 7820: Training Loss: 0.060888842990001045 Validation Loss: 0.7644217610359192\n",
      "Epoch 7821: Training Loss: 0.06160400062799454 Validation Loss: 0.7598642110824585\n",
      "Epoch 7822: Training Loss: 0.06110494211316109 Validation Loss: 0.7645533680915833\n",
      "Epoch 7823: Training Loss: 0.060908580819765724 Validation Loss: 0.7663390636444092\n",
      "Epoch 7824: Training Loss: 0.060926989962657295 Validation Loss: 0.766392171382904\n",
      "Epoch 7825: Training Loss: 0.06091080978512764 Validation Loss: 0.7659852504730225\n",
      "Epoch 7826: Training Loss: 0.06102824707825979 Validation Loss: 0.7653361558914185\n",
      "Epoch 7827: Training Loss: 0.0608420285085837 Validation Loss: 0.7630998492240906\n",
      "Epoch 7828: Training Loss: 0.060951657593250275 Validation Loss: 0.759536623954773\n",
      "Epoch 7829: Training Loss: 0.06090028335650762 Validation Loss: 0.7576500177383423\n",
      "Epoch 7830: Training Loss: 0.06092189128200213 Validation Loss: 0.7582718729972839\n",
      "Epoch 7831: Training Loss: 0.06096053992708524 Validation Loss: 0.7619558572769165\n",
      "Epoch 7832: Training Loss: 0.06099407747387886 Validation Loss: 0.7656375765800476\n",
      "Epoch 7833: Training Loss: 0.060897987335920334 Validation Loss: 0.7630813717842102\n",
      "Epoch 7834: Training Loss: 0.06088406965136528 Validation Loss: 0.7642865180969238\n",
      "Epoch 7835: Training Loss: 0.06081665928165118 Validation Loss: 0.76280277967453\n",
      "Epoch 7836: Training Loss: 0.060913080970446266 Validation Loss: 0.7603593468666077\n",
      "Epoch 7837: Training Loss: 0.060778735826412834 Validation Loss: 0.7607128024101257\n",
      "Epoch 7838: Training Loss: 0.060822622229655586 Validation Loss: 0.7623275518417358\n",
      "Epoch 7839: Training Loss: 0.06094571575522423 Validation Loss: 0.7649732232093811\n",
      "Epoch 7840: Training Loss: 0.06134525934855143 Validation Loss: 0.7600011825561523\n",
      "Epoch 7841: Training Loss: 0.06083965798219045 Validation Loss: 0.7607724666595459\n",
      "Epoch 7842: Training Loss: 0.06087352087100347 Validation Loss: 0.7638328075408936\n",
      "Epoch 7843: Training Loss: 0.06082231675585111 Validation Loss: 0.7635047435760498\n",
      "Epoch 7844: Training Loss: 0.06086629008253416 Validation Loss: 0.7660766839981079\n",
      "Epoch 7845: Training Loss: 0.06079391141732534 Validation Loss: 0.7664551734924316\n",
      "Epoch 7846: Training Loss: 0.06088691453138987 Validation Loss: 0.7635698914527893\n",
      "Epoch 7847: Training Loss: 0.060945138335227966 Validation Loss: 0.7601786851882935\n",
      "Epoch 7848: Training Loss: 0.06075165048241615 Validation Loss: 0.7604941129684448\n",
      "Epoch 7849: Training Loss: 0.06080013886094093 Validation Loss: 0.7636182904243469\n",
      "Epoch 7850: Training Loss: 0.0607748677333196 Validation Loss: 0.7625100612640381\n",
      "Epoch 7851: Training Loss: 0.060688779999812446 Validation Loss: 0.7640285491943359\n",
      "Epoch 7852: Training Loss: 0.06069005156556765 Validation Loss: 0.7639362812042236\n",
      "Epoch 7853: Training Loss: 0.06067497283220291 Validation Loss: 0.7634130120277405\n",
      "Epoch 7854: Training Loss: 0.06069269155462583 Validation Loss: 0.7654696106910706\n",
      "Epoch 7855: Training Loss: 0.06070327882965406 Validation Loss: 0.7660993933677673\n",
      "Epoch 7856: Training Loss: 0.06077541535099348 Validation Loss: 0.7656811475753784\n",
      "Epoch 7857: Training Loss: 0.06072946762045225 Validation Loss: 0.7653489112854004\n",
      "Epoch 7858: Training Loss: 0.06068702042102814 Validation Loss: 0.7613751292228699\n",
      "Epoch 7859: Training Loss: 0.0607120506465435 Validation Loss: 0.7596551775932312\n",
      "Epoch 7860: Training Loss: 0.060990940779447556 Validation Loss: 0.7625328302383423\n",
      "Epoch 7861: Training Loss: 0.06082163626948992 Validation Loss: 0.760366678237915\n",
      "Epoch 7862: Training Loss: 0.06064505378405253 Validation Loss: 0.7626782059669495\n",
      "Epoch 7863: Training Loss: 0.06067917992671331 Validation Loss: 0.7647777199745178\n",
      "Epoch 7864: Training Loss: 0.06076475729544958 Validation Loss: 0.7666939496994019\n",
      "Epoch 7865: Training Loss: 0.06062323600053787 Validation Loss: 0.7659111022949219\n",
      "Epoch 7866: Training Loss: 0.06062837069233259 Validation Loss: 0.7625083327293396\n",
      "Epoch 7867: Training Loss: 0.06066645557681719 Validation Loss: 0.7610103487968445\n",
      "Epoch 7868: Training Loss: 0.06062134231130282 Validation Loss: 0.7611784934997559\n",
      "Epoch 7869: Training Loss: 0.060762482384840645 Validation Loss: 0.7591785192489624\n",
      "Epoch 7870: Training Loss: 0.06083396698037783 Validation Loss: 0.7637536525726318\n",
      "Epoch 7871: Training Loss: 0.060603355367978416 Validation Loss: 0.7637647390365601\n",
      "Epoch 7872: Training Loss: 0.06054305161039034 Validation Loss: 0.7639841437339783\n",
      "Epoch 7873: Training Loss: 0.060677909602721534 Validation Loss: 0.7659986019134521\n",
      "Epoch 7874: Training Loss: 0.060585678865512214 Validation Loss: 0.7636604309082031\n",
      "Epoch 7875: Training Loss: 0.060557023932536445 Validation Loss: 0.7636092901229858\n",
      "Epoch 7876: Training Loss: 0.060499221086502075 Validation Loss: 0.7625623345375061\n",
      "Epoch 7877: Training Loss: 0.06057571123043696 Validation Loss: 0.7615824937820435\n",
      "Epoch 7878: Training Loss: 0.06057848781347275 Validation Loss: 0.7629042863845825\n",
      "Epoch 7879: Training Loss: 0.0607754314939181 Validation Loss: 0.7609282732009888\n",
      "Epoch 7880: Training Loss: 0.06072634210189184 Validation Loss: 0.7655023336410522\n",
      "Epoch 7881: Training Loss: 0.06066730121771494 Validation Loss: 0.7675694227218628\n",
      "Epoch 7882: Training Loss: 0.06059629221757253 Validation Loss: 0.7682040929794312\n",
      "Epoch 7883: Training Loss: 0.06062693024675051 Validation Loss: 0.7673627734184265\n",
      "Epoch 7884: Training Loss: 0.060558125376701355 Validation Loss: 0.7621889710426331\n",
      "Epoch 7885: Training Loss: 0.06057151034474373 Validation Loss: 0.7607238292694092\n",
      "Epoch 7886: Training Loss: 0.06084489822387695 Validation Loss: 0.7576183080673218\n",
      "Epoch 7887: Training Loss: 0.060533749560515084 Validation Loss: 0.7597100138664246\n",
      "Epoch 7888: Training Loss: 0.06050152207414309 Validation Loss: 0.7628064751625061\n",
      "Epoch 7889: Training Loss: 0.06039449075857798 Validation Loss: 0.7643781900405884\n",
      "Epoch 7890: Training Loss: 0.06041586150725683 Validation Loss: 0.765042781829834\n",
      "Epoch 7891: Training Loss: 0.060562546054522194 Validation Loss: 0.7665824890136719\n",
      "Epoch 7892: Training Loss: 0.0605748084684213 Validation Loss: 0.7638529539108276\n",
      "Epoch 7893: Training Loss: 0.06039199729760488 Validation Loss: 0.7625123858451843\n",
      "Epoch 7894: Training Loss: 0.060615586737791695 Validation Loss: 0.7605617046356201\n",
      "Epoch 7895: Training Loss: 0.06046303113301595 Validation Loss: 0.7625632882118225\n",
      "Epoch 7896: Training Loss: 0.06043564404050509 Validation Loss: 0.7621939778327942\n",
      "Epoch 7897: Training Loss: 0.06049605458974838 Validation Loss: 0.7626404762268066\n",
      "Epoch 7898: Training Loss: 0.06041860828797022 Validation Loss: 0.7639530897140503\n",
      "Epoch 7899: Training Loss: 0.060871065904696785 Validation Loss: 0.7625011205673218\n",
      "Epoch 7900: Training Loss: 0.0604722760617733 Validation Loss: 0.7678908705711365\n",
      "Epoch 7901: Training Loss: 0.06051594018936157 Validation Loss: 0.7703238129615784\n",
      "Epoch 7902: Training Loss: 0.06069628273447355 Validation Loss: 0.7708341479301453\n",
      "Epoch 7903: Training Loss: 0.060789315650860466 Validation Loss: 0.7627798318862915\n",
      "Epoch 7904: Training Loss: 0.060335403929154076 Validation Loss: 0.7607828378677368\n",
      "Epoch 7905: Training Loss: 0.060350113858779274 Validation Loss: 0.7593722939491272\n",
      "Epoch 7906: Training Loss: 0.06042355547348658 Validation Loss: 0.7593299150466919\n",
      "Epoch 7907: Training Loss: 0.06042233978708585 Validation Loss: 0.7629417777061462\n",
      "Epoch 7908: Training Loss: 0.06032204876343409 Validation Loss: 0.7648534178733826\n",
      "Epoch 7909: Training Loss: 0.06028435379266739 Validation Loss: 0.766206681728363\n",
      "Epoch 7910: Training Loss: 0.0604720413684845 Validation Loss: 0.7685728073120117\n",
      "Epoch 7911: Training Loss: 0.06044899548093478 Validation Loss: 0.7638589143753052\n",
      "Epoch 7912: Training Loss: 0.06025016804536184 Validation Loss: 0.7628701329231262\n",
      "Epoch 7913: Training Loss: 0.060594684133927025 Validation Loss: 0.7588459849357605\n",
      "Epoch 7914: Training Loss: 0.06044629092017809 Validation Loss: 0.7623105049133301\n",
      "Epoch 7915: Training Loss: 0.06038030609488487 Validation Loss: 0.7618904709815979\n",
      "Epoch 7916: Training Loss: 0.060324444125096 Validation Loss: 0.7632703185081482\n",
      "Epoch 7917: Training Loss: 0.060340902457634606 Validation Loss: 0.767027735710144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7918: Training Loss: 0.06031963974237442 Validation Loss: 0.767877459526062\n",
      "Epoch 7919: Training Loss: 0.060456667095422745 Validation Loss: 0.7653107047080994\n",
      "Epoch 7920: Training Loss: 0.06020360067486763 Validation Loss: 0.7650415897369385\n",
      "Epoch 7921: Training Loss: 0.060234248638153076 Validation Loss: 0.7639996409416199\n",
      "Epoch 7922: Training Loss: 0.06025581310192744 Validation Loss: 0.7649078369140625\n",
      "Epoch 7923: Training Loss: 0.06046942497293154 Validation Loss: 0.7622759342193604\n",
      "Epoch 7924: Training Loss: 0.0602827196319898 Validation Loss: 0.7615640759468079\n",
      "Epoch 7925: Training Loss: 0.06063813467820486 Validation Loss: 0.7666067481040955\n",
      "Epoch 7926: Training Loss: 0.060346342623233795 Validation Loss: 0.7659746408462524\n",
      "Epoch 7927: Training Loss: 0.06024862453341484 Validation Loss: 0.7646988034248352\n",
      "Epoch 7928: Training Loss: 0.0602778693040212 Validation Loss: 0.761835515499115\n",
      "Epoch 7929: Training Loss: 0.060233067721128464 Validation Loss: 0.7611002922058105\n",
      "Epoch 7930: Training Loss: 0.06047236422697703 Validation Loss: 0.7655532360076904\n",
      "Epoch 7931: Training Loss: 0.06035371000568072 Validation Loss: 0.7626769542694092\n",
      "Epoch 7932: Training Loss: 0.0604691244661808 Validation Loss: 0.7603200674057007\n",
      "Epoch 7933: Training Loss: 0.06026247516274452 Validation Loss: 0.7630581259727478\n",
      "Epoch 7934: Training Loss: 0.06015365943312645 Validation Loss: 0.7636043429374695\n",
      "Epoch 7935: Training Loss: 0.06028921281298002 Validation Loss: 0.7664918899536133\n",
      "Epoch 7936: Training Loss: 0.06019278491536776 Validation Loss: 0.7663859724998474\n",
      "Epoch 7937: Training Loss: 0.06029899915059408 Validation Loss: 0.7668084502220154\n",
      "Epoch 7938: Training Loss: 0.060205429792404175 Validation Loss: 0.7624337673187256\n",
      "Epoch 7939: Training Loss: 0.06016011287768682 Validation Loss: 0.7635061144828796\n",
      "Epoch 7940: Training Loss: 0.0601088119049867 Validation Loss: 0.7625541090965271\n",
      "Epoch 7941: Training Loss: 0.06015475715200106 Validation Loss: 0.763332188129425\n",
      "Epoch 7942: Training Loss: 0.06012630835175514 Validation Loss: 0.7622731924057007\n",
      "Epoch 7943: Training Loss: 0.06012732659777006 Validation Loss: 0.7627861499786377\n",
      "Epoch 7944: Training Loss: 0.06011746699611346 Validation Loss: 0.7624620199203491\n",
      "Epoch 7945: Training Loss: 0.060073972990115486 Validation Loss: 0.7653483748435974\n",
      "Epoch 7946: Training Loss: 0.060120205084482826 Validation Loss: 0.7656739950180054\n",
      "Epoch 7947: Training Loss: 0.06011417259772619 Validation Loss: 0.7671085596084595\n",
      "Epoch 7948: Training Loss: 0.06011029829581579 Validation Loss: 0.7653361558914185\n",
      "Epoch 7949: Training Loss: 0.06010026981433233 Validation Loss: 0.765718936920166\n",
      "Epoch 7950: Training Loss: 0.06001696735620499 Validation Loss: 0.7652693390846252\n",
      "Epoch 7951: Training Loss: 0.06012226144472758 Validation Loss: 0.7662444710731506\n",
      "Epoch 7952: Training Loss: 0.06035183866818746 Validation Loss: 0.7628557682037354\n",
      "Epoch 7953: Training Loss: 0.06013364717364311 Validation Loss: 0.7647890448570251\n",
      "Epoch 7954: Training Loss: 0.06018627807497978 Validation Loss: 0.7664809226989746\n",
      "Epoch 7955: Training Loss: 0.06002744659781456 Validation Loss: 0.7640314102172852\n",
      "Epoch 7956: Training Loss: 0.060055095702409744 Validation Loss: 0.7623016834259033\n",
      "Epoch 7957: Training Loss: 0.060013920068740845 Validation Loss: 0.7611978054046631\n",
      "Epoch 7958: Training Loss: 0.06004348024725914 Validation Loss: 0.7601632475852966\n",
      "Epoch 7959: Training Loss: 0.060350158562262855 Validation Loss: 0.7647985219955444\n",
      "Epoch 7960: Training Loss: 0.060084640979766846 Validation Loss: 0.7639340162277222\n",
      "Epoch 7961: Training Loss: 0.060162853449583054 Validation Loss: 0.7670762538909912\n",
      "Epoch 7962: Training Loss: 0.060141004621982574 Validation Loss: 0.7680628299713135\n",
      "Epoch 7963: Training Loss: 0.06002081433931986 Validation Loss: 0.7664691209793091\n",
      "Epoch 7964: Training Loss: 0.05998804792761803 Validation Loss: 0.7623035907745361\n",
      "Epoch 7965: Training Loss: 0.05995762596527735 Validation Loss: 0.7623438835144043\n",
      "Epoch 7966: Training Loss: 0.059970578799645104 Validation Loss: 0.7609511017799377\n",
      "Epoch 7967: Training Loss: 0.060089978079001106 Validation Loss: 0.762326180934906\n",
      "Epoch 7968: Training Loss: 0.06008132671316465 Validation Loss: 0.7646464109420776\n",
      "Epoch 7969: Training Loss: 0.06005935867627462 Validation Loss: 0.7640007138252258\n",
      "Epoch 7970: Training Loss: 0.06011676167448362 Validation Loss: 0.7675046324729919\n",
      "Epoch 7971: Training Loss: 0.05994774277011553 Validation Loss: 0.7677522897720337\n",
      "Epoch 7972: Training Loss: 0.06006649633248647 Validation Loss: 0.7638546824455261\n",
      "Epoch 7973: Training Loss: 0.0599697989722093 Validation Loss: 0.7620227336883545\n",
      "Epoch 7974: Training Loss: 0.05993237470587095 Validation Loss: 0.7633248567581177\n",
      "Epoch 7975: Training Loss: 0.05990687757730484 Validation Loss: 0.7653952836990356\n",
      "Epoch 7976: Training Loss: 0.05994237462679545 Validation Loss: 0.7659062147140503\n",
      "Epoch 7977: Training Loss: 0.06000654647747675 Validation Loss: 0.7639589309692383\n",
      "Epoch 7978: Training Loss: 0.05989712725083033 Validation Loss: 0.7653636932373047\n",
      "Epoch 7979: Training Loss: 0.05983762567241987 Validation Loss: 0.7654672861099243\n",
      "Epoch 7980: Training Loss: 0.0601977768043677 Validation Loss: 0.7614570260047913\n",
      "Epoch 7981: Training Loss: 0.0600299226740996 Validation Loss: 0.7643061876296997\n",
      "Epoch 7982: Training Loss: 0.05982323735952377 Validation Loss: 0.7649858593940735\n",
      "Epoch 7983: Training Loss: 0.05994015311201414 Validation Loss: 0.7672021985054016\n",
      "Epoch 7984: Training Loss: 0.059848581751187645 Validation Loss: 0.766316831111908\n",
      "Epoch 7985: Training Loss: 0.059963103383779526 Validation Loss: 0.7632060647010803\n",
      "Epoch 7986: Training Loss: 0.05988223602374395 Validation Loss: 0.7619488835334778\n",
      "Epoch 7987: Training Loss: 0.060122876117626824 Validation Loss: 0.7603150010108948\n",
      "Epoch 7988: Training Loss: 0.05985455711682638 Validation Loss: 0.7622408270835876\n",
      "Epoch 7989: Training Loss: 0.05976627146204313 Validation Loss: 0.7664520144462585\n",
      "Epoch 7990: Training Loss: 0.05987365419665972 Validation Loss: 0.7676728963851929\n",
      "Epoch 7991: Training Loss: 0.060374551763137184 Validation Loss: 0.7711523771286011\n",
      "Epoch 7992: Training Loss: 0.05997143809994062 Validation Loss: 0.7703896760940552\n",
      "Epoch 7993: Training Loss: 0.059980726490418114 Validation Loss: 0.7640674114227295\n",
      "Epoch 7994: Training Loss: 0.06033504009246826 Validation Loss: 0.7589574456214905\n",
      "Epoch 7995: Training Loss: 0.05986861636241277 Validation Loss: 0.7599478363990784\n",
      "Epoch 7996: Training Loss: 0.05989288414518038 Validation Loss: 0.761355996131897\n",
      "Epoch 7997: Training Loss: 0.05987224355340004 Validation Loss: 0.7669636607170105\n",
      "Epoch 7998: Training Loss: 0.05993097275495529 Validation Loss: 0.7700687646865845\n",
      "Epoch 7999: Training Loss: 0.05992580701907476 Validation Loss: 0.7672104239463806\n",
      "Epoch 8000: Training Loss: 0.059766825288534164 Validation Loss: 0.7651607990264893\n",
      "Epoch 8001: Training Loss: 0.05974211419622103 Validation Loss: 0.7649579048156738\n",
      "Epoch 8002: Training Loss: 0.060002839813629784 Validation Loss: 0.761921226978302\n",
      "Epoch 8003: Training Loss: 0.059961117804050446 Validation Loss: 0.7619205117225647\n",
      "Epoch 8004: Training Loss: 0.059836968779563904 Validation Loss: 0.7641549110412598\n",
      "Epoch 8005: Training Loss: 0.05972531313697497 Validation Loss: 0.7680559754371643\n",
      "Epoch 8006: Training Loss: 0.059834533681472145 Validation Loss: 0.7710525989532471\n",
      "Epoch 8007: Training Loss: 0.059791446973880134 Validation Loss: 0.7677369117736816\n",
      "Epoch 8008: Training Loss: 0.05972849577665329 Validation Loss: 0.7661771178245544\n",
      "Epoch 8009: Training Loss: 0.060169365257024765 Validation Loss: 0.7609694600105286\n",
      "Epoch 8010: Training Loss: 0.05973482007781664 Validation Loss: 0.7617540955543518\n",
      "Epoch 8011: Training Loss: 0.05971781785289446 Validation Loss: 0.764133870601654\n",
      "Epoch 8012: Training Loss: 0.05962929502129555 Validation Loss: 0.7649099230766296\n",
      "Epoch 8013: Training Loss: 0.05963154385487238 Validation Loss: 0.7663145065307617\n",
      "Epoch 8014: Training Loss: 0.05973856647809347 Validation Loss: 0.7647784352302551\n",
      "Epoch 8015: Training Loss: 0.05968420207500458 Validation Loss: 0.7639551758766174\n",
      "Epoch 8016: Training Loss: 0.06030355766415596 Validation Loss: 0.770034670829773\n",
      "Epoch 8017: Training Loss: 0.059690275539954506 Validation Loss: 0.7693148851394653\n",
      "Epoch 8018: Training Loss: 0.05977336565653483 Validation Loss: 0.7645213007926941\n",
      "Epoch 8019: Training Loss: 0.059925867865482964 Validation Loss: 0.7617380619049072\n",
      "Epoch 8020: Training Loss: 0.05965951085090637 Validation Loss: 0.7621709108352661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8021: Training Loss: 0.05961518113811811 Validation Loss: 0.7637525200843811\n",
      "Epoch 8022: Training Loss: 0.059826613714297615 Validation Loss: 0.7624610066413879\n",
      "Epoch 8023: Training Loss: 0.059500529120365776 Validation Loss: 0.7665649056434631\n",
      "Epoch 8024: Training Loss: 0.05953325082858404 Validation Loss: 0.7687914967536926\n",
      "Epoch 8025: Training Loss: 0.05960665146509806 Validation Loss: 0.7693391442298889\n",
      "Epoch 8026: Training Loss: 0.05967605486512184 Validation Loss: 0.7664909362792969\n",
      "Epoch 8027: Training Loss: 0.05961823711792628 Validation Loss: 0.7663021683692932\n",
      "Epoch 8028: Training Loss: 0.05955953523516655 Validation Loss: 0.7644426822662354\n",
      "Epoch 8029: Training Loss: 0.0596575898428758 Validation Loss: 0.7660128474235535\n",
      "Epoch 8030: Training Loss: 0.05957081913948059 Validation Loss: 0.7633723616600037\n",
      "Epoch 8031: Training Loss: 0.059717439115047455 Validation Loss: 0.7607251405715942\n",
      "Epoch 8032: Training Loss: 0.059599356104930244 Validation Loss: 0.7619460225105286\n",
      "Epoch 8033: Training Loss: 0.05955968797206879 Validation Loss: 0.7633371353149414\n",
      "Epoch 8034: Training Loss: 0.0595797598361969 Validation Loss: 0.7654224634170532\n",
      "Epoch 8035: Training Loss: 0.05956278989712397 Validation Loss: 0.7682764530181885\n",
      "Epoch 8036: Training Loss: 0.059767031421264015 Validation Loss: 0.7721569538116455\n",
      "Epoch 8037: Training Loss: 0.059634203712145485 Validation Loss: 0.7700918316841125\n",
      "Epoch 8038: Training Loss: 0.059569939970970154 Validation Loss: 0.7664300799369812\n",
      "Epoch 8039: Training Loss: 0.05959531913201014 Validation Loss: 0.7627406120300293\n",
      "Epoch 8040: Training Loss: 0.059776588032643 Validation Loss: 0.7657077312469482\n",
      "Epoch 8041: Training Loss: 0.05947749068339666 Validation Loss: 0.7639380097389221\n",
      "Epoch 8042: Training Loss: 0.05956148107846578 Validation Loss: 0.7626273036003113\n",
      "Epoch 8043: Training Loss: 0.059524934738874435 Validation Loss: 0.7628653645515442\n",
      "Epoch 8044: Training Loss: 0.05946440870563189 Validation Loss: 0.765877366065979\n",
      "Epoch 8045: Training Loss: 0.05944724256793658 Validation Loss: 0.7681316137313843\n",
      "Epoch 8046: Training Loss: 0.05945749580860138 Validation Loss: 0.767474889755249\n",
      "Epoch 8047: Training Loss: 0.05949438735842705 Validation Loss: 0.765506386756897\n",
      "Epoch 8048: Training Loss: 0.05951042349139849 Validation Loss: 0.7659865021705627\n",
      "Epoch 8049: Training Loss: 0.059474650770425797 Validation Loss: 0.7647313475608826\n",
      "Epoch 8050: Training Loss: 0.05951175962885221 Validation Loss: 0.7661656737327576\n",
      "Epoch 8051: Training Loss: 0.05940936257441839 Validation Loss: 0.7655406594276428\n",
      "Epoch 8052: Training Loss: 0.0594502625366052 Validation Loss: 0.7625612616539001\n",
      "Epoch 8053: Training Loss: 0.05944492047031721 Validation Loss: 0.7643739581108093\n",
      "Epoch 8054: Training Loss: 0.059465743601322174 Validation Loss: 0.7630093693733215\n",
      "Epoch 8055: Training Loss: 0.05949622392654419 Validation Loss: 0.7648677825927734\n",
      "Epoch 8056: Training Loss: 0.05934865400195122 Validation Loss: 0.7653052806854248\n",
      "Epoch 8057: Training Loss: 0.05980526159207026 Validation Loss: 0.7622279524803162\n",
      "Epoch 8058: Training Loss: 0.05951245625813802 Validation Loss: 0.7669188380241394\n",
      "Epoch 8059: Training Loss: 0.059579754869143166 Validation Loss: 0.7656456232070923\n",
      "Epoch 8060: Training Loss: 0.05930937081575394 Validation Loss: 0.7674170136451721\n",
      "Epoch 8061: Training Loss: 0.059307388961315155 Validation Loss: 0.7683786749839783\n",
      "Epoch 8062: Training Loss: 0.05934304868181547 Validation Loss: 0.7676845788955688\n",
      "Epoch 8063: Training Loss: 0.05944092944264412 Validation Loss: 0.7656872868537903\n",
      "Epoch 8064: Training Loss: 0.059444850931564965 Validation Loss: 0.7674616575241089\n",
      "Epoch 8065: Training Loss: 0.05934699500600497 Validation Loss: 0.7658076286315918\n",
      "Epoch 8066: Training Loss: 0.059448702881733574 Validation Loss: 0.7628136873245239\n",
      "Epoch 8067: Training Loss: 0.05948152020573616 Validation Loss: 0.765392005443573\n",
      "Epoch 8068: Training Loss: 0.05931784709294637 Validation Loss: 0.7651697397232056\n",
      "Epoch 8069: Training Loss: 0.05955436825752258 Validation Loss: 0.766682505607605\n",
      "Epoch 8070: Training Loss: 0.05941445380449295 Validation Loss: 0.7644518613815308\n",
      "Epoch 8071: Training Loss: 0.05935912330945333 Validation Loss: 0.7644677758216858\n",
      "Epoch 8072: Training Loss: 0.05932154258092245 Validation Loss: 0.7645704746246338\n",
      "Epoch 8073: Training Loss: 0.05940954014658928 Validation Loss: 0.7623070478439331\n",
      "Epoch 8074: Training Loss: 0.05932023127873739 Validation Loss: 0.7622426152229309\n",
      "Epoch 8075: Training Loss: 0.05932267134388288 Validation Loss: 0.7648890018463135\n",
      "Epoch 8076: Training Loss: 0.059311412274837494 Validation Loss: 0.766353964805603\n",
      "Epoch 8077: Training Loss: 0.059415104488531746 Validation Loss: 0.7687149047851562\n",
      "Epoch 8078: Training Loss: 0.05932948365807533 Validation Loss: 0.7666799426078796\n",
      "Epoch 8079: Training Loss: 0.05925675109028816 Validation Loss: 0.7650898694992065\n",
      "Epoch 8080: Training Loss: 0.059421610087156296 Validation Loss: 0.7648189663887024\n",
      "Epoch 8081: Training Loss: 0.05928500245014826 Validation Loss: 0.7656385898590088\n",
      "Epoch 8082: Training Loss: 0.05930102740724882 Validation Loss: 0.7674278020858765\n",
      "Epoch 8083: Training Loss: 0.05934573213259379 Validation Loss: 0.7676538825035095\n",
      "Epoch 8084: Training Loss: 0.05924990773200989 Validation Loss: 0.7650944590568542\n",
      "Epoch 8085: Training Loss: 0.05944992477695147 Validation Loss: 0.762071430683136\n",
      "Epoch 8086: Training Loss: 0.05918589731057485 Validation Loss: 0.763862669467926\n",
      "Epoch 8087: Training Loss: 0.05944784606496493 Validation Loss: 0.7636033296585083\n",
      "Epoch 8088: Training Loss: 0.05919840931892395 Validation Loss: 0.766396164894104\n",
      "Epoch 8089: Training Loss: 0.05917772650718689 Validation Loss: 0.7671635746955872\n",
      "Epoch 8090: Training Loss: 0.05925008902947108 Validation Loss: 0.7698991298675537\n",
      "Epoch 8091: Training Loss: 0.059233845522006355 Validation Loss: 0.7706324458122253\n",
      "Epoch 8092: Training Loss: 0.05938027302424113 Validation Loss: 0.7694880366325378\n",
      "Epoch 8093: Training Loss: 0.059155053148667015 Validation Loss: 0.7651306390762329\n",
      "Epoch 8094: Training Loss: 0.05925838649272919 Validation Loss: 0.7629227638244629\n",
      "Epoch 8095: Training Loss: 0.05927266925573349 Validation Loss: 0.7647720575332642\n",
      "Epoch 8096: Training Loss: 0.05921091636021932 Validation Loss: 0.765190839767456\n",
      "Epoch 8097: Training Loss: 0.059183343003193535 Validation Loss: 0.7638126015663147\n",
      "Epoch 8098: Training Loss: 0.05912171428402265 Validation Loss: 0.7639731168746948\n",
      "Epoch 8099: Training Loss: 0.05916487549742063 Validation Loss: 0.7662542462348938\n",
      "Epoch 8100: Training Loss: 0.05906854818264643 Validation Loss: 0.7664382457733154\n",
      "Epoch 8101: Training Loss: 0.05915513634681702 Validation Loss: 0.7673355340957642\n",
      "Epoch 8102: Training Loss: 0.059216691801945366 Validation Loss: 0.768730103969574\n",
      "Epoch 8103: Training Loss: 0.05913451686501503 Validation Loss: 0.7658494710922241\n",
      "Epoch 8104: Training Loss: 0.05914020165801048 Validation Loss: 0.7627792358398438\n",
      "Epoch 8105: Training Loss: 0.05916431794563929 Validation Loss: 0.764553427696228\n",
      "Epoch 8106: Training Loss: 0.059119428197542824 Validation Loss: 0.7654584646224976\n",
      "Epoch 8107: Training Loss: 0.05931818981965383 Validation Loss: 0.7672479748725891\n",
      "Epoch 8108: Training Loss: 0.059253366043170295 Validation Loss: 0.7686166167259216\n",
      "Epoch 8109: Training Loss: 0.059070389717817307 Validation Loss: 0.7665778994560242\n",
      "Epoch 8110: Training Loss: 0.05907279501358668 Validation Loss: 0.7643970847129822\n",
      "Epoch 8111: Training Loss: 0.0592507670323054 Validation Loss: 0.7659801244735718\n",
      "Epoch 8112: Training Loss: 0.059034919987122216 Validation Loss: 0.7654821276664734\n",
      "Epoch 8113: Training Loss: 0.059059008955955505 Validation Loss: 0.7629911303520203\n",
      "Epoch 8114: Training Loss: 0.05921997999151548 Validation Loss: 0.7623612880706787\n",
      "Epoch 8115: Training Loss: 0.059459457794825234 Validation Loss: 0.7606291770935059\n",
      "Epoch 8116: Training Loss: 0.05899564052621523 Validation Loss: 0.7645634412765503\n",
      "Epoch 8117: Training Loss: 0.05893176297346751 Validation Loss: 0.7671163082122803\n",
      "Epoch 8118: Training Loss: 0.0593193049232165 Validation Loss: 0.7731033563613892\n",
      "Epoch 8119: Training Loss: 0.05909600233038267 Validation Loss: 0.7716873288154602\n",
      "Epoch 8120: Training Loss: 0.05909903099139532 Validation Loss: 0.7694417238235474\n",
      "Epoch 8121: Training Loss: 0.05896696448326111 Validation Loss: 0.7682749032974243\n",
      "Epoch 8122: Training Loss: 0.05902942270040512 Validation Loss: 0.7648636102676392\n",
      "Epoch 8123: Training Loss: 0.05913595234354337 Validation Loss: 0.761487603187561\n",
      "Epoch 8124: Training Loss: 0.05909314503272375 Validation Loss: 0.7641909122467041\n",
      "Epoch 8125: Training Loss: 0.058949075639247894 Validation Loss: 0.7654852271080017\n",
      "Epoch 8126: Training Loss: 0.05904654040932655 Validation Loss: 0.7669678330421448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8127: Training Loss: 0.058983725806077324 Validation Loss: 0.7675907611846924\n",
      "Epoch 8128: Training Loss: 0.05892063304781914 Validation Loss: 0.7661941647529602\n",
      "Epoch 8129: Training Loss: 0.05896225571632385 Validation Loss: 0.7662762403488159\n",
      "Epoch 8130: Training Loss: 0.05914680163065592 Validation Loss: 0.7623962759971619\n",
      "Epoch 8131: Training Loss: 0.05898914486169815 Validation Loss: 0.7635398507118225\n",
      "Epoch 8132: Training Loss: 0.05891352643569311 Validation Loss: 0.764976978302002\n",
      "Epoch 8133: Training Loss: 0.05886828154325485 Validation Loss: 0.7662343978881836\n",
      "Epoch 8134: Training Loss: 0.05888143554329872 Validation Loss: 0.7665842175483704\n",
      "Epoch 8135: Training Loss: 0.05900619179010391 Validation Loss: 0.765523374080658\n",
      "Epoch 8136: Training Loss: 0.05902287612358729 Validation Loss: 0.768919050693512\n",
      "Epoch 8137: Training Loss: 0.05901416763663292 Validation Loss: 0.7668938040733337\n",
      "Epoch 8138: Training Loss: 0.05888018881281217 Validation Loss: 0.766383945941925\n",
      "Epoch 8139: Training Loss: 0.058870783696571984 Validation Loss: 0.7674323916435242\n",
      "Epoch 8140: Training Loss: 0.05886192371447881 Validation Loss: 0.7666276693344116\n",
      "Epoch 8141: Training Loss: 0.05889110888044039 Validation Loss: 0.7655938863754272\n",
      "Epoch 8142: Training Loss: 0.05919773876667023 Validation Loss: 0.7701451182365417\n",
      "Epoch 8143: Training Loss: 0.058892457435528435 Validation Loss: 0.7681546211242676\n",
      "Epoch 8144: Training Loss: 0.058834498127301536 Validation Loss: 0.7667355537414551\n",
      "Epoch 8145: Training Loss: 0.05880217875043551 Validation Loss: 0.7651683688163757\n",
      "Epoch 8146: Training Loss: 0.058878531058629356 Validation Loss: 0.7644032835960388\n",
      "Epoch 8147: Training Loss: 0.05883862326542536 Validation Loss: 0.7657162547111511\n",
      "Epoch 8148: Training Loss: 0.059018105268478394 Validation Loss: 0.7677951455116272\n",
      "Epoch 8149: Training Loss: 0.05886136864622434 Validation Loss: 0.76788729429245\n",
      "Epoch 8150: Training Loss: 0.059233926236629486 Validation Loss: 0.7626640796661377\n",
      "Epoch 8151: Training Loss: 0.059019931902488075 Validation Loss: 0.7649610638618469\n",
      "Epoch 8152: Training Loss: 0.05898292362689972 Validation Loss: 0.7628235816955566\n",
      "Epoch 8153: Training Loss: 0.058977432548999786 Validation Loss: 0.7669332027435303\n",
      "Epoch 8154: Training Loss: 0.0587469128270944 Validation Loss: 0.7679750919342041\n",
      "Epoch 8155: Training Loss: 0.05891156072417895 Validation Loss: 0.7689818143844604\n",
      "Epoch 8156: Training Loss: 0.05883398279547691 Validation Loss: 0.7696266770362854\n",
      "Epoch 8157: Training Loss: 0.058900121599435806 Validation Loss: 0.7697766423225403\n",
      "Epoch 8158: Training Loss: 0.05875952790180842 Validation Loss: 0.7657115459442139\n",
      "Epoch 8159: Training Loss: 0.05884116763869921 Validation Loss: 0.7639061808586121\n",
      "Epoch 8160: Training Loss: 0.05879169205824534 Validation Loss: 0.7632402181625366\n",
      "Epoch 8161: Training Loss: 0.058775644749403 Validation Loss: 0.7637419104576111\n",
      "Epoch 8162: Training Loss: 0.05871569365262985 Validation Loss: 0.7656844258308411\n",
      "Epoch 8163: Training Loss: 0.05900721997022629 Validation Loss: 0.7699170708656311\n",
      "Epoch 8164: Training Loss: 0.058782016237576805 Validation Loss: 0.769034743309021\n",
      "Epoch 8165: Training Loss: 0.05881532281637192 Validation Loss: 0.766883373260498\n",
      "Epoch 8166: Training Loss: 0.05879764879743258 Validation Loss: 0.7635484337806702\n",
      "Epoch 8167: Training Loss: 0.05877776568134626 Validation Loss: 0.7622566223144531\n",
      "Epoch 8168: Training Loss: 0.05889484037955602 Validation Loss: 0.7632891535758972\n",
      "Epoch 8169: Training Loss: 0.05903737246990204 Validation Loss: 0.7686735987663269\n",
      "Epoch 8170: Training Loss: 0.05882443115115166 Validation Loss: 0.7680680751800537\n",
      "Epoch 8171: Training Loss: 0.058743592351675034 Validation Loss: 0.7688355445861816\n",
      "Epoch 8172: Training Loss: 0.058739478389422096 Validation Loss: 0.7689856886863708\n",
      "Epoch 8173: Training Loss: 0.05864426369468371 Validation Loss: 0.7677552700042725\n",
      "Epoch 8174: Training Loss: 0.058734393368164696 Validation Loss: 0.7662668228149414\n",
      "Epoch 8175: Training Loss: 0.05879375338554382 Validation Loss: 0.7642835974693298\n",
      "Epoch 8176: Training Loss: 0.058668973545233406 Validation Loss: 0.7670549750328064\n",
      "Epoch 8177: Training Loss: 0.058671727776527405 Validation Loss: 0.7693480849266052\n",
      "Epoch 8178: Training Loss: 0.05874185015757879 Validation Loss: 0.7711508870124817\n",
      "Epoch 8179: Training Loss: 0.059069727857907615 Validation Loss: 0.7731402516365051\n",
      "Epoch 8180: Training Loss: 0.05879774192969004 Validation Loss: 0.7668619155883789\n",
      "Epoch 8181: Training Loss: 0.058725583056608834 Validation Loss: 0.7624714970588684\n",
      "Epoch 8182: Training Loss: 0.059064749628305435 Validation Loss: 0.7598224878311157\n",
      "Epoch 8183: Training Loss: 0.058888754496971764 Validation Loss: 0.7648105025291443\n",
      "Epoch 8184: Training Loss: 0.058604744573434196 Validation Loss: 0.7658993601799011\n",
      "Epoch 8185: Training Loss: 0.05855777238806089 Validation Loss: 0.7676447629928589\n",
      "Epoch 8186: Training Loss: 0.05861604834596316 Validation Loss: 0.7679294347763062\n",
      "Epoch 8187: Training Loss: 0.05868591368198395 Validation Loss: 0.7707161903381348\n",
      "Epoch 8188: Training Loss: 0.058701989551385246 Validation Loss: 0.7709035277366638\n",
      "Epoch 8189: Training Loss: 0.058617640286684036 Validation Loss: 0.7682956457138062\n",
      "Epoch 8190: Training Loss: 0.058664644757906594 Validation Loss: 0.7671506404876709\n",
      "Epoch 8191: Training Loss: 0.05874921381473541 Validation Loss: 0.7681195139884949\n",
      "Epoch 8192: Training Loss: 0.058533849815527596 Validation Loss: 0.7656509280204773\n",
      "Epoch 8193: Training Loss: 0.05861640224854151 Validation Loss: 0.7644846439361572\n",
      "Epoch 8194: Training Loss: 0.05867818867166837 Validation Loss: 0.7666359543800354\n",
      "Epoch 8195: Training Loss: 0.05852033197879791 Validation Loss: 0.7661376595497131\n",
      "Epoch 8196: Training Loss: 0.058695858965317406 Validation Loss: 0.7633758783340454\n",
      "Epoch 8197: Training Loss: 0.05858327200015386 Validation Loss: 0.7631519436836243\n",
      "Epoch 8198: Training Loss: 0.05851162721713384 Validation Loss: 0.7653848528862\n",
      "Epoch 8199: Training Loss: 0.05854732419053713 Validation Loss: 0.7681666612625122\n",
      "Epoch 8200: Training Loss: 0.058900054544210434 Validation Loss: 0.7666313052177429\n",
      "Epoch 8201: Training Loss: 0.058673880994319916 Validation Loss: 0.7709424495697021\n",
      "Epoch 8202: Training Loss: 0.05880296851197878 Validation Loss: 0.7673935294151306\n",
      "Epoch 8203: Training Loss: 0.05850998560587565 Validation Loss: 0.7662178874015808\n",
      "Epoch 8204: Training Loss: 0.05852096527814865 Validation Loss: 0.767436683177948\n",
      "Epoch 8205: Training Loss: 0.05861858526865641 Validation Loss: 0.7661895751953125\n",
      "Epoch 8206: Training Loss: 0.058496374636888504 Validation Loss: 0.768250048160553\n",
      "Epoch 8207: Training Loss: 0.05848942200342814 Validation Loss: 0.7690929174423218\n",
      "Epoch 8208: Training Loss: 0.05865163976947466 Validation Loss: 0.7714203596115112\n",
      "Epoch 8209: Training Loss: 0.05846970776716868 Validation Loss: 0.7686905860900879\n",
      "Epoch 8210: Training Loss: 0.05845117444793383 Validation Loss: 0.7665780782699585\n",
      "Epoch 8211: Training Loss: 0.05847035224239031 Validation Loss: 0.7641080021858215\n",
      "Epoch 8212: Training Loss: 0.05857580155134201 Validation Loss: 0.7659733891487122\n",
      "Epoch 8213: Training Loss: 0.05854783207178116 Validation Loss: 0.7638034224510193\n",
      "Epoch 8214: Training Loss: 0.058509111404418945 Validation Loss: 0.7653527855873108\n",
      "Epoch 8215: Training Loss: 0.058741810421148934 Validation Loss: 0.7693262696266174\n",
      "Epoch 8216: Training Loss: 0.05841311439871788 Validation Loss: 0.7670048475265503\n",
      "Epoch 8217: Training Loss: 0.05880268042286237 Validation Loss: 0.7690532207489014\n",
      "Epoch 8218: Training Loss: 0.05856122697393099 Validation Loss: 0.7644965052604675\n",
      "Epoch 8219: Training Loss: 0.058395622919003166 Validation Loss: 0.7654014825820923\n",
      "Epoch 8220: Training Loss: 0.05838020766774813 Validation Loss: 0.7664651274681091\n",
      "Epoch 8221: Training Loss: 0.058465324342250824 Validation Loss: 0.7665067911148071\n",
      "Epoch 8222: Training Loss: 0.05851787204543749 Validation Loss: 0.7684542536735535\n",
      "Epoch 8223: Training Loss: 0.05844006066521009 Validation Loss: 0.7664937973022461\n",
      "Epoch 8224: Training Loss: 0.05846992383400599 Validation Loss: 0.7685467600822449\n",
      "Epoch 8225: Training Loss: 0.058482264479001365 Validation Loss: 0.7702453136444092\n",
      "Epoch 8226: Training Loss: 0.058443921307722725 Validation Loss: 0.7663716673851013\n",
      "Epoch 8227: Training Loss: 0.058442167937755585 Validation Loss: 0.7674631476402283\n",
      "Epoch 8228: Training Loss: 0.058395170917113624 Validation Loss: 0.7648283839225769\n",
      "Epoch 8229: Training Loss: 0.05837859710057577 Validation Loss: 0.7663580775260925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8230: Training Loss: 0.0583296592036883 Validation Loss: 0.7670841813087463\n",
      "Epoch 8231: Training Loss: 0.058297173430522285 Validation Loss: 0.767261803150177\n",
      "Epoch 8232: Training Loss: 0.0583295983572801 Validation Loss: 0.7657164931297302\n",
      "Epoch 8233: Training Loss: 0.058369070291519165 Validation Loss: 0.767890989780426\n",
      "Epoch 8234: Training Loss: 0.05833106239636739 Validation Loss: 0.7676693201065063\n",
      "Epoch 8235: Training Loss: 0.058402507255474724 Validation Loss: 0.7678908109664917\n",
      "Epoch 8236: Training Loss: 0.05849124615391096 Validation Loss: 0.7652804255485535\n",
      "Epoch 8237: Training Loss: 0.05870199700196584 Validation Loss: 0.7699943780899048\n",
      "Epoch 8238: Training Loss: 0.05829827984174093 Validation Loss: 0.7689887285232544\n",
      "Epoch 8239: Training Loss: 0.05829036235809326 Validation Loss: 0.7685256004333496\n",
      "Epoch 8240: Training Loss: 0.05840345844626427 Validation Loss: 0.7675246596336365\n",
      "Epoch 8241: Training Loss: 0.058349672704935074 Validation Loss: 0.7683302164077759\n",
      "Epoch 8242: Training Loss: 0.05827962855497996 Validation Loss: 0.7659737467765808\n",
      "Epoch 8243: Training Loss: 0.05835337936878204 Validation Loss: 0.7658136487007141\n",
      "Epoch 8244: Training Loss: 0.058215116461118065 Validation Loss: 0.7650544047355652\n",
      "Epoch 8245: Training Loss: 0.058364689350128174 Validation Loss: 0.7634696960449219\n",
      "Epoch 8246: Training Loss: 0.05822785571217537 Validation Loss: 0.7650994062423706\n",
      "Epoch 8247: Training Loss: 0.058338901648918785 Validation Loss: 0.7691763043403625\n",
      "Epoch 8248: Training Loss: 0.05823963259657224 Validation Loss: 0.7699871063232422\n",
      "Epoch 8249: Training Loss: 0.05831243967016538 Validation Loss: 0.7720610499382019\n",
      "Epoch 8250: Training Loss: 0.05841099222501119 Validation Loss: 0.7682714462280273\n",
      "Epoch 8251: Training Loss: 0.05821052441994349 Validation Loss: 0.7665204405784607\n",
      "Epoch 8252: Training Loss: 0.05842570960521698 Validation Loss: 0.7646042108535767\n",
      "Epoch 8253: Training Loss: 0.058386302242676415 Validation Loss: 0.7678534984588623\n",
      "Epoch 8254: Training Loss: 0.05826923934121927 Validation Loss: 0.7703958749771118\n",
      "Epoch 8255: Training Loss: 0.05815496916572253 Validation Loss: 0.7698100805282593\n",
      "Epoch 8256: Training Loss: 0.05823848148187002 Validation Loss: 0.7674208879470825\n",
      "Epoch 8257: Training Loss: 0.058348496754964195 Validation Loss: 0.7693994641304016\n",
      "Epoch 8258: Training Loss: 0.0581986332933108 Validation Loss: 0.7693479061126709\n",
      "Epoch 8259: Training Loss: 0.05825595185160637 Validation Loss: 0.7648181319236755\n",
      "Epoch 8260: Training Loss: 0.05852656935652097 Validation Loss: 0.7608087062835693\n",
      "Epoch 8261: Training Loss: 0.05823910981416702 Validation Loss: 0.7637391090393066\n",
      "Epoch 8262: Training Loss: 0.05816579610109329 Validation Loss: 0.7671535015106201\n",
      "Epoch 8263: Training Loss: 0.05812527984380722 Validation Loss: 0.7689651846885681\n",
      "Epoch 8264: Training Loss: 0.05819224938750267 Validation Loss: 0.7682309746742249\n",
      "Epoch 8265: Training Loss: 0.05865181734164556 Validation Loss: 0.7732212543487549\n",
      "Epoch 8266: Training Loss: 0.058293712635835014 Validation Loss: 0.7693681120872498\n",
      "Epoch 8267: Training Loss: 0.05816348890463511 Validation Loss: 0.7674747109413147\n",
      "Epoch 8268: Training Loss: 0.05813621977965037 Validation Loss: 0.766502857208252\n",
      "Epoch 8269: Training Loss: 0.058169263104597725 Validation Loss: 0.7678634524345398\n",
      "Epoch 8270: Training Loss: 0.05816888064146042 Validation Loss: 0.7691853046417236\n",
      "Epoch 8271: Training Loss: 0.05834121505419413 Validation Loss: 0.770661473274231\n",
      "Epoch 8272: Training Loss: 0.05808196340998014 Validation Loss: 0.7685641050338745\n",
      "Epoch 8273: Training Loss: 0.05805537601312002 Validation Loss: 0.7662979364395142\n",
      "Epoch 8274: Training Loss: 0.05808266997337341 Validation Loss: 0.7650408148765564\n",
      "Epoch 8275: Training Loss: 0.058128400395313896 Validation Loss: 0.7647386789321899\n",
      "Epoch 8276: Training Loss: 0.05821262796719869 Validation Loss: 0.7654584646224976\n",
      "Epoch 8277: Training Loss: 0.05805280307928721 Validation Loss: 0.7660892009735107\n",
      "Epoch 8278: Training Loss: 0.05803363770246506 Validation Loss: 0.7674315571784973\n",
      "Epoch 8279: Training Loss: 0.058117303997278214 Validation Loss: 0.7691229581832886\n",
      "Epoch 8280: Training Loss: 0.0581660270690918 Validation Loss: 0.7699531316757202\n",
      "Epoch 8281: Training Loss: 0.058024766544500984 Validation Loss: 0.7682912349700928\n",
      "Epoch 8282: Training Loss: 0.05804670602083206 Validation Loss: 0.7654948234558105\n",
      "Epoch 8283: Training Loss: 0.05818405250708262 Validation Loss: 0.7634239196777344\n",
      "Epoch 8284: Training Loss: 0.0581203339000543 Validation Loss: 0.7636404633522034\n",
      "Epoch 8285: Training Loss: 0.058050207793712616 Validation Loss: 0.7669404745101929\n",
      "Epoch 8286: Training Loss: 0.058048754930496216 Validation Loss: 0.76934415102005\n",
      "Epoch 8287: Training Loss: 0.058280998220046364 Validation Loss: 0.7743110656738281\n",
      "Epoch 8288: Training Loss: 0.05824829638004303 Validation Loss: 0.7757195234298706\n",
      "Epoch 8289: Training Loss: 0.05806127066413561 Validation Loss: 0.7732700109481812\n",
      "Epoch 8290: Training Loss: 0.05811799565951029 Validation Loss: 0.7666972875595093\n",
      "Epoch 8291: Training Loss: 0.05830400933821996 Validation Loss: 0.7631491422653198\n",
      "Epoch 8292: Training Loss: 0.058106364061435066 Validation Loss: 0.7635454535484314\n",
      "Epoch 8293: Training Loss: 0.0580197237432003 Validation Loss: 0.765100359916687\n",
      "Epoch 8294: Training Loss: 0.058168948938449226 Validation Loss: 0.7695540189743042\n",
      "Epoch 8295: Training Loss: 0.05800831566254298 Validation Loss: 0.7682711482048035\n",
      "Epoch 8296: Training Loss: 0.05792406698067983 Validation Loss: 0.7687667012214661\n",
      "Epoch 8297: Training Loss: 0.05795476958155632 Validation Loss: 0.7691368460655212\n",
      "Epoch 8298: Training Loss: 0.05840702603260676 Validation Loss: 0.7726271748542786\n",
      "Epoch 8299: Training Loss: 0.05843492969870567 Validation Loss: 0.7659832835197449\n",
      "Epoch 8300: Training Loss: 0.05808984115719795 Validation Loss: 0.7643427848815918\n",
      "Epoch 8301: Training Loss: 0.058004215359687805 Validation Loss: 0.7664238214492798\n",
      "Epoch 8302: Training Loss: 0.05799569934606552 Validation Loss: 0.7671680450439453\n",
      "Epoch 8303: Training Loss: 0.05796510850389799 Validation Loss: 0.7694805860519409\n",
      "Epoch 8304: Training Loss: 0.05785815169413885 Validation Loss: 0.7693063616752625\n",
      "Epoch 8305: Training Loss: 0.05792109792431196 Validation Loss: 0.7675184011459351\n",
      "Epoch 8306: Training Loss: 0.05786442259947459 Validation Loss: 0.7674866914749146\n",
      "Epoch 8307: Training Loss: 0.05788243810335795 Validation Loss: 0.7681930661201477\n",
      "Epoch 8308: Training Loss: 0.057960369934638344 Validation Loss: 0.7681042551994324\n",
      "Epoch 8309: Training Loss: 0.057874089727799095 Validation Loss: 0.7675531506538391\n",
      "Epoch 8310: Training Loss: 0.05788243189454079 Validation Loss: 0.7679716944694519\n",
      "Epoch 8311: Training Loss: 0.05792201186219851 Validation Loss: 0.7672722339630127\n",
      "Epoch 8312: Training Loss: 0.05793603012959162 Validation Loss: 0.7702280282974243\n",
      "Epoch 8313: Training Loss: 0.057858566443125405 Validation Loss: 0.7699090242385864\n",
      "Epoch 8314: Training Loss: 0.05785350129008293 Validation Loss: 0.7699408531188965\n",
      "Epoch 8315: Training Loss: 0.057857914517323174 Validation Loss: 0.7682188749313354\n",
      "Epoch 8316: Training Loss: 0.05783518776297569 Validation Loss: 0.7673203945159912\n",
      "Epoch 8317: Training Loss: 0.058004289865493774 Validation Loss: 0.7694959044456482\n",
      "Epoch 8318: Training Loss: 0.057894688099622726 Validation Loss: 0.7694695591926575\n",
      "Epoch 8319: Training Loss: 0.0579246369500955 Validation Loss: 0.7701787352561951\n",
      "Epoch 8320: Training Loss: 0.057955397913853325 Validation Loss: 0.769803524017334\n",
      "Epoch 8321: Training Loss: 0.05775384604930878 Validation Loss: 0.7672845721244812\n",
      "Epoch 8322: Training Loss: 0.058318128188451133 Validation Loss: 0.7616975903511047\n",
      "Epoch 8323: Training Loss: 0.058170211811860405 Validation Loss: 0.7654739618301392\n",
      "Epoch 8324: Training Loss: 0.057977572083473206 Validation Loss: 0.7691929340362549\n",
      "Epoch 8325: Training Loss: 0.057774136463801064 Validation Loss: 0.7688669562339783\n",
      "Epoch 8326: Training Loss: 0.05775397767623266 Validation Loss: 0.768604576587677\n",
      "Epoch 8327: Training Loss: 0.05809612448016802 Validation Loss: 0.7656975984573364\n",
      "Epoch 8328: Training Loss: 0.0577671118080616 Validation Loss: 0.7677754163742065\n",
      "Epoch 8329: Training Loss: 0.05778709426522255 Validation Loss: 0.7697436213493347\n",
      "Epoch 8330: Training Loss: 0.05772192900379499 Validation Loss: 0.7709128260612488\n",
      "Epoch 8331: Training Loss: 0.05802922944227854 Validation Loss: 0.7734609246253967\n",
      "Epoch 8332: Training Loss: 0.05782766764362653 Validation Loss: 0.7720111012458801\n",
      "Epoch 8333: Training Loss: 0.05789801602562269 Validation Loss: 0.7667699456214905\n",
      "Epoch 8334: Training Loss: 0.05770557249585787 Validation Loss: 0.7653473615646362\n",
      "Epoch 8335: Training Loss: 0.0579728993276755 Validation Loss: 0.7625730037689209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8336: Training Loss: 0.057973251988490425 Validation Loss: 0.7663767337799072\n",
      "Epoch 8337: Training Loss: 0.05781779189904531 Validation Loss: 0.7670238018035889\n",
      "Epoch 8338: Training Loss: 0.05770451699693998 Validation Loss: 0.7692834734916687\n",
      "Epoch 8339: Training Loss: 0.05780732507507006 Validation Loss: 0.7728605270385742\n",
      "Epoch 8340: Training Loss: 0.05779906486471494 Validation Loss: 0.7704535126686096\n",
      "Epoch 8341: Training Loss: 0.05773083617289861 Validation Loss: 0.7713525891304016\n",
      "Epoch 8342: Training Loss: 0.0578078031539917 Validation Loss: 0.7691789269447327\n",
      "Epoch 8343: Training Loss: 0.057822986195484795 Validation Loss: 0.7705641388893127\n",
      "Epoch 8344: Training Loss: 0.05783472706874212 Validation Loss: 0.7668037414550781\n",
      "Epoch 8345: Training Loss: 0.05769965425133705 Validation Loss: 0.7658706307411194\n",
      "Epoch 8346: Training Loss: 0.05784644931554794 Validation Loss: 0.7646753191947937\n",
      "Epoch 8347: Training Loss: 0.05766594658295313 Validation Loss: 0.767951488494873\n",
      "Epoch 8348: Training Loss: 0.05770948901772499 Validation Loss: 0.7717018723487854\n",
      "Epoch 8349: Training Loss: 0.057852948705355324 Validation Loss: 0.7741991877555847\n",
      "Epoch 8350: Training Loss: 0.05772095049420992 Validation Loss: 0.7709667682647705\n",
      "Epoch 8351: Training Loss: 0.05764902507265409 Validation Loss: 0.7691684365272522\n",
      "Epoch 8352: Training Loss: 0.05775021140774091 Validation Loss: 0.7690533995628357\n",
      "Epoch 8353: Training Loss: 0.05761035904288292 Validation Loss: 0.7663388252258301\n",
      "Epoch 8354: Training Loss: 0.057951249182224274 Validation Loss: 0.7647789120674133\n",
      "Epoch 8355: Training Loss: 0.0577269047498703 Validation Loss: 0.7650962471961975\n",
      "Epoch 8356: Training Loss: 0.05761547262469927 Validation Loss: 0.7669891119003296\n",
      "Epoch 8357: Training Loss: 0.057576924562454224 Validation Loss: 0.7697420120239258\n",
      "Epoch 8358: Training Loss: 0.057600596298774086 Validation Loss: 0.770763099193573\n",
      "Epoch 8359: Training Loss: 0.0576487680276235 Validation Loss: 0.769483745098114\n",
      "Epoch 8360: Training Loss: 0.057730612655480705 Validation Loss: 0.7692878246307373\n",
      "Epoch 8361: Training Loss: 0.05769419918457667 Validation Loss: 0.7712419629096985\n",
      "Epoch 8362: Training Loss: 0.05757096533974012 Validation Loss: 0.7703689932823181\n",
      "Epoch 8363: Training Loss: 0.057616391529639564 Validation Loss: 0.7680575847625732\n",
      "Epoch 8364: Training Loss: 0.057566401859124504 Validation Loss: 0.7679153680801392\n",
      "Epoch 8365: Training Loss: 0.057748470455408096 Validation Loss: 0.7660091519355774\n",
      "Epoch 8366: Training Loss: 0.057560199250777565 Validation Loss: 0.7682560086250305\n",
      "Epoch 8367: Training Loss: 0.057815669725338616 Validation Loss: 0.7726815938949585\n",
      "Epoch 8368: Training Loss: 0.05761512741446495 Validation Loss: 0.7723084688186646\n",
      "Epoch 8369: Training Loss: 0.057576894760131836 Validation Loss: 0.7714696526527405\n",
      "Epoch 8370: Training Loss: 0.05756898348530134 Validation Loss: 0.7703351378440857\n",
      "Epoch 8371: Training Loss: 0.05751527970035871 Validation Loss: 0.7672827839851379\n",
      "Epoch 8372: Training Loss: 0.05770542472600937 Validation Loss: 0.7635672688484192\n",
      "Epoch 8373: Training Loss: 0.057694012920061745 Validation Loss: 0.7631710171699524\n",
      "Epoch 8374: Training Loss: 0.057749335964520775 Validation Loss: 0.768130898475647\n",
      "Epoch 8375: Training Loss: 0.057568019876877465 Validation Loss: 0.7707503437995911\n",
      "Epoch 8376: Training Loss: 0.05754068121314049 Validation Loss: 0.7699843049049377\n",
      "Epoch 8377: Training Loss: 0.05749525253971418 Validation Loss: 0.7703127264976501\n",
      "Epoch 8378: Training Loss: 0.057557849834362663 Validation Loss: 0.7680007815361023\n",
      "Epoch 8379: Training Loss: 0.0575990155339241 Validation Loss: 0.7680873870849609\n",
      "Epoch 8380: Training Loss: 0.058042146265506744 Validation Loss: 0.7643808126449585\n",
      "Epoch 8381: Training Loss: 0.05754509195685387 Validation Loss: 0.7660781145095825\n",
      "Epoch 8382: Training Loss: 0.05768967419862747 Validation Loss: 0.7722318172454834\n",
      "Epoch 8383: Training Loss: 0.057562178621689476 Validation Loss: 0.7711763978004456\n",
      "Epoch 8384: Training Loss: 0.057446045180161796 Validation Loss: 0.772187352180481\n",
      "Epoch 8385: Training Loss: 0.05794547746578852 Validation Loss: 0.7755290865898132\n",
      "Epoch 8386: Training Loss: 0.057542952398459114 Validation Loss: 0.7715150117874146\n",
      "Epoch 8387: Training Loss: 0.05753926932811737 Validation Loss: 0.7662256360054016\n",
      "Epoch 8388: Training Loss: 0.05759003137548765 Validation Loss: 0.7642226219177246\n",
      "Epoch 8389: Training Loss: 0.05788856248060862 Validation Loss: 0.7625859975814819\n",
      "Epoch 8390: Training Loss: 0.05756155028939247 Validation Loss: 0.7659408450126648\n",
      "Epoch 8391: Training Loss: 0.057332348078489304 Validation Loss: 0.7698628902435303\n",
      "Epoch 8392: Training Loss: 0.057372190058231354 Validation Loss: 0.7719129323959351\n",
      "Epoch 8393: Training Loss: 0.057474635541439056 Validation Loss: 0.7746776342391968\n",
      "Epoch 8394: Training Loss: 0.057525775084892906 Validation Loss: 0.7739866971969604\n",
      "Epoch 8395: Training Loss: 0.05757812410593033 Validation Loss: 0.7727609276771545\n",
      "Epoch 8396: Training Loss: 0.05748880530397097 Validation Loss: 0.77214515209198\n",
      "Epoch 8397: Training Loss: 0.057380691170692444 Validation Loss: 0.7689394354820251\n",
      "Epoch 8398: Training Loss: 0.05736849829554558 Validation Loss: 0.7669872045516968\n",
      "Epoch 8399: Training Loss: 0.05741901323199272 Validation Loss: 0.7675178050994873\n",
      "Epoch 8400: Training Loss: 0.05766334136327108 Validation Loss: 0.7652088403701782\n",
      "Epoch 8401: Training Loss: 0.057474538683891296 Validation Loss: 0.7664549350738525\n",
      "Epoch 8402: Training Loss: 0.05736139789223671 Validation Loss: 0.7687263488769531\n",
      "Epoch 8403: Training Loss: 0.05747869362433752 Validation Loss: 0.7732506990432739\n",
      "Epoch 8404: Training Loss: 0.05747290452321371 Validation Loss: 0.7714629173278809\n",
      "Epoch 8405: Training Loss: 0.05773807317018509 Validation Loss: 0.7687114477157593\n",
      "Epoch 8406: Training Loss: 0.05732194830973943 Validation Loss: 0.7691175937652588\n",
      "Epoch 8407: Training Loss: 0.05735437572002411 Validation Loss: 0.7695198059082031\n",
      "Epoch 8408: Training Loss: 0.057351419081290565 Validation Loss: 0.7683011889457703\n",
      "Epoch 8409: Training Loss: 0.05734029784798622 Validation Loss: 0.769684374332428\n",
      "Epoch 8410: Training Loss: 0.05733534569541613 Validation Loss: 0.7708905935287476\n",
      "Epoch 8411: Training Loss: 0.057737654695908226 Validation Loss: 0.7733033895492554\n",
      "Epoch 8412: Training Loss: 0.05750309551755587 Validation Loss: 0.7686243057250977\n",
      "Epoch 8413: Training Loss: 0.057377652575572334 Validation Loss: 0.7654170989990234\n",
      "Epoch 8414: Training Loss: 0.05740658938884735 Validation Loss: 0.7646597623825073\n",
      "Epoch 8415: Training Loss: 0.05733583370844523 Validation Loss: 0.7669901251792908\n",
      "Epoch 8416: Training Loss: 0.05740409965316454 Validation Loss: 0.7705996036529541\n",
      "Epoch 8417: Training Loss: 0.057303511848052345 Validation Loss: 0.7701157331466675\n",
      "Epoch 8418: Training Loss: 0.05732861782113711 Validation Loss: 0.7689903378486633\n",
      "Epoch 8419: Training Loss: 0.057226830472548805 Validation Loss: 0.7707656025886536\n",
      "Epoch 8420: Training Loss: 0.05732072268923124 Validation Loss: 0.7725525498390198\n",
      "Epoch 8421: Training Loss: 0.05733675385514895 Validation Loss: 0.7726221084594727\n",
      "Epoch 8422: Training Loss: 0.05735740934809049 Validation Loss: 0.7690961956977844\n",
      "Epoch 8423: Training Loss: 0.057517554610967636 Validation Loss: 0.771172046661377\n",
      "Epoch 8424: Training Loss: 0.057712300370136894 Validation Loss: 0.7737594246864319\n",
      "Epoch 8425: Training Loss: 0.057192977517843246 Validation Loss: 0.7706931829452515\n",
      "Epoch 8426: Training Loss: 0.05718176687757174 Validation Loss: 0.7672580480575562\n",
      "Epoch 8427: Training Loss: 0.057380713522434235 Validation Loss: 0.7635482549667358\n",
      "Epoch 8428: Training Loss: 0.057313998540242515 Validation Loss: 0.764681875705719\n",
      "Epoch 8429: Training Loss: 0.05729407692948977 Validation Loss: 0.7667177319526672\n",
      "Epoch 8430: Training Loss: 0.057274402429660164 Validation Loss: 0.7684004902839661\n",
      "Epoch 8431: Training Loss: 0.05732820307215055 Validation Loss: 0.7733879685401917\n",
      "Epoch 8432: Training Loss: 0.05726867417494456 Validation Loss: 0.7750551700592041\n",
      "Epoch 8433: Training Loss: 0.05722861737012863 Validation Loss: 0.7727761268615723\n",
      "Epoch 8434: Training Loss: 0.057186818371216454 Validation Loss: 0.7705685496330261\n",
      "Epoch 8435: Training Loss: 0.057186514139175415 Validation Loss: 0.7702850699424744\n",
      "Epoch 8436: Training Loss: 0.05733102932572365 Validation Loss: 0.7655395269393921\n",
      "Epoch 8437: Training Loss: 0.057182084769010544 Validation Loss: 0.7658478021621704\n",
      "Epoch 8438: Training Loss: 0.05722407748301824 Validation Loss: 0.7658146023750305\n",
      "Epoch 8439: Training Loss: 0.0571334200600783 Validation Loss: 0.7672252058982849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8440: Training Loss: 0.057251582543055214 Validation Loss: 0.7676576972007751\n",
      "Epoch 8441: Training Loss: 0.05709434052308401 Validation Loss: 0.769812822341919\n",
      "Epoch 8442: Training Loss: 0.05720546841621399 Validation Loss: 0.7729663252830505\n",
      "Epoch 8443: Training Loss: 0.057136776546637215 Validation Loss: 0.773385763168335\n",
      "Epoch 8444: Training Loss: 0.05711636816461881 Validation Loss: 0.7726197242736816\n",
      "Epoch 8445: Training Loss: 0.05722844103972117 Validation Loss: 0.7693533897399902\n",
      "Epoch 8446: Training Loss: 0.05712092543641726 Validation Loss: 0.7696550488471985\n",
      "Epoch 8447: Training Loss: 0.057131119072437286 Validation Loss: 0.7686262726783752\n",
      "Epoch 8448: Training Loss: 0.057154872765143715 Validation Loss: 0.7687436938285828\n",
      "Epoch 8449: Training Loss: 0.05714232847094536 Validation Loss: 0.7705522775650024\n",
      "Epoch 8450: Training Loss: 0.0574365829428037 Validation Loss: 0.7735310196876526\n",
      "Epoch 8451: Training Loss: 0.057339929044246674 Validation Loss: 0.7693705558776855\n",
      "Epoch 8452: Training Loss: 0.05776922901471456 Validation Loss: 0.7643327116966248\n",
      "Epoch 8453: Training Loss: 0.05724383518099785 Validation Loss: 0.7645562887191772\n",
      "Epoch 8454: Training Loss: 0.05710895359516144 Validation Loss: 0.768049955368042\n",
      "Epoch 8455: Training Loss: 0.057293329387903214 Validation Loss: 0.7737883925437927\n",
      "Epoch 8456: Training Loss: 0.05713797236482302 Validation Loss: 0.775426983833313\n",
      "Epoch 8457: Training Loss: 0.05775211751461029 Validation Loss: 0.7707550525665283\n",
      "Epoch 8458: Training Loss: 0.05704349900285403 Validation Loss: 0.7716445922851562\n",
      "Epoch 8459: Training Loss: 0.05737446372707685 Validation Loss: 0.7746096253395081\n",
      "Epoch 8460: Training Loss: 0.05703891565402349 Validation Loss: 0.7717886567115784\n",
      "Epoch 8461: Training Loss: 0.05709883198142052 Validation Loss: 0.7689065337181091\n",
      "Epoch 8462: Training Loss: 0.057416025549173355 Validation Loss: 0.7652443647384644\n",
      "Epoch 8463: Training Loss: 0.05728763093551 Validation Loss: 0.7695148587226868\n",
      "Epoch 8464: Training Loss: 0.05724245930711428 Validation Loss: 0.76850426197052\n",
      "Epoch 8465: Training Loss: 0.056954436004161835 Validation Loss: 0.7703356742858887\n",
      "Epoch 8466: Training Loss: 0.057057314862807594 Validation Loss: 0.7731937766075134\n",
      "Epoch 8467: Training Loss: 0.057028102378050484 Validation Loss: 0.7737365961074829\n",
      "Epoch 8468: Training Loss: 0.057122040539979935 Validation Loss: 0.7702347636222839\n",
      "Epoch 8469: Training Loss: 0.05701344832777977 Validation Loss: 0.7684345841407776\n",
      "Epoch 8470: Training Loss: 0.05705516661206881 Validation Loss: 0.7671296000480652\n",
      "Epoch 8471: Training Loss: 0.05700356264909109 Validation Loss: 0.7675591111183167\n",
      "Epoch 8472: Training Loss: 0.05706490452090899 Validation Loss: 0.7672834396362305\n",
      "Epoch 8473: Training Loss: 0.0569599394996961 Validation Loss: 0.7689684629440308\n",
      "Epoch 8474: Training Loss: 0.05697888260086378 Validation Loss: 0.7696607708930969\n",
      "Epoch 8475: Training Loss: 0.05699252337217331 Validation Loss: 0.7723104357719421\n",
      "Epoch 8476: Training Loss: 0.05708613991737366 Validation Loss: 0.7747330069541931\n",
      "Epoch 8477: Training Loss: 0.057273091127475105 Validation Loss: 0.7755846381187439\n",
      "Epoch 8478: Training Loss: 0.05700252205133438 Validation Loss: 0.7704082727432251\n",
      "Epoch 8479: Training Loss: 0.05691880484422048 Validation Loss: 0.7666389346122742\n",
      "Epoch 8480: Training Loss: 0.057116700957218804 Validation Loss: 0.7667462825775146\n",
      "Epoch 8481: Training Loss: 0.05694233626127243 Validation Loss: 0.7668317556381226\n",
      "Epoch 8482: Training Loss: 0.057183983425299324 Validation Loss: 0.7651082277297974\n",
      "Epoch 8483: Training Loss: 0.057173931350310646 Validation Loss: 0.769605278968811\n",
      "Epoch 8484: Training Loss: 0.056957945227622986 Validation Loss: 0.7724491357803345\n",
      "Epoch 8485: Training Loss: 0.05701820303996404 Validation Loss: 0.7723720669746399\n",
      "Epoch 8486: Training Loss: 0.056925432135661445 Validation Loss: 0.772247850894928\n",
      "Epoch 8487: Training Loss: 0.057025776555140816 Validation Loss: 0.7709681987762451\n",
      "Epoch 8488: Training Loss: 0.05690175046523412 Validation Loss: 0.7708574533462524\n",
      "Epoch 8489: Training Loss: 0.05705495551228523 Validation Loss: 0.7706735730171204\n",
      "Epoch 8490: Training Loss: 0.056939225643873215 Validation Loss: 0.7684649229049683\n",
      "Epoch 8491: Training Loss: 0.05691311384240786 Validation Loss: 0.7674129605293274\n",
      "Epoch 8492: Training Loss: 0.056916743516922 Validation Loss: 0.7697929739952087\n",
      "Epoch 8493: Training Loss: 0.05706747124592463 Validation Loss: 0.7671979069709778\n",
      "Epoch 8494: Training Loss: 0.05742788314819336 Validation Loss: 0.7731634378433228\n",
      "Epoch 8495: Training Loss: 0.056977976113557816 Validation Loss: 0.7738370895385742\n",
      "Epoch 8496: Training Loss: 0.05694271003206571 Validation Loss: 0.773639976978302\n",
      "Epoch 8497: Training Loss: 0.05693262442946434 Validation Loss: 0.7690293192863464\n",
      "Epoch 8498: Training Loss: 0.0569518581032753 Validation Loss: 0.7673339247703552\n",
      "Epoch 8499: Training Loss: 0.05700649693608284 Validation Loss: 0.765411913394928\n",
      "Epoch 8500: Training Loss: 0.05716267849008242 Validation Loss: 0.7643702626228333\n",
      "Epoch 8501: Training Loss: 0.056892244766155876 Validation Loss: 0.7705273628234863\n",
      "Epoch 8502: Training Loss: 0.056747861206531525 Validation Loss: 0.7730022072792053\n",
      "Epoch 8503: Training Loss: 0.0569166565934817 Validation Loss: 0.775390625\n",
      "Epoch 8504: Training Loss: 0.05700689678390821 Validation Loss: 0.7759916186332703\n",
      "Epoch 8505: Training Loss: 0.056889968613783516 Validation Loss: 0.772875964641571\n",
      "Epoch 8506: Training Loss: 0.056731256345907845 Validation Loss: 0.7705737948417664\n",
      "Epoch 8507: Training Loss: 0.05694681778550148 Validation Loss: 0.7680419087409973\n",
      "Epoch 8508: Training Loss: 0.056894672413667045 Validation Loss: 0.7695509195327759\n",
      "Epoch 8509: Training Loss: 0.05689931164185206 Validation Loss: 0.7694587111473083\n",
      "Epoch 8510: Training Loss: 0.05681413287917773 Validation Loss: 0.7706335783004761\n",
      "Epoch 8511: Training Loss: 0.05679239829381307 Validation Loss: 0.7690350413322449\n",
      "Epoch 8512: Training Loss: 0.05677100643515587 Validation Loss: 0.7700564861297607\n",
      "Epoch 8513: Training Loss: 0.05685710037748019 Validation Loss: 0.7700449228286743\n",
      "Epoch 8514: Training Loss: 0.05682507281502088 Validation Loss: 0.7684527039527893\n",
      "Epoch 8515: Training Loss: 0.05676965415477753 Validation Loss: 0.7680055499076843\n",
      "Epoch 8516: Training Loss: 0.05675513421495756 Validation Loss: 0.7703367471694946\n",
      "Epoch 8517: Training Loss: 0.05694892878333727 Validation Loss: 0.7686432600021362\n",
      "Epoch 8518: Training Loss: 0.0568634569644928 Validation Loss: 0.7726995348930359\n",
      "Epoch 8519: Training Loss: 0.05699888120094935 Validation Loss: 0.7754991054534912\n",
      "Epoch 8520: Training Loss: 0.05684925243258476 Validation Loss: 0.7714375853538513\n",
      "Epoch 8521: Training Loss: 0.05668705950180689 Validation Loss: 0.7694044709205627\n",
      "Epoch 8522: Training Loss: 0.05674366280436516 Validation Loss: 0.7686051726341248\n",
      "Epoch 8523: Training Loss: 0.0567002035677433 Validation Loss: 0.7682579755783081\n",
      "Epoch 8524: Training Loss: 0.05678486575682958 Validation Loss: 0.7710657119750977\n",
      "Epoch 8525: Training Loss: 0.05671121304233869 Validation Loss: 0.7714217305183411\n",
      "Epoch 8526: Training Loss: 0.05669405062993368 Validation Loss: 0.7719284296035767\n",
      "Epoch 8527: Training Loss: 0.05669712151090304 Validation Loss: 0.7703838348388672\n",
      "Epoch 8528: Training Loss: 0.05673165122667948 Validation Loss: 0.7688333988189697\n",
      "Epoch 8529: Training Loss: 0.056684220830599465 Validation Loss: 0.7686461806297302\n",
      "Epoch 8530: Training Loss: 0.05669743691881498 Validation Loss: 0.7684627175331116\n",
      "Epoch 8531: Training Loss: 0.05730741595228513 Validation Loss: 0.7748537659645081\n",
      "Epoch 8532: Training Loss: 0.056826515744129814 Validation Loss: 0.7752770781517029\n",
      "Epoch 8533: Training Loss: 0.056688801695903145 Validation Loss: 0.7709877490997314\n",
      "Epoch 8534: Training Loss: 0.057380300015211105 Validation Loss: 0.7657216787338257\n",
      "Epoch 8535: Training Loss: 0.056702585270007454 Validation Loss: 0.7657337784767151\n",
      "Epoch 8536: Training Loss: 0.05672256276011467 Validation Loss: 0.7666077017784119\n",
      "Epoch 8537: Training Loss: 0.0566394937535127 Validation Loss: 0.7710171341896057\n",
      "Epoch 8538: Training Loss: 0.05675452450911204 Validation Loss: 0.7748447060585022\n",
      "Epoch 8539: Training Loss: 0.05681677659352621 Validation Loss: 0.7730991244316101\n",
      "Epoch 8540: Training Loss: 0.05667464186747869 Validation Loss: 0.773337185382843\n",
      "Epoch 8541: Training Loss: 0.056746880213419594 Validation Loss: 0.7741777300834656\n",
      "Epoch 8542: Training Loss: 0.05662154903014501 Validation Loss: 0.7726130485534668\n",
      "Epoch 8543: Training Loss: 0.056621781239906945 Validation Loss: 0.7687844634056091\n",
      "Epoch 8544: Training Loss: 0.05658887947599093 Validation Loss: 0.7671396732330322\n",
      "Epoch 8545: Training Loss: 0.056676131983598076 Validation Loss: 0.769008457660675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8546: Training Loss: 0.056955392162005104 Validation Loss: 0.7724955081939697\n",
      "Epoch 8547: Training Loss: 0.05661001428961754 Validation Loss: 0.7717157006263733\n",
      "Epoch 8548: Training Loss: 0.056677330285310745 Validation Loss: 0.768643856048584\n",
      "Epoch 8549: Training Loss: 0.056561281283696495 Validation Loss: 0.7688819766044617\n",
      "Epoch 8550: Training Loss: 0.05674108862876892 Validation Loss: 0.770630419254303\n",
      "Epoch 8551: Training Loss: 0.05660699804623922 Validation Loss: 0.7705584764480591\n",
      "Epoch 8552: Training Loss: 0.05668410534660021 Validation Loss: 0.7723075747489929\n",
      "Epoch 8553: Training Loss: 0.05655927459398905 Validation Loss: 0.7710485458374023\n",
      "Epoch 8554: Training Loss: 0.056548960506916046 Validation Loss: 0.7689512372016907\n",
      "Epoch 8555: Training Loss: 0.056590368350346885 Validation Loss: 0.7690854072570801\n",
      "Epoch 8556: Training Loss: 0.056548845022916794 Validation Loss: 0.7690553069114685\n",
      "Epoch 8557: Training Loss: 0.056649246563514076 Validation Loss: 0.7713316082954407\n",
      "Epoch 8558: Training Loss: 0.05661857624848684 Validation Loss: 0.7719288468360901\n",
      "Epoch 8559: Training Loss: 0.05653029804428419 Validation Loss: 0.7722578644752502\n",
      "Epoch 8560: Training Loss: 0.056616853922605515 Validation Loss: 0.7687121629714966\n",
      "Epoch 8561: Training Loss: 0.056502612928549446 Validation Loss: 0.7687279582023621\n",
      "Epoch 8562: Training Loss: 0.05647177125016848 Validation Loss: 0.7692402005195618\n",
      "Epoch 8563: Training Loss: 0.05657739813129107 Validation Loss: 0.7701894640922546\n",
      "Epoch 8564: Training Loss: 0.05655936896800995 Validation Loss: 0.769604504108429\n",
      "Epoch 8565: Training Loss: 0.056474766383568444 Validation Loss: 0.7724684476852417\n",
      "Epoch 8566: Training Loss: 0.056560839215914406 Validation Loss: 0.7742206454277039\n",
      "Epoch 8567: Training Loss: 0.056580918530623116 Validation Loss: 0.7719327807426453\n",
      "Epoch 8568: Training Loss: 0.056423322608073555 Validation Loss: 0.7720092535018921\n",
      "Epoch 8569: Training Loss: 0.05644598106543223 Validation Loss: 0.7711713910102844\n",
      "Epoch 8570: Training Loss: 0.056494928896427155 Validation Loss: 0.7699156999588013\n",
      "Epoch 8571: Training Loss: 0.05662029609084129 Validation Loss: 0.7680513858795166\n",
      "Epoch 8572: Training Loss: 0.056473596642414726 Validation Loss: 0.7709115147590637\n",
      "Epoch 8573: Training Loss: 0.0564467099805673 Validation Loss: 0.7724481225013733\n",
      "Epoch 8574: Training Loss: 0.05642523244023323 Validation Loss: 0.771691083908081\n",
      "Epoch 8575: Training Loss: 0.05640279625852903 Validation Loss: 0.7719166874885559\n",
      "Epoch 8576: Training Loss: 0.05658023928602537 Validation Loss: 0.772706925868988\n",
      "Epoch 8577: Training Loss: 0.0564312698940436 Validation Loss: 0.7721900343894958\n",
      "Epoch 8578: Training Loss: 0.05652206142743429 Validation Loss: 0.7730668187141418\n",
      "Epoch 8579: Training Loss: 0.056584201753139496 Validation Loss: 0.7684949636459351\n",
      "Epoch 8580: Training Loss: 0.05644010131557783 Validation Loss: 0.7679293751716614\n",
      "Epoch 8581: Training Loss: 0.05641227836410204 Validation Loss: 0.769666850566864\n",
      "Epoch 8582: Training Loss: 0.05635976046323776 Validation Loss: 0.7706587910652161\n",
      "Epoch 8583: Training Loss: 0.056348029524087906 Validation Loss: 0.7723726034164429\n",
      "Epoch 8584: Training Loss: 0.05668937414884567 Validation Loss: 0.7758042216300964\n",
      "Epoch 8585: Training Loss: 0.05638632426659266 Validation Loss: 0.7750228047370911\n",
      "Epoch 8586: Training Loss: 0.056355539709329605 Validation Loss: 0.7725750803947449\n",
      "Epoch 8587: Training Loss: 0.05636913329362869 Validation Loss: 0.7693314552307129\n",
      "Epoch 8588: Training Loss: 0.05639565239350001 Validation Loss: 0.7703631520271301\n",
      "Epoch 8589: Training Loss: 0.056391620387633644 Validation Loss: 0.7701610922813416\n",
      "Epoch 8590: Training Loss: 0.05633596579233805 Validation Loss: 0.769888699054718\n",
      "Epoch 8591: Training Loss: 0.05644481753309568 Validation Loss: 0.7718642354011536\n",
      "Epoch 8592: Training Loss: 0.05638225500782331 Validation Loss: 0.7697988152503967\n",
      "Epoch 8593: Training Loss: 0.05640005196134249 Validation Loss: 0.7719547152519226\n",
      "Epoch 8594: Training Loss: 0.05646128952503204 Validation Loss: 0.7697710990905762\n",
      "Epoch 8595: Training Loss: 0.05644667148590088 Validation Loss: 0.7719435691833496\n",
      "Epoch 8596: Training Loss: 0.05637489507595698 Validation Loss: 0.7722348570823669\n",
      "Epoch 8597: Training Loss: 0.056393993397553764 Validation Loss: 0.7709032297134399\n",
      "Epoch 8598: Training Loss: 0.05638270080089569 Validation Loss: 0.7692646980285645\n",
      "Epoch 8599: Training Loss: 0.05642936130364736 Validation Loss: 0.7726405262947083\n",
      "Epoch 8600: Training Loss: 0.056368873765071235 Validation Loss: 0.7704246044158936\n",
      "Epoch 8601: Training Loss: 0.05630182599027952 Validation Loss: 0.770815908908844\n",
      "Epoch 8602: Training Loss: 0.056349895894527435 Validation Loss: 0.7730569243431091\n",
      "Epoch 8603: Training Loss: 0.05629500995079676 Validation Loss: 0.7719680070877075\n",
      "Epoch 8604: Training Loss: 0.0563499021033446 Validation Loss: 0.7717870473861694\n",
      "Epoch 8605: Training Loss: 0.05626613895098368 Validation Loss: 0.7708679437637329\n",
      "Epoch 8606: Training Loss: 0.05638009185592333 Validation Loss: 0.7720149159431458\n",
      "Epoch 8607: Training Loss: 0.05642899374167124 Validation Loss: 0.7737064957618713\n",
      "Epoch 8608: Training Loss: 0.056221148620049156 Validation Loss: 0.7724996209144592\n",
      "Epoch 8609: Training Loss: 0.056191260615984596 Validation Loss: 0.7700083255767822\n",
      "Epoch 8610: Training Loss: 0.056253028412659965 Validation Loss: 0.7687317728996277\n",
      "Epoch 8611: Training Loss: 0.056272478153308235 Validation Loss: 0.7683101892471313\n",
      "Epoch 8612: Training Loss: 0.056280133624871574 Validation Loss: 0.7709664702415466\n",
      "Epoch 8613: Training Loss: 0.05625952407717705 Validation Loss: 0.7705618143081665\n",
      "Epoch 8614: Training Loss: 0.05622700725992521 Validation Loss: 0.7711714506149292\n",
      "Epoch 8615: Training Loss: 0.056338795771201454 Validation Loss: 0.7710424065589905\n",
      "Epoch 8616: Training Loss: 0.05644821127255758 Validation Loss: 0.7758336663246155\n",
      "Epoch 8617: Training Loss: 0.056275044878323875 Validation Loss: 0.7749432921409607\n",
      "Epoch 8618: Training Loss: 0.05637838070591291 Validation Loss: 0.7717083692550659\n",
      "Epoch 8619: Training Loss: 0.056190104534228645 Validation Loss: 0.7711115479469299\n",
      "Epoch 8620: Training Loss: 0.05621881286303202 Validation Loss: 0.7718729376792908\n",
      "Epoch 8621: Training Loss: 0.05615739276011785 Validation Loss: 0.7711983323097229\n",
      "Epoch 8622: Training Loss: 0.0561856838564078 Validation Loss: 0.7717840075492859\n",
      "Epoch 8623: Training Loss: 0.05615432187914848 Validation Loss: 0.7718690633773804\n",
      "Epoch 8624: Training Loss: 0.05626819531122843 Validation Loss: 0.7713814377784729\n",
      "Epoch 8625: Training Loss: 0.056364423284928 Validation Loss: 0.7732020616531372\n",
      "Epoch 8626: Training Loss: 0.05619996413588524 Validation Loss: 0.7698430418968201\n",
      "Epoch 8627: Training Loss: 0.05643140648802122 Validation Loss: 0.7669145464897156\n",
      "Epoch 8628: Training Loss: 0.056398600339889526 Validation Loss: 0.7659841179847717\n",
      "Epoch 8629: Training Loss: 0.056292193631331124 Validation Loss: 0.7706366181373596\n",
      "Epoch 8630: Training Loss: 0.05638377492626508 Validation Loss: 0.7755123972892761\n",
      "Epoch 8631: Training Loss: 0.05626353248953819 Validation Loss: 0.7747586965560913\n",
      "Epoch 8632: Training Loss: 0.05615640928347906 Validation Loss: 0.773675799369812\n",
      "Epoch 8633: Training Loss: 0.05611575643221537 Validation Loss: 0.7726334929466248\n",
      "Epoch 8634: Training Loss: 0.056159784396489464 Validation Loss: 0.7697291970252991\n",
      "Epoch 8635: Training Loss: 0.05627369383970896 Validation Loss: 0.7683984041213989\n",
      "Epoch 8636: Training Loss: 0.05617488920688629 Validation Loss: 0.7713038325309753\n",
      "Epoch 8637: Training Loss: 0.05613990252216657 Validation Loss: 0.7731210589408875\n",
      "Epoch 8638: Training Loss: 0.05607777585585912 Validation Loss: 0.7739596962928772\n",
      "Epoch 8639: Training Loss: 0.056171830743551254 Validation Loss: 0.7721571326255798\n",
      "Epoch 8640: Training Loss: 0.056086473166942596 Validation Loss: 0.7719690203666687\n",
      "Epoch 8641: Training Loss: 0.05610475316643715 Validation Loss: 0.7723848819732666\n",
      "Epoch 8642: Training Loss: 0.05611199761430422 Validation Loss: 0.7710879445075989\n",
      "Epoch 8643: Training Loss: 0.056135065853595734 Validation Loss: 0.7708312273025513\n",
      "Epoch 8644: Training Loss: 0.056056611239910126 Validation Loss: 0.7710824608802795\n",
      "Epoch 8645: Training Loss: 0.0561550036072731 Validation Loss: 0.7697932124137878\n",
      "Epoch 8646: Training Loss: 0.056390199810266495 Validation Loss: 0.7743552327156067\n",
      "Epoch 8647: Training Loss: 0.056070288022359215 Validation Loss: 0.7745604515075684\n",
      "Epoch 8648: Training Loss: 0.05607243999838829 Validation Loss: 0.7722491025924683\n",
      "Epoch 8649: Training Loss: 0.05603647604584694 Validation Loss: 0.7713097929954529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8650: Training Loss: 0.05607201159000397 Validation Loss: 0.7704284191131592\n",
      "Epoch 8651: Training Loss: 0.056105438619852066 Validation Loss: 0.7693066000938416\n",
      "Epoch 8652: Training Loss: 0.05611217146118482 Validation Loss: 0.7717952132225037\n",
      "Epoch 8653: Training Loss: 0.056071870028972626 Validation Loss: 0.7716392874717712\n",
      "Epoch 8654: Training Loss: 0.05629377936323484 Validation Loss: 0.7751173973083496\n",
      "Epoch 8655: Training Loss: 0.056078958014647164 Validation Loss: 0.7748743891716003\n",
      "Epoch 8656: Training Loss: 0.05606901148955027 Validation Loss: 0.773932695388794\n",
      "Epoch 8657: Training Loss: 0.05608480175336202 Validation Loss: 0.7709214091300964\n",
      "Epoch 8658: Training Loss: 0.05595125754674276 Validation Loss: 0.7702445983886719\n",
      "Epoch 8659: Training Loss: 0.056173376739025116 Validation Loss: 0.7716597318649292\n",
      "Epoch 8660: Training Loss: 0.05601982275644938 Validation Loss: 0.7711933851242065\n",
      "Epoch 8661: Training Loss: 0.05603699137767156 Validation Loss: 0.7692785263061523\n",
      "Epoch 8662: Training Loss: 0.0560135580599308 Validation Loss: 0.7684147357940674\n",
      "Epoch 8663: Training Loss: 0.05597065885861715 Validation Loss: 0.771049976348877\n",
      "Epoch 8664: Training Loss: 0.0561892402668794 Validation Loss: 0.7750193476676941\n",
      "Epoch 8665: Training Loss: 0.05597136418024699 Validation Loss: 0.7747789025306702\n",
      "Epoch 8666: Training Loss: 0.055932248632113137 Validation Loss: 0.7731338143348694\n",
      "Epoch 8667: Training Loss: 0.05591356381773949 Validation Loss: 0.771030843257904\n",
      "Epoch 8668: Training Loss: 0.0559103861451149 Validation Loss: 0.7702327966690063\n",
      "Epoch 8669: Training Loss: 0.05605403333902359 Validation Loss: 0.7684946060180664\n",
      "Epoch 8670: Training Loss: 0.056119573613007866 Validation Loss: 0.7680858969688416\n",
      "Epoch 8671: Training Loss: 0.05593534434835116 Validation Loss: 0.7705082297325134\n",
      "Epoch 8672: Training Loss: 0.05591994275649389 Validation Loss: 0.7720882892608643\n",
      "Epoch 8673: Training Loss: 0.05587644005815188 Validation Loss: 0.7734696865081787\n",
      "Epoch 8674: Training Loss: 0.05604727193713188 Validation Loss: 0.7768772840499878\n",
      "Epoch 8675: Training Loss: 0.05604965550204118 Validation Loss: 0.7762729525566101\n",
      "Epoch 8676: Training Loss: 0.056000515818595886 Validation Loss: 0.7762252688407898\n",
      "Epoch 8677: Training Loss: 0.055880628526210785 Validation Loss: 0.7732207179069519\n",
      "Epoch 8678: Training Loss: 0.055841670682032905 Validation Loss: 0.7709300518035889\n",
      "Epoch 8679: Training Loss: 0.05586721623937289 Validation Loss: 0.7695292234420776\n",
      "Epoch 8680: Training Loss: 0.056023553013801575 Validation Loss: 0.7688267230987549\n",
      "Epoch 8681: Training Loss: 0.05602761233846346 Validation Loss: 0.7711296677589417\n",
      "Epoch 8682: Training Loss: 0.05588226765394211 Validation Loss: 0.7721912860870361\n",
      "Epoch 8683: Training Loss: 0.055852241814136505 Validation Loss: 0.7728033661842346\n",
      "Epoch 8684: Training Loss: 0.056061903635660805 Validation Loss: 0.7745754718780518\n",
      "Epoch 8685: Training Loss: 0.056301952650149666 Validation Loss: 0.769464910030365\n",
      "Epoch 8686: Training Loss: 0.05613085503379504 Validation Loss: 0.7675739526748657\n",
      "Epoch 8687: Training Loss: 0.05591540659467379 Validation Loss: 0.7704024314880371\n",
      "Epoch 8688: Training Loss: 0.0557974341015021 Validation Loss: 0.7729034423828125\n",
      "Epoch 8689: Training Loss: 0.05590068052212397 Validation Loss: 0.7764538526535034\n",
      "Epoch 8690: Training Loss: 0.05592793971300125 Validation Loss: 0.7752566933631897\n",
      "Epoch 8691: Training Loss: 0.05610739439725876 Validation Loss: 0.7745519876480103\n",
      "Epoch 8692: Training Loss: 0.05591051901380221 Validation Loss: 0.7752132415771484\n",
      "Epoch 8693: Training Loss: 0.056120406836271286 Validation Loss: 0.7701593637466431\n",
      "Epoch 8694: Training Loss: 0.05605507021149 Validation Loss: 0.7730249762535095\n",
      "Epoch 8695: Training Loss: 0.05583961680531502 Validation Loss: 0.7730854749679565\n",
      "Epoch 8696: Training Loss: 0.055827771623929344 Validation Loss: 0.7724894285202026\n",
      "Epoch 8697: Training Loss: 0.05581204096476237 Validation Loss: 0.7708330750465393\n",
      "Epoch 8698: Training Loss: 0.05587902292609215 Validation Loss: 0.7693679928779602\n",
      "Epoch 8699: Training Loss: 0.0559491291642189 Validation Loss: 0.7728964686393738\n",
      "Epoch 8700: Training Loss: 0.055854582538207374 Validation Loss: 0.7737969756126404\n",
      "Epoch 8701: Training Loss: 0.05629578729470571 Validation Loss: 0.7694084644317627\n",
      "Epoch 8702: Training Loss: 0.05577409888307253 Validation Loss: 0.7698497772216797\n",
      "Epoch 8703: Training Loss: 0.05573998888333639 Validation Loss: 0.7715582847595215\n",
      "Epoch 8704: Training Loss: 0.055739047626654305 Validation Loss: 0.7736047506332397\n",
      "Epoch 8705: Training Loss: 0.05575257415572802 Validation Loss: 0.7749416828155518\n",
      "Epoch 8706: Training Loss: 0.0557762049138546 Validation Loss: 0.7754158973693848\n",
      "Epoch 8707: Training Loss: 0.05574570596218109 Validation Loss: 0.774343729019165\n",
      "Epoch 8708: Training Loss: 0.05587217087546984 Validation Loss: 0.7708745002746582\n",
      "Epoch 8709: Training Loss: 0.05591735119620959 Validation Loss: 0.7689907550811768\n",
      "Epoch 8710: Training Loss: 0.05574195459485054 Validation Loss: 0.7706642150878906\n",
      "Epoch 8711: Training Loss: 0.055949060867230095 Validation Loss: 0.7693137526512146\n",
      "Epoch 8712: Training Loss: 0.05678319310148557 Validation Loss: 0.777704119682312\n",
      "Epoch 8713: Training Loss: 0.05579090739289919 Validation Loss: 0.7766799330711365\n",
      "Epoch 8714: Training Loss: 0.05575591449936231 Validation Loss: 0.7751720547676086\n",
      "Epoch 8715: Training Loss: 0.055745482444763184 Validation Loss: 0.7742269039154053\n",
      "Epoch 8716: Training Loss: 0.05571919431289037 Validation Loss: 0.770621657371521\n",
      "Epoch 8717: Training Loss: 0.055964062611262 Validation Loss: 0.767448902130127\n",
      "Epoch 8718: Training Loss: 0.05590639263391495 Validation Loss: 0.7711886167526245\n",
      "Epoch 8719: Training Loss: 0.055657471219698586 Validation Loss: 0.7721846103668213\n",
      "Epoch 8720: Training Loss: 0.055738918483257294 Validation Loss: 0.7750765681266785\n",
      "Epoch 8721: Training Loss: 0.055827723195155464 Validation Loss: 0.7756073474884033\n",
      "Epoch 8722: Training Loss: 0.055965746442476906 Validation Loss: 0.777398407459259\n",
      "Epoch 8723: Training Loss: 0.055710124472777046 Validation Loss: 0.774457573890686\n",
      "Epoch 8724: Training Loss: 0.05574568112691244 Validation Loss: 0.7713987827301025\n",
      "Epoch 8725: Training Loss: 0.05567756171027819 Validation Loss: 0.7714052796363831\n",
      "Epoch 8726: Training Loss: 0.0559720570842425 Validation Loss: 0.7739230394363403\n",
      "Epoch 8727: Training Loss: 0.055767469108104706 Validation Loss: 0.7714727520942688\n",
      "Epoch 8728: Training Loss: 0.05569951608777046 Validation Loss: 0.7720954418182373\n",
      "Epoch 8729: Training Loss: 0.05559125542640686 Validation Loss: 0.771719217300415\n",
      "Epoch 8730: Training Loss: 0.05563367158174515 Validation Loss: 0.7707684636116028\n",
      "Epoch 8731: Training Loss: 0.056057444463173546 Validation Loss: 0.7675891518592834\n",
      "Epoch 8732: Training Loss: 0.05583947276075681 Validation Loss: 0.7691677808761597\n",
      "Epoch 8733: Training Loss: 0.05562936638792356 Validation Loss: 0.7733845114707947\n",
      "Epoch 8734: Training Loss: 0.05558341617385546 Validation Loss: 0.7757628560066223\n",
      "Epoch 8735: Training Loss: 0.05565133566657702 Validation Loss: 0.7778174877166748\n",
      "Epoch 8736: Training Loss: 0.05578161031007767 Validation Loss: 0.7789980173110962\n",
      "Epoch 8737: Training Loss: 0.0556638166308403 Validation Loss: 0.7757120728492737\n",
      "Epoch 8738: Training Loss: 0.055743634700775146 Validation Loss: 0.7716186046600342\n",
      "Epoch 8739: Training Loss: 0.055674328158299126 Validation Loss: 0.7692254185676575\n",
      "Epoch 8740: Training Loss: 0.05567578971385956 Validation Loss: 0.769662082195282\n",
      "Epoch 8741: Training Loss: 0.05558649202187856 Validation Loss: 0.7704172730445862\n",
      "Epoch 8742: Training Loss: 0.05580263460675875 Validation Loss: 0.7751211524009705\n",
      "Epoch 8743: Training Loss: 0.05585873872041702 Validation Loss: 0.7780242562294006\n",
      "Epoch 8744: Training Loss: 0.055797563244899116 Validation Loss: 0.7737124562263489\n",
      "Epoch 8745: Training Loss: 0.05577896162867546 Validation Loss: 0.7697234749794006\n",
      "Epoch 8746: Training Loss: 0.05584647630651792 Validation Loss: 0.7726888656616211\n",
      "Epoch 8747: Training Loss: 0.05551219855745634 Validation Loss: 0.7724249958992004\n",
      "Epoch 8748: Training Loss: 0.05558414260546366 Validation Loss: 0.7716524600982666\n",
      "Epoch 8749: Training Loss: 0.05554768815636635 Validation Loss: 0.7725911736488342\n",
      "Epoch 8750: Training Loss: 0.05596359198292097 Validation Loss: 0.7761439681053162\n",
      "Epoch 8751: Training Loss: 0.05560025696953138 Validation Loss: 0.7752212285995483\n",
      "Epoch 8752: Training Loss: 0.05555081988374392 Validation Loss: 0.7718672752380371\n",
      "Epoch 8753: Training Loss: 0.0555442509551843 Validation Loss: 0.7700341939926147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8754: Training Loss: 0.05579644193251928 Validation Loss: 0.7670064568519592\n",
      "Epoch 8755: Training Loss: 0.055557637164990105 Validation Loss: 0.7700327634811401\n",
      "Epoch 8756: Training Loss: 0.055803402016560234 Validation Loss: 0.7696744799613953\n",
      "Epoch 8757: Training Loss: 0.05545634776353836 Validation Loss: 0.773116946220398\n",
      "Epoch 8758: Training Loss: 0.055809080600738525 Validation Loss: 0.780069887638092\n",
      "Epoch 8759: Training Loss: 0.055643814305464424 Validation Loss: 0.7799001336097717\n",
      "Epoch 8760: Training Loss: 0.055575759460528694 Validation Loss: 0.7775960564613342\n",
      "Epoch 8761: Training Loss: 0.05569943661491076 Validation Loss: 0.7731987833976746\n",
      "Epoch 8762: Training Loss: 0.055572601656119026 Validation Loss: 0.7736995220184326\n",
      "Epoch 8763: Training Loss: 0.055483161161343254 Validation Loss: 0.7726913690567017\n",
      "Epoch 8764: Training Loss: 0.055428411811590195 Validation Loss: 0.7713438868522644\n",
      "Epoch 8765: Training Loss: 0.05547351762652397 Validation Loss: 0.7692158222198486\n",
      "Epoch 8766: Training Loss: 0.05564070741335551 Validation Loss: 0.767907977104187\n",
      "Epoch 8767: Training Loss: 0.05559679990013441 Validation Loss: 0.7691535353660583\n",
      "Epoch 8768: Training Loss: 0.055526903520027794 Validation Loss: 0.7737826108932495\n",
      "Epoch 8769: Training Loss: 0.05545379842321078 Validation Loss: 0.775449275970459\n",
      "Epoch 8770: Training Loss: 0.05540959909558296 Validation Loss: 0.7768898606300354\n",
      "Epoch 8771: Training Loss: 0.055527212719122566 Validation Loss: 0.7748361229896545\n",
      "Epoch 8772: Training Loss: 0.05544020483891169 Validation Loss: 0.774282693862915\n",
      "Epoch 8773: Training Loss: 0.05545098582903544 Validation Loss: 0.7748691439628601\n",
      "Epoch 8774: Training Loss: 0.055394637087980904 Validation Loss: 0.7738828063011169\n",
      "Epoch 8775: Training Loss: 0.055562163392702736 Validation Loss: 0.7703872323036194\n",
      "Epoch 8776: Training Loss: 0.05558385451634725 Validation Loss: 0.7721436023712158\n",
      "Epoch 8777: Training Loss: 0.055510846277077995 Validation Loss: 0.7710052728652954\n",
      "Epoch 8778: Training Loss: 0.05542415380477905 Validation Loss: 0.7732507586479187\n",
      "Epoch 8779: Training Loss: 0.05552861963709196 Validation Loss: 0.7763935327529907\n",
      "Epoch 8780: Training Loss: 0.055527135729789734 Validation Loss: 0.7772443294525146\n",
      "Epoch 8781: Training Loss: 0.055502661814292274 Validation Loss: 0.772504448890686\n",
      "Epoch 8782: Training Loss: 0.05550324420134226 Validation Loss: 0.7693454027175903\n",
      "Epoch 8783: Training Loss: 0.05541186034679413 Validation Loss: 0.7696961760520935\n",
      "Epoch 8784: Training Loss: 0.05542697633306185 Validation Loss: 0.7714752554893494\n",
      "Epoch 8785: Training Loss: 0.05541823183496793 Validation Loss: 0.7728678584098816\n",
      "Epoch 8786: Training Loss: 0.05533728872736295 Validation Loss: 0.7750717401504517\n",
      "Epoch 8787: Training Loss: 0.055462015171845756 Validation Loss: 0.7778475880622864\n",
      "Epoch 8788: Training Loss: 0.0555735689898332 Validation Loss: 0.7745910882949829\n",
      "Epoch 8789: Training Loss: 0.05549096564451853 Validation Loss: 0.772269606590271\n",
      "Epoch 8790: Training Loss: 0.055389683693647385 Validation Loss: 0.7719704508781433\n",
      "Epoch 8791: Training Loss: 0.05548832441369692 Validation Loss: 0.7740036845207214\n",
      "Epoch 8792: Training Loss: 0.055420465767383575 Validation Loss: 0.77303546667099\n",
      "Epoch 8793: Training Loss: 0.055536569406588875 Validation Loss: 0.7768561244010925\n",
      "Epoch 8794: Training Loss: 0.05538798744479815 Validation Loss: 0.777237057685852\n",
      "Epoch 8795: Training Loss: 0.05545428395271301 Validation Loss: 0.7742630839347839\n",
      "Epoch 8796: Training Loss: 0.0553883413473765 Validation Loss: 0.7730579972267151\n",
      "Epoch 8797: Training Loss: 0.055326792101065315 Validation Loss: 0.7739471793174744\n",
      "Epoch 8798: Training Loss: 0.0552783856789271 Validation Loss: 0.7730523347854614\n",
      "Epoch 8799: Training Loss: 0.05535859242081642 Validation Loss: 0.771708071231842\n",
      "Epoch 8800: Training Loss: 0.055306460708379745 Validation Loss: 0.772377610206604\n",
      "Epoch 8801: Training Loss: 0.055285183091958366 Validation Loss: 0.7736670970916748\n",
      "Epoch 8802: Training Loss: 0.055408610651890434 Validation Loss: 0.7733620405197144\n",
      "Epoch 8803: Training Loss: 0.05539765457312266 Validation Loss: 0.7757629752159119\n",
      "Epoch 8804: Training Loss: 0.05559613058964411 Validation Loss: 0.772042989730835\n",
      "Epoch 8805: Training Loss: 0.055296167731285095 Validation Loss: 0.7716585397720337\n",
      "Epoch 8806: Training Loss: 0.0552596685787042 Validation Loss: 0.7723762392997742\n",
      "Epoch 8807: Training Loss: 0.055315924187501274 Validation Loss: 0.7747165560722351\n",
      "Epoch 8808: Training Loss: 0.05524038647611936 Validation Loss: 0.7757117748260498\n",
      "Epoch 8809: Training Loss: 0.05533824612696966 Validation Loss: 0.7764644622802734\n",
      "Epoch 8810: Training Loss: 0.05532858024040858 Validation Loss: 0.7733255624771118\n",
      "Epoch 8811: Training Loss: 0.05563012262185415 Validation Loss: 0.7766649723052979\n",
      "Epoch 8812: Training Loss: 0.055196501314640045 Validation Loss: 0.7745198607444763\n",
      "Epoch 8813: Training Loss: 0.055590249598026276 Validation Loss: 0.7700084447860718\n",
      "Epoch 8814: Training Loss: 0.055250417441129684 Validation Loss: 0.7699573040008545\n",
      "Epoch 8815: Training Loss: 0.05563260490695635 Validation Loss: 0.7744844555854797\n",
      "Epoch 8816: Training Loss: 0.055245257914066315 Validation Loss: 0.7740249037742615\n",
      "Epoch 8817: Training Loss: 0.05529411882162094 Validation Loss: 0.7737138271331787\n",
      "Epoch 8818: Training Loss: 0.05523897334933281 Validation Loss: 0.7722664475440979\n",
      "Epoch 8819: Training Loss: 0.055222038179636 Validation Loss: 0.7709730863571167\n",
      "Epoch 8820: Training Loss: 0.05521670853098234 Validation Loss: 0.7724354863166809\n",
      "Epoch 8821: Training Loss: 0.05530773972471555 Validation Loss: 0.7716150879859924\n",
      "Epoch 8822: Training Loss: 0.0553020549317201 Validation Loss: 0.7754691243171692\n",
      "Epoch 8823: Training Loss: 0.0551740862429142 Validation Loss: 0.7766132950782776\n",
      "Epoch 8824: Training Loss: 0.05527494599421819 Validation Loss: 0.7747581601142883\n",
      "Epoch 8825: Training Loss: 0.055543847382068634 Validation Loss: 0.7768615484237671\n",
      "Epoch 8826: Training Loss: 0.05537774289647738 Validation Loss: 0.7773289680480957\n",
      "Epoch 8827: Training Loss: 0.05513289819161097 Validation Loss: 0.7753741145133972\n",
      "Epoch 8828: Training Loss: 0.05519595742225647 Validation Loss: 0.7715139389038086\n",
      "Epoch 8829: Training Loss: 0.055185042321681976 Validation Loss: 0.7714467644691467\n",
      "Epoch 8830: Training Loss: 0.05530867725610733 Validation Loss: 0.7699359059333801\n",
      "Epoch 8831: Training Loss: 0.05525833616654078 Validation Loss: 0.7695105671882629\n",
      "Epoch 8832: Training Loss: 0.0552072674036026 Validation Loss: 0.7727221250534058\n",
      "Epoch 8833: Training Loss: 0.05512508129080137 Validation Loss: 0.7747659683227539\n",
      "Epoch 8834: Training Loss: 0.055158790200948715 Validation Loss: 0.7775179743766785\n",
      "Epoch 8835: Training Loss: 0.05525354544321696 Validation Loss: 0.7784379124641418\n",
      "Epoch 8836: Training Loss: 0.055191713074843086 Validation Loss: 0.7746263742446899\n",
      "Epoch 8837: Training Loss: 0.05518519009153048 Validation Loss: 0.7712201476097107\n",
      "Epoch 8838: Training Loss: 0.05526969706018766 Validation Loss: 0.7730454206466675\n",
      "Epoch 8839: Training Loss: 0.05509636178612709 Validation Loss: 0.7725980281829834\n",
      "Epoch 8840: Training Loss: 0.05563949793577194 Validation Loss: 0.775486946105957\n",
      "Epoch 8841: Training Loss: 0.055302261064449944 Validation Loss: 0.7711662650108337\n",
      "Epoch 8842: Training Loss: 0.05523045981923739 Validation Loss: 0.7697020173072815\n",
      "Epoch 8843: Training Loss: 0.05511536573370298 Validation Loss: 0.770454466342926\n",
      "Epoch 8844: Training Loss: 0.05509962265690168 Validation Loss: 0.771472692489624\n",
      "Epoch 8845: Training Loss: 0.05526199440161387 Validation Loss: 0.7715582251548767\n",
      "Epoch 8846: Training Loss: 0.05502240235606829 Validation Loss: 0.7768250107765198\n",
      "Epoch 8847: Training Loss: 0.05516806741555532 Validation Loss: 0.7798126339912415\n",
      "Epoch 8848: Training Loss: 0.05510214840372404 Validation Loss: 0.7794352173805237\n",
      "Epoch 8849: Training Loss: 0.0551506814857324 Validation Loss: 0.7764316201210022\n",
      "Epoch 8850: Training Loss: 0.055260355273882546 Validation Loss: 0.7724828720092773\n",
      "Epoch 8851: Training Loss: 0.05504980186621348 Validation Loss: 0.7724976539611816\n",
      "Epoch 8852: Training Loss: 0.05514942854642868 Validation Loss: 0.7731699347496033\n",
      "Epoch 8853: Training Loss: 0.055074337869882584 Validation Loss: 0.7728456258773804\n",
      "Epoch 8854: Training Loss: 0.0552193857729435 Validation Loss: 0.7761120200157166\n",
      "Epoch 8855: Training Loss: 0.05514384681979815 Validation Loss: 0.7731932997703552\n",
      "Epoch 8856: Training Loss: 0.05525044724345207 Validation Loss: 0.7713597416877747\n",
      "Epoch 8857: Training Loss: 0.05500825121998787 Validation Loss: 0.7729811668395996\n",
      "Epoch 8858: Training Loss: 0.055167446533838906 Validation Loss: 0.7765839695930481\n",
      "Epoch 8859: Training Loss: 0.0551167664428552 Validation Loss: 0.7780337333679199\n",
      "Epoch 8860: Training Loss: 0.055120219786961876 Validation Loss: 0.7756606340408325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8861: Training Loss: 0.055212936053673424 Validation Loss: 0.7732142210006714\n",
      "Epoch 8862: Training Loss: 0.05504563947518667 Validation Loss: 0.7719385027885437\n",
      "Epoch 8863: Training Loss: 0.05520222211877505 Validation Loss: 0.7739651799201965\n",
      "Epoch 8864: Training Loss: 0.05501309037208557 Validation Loss: 0.7740810513496399\n",
      "Epoch 8865: Training Loss: 0.055034189174572624 Validation Loss: 0.7742618322372437\n",
      "Epoch 8866: Training Loss: 0.05497062082091967 Validation Loss: 0.7742484211921692\n",
      "Epoch 8867: Training Loss: 0.05504028623302778 Validation Loss: 0.771598219871521\n",
      "Epoch 8868: Training Loss: 0.05497836569945017 Validation Loss: 0.7716315984725952\n",
      "Epoch 8869: Training Loss: 0.05502135927478472 Validation Loss: 0.7742776870727539\n",
      "Epoch 8870: Training Loss: 0.05503764500220617 Validation Loss: 0.7760287523269653\n",
      "Epoch 8871: Training Loss: 0.05493708079059919 Validation Loss: 0.7745739817619324\n",
      "Epoch 8872: Training Loss: 0.05498633161187172 Validation Loss: 0.7728816866874695\n",
      "Epoch 8873: Training Loss: 0.054920587688684464 Validation Loss: 0.7724976539611816\n",
      "Epoch 8874: Training Loss: 0.054985309640566506 Validation Loss: 0.7718192338943481\n",
      "Epoch 8875: Training Loss: 0.05501634503404299 Validation Loss: 0.7721588611602783\n",
      "Epoch 8876: Training Loss: 0.0548915167649587 Validation Loss: 0.7740559577941895\n",
      "Epoch 8877: Training Loss: 0.05489648630221685 Validation Loss: 0.7765616774559021\n",
      "Epoch 8878: Training Loss: 0.05530173207322756 Validation Loss: 0.7800275683403015\n",
      "Epoch 8879: Training Loss: 0.05514317378401756 Validation Loss: 0.7759255170822144\n",
      "Epoch 8880: Training Loss: 0.0549729069073995 Validation Loss: 0.7758768796920776\n",
      "Epoch 8881: Training Loss: 0.05513614291946093 Validation Loss: 0.7716733813285828\n",
      "Epoch 8882: Training Loss: 0.05491434410214424 Validation Loss: 0.7723751664161682\n",
      "Epoch 8883: Training Loss: 0.05497839426000913 Validation Loss: 0.7716863751411438\n",
      "Epoch 8884: Training Loss: 0.05492198467254639 Validation Loss: 0.7747803330421448\n",
      "Epoch 8885: Training Loss: 0.05499287570516268 Validation Loss: 0.776146411895752\n",
      "Epoch 8886: Training Loss: 0.05497640619675318 Validation Loss: 0.7778784036636353\n",
      "Epoch 8887: Training Loss: 0.05494792635242144 Validation Loss: 0.7778012156486511\n",
      "Epoch 8888: Training Loss: 0.05493606006105741 Validation Loss: 0.7734283208847046\n",
      "Epoch 8889: Training Loss: 0.0548692320783933 Validation Loss: 0.7718848586082458\n",
      "Epoch 8890: Training Loss: 0.05487584322690964 Validation Loss: 0.7706824541091919\n",
      "Epoch 8891: Training Loss: 0.05498192831873894 Validation Loss: 0.7727816104888916\n",
      "Epoch 8892: Training Loss: 0.054976960023244224 Validation Loss: 0.7714777588844299\n",
      "Epoch 8893: Training Loss: 0.05488131567835808 Validation Loss: 0.7713190317153931\n",
      "Epoch 8894: Training Loss: 0.05494303132096926 Validation Loss: 0.7722759246826172\n",
      "Epoch 8895: Training Loss: 0.05513761689265569 Validation Loss: 0.7785161137580872\n",
      "Epoch 8896: Training Loss: 0.05485172321399053 Validation Loss: 0.7782221436500549\n",
      "Epoch 8897: Training Loss: 0.055005559076865516 Validation Loss: 0.7745888829231262\n",
      "Epoch 8898: Training Loss: 0.0550862563153108 Validation Loss: 0.7722755670547485\n",
      "Epoch 8899: Training Loss: 0.0548766553401947 Validation Loss: 0.772109866142273\n",
      "Epoch 8900: Training Loss: 0.05481236924727758 Validation Loss: 0.7751725316047668\n",
      "Epoch 8901: Training Loss: 0.054835577805837 Validation Loss: 0.7775857448577881\n",
      "Epoch 8902: Training Loss: 0.05489513774712881 Validation Loss: 0.7760148644447327\n",
      "Epoch 8903: Training Loss: 0.05491887032985687 Validation Loss: 0.7783306241035461\n",
      "Epoch 8904: Training Loss: 0.055041136840979256 Validation Loss: 0.7770987749099731\n",
      "Epoch 8905: Training Loss: 0.05505207180976868 Validation Loss: 0.7775528430938721\n",
      "Epoch 8906: Training Loss: 0.0551038458943367 Validation Loss: 0.7707127332687378\n",
      "Epoch 8907: Training Loss: 0.05493667721748352 Validation Loss: 0.7686728835105896\n",
      "Epoch 8908: Training Loss: 0.05522180969516436 Validation Loss: 0.7672821283340454\n",
      "Epoch 8909: Training Loss: 0.05499929189682007 Validation Loss: 0.7742727398872375\n",
      "Epoch 8910: Training Loss: 0.054709237068891525 Validation Loss: 0.7780724167823792\n",
      "Epoch 8911: Training Loss: 0.054890954246123634 Validation Loss: 0.7775556445121765\n",
      "Epoch 8912: Training Loss: 0.05472539737820625 Validation Loss: 0.7782400250434875\n",
      "Epoch 8913: Training Loss: 0.054781014720598854 Validation Loss: 0.7780793905258179\n",
      "Epoch 8914: Training Loss: 0.05496884509921074 Validation Loss: 0.7748232483863831\n",
      "Epoch 8915: Training Loss: 0.05479006593426069 Validation Loss: 0.7737393379211426\n",
      "Epoch 8916: Training Loss: 0.0549305168290933 Validation Loss: 0.7744334936141968\n",
      "Epoch 8917: Training Loss: 0.055171892046928406 Validation Loss: 0.7791022062301636\n",
      "Epoch 8918: Training Loss: 0.05476466566324234 Validation Loss: 0.7771064639091492\n",
      "Epoch 8919: Training Loss: 0.05486161758502325 Validation Loss: 0.7732114195823669\n",
      "Epoch 8920: Training Loss: 0.05471790457765261 Validation Loss: 0.7715629935264587\n",
      "Epoch 8921: Training Loss: 0.05483780180414518 Validation Loss: 0.7705678343772888\n",
      "Epoch 8922: Training Loss: 0.054728527863820396 Validation Loss: 0.7732728719711304\n",
      "Epoch 8923: Training Loss: 0.05568579211831093 Validation Loss: 0.7804753184318542\n",
      "Epoch 8924: Training Loss: 0.054777504255374275 Validation Loss: 0.7780473828315735\n",
      "Epoch 8925: Training Loss: 0.05467392007509867 Validation Loss: 0.7761141657829285\n",
      "Epoch 8926: Training Loss: 0.054680281629165016 Validation Loss: 0.7751494646072388\n",
      "Epoch 8927: Training Loss: 0.054650219778219856 Validation Loss: 0.7740110754966736\n",
      "Epoch 8928: Training Loss: 0.05468369399507841 Validation Loss: 0.7718729972839355\n",
      "Epoch 8929: Training Loss: 0.054706777135531105 Validation Loss: 0.7720047235488892\n",
      "Epoch 8930: Training Loss: 0.05480824410915375 Validation Loss: 0.7738348841667175\n",
      "Epoch 8931: Training Loss: 0.05471195156375567 Validation Loss: 0.774911105632782\n",
      "Epoch 8932: Training Loss: 0.054709369937578835 Validation Loss: 0.7759181261062622\n",
      "Epoch 8933: Training Loss: 0.05477084840337435 Validation Loss: 0.7731286287307739\n",
      "Epoch 8934: Training Loss: 0.05476093664765358 Validation Loss: 0.7742328643798828\n",
      "Epoch 8935: Training Loss: 0.05475695182879766 Validation Loss: 0.7753086686134338\n",
      "Epoch 8936: Training Loss: 0.054842413713534675 Validation Loss: 0.7719793915748596\n",
      "Epoch 8937: Training Loss: 0.05470316236217817 Validation Loss: 0.7721202969551086\n",
      "Epoch 8938: Training Loss: 0.054814388354619346 Validation Loss: 0.772650957107544\n",
      "Epoch 8939: Training Loss: 0.05463173985481262 Validation Loss: 0.7753610014915466\n",
      "Epoch 8940: Training Loss: 0.054830500235160194 Validation Loss: 0.7804820537567139\n",
      "Epoch 8941: Training Loss: 0.054740396638711296 Validation Loss: 0.7808525562286377\n",
      "Epoch 8942: Training Loss: 0.0547797717154026 Validation Loss: 0.7795823812484741\n",
      "Epoch 8943: Training Loss: 0.05507831151286761 Validation Loss: 0.7725690603256226\n",
      "Epoch 8944: Training Loss: 0.0547091340025266 Validation Loss: 0.7734643220901489\n",
      "Epoch 8945: Training Loss: 0.054687149822711945 Validation Loss: 0.7747291922569275\n",
      "Epoch 8946: Training Loss: 0.054624212284882866 Validation Loss: 0.7750260233879089\n",
      "Epoch 8947: Training Loss: 0.054575531433025994 Validation Loss: 0.7739596962928772\n",
      "Epoch 8948: Training Loss: 0.05459454903999964 Validation Loss: 0.7731870412826538\n",
      "Epoch 8949: Training Loss: 0.05481454978386561 Validation Loss: 0.7704012989997864\n",
      "Epoch 8950: Training Loss: 0.05459929878513018 Validation Loss: 0.77269446849823\n",
      "Epoch 8951: Training Loss: 0.05480289086699486 Validation Loss: 0.7780141830444336\n",
      "Epoch 8952: Training Loss: 0.05462427934010824 Validation Loss: 0.7788282632827759\n",
      "Epoch 8953: Training Loss: 0.05477929736177126 Validation Loss: 0.7751869559288025\n",
      "Epoch 8954: Training Loss: 0.05453522006670634 Validation Loss: 0.7759057879447937\n",
      "Epoch 8955: Training Loss: 0.05483552316824595 Validation Loss: 0.7781708240509033\n",
      "Epoch 8956: Training Loss: 0.05458629379669825 Validation Loss: 0.7747567296028137\n",
      "Epoch 8957: Training Loss: 0.05453075344363848 Validation Loss: 0.7736226916313171\n",
      "Epoch 8958: Training Loss: 0.054552032301823296 Validation Loss: 0.7743462324142456\n",
      "Epoch 8959: Training Loss: 0.05464924747745196 Validation Loss: 0.7716652750968933\n",
      "Epoch 8960: Training Loss: 0.05457848682999611 Validation Loss: 0.7735022306442261\n",
      "Epoch 8961: Training Loss: 0.05453153202931086 Validation Loss: 0.7746686339378357\n",
      "Epoch 8962: Training Loss: 0.054490093141794205 Validation Loss: 0.776205837726593\n",
      "Epoch 8963: Training Loss: 0.05456565568844477 Validation Loss: 0.7754450440406799\n",
      "Epoch 8964: Training Loss: 0.054519890497128166 Validation Loss: 0.775275468826294\n",
      "Epoch 8965: Training Loss: 0.054590156922737755 Validation Loss: 0.7740631699562073\n",
      "Epoch 8966: Training Loss: 0.05460396160682043 Validation Loss: 0.7777088284492493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8967: Training Loss: 0.05469933773080508 Validation Loss: 0.7802974581718445\n",
      "Epoch 8968: Training Loss: 0.05451323216160139 Validation Loss: 0.7789685130119324\n",
      "Epoch 8969: Training Loss: 0.05456055204073588 Validation Loss: 0.775518000125885\n",
      "Epoch 8970: Training Loss: 0.05444180220365524 Validation Loss: 0.7737499475479126\n",
      "Epoch 8971: Training Loss: 0.055018155525128044 Validation Loss: 0.7689393162727356\n",
      "Epoch 8972: Training Loss: 0.05498195067048073 Validation Loss: 0.7752168774604797\n",
      "Epoch 8973: Training Loss: 0.05442483847339948 Validation Loss: 0.7751376628875732\n",
      "Epoch 8974: Training Loss: 0.054443083703517914 Validation Loss: 0.7751765251159668\n",
      "Epoch 8975: Training Loss: 0.05442118396361669 Validation Loss: 0.7765735387802124\n",
      "Epoch 8976: Training Loss: 0.05456497396032015 Validation Loss: 0.7741953730583191\n",
      "Epoch 8977: Training Loss: 0.05444308121999105 Validation Loss: 0.7746869325637817\n",
      "Epoch 8978: Training Loss: 0.05460375671585401 Validation Loss: 0.7783994674682617\n",
      "Epoch 8979: Training Loss: 0.054437107096115746 Validation Loss: 0.7763046622276306\n",
      "Epoch 8980: Training Loss: 0.05446568255623182 Validation Loss: 0.7749572396278381\n",
      "Epoch 8981: Training Loss: 0.05459049344062805 Validation Loss: 0.7724599242210388\n",
      "Epoch 8982: Training Loss: 0.05448748916387558 Validation Loss: 0.7751172184944153\n",
      "Epoch 8983: Training Loss: 0.05450031658013662 Validation Loss: 0.7773381471633911\n",
      "Epoch 8984: Training Loss: 0.05440047880013784 Validation Loss: 0.7760425806045532\n",
      "Epoch 8985: Training Loss: 0.054468428095181785 Validation Loss: 0.7766913175582886\n",
      "Epoch 8986: Training Loss: 0.05436477313439051 Validation Loss: 0.7753437161445618\n",
      "Epoch 8987: Training Loss: 0.0543863537410895 Validation Loss: 0.7731698751449585\n",
      "Epoch 8988: Training Loss: 0.05494800334175428 Validation Loss: 0.7773190140724182\n",
      "Epoch 8989: Training Loss: 0.05457501237591108 Validation Loss: 0.7729200720787048\n",
      "Epoch 8990: Training Loss: 0.054394462456305824 Validation Loss: 0.7733287811279297\n",
      "Epoch 8991: Training Loss: 0.05454284821947416 Validation Loss: 0.7758745551109314\n",
      "Epoch 8992: Training Loss: 0.054365165531635284 Validation Loss: 0.7763717174530029\n",
      "Epoch 8993: Training Loss: 0.05438859760761261 Validation Loss: 0.7769662737846375\n",
      "Epoch 8994: Training Loss: 0.054513139029343925 Validation Loss: 0.7777197957038879\n",
      "Epoch 8995: Training Loss: 0.054368569205204643 Validation Loss: 0.7755393981933594\n",
      "Epoch 8996: Training Loss: 0.05438125506043434 Validation Loss: 0.7725616693496704\n",
      "Epoch 8997: Training Loss: 0.05437047779560089 Validation Loss: 0.7714253664016724\n",
      "Epoch 8998: Training Loss: 0.05439910168449084 Validation Loss: 0.7723233699798584\n",
      "Epoch 8999: Training Loss: 0.054399638126293816 Validation Loss: 0.7750999331474304\n",
      "Epoch 9000: Training Loss: 0.05437643577655157 Validation Loss: 0.774959146976471\n",
      "Epoch 9001: Training Loss: 0.05437158172329267 Validation Loss: 0.7779809236526489\n",
      "Epoch 9002: Training Loss: 0.05442125350236893 Validation Loss: 0.7794554829597473\n",
      "Epoch 9003: Training Loss: 0.05482375497619311 Validation Loss: 0.7745063304901123\n",
      "Epoch 9004: Training Loss: 0.0543304868042469 Validation Loss: 0.7749800682067871\n",
      "Epoch 9005: Training Loss: 0.05433563763896624 Validation Loss: 0.7764865756034851\n",
      "Epoch 9006: Training Loss: 0.05448159078756968 Validation Loss: 0.7799068689346313\n",
      "Epoch 9007: Training Loss: 0.05429016798734665 Validation Loss: 0.778427243232727\n",
      "Epoch 9008: Training Loss: 0.054374941935141884 Validation Loss: 0.7781829237937927\n",
      "Epoch 9009: Training Loss: 0.054305173456668854 Validation Loss: 0.7764053344726562\n",
      "Epoch 9010: Training Loss: 0.05429117754101753 Validation Loss: 0.7734787464141846\n",
      "Epoch 9011: Training Loss: 0.054804276674985886 Validation Loss: 0.7690489888191223\n",
      "Epoch 9012: Training Loss: 0.05440829570094744 Validation Loss: 0.7710770964622498\n",
      "Epoch 9013: Training Loss: 0.05433213214079539 Validation Loss: 0.7733666896820068\n",
      "Epoch 9014: Training Loss: 0.05419893686970075 Validation Loss: 0.7781317234039307\n",
      "Epoch 9015: Training Loss: 0.05441481495896975 Validation Loss: 0.7826523184776306\n",
      "Epoch 9016: Training Loss: 0.05438565214474996 Validation Loss: 0.7817926406860352\n",
      "Epoch 9017: Training Loss: 0.05456075072288513 Validation Loss: 0.7770460247993469\n",
      "Epoch 9018: Training Loss: 0.0542370950182279 Validation Loss: 0.7757178544998169\n",
      "Epoch 9019: Training Loss: 0.05430337662498156 Validation Loss: 0.7740022540092468\n",
      "Epoch 9020: Training Loss: 0.05430320153633753 Validation Loss: 0.7723577618598938\n",
      "Epoch 9021: Training Loss: 0.054249907533327736 Validation Loss: 0.77347731590271\n",
      "Epoch 9022: Training Loss: 0.05428165818254153 Validation Loss: 0.7760933637619019\n",
      "Epoch 9023: Training Loss: 0.0542568638920784 Validation Loss: 0.7758352160453796\n",
      "Epoch 9024: Training Loss: 0.05494025473793348 Validation Loss: 0.7816715836524963\n",
      "Epoch 9025: Training Loss: 0.05450820798675219 Validation Loss: 0.7762772440910339\n",
      "Epoch 9026: Training Loss: 0.05423584207892418 Validation Loss: 0.7756385803222656\n",
      "Epoch 9027: Training Loss: 0.05427298198143641 Validation Loss: 0.7739613652229309\n",
      "Epoch 9028: Training Loss: 0.05429481963316599 Validation Loss: 0.774772584438324\n",
      "Epoch 9029: Training Loss: 0.05423301085829735 Validation Loss: 0.7741130590438843\n",
      "Epoch 9030: Training Loss: 0.054281312972307205 Validation Loss: 0.7772212028503418\n",
      "Epoch 9031: Training Loss: 0.05432782880961895 Validation Loss: 0.7748621702194214\n",
      "Epoch 9032: Training Loss: 0.05422075092792511 Validation Loss: 0.7748280763626099\n",
      "Epoch 9033: Training Loss: 0.054298670341571174 Validation Loss: 0.7745312452316284\n",
      "Epoch 9034: Training Loss: 0.054269373416900635 Validation Loss: 0.7772912979125977\n",
      "Epoch 9035: Training Loss: 0.05427886297305425 Validation Loss: 0.7776137590408325\n",
      "Epoch 9036: Training Loss: 0.05453922847906748 Validation Loss: 0.7810326218605042\n",
      "Epoch 9037: Training Loss: 0.05425769214828809 Validation Loss: 0.7768906950950623\n",
      "Epoch 9038: Training Loss: 0.05455475673079491 Validation Loss: 0.7713279724121094\n",
      "Epoch 9039: Training Loss: 0.054237655053536095 Validation Loss: 0.771064817905426\n",
      "Epoch 9040: Training Loss: 0.05423654864231745 Validation Loss: 0.7735853791236877\n",
      "Epoch 9041: Training Loss: 0.05414128303527832 Validation Loss: 0.7748085260391235\n",
      "Epoch 9042: Training Loss: 0.054145981868108116 Validation Loss: 0.7766886949539185\n",
      "Epoch 9043: Training Loss: 0.054233646020293236 Validation Loss: 0.7760564684867859\n",
      "Epoch 9044: Training Loss: 0.05418418223659197 Validation Loss: 0.7783362865447998\n",
      "Epoch 9045: Training Loss: 0.054228298366069794 Validation Loss: 0.7774980664253235\n",
      "Epoch 9046: Training Loss: 0.05415023863315582 Validation Loss: 0.7776451706886292\n",
      "Epoch 9047: Training Loss: 0.05409993728001913 Validation Loss: 0.776268482208252\n",
      "Epoch 9048: Training Loss: 0.054564050088326134 Validation Loss: 0.7716859579086304\n",
      "Epoch 9049: Training Loss: 0.054263959328333534 Validation Loss: 0.7734439969062805\n",
      "Epoch 9050: Training Loss: 0.05429869766036669 Validation Loss: 0.7782174348831177\n",
      "Epoch 9051: Training Loss: 0.054213013499975204 Validation Loss: 0.7798416614532471\n",
      "Epoch 9052: Training Loss: 0.05419983838995298 Validation Loss: 0.7805821299552917\n",
      "Epoch 9053: Training Loss: 0.05423549686868986 Validation Loss: 0.7796823978424072\n",
      "Epoch 9054: Training Loss: 0.05404903615514437 Validation Loss: 0.776070237159729\n",
      "Epoch 9055: Training Loss: 0.05410366877913475 Validation Loss: 0.7742584943771362\n",
      "Epoch 9056: Training Loss: 0.05428865489860376 Validation Loss: 0.7707732915878296\n",
      "Epoch 9057: Training Loss: 0.0546438954770565 Validation Loss: 0.7739099860191345\n",
      "Epoch 9058: Training Loss: 0.054104726761579514 Validation Loss: 0.7731316685676575\n",
      "Epoch 9059: Training Loss: 0.05421111856897672 Validation Loss: 0.7768115401268005\n",
      "Epoch 9060: Training Loss: 0.05419188737869263 Validation Loss: 0.7748033404350281\n",
      "Epoch 9061: Training Loss: 0.05422623579700788 Validation Loss: 0.7739343643188477\n",
      "Epoch 9062: Training Loss: 0.054344795644283295 Validation Loss: 0.7791329622268677\n",
      "Epoch 9063: Training Loss: 0.05412524069348971 Validation Loss: 0.7798664569854736\n",
      "Epoch 9064: Training Loss: 0.054099456717570625 Validation Loss: 0.7786546349525452\n",
      "Epoch 9065: Training Loss: 0.05421698341766993 Validation Loss: 0.7742316722869873\n",
      "Epoch 9066: Training Loss: 0.054051811496416725 Validation Loss: 0.7738591432571411\n",
      "Epoch 9067: Training Loss: 0.05410146961609522 Validation Loss: 0.7740265130996704\n",
      "Epoch 9068: Training Loss: 0.054309734453757606 Validation Loss: 0.7775511741638184\n",
      "Epoch 9069: Training Loss: 0.05404474213719368 Validation Loss: 0.7766215801239014\n",
      "Epoch 9070: Training Loss: 0.05404781922698021 Validation Loss: 0.7772760987281799\n",
      "Epoch 9071: Training Loss: 0.05407446871201197 Validation Loss: 0.7781078219413757\n",
      "Epoch 9072: Training Loss: 0.05404296393195788 Validation Loss: 0.7773296236991882\n",
      "Epoch 9073: Training Loss: 0.05401068553328514 Validation Loss: 0.7756708264350891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9074: Training Loss: 0.05403927837808927 Validation Loss: 0.77325439453125\n",
      "Epoch 9075: Training Loss: 0.05406169220805168 Validation Loss: 0.7733443975448608\n",
      "Epoch 9076: Training Loss: 0.054190922528505325 Validation Loss: 0.7715644836425781\n",
      "Epoch 9077: Training Loss: 0.05398797740538915 Validation Loss: 0.7740533947944641\n",
      "Epoch 9078: Training Loss: 0.054007843136787415 Validation Loss: 0.778741180896759\n",
      "Epoch 9079: Training Loss: 0.05401756366093954 Validation Loss: 0.7784562110900879\n",
      "Epoch 9080: Training Loss: 0.05401485785841942 Validation Loss: 0.7790099382400513\n",
      "Epoch 9081: Training Loss: 0.05410836140314738 Validation Loss: 0.7795605659484863\n",
      "Epoch 9082: Training Loss: 0.05398791283369064 Validation Loss: 0.7784489393234253\n",
      "Epoch 9083: Training Loss: 0.054123248904943466 Validation Loss: 0.7770992517471313\n",
      "Epoch 9084: Training Loss: 0.05402886619170507 Validation Loss: 0.7762615084648132\n",
      "Epoch 9085: Training Loss: 0.054058523227771126 Validation Loss: 0.7747206091880798\n",
      "Epoch 9086: Training Loss: 0.054002506037553154 Validation Loss: 0.7730454802513123\n",
      "Epoch 9087: Training Loss: 0.05397961288690567 Validation Loss: 0.774724006652832\n",
      "Epoch 9088: Training Loss: 0.05407603705922762 Validation Loss: 0.774995744228363\n",
      "Epoch 9089: Training Loss: 0.05392662932475408 Validation Loss: 0.7767688632011414\n",
      "Epoch 9090: Training Loss: 0.05388400827844938 Validation Loss: 0.7774180769920349\n",
      "Epoch 9091: Training Loss: 0.054014148811499275 Validation Loss: 0.7775357365608215\n",
      "Epoch 9092: Training Loss: 0.05401262640953064 Validation Loss: 0.7790023684501648\n",
      "Epoch 9093: Training Loss: 0.05411336570978165 Validation Loss: 0.7764233946800232\n",
      "Epoch 9094: Training Loss: 0.0539751760661602 Validation Loss: 0.7757777571678162\n",
      "Epoch 9095: Training Loss: 0.054088460902372994 Validation Loss: 0.7784554958343506\n",
      "Epoch 9096: Training Loss: 0.053980047504107155 Validation Loss: 0.7765573859214783\n",
      "Epoch 9097: Training Loss: 0.05387088159720103 Validation Loss: 0.7760960459709167\n",
      "Epoch 9098: Training Loss: 0.0540298285583655 Validation Loss: 0.7752574682235718\n",
      "Epoch 9099: Training Loss: 0.053970906883478165 Validation Loss: 0.7770845293998718\n",
      "Epoch 9100: Training Loss: 0.053880639374256134 Validation Loss: 0.7771733403205872\n",
      "Epoch 9101: Training Loss: 0.054118867963552475 Validation Loss: 0.7805879712104797\n",
      "Epoch 9102: Training Loss: 0.05415184671680132 Validation Loss: 0.7785307765007019\n",
      "Epoch 9103: Training Loss: 0.05386850362022718 Validation Loss: 0.7770331501960754\n",
      "Epoch 9104: Training Loss: 0.05394533773263296 Validation Loss: 0.7757657766342163\n",
      "Epoch 9105: Training Loss: 0.05394124612212181 Validation Loss: 0.7742144465446472\n",
      "Epoch 9106: Training Loss: 0.05398849273721377 Validation Loss: 0.7751513123512268\n",
      "Epoch 9107: Training Loss: 0.053993203987677894 Validation Loss: 0.7737036347389221\n",
      "Epoch 9108: Training Loss: 0.05388088896870613 Validation Loss: 0.7763763070106506\n",
      "Epoch 9109: Training Loss: 0.05384471267461777 Validation Loss: 0.7771990299224854\n",
      "Epoch 9110: Training Loss: 0.05388052513202032 Validation Loss: 0.7764573097229004\n",
      "Epoch 9111: Training Loss: 0.05381458252668381 Validation Loss: 0.7767268419265747\n",
      "Epoch 9112: Training Loss: 0.05388894925514857 Validation Loss: 0.7782270908355713\n",
      "Epoch 9113: Training Loss: 0.053819991648197174 Validation Loss: 0.7794747948646545\n",
      "Epoch 9114: Training Loss: 0.0538998618721962 Validation Loss: 0.7797309756278992\n",
      "Epoch 9115: Training Loss: 0.05416088551282883 Validation Loss: 0.7756823301315308\n",
      "Epoch 9116: Training Loss: 0.053891083846489586 Validation Loss: 0.7772225737571716\n",
      "Epoch 9117: Training Loss: 0.05388513455788294 Validation Loss: 0.7747076153755188\n",
      "Epoch 9118: Training Loss: 0.05386774490276972 Validation Loss: 0.7736141085624695\n",
      "Epoch 9119: Training Loss: 0.05389902492364248 Validation Loss: 0.7737162113189697\n",
      "Epoch 9120: Training Loss: 0.05393282696604729 Validation Loss: 0.7779742479324341\n",
      "Epoch 9121: Training Loss: 0.05397562930981318 Validation Loss: 0.78200364112854\n",
      "Epoch 9122: Training Loss: 0.054068454851706825 Validation Loss: 0.7779308557510376\n",
      "Epoch 9123: Training Loss: 0.05384287486473719 Validation Loss: 0.7767912745475769\n",
      "Epoch 9124: Training Loss: 0.05402680983146032 Validation Loss: 0.7800589799880981\n",
      "Epoch 9125: Training Loss: 0.05379592378934225 Validation Loss: 0.7796670198440552\n",
      "Epoch 9126: Training Loss: 0.05378977954387665 Validation Loss: 0.7768839597702026\n",
      "Epoch 9127: Training Loss: 0.05394607037305832 Validation Loss: 0.7776212692260742\n",
      "Epoch 9128: Training Loss: 0.05383847529689471 Validation Loss: 0.7778422832489014\n",
      "Epoch 9129: Training Loss: 0.053773837784926094 Validation Loss: 0.7757706642150879\n",
      "Epoch 9130: Training Loss: 0.05373308683435122 Validation Loss: 0.7744690179824829\n",
      "Epoch 9131: Training Loss: 0.053863187630971275 Validation Loss: 0.7757038474082947\n",
      "Epoch 9132: Training Loss: 0.05379562949140867 Validation Loss: 0.7736594080924988\n",
      "Epoch 9133: Training Loss: 0.05388793225089709 Validation Loss: 0.7763395309448242\n",
      "Epoch 9134: Training Loss: 0.05374025056759516 Validation Loss: 0.7751267552375793\n",
      "Epoch 9135: Training Loss: 0.05371114859978358 Validation Loss: 0.775408148765564\n",
      "Epoch 9136: Training Loss: 0.05371176699797312 Validation Loss: 0.7759162187576294\n",
      "Epoch 9137: Training Loss: 0.05373456949989001 Validation Loss: 0.777787446975708\n",
      "Epoch 9138: Training Loss: 0.05388955275217692 Validation Loss: 0.7791252136230469\n",
      "Epoch 9139: Training Loss: 0.05374274651209513 Validation Loss: 0.7784905433654785\n",
      "Epoch 9140: Training Loss: 0.05379684393604597 Validation Loss: 0.7780223488807678\n",
      "Epoch 9141: Training Loss: 0.053782641887664795 Validation Loss: 0.7771553993225098\n",
      "Epoch 9142: Training Loss: 0.05383111163973808 Validation Loss: 0.7744962573051453\n",
      "Epoch 9143: Training Loss: 0.053792464236418404 Validation Loss: 0.7745136618614197\n",
      "Epoch 9144: Training Loss: 0.053874352325995765 Validation Loss: 0.7784693241119385\n",
      "Epoch 9145: Training Loss: 0.05379838248093923 Validation Loss: 0.776837944984436\n",
      "Epoch 9146: Training Loss: 0.05368468165397644 Validation Loss: 0.7764096260070801\n",
      "Epoch 9147: Training Loss: 0.05369133750597636 Validation Loss: 0.7768592834472656\n",
      "Epoch 9148: Training Loss: 0.053718494872252144 Validation Loss: 0.7779129147529602\n",
      "Epoch 9149: Training Loss: 0.05369254450003306 Validation Loss: 0.77815181016922\n",
      "Epoch 9150: Training Loss: 0.05381487930814425 Validation Loss: 0.776337742805481\n",
      "Epoch 9151: Training Loss: 0.053764515866835914 Validation Loss: 0.7785367965698242\n",
      "Epoch 9152: Training Loss: 0.05386026451985041 Validation Loss: 0.7810606360435486\n",
      "Epoch 9153: Training Loss: 0.053803631414969764 Validation Loss: 0.7776016592979431\n",
      "Epoch 9154: Training Loss: 0.05374648297826449 Validation Loss: 0.7753556370735168\n",
      "Epoch 9155: Training Loss: 0.0536742073794206 Validation Loss: 0.7764999866485596\n",
      "Epoch 9156: Training Loss: 0.05361667896310488 Validation Loss: 0.7769938707351685\n",
      "Epoch 9157: Training Loss: 0.053613971918821335 Validation Loss: 0.7760425806045532\n",
      "Epoch 9158: Training Loss: 0.05386920770009359 Validation Loss: 0.7770776748657227\n",
      "Epoch 9159: Training Loss: 0.0539235919713974 Validation Loss: 0.7798325419425964\n",
      "Epoch 9160: Training Loss: 0.053948438415924706 Validation Loss: 0.7748637199401855\n",
      "Epoch 9161: Training Loss: 0.053734079003334045 Validation Loss: 0.7770665884017944\n",
      "Epoch 9162: Training Loss: 0.05364845817287763 Validation Loss: 0.7779278755187988\n",
      "Epoch 9163: Training Loss: 0.0536532128850619 Validation Loss: 0.7760911583900452\n",
      "Epoch 9164: Training Loss: 0.05379030480980873 Validation Loss: 0.7759235501289368\n",
      "Epoch 9165: Training Loss: 0.05362800508737564 Validation Loss: 0.7776634693145752\n",
      "Epoch 9166: Training Loss: 0.05359011019269625 Validation Loss: 0.7777411341667175\n",
      "Epoch 9167: Training Loss: 0.053872790187597275 Validation Loss: 0.7752917408943176\n",
      "Epoch 9168: Training Loss: 0.05366420497496923 Validation Loss: 0.7757093906402588\n",
      "Epoch 9169: Training Loss: 0.05354888240496317 Validation Loss: 0.779072105884552\n",
      "Epoch 9170: Training Loss: 0.0536573218802611 Validation Loss: 0.7789123058319092\n",
      "Epoch 9171: Training Loss: 0.054140700648228325 Validation Loss: 0.7846208810806274\n",
      "Epoch 9172: Training Loss: 0.05384878565867742 Validation Loss: 0.7847049832344055\n",
      "Epoch 9173: Training Loss: 0.05364830916126569 Validation Loss: 0.7788609266281128\n",
      "Epoch 9174: Training Loss: 0.053557172417640686 Validation Loss: 0.7744441628456116\n",
      "Epoch 9175: Training Loss: 0.0536585363248984 Validation Loss: 0.774652898311615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9176: Training Loss: 0.053572537998358406 Validation Loss: 0.7742759585380554\n",
      "Epoch 9177: Training Loss: 0.053623978048563004 Validation Loss: 0.7748377323150635\n",
      "Epoch 9178: Training Loss: 0.05353526398539543 Validation Loss: 0.7760341167449951\n",
      "Epoch 9179: Training Loss: 0.05352742224931717 Validation Loss: 0.7759769558906555\n",
      "Epoch 9180: Training Loss: 0.053886204957962036 Validation Loss: 0.7798494696617126\n",
      "Epoch 9181: Training Loss: 0.053583422054847084 Validation Loss: 0.7794715762138367\n",
      "Epoch 9182: Training Loss: 0.053469669073820114 Validation Loss: 0.7774858474731445\n",
      "Epoch 9183: Training Loss: 0.053567852824926376 Validation Loss: 0.7753363251686096\n",
      "Epoch 9184: Training Loss: 0.05352447306116422 Validation Loss: 0.7742895483970642\n",
      "Epoch 9185: Training Loss: 0.053601098557313286 Validation Loss: 0.7731683254241943\n",
      "Epoch 9186: Training Loss: 0.05393932883938154 Validation Loss: 0.7784326076507568\n",
      "Epoch 9187: Training Loss: 0.05357256531715393 Validation Loss: 0.7793061137199402\n",
      "Epoch 9188: Training Loss: 0.053938603649536766 Validation Loss: 0.7748931646347046\n",
      "Epoch 9189: Training Loss: 0.05349070951342583 Validation Loss: 0.7756284475326538\n",
      "Epoch 9190: Training Loss: 0.053513837357362114 Validation Loss: 0.7783806920051575\n",
      "Epoch 9191: Training Loss: 0.053485227127869926 Validation Loss: 0.7793168425559998\n",
      "Epoch 9192: Training Loss: 0.05347088351845741 Validation Loss: 0.7787415385246277\n",
      "Epoch 9193: Training Loss: 0.05371377865473429 Validation Loss: 0.7802512049674988\n",
      "Epoch 9194: Training Loss: 0.05346976965665817 Validation Loss: 0.7780987620353699\n",
      "Epoch 9195: Training Loss: 0.05358729759852091 Validation Loss: 0.7790117263793945\n",
      "Epoch 9196: Training Loss: 0.05348964408040047 Validation Loss: 0.7760295867919922\n",
      "Epoch 9197: Training Loss: 0.05371177817384402 Validation Loss: 0.77330482006073\n",
      "Epoch 9198: Training Loss: 0.05361842239896456 Validation Loss: 0.776351273059845\n",
      "Epoch 9199: Training Loss: 0.053465645760297775 Validation Loss: 0.7771657705307007\n",
      "Epoch 9200: Training Loss: 0.05355424558122953 Validation Loss: 0.7804014086723328\n",
      "Epoch 9201: Training Loss: 0.05356328561902046 Validation Loss: 0.7823130488395691\n",
      "Epoch 9202: Training Loss: 0.05348324651519457 Validation Loss: 0.779183566570282\n",
      "Epoch 9203: Training Loss: 0.05354640260338783 Validation Loss: 0.7761209607124329\n",
      "Epoch 9204: Training Loss: 0.05343120793501536 Validation Loss: 0.7759112119674683\n",
      "Epoch 9205: Training Loss: 0.05366624643405279 Validation Loss: 0.7784562706947327\n",
      "Epoch 9206: Training Loss: 0.053628494342168175 Validation Loss: 0.7798929214477539\n",
      "Epoch 9207: Training Loss: 0.05358181893825531 Validation Loss: 0.7762031555175781\n",
      "Epoch 9208: Training Loss: 0.05358995124697685 Validation Loss: 0.7783685922622681\n",
      "Epoch 9209: Training Loss: 0.05338136603434881 Validation Loss: 0.7770718336105347\n",
      "Epoch 9210: Training Loss: 0.05351682007312775 Validation Loss: 0.7745610475540161\n",
      "Epoch 9211: Training Loss: 0.05353603263696035 Validation Loss: 0.7744619846343994\n",
      "Epoch 9212: Training Loss: 0.053883989651997886 Validation Loss: 0.7796234488487244\n",
      "Epoch 9213: Training Loss: 0.053527352710564934 Validation Loss: 0.7774889469146729\n",
      "Epoch 9214: Training Loss: 0.05349783351023992 Validation Loss: 0.7794197797775269\n",
      "Epoch 9215: Training Loss: 0.053947276125351586 Validation Loss: 0.7748677134513855\n",
      "Epoch 9216: Training Loss: 0.05351291596889496 Validation Loss: 0.7788530588150024\n",
      "Epoch 9217: Training Loss: 0.05335096766551336 Validation Loss: 0.7793693542480469\n",
      "Epoch 9218: Training Loss: 0.05346542845169703 Validation Loss: 0.780857264995575\n",
      "Epoch 9219: Training Loss: 0.05339142680168152 Validation Loss: 0.7800478339195251\n",
      "Epoch 9220: Training Loss: 0.0534830279648304 Validation Loss: 0.7783993482589722\n",
      "Epoch 9221: Training Loss: 0.05361124003926913 Validation Loss: 0.7742969989776611\n",
      "Epoch 9222: Training Loss: 0.05338729297121366 Validation Loss: 0.7752863764762878\n",
      "Epoch 9223: Training Loss: 0.05346772571404775 Validation Loss: 0.7794266939163208\n",
      "Epoch 9224: Training Loss: 0.05368676160772642 Validation Loss: 0.7760630249977112\n",
      "Epoch 9225: Training Loss: 0.05334146941701571 Validation Loss: 0.7783884406089783\n",
      "Epoch 9226: Training Loss: 0.053424686193466187 Validation Loss: 0.7802373170852661\n",
      "Epoch 9227: Training Loss: 0.05339558298389117 Validation Loss: 0.7788420915603638\n",
      "Epoch 9228: Training Loss: 0.05333114539583524 Validation Loss: 0.7787766456604004\n",
      "Epoch 9229: Training Loss: 0.053311737875143685 Validation Loss: 0.779483437538147\n",
      "Epoch 9230: Training Loss: 0.053471521784861885 Validation Loss: 0.7763501405715942\n",
      "Epoch 9231: Training Loss: 0.05330633496244749 Validation Loss: 0.7766153812408447\n",
      "Epoch 9232: Training Loss: 0.05341742063562075 Validation Loss: 0.7754678726196289\n",
      "Epoch 9233: Training Loss: 0.05346631879607836 Validation Loss: 0.7799380421638489\n",
      "Epoch 9234: Training Loss: 0.05368734275301298 Validation Loss: 0.7842217683792114\n",
      "Epoch 9235: Training Loss: 0.05351561183730761 Validation Loss: 0.7808417081832886\n",
      "Epoch 9236: Training Loss: 0.053309823075930275 Validation Loss: 0.7784302830696106\n",
      "Epoch 9237: Training Loss: 0.053321058551470436 Validation Loss: 0.7767918109893799\n",
      "Epoch 9238: Training Loss: 0.05336716150244077 Validation Loss: 0.7745950818061829\n",
      "Epoch 9239: Training Loss: 0.05333951239784559 Validation Loss: 0.7759971022605896\n",
      "Epoch 9240: Training Loss: 0.053285972525676094 Validation Loss: 0.7785078287124634\n",
      "Epoch 9241: Training Loss: 0.05340171108643214 Validation Loss: 0.7767773866653442\n",
      "Epoch 9242: Training Loss: 0.05332227051258087 Validation Loss: 0.7797648310661316\n",
      "Epoch 9243: Training Loss: 0.05334360897541046 Validation Loss: 0.7806485295295715\n",
      "Epoch 9244: Training Loss: 0.05326557531952858 Validation Loss: 0.7788292765617371\n",
      "Epoch 9245: Training Loss: 0.05329327657818794 Validation Loss: 0.7791499495506287\n",
      "Epoch 9246: Training Loss: 0.053234810630480446 Validation Loss: 0.7781041860580444\n",
      "Epoch 9247: Training Loss: 0.05329719682534536 Validation Loss: 0.7775139212608337\n",
      "Epoch 9248: Training Loss: 0.0533082423110803 Validation Loss: 0.7786499261856079\n",
      "Epoch 9249: Training Loss: 0.05335472399989764 Validation Loss: 0.7758937478065491\n",
      "Epoch 9250: Training Loss: 0.05337622885902723 Validation Loss: 0.7749789357185364\n",
      "Epoch 9251: Training Loss: 0.05322769905130068 Validation Loss: 0.7766526937484741\n",
      "Epoch 9252: Training Loss: 0.053500838577747345 Validation Loss: 0.781151533126831\n",
      "Epoch 9253: Training Loss: 0.05333832775553068 Validation Loss: 0.7790514230728149\n",
      "Epoch 9254: Training Loss: 0.053488995879888535 Validation Loss: 0.7761064171791077\n",
      "Epoch 9255: Training Loss: 0.05323526014884313 Validation Loss: 0.7767555117607117\n",
      "Epoch 9256: Training Loss: 0.05333355193336805 Validation Loss: 0.7759719491004944\n",
      "Epoch 9257: Training Loss: 0.053551399459441505 Validation Loss: 0.7819392681121826\n",
      "Epoch 9258: Training Loss: 0.053465461979309716 Validation Loss: 0.7842381000518799\n",
      "Epoch 9259: Training Loss: 0.05328793078660965 Validation Loss: 0.7805783152580261\n",
      "Epoch 9260: Training Loss: 0.053370715429385505 Validation Loss: 0.7751662731170654\n",
      "Epoch 9261: Training Loss: 0.05321234588821729 Validation Loss: 0.7749629616737366\n",
      "Epoch 9262: Training Loss: 0.05328410491347313 Validation Loss: 0.7770274877548218\n",
      "Epoch 9263: Training Loss: 0.05329900607466698 Validation Loss: 0.7792247533798218\n",
      "Epoch 9264: Training Loss: 0.0531922330458959 Validation Loss: 0.7779976725578308\n",
      "Epoch 9265: Training Loss: 0.053191181272268295 Validation Loss: 0.7781978249549866\n",
      "Epoch 9266: Training Loss: 0.0531951996187369 Validation Loss: 0.7790810465812683\n",
      "Epoch 9267: Training Loss: 0.05352476239204407 Validation Loss: 0.7821794748306274\n",
      "Epoch 9268: Training Loss: 0.05340389162302017 Validation Loss: 0.7774048447608948\n",
      "Epoch 9269: Training Loss: 0.05316255489985148 Validation Loss: 0.7782008051872253\n",
      "Epoch 9270: Training Loss: 0.05319048836827278 Validation Loss: 0.7762659788131714\n",
      "Epoch 9271: Training Loss: 0.053150600443283715 Validation Loss: 0.7774028182029724\n",
      "Epoch 9272: Training Loss: 0.053512309988339744 Validation Loss: 0.7816016674041748\n",
      "Epoch 9273: Training Loss: 0.05319805070757866 Validation Loss: 0.7814492583274841\n",
      "Epoch 9274: Training Loss: 0.05325913056731224 Validation Loss: 0.7768172025680542\n",
      "Epoch 9275: Training Loss: 0.05314269537727038 Validation Loss: 0.7759149670600891\n",
      "Epoch 9276: Training Loss: 0.0532943457365036 Validation Loss: 0.7741797566413879\n",
      "Epoch 9277: Training Loss: 0.053125109523534775 Validation Loss: 0.776133120059967\n",
      "Epoch 9278: Training Loss: 0.05314175163706144 Validation Loss: 0.7802376747131348\n",
      "Epoch 9279: Training Loss: 0.05310965081055959 Validation Loss: 0.7809486985206604\n",
      "Epoch 9280: Training Loss: 0.053096216171979904 Validation Loss: 0.7816882133483887\n",
      "Epoch 9281: Training Loss: 0.05315721904238065 Validation Loss: 0.780339777469635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9282: Training Loss: 0.05329957231879234 Validation Loss: 0.7773991823196411\n",
      "Epoch 9283: Training Loss: 0.05319874485333761 Validation Loss: 0.780252993106842\n",
      "Epoch 9284: Training Loss: 0.053232124696175255 Validation Loss: 0.7822520136833191\n",
      "Epoch 9285: Training Loss: 0.053362121184666954 Validation Loss: 0.7773998379707336\n",
      "Epoch 9286: Training Loss: 0.05311118314663569 Validation Loss: 0.7761077284812927\n",
      "Epoch 9287: Training Loss: 0.05315787966052691 Validation Loss: 0.7777060866355896\n",
      "Epoch 9288: Training Loss: 0.053039719661076866 Validation Loss: 0.7781184911727905\n",
      "Epoch 9289: Training Loss: 0.05331310133139292 Validation Loss: 0.7763732075691223\n",
      "Epoch 9290: Training Loss: 0.05311972772081693 Validation Loss: 0.7766398787498474\n",
      "Epoch 9291: Training Loss: 0.05322432890534401 Validation Loss: 0.7811641693115234\n",
      "Epoch 9292: Training Loss: 0.053311847150325775 Validation Loss: 0.778986394405365\n",
      "Epoch 9293: Training Loss: 0.053074544916550316 Validation Loss: 0.7792328000068665\n",
      "Epoch 9294: Training Loss: 0.053252484649419785 Validation Loss: 0.7816917896270752\n",
      "Epoch 9295: Training Loss: 0.0531929520269235 Validation Loss: 0.7787172794342041\n",
      "Epoch 9296: Training Loss: 0.05303543930252393 Validation Loss: 0.7779456973075867\n",
      "Epoch 9297: Training Loss: 0.05311882868409157 Validation Loss: 0.7793275117874146\n",
      "Epoch 9298: Training Loss: 0.053023289889097214 Validation Loss: 0.7780818343162537\n",
      "Epoch 9299: Training Loss: 0.05329165110985438 Validation Loss: 0.7806819081306458\n",
      "Epoch 9300: Training Loss: 0.053065078953901924 Validation Loss: 0.7772226929664612\n",
      "Epoch 9301: Training Loss: 0.05303946137428284 Validation Loss: 0.7766284346580505\n",
      "Epoch 9302: Training Loss: 0.053023386746644974 Validation Loss: 0.7762933969497681\n",
      "Epoch 9303: Training Loss: 0.05315778901179632 Validation Loss: 0.7747465968132019\n",
      "Epoch 9304: Training Loss: 0.05342156936724981 Validation Loss: 0.7808598875999451\n",
      "Epoch 9305: Training Loss: 0.053079129507144295 Validation Loss: 0.7822266221046448\n",
      "Epoch 9306: Training Loss: 0.053356677293777466 Validation Loss: 0.7837982177734375\n",
      "Epoch 9307: Training Loss: 0.053311217576265335 Validation Loss: 0.7770686149597168\n",
      "Epoch 9308: Training Loss: 0.0531004990140597 Validation Loss: 0.777213454246521\n",
      "Epoch 9309: Training Loss: 0.05311215048034986 Validation Loss: 0.7768829464912415\n",
      "Epoch 9310: Training Loss: 0.05304193993409475 Validation Loss: 0.7760180830955505\n",
      "Epoch 9311: Training Loss: 0.053121271232763924 Validation Loss: 0.7770851850509644\n",
      "Epoch 9312: Training Loss: 0.05303890506426493 Validation Loss: 0.7790926694869995\n",
      "Epoch 9313: Training Loss: 0.05302691583832105 Validation Loss: 0.7799432277679443\n",
      "Epoch 9314: Training Loss: 0.05319036915898323 Validation Loss: 0.7822533249855042\n",
      "Epoch 9315: Training Loss: 0.05325710649291674 Validation Loss: 0.7765750288963318\n",
      "Epoch 9316: Training Loss: 0.05313281590739886 Validation Loss: 0.7744244337081909\n",
      "Epoch 9317: Training Loss: 0.05300176267822584 Validation Loss: 0.7751330137252808\n",
      "Epoch 9318: Training Loss: 0.05307766670982043 Validation Loss: 0.7798850536346436\n",
      "Epoch 9319: Training Loss: 0.05322892094651858 Validation Loss: 0.7843741774559021\n",
      "Epoch 9320: Training Loss: 0.05300317332148552 Validation Loss: 0.7831366062164307\n",
      "Epoch 9321: Training Loss: 0.05298894022901853 Validation Loss: 0.7814765572547913\n",
      "Epoch 9322: Training Loss: 0.053043914337952934 Validation Loss: 0.781679630279541\n",
      "Epoch 9323: Training Loss: 0.05290441463390986 Validation Loss: 0.777498722076416\n",
      "Epoch 9324: Training Loss: 0.05294175694386164 Validation Loss: 0.7758873105049133\n",
      "Epoch 9325: Training Loss: 0.053451862186193466 Validation Loss: 0.7716265320777893\n",
      "Epoch 9326: Training Loss: 0.053347392628590264 Validation Loss: 0.7773563265800476\n",
      "Epoch 9327: Training Loss: 0.05287729576230049 Validation Loss: 0.7796298861503601\n",
      "Epoch 9328: Training Loss: 0.0530690128604571 Validation Loss: 0.7828165292739868\n",
      "Epoch 9329: Training Loss: 0.05295996616284052 Validation Loss: 0.7808834910392761\n",
      "Epoch 9330: Training Loss: 0.052955073614915214 Validation Loss: 0.7800215482711792\n",
      "Epoch 9331: Training Loss: 0.05296170090635618 Validation Loss: 0.7792263627052307\n",
      "Epoch 9332: Training Loss: 0.05292782187461853 Validation Loss: 0.7779141068458557\n",
      "Epoch 9333: Training Loss: 0.05298948412140211 Validation Loss: 0.7764467000961304\n",
      "Epoch 9334: Training Loss: 0.05289934451381365 Validation Loss: 0.7779350280761719\n",
      "Epoch 9335: Training Loss: 0.052968911826610565 Validation Loss: 0.7794732451438904\n",
      "Epoch 9336: Training Loss: 0.05282480518023173 Validation Loss: 0.7806326150894165\n",
      "Epoch 9337: Training Loss: 0.05298541113734245 Validation Loss: 0.7831128835678101\n",
      "Epoch 9338: Training Loss: 0.05288734162847201 Validation Loss: 0.782056450843811\n",
      "Epoch 9339: Training Loss: 0.052972378830115 Validation Loss: 0.7780210971832275\n",
      "Epoch 9340: Training Loss: 0.052926732848087944 Validation Loss: 0.7781896591186523\n",
      "Epoch 9341: Training Loss: 0.05302835007508596 Validation Loss: 0.780333936214447\n",
      "Epoch 9342: Training Loss: 0.05294458940625191 Validation Loss: 0.7797834277153015\n",
      "Epoch 9343: Training Loss: 0.052832389871279396 Validation Loss: 0.7776404023170471\n",
      "Epoch 9344: Training Loss: 0.052816241979599 Validation Loss: 0.7765951156616211\n",
      "Epoch 9345: Training Loss: 0.05303818235794703 Validation Loss: 0.7741942405700684\n",
      "Epoch 9346: Training Loss: 0.05291391536593437 Validation Loss: 0.7775697112083435\n",
      "Epoch 9347: Training Loss: 0.0528315876921018 Validation Loss: 0.7781546115875244\n",
      "Epoch 9348: Training Loss: 0.05278472229838371 Validation Loss: 0.7798905372619629\n",
      "Epoch 9349: Training Loss: 0.05281568566958109 Validation Loss: 0.781916618347168\n",
      "Epoch 9350: Training Loss: 0.05282929912209511 Validation Loss: 0.7824823260307312\n",
      "Epoch 9351: Training Loss: 0.053026795387268066 Validation Loss: 0.779586136341095\n",
      "Epoch 9352: Training Loss: 0.053003442784150444 Validation Loss: 0.7824448943138123\n",
      "Epoch 9353: Training Loss: 0.052792612463235855 Validation Loss: 0.7816615700721741\n",
      "Epoch 9354: Training Loss: 0.053034658233324684 Validation Loss: 0.7775368094444275\n",
      "Epoch 9355: Training Loss: 0.052877320597569145 Validation Loss: 0.7779181003570557\n",
      "Epoch 9356: Training Loss: 0.052803137029210724 Validation Loss: 0.7790562510490417\n",
      "Epoch 9357: Training Loss: 0.05304666981101036 Validation Loss: 0.7757187485694885\n",
      "Epoch 9358: Training Loss: 0.05295183137059212 Validation Loss: 0.78010094165802\n",
      "Epoch 9359: Training Loss: 0.05304366101821264 Validation Loss: 0.778607189655304\n",
      "Epoch 9360: Training Loss: 0.052928650130828224 Validation Loss: 0.7811442017555237\n",
      "Epoch 9361: Training Loss: 0.0529123010734717 Validation Loss: 0.7821295261383057\n",
      "Epoch 9362: Training Loss: 0.05281878262758255 Validation Loss: 0.7787795066833496\n",
      "Epoch 9363: Training Loss: 0.05289276565114657 Validation Loss: 0.7752723693847656\n",
      "Epoch 9364: Training Loss: 0.05277170116702715 Validation Loss: 0.7758674621582031\n",
      "Epoch 9365: Training Loss: 0.05290297418832779 Validation Loss: 0.7760633230209351\n",
      "Epoch 9366: Training Loss: 0.05280044302344322 Validation Loss: 0.7766783833503723\n",
      "Epoch 9367: Training Loss: 0.05274557570616404 Validation Loss: 0.7791873812675476\n",
      "Epoch 9368: Training Loss: 0.05274185165762901 Validation Loss: 0.7832393050193787\n",
      "Epoch 9369: Training Loss: 0.052840677400430046 Validation Loss: 0.7826948165893555\n",
      "Epoch 9370: Training Loss: 0.05287914598981539 Validation Loss: 0.7837706208229065\n",
      "Epoch 9371: Training Loss: 0.05304087077577909 Validation Loss: 0.7788421511650085\n",
      "Epoch 9372: Training Loss: 0.052797126273314156 Validation Loss: 0.7785025835037231\n",
      "Epoch 9373: Training Loss: 0.05285451064507166 Validation Loss: 0.7807512879371643\n",
      "Epoch 9374: Training Loss: 0.05281045784552892 Validation Loss: 0.7814157009124756\n",
      "Epoch 9375: Training Loss: 0.052756018936634064 Validation Loss: 0.7803272604942322\n",
      "Epoch 9376: Training Loss: 0.05268790324529012 Validation Loss: 0.7780805826187134\n",
      "Epoch 9377: Training Loss: 0.052796694139639534 Validation Loss: 0.7769367098808289\n",
      "Epoch 9378: Training Loss: 0.05307397743066152 Validation Loss: 0.7803008556365967\n",
      "Epoch 9379: Training Loss: 0.052871705343325935 Validation Loss: 0.7825044989585876\n",
      "Epoch 9380: Training Loss: 0.052814461290836334 Validation Loss: 0.7790557146072388\n",
      "Epoch 9381: Training Loss: 0.052691140522559486 Validation Loss: 0.7776962518692017\n",
      "Epoch 9382: Training Loss: 0.052728685239950814 Validation Loss: 0.7778254747390747\n",
      "Epoch 9383: Training Loss: 0.05266060928503672 Validation Loss: 0.7789249420166016\n",
      "Epoch 9384: Training Loss: 0.05269067858656248 Validation Loss: 0.7799416780471802\n",
      "Epoch 9385: Training Loss: 0.05278406913081805 Validation Loss: 0.7820671796798706\n",
      "Epoch 9386: Training Loss: 0.05265153447786967 Validation Loss: 0.7812960147857666\n",
      "Epoch 9387: Training Loss: 0.05267258236805598 Validation Loss: 0.7787982821464539\n",
      "Epoch 9388: Training Loss: 0.052665996054808296 Validation Loss: 0.7777454257011414\n",
      "Epoch 9389: Training Loss: 0.05296040574709574 Validation Loss: 0.7750418782234192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9390: Training Loss: 0.05284690484404564 Validation Loss: 0.7802180051803589\n",
      "Epoch 9391: Training Loss: 0.052626886715491615 Validation Loss: 0.7804388999938965\n",
      "Epoch 9392: Training Loss: 0.0527093398074309 Validation Loss: 0.7823984622955322\n",
      "Epoch 9393: Training Loss: 0.05263475452860197 Validation Loss: 0.7823529243469238\n",
      "Epoch 9394: Training Loss: 0.05275035401185354 Validation Loss: 0.7833631634712219\n",
      "Epoch 9395: Training Loss: 0.05274656663338343 Validation Loss: 0.7789614200592041\n",
      "Epoch 9396: Training Loss: 0.05267518013715744 Validation Loss: 0.7793858647346497\n",
      "Epoch 9397: Training Loss: 0.05260715385278066 Validation Loss: 0.7784332633018494\n",
      "Epoch 9398: Training Loss: 0.05260150879621506 Validation Loss: 0.7779849171638489\n",
      "Epoch 9399: Training Loss: 0.052604105323553085 Validation Loss: 0.7784420847892761\n",
      "Epoch 9400: Training Loss: 0.052626125514507294 Validation Loss: 0.7787772417068481\n",
      "Epoch 9401: Training Loss: 0.052567544082800545 Validation Loss: 0.7799816727638245\n",
      "Epoch 9402: Training Loss: 0.05260344470540682 Validation Loss: 0.7792826294898987\n",
      "Epoch 9403: Training Loss: 0.05268881842494011 Validation Loss: 0.7818829417228699\n",
      "Epoch 9404: Training Loss: 0.05268078421552976 Validation Loss: 0.7816680073738098\n",
      "Epoch 9405: Training Loss: 0.05258071546753248 Validation Loss: 0.7802289128303528\n",
      "Epoch 9406: Training Loss: 0.052677189310391746 Validation Loss: 0.77760910987854\n",
      "Epoch 9407: Training Loss: 0.05274909858902296 Validation Loss: 0.77683025598526\n",
      "Epoch 9408: Training Loss: 0.052643155058225 Validation Loss: 0.7794879078865051\n",
      "Epoch 9409: Training Loss: 0.05260078112284342 Validation Loss: 0.7801656723022461\n",
      "Epoch 9410: Training Loss: 0.05255988985300064 Validation Loss: 0.7801565527915955\n",
      "Epoch 9411: Training Loss: 0.05271524439255396 Validation Loss: 0.7818276882171631\n",
      "Epoch 9412: Training Loss: 0.052601125091314316 Validation Loss: 0.780763566493988\n",
      "Epoch 9413: Training Loss: 0.052613300581773124 Validation Loss: 0.7809327244758606\n",
      "Epoch 9414: Training Loss: 0.05256453653176626 Validation Loss: 0.779827356338501\n",
      "Epoch 9415: Training Loss: 0.05255685622493426 Validation Loss: 0.7778611779212952\n",
      "Epoch 9416: Training Loss: 0.052528069665034614 Validation Loss: 0.7779908776283264\n",
      "Epoch 9417: Training Loss: 0.052524633705616 Validation Loss: 0.7771853804588318\n",
      "Epoch 9418: Training Loss: 0.05262681717673937 Validation Loss: 0.7762300372123718\n",
      "Epoch 9419: Training Loss: 0.05252758041024208 Validation Loss: 0.7793243527412415\n",
      "Epoch 9420: Training Loss: 0.05273292462031046 Validation Loss: 0.7828458547592163\n",
      "Epoch 9421: Training Loss: 0.052579354494810104 Validation Loss: 0.7825339436531067\n",
      "Epoch 9422: Training Loss: 0.05251363292336464 Validation Loss: 0.7825391888618469\n",
      "Epoch 9423: Training Loss: 0.052741111566623054 Validation Loss: 0.7826768159866333\n",
      "Epoch 9424: Training Loss: 0.05257530013720194 Validation Loss: 0.7794987559318542\n",
      "Epoch 9425: Training Loss: 0.05271053190032641 Validation Loss: 0.7753891348838806\n",
      "Epoch 9426: Training Loss: 0.05280879388252894 Validation Loss: 0.7794591188430786\n",
      "Epoch 9427: Training Loss: 0.052499137818813324 Validation Loss: 0.779432475566864\n",
      "Epoch 9428: Training Loss: 0.052465337018171944 Validation Loss: 0.7797344923019409\n",
      "Epoch 9429: Training Loss: 0.052526697516441345 Validation Loss: 0.7789937257766724\n",
      "Epoch 9430: Training Loss: 0.05255268017450968 Validation Loss: 0.7816049456596375\n",
      "Epoch 9431: Training Loss: 0.05252399668097496 Validation Loss: 0.7816286683082581\n",
      "Epoch 9432: Training Loss: 0.052539385855197906 Validation Loss: 0.780674934387207\n",
      "Epoch 9433: Training Loss: 0.05249405652284622 Validation Loss: 0.7816576957702637\n",
      "Epoch 9434: Training Loss: 0.052446951468785606 Validation Loss: 0.7803277373313904\n",
      "Epoch 9435: Training Loss: 0.05243816847602526 Validation Loss: 0.7800425291061401\n",
      "Epoch 9436: Training Loss: 0.052571624517440796 Validation Loss: 0.779283881187439\n",
      "Epoch 9437: Training Loss: 0.052433191488186516 Validation Loss: 0.7796360850334167\n",
      "Epoch 9438: Training Loss: 0.05296019837260246 Validation Loss: 0.7844136357307434\n",
      "Epoch 9439: Training Loss: 0.05266877760489782 Validation Loss: 0.7805733680725098\n",
      "Epoch 9440: Training Loss: 0.052551248421271644 Validation Loss: 0.7811263203620911\n",
      "Epoch 9441: Training Loss: 0.05242169524232546 Validation Loss: 0.7802204489707947\n",
      "Epoch 9442: Training Loss: 0.05245896180470785 Validation Loss: 0.7778366208076477\n",
      "Epoch 9443: Training Loss: 0.052523436645666756 Validation Loss: 0.7764819264411926\n",
      "Epoch 9444: Training Loss: 0.05283313741286596 Validation Loss: 0.78093022108078\n",
      "Epoch 9445: Training Loss: 0.05246756846706072 Validation Loss: 0.7795811295509338\n",
      "Epoch 9446: Training Loss: 0.05261044576764107 Validation Loss: 0.7800986766815186\n",
      "Epoch 9447: Training Loss: 0.05256078392267227 Validation Loss: 0.779710054397583\n",
      "Epoch 9448: Training Loss: 0.052425497521956764 Validation Loss: 0.7780752182006836\n",
      "Epoch 9449: Training Loss: 0.052661456167697906 Validation Loss: 0.7760699391365051\n",
      "Epoch 9450: Training Loss: 0.05280740559101105 Validation Loss: 0.7768087983131409\n",
      "Epoch 9451: Training Loss: 0.05236114685734113 Validation Loss: 0.7794172167778015\n",
      "Epoch 9452: Training Loss: 0.05240999534726143 Validation Loss: 0.7834116220474243\n",
      "Epoch 9453: Training Loss: 0.05242301275332769 Validation Loss: 0.7855628728866577\n",
      "Epoch 9454: Training Loss: 0.052501012881596885 Validation Loss: 0.7851940989494324\n",
      "Epoch 9455: Training Loss: 0.05254212891062101 Validation Loss: 0.7849878072738647\n",
      "Epoch 9456: Training Loss: 0.05234411607185999 Validation Loss: 0.7809681296348572\n",
      "Epoch 9457: Training Loss: 0.05311260869105657 Validation Loss: 0.7734296321868896\n",
      "Epoch 9458: Training Loss: 0.05250266194343567 Validation Loss: 0.7737939357757568\n",
      "Epoch 9459: Training Loss: 0.052622651060422264 Validation Loss: 0.7794671654701233\n",
      "Epoch 9460: Training Loss: 0.05263784651954969 Validation Loss: 0.7838969230651855\n",
      "Epoch 9461: Training Loss: 0.052419442062576614 Validation Loss: 0.7825968861579895\n",
      "Epoch 9462: Training Loss: 0.05232629676659902 Validation Loss: 0.782048761844635\n",
      "Epoch 9463: Training Loss: 0.05236970136562983 Validation Loss: 0.782058835029602\n",
      "Epoch 9464: Training Loss: 0.05247874930500984 Validation Loss: 0.7781146764755249\n",
      "Epoch 9465: Training Loss: 0.05257810279726982 Validation Loss: 0.7760270833969116\n",
      "Epoch 9466: Training Loss: 0.05247474958499273 Validation Loss: 0.7801129221916199\n",
      "Epoch 9467: Training Loss: 0.05243571226795515 Validation Loss: 0.7810290455818176\n",
      "Epoch 9468: Training Loss: 0.052320475379625954 Validation Loss: 0.7813590168952942\n",
      "Epoch 9469: Training Loss: 0.052379158635934196 Validation Loss: 0.7804759740829468\n",
      "Epoch 9470: Training Loss: 0.05230904991428057 Validation Loss: 0.7820107340812683\n",
      "Epoch 9471: Training Loss: 0.05240491653482119 Validation Loss: 0.7814664840698242\n",
      "Epoch 9472: Training Loss: 0.05259285494685173 Validation Loss: 0.778613269329071\n",
      "Epoch 9473: Training Loss: 0.052312434961398445 Validation Loss: 0.7804051637649536\n",
      "Epoch 9474: Training Loss: 0.05263683199882507 Validation Loss: 0.7851914763450623\n",
      "Epoch 9475: Training Loss: 0.052425907303889595 Validation Loss: 0.7853701710700989\n",
      "Epoch 9476: Training Loss: 0.052448820322752 Validation Loss: 0.7805060744285583\n",
      "Epoch 9477: Training Loss: 0.0522780679166317 Validation Loss: 0.7792263627052307\n",
      "Epoch 9478: Training Loss: 0.052669948587814965 Validation Loss: 0.774954080581665\n",
      "Epoch 9479: Training Loss: 0.05233469481269518 Validation Loss: 0.7778077125549316\n",
      "Epoch 9480: Training Loss: 0.052364872147639595 Validation Loss: 0.7810744643211365\n",
      "Epoch 9481: Training Loss: 0.0523009200890859 Validation Loss: 0.7822957634925842\n",
      "Epoch 9482: Training Loss: 0.052424218505620956 Validation Loss: 0.7834081649780273\n",
      "Epoch 9483: Training Loss: 0.05231152723232905 Validation Loss: 0.782559871673584\n",
      "Epoch 9484: Training Loss: 0.052371361603339515 Validation Loss: 0.7781068682670593\n",
      "Epoch 9485: Training Loss: 0.05238832160830498 Validation Loss: 0.7764266133308411\n",
      "Epoch 9486: Training Loss: 0.05239446212848028 Validation Loss: 0.7784604430198669\n",
      "Epoch 9487: Training Loss: 0.05240827798843384 Validation Loss: 0.78080153465271\n",
      "Epoch 9488: Training Loss: 0.052209259321292244 Validation Loss: 0.7804762125015259\n",
      "Epoch 9489: Training Loss: 0.052239603052536644 Validation Loss: 0.780146062374115\n",
      "Epoch 9490: Training Loss: 0.05237978448470434 Validation Loss: 0.7826284766197205\n",
      "Epoch 9491: Training Loss: 0.0522518257300059 Validation Loss: 0.7824685573577881\n",
      "Epoch 9492: Training Loss: 0.05226036657889684 Validation Loss: 0.7823953628540039\n",
      "Epoch 9493: Training Loss: 0.05292895808815956 Validation Loss: 0.776618480682373\n",
      "Epoch 9494: Training Loss: 0.05232655256986618 Validation Loss: 0.7767002582550049\n",
      "Epoch 9495: Training Loss: 0.05221609895428022 Validation Loss: 0.7791542410850525\n",
      "Epoch 9496: Training Loss: 0.05230811362465223 Validation Loss: 0.7828492522239685\n",
      "Epoch 9497: Training Loss: 0.052388367553551994 Validation Loss: 0.7846768498420715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9498: Training Loss: 0.052476865549882255 Validation Loss: 0.7811408638954163\n",
      "Epoch 9499: Training Loss: 0.052366302659114204 Validation Loss: 0.7785940766334534\n",
      "Epoch 9500: Training Loss: 0.05262211958567301 Validation Loss: 0.7762192487716675\n",
      "Epoch 9501: Training Loss: 0.05238805462916692 Validation Loss: 0.781505823135376\n",
      "Epoch 9502: Training Loss: 0.052309410025676094 Validation Loss: 0.7810519933700562\n",
      "Epoch 9503: Training Loss: 0.05224800234039625 Validation Loss: 0.7845028638839722\n",
      "Epoch 9504: Training Loss: 0.0523051992058754 Validation Loss: 0.7831382751464844\n",
      "Epoch 9505: Training Loss: 0.05265961214900017 Validation Loss: 0.7789732217788696\n",
      "Epoch 9506: Training Loss: 0.05229418848951658 Validation Loss: 0.7815243005752563\n",
      "Epoch 9507: Training Loss: 0.05231118698914846 Validation Loss: 0.7814579606056213\n",
      "Epoch 9508: Training Loss: 0.052199924985567726 Validation Loss: 0.7818201780319214\n",
      "Epoch 9509: Training Loss: 0.05236684655149778 Validation Loss: 0.784636914730072\n",
      "Epoch 9510: Training Loss: 0.05224962905049324 Validation Loss: 0.7843191027641296\n",
      "Epoch 9511: Training Loss: 0.0522016870478789 Validation Loss: 0.7810118794441223\n",
      "Epoch 9512: Training Loss: 0.05239957198500633 Validation Loss: 0.7775657773017883\n",
      "Epoch 9513: Training Loss: 0.05235122020045916 Validation Loss: 0.780246376991272\n",
      "Epoch 9514: Training Loss: 0.05275097241004308 Validation Loss: 0.7844446897506714\n",
      "Epoch 9515: Training Loss: 0.05216284468770027 Validation Loss: 0.7811056971549988\n",
      "Epoch 9516: Training Loss: 0.052383068948984146 Validation Loss: 0.7771376371383667\n",
      "Epoch 9517: Training Loss: 0.05227340261141459 Validation Loss: 0.7765790224075317\n",
      "Epoch 9518: Training Loss: 0.05238045503695806 Validation Loss: 0.7815625667572021\n",
      "Epoch 9519: Training Loss: 0.05235980078577995 Validation Loss: 0.7842465043067932\n",
      "Epoch 9520: Training Loss: 0.05219007655978203 Validation Loss: 0.782082736492157\n",
      "Epoch 9521: Training Loss: 0.05209956814845403 Validation Loss: 0.7805795669555664\n",
      "Epoch 9522: Training Loss: 0.052257005125284195 Validation Loss: 0.7788189649581909\n",
      "Epoch 9523: Training Loss: 0.05216877038280169 Validation Loss: 0.7790669798851013\n",
      "Epoch 9524: Training Loss: 0.05211179330945015 Validation Loss: 0.780986487865448\n",
      "Epoch 9525: Training Loss: 0.052232775837183 Validation Loss: 0.7840542197227478\n",
      "Epoch 9526: Training Loss: 0.05227503056327502 Validation Loss: 0.7822878956794739\n",
      "Epoch 9527: Training Loss: 0.05227905263503393 Validation Loss: 0.7797736525535583\n",
      "Epoch 9528: Training Loss: 0.052109140902757645 Validation Loss: 0.781505286693573\n",
      "Epoch 9529: Training Loss: 0.05233181267976761 Validation Loss: 0.784251868724823\n",
      "Epoch 9530: Training Loss: 0.052184356997410454 Validation Loss: 0.7848407626152039\n",
      "Epoch 9531: Training Loss: 0.052343898763259254 Validation Loss: 0.7799547910690308\n",
      "Epoch 9532: Training Loss: 0.052223093807697296 Validation Loss: 0.7772617936134338\n",
      "Epoch 9533: Training Loss: 0.052108207096656166 Validation Loss: 0.778908908367157\n",
      "Epoch 9534: Training Loss: 0.052039165049791336 Validation Loss: 0.7808830738067627\n",
      "Epoch 9535: Training Loss: 0.0520483081539472 Validation Loss: 0.782868504524231\n",
      "Epoch 9536: Training Loss: 0.05223458136121432 Validation Loss: 0.7856826782226562\n",
      "Epoch 9537: Training Loss: 0.05209984381993612 Validation Loss: 0.7842283248901367\n",
      "Epoch 9538: Training Loss: 0.05208737030625343 Validation Loss: 0.7815156579017639\n",
      "Epoch 9539: Training Loss: 0.052018934239943825 Validation Loss: 0.7797933220863342\n",
      "Epoch 9540: Training Loss: 0.05208376546700796 Validation Loss: 0.7783505320549011\n",
      "Epoch 9541: Training Loss: 0.05205997948845228 Validation Loss: 0.7780400514602661\n",
      "Epoch 9542: Training Loss: 0.05210963015755018 Validation Loss: 0.7806081175804138\n",
      "Epoch 9543: Training Loss: 0.05204928293824196 Validation Loss: 0.7803874015808105\n",
      "Epoch 9544: Training Loss: 0.05202868456641833 Validation Loss: 0.7800617218017578\n",
      "Epoch 9545: Training Loss: 0.05212500070532163 Validation Loss: 0.7794703245162964\n",
      "Epoch 9546: Training Loss: 0.05215203141172727 Validation Loss: 0.7823695540428162\n",
      "Epoch 9547: Training Loss: 0.05211140215396881 Validation Loss: 0.7841976284980774\n",
      "Epoch 9548: Training Loss: 0.05205980067451795 Validation Loss: 0.7819930911064148\n",
      "Epoch 9549: Training Loss: 0.05197409292062124 Validation Loss: 0.7811782956123352\n",
      "Epoch 9550: Training Loss: 0.05214393883943558 Validation Loss: 0.7821897268295288\n",
      "Epoch 9551: Training Loss: 0.05212829262018204 Validation Loss: 0.7827925086021423\n",
      "Epoch 9552: Training Loss: 0.05201955263813337 Validation Loss: 0.781349241733551\n",
      "Epoch 9553: Training Loss: 0.0520798514286677 Validation Loss: 0.7806980013847351\n",
      "Epoch 9554: Training Loss: 0.05236894761522611 Validation Loss: 0.7748847603797913\n",
      "Epoch 9555: Training Loss: 0.05209390198191007 Validation Loss: 0.7756445407867432\n",
      "Epoch 9556: Training Loss: 0.0522524726887544 Validation Loss: 0.7760515809059143\n",
      "Epoch 9557: Training Loss: 0.05197835589448611 Validation Loss: 0.7797574996948242\n",
      "Epoch 9558: Training Loss: 0.05193939929207166 Validation Loss: 0.7835274338722229\n",
      "Epoch 9559: Training Loss: 0.05196386327346166 Validation Loss: 0.7849852442741394\n",
      "Epoch 9560: Training Loss: 0.052063009391228356 Validation Loss: 0.7843019962310791\n",
      "Epoch 9561: Training Loss: 0.052139500776926674 Validation Loss: 0.7867194414138794\n",
      "Epoch 9562: Training Loss: 0.05205396314462026 Validation Loss: 0.7832880020141602\n",
      "Epoch 9563: Training Loss: 0.051930543035268784 Validation Loss: 0.7819988131523132\n",
      "Epoch 9564: Training Loss: 0.05200483277440071 Validation Loss: 0.7796701192855835\n",
      "Epoch 9565: Training Loss: 0.05205452938874563 Validation Loss: 0.7780176401138306\n",
      "Epoch 9566: Training Loss: 0.052031642446915306 Validation Loss: 0.7787092328071594\n",
      "Epoch 9567: Training Loss: 0.05197553833325704 Validation Loss: 0.779585063457489\n",
      "Epoch 9568: Training Loss: 0.05224474767843882 Validation Loss: 0.7848027944564819\n",
      "Epoch 9569: Training Loss: 0.052146164079507194 Validation Loss: 0.7821506261825562\n",
      "Epoch 9570: Training Loss: 0.05211105942726135 Validation Loss: 0.7843508720397949\n",
      "Epoch 9571: Training Loss: 0.0519208088517189 Validation Loss: 0.7827426791191101\n",
      "Epoch 9572: Training Loss: 0.05200300986568133 Validation Loss: 0.7796614170074463\n",
      "Epoch 9573: Training Loss: 0.05193719764550527 Validation Loss: 0.7811775207519531\n",
      "Epoch 9574: Training Loss: 0.05189405878384908 Validation Loss: 0.7815766334533691\n",
      "Epoch 9575: Training Loss: 0.051964721331993736 Validation Loss: 0.7814196348190308\n",
      "Epoch 9576: Training Loss: 0.052052225917577744 Validation Loss: 0.7836246490478516\n",
      "Epoch 9577: Training Loss: 0.05192655076583227 Validation Loss: 0.7837393879890442\n",
      "Epoch 9578: Training Loss: 0.05192481726408005 Validation Loss: 0.7823929190635681\n",
      "Epoch 9579: Training Loss: 0.05189722776412964 Validation Loss: 0.781085729598999\n",
      "Epoch 9580: Training Loss: 0.05196246753136317 Validation Loss: 0.7816424369812012\n",
      "Epoch 9581: Training Loss: 0.052193048099676766 Validation Loss: 0.7776923179626465\n",
      "Epoch 9582: Training Loss: 0.05193415408333143 Validation Loss: 0.777929961681366\n",
      "Epoch 9583: Training Loss: 0.05200816070040067 Validation Loss: 0.7823690176010132\n",
      "Epoch 9584: Training Loss: 0.05197271332144737 Validation Loss: 0.7816585898399353\n",
      "Epoch 9585: Training Loss: 0.05188640579581261 Validation Loss: 0.7838333249092102\n",
      "Epoch 9586: Training Loss: 0.051849547773599625 Validation Loss: 0.7841450572013855\n",
      "Epoch 9587: Training Loss: 0.052077883233626686 Validation Loss: 0.7827788591384888\n",
      "Epoch 9588: Training Loss: 0.05208035558462143 Validation Loss: 0.7806999087333679\n",
      "Epoch 9589: Training Loss: 0.05186253786087036 Validation Loss: 0.7813333868980408\n",
      "Epoch 9590: Training Loss: 0.052303424725929894 Validation Loss: 0.7842828631401062\n",
      "Epoch 9591: Training Loss: 0.05180159583687782 Validation Loss: 0.7822452783584595\n",
      "Epoch 9592: Training Loss: 0.05257681633035342 Validation Loss: 0.7763696312904358\n",
      "Epoch 9593: Training Loss: 0.05192592740058899 Validation Loss: 0.7774902582168579\n",
      "Epoch 9594: Training Loss: 0.051839551577965416 Validation Loss: 0.7797903418540955\n",
      "Epoch 9595: Training Loss: 0.051945578306913376 Validation Loss: 0.7838912606239319\n",
      "Epoch 9596: Training Loss: 0.05185610055923462 Validation Loss: 0.7860175967216492\n",
      "Epoch 9597: Training Loss: 0.051891533037026726 Validation Loss: 0.7862880229949951\n",
      "Epoch 9598: Training Loss: 0.05187544971704483 Validation Loss: 0.7850937247276306\n",
      "Epoch 9599: Training Loss: 0.05208836371699969 Validation Loss: 0.7806830406188965\n",
      "Epoch 9600: Training Loss: 0.051819734275341034 Validation Loss: 0.778816819190979\n",
      "Epoch 9601: Training Loss: 0.05205169195930163 Validation Loss: 0.7772124409675598\n",
      "Epoch 9602: Training Loss: 0.052165038883686066 Validation Loss: 0.7821851968765259\n",
      "Epoch 9603: Training Loss: 0.052216336131095886 Validation Loss: 0.7800673842430115\n",
      "Epoch 9604: Training Loss: 0.05195098494489988 Validation Loss: 0.7841934561729431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9605: Training Loss: 0.051882073283195496 Validation Loss: 0.783781111240387\n",
      "Epoch 9606: Training Loss: 0.051782477647066116 Validation Loss: 0.7845754027366638\n",
      "Epoch 9607: Training Loss: 0.05191966270407041 Validation Loss: 0.7815781235694885\n",
      "Epoch 9608: Training Loss: 0.05184552570184072 Validation Loss: 0.7824870944023132\n",
      "Epoch 9609: Training Loss: 0.05185182144244512 Validation Loss: 0.7826075553894043\n",
      "Epoch 9610: Training Loss: 0.05195979277292887 Validation Loss: 0.7843950986862183\n",
      "Epoch 9611: Training Loss: 0.05183834210038185 Validation Loss: 0.7826080322265625\n",
      "Epoch 9612: Training Loss: 0.05204059680302938 Validation Loss: 0.7839012145996094\n",
      "Epoch 9613: Training Loss: 0.051799570520718895 Validation Loss: 0.779128909111023\n",
      "Epoch 9614: Training Loss: 0.051829143116871514 Validation Loss: 0.777005672454834\n",
      "Epoch 9615: Training Loss: 0.051877407977978386 Validation Loss: 0.7787295579910278\n",
      "Epoch 9616: Training Loss: 0.051784822096427284 Validation Loss: 0.7804779410362244\n",
      "Epoch 9617: Training Loss: 0.05178014189004898 Validation Loss: 0.7823584079742432\n",
      "Epoch 9618: Training Loss: 0.0519772544503212 Validation Loss: 0.7849703431129456\n",
      "Epoch 9619: Training Loss: 0.0517806646724542 Validation Loss: 0.7823265194892883\n",
      "Epoch 9620: Training Loss: 0.05182242393493652 Validation Loss: 0.7794556617736816\n",
      "Epoch 9621: Training Loss: 0.05192639554540316 Validation Loss: 0.7829121947288513\n",
      "Epoch 9622: Training Loss: 0.05181385204195976 Validation Loss: 0.7817667722702026\n",
      "Epoch 9623: Training Loss: 0.051710174729426704 Validation Loss: 0.7816963195800781\n",
      "Epoch 9624: Training Loss: 0.051933616399765015 Validation Loss: 0.7844419479370117\n",
      "Epoch 9625: Training Loss: 0.0518055334687233 Validation Loss: 0.784558892250061\n",
      "Epoch 9626: Training Loss: 0.05197427421808243 Validation Loss: 0.7803828716278076\n",
      "Epoch 9627: Training Loss: 0.051763277500867844 Validation Loss: 0.778821587562561\n",
      "Epoch 9628: Training Loss: 0.05173613255222639 Validation Loss: 0.7803906798362732\n",
      "Epoch 9629: Training Loss: 0.051887294898430504 Validation Loss: 0.7831830978393555\n",
      "Epoch 9630: Training Loss: 0.051846105605363846 Validation Loss: 0.780868649482727\n",
      "Epoch 9631: Training Loss: 0.05175388356049856 Validation Loss: 0.7829164862632751\n",
      "Epoch 9632: Training Loss: 0.05177196736137072 Validation Loss: 0.7826942801475525\n",
      "Epoch 9633: Training Loss: 0.05190310627222061 Validation Loss: 0.779862105846405\n",
      "Epoch 9634: Training Loss: 0.05176391204198202 Validation Loss: 0.7823686003684998\n",
      "Epoch 9635: Training Loss: 0.05181838075319926 Validation Loss: 0.7815818190574646\n",
      "Epoch 9636: Training Loss: 0.05167000864942869 Validation Loss: 0.7820349335670471\n",
      "Epoch 9637: Training Loss: 0.051759899904330574 Validation Loss: 0.7809723019599915\n",
      "Epoch 9638: Training Loss: 0.051692785074313484 Validation Loss: 0.7839906215667725\n",
      "Epoch 9639: Training Loss: 0.05168622483809789 Validation Loss: 0.7835475206375122\n",
      "Epoch 9640: Training Loss: 0.051709519078334175 Validation Loss: 0.7827468514442444\n",
      "Epoch 9641: Training Loss: 0.05178202440341314 Validation Loss: 0.7849627733230591\n",
      "Epoch 9642: Training Loss: 0.051665528366963066 Validation Loss: 0.7843184471130371\n",
      "Epoch 9643: Training Loss: 0.051678366959095 Validation Loss: 0.7831203937530518\n",
      "Epoch 9644: Training Loss: 0.051625764618317284 Validation Loss: 0.7807995080947876\n",
      "Epoch 9645: Training Loss: 0.051703366140524544 Validation Loss: 0.7780136466026306\n",
      "Epoch 9646: Training Loss: 0.05175963416695595 Validation Loss: 0.7780286073684692\n",
      "Epoch 9647: Training Loss: 0.0517056609193484 Validation Loss: 0.780742347240448\n",
      "Epoch 9648: Training Loss: 0.05181311443448067 Validation Loss: 0.7833390235900879\n",
      "Epoch 9649: Training Loss: 0.05173408364256223 Validation Loss: 0.7865561842918396\n",
      "Epoch 9650: Training Loss: 0.051915893952051796 Validation Loss: 0.7826743125915527\n",
      "Epoch 9651: Training Loss: 0.05192065859834353 Validation Loss: 0.7803025245666504\n",
      "Epoch 9652: Training Loss: 0.05171689515312513 Validation Loss: 0.7820872068405151\n",
      "Epoch 9653: Training Loss: 0.051653182754913964 Validation Loss: 0.7824641466140747\n",
      "Epoch 9654: Training Loss: 0.05164511501789093 Validation Loss: 0.7830075025558472\n",
      "Epoch 9655: Training Loss: 0.0517150970796744 Validation Loss: 0.7859192490577698\n",
      "Epoch 9656: Training Loss: 0.05166336397329966 Validation Loss: 0.7848602533340454\n",
      "Epoch 9657: Training Loss: 0.05174554387728373 Validation Loss: 0.7808530926704407\n",
      "Epoch 9658: Training Loss: 0.051589117695887886 Validation Loss: 0.7809173464775085\n",
      "Epoch 9659: Training Loss: 0.05173536265889803 Validation Loss: 0.7792000770568848\n",
      "Epoch 9660: Training Loss: 0.05160903682311376 Validation Loss: 0.780687153339386\n",
      "Epoch 9661: Training Loss: 0.051654002318779625 Validation Loss: 0.7813174724578857\n",
      "Epoch 9662: Training Loss: 0.051584662248690925 Validation Loss: 0.7843505144119263\n",
      "Epoch 9663: Training Loss: 0.051707204431295395 Validation Loss: 0.787151038646698\n",
      "Epoch 9664: Training Loss: 0.051746517419815063 Validation Loss: 0.7858230471611023\n",
      "Epoch 9665: Training Loss: 0.051745341469844185 Validation Loss: 0.7829970121383667\n",
      "Epoch 9666: Training Loss: 0.05170132468144099 Validation Loss: 0.7809270620346069\n",
      "Epoch 9667: Training Loss: 0.05197382097442945 Validation Loss: 0.779559314250946\n",
      "Epoch 9668: Training Loss: 0.051544164617856346 Validation Loss: 0.7810405492782593\n",
      "Epoch 9669: Training Loss: 0.05153525496522585 Validation Loss: 0.7840827107429504\n",
      "Epoch 9670: Training Loss: 0.05165866017341614 Validation Loss: 0.7837976217269897\n",
      "Epoch 9671: Training Loss: 0.05199847867091497 Validation Loss: 0.7887665629386902\n",
      "Epoch 9672: Training Loss: 0.051637849460045494 Validation Loss: 0.7851871252059937\n",
      "Epoch 9673: Training Loss: 0.051520983378092446 Validation Loss: 0.7835897207260132\n",
      "Epoch 9674: Training Loss: 0.0516262042025725 Validation Loss: 0.7819089293479919\n",
      "Epoch 9675: Training Loss: 0.051502241442600884 Validation Loss: 0.7802075147628784\n",
      "Epoch 9676: Training Loss: 0.051550945887962975 Validation Loss: 0.7796260714530945\n",
      "Epoch 9677: Training Loss: 0.05173002307613691 Validation Loss: 0.777445375919342\n",
      "Epoch 9678: Training Loss: 0.05154922356208166 Validation Loss: 0.7795724868774414\n",
      "Epoch 9679: Training Loss: 0.0516493134200573 Validation Loss: 0.7845221161842346\n",
      "Epoch 9680: Training Loss: 0.05161319052179655 Validation Loss: 0.7833157181739807\n",
      "Epoch 9681: Training Loss: 0.05146686111887296 Validation Loss: 0.7848424315452576\n",
      "Epoch 9682: Training Loss: 0.05175183713436127 Validation Loss: 0.7880043983459473\n",
      "Epoch 9683: Training Loss: 0.05155219634373983 Validation Loss: 0.7864285707473755\n",
      "Epoch 9684: Training Loss: 0.05147302026549975 Validation Loss: 0.7838608622550964\n",
      "Epoch 9685: Training Loss: 0.051535887022813164 Validation Loss: 0.7818091511726379\n",
      "Epoch 9686: Training Loss: 0.05167746047178904 Validation Loss: 0.7770066857337952\n",
      "Epoch 9687: Training Loss: 0.051546644419431686 Validation Loss: 0.7774569988250732\n",
      "Epoch 9688: Training Loss: 0.05154989411433538 Validation Loss: 0.7783538699150085\n",
      "Epoch 9689: Training Loss: 0.051609416802724205 Validation Loss: 0.7826568484306335\n",
      "Epoch 9690: Training Loss: 0.05167126655578613 Validation Loss: 0.7868580222129822\n",
      "Epoch 9691: Training Loss: 0.05153122047583262 Validation Loss: 0.7847833037376404\n",
      "Epoch 9692: Training Loss: 0.05169240261117617 Validation Loss: 0.7866086959838867\n",
      "Epoch 9693: Training Loss: 0.05152652909358343 Validation Loss: 0.7820846438407898\n",
      "Epoch 9694: Training Loss: 0.05147760485609373 Validation Loss: 0.7802008390426636\n",
      "Epoch 9695: Training Loss: 0.05146028846502304 Validation Loss: 0.7789438366889954\n",
      "Epoch 9696: Training Loss: 0.05151438837250074 Validation Loss: 0.7804460525512695\n",
      "Epoch 9697: Training Loss: 0.05170347914099693 Validation Loss: 0.7783000469207764\n",
      "Epoch 9698: Training Loss: 0.051477592438459396 Validation Loss: 0.7823865413665771\n",
      "Epoch 9699: Training Loss: 0.05156437307596207 Validation Loss: 0.7839227914810181\n",
      "Epoch 9700: Training Loss: 0.051865306993325554 Validation Loss: 0.7894188165664673\n",
      "Epoch 9701: Training Loss: 0.05180328587690989 Validation Loss: 0.7849773168563843\n",
      "Epoch 9702: Training Loss: 0.05148588493466377 Validation Loss: 0.7835230827331543\n",
      "Epoch 9703: Training Loss: 0.0515475831925869 Validation Loss: 0.7854800820350647\n",
      "Epoch 9704: Training Loss: 0.05144074186682701 Validation Loss: 0.7832849621772766\n",
      "Epoch 9705: Training Loss: 0.05144812042514483 Validation Loss: 0.783295214176178\n",
      "Epoch 9706: Training Loss: 0.05137446398536364 Validation Loss: 0.7823062539100647\n",
      "Epoch 9707: Training Loss: 0.05140435819824537 Validation Loss: 0.7812038064002991\n",
      "Epoch 9708: Training Loss: 0.05164794623851776 Validation Loss: 0.7802894115447998\n",
      "Epoch 9709: Training Loss: 0.051445734997590385 Validation Loss: 0.7819106578826904\n",
      "Epoch 9710: Training Loss: 0.0521227257947127 Validation Loss: 0.7874734997749329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9711: Training Loss: 0.05143774921695391 Validation Loss: 0.7853648066520691\n",
      "Epoch 9712: Training Loss: 0.05138947876791159 Validation Loss: 0.7832548022270203\n",
      "Epoch 9713: Training Loss: 0.051428026209274925 Validation Loss: 0.7801632881164551\n",
      "Epoch 9714: Training Loss: 0.05144695440928141 Validation Loss: 0.7813801765441895\n",
      "Epoch 9715: Training Loss: 0.051483084758122764 Validation Loss: 0.7829292416572571\n",
      "Epoch 9716: Training Loss: 0.051515584190686546 Validation Loss: 0.7808459997177124\n",
      "Epoch 9717: Training Loss: 0.05134927108883858 Validation Loss: 0.7824082970619202\n",
      "Epoch 9718: Training Loss: 0.05135813479622205 Validation Loss: 0.7834600806236267\n",
      "Epoch 9719: Training Loss: 0.05143788208564123 Validation Loss: 0.783959150314331\n",
      "Epoch 9720: Training Loss: 0.051465440541505814 Validation Loss: 0.7823780179023743\n",
      "Epoch 9721: Training Loss: 0.051501130064328514 Validation Loss: 0.7815254926681519\n",
      "Epoch 9722: Training Loss: 0.05142862722277641 Validation Loss: 0.7839332222938538\n",
      "Epoch 9723: Training Loss: 0.05135484350224336 Validation Loss: 0.784037172794342\n",
      "Epoch 9724: Training Loss: 0.051344492783149086 Validation Loss: 0.7840291261672974\n",
      "Epoch 9725: Training Loss: 0.05139827604095141 Validation Loss: 0.7848942875862122\n",
      "Epoch 9726: Training Loss: 0.05134827519456545 Validation Loss: 0.7844385504722595\n",
      "Epoch 9727: Training Loss: 0.05137656877438227 Validation Loss: 0.7831453084945679\n",
      "Epoch 9728: Training Loss: 0.05137722194194794 Validation Loss: 0.7806698679924011\n",
      "Epoch 9729: Training Loss: 0.05142270028591156 Validation Loss: 0.7804392576217651\n",
      "Epoch 9730: Training Loss: 0.05148914332191149 Validation Loss: 0.7829067707061768\n",
      "Epoch 9731: Training Loss: 0.051392236103614174 Validation Loss: 0.7845408320426941\n",
      "Epoch 9732: Training Loss: 0.05137117703755697 Validation Loss: 0.7847349047660828\n",
      "Epoch 9733: Training Loss: 0.051356213788191475 Validation Loss: 0.7842721343040466\n",
      "Epoch 9734: Training Loss: 0.05135174095630646 Validation Loss: 0.7811753749847412\n",
      "Epoch 9735: Training Loss: 0.051315064231554665 Validation Loss: 0.7806004881858826\n",
      "Epoch 9736: Training Loss: 0.05152226611971855 Validation Loss: 0.7826859951019287\n",
      "Epoch 9737: Training Loss: 0.051656365394592285 Validation Loss: 0.7801836729049683\n",
      "Epoch 9738: Training Loss: 0.051333244889974594 Validation Loss: 0.7811123728752136\n",
      "Epoch 9739: Training Loss: 0.05128880714376768 Validation Loss: 0.7844008207321167\n",
      "Epoch 9740: Training Loss: 0.051575493067502975 Validation Loss: 0.7884275913238525\n",
      "Epoch 9741: Training Loss: 0.05139956747492155 Validation Loss: 0.7855326533317566\n",
      "Epoch 9742: Training Loss: 0.05130850772062937 Validation Loss: 0.7840808033943176\n",
      "Epoch 9743: Training Loss: 0.051274905602137245 Validation Loss: 0.7818676233291626\n",
      "Epoch 9744: Training Loss: 0.05127033839623133 Validation Loss: 0.7820512056350708\n",
      "Epoch 9745: Training Loss: 0.05128224318226179 Validation Loss: 0.7804909944534302\n",
      "Epoch 9746: Training Loss: 0.051368970423936844 Validation Loss: 0.7800514101982117\n",
      "Epoch 9747: Training Loss: 0.05157473807533582 Validation Loss: 0.7850350737571716\n",
      "Epoch 9748: Training Loss: 0.05124548077583313 Validation Loss: 0.7848452925682068\n",
      "Epoch 9749: Training Loss: 0.051284871995449066 Validation Loss: 0.7853416204452515\n",
      "Epoch 9750: Training Loss: 0.05129622543851534 Validation Loss: 0.7828004956245422\n",
      "Epoch 9751: Training Loss: 0.0512325552602609 Validation Loss: 0.7823322415351868\n",
      "Epoch 9752: Training Loss: 0.051294419914484024 Validation Loss: 0.7827635407447815\n",
      "Epoch 9753: Training Loss: 0.05123830338319143 Validation Loss: 0.7824134230613708\n",
      "Epoch 9754: Training Loss: 0.051237188279628754 Validation Loss: 0.7825247645378113\n",
      "Epoch 9755: Training Loss: 0.051366726557413735 Validation Loss: 0.7853555679321289\n",
      "Epoch 9756: Training Loss: 0.051285081853469215 Validation Loss: 0.7855870723724365\n",
      "Epoch 9757: Training Loss: 0.0512394147614638 Validation Loss: 0.7836431264877319\n",
      "Epoch 9758: Training Loss: 0.05120690291126569 Validation Loss: 0.7833642363548279\n",
      "Epoch 9759: Training Loss: 0.05132529139518738 Validation Loss: 0.7808854579925537\n",
      "Epoch 9760: Training Loss: 0.051648507515589394 Validation Loss: 0.784740686416626\n",
      "Epoch 9761: Training Loss: 0.05154389018813769 Validation Loss: 0.7804725766181946\n",
      "Epoch 9762: Training Loss: 0.05123823508620262 Validation Loss: 0.7812721729278564\n",
      "Epoch 9763: Training Loss: 0.05120725060502688 Validation Loss: 0.7826339602470398\n",
      "Epoch 9764: Training Loss: 0.0513091670970122 Validation Loss: 0.7857570052146912\n",
      "Epoch 9765: Training Loss: 0.05117702235778173 Validation Loss: 0.7849050760269165\n",
      "Epoch 9766: Training Loss: 0.051444992423057556 Validation Loss: 0.7814645767211914\n",
      "Epoch 9767: Training Loss: 0.05130019411444664 Validation Loss: 0.7806597352027893\n",
      "Epoch 9768: Training Loss: 0.051218551894028984 Validation Loss: 0.7809244990348816\n",
      "Epoch 9769: Training Loss: 0.05138186986247698 Validation Loss: 0.785794734954834\n",
      "Epoch 9770: Training Loss: 0.05125435069203377 Validation Loss: 0.7852967381477356\n",
      "Epoch 9771: Training Loss: 0.051426113893588386 Validation Loss: 0.7829884886741638\n",
      "Epoch 9772: Training Loss: 0.05116829896966616 Validation Loss: 0.7854773998260498\n",
      "Epoch 9773: Training Loss: 0.05116857402026653 Validation Loss: 0.786081850528717\n",
      "Epoch 9774: Training Loss: 0.05145195126533508 Validation Loss: 0.7886433005332947\n",
      "Epoch 9775: Training Loss: 0.05123266081015269 Validation Loss: 0.7872534990310669\n",
      "Epoch 9776: Training Loss: 0.051284355421861015 Validation Loss: 0.7815742492675781\n",
      "Epoch 9777: Training Loss: 0.05121161291996638 Validation Loss: 0.778468668460846\n",
      "Epoch 9778: Training Loss: 0.05127744873364767 Validation Loss: 0.778369665145874\n",
      "Epoch 9779: Training Loss: 0.05141699065764745 Validation Loss: 0.7806239724159241\n",
      "Epoch 9780: Training Loss: 0.051102754970391594 Validation Loss: 0.7836369872093201\n",
      "Epoch 9781: Training Loss: 0.05122261866927147 Validation Loss: 0.787713885307312\n",
      "Epoch 9782: Training Loss: 0.051264276107152305 Validation Loss: 0.7889911532402039\n",
      "Epoch 9783: Training Loss: 0.051265726486841835 Validation Loss: 0.7873941659927368\n",
      "Epoch 9784: Training Loss: 0.051235231260458626 Validation Loss: 0.7836806178092957\n",
      "Epoch 9785: Training Loss: 0.05126611019174258 Validation Loss: 0.7803423404693604\n",
      "Epoch 9786: Training Loss: 0.05116157606244087 Validation Loss: 0.7804746627807617\n",
      "Epoch 9787: Training Loss: 0.051104736824830375 Validation Loss: 0.7816934585571289\n",
      "Epoch 9788: Training Loss: 0.051125304152568184 Validation Loss: 0.7837093472480774\n",
      "Epoch 9789: Training Loss: 0.0517145519455274 Validation Loss: 0.7809838056564331\n",
      "Epoch 9790: Training Loss: 0.05111324414610863 Validation Loss: 0.7857891917228699\n",
      "Epoch 9791: Training Loss: 0.05113441372911135 Validation Loss: 0.7865322828292847\n",
      "Epoch 9792: Training Loss: 0.05116774265964826 Validation Loss: 0.7862827181816101\n",
      "Epoch 9793: Training Loss: 0.05125699316461881 Validation Loss: 0.7891735434532166\n",
      "Epoch 9794: Training Loss: 0.05134144425392151 Validation Loss: 0.7896392345428467\n",
      "Epoch 9795: Training Loss: 0.051217870165904365 Validation Loss: 0.7836434841156006\n",
      "Epoch 9796: Training Loss: 0.05100919306278229 Validation Loss: 0.7808123230934143\n",
      "Epoch 9797: Training Loss: 0.051141053438186646 Validation Loss: 0.7815755009651184\n",
      "Epoch 9798: Training Loss: 0.05106967439254125 Validation Loss: 0.7806364297866821\n",
      "Epoch 9799: Training Loss: 0.05114355807503065 Validation Loss: 0.7793914079666138\n",
      "Epoch 9800: Training Loss: 0.051099477956692375 Validation Loss: 0.780817985534668\n",
      "Epoch 9801: Training Loss: 0.05118004853526751 Validation Loss: 0.7847091555595398\n",
      "Epoch 9802: Training Loss: 0.05125982935229937 Validation Loss: 0.7830149531364441\n",
      "Epoch 9803: Training Loss: 0.05118993173042933 Validation Loss: 0.7870358824729919\n",
      "Epoch 9804: Training Loss: 0.05137935901681582 Validation Loss: 0.7835960388183594\n",
      "Epoch 9805: Training Loss: 0.05118394270539284 Validation Loss: 0.7871339321136475\n",
      "Epoch 9806: Training Loss: 0.05104660615324974 Validation Loss: 0.7870902419090271\n",
      "Epoch 9807: Training Loss: 0.05107043807705244 Validation Loss: 0.7846800088882446\n",
      "Epoch 9808: Training Loss: 0.051023948937654495 Validation Loss: 0.7826846837997437\n",
      "Epoch 9809: Training Loss: 0.05110129341483116 Validation Loss: 0.7836044430732727\n",
      "Epoch 9810: Training Loss: 0.05105717976888021 Validation Loss: 0.7826901078224182\n",
      "Epoch 9811: Training Loss: 0.05100679273406664 Validation Loss: 0.7827704548835754\n",
      "Epoch 9812: Training Loss: 0.05106143653392792 Validation Loss: 0.7837367057800293\n",
      "Epoch 9813: Training Loss: 0.05103947098056475 Validation Loss: 0.7845211029052734\n",
      "Epoch 9814: Training Loss: 0.0513961153725783 Validation Loss: 0.785881519317627\n",
      "Epoch 9815: Training Loss: 0.05102774562935034 Validation Loss: 0.7843654751777649\n",
      "Epoch 9816: Training Loss: 0.05110345656673113 Validation Loss: 0.7856616377830505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9817: Training Loss: 0.051325382043917976 Validation Loss: 0.7818657755851746\n",
      "Epoch 9818: Training Loss: 0.05104765544335047 Validation Loss: 0.7811182737350464\n",
      "Epoch 9819: Training Loss: 0.051028323670228325 Validation Loss: 0.7841727137565613\n",
      "Epoch 9820: Training Loss: 0.05100066587328911 Validation Loss: 0.7859430313110352\n",
      "Epoch 9821: Training Loss: 0.05100497975945473 Validation Loss: 0.7865669131278992\n",
      "Epoch 9822: Training Loss: 0.05150904506444931 Validation Loss: 0.7814955115318298\n",
      "Epoch 9823: Training Loss: 0.05111895874142647 Validation Loss: 0.7832939028739929\n",
      "Epoch 9824: Training Loss: 0.0511485810081164 Validation Loss: 0.782336413860321\n",
      "Epoch 9825: Training Loss: 0.05096366504828135 Validation Loss: 0.7832857370376587\n",
      "Epoch 9826: Training Loss: 0.05104194954037666 Validation Loss: 0.7833330035209656\n",
      "Epoch 9827: Training Loss: 0.05135221406817436 Validation Loss: 0.7893552780151367\n",
      "Epoch 9828: Training Loss: 0.05109539503852526 Validation Loss: 0.7901492714881897\n",
      "Epoch 9829: Training Loss: 0.050975813219944634 Validation Loss: 0.7880649566650391\n",
      "Epoch 9830: Training Loss: 0.05101043855150541 Validation Loss: 0.7863863706588745\n",
      "Epoch 9831: Training Loss: 0.05106116955478986 Validation Loss: 0.7805510759353638\n",
      "Epoch 9832: Training Loss: 0.0513493667046229 Validation Loss: 0.7759445309638977\n",
      "Epoch 9833: Training Loss: 0.05110388249158859 Validation Loss: 0.7780938148498535\n",
      "Epoch 9834: Training Loss: 0.05115888516108195 Validation Loss: 0.7838701009750366\n",
      "Epoch 9835: Training Loss: 0.0511108860373497 Validation Loss: 0.7889856100082397\n",
      "Epoch 9836: Training Loss: 0.05100667476654053 Validation Loss: 0.7896814942359924\n",
      "Epoch 9837: Training Loss: 0.05117036774754524 Validation Loss: 0.7896811366081238\n",
      "Epoch 9838: Training Loss: 0.051005994280179344 Validation Loss: 0.7850424647331238\n",
      "Epoch 9839: Training Loss: 0.051048311094443 Validation Loss: 0.7798176407814026\n",
      "Epoch 9840: Training Loss: 0.05109163001179695 Validation Loss: 0.7786956429481506\n",
      "Epoch 9841: Training Loss: 0.05098207915822665 Validation Loss: 0.7811779379844666\n",
      "Epoch 9842: Training Loss: 0.051162246614694595 Validation Loss: 0.7804834842681885\n",
      "Epoch 9843: Training Loss: 0.051032433907190956 Validation Loss: 0.7855889201164246\n",
      "Epoch 9844: Training Loss: 0.05090026557445526 Validation Loss: 0.7874442338943481\n",
      "Epoch 9845: Training Loss: 0.05096492419640223 Validation Loss: 0.7859827280044556\n",
      "Epoch 9846: Training Loss: 0.05094988395770391 Validation Loss: 0.7856283783912659\n",
      "Epoch 9847: Training Loss: 0.050909873098134995 Validation Loss: 0.7852192521095276\n",
      "Epoch 9848: Training Loss: 0.05121416598558426 Validation Loss: 0.78246009349823\n",
      "Epoch 9849: Training Loss: 0.05093555276592573 Validation Loss: 0.7849790453910828\n",
      "Epoch 9850: Training Loss: 0.05089905112981796 Validation Loss: 0.7848660945892334\n",
      "Epoch 9851: Training Loss: 0.05090974271297455 Validation Loss: 0.7857688069343567\n",
      "Epoch 9852: Training Loss: 0.05097469314932823 Validation Loss: 0.7833840250968933\n",
      "Epoch 9853: Training Loss: 0.05088917538523674 Validation Loss: 0.7844610214233398\n",
      "Epoch 9854: Training Loss: 0.051142677664756775 Validation Loss: 0.7814294695854187\n",
      "Epoch 9855: Training Loss: 0.05088782558838526 Validation Loss: 0.7838134169578552\n",
      "Epoch 9856: Training Loss: 0.050970314691464104 Validation Loss: 0.7841853499412537\n",
      "Epoch 9857: Training Loss: 0.050946989407142006 Validation Loss: 0.784599781036377\n",
      "Epoch 9858: Training Loss: 0.05109605814019839 Validation Loss: 0.7867729663848877\n",
      "Epoch 9859: Training Loss: 0.050862427800893784 Validation Loss: 0.7872675657272339\n",
      "Epoch 9860: Training Loss: 0.05101859321196874 Validation Loss: 0.7839829921722412\n",
      "Epoch 9861: Training Loss: 0.05086566756169001 Validation Loss: 0.7834015488624573\n",
      "Epoch 9862: Training Loss: 0.050823211669921875 Validation Loss: 0.7835417985916138\n",
      "Epoch 9863: Training Loss: 0.05105598643422127 Validation Loss: 0.7847074866294861\n",
      "Epoch 9864: Training Loss: 0.05081998805205027 Validation Loss: 0.7843408584594727\n",
      "Epoch 9865: Training Loss: 0.05083217720190684 Validation Loss: 0.7852224111557007\n",
      "Epoch 9866: Training Loss: 0.050847943872213364 Validation Loss: 0.7860471606254578\n",
      "Epoch 9867: Training Loss: 0.05085373669862747 Validation Loss: 0.7855995893478394\n",
      "Epoch 9868: Training Loss: 0.05101650953292847 Validation Loss: 0.7819631695747375\n",
      "Epoch 9869: Training Loss: 0.0508397047718366 Validation Loss: 0.7821656465530396\n",
      "Epoch 9870: Training Loss: 0.05082917710145315 Validation Loss: 0.7819288969039917\n",
      "Epoch 9871: Training Loss: 0.05084321772058805 Validation Loss: 0.7834466099739075\n",
      "Epoch 9872: Training Loss: 0.05087252954641978 Validation Loss: 0.7848058938980103\n",
      "Epoch 9873: Training Loss: 0.050915167977412544 Validation Loss: 0.7877985239028931\n",
      "Epoch 9874: Training Loss: 0.0508334015806516 Validation Loss: 0.7869131565093994\n",
      "Epoch 9875: Training Loss: 0.05118389427661896 Validation Loss: 0.7831143140792847\n",
      "Epoch 9876: Training Loss: 0.05082635208964348 Validation Loss: 0.7848169207572937\n",
      "Epoch 9877: Training Loss: 0.050776481007536255 Validation Loss: 0.785620391368866\n",
      "Epoch 9878: Training Loss: 0.05093099425236384 Validation Loss: 0.7872107625007629\n",
      "Epoch 9879: Training Loss: 0.05091710761189461 Validation Loss: 0.7876114249229431\n",
      "Epoch 9880: Training Loss: 0.05084019899368286 Validation Loss: 0.7861716151237488\n",
      "Epoch 9881: Training Loss: 0.050922881811857224 Validation Loss: 0.7813931107521057\n",
      "Epoch 9882: Training Loss: 0.050792153303821884 Validation Loss: 0.779253363609314\n",
      "Epoch 9883: Training Loss: 0.051028924683729805 Validation Loss: 0.7823257446289062\n",
      "Epoch 9884: Training Loss: 0.05074769879380862 Validation Loss: 0.782561182975769\n",
      "Epoch 9885: Training Loss: 0.050816419223944344 Validation Loss: 0.7824557423591614\n",
      "Epoch 9886: Training Loss: 0.05075180158019066 Validation Loss: 0.7844455242156982\n",
      "Epoch 9887: Training Loss: 0.05073787271976471 Validation Loss: 0.7849555015563965\n",
      "Epoch 9888: Training Loss: 0.05085087319215139 Validation Loss: 0.7881594300270081\n",
      "Epoch 9889: Training Loss: 0.05084743723273277 Validation Loss: 0.7889853715896606\n",
      "Epoch 9890: Training Loss: 0.05094616239269575 Validation Loss: 0.7843089699745178\n",
      "Epoch 9891: Training Loss: 0.05080236370364825 Validation Loss: 0.785291314125061\n",
      "Epoch 9892: Training Loss: 0.050752975046634674 Validation Loss: 0.7849637866020203\n",
      "Epoch 9893: Training Loss: 0.050770255426565804 Validation Loss: 0.7829391360282898\n",
      "Epoch 9894: Training Loss: 0.05092407390475273 Validation Loss: 0.7846365571022034\n",
      "Epoch 9895: Training Loss: 0.05131624018152555 Validation Loss: 0.7794010639190674\n",
      "Epoch 9896: Training Loss: 0.05085221926371256 Validation Loss: 0.782565176486969\n",
      "Epoch 9897: Training Loss: 0.05083433041969935 Validation Loss: 0.7846119999885559\n",
      "Epoch 9898: Training Loss: 0.050953938315312065 Validation Loss: 0.7880541086196899\n",
      "Epoch 9899: Training Loss: 0.051067094008127846 Validation Loss: 0.7893788814544678\n",
      "Epoch 9900: Training Loss: 0.0506784375756979 Validation Loss: 0.786788284778595\n",
      "Epoch 9901: Training Loss: 0.05089634284377098 Validation Loss: 0.7812923789024353\n",
      "Epoch 9902: Training Loss: 0.05075855180621147 Validation Loss: 0.779992938041687\n",
      "Epoch 9903: Training Loss: 0.05099483268956343 Validation Loss: 0.7834926843643188\n",
      "Epoch 9904: Training Loss: 0.05093128730853399 Validation Loss: 0.7827078700065613\n",
      "Epoch 9905: Training Loss: 0.05076794574658076 Validation Loss: 0.7862736582756042\n",
      "Epoch 9906: Training Loss: 0.05076896771788597 Validation Loss: 0.7859293818473816\n",
      "Epoch 9907: Training Loss: 0.050708357244729996 Validation Loss: 0.7878507971763611\n",
      "Epoch 9908: Training Loss: 0.050734483947356544 Validation Loss: 0.7878776788711548\n",
      "Epoch 9909: Training Loss: 0.05071002741654714 Validation Loss: 0.7845713496208191\n",
      "Epoch 9910: Training Loss: 0.05073473478356997 Validation Loss: 0.7827774286270142\n",
      "Epoch 9911: Training Loss: 0.05064249038696289 Validation Loss: 0.7826163172721863\n",
      "Epoch 9912: Training Loss: 0.05126128469904264 Validation Loss: 0.7786889672279358\n",
      "Epoch 9913: Training Loss: 0.05067787195245425 Validation Loss: 0.7824738621711731\n",
      "Epoch 9914: Training Loss: 0.05077961087226868 Validation Loss: 0.787714958190918\n",
      "Epoch 9915: Training Loss: 0.051510680466890335 Validation Loss: 0.7936918139457703\n",
      "Epoch 9916: Training Loss: 0.05086752399802208 Validation Loss: 0.7895135283470154\n",
      "Epoch 9917: Training Loss: 0.05108258624871572 Validation Loss: 0.7819276452064514\n",
      "Epoch 9918: Training Loss: 0.05063332368930181 Validation Loss: 0.7812730073928833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9919: Training Loss: 0.05083914225300153 Validation Loss: 0.7839931845664978\n",
      "Epoch 9920: Training Loss: 0.05064661428332329 Validation Loss: 0.7827274799346924\n",
      "Epoch 9921: Training Loss: 0.05073627457022667 Validation Loss: 0.7839576601982117\n",
      "Epoch 9922: Training Loss: 0.050690912952025734 Validation Loss: 0.785149872303009\n",
      "Epoch 9923: Training Loss: 0.05071237434943517 Validation Loss: 0.7873651385307312\n",
      "Epoch 9924: Training Loss: 0.05069355418284734 Validation Loss: 0.7852203249931335\n",
      "Epoch 9925: Training Loss: 0.05087004601955414 Validation Loss: 0.7826641201972961\n",
      "Epoch 9926: Training Loss: 0.050653244058291115 Validation Loss: 0.7823081612586975\n",
      "Epoch 9927: Training Loss: 0.05063389986753464 Validation Loss: 0.7839925289154053\n",
      "Epoch 9928: Training Loss: 0.050984833389520645 Validation Loss: 0.7895542979240417\n",
      "Epoch 9929: Training Loss: 0.0506106565395991 Validation Loss: 0.7895016670227051\n",
      "Epoch 9930: Training Loss: 0.05058359851439794 Validation Loss: 0.7874559760093689\n",
      "Epoch 9931: Training Loss: 0.05063916804889838 Validation Loss: 0.7867782711982727\n",
      "Epoch 9932: Training Loss: 0.05067980537811915 Validation Loss: 0.7829119563102722\n",
      "Epoch 9933: Training Loss: 0.05065024271607399 Validation Loss: 0.7833183407783508\n",
      "Epoch 9934: Training Loss: 0.05081532274683317 Validation Loss: 0.7805847525596619\n",
      "Epoch 9935: Training Loss: 0.05066215991973877 Validation Loss: 0.7815346121788025\n",
      "Epoch 9936: Training Loss: 0.05096588283777237 Validation Loss: 0.788206934928894\n",
      "Epoch 9937: Training Loss: 0.050600441793600716 Validation Loss: 0.7877199649810791\n",
      "Epoch 9938: Training Loss: 0.05068170030911764 Validation Loss: 0.7857528924942017\n",
      "Epoch 9939: Training Loss: 0.05069849640130997 Validation Loss: 0.788275420665741\n",
      "Epoch 9940: Training Loss: 0.050564209620157875 Validation Loss: 0.7861897349357605\n",
      "Epoch 9941: Training Loss: 0.05064173539479574 Validation Loss: 0.783261775970459\n",
      "Epoch 9942: Training Loss: 0.050725662459929786 Validation Loss: 0.7812804579734802\n",
      "Epoch 9943: Training Loss: 0.050970230251550674 Validation Loss: 0.7861219048500061\n",
      "Epoch 9944: Training Loss: 0.0507664829492569 Validation Loss: 0.7837832570075989\n",
      "Epoch 9945: Training Loss: 0.05069915826121966 Validation Loss: 0.7869927883148193\n",
      "Epoch 9946: Training Loss: 0.050511643290519714 Validation Loss: 0.7867920398712158\n",
      "Epoch 9947: Training Loss: 0.05054306363066038 Validation Loss: 0.7857571840286255\n",
      "Epoch 9948: Training Loss: 0.05051792412996292 Validation Loss: 0.7856424450874329\n",
      "Epoch 9949: Training Loss: 0.05092365170518557 Validation Loss: 0.7820057272911072\n",
      "Epoch 9950: Training Loss: 0.05065819248557091 Validation Loss: 0.7820517420768738\n",
      "Epoch 9951: Training Loss: 0.050577194740374885 Validation Loss: 0.7840693593025208\n",
      "Epoch 9952: Training Loss: 0.0504915714263916 Validation Loss: 0.7854664325714111\n",
      "Epoch 9953: Training Loss: 0.05049460381269455 Validation Loss: 0.7885359525680542\n",
      "Epoch 9954: Training Loss: 0.05085953325033188 Validation Loss: 0.7912144064903259\n",
      "Epoch 9955: Training Loss: 0.05064915493130684 Validation Loss: 0.7880159020423889\n",
      "Epoch 9956: Training Loss: 0.05064668506383896 Validation Loss: 0.7835351228713989\n",
      "Epoch 9957: Training Loss: 0.05052179346481959 Validation Loss: 0.7840721607208252\n",
      "Epoch 9958: Training Loss: 0.050579022616147995 Validation Loss: 0.7848259806632996\n",
      "Epoch 9959: Training Loss: 0.05058001851042112 Validation Loss: 0.7859628796577454\n",
      "Epoch 9960: Training Loss: 0.05059458687901497 Validation Loss: 0.7827991247177124\n",
      "Epoch 9961: Training Loss: 0.050652275482813515 Validation Loss: 0.7853374481201172\n",
      "Epoch 9962: Training Loss: 0.050521845618883766 Validation Loss: 0.7855761051177979\n",
      "Epoch 9963: Training Loss: 0.0506047730644544 Validation Loss: 0.7828629016876221\n",
      "Epoch 9964: Training Loss: 0.05056144545475642 Validation Loss: 0.7832960486412048\n",
      "Epoch 9965: Training Loss: 0.05046714593966802 Validation Loss: 0.784445583820343\n",
      "Epoch 9966: Training Loss: 0.05049229537447294 Validation Loss: 0.7857351303100586\n",
      "Epoch 9967: Training Loss: 0.050886928414305053 Validation Loss: 0.7909007668495178\n",
      "Epoch 9968: Training Loss: 0.05066441372036934 Validation Loss: 0.7914829254150391\n",
      "Epoch 9969: Training Loss: 0.05055488273501396 Validation Loss: 0.7885797023773193\n",
      "Epoch 9970: Training Loss: 0.0506354421377182 Validation Loss: 0.7827542424201965\n",
      "Epoch 9971: Training Loss: 0.05059000477194786 Validation Loss: 0.7843742370605469\n",
      "Epoch 9972: Training Loss: 0.0505362277229627 Validation Loss: 0.781399130821228\n",
      "Epoch 9973: Training Loss: 0.05067609747250875 Validation Loss: 0.780207633972168\n",
      "Epoch 9974: Training Loss: 0.05049711465835571 Validation Loss: 0.7828870415687561\n",
      "Epoch 9975: Training Loss: 0.050553993632396065 Validation Loss: 0.7861721515655518\n",
      "Epoch 9976: Training Loss: 0.05054457361499468 Validation Loss: 0.7871046662330627\n",
      "Epoch 9977: Training Loss: 0.05058254053195318 Validation Loss: 0.7898536324501038\n",
      "Epoch 9978: Training Loss: 0.05053281535704931 Validation Loss: 0.7901009321212769\n",
      "Epoch 9979: Training Loss: 0.050419061134258904 Validation Loss: 0.7864436507225037\n",
      "Epoch 9980: Training Loss: 0.050495944917201996 Validation Loss: 0.7839171290397644\n",
      "Epoch 9981: Training Loss: 0.050451101114352546 Validation Loss: 0.7822031378746033\n",
      "Epoch 9982: Training Loss: 0.05048421770334244 Validation Loss: 0.7822290658950806\n",
      "Epoch 9983: Training Loss: 0.050430528819561005 Validation Loss: 0.7827264070510864\n",
      "Epoch 9984: Training Loss: 0.050426218658685684 Validation Loss: 0.7851218581199646\n",
      "Epoch 9985: Training Loss: 0.05037076771259308 Validation Loss: 0.7864800095558167\n",
      "Epoch 9986: Training Loss: 0.05059391881028811 Validation Loss: 0.7846589684486389\n",
      "Epoch 9987: Training Loss: 0.05035355190436045 Validation Loss: 0.7861400246620178\n",
      "Epoch 9988: Training Loss: 0.05037127062678337 Validation Loss: 0.786872386932373\n",
      "Epoch 9989: Training Loss: 0.05060852939883868 Validation Loss: 0.7905085682868958\n",
      "Epoch 9990: Training Loss: 0.05053334683179855 Validation Loss: 0.7870867848396301\n",
      "Epoch 9991: Training Loss: 0.05075353259841601 Validation Loss: 0.7837548851966858\n",
      "Epoch 9992: Training Loss: 0.050453985730806984 Validation Loss: 0.7828298211097717\n",
      "Epoch 9993: Training Loss: 0.05050231019655863 Validation Loss: 0.7830753922462463\n",
      "Epoch 9994: Training Loss: 0.050394160052140556 Validation Loss: 0.7841566205024719\n",
      "Epoch 9995: Training Loss: 0.05038635805249214 Validation Loss: 0.7860374450683594\n",
      "Epoch 9996: Training Loss: 0.050400430957476296 Validation Loss: 0.786910355091095\n",
      "Epoch 9997: Training Loss: 0.050648907820383705 Validation Loss: 0.791885495185852\n",
      "Epoch 9998: Training Loss: 0.05053673560420672 Validation Loss: 0.7915794253349304\n",
      "Epoch 9999: Training Loss: 0.05101213604211807 Validation Loss: 0.7841296792030334\n",
      "Epoch 10000: Training Loss: 0.050420998285214104 Validation Loss: 0.7826480865478516\n"
     ]
    }
   ],
   "source": [
    "fit(Xtra, ytra, Xtes, ytes, net, optimizer, criterion, n_epochs, n_batches, device, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data\\\\model_checkpoint.pt'\n",
    "device = torch.device('cpu')\n",
    "net = Net1()\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7188012003898621\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    inputs = torch.FloatTensor(Xtes)\n",
    "    labels = torch.tensor(ytes, dtype=torch.long)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net.forward(inputs)\n",
    "    loss = error(outputs, labels) \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "for v,p in enumerate(net.parameters()):\n",
    "    param_dict[v] = p.data.numpy()\n",
    "    \n",
    "W0 = param_dict[0].T\n",
    "b0 = param_dict[1]\n",
    "W1 = param_dict[2].T\n",
    "b1 = param_dict[3]\n",
    "\n",
    "\n",
    "h0 = np.matmul(Xtes, W0) + b0\n",
    "h1 = np.tanh(h0)\n",
    "h2 = np.matmul(h1, W1) + b1\n",
    "h3 = np.exp(h2)\n",
    "o = h3/np.sum(h3,axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(prob, topics):    \n",
    "    return topics[np.argmax(prob)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "topics = ['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
    "for c in o:\n",
    "    pred_labels.append(y_hat(c, topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = data.Pregunta[btes]\n",
    "labels = data.Tema[btes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtes = np.column_stack((preguntas, labels, pred_labels, Xtes, o))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(Xtes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load('Data\\\\vocab.npy', allow_pickle=True)\n",
    "colnames = np.concatenate((['pregunta', 'label', 'pred_label'], vocab, ['n_token', 'perc_greet', 'polarity'], ['Jubilacion Patronal', 'Consultoria', \\\n",
    "                                                                                                     'Renuncia/Despido/Desahucio', 'IESS', 'Greeting', \\\n",
    "                                                                                                         'Contacto', 'No Topic', 'Queja', 'Otros servicios', \\\n",
    "                                                                                                 'Charlas/Capacitaciones', 'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']))\n",
    "data2.columns = colnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregunta</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>hol</th>\n",
       "      <th>una</th>\n",
       "      <th>pregunt</th>\n",
       "      <th>en</th>\n",
       "      <th>que</th>\n",
       "      <th>perjudic</th>\n",
       "      <th>el</th>\n",
       "      <th>...</th>\n",
       "      <th>IESS</th>\n",
       "      <th>Greeting</th>\n",
       "      <th>Contacto</th>\n",
       "      <th>No Topic</th>\n",
       "      <th>Queja</th>\n",
       "      <th>Otros servicios</th>\n",
       "      <th>Charlas/Capacitaciones</th>\n",
       "      <th>Hi Five</th>\n",
       "      <th>job seeker</th>\n",
       "      <th>Facturacion/Retencion/Cobros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Necesito informacion sobre su producto ACTUA R2O</td>\n",
       "      <td>Consultoria</td>\n",
       "      <td>Consultoria</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00122106</td>\n",
       "      <td>0.00256981</td>\n",
       "      <td>0.00148832</td>\n",
       "      <td>0.0257649</td>\n",
       "      <td>0.00512395</td>\n",
       "      <td>0.138984</td>\n",
       "      <td>0.0159844</td>\n",
       "      <td>0.00304579</td>\n",
       "      <td>0.00412263</td>\n",
       "      <td>0.00121621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buenos dias tengo dudas en cuanto a los servic...</td>\n",
       "      <td>Contacto</td>\n",
       "      <td>Contacto</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0292173</td>\n",
       "      <td>0.00352382</td>\n",
       "      <td>0.503212</td>\n",
       "      <td>0.00817231</td>\n",
       "      <td>0.100545</td>\n",
       "      <td>0.0475307</td>\n",
       "      <td>0.0319105</td>\n",
       "      <td>0.0536152</td>\n",
       "      <td>0.048752</td>\n",
       "      <td>0.0778633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Liquidacin por desahucio por 7 aos</td>\n",
       "      <td>Renuncia/Despido/Desahucio</td>\n",
       "      <td>Renuncia/Despido/Desahucio</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00118378</td>\n",
       "      <td>0.000490635</td>\n",
       "      <td>5.81032e-05</td>\n",
       "      <td>0.000960213</td>\n",
       "      <td>0.000626847</td>\n",
       "      <td>0.000639163</td>\n",
       "      <td>0.00037006</td>\n",
       "      <td>0.00155689</td>\n",
       "      <td>0.000649705</td>\n",
       "      <td>0.000110804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  484 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pregunta  \\\n",
       "0   Necesito informacion sobre su producto ACTUA R2O   \n",
       "1  Buenos dias tengo dudas en cuanto a los servic...   \n",
       "2               Liquidacin por desahucio por 7 aos   \n",
       "\n",
       "                        label                  pred_label hol una pregunt en  \\\n",
       "0                 Consultoria                 Consultoria   0   0       0  0   \n",
       "1                    Contacto                    Contacto   0   0       0  1   \n",
       "2  Renuncia/Despido/Desahucio  Renuncia/Despido/Desahucio   0   0       0  0   \n",
       "\n",
       "  que perjudic el  ...        IESS     Greeting     Contacto     No Topic  \\\n",
       "0   0        0  0  ...  0.00122106   0.00256981   0.00148832    0.0257649   \n",
       "1   1        0  0  ...   0.0292173   0.00352382     0.503212   0.00817231   \n",
       "2   0        0  0  ...  0.00118378  0.000490635  5.81032e-05  0.000960213   \n",
       "\n",
       "         Queja Otros servicios Charlas/Capacitaciones     Hi Five  \\\n",
       "0   0.00512395        0.138984              0.0159844  0.00304579   \n",
       "1     0.100545       0.0475307              0.0319105   0.0536152   \n",
       "2  0.000626847     0.000639163             0.00037006  0.00155689   \n",
       "\n",
       "    job seeker Facturacion/Retencion/Cobros  \n",
       "0   0.00412263                   0.00121621  \n",
       "1     0.048752                    0.0778633  \n",
       "2  0.000649705                  0.000110804  \n",
       "\n",
       "[3 rows x 484 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('Data\\\\Xtes.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
