{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data\\\\preguntas.csv\", sep=',', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('Data\\\\X.npy', allow_pickle=True)\n",
    "y = np.load('Data\\\\y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_oh = np.zeros((y.shape[0], max(y)+1))\n",
    "#y_oh[np.arange(y.shape[0]), y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "idx = np.arange(X.shape[0])\n",
    "random.shuffle(idx)  \n",
    "btra = np.random.choice(idx, int(0.8*X.shape[0]), replace=False)\n",
    "btes = [i for i in idx if i not in btra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtra = X[btra]\n",
    "ytra = y[btra]\n",
    "\n",
    "Xtes = X[btes]\n",
    "ytes = y[btes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 574)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, n_batches):  \n",
    "    \n",
    "    random.seed(123)\n",
    "    \n",
    "    batch_size = X.shape[0] // n_batches\n",
    "    \n",
    "    idx = np.arange(X.shape[0])\n",
    "    random.shuffle(idx)    \n",
    "    idx = idx[:n_batches*batch_size]\n",
    "        \n",
    "    for i in range(n_batches):            \n",
    "        bi = np.random.choice(idx, batch_size, replace=False)\n",
    "        X_batch = X[bi]\n",
    "        Y_batch = Y[bi]\n",
    "        idx = [i for i in idx if i not in bi]\n",
    "        yield (X_batch,Y_batch)       \n",
    "        \n",
    "        \n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc11 = nn.Linear(574, 24, bias=True)\n",
    "        self.fc12 = nn.Linear(24, 13, bias=True) \n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = torch.tanh(self.fc11(x))\n",
    "        x1 = self.fc12(x1)     \n",
    "        return x1\n",
    "    \n",
    "    \n",
    "def fit(X, Y, Xt, Yt, net, optimizer, error, n_epochs, n_batches, device, PATH, min_val_loss = float('inf')):\n",
    "    \n",
    "    net = net.to(device)    \n",
    "    losses = []    \n",
    "    val_losses = []\n",
    "\n",
    "    val_inputs = torch.FloatTensor(Xt)\n",
    "    val_labels = torch.tensor(Yt, dtype=torch.long)\n",
    "    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)  \n",
    "    \n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    \n",
    "        running_loss = 0  \n",
    "         \n",
    "        \n",
    "        for batch_x, batch_y in batch_generator(X, Y, n_batches):  \n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # get the inputs\n",
    "            inputs = torch.FloatTensor(batch_x)\n",
    "            labels = torch.tensor(batch_y, dtype=torch.long)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)             \n",
    "                \n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = error(outputs, labels)\n",
    "                        \n",
    "            loss.backward()    #obtain gradients      \n",
    "            optimizer.step()   #optimize\n",
    "                \n",
    "            running_loss += loss.item()      \n",
    "                \n",
    "        running_loss = running_loss/n_batches    \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_outputs = net.forward(val_inputs)\n",
    "            val_loss = error(val_outputs, val_labels) \n",
    "        \n",
    "        losses.append(running_loss)   \n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        \n",
    "        print('Epoch {0}: Training Loss: {1} Validation Loss: {2}'.format(epoch+1, running_loss, val_loss.item()))\n",
    "        \n",
    "        if val_loss.item() < min_val_loss:\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            print('New Checkpoint Saved into PATH')\n",
    "            min_val_loss = val_loss.item()\n",
    "        \n",
    "        \n",
    "def fweights_init_normal(m):     \n",
    "    classname = m.__class__.__name__\n",
    "    torch.manual_seed(0)\n",
    "    if classname.find('Linear') != -1:\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n-1)\n",
    "        #y = 0.0001\n",
    "        m.weight.data.normal_(0, y)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.normal_(0, y)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15000\n",
    "lr = 0.001\n",
    "n_batches = 3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH = 'Data\\\\model_checkpoint.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.sum(np.unique(ytes, return_counts=True)[1])/np.unique(ytes, return_counts=True)[1]\n",
    "class_weights = torch.FloatTensor(weights).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net1()\n",
    "net.apply(fweights_init_normal)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 2.7103567123413086 Validation Loss: 2.689720630645752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2: Training Loss: 2.6966920693715415 Validation Loss: 2.675969362258911\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3: Training Loss: 2.6760499477386475 Validation Loss: 2.6599626541137695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 4: Training Loss: 2.6515068213144937 Validation Loss: 2.645219326019287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 5: Training Loss: 2.630077918370565 Validation Loss: 2.632276773452759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 6: Training Loss: 2.6087330182393393 Validation Loss: 2.6210880279541016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 7: Training Loss: 2.590367158253988 Validation Loss: 2.6111342906951904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 8: Training Loss: 2.5740405718485513 Validation Loss: 2.602630138397217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 9: Training Loss: 2.5589752197265625 Validation Loss: 2.595641613006592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 10: Training Loss: 2.5451839764912925 Validation Loss: 2.5903446674346924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 11: Training Loss: 2.528306484222412 Validation Loss: 2.5873935222625732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 12: Training Loss: 2.516905148824056 Validation Loss: 2.5843558311462402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 13: Training Loss: 2.5087278683980307 Validation Loss: 2.581451892852783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 14: Training Loss: 2.49971596399943 Validation Loss: 2.5783467292785645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 15: Training Loss: 2.496626138687134 Validation Loss: 2.5758888721466064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 16: Training Loss: 2.4937245845794678 Validation Loss: 2.574035167694092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 17: Training Loss: 2.4908992449442544 Validation Loss: 2.572486639022827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 18: Training Loss: 2.486257791519165 Validation Loss: 2.5711770057678223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 19: Training Loss: 2.486201763153076 Validation Loss: 2.5701029300689697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 20: Training Loss: 2.483853896458944 Validation Loss: 2.5691821575164795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 21: Training Loss: 2.480834166208903 Validation Loss: 2.568288564682007\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 22: Training Loss: 2.4805113474527993 Validation Loss: 2.567471504211426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 23: Training Loss: 2.476912021636963 Validation Loss: 2.566474437713623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 24: Training Loss: 2.474246342976888 Validation Loss: 2.5656869411468506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 25: Training Loss: 2.474083423614502 Validation Loss: 2.5649824142456055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 26: Training Loss: 2.471003452936808 Validation Loss: 2.5640311241149902\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 27: Training Loss: 2.4687346617380777 Validation Loss: 2.5630078315734863\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 28: Training Loss: 2.467698415120443 Validation Loss: 2.562026262283325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 29: Training Loss: 2.465837081273397 Validation Loss: 2.5609917640686035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 30: Training Loss: 2.4639950593312583 Validation Loss: 2.559847116470337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 31: Training Loss: 2.4626503785451255 Validation Loss: 2.558969020843506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 32: Training Loss: 2.4605021476745605 Validation Loss: 2.558056592941284\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 33: Training Loss: 2.4586735566457114 Validation Loss: 2.5568747520446777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 34: Training Loss: 2.4561166763305664 Validation Loss: 2.5558252334594727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 35: Training Loss: 2.455317497253418 Validation Loss: 2.554919958114624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 36: Training Loss: 2.4535820484161377 Validation Loss: 2.553722381591797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 37: Training Loss: 2.451934734980265 Validation Loss: 2.552626609802246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 38: Training Loss: 2.44917098681132 Validation Loss: 2.5515196323394775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 39: Training Loss: 2.4487218856811523 Validation Loss: 2.5505588054656982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 40: Training Loss: 2.445991595586141 Validation Loss: 2.5493969917297363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 41: Training Loss: 2.4463295936584473 Validation Loss: 2.5481209754943848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 42: Training Loss: 2.443960110346476 Validation Loss: 2.547020673751831\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 43: Training Loss: 2.443357308705648 Validation Loss: 2.5461585521698\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 44: Training Loss: 2.4399542808532715 Validation Loss: 2.545069932937622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 45: Training Loss: 2.4394057591756186 Validation Loss: 2.544027328491211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 46: Training Loss: 2.4341184298197427 Validation Loss: 2.543102979660034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 47: Training Loss: 2.4360289573669434 Validation Loss: 2.542348861694336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 48: Training Loss: 2.4345274766286216 Validation Loss: 2.541321277618408\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 49: Training Loss: 2.432793378829956 Validation Loss: 2.5404319763183594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 50: Training Loss: 2.430250406265259 Validation Loss: 2.5392801761627197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 51: Training Loss: 2.429276625315348 Validation Loss: 2.5381383895874023\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 52: Training Loss: 2.42940624554952 Validation Loss: 2.5370616912841797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 53: Training Loss: 2.4262100060780845 Validation Loss: 2.5358850955963135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 54: Training Loss: 2.425715923309326 Validation Loss: 2.53486967086792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 55: Training Loss: 2.4231186707814536 Validation Loss: 2.533862352371216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 56: Training Loss: 2.4208972454071045 Validation Loss: 2.5328104496002197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 57: Training Loss: 2.4196647802988687 Validation Loss: 2.5315935611724854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 58: Training Loss: 2.4182302157084146 Validation Loss: 2.5305652618408203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 59: Training Loss: 2.4164876143137612 Validation Loss: 2.5295422077178955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 60: Training Loss: 2.4146827856699624 Validation Loss: 2.5286448001861572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 61: Training Loss: 2.4131972789764404 Validation Loss: 2.527493715286255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 62: Training Loss: 2.4115871588389077 Validation Loss: 2.5263144969940186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 63: Training Loss: 2.4109973907470703 Validation Loss: 2.5253257751464844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 64: Training Loss: 2.408161163330078 Validation Loss: 2.5240318775177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 65: Training Loss: 2.4065306186676025 Validation Loss: 2.522709608078003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 66: Training Loss: 2.4044906298319497 Validation Loss: 2.5214850902557373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 67: Training Loss: 2.402482350667318 Validation Loss: 2.5203938484191895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 68: Training Loss: 2.40152907371521 Validation Loss: 2.5192699432373047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 69: Training Loss: 2.400368611017863 Validation Loss: 2.5182533264160156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 70: Training Loss: 2.3979345162709556 Validation Loss: 2.516909599304199\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 71: Training Loss: 2.396839698155721 Validation Loss: 2.515803337097168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 72: Training Loss: 2.3940526644388833 Validation Loss: 2.5146708488464355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 73: Training Loss: 2.3936429023742676 Validation Loss: 2.513578176498413\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 74: Training Loss: 2.3916529019673667 Validation Loss: 2.5123112201690674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 75: Training Loss: 2.389488617579142 Validation Loss: 2.5112905502319336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 76: Training Loss: 2.3877271016438804 Validation Loss: 2.510322093963623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 77: Training Loss: 2.386305809020996 Validation Loss: 2.5092337131500244\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 78: Training Loss: 2.383263031641642 Validation Loss: 2.5082592964172363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 79: Training Loss: 2.382312615712484 Validation Loss: 2.5068609714508057\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80: Training Loss: 2.3811514377593994 Validation Loss: 2.505686044692993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 81: Training Loss: 2.3798999786376953 Validation Loss: 2.5042641162872314\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 82: Training Loss: 2.3767637411753335 Validation Loss: 2.5028576850891113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 83: Training Loss: 2.3737751642862954 Validation Loss: 2.5014195442199707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 84: Training Loss: 2.3741438388824463 Validation Loss: 2.4998881816864014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 85: Training Loss: 2.371887763341268 Validation Loss: 2.4983673095703125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 86: Training Loss: 2.37114151318868 Validation Loss: 2.4969944953918457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 87: Training Loss: 2.369213660558065 Validation Loss: 2.4957754611968994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 88: Training Loss: 2.368229071299235 Validation Loss: 2.494401216506958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 89: Training Loss: 2.366142988204956 Validation Loss: 2.493030071258545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 90: Training Loss: 2.3647305170694985 Validation Loss: 2.491515874862671\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 91: Training Loss: 2.3628714879353843 Validation Loss: 2.490126371383667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 92: Training Loss: 2.3624844551086426 Validation Loss: 2.4888875484466553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 93: Training Loss: 2.359989563624064 Validation Loss: 2.4875926971435547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 94: Training Loss: 2.3582948048909507 Validation Loss: 2.486182928085327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 95: Training Loss: 2.3563204606374106 Validation Loss: 2.4846115112304688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 96: Training Loss: 2.354712645212809 Validation Loss: 2.48319149017334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 97: Training Loss: 2.3537946542104087 Validation Loss: 2.481717586517334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 98: Training Loss: 2.3511903285980225 Validation Loss: 2.4805612564086914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 99: Training Loss: 2.3508822123209634 Validation Loss: 2.478950262069702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 100: Training Loss: 2.3472727139790854 Validation Loss: 2.4776670932769775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 101: Training Loss: 2.3480250040690103 Validation Loss: 2.4763834476470947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 102: Training Loss: 2.345010677973429 Validation Loss: 2.474921226501465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 103: Training Loss: 2.3431828816731772 Validation Loss: 2.4734911918640137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 104: Training Loss: 2.3423918088277182 Validation Loss: 2.472270965576172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 105: Training Loss: 2.3389111359914145 Validation Loss: 2.4709582328796387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 106: Training Loss: 2.3399763902028403 Validation Loss: 2.469411849975586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 107: Training Loss: 2.3389833768208823 Validation Loss: 2.4680659770965576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 108: Training Loss: 2.3366129398345947 Validation Loss: 2.466519355773926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 109: Training Loss: 2.3341277440389 Validation Loss: 2.465596914291382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 110: Training Loss: 2.3326613903045654 Validation Loss: 2.4641404151916504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 111: Training Loss: 2.3323749701182046 Validation Loss: 2.4629294872283936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 112: Training Loss: 2.3289384841918945 Validation Loss: 2.4617111682891846\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 113: Training Loss: 2.3281871477762857 Validation Loss: 2.4604711532592773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 114: Training Loss: 2.326930046081543 Validation Loss: 2.459078550338745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 115: Training Loss: 2.3248535792032876 Validation Loss: 2.457754611968994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 116: Training Loss: 2.323172092437744 Validation Loss: 2.456521511077881\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 117: Training Loss: 2.3207345803578696 Validation Loss: 2.4550745487213135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 118: Training Loss: 2.320383310317993 Validation Loss: 2.4536516666412354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 119: Training Loss: 2.3187385400136313 Validation Loss: 2.4522507190704346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 120: Training Loss: 2.316626707712809 Validation Loss: 2.4511499404907227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 121: Training Loss: 2.3145790100097656 Validation Loss: 2.4499824047088623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 122: Training Loss: 2.3152880668640137 Validation Loss: 2.4486243724823\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 123: Training Loss: 2.3120338916778564 Validation Loss: 2.447308301925659\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 124: Training Loss: 2.3107927640279136 Validation Loss: 2.4461777210235596\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 125: Training Loss: 2.308645327885946 Validation Loss: 2.4446656703948975\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 126: Training Loss: 2.3067121505737305 Validation Loss: 2.4433085918426514\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 127: Training Loss: 2.30512801806132 Validation Loss: 2.441971778869629\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 128: Training Loss: 2.303577105204264 Validation Loss: 2.44047212600708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 129: Training Loss: 2.302464723587036 Validation Loss: 2.438957691192627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 130: Training Loss: 2.3007119496663413 Validation Loss: 2.437636613845825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 131: Training Loss: 2.299186627070109 Validation Loss: 2.4363017082214355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 132: Training Loss: 2.297554890314738 Validation Loss: 2.435084819793701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 133: Training Loss: 2.295383850733439 Validation Loss: 2.433969259262085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 134: Training Loss: 2.295835574467977 Validation Loss: 2.432927370071411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 135: Training Loss: 2.2930612564086914 Validation Loss: 2.431353807449341\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 136: Training Loss: 2.2917233308156333 Validation Loss: 2.429650068283081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 137: Training Loss: 2.2885055541992188 Validation Loss: 2.428267478942871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 138: Training Loss: 2.2875810464223227 Validation Loss: 2.4270029067993164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 139: Training Loss: 2.285697062810262 Validation Loss: 2.4259297847747803\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 140: Training Loss: 2.28432289759318 Validation Loss: 2.424715280532837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 141: Training Loss: 2.2829317251841226 Validation Loss: 2.423353672027588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 142: Training Loss: 2.281208594640096 Validation Loss: 2.422083854675293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 143: Training Loss: 2.2798173427581787 Validation Loss: 2.420931577682495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 144: Training Loss: 2.277990738550822 Validation Loss: 2.419570207595825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 145: Training Loss: 2.2755778630574546 Validation Loss: 2.4183928966522217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 146: Training Loss: 2.2744439442952475 Validation Loss: 2.4171299934387207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 147: Training Loss: 2.2731356620788574 Validation Loss: 2.4159018993377686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 148: Training Loss: 2.2709933121999106 Validation Loss: 2.4149093627929688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 149: Training Loss: 2.267603317896525 Validation Loss: 2.413726806640625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 150: Training Loss: 2.2670746644337973 Validation Loss: 2.4125747680664062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 151: Training Loss: 2.2671128114064536 Validation Loss: 2.4112772941589355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 152: Training Loss: 2.264305591583252 Validation Loss: 2.409907102584839\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 153: Training Loss: 2.262969891230265 Validation Loss: 2.4087460041046143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 154: Training Loss: 2.2647200425465903 Validation Loss: 2.4075872898101807\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 155: Training Loss: 2.2601136366526284 Validation Loss: 2.406123161315918\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156: Training Loss: 2.2585081259409585 Validation Loss: 2.4048919677734375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 157: Training Loss: 2.256566047668457 Validation Loss: 2.4036476612091064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 158: Training Loss: 2.2548367977142334 Validation Loss: 2.4024605751037598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 159: Training Loss: 2.2535926500956216 Validation Loss: 2.401137351989746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 160: Training Loss: 2.2510392665863037 Validation Loss: 2.4000205993652344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 161: Training Loss: 2.2494970162709556 Validation Loss: 2.398698091506958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 162: Training Loss: 2.2474973996480307 Validation Loss: 2.3975791931152344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 163: Training Loss: 2.2462772528330484 Validation Loss: 2.3964998722076416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 164: Training Loss: 2.244409720102946 Validation Loss: 2.3951921463012695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 165: Training Loss: 2.2427399158477783 Validation Loss: 2.3940963745117188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 166: Training Loss: 2.2415122985839844 Validation Loss: 2.392922878265381\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 167: Training Loss: 2.2395551204681396 Validation Loss: 2.3917903900146484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 168: Training Loss: 2.238316377003988 Validation Loss: 2.3901870250701904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 169: Training Loss: 2.237629493077596 Validation Loss: 2.389152765274048\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 170: Training Loss: 2.234633763631185 Validation Loss: 2.3880345821380615\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 171: Training Loss: 2.233044703801473 Validation Loss: 2.3865489959716797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 172: Training Loss: 2.2317009766896567 Validation Loss: 2.385140895843506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 173: Training Loss: 2.2303761641184487 Validation Loss: 2.383866786956787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 174: Training Loss: 2.228530009587606 Validation Loss: 2.38273286819458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 175: Training Loss: 2.2266964117685952 Validation Loss: 2.38153076171875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 176: Training Loss: 2.2261646588643393 Validation Loss: 2.3805718421936035\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 177: Training Loss: 2.2236998081207275 Validation Loss: 2.3793342113494873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 178: Training Loss: 2.221811215082804 Validation Loss: 2.3779025077819824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 179: Training Loss: 2.2213498751322427 Validation Loss: 2.3760454654693604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 180: Training Loss: 2.219678004582723 Validation Loss: 2.3746957778930664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 181: Training Loss: 2.2181906700134277 Validation Loss: 2.3737032413482666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 182: Training Loss: 2.2158494790395102 Validation Loss: 2.372208595275879\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 183: Training Loss: 2.2137580712636313 Validation Loss: 2.370872974395752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 184: Training Loss: 2.212620496749878 Validation Loss: 2.3696506023406982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 185: Training Loss: 2.2088586489359536 Validation Loss: 2.3686723709106445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 186: Training Loss: 2.208510080973307 Validation Loss: 2.367459297180176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 187: Training Loss: 2.2086759408315024 Validation Loss: 2.3661909103393555\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 188: Training Loss: 2.2056471506754556 Validation Loss: 2.3649916648864746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 189: Training Loss: 2.2041068871816 Validation Loss: 2.3634233474731445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 190: Training Loss: 2.2011256217956543 Validation Loss: 2.362349271774292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 191: Training Loss: 2.2011446158091226 Validation Loss: 2.3611724376678467\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 192: Training Loss: 2.198812166849772 Validation Loss: 2.359875440597534\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 193: Training Loss: 2.1978216965993247 Validation Loss: 2.3583579063415527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 194: Training Loss: 2.195486148198446 Validation Loss: 2.3571584224700928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 195: Training Loss: 2.193444808324178 Validation Loss: 2.3562064170837402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 196: Training Loss: 2.1920575300852456 Validation Loss: 2.3546957969665527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 197: Training Loss: 2.19051464398702 Validation Loss: 2.353616952896118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 198: Training Loss: 2.191474517186483 Validation Loss: 2.35225510597229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 199: Training Loss: 2.185973564783732 Validation Loss: 2.3506863117218018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 200: Training Loss: 2.185138781865438 Validation Loss: 2.3494818210601807\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 201: Training Loss: 2.1833367347717285 Validation Loss: 2.348285436630249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 202: Training Loss: 2.1817680994669595 Validation Loss: 2.3469741344451904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 203: Training Loss: 2.1794846057891846 Validation Loss: 2.345620632171631\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 204: Training Loss: 2.1786983013153076 Validation Loss: 2.344365119934082\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 205: Training Loss: 2.1776463190714517 Validation Loss: 2.3432791233062744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 206: Training Loss: 2.175489664077759 Validation Loss: 2.341857433319092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 207: Training Loss: 2.1746068795522056 Validation Loss: 2.340810537338257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 208: Training Loss: 2.171473503112793 Validation Loss: 2.3397116661071777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 209: Training Loss: 2.1698837280273438 Validation Loss: 2.3380706310272217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 210: Training Loss: 2.169514815012614 Validation Loss: 2.337034225463867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 211: Training Loss: 2.1687849362691245 Validation Loss: 2.3355860710144043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 212: Training Loss: 2.1658263206481934 Validation Loss: 2.3343698978424072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 213: Training Loss: 2.161737044652303 Validation Loss: 2.3334670066833496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 214: Training Loss: 2.161827484766642 Validation Loss: 2.332050323486328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 215: Training Loss: 2.160261551539103 Validation Loss: 2.330928087234497\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 216: Training Loss: 2.1599897543589273 Validation Loss: 2.3294517993927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 217: Training Loss: 2.1577746868133545 Validation Loss: 2.3281352519989014\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 218: Training Loss: 2.1560028394063315 Validation Loss: 2.3262720108032227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 219: Training Loss: 2.1536879539489746 Validation Loss: 2.3252029418945312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 220: Training Loss: 2.1510887940724692 Validation Loss: 2.324218988418579\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 221: Training Loss: 2.1492854754130044 Validation Loss: 2.322911262512207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 222: Training Loss: 2.145932992299398 Validation Loss: 2.321889877319336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 223: Training Loss: 2.1460235913594565 Validation Loss: 2.320948600769043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 224: Training Loss: 2.1458664735158286 Validation Loss: 2.3197689056396484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 225: Training Loss: 2.1426237424214682 Validation Loss: 2.3181653022766113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 226: Training Loss: 2.1420636971791587 Validation Loss: 2.316545248031616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 227: Training Loss: 2.1410722732543945 Validation Loss: 2.315211772918701\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 228: Training Loss: 2.13879386583964 Validation Loss: 2.3138105869293213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 229: Training Loss: 2.1366731325785318 Validation Loss: 2.3124687671661377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 230: Training Loss: 2.1359025637308755 Validation Loss: 2.3116092681884766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 231: Training Loss: 2.133717695871989 Validation Loss: 2.310058116912842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 232: Training Loss: 2.131906191507975 Validation Loss: 2.308695077896118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 233: Training Loss: 2.1305569807688394 Validation Loss: 2.307248592376709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 234: Training Loss: 2.1287418206532798 Validation Loss: 2.3061156272888184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 235: Training Loss: 2.1263171831766763 Validation Loss: 2.30513334274292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 236: Training Loss: 2.1258745988210044 Validation Loss: 2.3041136264801025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 237: Training Loss: 2.1232791741689048 Validation Loss: 2.3029873371124268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 238: Training Loss: 2.121604045232137 Validation Loss: 2.301816463470459\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 239: Training Loss: 2.1206111907958984 Validation Loss: 2.3002257347106934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 240: Training Loss: 2.1210014820098877 Validation Loss: 2.298321485519409\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 241: Training Loss: 2.1177910963694253 Validation Loss: 2.296846866607666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 242: Training Loss: 2.115181843439738 Validation Loss: 2.2957513332366943\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 243: Training Loss: 2.1128766536712646 Validation Loss: 2.2948172092437744\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 244: Training Loss: 2.111332654953003 Validation Loss: 2.2940750122070312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 245: Training Loss: 2.1108245054880777 Validation Loss: 2.292710065841675\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 246: Training Loss: 2.107747713724772 Validation Loss: 2.2917592525482178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 247: Training Loss: 2.1067981719970703 Validation Loss: 2.290247917175293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 248: Training Loss: 2.1055418650309243 Validation Loss: 2.288869857788086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 249: Training Loss: 2.1063831647237143 Validation Loss: 2.2871508598327637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 250: Training Loss: 2.102038780848185 Validation Loss: 2.2855074405670166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 251: Training Loss: 2.1000664234161377 Validation Loss: 2.283816337585449\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 252: Training Loss: 2.098750670750936 Validation Loss: 2.282484292984009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 253: Training Loss: 2.096522410710653 Validation Loss: 2.2817604541778564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 254: Training Loss: 2.0923098723093667 Validation Loss: 2.281010150909424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 255: Training Loss: 2.0949947039286294 Validation Loss: 2.2798702716827393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 256: Training Loss: 2.0914207299550376 Validation Loss: 2.2786083221435547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 257: Training Loss: 2.0900051593780518 Validation Loss: 2.2768375873565674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 258: Training Loss: 2.089009443918864 Validation Loss: 2.2753920555114746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 259: Training Loss: 2.0859986941019693 Validation Loss: 2.27377986907959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 260: Training Loss: 2.0845948855082193 Validation Loss: 2.272153615951538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 261: Training Loss: 2.084587653477987 Validation Loss: 2.2715272903442383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 262: Training Loss: 2.0816490650177 Validation Loss: 2.2704601287841797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 263: Training Loss: 2.0794740517934165 Validation Loss: 2.2691948413848877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 264: Training Loss: 2.0805506308873496 Validation Loss: 2.2677855491638184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 265: Training Loss: 2.0765296618143716 Validation Loss: 2.2661991119384766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 266: Training Loss: 2.0750784873962402 Validation Loss: 2.2646758556365967\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 267: Training Loss: 2.071485678354899 Validation Loss: 2.263293504714966\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 268: Training Loss: 2.071039915084839 Validation Loss: 2.262434959411621\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 269: Training Loss: 2.0698671340942383 Validation Loss: 2.2610161304473877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 270: Training Loss: 2.0674588680267334 Validation Loss: 2.259972095489502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 271: Training Loss: 2.067237218221029 Validation Loss: 2.2585573196411133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 272: Training Loss: 2.0641555786132812 Validation Loss: 2.257352828979492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 273: Training Loss: 2.063584884007772 Validation Loss: 2.2562103271484375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 274: Training Loss: 2.0613340536753335 Validation Loss: 2.2549521923065186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 275: Training Loss: 2.0591845512390137 Validation Loss: 2.253558397293091\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 276: Training Loss: 2.0568841298421225 Validation Loss: 2.251988172531128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 277: Training Loss: 2.0560118357340493 Validation Loss: 2.251145839691162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 278: Training Loss: 2.0557735761006675 Validation Loss: 2.2498955726623535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 279: Training Loss: 2.0509382088979087 Validation Loss: 2.2485382556915283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 280: Training Loss: 2.050959507624308 Validation Loss: 2.246917247772217\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 281: Training Loss: 2.049473206202189 Validation Loss: 2.2454049587249756\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 282: Training Loss: 2.0465540091196694 Validation Loss: 2.2440860271453857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 283: Training Loss: 2.0438597202301025 Validation Loss: 2.2430732250213623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 284: Training Loss: 2.0444538593292236 Validation Loss: 2.24177885055542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 285: Training Loss: 2.0441033840179443 Validation Loss: 2.240138053894043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 286: Training Loss: 2.0415945847829184 Validation Loss: 2.2390260696411133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 287: Training Loss: 2.039319117863973 Validation Loss: 2.237514019012451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 288: Training Loss: 2.0387867291768393 Validation Loss: 2.236797571182251\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 289: Training Loss: 2.036020040512085 Validation Loss: 2.23579740524292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 290: Training Loss: 2.0339282353719077 Validation Loss: 2.2338881492614746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 291: Training Loss: 2.032097895940145 Validation Loss: 2.23234224319458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 292: Training Loss: 2.0294637282689414 Validation Loss: 2.231336832046509\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 293: Training Loss: 2.0289093653361 Validation Loss: 2.2297210693359375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 294: Training Loss: 2.027360757191976 Validation Loss: 2.2284903526306152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 295: Training Loss: 2.025181293487549 Validation Loss: 2.2267556190490723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 296: Training Loss: 2.0228625933329263 Validation Loss: 2.225213050842285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 297: Training Loss: 2.02028481165568 Validation Loss: 2.224167823791504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 298: Training Loss: 2.0197381178538003 Validation Loss: 2.2232859134674072\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 299: Training Loss: 2.0180097818374634 Validation Loss: 2.222064971923828\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 300: Training Loss: 2.0163151025772095 Validation Loss: 2.2209858894348145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 301: Training Loss: 2.0151772101720176 Validation Loss: 2.2196097373962402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 302: Training Loss: 2.014678875605265 Validation Loss: 2.2179088592529297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 303: Training Loss: 2.0134138266245523 Validation Loss: 2.216735363006592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 304: Training Loss: 2.0091704527537027 Validation Loss: 2.2155022621154785\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 305: Training Loss: 2.0086468855539956 Validation Loss: 2.214111566543579\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 306: Training Loss: 2.007270614306132 Validation Loss: 2.212719678878784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 307: Training Loss: 2.0064991315205893 Validation Loss: 2.2114310264587402\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 308: Training Loss: 2.0047816038131714 Validation Loss: 2.2103164196014404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 309: Training Loss: 2.0010682344436646 Validation Loss: 2.2087557315826416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 310: Training Loss: 2.000036875406901 Validation Loss: 2.2074711322784424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 311: Training Loss: 1.998576243718465 Validation Loss: 2.205714464187622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 312: Training Loss: 1.9971072673797607 Validation Loss: 2.2048373222351074\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 313: Training Loss: 1.9937897523244221 Validation Loss: 2.2034120559692383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 314: Training Loss: 1.9941426118214924 Validation Loss: 2.2020225524902344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 315: Training Loss: 1.9918860991795857 Validation Loss: 2.200960397720337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 316: Training Loss: 1.9892995754877727 Validation Loss: 2.199730634689331\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 317: Training Loss: 1.9879136482874553 Validation Loss: 2.198258638381958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 318: Training Loss: 1.9861388603846233 Validation Loss: 2.197235107421875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 319: Training Loss: 1.9844049612681072 Validation Loss: 2.1957428455352783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 320: Training Loss: 1.983550985654195 Validation Loss: 2.194542407989502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 321: Training Loss: 1.9811113278071086 Validation Loss: 2.1925790309906006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 322: Training Loss: 1.9779082934061687 Validation Loss: 2.191230297088623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 323: Training Loss: 1.9814257621765137 Validation Loss: 2.1904473304748535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 324: Training Loss: 1.9802753925323486 Validation Loss: 2.1892311573028564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 325: Training Loss: 1.9745204448699951 Validation Loss: 2.1879689693450928\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 326: Training Loss: 1.9725075562795003 Validation Loss: 2.1865854263305664\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 327: Training Loss: 1.969537377357483 Validation Loss: 2.18515682220459\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 328: Training Loss: 1.9697770675023396 Validation Loss: 2.183530569076538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 329: Training Loss: 1.9659956296284993 Validation Loss: 2.1820759773254395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 330: Training Loss: 1.9666382869084675 Validation Loss: 2.1811017990112305\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 331: Training Loss: 1.9652018149693806 Validation Loss: 2.179771900177002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 332: Training Loss: 1.9630582332611084 Validation Loss: 2.1783804893493652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 333: Training Loss: 1.960675875345866 Validation Loss: 2.177093744277954\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 334: Training Loss: 1.9576126337051392 Validation Loss: 2.176114797592163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 335: Training Loss: 1.9575703541437786 Validation Loss: 2.1750268936157227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 336: Training Loss: 1.954860766728719 Validation Loss: 2.1732752323150635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 337: Training Loss: 1.9537956714630127 Validation Loss: 2.1720774173736572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 338: Training Loss: 1.9491028388341267 Validation Loss: 2.171138286590576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 339: Training Loss: 1.9503690004348755 Validation Loss: 2.169013500213623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 340: Training Loss: 1.948131521542867 Validation Loss: 2.1675455570220947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 341: Training Loss: 1.9468209743499756 Validation Loss: 2.166867256164551\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 342: Training Loss: 1.9444596370061238 Validation Loss: 2.1655640602111816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 343: Training Loss: 1.9430054426193237 Validation Loss: 2.1641674041748047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 344: Training Loss: 1.941672960917155 Validation Loss: 2.1632497310638428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 345: Training Loss: 1.9408037662506104 Validation Loss: 2.1620235443115234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 346: Training Loss: 1.9421727657318115 Validation Loss: 2.16087007522583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 347: Training Loss: 1.936599850654602 Validation Loss: 2.158867835998535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 348: Training Loss: 1.9356468518575032 Validation Loss: 2.156846046447754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 349: Training Loss: 1.9335296154022217 Validation Loss: 2.1554758548736572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 350: Training Loss: 1.9317792654037476 Validation Loss: 2.154296875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 351: Training Loss: 1.930652419726054 Validation Loss: 2.153669595718384\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 352: Training Loss: 1.9275222222010295 Validation Loss: 2.1523280143737793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 353: Training Loss: 1.9268227020899455 Validation Loss: 2.151455879211426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 354: Training Loss: 1.9251553614934285 Validation Loss: 2.150300979614258\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 355: Training Loss: 1.922145167986552 Validation Loss: 2.1487200260162354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 356: Training Loss: 1.9232066869735718 Validation Loss: 2.147242307662964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 357: Training Loss: 1.9201722542444866 Validation Loss: 2.1460256576538086\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 358: Training Loss: 1.9197442531585693 Validation Loss: 2.144341230392456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 359: Training Loss: 1.9166552225748699 Validation Loss: 2.143138885498047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 360: Training Loss: 1.9142235120137532 Validation Loss: 2.1413815021514893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 361: Training Loss: 1.9126973152160645 Validation Loss: 2.140399932861328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 362: Training Loss: 1.91093111038208 Validation Loss: 2.13901424407959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 363: Training Loss: 1.9089497725168865 Validation Loss: 2.137669324874878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 364: Training Loss: 1.9081859588623047 Validation Loss: 2.136277914047241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 365: Training Loss: 1.9060248136520386 Validation Loss: 2.135030508041382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 366: Training Loss: 1.903826117515564 Validation Loss: 2.1339495182037354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 367: Training Loss: 1.9026905695597331 Validation Loss: 2.132948875427246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 368: Training Loss: 1.9010550578435261 Validation Loss: 2.1316893100738525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 369: Training Loss: 1.899005651473999 Validation Loss: 2.1300456523895264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 370: Training Loss: 1.8983728885650635 Validation Loss: 2.1288533210754395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 371: Training Loss: 1.8953771988550823 Validation Loss: 2.127501964569092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 372: Training Loss: 1.89344322681427 Validation Loss: 2.1256134510040283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 373: Training Loss: 1.8913583755493164 Validation Loss: 2.1243603229522705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 374: Training Loss: 1.8907030820846558 Validation Loss: 2.1229794025421143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 375: Training Loss: 1.8888526757558186 Validation Loss: 2.121619462966919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 376: Training Loss: 1.887147347132365 Validation Loss: 2.120716094970703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 377: Training Loss: 1.8863699833552043 Validation Loss: 2.1196930408477783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 378: Training Loss: 1.884389082590739 Validation Loss: 2.1182632446289062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 379: Training Loss: 1.8824454148610432 Validation Loss: 2.11671781539917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 380: Training Loss: 1.8808292150497437 Validation Loss: 2.115366220474243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 381: Training Loss: 1.8800592422485352 Validation Loss: 2.1142711639404297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 382: Training Loss: 1.8776434659957886 Validation Loss: 2.1126277446746826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 383: Training Loss: 1.8764775196711223 Validation Loss: 2.1113383769989014\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384: Training Loss: 1.8734851280848186 Validation Loss: 2.1099965572357178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 385: Training Loss: 1.872058391571045 Validation Loss: 2.108623743057251\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 386: Training Loss: 1.8732226292292278 Validation Loss: 2.107262134552002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 387: Training Loss: 1.8684355020523071 Validation Loss: 2.1057677268981934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 388: Training Loss: 1.8667537371317546 Validation Loss: 2.1046268939971924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 389: Training Loss: 1.8647794723510742 Validation Loss: 2.1040165424346924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 390: Training Loss: 1.8626295328140259 Validation Loss: 2.1024010181427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 391: Training Loss: 1.8618686993916829 Validation Loss: 2.1011786460876465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 392: Training Loss: 1.8612990776697795 Validation Loss: 2.099055051803589\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 393: Training Loss: 1.8585031429926555 Validation Loss: 2.0977420806884766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 394: Training Loss: 1.857129176457723 Validation Loss: 2.0967724323272705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 395: Training Loss: 1.8551560242970784 Validation Loss: 2.096458911895752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 396: Training Loss: 1.8542435963948567 Validation Loss: 2.0949838161468506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 397: Training Loss: 1.851914366086324 Validation Loss: 2.0934510231018066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 398: Training Loss: 1.8503419160842896 Validation Loss: 2.0919132232666016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 399: Training Loss: 1.849065105120341 Validation Loss: 2.090385913848877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 400: Training Loss: 1.8452741305033367 Validation Loss: 2.088996648788452\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 401: Training Loss: 1.8451377550760906 Validation Loss: 2.0874500274658203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 402: Training Loss: 1.8434699773788452 Validation Loss: 2.086359739303589\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 403: Training Loss: 1.8433484236399333 Validation Loss: 2.085021495819092\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 404: Training Loss: 1.839633862177531 Validation Loss: 2.083465814590454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 405: Training Loss: 1.8411392370859783 Validation Loss: 2.0822627544403076\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 406: Training Loss: 1.8366379340489705 Validation Loss: 2.0813472270965576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 407: Training Loss: 1.833821694056193 Validation Loss: 2.0799436569213867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 408: Training Loss: 1.8336586157480876 Validation Loss: 2.078857183456421\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 409: Training Loss: 1.8317025105158489 Validation Loss: 2.0778040885925293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 410: Training Loss: 1.8313713471094768 Validation Loss: 2.0770092010498047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 411: Training Loss: 1.8284753958384197 Validation Loss: 2.075195789337158\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 412: Training Loss: 1.827043056488037 Validation Loss: 2.0734806060791016\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 413: Training Loss: 1.8245054483413696 Validation Loss: 2.071820020675659\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 414: Training Loss: 1.8246089220046997 Validation Loss: 2.0707075595855713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 415: Training Loss: 1.8216821750005086 Validation Loss: 2.069760322570801\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 416: Training Loss: 1.819248914718628 Validation Loss: 2.0683960914611816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 417: Training Loss: 1.8185638586680095 Validation Loss: 2.066948175430298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 418: Training Loss: 1.81645929813385 Validation Loss: 2.065643310546875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 419: Training Loss: 1.815442164738973 Validation Loss: 2.064481496810913\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 420: Training Loss: 1.8133737246195476 Validation Loss: 2.062540292739868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 421: Training Loss: 1.81359068552653 Validation Loss: 2.061386823654175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 422: Training Loss: 1.8093561331431072 Validation Loss: 2.0603208541870117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 423: Training Loss: 1.8081600268681843 Validation Loss: 2.059250593185425\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 424: Training Loss: 1.8070176839828491 Validation Loss: 2.0579068660736084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 425: Training Loss: 1.8046975533167522 Validation Loss: 2.0560925006866455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 426: Training Loss: 1.8036659161249797 Validation Loss: 2.0549116134643555\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 427: Training Loss: 1.8029861052831013 Validation Loss: 2.054319381713867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 428: Training Loss: 1.8000768025716145 Validation Loss: 2.0529673099517822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 429: Training Loss: 1.7983734210332234 Validation Loss: 2.051212787628174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 430: Training Loss: 1.797058343887329 Validation Loss: 2.050051689147949\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 431: Training Loss: 1.7951485713322957 Validation Loss: 2.048435688018799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 432: Training Loss: 1.7935892343521118 Validation Loss: 2.04707932472229\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 433: Training Loss: 1.7918248971303303 Validation Loss: 2.045865297317505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 434: Training Loss: 1.7900850375493367 Validation Loss: 2.0446300506591797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 435: Training Loss: 1.7887242237726848 Validation Loss: 2.042943239212036\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 436: Training Loss: 1.7879184087117512 Validation Loss: 2.0424346923828125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 437: Training Loss: 1.785139838854472 Validation Loss: 2.041248083114624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 438: Training Loss: 1.783971110979716 Validation Loss: 2.0391836166381836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 439: Training Loss: 1.781486709912618 Validation Loss: 2.0381784439086914\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 440: Training Loss: 1.7796303828557332 Validation Loss: 2.0365355014801025\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 441: Training Loss: 1.7817135254542034 Validation Loss: 2.035330057144165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 442: Training Loss: 1.7771252791086833 Validation Loss: 2.034029006958008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 443: Training Loss: 1.7752392292022705 Validation Loss: 2.0324840545654297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 444: Training Loss: 1.773897926012675 Validation Loss: 2.0315823554992676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 445: Training Loss: 1.7738194465637207 Validation Loss: 2.0302157402038574\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 446: Training Loss: 1.7742430766423543 Validation Loss: 2.029210090637207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 447: Training Loss: 1.7703367471694946 Validation Loss: 2.028144359588623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 448: Training Loss: 1.7674141724904378 Validation Loss: 2.0269875526428223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 449: Training Loss: 1.7652979691823323 Validation Loss: 2.025404214859009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 450: Training Loss: 1.7661181688308716 Validation Loss: 2.023839235305786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 451: Training Loss: 1.7611275513966878 Validation Loss: 2.021923542022705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 452: Training Loss: 1.763243595759074 Validation Loss: 2.0209879875183105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 453: Training Loss: 1.760553201039632 Validation Loss: 2.019679307937622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 454: Training Loss: 1.7577195167541504 Validation Loss: 2.018794298171997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 455: Training Loss: 1.7554848591486614 Validation Loss: 2.0180823802948\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 456: Training Loss: 1.7541397015253704 Validation Loss: 2.0172319412231445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 457: Training Loss: 1.753878156344096 Validation Loss: 2.0155959129333496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 458: Training Loss: 1.75098717212677 Validation Loss: 2.0140881538391113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 459: Training Loss: 1.7491015195846558 Validation Loss: 2.012352466583252\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460: Training Loss: 1.74940820535024 Validation Loss: 2.0104358196258545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 461: Training Loss: 1.7457116047541301 Validation Loss: 2.0091960430145264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 462: Training Loss: 1.7440338532129924 Validation Loss: 2.007854700088501\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 463: Training Loss: 1.7437770366668701 Validation Loss: 2.0066096782684326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 464: Training Loss: 1.7406504154205322 Validation Loss: 2.0055758953094482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 465: Training Loss: 1.7399154901504517 Validation Loss: 2.004951000213623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 466: Training Loss: 1.7386269172032673 Validation Loss: 2.00325870513916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 467: Training Loss: 1.7364336649576824 Validation Loss: 2.0018913745880127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 468: Training Loss: 1.734824577967326 Validation Loss: 2.0010838508605957\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 469: Training Loss: 1.7317678531010945 Validation Loss: 2.0001089572906494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 470: Training Loss: 1.7344223260879517 Validation Loss: 1.998525857925415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 471: Training Loss: 1.7300325234731038 Validation Loss: 1.9972227811813354\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 472: Training Loss: 1.7293674548467 Validation Loss: 1.996106505393982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 473: Training Loss: 1.7261027892430623 Validation Loss: 1.9950354099273682\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 474: Training Loss: 1.7264317274093628 Validation Loss: 1.99320387840271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 475: Training Loss: 1.723581592241923 Validation Loss: 1.9923710823059082\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 476: Training Loss: 1.7221778631210327 Validation Loss: 1.9911069869995117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 477: Training Loss: 1.722643534342448 Validation Loss: 1.989385724067688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 478: Training Loss: 1.71848460038503 Validation Loss: 1.9882173538208008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 479: Training Loss: 1.7160678307215373 Validation Loss: 1.9865738153457642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 480: Training Loss: 1.7162901957829793 Validation Loss: 1.9851831197738647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 481: Training Loss: 1.7155249913533528 Validation Loss: 1.98365318775177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 482: Training Loss: 1.711951454480489 Validation Loss: 1.9819749593734741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 483: Training Loss: 1.7084426879882812 Validation Loss: 1.9810760021209717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 484: Training Loss: 1.7094793717066448 Validation Loss: 1.9804847240447998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 485: Training Loss: 1.7076412836710613 Validation Loss: 1.979736566543579\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 486: Training Loss: 1.7064820130666096 Validation Loss: 1.9785916805267334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 487: Training Loss: 1.7052661180496216 Validation Loss: 1.9769586324691772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 488: Training Loss: 1.7042189836502075 Validation Loss: 1.9752236604690552\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 489: Training Loss: 1.7023146549860637 Validation Loss: 1.973936915397644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 490: Training Loss: 1.6998122533162434 Validation Loss: 1.9725145101547241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 491: Training Loss: 1.6982314984003704 Validation Loss: 1.9713780879974365\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 492: Training Loss: 1.696855107943217 Validation Loss: 1.9700275659561157\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 493: Training Loss: 1.6960203250249226 Validation Loss: 1.9687483310699463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 494: Training Loss: 1.693487246831258 Validation Loss: 1.9675664901733398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 495: Training Loss: 1.691998283068339 Validation Loss: 1.9658880233764648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 496: Training Loss: 1.6904948949813843 Validation Loss: 1.9642293453216553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 497: Training Loss: 1.6894768873850505 Validation Loss: 1.9637353420257568\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 498: Training Loss: 1.6881680885950725 Validation Loss: 1.9622341394424438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 499: Training Loss: 1.685567061106364 Validation Loss: 1.9612685441970825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 500: Training Loss: 1.683418830235799 Validation Loss: 1.9605695009231567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 501: Training Loss: 1.6811890999476116 Validation Loss: 1.9597437381744385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 502: Training Loss: 1.6812078952789307 Validation Loss: 1.9583251476287842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 503: Training Loss: 1.6815518140792847 Validation Loss: 1.9568679332733154\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 504: Training Loss: 1.6779470841089885 Validation Loss: 1.9547938108444214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 505: Training Loss: 1.6766791741053264 Validation Loss: 1.9528220891952515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 506: Training Loss: 1.67610498269399 Validation Loss: 1.9521641731262207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 507: Training Loss: 1.67344864209493 Validation Loss: 1.9511048793792725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 508: Training Loss: 1.672490159670512 Validation Loss: 1.9496155977249146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 509: Training Loss: 1.6708430051803589 Validation Loss: 1.9482651948928833\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 510: Training Loss: 1.6689139207204182 Validation Loss: 1.946990728378296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 511: Training Loss: 1.6664883295694988 Validation Loss: 1.9463512897491455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 512: Training Loss: 1.6642290751139324 Validation Loss: 1.945725440979004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 513: Training Loss: 1.6632833083470662 Validation Loss: 1.9442540407180786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 514: Training Loss: 1.6635317007700603 Validation Loss: 1.9423809051513672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 515: Training Loss: 1.6602255900700886 Validation Loss: 1.940905213356018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 516: Training Loss: 1.664264718691508 Validation Loss: 1.9400979280471802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 517: Training Loss: 1.6594263712565105 Validation Loss: 1.9385875463485718\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 518: Training Loss: 1.6569945017496746 Validation Loss: 1.9368635416030884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 519: Training Loss: 1.6545870701471965 Validation Loss: 1.9357248544692993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 520: Training Loss: 1.6522510051727295 Validation Loss: 1.9344768524169922\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 521: Training Loss: 1.6518096129099529 Validation Loss: 1.933951497077942\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 522: Training Loss: 1.6507141987482707 Validation Loss: 1.932466745376587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 523: Training Loss: 1.6493558883666992 Validation Loss: 1.9308013916015625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 524: Training Loss: 1.649243672688802 Validation Loss: 1.9303643703460693\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 525: Training Loss: 1.642888108889262 Validation Loss: 1.9291456937789917\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 526: Training Loss: 1.6436250607172649 Validation Loss: 1.9282187223434448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 527: Training Loss: 1.6490227381388347 Validation Loss: 1.9271308183670044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 528: Training Loss: 1.6445546944936116 Validation Loss: 1.9260079860687256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 529: Training Loss: 1.639737884203593 Validation Loss: 1.9235079288482666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 530: Training Loss: 1.637515703837077 Validation Loss: 1.9222809076309204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 531: Training Loss: 1.6340420643488567 Validation Loss: 1.9211716651916504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 532: Training Loss: 1.6344314018885295 Validation Loss: 1.92000150680542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 533: Training Loss: 1.6334769328435261 Validation Loss: 1.9189214706420898\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 534: Training Loss: 1.6323046684265137 Validation Loss: 1.9181698560714722\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 535: Training Loss: 1.629420518875122 Validation Loss: 1.9165738821029663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 536: Training Loss: 1.6271952390670776 Validation Loss: 1.9152380228042603\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 537: Training Loss: 1.6315055688222249 Validation Loss: 1.9143040180206299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 538: Training Loss: 1.6255568663279216 Validation Loss: 1.913462519645691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 539: Training Loss: 1.6247511704762776 Validation Loss: 1.9119585752487183\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 540: Training Loss: 1.6233516136805217 Validation Loss: 1.909850835800171\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 541: Training Loss: 1.621016263961792 Validation Loss: 1.9085960388183594\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 542: Training Loss: 1.621542493502299 Validation Loss: 1.9075145721435547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 543: Training Loss: 1.617877761522929 Validation Loss: 1.9063082933425903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 544: Training Loss: 1.6199923753738403 Validation Loss: 1.9053479433059692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 545: Training Loss: 1.6161757707595825 Validation Loss: 1.9046192169189453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 546: Training Loss: 1.6145817041397095 Validation Loss: 1.9035115242004395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 547: Training Loss: 1.6123647689819336 Validation Loss: 1.9023752212524414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 548: Training Loss: 1.6132687330245972 Validation Loss: 1.901136875152588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 549: Training Loss: 1.6087876558303833 Validation Loss: 1.899926781654358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 550: Training Loss: 1.607814351717631 Validation Loss: 1.8980361223220825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 551: Training Loss: 1.6060134172439575 Validation Loss: 1.8966875076293945\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 552: Training Loss: 1.6024564901987712 Validation Loss: 1.8952289819717407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 553: Training Loss: 1.603481690088908 Validation Loss: 1.8935518264770508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 554: Training Loss: 1.6006542841593425 Validation Loss: 1.8932479619979858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 555: Training Loss: 1.6011213064193726 Validation Loss: 1.8925303220748901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 556: Training Loss: 1.5984015067418416 Validation Loss: 1.8912714719772339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 557: Training Loss: 1.597235123316447 Validation Loss: 1.8897244930267334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 558: Training Loss: 1.596282720565796 Validation Loss: 1.8875833749771118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 559: Training Loss: 1.5940776268641155 Validation Loss: 1.8865323066711426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 560: Training Loss: 1.5935544570287068 Validation Loss: 1.8857232332229614\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 561: Training Loss: 1.5941297213236492 Validation Loss: 1.8848415613174438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 562: Training Loss: 1.5917198260625203 Validation Loss: 1.8833671808242798\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 563: Training Loss: 1.587687651316325 Validation Loss: 1.8818975687026978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 564: Training Loss: 1.586537480354309 Validation Loss: 1.8809460401535034\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 565: Training Loss: 1.5851912101109822 Validation Loss: 1.8799806833267212\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 566: Training Loss: 1.5861929655075073 Validation Loss: 1.8794440031051636\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 567: Training Loss: 1.5827960570653279 Validation Loss: 1.877925157546997\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 568: Training Loss: 1.5810538530349731 Validation Loss: 1.8763622045516968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 569: Training Loss: 1.5794413884480794 Validation Loss: 1.8748632669448853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 570: Training Loss: 1.578625996907552 Validation Loss: 1.8743971586227417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 571: Training Loss: 1.5774152676264446 Validation Loss: 1.872596025466919\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 572: Training Loss: 1.5762539704640706 Validation Loss: 1.8714295625686646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 573: Training Loss: 1.573932369550069 Validation Loss: 1.870097041130066\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 574: Training Loss: 1.5706427892049153 Validation Loss: 1.868205189704895\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 575: Training Loss: 1.5707691510518391 Validation Loss: 1.8669822216033936\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 576: Training Loss: 1.5692683060963948 Validation Loss: 1.8664659261703491\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 577: Training Loss: 1.567266821861267 Validation Loss: 1.865104079246521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 578: Training Loss: 1.5654197533925374 Validation Loss: 1.8641866445541382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 579: Training Loss: 1.5650538603464763 Validation Loss: 1.8632709980010986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 580: Training Loss: 1.563083251317342 Validation Loss: 1.8620964288711548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 581: Training Loss: 1.562204082806905 Validation Loss: 1.8605270385742188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 582: Training Loss: 1.5603828032811482 Validation Loss: 1.8593530654907227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 583: Training Loss: 1.5604735215504963 Validation Loss: 1.8585957288742065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 584: Training Loss: 1.5585704644521077 Validation Loss: 1.8571633100509644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 585: Training Loss: 1.553807258605957 Validation Loss: 1.8564339876174927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 586: Training Loss: 1.5546621878941853 Validation Loss: 1.8543578386306763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 587: Training Loss: 1.554135799407959 Validation Loss: 1.8528701066970825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 588: Training Loss: 1.5547152360280354 Validation Loss: 1.8517197370529175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 589: Training Loss: 1.5508291323979695 Validation Loss: 1.850598692893982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 590: Training Loss: 1.5517245531082153 Validation Loss: 1.8499703407287598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 591: Training Loss: 1.550861398379008 Validation Loss: 1.8480335474014282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 592: Training Loss: 1.5459975798924763 Validation Loss: 1.8476306200027466\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 593: Training Loss: 1.5399256149927776 Validation Loss: 1.8463069200515747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 594: Training Loss: 1.5454488197962444 Validation Loss: 1.845892071723938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 595: Training Loss: 1.5417946577072144 Validation Loss: 1.8457932472229004\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 596: Training Loss: 1.540144960085551 Validation Loss: 1.8444873094558716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 597: Training Loss: 1.5373767217000325 Validation Loss: 1.842317819595337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 598: Training Loss: 1.5361608664194744 Validation Loss: 1.8407459259033203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 599: Training Loss: 1.5354721943537395 Validation Loss: 1.8387314081192017\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 600: Training Loss: 1.5335798263549805 Validation Loss: 1.836984634399414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 601: Training Loss: 1.5340060790379841 Validation Loss: 1.8367420434951782\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 602: Training Loss: 1.5312193234761555 Validation Loss: 1.8364139795303345\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 603: Training Loss: 1.5304783582687378 Validation Loss: 1.8352625370025635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 604: Training Loss: 1.529197136561076 Validation Loss: 1.8334393501281738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 605: Training Loss: 1.5273579359054565 Validation Loss: 1.8325155973434448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 606: Training Loss: 1.526324987411499 Validation Loss: 1.8315911293029785\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 607: Training Loss: 1.5256201823552449 Validation Loss: 1.82966947555542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 608: Training Loss: 1.5227432250976562 Validation Loss: 1.8284822702407837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 609: Training Loss: 1.522284984588623 Validation Loss: 1.8275052309036255\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 610: Training Loss: 1.5221013228098552 Validation Loss: 1.8269147872924805\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 611: Training Loss: 1.519890824953715 Validation Loss: 1.825774073600769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 612: Training Loss: 1.5173146327336628 Validation Loss: 1.8248339891433716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 613: Training Loss: 1.5167761246363323 Validation Loss: 1.8233928680419922\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 614: Training Loss: 1.514779011408488 Validation Loss: 1.8221908807754517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 615: Training Loss: 1.5132325490315754 Validation Loss: 1.8203948736190796\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 616: Training Loss: 1.5133748451868694 Validation Loss: 1.8202036619186401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 617: Training Loss: 1.5108989874521892 Validation Loss: 1.8182905912399292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 618: Training Loss: 1.5079982280731201 Validation Loss: 1.8171563148498535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 619: Training Loss: 1.5064600308736165 Validation Loss: 1.8157764673233032\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 620: Training Loss: 1.5058695077896118 Validation Loss: 1.8146237134933472\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 621: Training Loss: 1.5062241951624553 Validation Loss: 1.8146193027496338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 622: Training Loss: 1.506364345550537 Validation Loss: 1.813296914100647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 623: Training Loss: 1.5019877354303997 Validation Loss: 1.812559962272644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 624: Training Loss: 1.4989968140920003 Validation Loss: 1.8106436729431152\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 625: Training Loss: 1.4993517398834229 Validation Loss: 1.8093405961990356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 626: Training Loss: 1.4982373317082722 Validation Loss: 1.8074134588241577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 627: Training Loss: 1.4972762664159138 Validation Loss: 1.8059052228927612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 628: Training Loss: 1.495924989382426 Validation Loss: 1.8046698570251465\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 629: Training Loss: 1.4941455125808716 Validation Loss: 1.8048423528671265\n",
      "Epoch 630: Training Loss: 1.4928025007247925 Validation Loss: 1.803905725479126\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 631: Training Loss: 1.4917103052139282 Validation Loss: 1.8024052381515503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 632: Training Loss: 1.4896472295125325 Validation Loss: 1.801393985748291\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 633: Training Loss: 1.4906073808670044 Validation Loss: 1.8000918626785278\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 634: Training Loss: 1.4874934355417888 Validation Loss: 1.7982310056686401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 635: Training Loss: 1.4856089353561401 Validation Loss: 1.7970770597457886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 636: Training Loss: 1.4839543501536052 Validation Loss: 1.79587984085083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 637: Training Loss: 1.4832147359848022 Validation Loss: 1.7956690788269043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 638: Training Loss: 1.4824333985646565 Validation Loss: 1.795081615447998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 639: Training Loss: 1.4817448059717815 Validation Loss: 1.794165015220642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 640: Training Loss: 1.4800341924031575 Validation Loss: 1.792374610900879\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 641: Training Loss: 1.477295160293579 Validation Loss: 1.7911685705184937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 642: Training Loss: 1.4764823118845622 Validation Loss: 1.7900582551956177\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 643: Training Loss: 1.4742481708526611 Validation Loss: 1.788772463798523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 644: Training Loss: 1.47730286916097 Validation Loss: 1.787529706954956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 645: Training Loss: 1.4726122220357258 Validation Loss: 1.7867106199264526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 646: Training Loss: 1.4704705079396565 Validation Loss: 1.7863062620162964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 647: Training Loss: 1.4703377485275269 Validation Loss: 1.7850162982940674\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 648: Training Loss: 1.468289573987325 Validation Loss: 1.7828971147537231\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 649: Training Loss: 1.4673014084498088 Validation Loss: 1.7823847532272339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 650: Training Loss: 1.4679842789967854 Validation Loss: 1.7811113595962524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 651: Training Loss: 1.462658127148946 Validation Loss: 1.7801039218902588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 652: Training Loss: 1.4625815947850545 Validation Loss: 1.7790948152542114\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 653: Training Loss: 1.4618682066599529 Validation Loss: 1.7774896621704102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 654: Training Loss: 1.4602661927541096 Validation Loss: 1.7764161825180054\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 655: Training Loss: 1.4591593742370605 Validation Loss: 1.774882435798645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 656: Training Loss: 1.457022984822591 Validation Loss: 1.7737400531768799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 657: Training Loss: 1.4557544787724812 Validation Loss: 1.773399829864502\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 658: Training Loss: 1.4537352720896404 Validation Loss: 1.7729475498199463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 659: Training Loss: 1.4540897607803345 Validation Loss: 1.7713290452957153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 660: Training Loss: 1.4542924165725708 Validation Loss: 1.7695033550262451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 661: Training Loss: 1.450394074122111 Validation Loss: 1.768103003501892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 662: Training Loss: 1.4496098359425862 Validation Loss: 1.7671254873275757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 663: Training Loss: 1.4490160544713337 Validation Loss: 1.7655216455459595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 664: Training Loss: 1.4479900201161702 Validation Loss: 1.7647244930267334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 665: Training Loss: 1.4438076416651409 Validation Loss: 1.7644754648208618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 666: Training Loss: 1.443877100944519 Validation Loss: 1.7637405395507812\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 667: Training Loss: 1.4447992245356243 Validation Loss: 1.7630459070205688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 668: Training Loss: 1.4424023230870564 Validation Loss: 1.7614810466766357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 669: Training Loss: 1.4405637582143147 Validation Loss: 1.7598017454147339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 670: Training Loss: 1.4387516180674236 Validation Loss: 1.7592923641204834\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 671: Training Loss: 1.4364554087320964 Validation Loss: 1.757519006729126\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 672: Training Loss: 1.436358133951823 Validation Loss: 1.7566373348236084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 673: Training Loss: 1.4349401791890461 Validation Loss: 1.7564051151275635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 674: Training Loss: 1.4333539803822835 Validation Loss: 1.7544385194778442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 675: Training Loss: 1.4331878821055095 Validation Loss: 1.753940463066101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 676: Training Loss: 1.43098779519399 Validation Loss: 1.7516522407531738\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 677: Training Loss: 1.4300660689671834 Validation Loss: 1.7502509355545044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 678: Training Loss: 1.428348422050476 Validation Loss: 1.7488203048706055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 679: Training Loss: 1.4272761742273967 Validation Loss: 1.7485973834991455\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 680: Training Loss: 1.4256593386332195 Validation Loss: 1.7475659847259521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 681: Training Loss: 1.4235938390096028 Validation Loss: 1.746614694595337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 682: Training Loss: 1.42334779103597 Validation Loss: 1.7447444200515747\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 683: Training Loss: 1.423326015472412 Validation Loss: 1.7434651851654053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 684: Training Loss: 1.4208609263102214 Validation Loss: 1.742929458618164\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 685: Training Loss: 1.4191169738769531 Validation Loss: 1.741722822189331\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 686: Training Loss: 1.4175599416097004 Validation Loss: 1.7406196594238281\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 687: Training Loss: 1.4150648911794026 Validation Loss: 1.7395812273025513\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 688: Training Loss: 1.4132635593414307 Validation Loss: 1.7379908561706543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 689: Training Loss: 1.4133403698603313 Validation Loss: 1.7368111610412598\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 690: Training Loss: 1.412989894549052 Validation Loss: 1.7359130382537842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 691: Training Loss: 1.4108630021413167 Validation Loss: 1.7346887588500977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 692: Training Loss: 1.4088149865468342 Validation Loss: 1.7335840463638306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 693: Training Loss: 1.4090241988499959 Validation Loss: 1.731987476348877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 694: Training Loss: 1.4068698485692341 Validation Loss: 1.7305188179016113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 695: Training Loss: 1.4054017066955566 Validation Loss: 1.728912353515625\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 696: Training Loss: 1.4048332770665486 Validation Loss: 1.727982997894287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 697: Training Loss: 1.4032831192016602 Validation Loss: 1.7273699045181274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 698: Training Loss: 1.401756485303243 Validation Loss: 1.7264100313186646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 699: Training Loss: 1.4012375672658284 Validation Loss: 1.724969506263733\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 700: Training Loss: 1.3985967636108398 Validation Loss: 1.7235037088394165\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 701: Training Loss: 1.3979451656341553 Validation Loss: 1.7226500511169434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 702: Training Loss: 1.39653476079305 Validation Loss: 1.7214897871017456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 703: Training Loss: 1.3946164846420288 Validation Loss: 1.720184326171875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 704: Training Loss: 1.396113355954488 Validation Loss: 1.7194669246673584\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 705: Training Loss: 1.3939022620519002 Validation Loss: 1.7189505100250244\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 706: Training Loss: 1.3905964295069377 Validation Loss: 1.717990517616272\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 707: Training Loss: 1.393298625946045 Validation Loss: 1.7161073684692383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 708: Training Loss: 1.3907061020533245 Validation Loss: 1.7151381969451904\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 709: Training Loss: 1.3900102376937866 Validation Loss: 1.714633822441101\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 710: Training Loss: 1.3873312870661418 Validation Loss: 1.7132493257522583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 711: Training Loss: 1.3842971324920654 Validation Loss: 1.7115604877471924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 712: Training Loss: 1.384644627571106 Validation Loss: 1.709669589996338\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 713: Training Loss: 1.3821816444396973 Validation Loss: 1.7090764045715332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 714: Training Loss: 1.3815526167551677 Validation Loss: 1.7097020149230957\n",
      "Epoch 715: Training Loss: 1.3782250881195068 Validation Loss: 1.7085089683532715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 716: Training Loss: 1.379206379254659 Validation Loss: 1.706045389175415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 717: Training Loss: 1.3792531490325928 Validation Loss: 1.7052488327026367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 718: Training Loss: 1.376681129137675 Validation Loss: 1.7036209106445312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 719: Training Loss: 1.3741957346598308 Validation Loss: 1.7028096914291382\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 720: Training Loss: 1.3741803169250488 Validation Loss: 1.7018018960952759\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 721: Training Loss: 1.3715832233428955 Validation Loss: 1.7020341157913208\n",
      "Epoch 722: Training Loss: 1.3717490434646606 Validation Loss: 1.701109528541565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 723: Training Loss: 1.370924472808838 Validation Loss: 1.6998379230499268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 724: Training Loss: 1.3693960905075073 Validation Loss: 1.6983660459518433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 725: Training Loss: 1.367025097211202 Validation Loss: 1.6966381072998047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 726: Training Loss: 1.3658844629923503 Validation Loss: 1.6953699588775635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 727: Training Loss: 1.3655717372894287 Validation Loss: 1.694240689277649\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 728: Training Loss: 1.3660868008931477 Validation Loss: 1.6940374374389648\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 729: Training Loss: 1.3640485207239788 Validation Loss: 1.6924552917480469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 730: Training Loss: 1.3637436628341675 Validation Loss: 1.6912586688995361\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 731: Training Loss: 1.3602486848831177 Validation Loss: 1.6900343894958496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 732: Training Loss: 1.360107660293579 Validation Loss: 1.6890668869018555\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 733: Training Loss: 1.3579849004745483 Validation Loss: 1.688490629196167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 734: Training Loss: 1.3583918412526448 Validation Loss: 1.6879628896713257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 735: Training Loss: 1.3552056550979614 Validation Loss: 1.687423586845398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 736: Training Loss: 1.3539618651072185 Validation Loss: 1.6853784322738647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 737: Training Loss: 1.35337233543396 Validation Loss: 1.6836216449737549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 738: Training Loss: 1.3517510493596394 Validation Loss: 1.6831508874893188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 739: Training Loss: 1.3516306479771931 Validation Loss: 1.6820809841156006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 740: Training Loss: 1.3496597210566204 Validation Loss: 1.6820957660675049\n",
      "Epoch 741: Training Loss: 1.3484110832214355 Validation Loss: 1.6812901496887207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 742: Training Loss: 1.3470827341079712 Validation Loss: 1.6792004108428955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 743: Training Loss: 1.3468315998713176 Validation Loss: 1.6774933338165283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 744: Training Loss: 1.3440028826395671 Validation Loss: 1.6762752532958984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 745: Training Loss: 1.3430365721384685 Validation Loss: 1.674202799797058\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 746: Training Loss: 1.3432668050130208 Validation Loss: 1.673980474472046\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 747: Training Loss: 1.3393694162368774 Validation Loss: 1.6740564107894897\n",
      "Epoch 748: Training Loss: 1.3398418823877971 Validation Loss: 1.673486351966858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 749: Training Loss: 1.3378243048985798 Validation Loss: 1.6732945442199707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 750: Training Loss: 1.3412761688232422 Validation Loss: 1.671243667602539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 751: Training Loss: 1.3373801310857136 Validation Loss: 1.670074224472046\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 752: Training Loss: 1.3353335460027058 Validation Loss: 1.6688358783721924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 753: Training Loss: 1.3331615924835205 Validation Loss: 1.667819857597351\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 754: Training Loss: 1.3325757185618083 Validation Loss: 1.666634202003479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 755: Training Loss: 1.3311834335327148 Validation Loss: 1.6652697324752808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 756: Training Loss: 1.3296969731648762 Validation Loss: 1.664853811264038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 757: Training Loss: 1.3282434145609539 Validation Loss: 1.663787841796875\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 758: Training Loss: 1.3298675219217937 Validation Loss: 1.6622951030731201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 759: Training Loss: 1.3266949653625488 Validation Loss: 1.6621391773223877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 760: Training Loss: 1.3244716723759968 Validation Loss: 1.660767674446106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 761: Training Loss: 1.3237384955088298 Validation Loss: 1.6595228910446167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 762: Training Loss: 1.3245232105255127 Validation Loss: 1.6586095094680786\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 763: Training Loss: 1.3208178281784058 Validation Loss: 1.6576534509658813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 764: Training Loss: 1.3221219778060913 Validation Loss: 1.6567282676696777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 765: Training Loss: 1.3222287495930989 Validation Loss: 1.6554614305496216\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 766: Training Loss: 1.3213346401850383 Validation Loss: 1.6549919843673706\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 767: Training Loss: 1.318138599395752 Validation Loss: 1.6545367240905762\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 768: Training Loss: 1.3157384792963664 Validation Loss: 1.6535253524780273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 769: Training Loss: 1.3152460257212322 Validation Loss: 1.6520600318908691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 770: Training Loss: 1.3141069014867146 Validation Loss: 1.6501538753509521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 771: Training Loss: 1.3115412791570027 Validation Loss: 1.6488105058670044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 772: Training Loss: 1.3114922444025676 Validation Loss: 1.6480568647384644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 773: Training Loss: 1.3100988864898682 Validation Loss: 1.6471689939498901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 774: Training Loss: 1.3082661231358845 Validation Loss: 1.646220326423645\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 775: Training Loss: 1.3076536655426025 Validation Loss: 1.6452430486679077\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 776: Training Loss: 1.3084684610366821 Validation Loss: 1.6443040370941162\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 777: Training Loss: 1.3058481216430664 Validation Loss: 1.6435173749923706\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 778: Training Loss: 1.3034180800120037 Validation Loss: 1.6436306238174438\n",
      "Epoch 779: Training Loss: 1.3020607233047485 Validation Loss: 1.6426926851272583\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 780: Training Loss: 1.3019558986028035 Validation Loss: 1.6410222053527832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 781: Training Loss: 1.3002108335494995 Validation Loss: 1.6403034925460815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 782: Training Loss: 1.2994712988535564 Validation Loss: 1.6396875381469727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 783: Training Loss: 1.2990480661392212 Validation Loss: 1.6382355690002441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 784: Training Loss: 1.303287943204244 Validation Loss: 1.6369044780731201\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 785: Training Loss: 1.2959555784861247 Validation Loss: 1.6350866556167603\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 786: Training Loss: 1.2956786155700684 Validation Loss: 1.634558081626892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 787: Training Loss: 1.29288383324941 Validation Loss: 1.633233904838562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 788: Training Loss: 1.293361981709798 Validation Loss: 1.6322429180145264\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 789: Training Loss: 1.290613015492757 Validation Loss: 1.6317194700241089\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 790: Training Loss: 1.2921775579452515 Validation Loss: 1.631123423576355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 791: Training Loss: 1.2891819079717 Validation Loss: 1.6303932666778564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 792: Training Loss: 1.2885825236638386 Validation Loss: 1.629652738571167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 793: Training Loss: 1.288683255513509 Validation Loss: 1.6293482780456543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 794: Training Loss: 1.286910096804301 Validation Loss: 1.6281622648239136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 795: Training Loss: 1.2868297100067139 Validation Loss: 1.626135230064392\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 796: Training Loss: 1.2832181851069133 Validation Loss: 1.6249247789382935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 797: Training Loss: 1.2820188999176025 Validation Loss: 1.6238774061203003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 798: Training Loss: 1.2812540928522747 Validation Loss: 1.6233739852905273\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 799: Training Loss: 1.280330777168274 Validation Loss: 1.6221930980682373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 800: Training Loss: 1.2781658967336018 Validation Loss: 1.6213349103927612\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 801: Training Loss: 1.27812393506368 Validation Loss: 1.620166301727295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 802: Training Loss: 1.2769362131754558 Validation Loss: 1.619195818901062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 803: Training Loss: 1.2755047082901 Validation Loss: 1.6179087162017822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 804: Training Loss: 1.2745417753855388 Validation Loss: 1.6169724464416504\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 805: Training Loss: 1.2709571917851765 Validation Loss: 1.6166937351226807\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 806: Training Loss: 1.2726284265518188 Validation Loss: 1.6154370307922363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 807: Training Loss: 1.273484468460083 Validation Loss: 1.6150983572006226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 808: Training Loss: 1.2702101866404216 Validation Loss: 1.6142280101776123\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 809: Training Loss: 1.2717012166976929 Validation Loss: 1.6137001514434814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 810: Training Loss: 1.2667265335718791 Validation Loss: 1.6124670505523682\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 811: Training Loss: 1.2663950125376384 Validation Loss: 1.6115257740020752\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 812: Training Loss: 1.2651166915893555 Validation Loss: 1.6095850467681885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 813: Training Loss: 1.2657219568888347 Validation Loss: 1.608085036277771\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 814: Training Loss: 1.264423131942749 Validation Loss: 1.6072617769241333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 815: Training Loss: 1.2619232336680095 Validation Loss: 1.6071288585662842\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 816: Training Loss: 1.2622629801432292 Validation Loss: 1.6066272258758545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 817: Training Loss: 1.2581876913706462 Validation Loss: 1.6051921844482422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 818: Training Loss: 1.2598220904668171 Validation Loss: 1.6039519309997559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 819: Training Loss: 1.257055640220642 Validation Loss: 1.603813886642456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 820: Training Loss: 1.2577986319859822 Validation Loss: 1.6024837493896484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 821: Training Loss: 1.2545429865519206 Validation Loss: 1.601778268814087\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 822: Training Loss: 1.2557942469914753 Validation Loss: 1.601091742515564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 823: Training Loss: 1.2551027139027913 Validation Loss: 1.6002532243728638\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 824: Training Loss: 1.2518943150838215 Validation Loss: 1.598891019821167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 825: Training Loss: 1.25079345703125 Validation Loss: 1.5982011556625366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 826: Training Loss: 1.2498549620310466 Validation Loss: 1.5970991849899292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 827: Training Loss: 1.2517104943593342 Validation Loss: 1.5958054065704346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 828: Training Loss: 1.2537294228871663 Validation Loss: 1.5953789949417114\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 829: Training Loss: 1.248371998469035 Validation Loss: 1.5938640832901\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 830: Training Loss: 1.2461084127426147 Validation Loss: 1.5923104286193848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 831: Training Loss: 1.2457197507222493 Validation Loss: 1.5918192863464355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 832: Training Loss: 1.2431412935256958 Validation Loss: 1.5905964374542236\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 833: Training Loss: 1.2442796230316162 Validation Loss: 1.5898686647415161\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 834: Training Loss: 1.2408267656962078 Validation Loss: 1.589399814605713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 835: Training Loss: 1.2416090965270996 Validation Loss: 1.5886259078979492\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 836: Training Loss: 1.2391358613967896 Validation Loss: 1.588236927986145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 837: Training Loss: 1.2402617931365967 Validation Loss: 1.5871562957763672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 838: Training Loss: 1.238064169883728 Validation Loss: 1.5864306688308716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 839: Training Loss: 1.2378346125284831 Validation Loss: 1.5854496955871582\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 840: Training Loss: 1.2354414065678914 Validation Loss: 1.5840402841567993\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 841: Training Loss: 1.2337252696355183 Validation Loss: 1.5833234786987305\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 842: Training Loss: 1.2364272276560466 Validation Loss: 1.5825610160827637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 843: Training Loss: 1.2342339356740315 Validation Loss: 1.5819313526153564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 844: Training Loss: 1.230249047279358 Validation Loss: 1.580856442451477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 845: Training Loss: 1.2303868532180786 Validation Loss: 1.578940510749817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 846: Training Loss: 1.2297634681065877 Validation Loss: 1.5783673524856567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 847: Training Loss: 1.2283142805099487 Validation Loss: 1.5777015686035156\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 848: Training Loss: 1.2260916233062744 Validation Loss: 1.575892448425293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 849: Training Loss: 1.2247897386550903 Validation Loss: 1.5751570463180542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 850: Training Loss: 1.2248948017756145 Validation Loss: 1.5743165016174316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 851: Training Loss: 1.2228564818700154 Validation Loss: 1.5734487771987915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 852: Training Loss: 1.2228409051895142 Validation Loss: 1.5725551843643188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 853: Training Loss: 1.2220999797185261 Validation Loss: 1.5719342231750488\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 854: Training Loss: 1.2188611427942913 Validation Loss: 1.5718107223510742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 855: Training Loss: 1.2224429845809937 Validation Loss: 1.5708107948303223\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 856: Training Loss: 1.2178184191385906 Validation Loss: 1.570234775543213\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 857: Training Loss: 1.2182765404383342 Validation Loss: 1.568604826927185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 858: Training Loss: 1.2146715720494587 Validation Loss: 1.5676451921463013\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 859: Training Loss: 1.2144907712936401 Validation Loss: 1.5665194988250732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 860: Training Loss: 1.2138314644495647 Validation Loss: 1.5658740997314453\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 861: Training Loss: 1.2141783237457275 Validation Loss: 1.5654319524765015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 862: Training Loss: 1.211288571357727 Validation Loss: 1.5649346113204956\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 863: Training Loss: 1.2103962500890095 Validation Loss: 1.5638660192489624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 864: Training Loss: 1.2097957134246826 Validation Loss: 1.5631635189056396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 865: Training Loss: 1.2090142170588176 Validation Loss: 1.562054991722107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 866: Training Loss: 1.2075248956680298 Validation Loss: 1.5612342357635498\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 867: Training Loss: 1.206387956937154 Validation Loss: 1.5598818063735962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 868: Training Loss: 1.2051724592844646 Validation Loss: 1.5579800605773926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 869: Training Loss: 1.2035668690999348 Validation Loss: 1.5577844381332397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 870: Training Loss: 1.2056915760040283 Validation Loss: 1.5579628944396973\n",
      "Epoch 871: Training Loss: 1.1986889441808064 Validation Loss: 1.5579742193222046\n",
      "Epoch 872: Training Loss: 1.2004478375116985 Validation Loss: 1.5570541620254517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 873: Training Loss: 1.1999943653742473 Validation Loss: 1.5552027225494385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 874: Training Loss: 1.2011696100234985 Validation Loss: 1.5532211065292358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 875: Training Loss: 1.1971617937088013 Validation Loss: 1.5515056848526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 876: Training Loss: 1.1962977250417073 Validation Loss: 1.5514557361602783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 877: Training Loss: 1.1986090342203777 Validation Loss: 1.5508991479873657\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 878: Training Loss: 1.192889889081319 Validation Loss: 1.5515968799591064\n",
      "Epoch 879: Training Loss: 1.1934699614842732 Validation Loss: 1.5506749153137207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 880: Training Loss: 1.1926893790562947 Validation Loss: 1.548883318901062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 881: Training Loss: 1.1906767288843791 Validation Loss: 1.5475876331329346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 882: Training Loss: 1.1913023392359416 Validation Loss: 1.5466033220291138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 883: Training Loss: 1.189979116121928 Validation Loss: 1.5459575653076172\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 884: Training Loss: 1.1884634097417195 Validation Loss: 1.5445510149002075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 885: Training Loss: 1.187467058499654 Validation Loss: 1.5444196462631226\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 886: Training Loss: 1.1882920662562053 Validation Loss: 1.5445241928100586\n",
      "Epoch 887: Training Loss: 1.1849457422892253 Validation Loss: 1.5441999435424805\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 888: Training Loss: 1.183546543121338 Validation Loss: 1.5429412126541138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 889: Training Loss: 1.1885838905970256 Validation Loss: 1.5410782098770142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 890: Training Loss: 1.1825442711512248 Validation Loss: 1.539272665977478\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 891: Training Loss: 1.182123064994812 Validation Loss: 1.5381006002426147\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 892: Training Loss: 1.182787299156189 Validation Loss: 1.537583827972412\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 893: Training Loss: 1.179151177406311 Validation Loss: 1.5364534854888916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 894: Training Loss: 1.1781925757726033 Validation Loss: 1.5367190837860107\n",
      "Epoch 895: Training Loss: 1.1773849725723267 Validation Loss: 1.5367119312286377\n",
      "Epoch 896: Training Loss: 1.176265557607015 Validation Loss: 1.5358854532241821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 897: Training Loss: 1.1758199135462444 Validation Loss: 1.5346354246139526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 898: Training Loss: 1.1768019596735637 Validation Loss: 1.5334484577178955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 899: Training Loss: 1.1734858353932698 Validation Loss: 1.5328329801559448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 900: Training Loss: 1.172994613647461 Validation Loss: 1.5313864946365356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 901: Training Loss: 1.1704135338465373 Validation Loss: 1.5312161445617676\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 902: Training Loss: 1.1685362259546916 Validation Loss: 1.5308488607406616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 903: Training Loss: 1.1701133251190186 Validation Loss: 1.529923915863037\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 904: Training Loss: 1.169873873392741 Validation Loss: 1.5283490419387817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 905: Training Loss: 1.1674917141596477 Validation Loss: 1.5263460874557495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 906: Training Loss: 1.1664177576700847 Validation Loss: 1.5258910655975342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 907: Training Loss: 1.1673268874486287 Validation Loss: 1.525205373764038\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 908: Training Loss: 1.1657544374465942 Validation Loss: 1.5251364707946777\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 909: Training Loss: 1.1637513637542725 Validation Loss: 1.5252572298049927\n",
      "Epoch 910: Training Loss: 1.163905421892802 Validation Loss: 1.524298906326294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 911: Training Loss: 1.16054101785024 Validation Loss: 1.5230003595352173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 912: Training Loss: 1.1590222120285034 Validation Loss: 1.5220539569854736\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 913: Training Loss: 1.1601191759109497 Validation Loss: 1.5209523439407349\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 914: Training Loss: 1.1581836541493733 Validation Loss: 1.519994854927063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 915: Training Loss: 1.159894863764445 Validation Loss: 1.5192415714263916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 916: Training Loss: 1.1565017700195312 Validation Loss: 1.5180561542510986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 917: Training Loss: 1.155642032623291 Validation Loss: 1.517226219177246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 918: Training Loss: 1.1554815769195557 Validation Loss: 1.5168814659118652\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 919: Training Loss: 1.1540282169977825 Validation Loss: 1.5158671140670776\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 920: Training Loss: 1.1545406182607014 Validation Loss: 1.5152127742767334\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 921: Training Loss: 1.151821255683899 Validation Loss: 1.5146472454071045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 922: Training Loss: 1.1509678363800049 Validation Loss: 1.5140659809112549\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 923: Training Loss: 1.1505518754323323 Validation Loss: 1.5129995346069336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 924: Training Loss: 1.1484529972076416 Validation Loss: 1.512922763824463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 925: Training Loss: 1.1491676568984985 Validation Loss: 1.5116467475891113\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 926: Training Loss: 1.1468384265899658 Validation Loss: 1.5093693733215332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 927: Training Loss: 1.1453712781270344 Validation Loss: 1.508239984512329\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 928: Training Loss: 1.1452057361602783 Validation Loss: 1.5083431005477905\n",
      "Epoch 929: Training Loss: 1.144789457321167 Validation Loss: 1.5078396797180176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 930: Training Loss: 1.1415025393168132 Validation Loss: 1.5068913698196411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 931: Training Loss: 1.1420173247655232 Validation Loss: 1.5058218240737915\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 932: Training Loss: 1.1411716143290203 Validation Loss: 1.5055153369903564\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 933: Training Loss: 1.1411566734313965 Validation Loss: 1.5046288967132568\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 934: Training Loss: 1.1393666664759319 Validation Loss: 1.5045219659805298\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 935: Training Loss: 1.1379822095235188 Validation Loss: 1.5038496255874634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 936: Training Loss: 1.136832316716512 Validation Loss: 1.5025885105133057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 937: Training Loss: 1.1379618247350056 Validation Loss: 1.5026105642318726\n",
      "Epoch 938: Training Loss: 1.1354899406433105 Validation Loss: 1.5007195472717285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 939: Training Loss: 1.135980526606242 Validation Loss: 1.500015377998352\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 940: Training Loss: 1.134493072827657 Validation Loss: 1.4985803365707397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 941: Training Loss: 1.132190465927124 Validation Loss: 1.4979716539382935\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 942: Training Loss: 1.133979042371114 Validation Loss: 1.4973742961883545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 943: Training Loss: 1.1323766708374023 Validation Loss: 1.4968602657318115\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 944: Training Loss: 1.1311397949854534 Validation Loss: 1.49745512008667\n",
      "Epoch 945: Training Loss: 1.129742980003357 Validation Loss: 1.4957809448242188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 946: Training Loss: 1.12754225730896 Validation Loss: 1.4942994117736816\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 947: Training Loss: 1.1283496220906575 Validation Loss: 1.493141770362854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 948: Training Loss: 1.1276277303695679 Validation Loss: 1.4927326440811157\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 949: Training Loss: 1.1239537000656128 Validation Loss: 1.4919302463531494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 950: Training Loss: 1.1244664986928303 Validation Loss: 1.4912078380584717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 951: Training Loss: 1.123540957768758 Validation Loss: 1.4904544353485107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 952: Training Loss: 1.1218890746434529 Validation Loss: 1.4891372919082642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 953: Training Loss: 1.121170202891032 Validation Loss: 1.4877471923828125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 954: Training Loss: 1.1200143496195476 Validation Loss: 1.488025188446045\n",
      "Epoch 955: Training Loss: 1.1199476321538289 Validation Loss: 1.488745093345642\n",
      "Epoch 956: Training Loss: 1.12404465675354 Validation Loss: 1.4874037504196167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 957: Training Loss: 1.1157585382461548 Validation Loss: 1.48551607131958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 958: Training Loss: 1.1168715556462605 Validation Loss: 1.484450340270996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 959: Training Loss: 1.113053520520528 Validation Loss: 1.4835883378982544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 960: Training Loss: 1.11467711130778 Validation Loss: 1.483161211013794\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 961: Training Loss: 1.1137450536092122 Validation Loss: 1.483228325843811\n",
      "Epoch 962: Training Loss: 1.1133315165837605 Validation Loss: 1.482109546661377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 963: Training Loss: 1.11170494556427 Validation Loss: 1.4812215566635132\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 964: Training Loss: 1.1113482316335042 Validation Loss: 1.4801491498947144\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 965: Training Loss: 1.1124353806177776 Validation Loss: 1.4787580966949463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 966: Training Loss: 1.109012206395467 Validation Loss: 1.478912353515625\n",
      "Epoch 967: Training Loss: 1.1071029504140217 Validation Loss: 1.4781014919281006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 968: Training Loss: 1.110672911008199 Validation Loss: 1.477356195449829\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 969: Training Loss: 1.1055227518081665 Validation Loss: 1.4758509397506714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 970: Training Loss: 1.1052720944086711 Validation Loss: 1.4748646020889282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 971: Training Loss: 1.104514757792155 Validation Loss: 1.4740315675735474\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 972: Training Loss: 1.1029004255930583 Validation Loss: 1.4738848209381104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 973: Training Loss: 1.102066993713379 Validation Loss: 1.4747337102890015\n",
      "Epoch 974: Training Loss: 1.1021080017089844 Validation Loss: 1.473830223083496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 975: Training Loss: 1.1007369756698608 Validation Loss: 1.4717460870742798\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 976: Training Loss: 1.0998046000798543 Validation Loss: 1.4713362455368042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 977: Training Loss: 1.1007088422775269 Validation Loss: 1.4707163572311401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 978: Training Loss: 1.0998010635375977 Validation Loss: 1.4696110486984253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 979: Training Loss: 1.0980382760365803 Validation Loss: 1.4690455198287964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 980: Training Loss: 1.09745987256368 Validation Loss: 1.4685049057006836\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 981: Training Loss: 1.0930461088816326 Validation Loss: 1.467587947845459\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 982: Training Loss: 1.0951392650604248 Validation Loss: 1.465421438217163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 983: Training Loss: 1.094363808631897 Validation Loss: 1.4650534391403198\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 984: Training Loss: 1.093120853106181 Validation Loss: 1.4653900861740112\n",
      "Epoch 985: Training Loss: 1.0934494336446126 Validation Loss: 1.4645130634307861\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 986: Training Loss: 1.0899519125620525 Validation Loss: 1.4639031887054443\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 987: Training Loss: 1.0892359813054402 Validation Loss: 1.4631518125534058\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 988: Training Loss: 1.0898458162943523 Validation Loss: 1.461901307106018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 989: Training Loss: 1.0878707965215046 Validation Loss: 1.4603906869888306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 990: Training Loss: 1.0871530771255493 Validation Loss: 1.4601424932479858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 991: Training Loss: 1.0866620143254597 Validation Loss: 1.4607068300247192\n",
      "Epoch 992: Training Loss: 1.0853633483250935 Validation Loss: 1.4599595069885254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 993: Training Loss: 1.0875949462254841 Validation Loss: 1.4588509798049927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 994: Training Loss: 1.0853386123975117 Validation Loss: 1.4575632810592651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 995: Training Loss: 1.083068052927653 Validation Loss: 1.456242322921753\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 996: Training Loss: 1.0813616514205933 Validation Loss: 1.4560343027114868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 997: Training Loss: 1.0810250441233318 Validation Loss: 1.4562153816223145\n",
      "Epoch 998: Training Loss: 1.0798840920130413 Validation Loss: 1.4558093547821045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 999: Training Loss: 1.0789766709009807 Validation Loss: 1.4536305665969849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1000: Training Loss: 1.0783613125483196 Validation Loss: 1.4520360231399536\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1001: Training Loss: 1.0753186146418254 Validation Loss: 1.4511853456497192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1002: Training Loss: 1.077292005221049 Validation Loss: 1.4514939785003662\n",
      "Epoch 1003: Training Loss: 1.0759613116582234 Validation Loss: 1.451112985610962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1004: Training Loss: 1.0716893672943115 Validation Loss: 1.4509217739105225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1005: Training Loss: 1.0753665765126545 Validation Loss: 1.4501665830612183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1006: Training Loss: 1.0727392037709553 Validation Loss: 1.4487611055374146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1007: Training Loss: 1.072064757347107 Validation Loss: 1.4473488330841064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1008: Training Loss: 1.0710969765981038 Validation Loss: 1.4470971822738647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1009: Training Loss: 1.0701824426651 Validation Loss: 1.4462521076202393\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1010: Training Loss: 1.069348414738973 Validation Loss: 1.4456909894943237\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1011: Training Loss: 1.0680980483690898 Validation Loss: 1.445987343788147\n",
      "Epoch 1012: Training Loss: 1.068868080774943 Validation Loss: 1.4450159072875977\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1013: Training Loss: 1.0730216304461162 Validation Loss: 1.444069504737854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1014: Training Loss: 1.0656109650929768 Validation Loss: 1.4429913759231567\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1015: Training Loss: 1.0671554803848267 Validation Loss: 1.4419856071472168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1016: Training Loss: 1.0658840735753377 Validation Loss: 1.4413222074508667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1017: Training Loss: 1.0626312096913655 Validation Loss: 1.441857933998108\n",
      "Epoch 1018: Training Loss: 1.0654980540275574 Validation Loss: 1.4412490129470825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1019: Training Loss: 1.0618083874384563 Validation Loss: 1.4406527280807495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1020: Training Loss: 1.0616405407587688 Validation Loss: 1.4389925003051758\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1021: Training Loss: 1.059952974319458 Validation Loss: 1.4375070333480835\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1022: Training Loss: 1.0595043102900188 Validation Loss: 1.4363725185394287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1023: Training Loss: 1.058058460553487 Validation Loss: 1.4353172779083252\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1024: Training Loss: 1.0600841244061787 Validation Loss: 1.4360413551330566\n",
      "Epoch 1025: Training Loss: 1.0565377871195476 Validation Loss: 1.4355394840240479\n",
      "Epoch 1026: Training Loss: 1.0572434663772583 Validation Loss: 1.4347678422927856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1027: Training Loss: 1.0548031330108643 Validation Loss: 1.4335428476333618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1028: Training Loss: 1.0538443326950073 Validation Loss: 1.4328596591949463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1029: Training Loss: 1.0529165267944336 Validation Loss: 1.4317412376403809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1030: Training Loss: 1.0515030225118 Validation Loss: 1.4316587448120117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1031: Training Loss: 1.0542898178100586 Validation Loss: 1.4317278861999512\n",
      "Epoch 1032: Training Loss: 1.0484100778897603 Validation Loss: 1.4309579133987427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1033: Training Loss: 1.0504108270009358 Validation Loss: 1.4306138753890991\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1034: Training Loss: 1.049670934677124 Validation Loss: 1.4303112030029297\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1035: Training Loss: 1.0488195816675823 Validation Loss: 1.4291343688964844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1036: Training Loss: 1.0473235448201497 Validation Loss: 1.4278769493103027\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1037: Training Loss: 1.045785625775655 Validation Loss: 1.4259154796600342\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1038: Training Loss: 1.045908530553182 Validation Loss: 1.424631953239441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1039: Training Loss: 1.0453288555145264 Validation Loss: 1.423959493637085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1040: Training Loss: 1.0476287603378296 Validation Loss: 1.4243855476379395\n",
      "Epoch 1041: Training Loss: 1.0454679528872173 Validation Loss: 1.4248783588409424\n",
      "Epoch 1042: Training Loss: 1.04281751314799 Validation Loss: 1.4239943027496338\n",
      "Epoch 1043: Training Loss: 1.041232665379842 Validation Loss: 1.4225341081619263\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1044: Training Loss: 1.0404189626375835 Validation Loss: 1.4220209121704102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1045: Training Loss: 1.039196213086446 Validation Loss: 1.4209095239639282\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1046: Training Loss: 1.0396737655003865 Validation Loss: 1.4203652143478394\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1047: Training Loss: 1.0381513237953186 Validation Loss: 1.4198137521743774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1048: Training Loss: 1.040205717086792 Validation Loss: 1.419050693511963\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1049: Training Loss: 1.037893831729889 Validation Loss: 1.4179247617721558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1050: Training Loss: 1.035241961479187 Validation Loss: 1.4174693822860718\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1051: Training Loss: 1.035382827123006 Validation Loss: 1.416929841041565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1052: Training Loss: 1.0343165795008342 Validation Loss: 1.4171468019485474\n",
      "Epoch 1053: Training Loss: 1.0319262941678364 Validation Loss: 1.4163721799850464\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1054: Training Loss: 1.0340768098831177 Validation Loss: 1.4155356884002686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1055: Training Loss: 1.032414714495341 Validation Loss: 1.4135992527008057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1056: Training Loss: 1.030933141708374 Validation Loss: 1.4124881029129028\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1057: Training Loss: 1.0306917826334636 Validation Loss: 1.4122190475463867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1058: Training Loss: 1.0285655458768208 Validation Loss: 1.4111977815628052\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1059: Training Loss: 1.027730385462443 Validation Loss: 1.4114199876785278\n",
      "Epoch 1060: Training Loss: 1.0268616676330566 Validation Loss: 1.4111586809158325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1061: Training Loss: 1.0256297985712688 Validation Loss: 1.410618543624878\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1062: Training Loss: 1.0274490118026733 Validation Loss: 1.4102991819381714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1063: Training Loss: 1.0243348280588787 Validation Loss: 1.4093446731567383\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1064: Training Loss: 1.0241209268569946 Validation Loss: 1.4079314470291138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1065: Training Loss: 1.0240684549013774 Validation Loss: 1.4070780277252197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1066: Training Loss: 1.022246499856313 Validation Loss: 1.4070425033569336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1067: Training Loss: 1.0217441121737163 Validation Loss: 1.4066461324691772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1068: Training Loss: 1.0201139847437541 Validation Loss: 1.4067466259002686\n",
      "Epoch 1069: Training Loss: 1.019263486067454 Validation Loss: 1.4051578044891357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1070: Training Loss: 1.0194420019785564 Validation Loss: 1.4047062397003174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1071: Training Loss: 1.0199275811513264 Validation Loss: 1.402975082397461\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1072: Training Loss: 1.0178550481796265 Validation Loss: 1.403174877166748\n",
      "Epoch 1073: Training Loss: 1.0173592170079548 Validation Loss: 1.4020487070083618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1074: Training Loss: 1.015842119852702 Validation Loss: 1.4013346433639526\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1075: Training Loss: 1.0165627598762512 Validation Loss: 1.4012402296066284\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1076: Training Loss: 1.0140231649080913 Validation Loss: 1.4002045392990112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1077: Training Loss: 1.0131865342458088 Validation Loss: 1.399703025817871\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1078: Training Loss: 1.0120526750882466 Validation Loss: 1.398771047592163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1079: Training Loss: 1.0111740827560425 Validation Loss: 1.3981331586837769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1080: Training Loss: 1.0117769440015156 Validation Loss: 1.3974288702011108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1081: Training Loss: 1.0101995468139648 Validation Loss: 1.3962364196777344\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1082: Training Loss: 1.0092065731684368 Validation Loss: 1.3956456184387207\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1083: Training Loss: 1.0090253750483196 Validation Loss: 1.3949165344238281\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1084: Training Loss: 1.008104940255483 Validation Loss: 1.3954145908355713\n",
      "Epoch 1085: Training Loss: 1.0075271129608154 Validation Loss: 1.394893765449524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1086: Training Loss: 1.0061541398366292 Validation Loss: 1.3952556848526\n",
      "Epoch 1087: Training Loss: 1.0057279864947002 Validation Loss: 1.3950859308242798\n",
      "Epoch 1088: Training Loss: 1.0048039555549622 Validation Loss: 1.3935800790786743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1089: Training Loss: 1.0042766531308491 Validation Loss: 1.391870379447937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1090: Training Loss: 1.0038766860961914 Validation Loss: 1.3897554874420166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1091: Training Loss: 1.0035133759180705 Validation Loss: 1.3888813257217407\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1092: Training Loss: 1.0014808376630147 Validation Loss: 1.388696551322937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1093: Training Loss: 1.0009997884432476 Validation Loss: 1.3885178565979004\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1094: Training Loss: 1.001844048500061 Validation Loss: 1.3887958526611328\n",
      "Epoch 1095: Training Loss: 0.9993374347686768 Validation Loss: 1.3897463083267212\n",
      "Epoch 1096: Training Loss: 0.9981167912483215 Validation Loss: 1.389352560043335\n",
      "Epoch 1097: Training Loss: 0.998186727364858 Validation Loss: 1.387270212173462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1098: Training Loss: 0.9962162971496582 Validation Loss: 1.3859070539474487\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1099: Training Loss: 0.9965178767840067 Validation Loss: 1.3844608068466187\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1100: Training Loss: 0.9960935711860657 Validation Loss: 1.3847057819366455\n",
      "Epoch 1101: Training Loss: 0.9943577249844869 Validation Loss: 1.3843441009521484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1102: Training Loss: 0.9932437141736349 Validation Loss: 1.3832544088363647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1103: Training Loss: 0.9918227990468343 Validation Loss: 1.3820160627365112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1104: Training Loss: 0.9909858902295431 Validation Loss: 1.381138801574707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1105: Training Loss: 0.9934825301170349 Validation Loss: 1.3818509578704834\n",
      "Epoch 1106: Training Loss: 0.9895298679669698 Validation Loss: 1.3806174993515015\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1107: Training Loss: 0.988324244817098 Validation Loss: 1.3802474737167358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1108: Training Loss: 0.9885027607282003 Validation Loss: 1.3799070119857788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1109: Training Loss: 0.9877482652664185 Validation Loss: 1.3794646263122559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1110: Training Loss: 0.9894707798957825 Validation Loss: 1.3797271251678467\n",
      "Epoch 1111: Training Loss: 0.9852691292762756 Validation Loss: 1.3783386945724487\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1112: Training Loss: 0.985258142153422 Validation Loss: 1.3779395818710327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1113: Training Loss: 0.985550324122111 Validation Loss: 1.3767600059509277\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1114: Training Loss: 0.9850908716519674 Validation Loss: 1.3758677244186401\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1115: Training Loss: 0.986293097337087 Validation Loss: 1.3740328550338745\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1116: Training Loss: 0.9824365576108297 Validation Loss: 1.3743642568588257\n",
      "Epoch 1117: Training Loss: 0.9822673797607422 Validation Loss: 1.3741103410720825\n",
      "Epoch 1118: Training Loss: 0.9820660750071207 Validation Loss: 1.3730711936950684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1119: Training Loss: 0.9799367189407349 Validation Loss: 1.3724499940872192\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1120: Training Loss: 0.9788539608319601 Validation Loss: 1.3719886541366577\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1121: Training Loss: 0.9784115751584371 Validation Loss: 1.3710061311721802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1122: Training Loss: 0.9780041774113973 Validation Loss: 1.3707293272018433\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1123: Training Loss: 0.9765400886535645 Validation Loss: 1.3703514337539673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1124: Training Loss: 0.9756203889846802 Validation Loss: 1.3700398206710815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1125: Training Loss: 0.9752059181531271 Validation Loss: 1.3697621822357178\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1126: Training Loss: 0.9753067493438721 Validation Loss: 1.3685393333435059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1127: Training Loss: 0.9777869979540507 Validation Loss: 1.3676106929779053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1128: Training Loss: 0.9753365715344747 Validation Loss: 1.36769437789917\n",
      "Epoch 1129: Training Loss: 0.9737173120180765 Validation Loss: 1.3664692640304565\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1130: Training Loss: 0.9722652832667033 Validation Loss: 1.3657675981521606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1131: Training Loss: 0.9706742763519287 Validation Loss: 1.3646788597106934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1132: Training Loss: 0.9704142610232035 Validation Loss: 1.3638253211975098\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1133: Training Loss: 0.9692925214767456 Validation Loss: 1.3636358976364136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1134: Training Loss: 0.9696511824925741 Validation Loss: 1.363812804222107\n",
      "Epoch 1135: Training Loss: 0.9681397676467896 Validation Loss: 1.3627814054489136\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1136: Training Loss: 0.966811994711558 Validation Loss: 1.3626154661178589\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1137: Training Loss: 0.966517170270284 Validation Loss: 1.3619968891143799\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1138: Training Loss: 0.9664621353149414 Validation Loss: 1.3612139225006104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1139: Training Loss: 0.9694154262542725 Validation Loss: 1.3616660833358765\n",
      "Epoch 1140: Training Loss: 0.9637772440910339 Validation Loss: 1.3612511157989502\n",
      "Epoch 1141: Training Loss: 0.9627954165140787 Validation Loss: 1.3596127033233643\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1142: Training Loss: 0.9632419943809509 Validation Loss: 1.3588635921478271\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1143: Training Loss: 0.9610135555267334 Validation Loss: 1.3581598997116089\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1144: Training Loss: 0.9609946409861246 Validation Loss: 1.357926845550537\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1145: Training Loss: 0.9629808664321899 Validation Loss: 1.3566776514053345\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1146: Training Loss: 0.9589241941769918 Validation Loss: 1.356050968170166\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1147: Training Loss: 0.958628257115682 Validation Loss: 1.3545796871185303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1148: Training Loss: 0.9587829907735189 Validation Loss: 1.353906273841858\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1149: Training Loss: 0.956027090549469 Validation Loss: 1.353254795074463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1150: Training Loss: 0.9597246448198954 Validation Loss: 1.3543462753295898\n",
      "Epoch 1151: Training Loss: 0.9585378567377726 Validation Loss: 1.3546407222747803\n",
      "Epoch 1152: Training Loss: 0.954278846581777 Validation Loss: 1.3545475006103516\n",
      "Epoch 1153: Training Loss: 0.9551637172698975 Validation Loss: 1.3531540632247925\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1154: Training Loss: 0.9535078207651774 Validation Loss: 1.3519153594970703\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1155: Training Loss: 0.9526315927505493 Validation Loss: 1.3504797220230103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1156: Training Loss: 0.9525439739227295 Validation Loss: 1.348945140838623\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1157: Training Loss: 0.9531848827997843 Validation Loss: 1.349024772644043\n",
      "Epoch 1158: Training Loss: 0.9513846238454183 Validation Loss: 1.3490208387374878\n",
      "Epoch 1159: Training Loss: 0.9518824617067972 Validation Loss: 1.349806547164917\n",
      "Epoch 1160: Training Loss: 0.9490791161855062 Validation Loss: 1.349258303642273\n",
      "Epoch 1161: Training Loss: 0.9483740329742432 Validation Loss: 1.3471592664718628\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1162: Training Loss: 0.9491027593612671 Validation Loss: 1.3455560207366943\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1163: Training Loss: 0.9472746650377909 Validation Loss: 1.3455545902252197\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1164: Training Loss: 0.9444567561149597 Validation Loss: 1.345973253250122\n",
      "Epoch 1165: Training Loss: 0.9442700942357382 Validation Loss: 1.3461956977844238\n",
      "Epoch 1166: Training Loss: 0.9448083440462748 Validation Loss: 1.3450125455856323\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1167: Training Loss: 0.943211555480957 Validation Loss: 1.3436343669891357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1168: Training Loss: 0.9435515403747559 Validation Loss: 1.3438092470169067\n",
      "Epoch 1169: Training Loss: 0.9428597688674927 Validation Loss: 1.3431767225265503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1170: Training Loss: 0.9415514866511027 Validation Loss: 1.3417284488677979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1171: Training Loss: 0.9419528245925903 Validation Loss: 1.341625690460205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1172: Training Loss: 0.9457695484161377 Validation Loss: 1.3408573865890503\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1173: Training Loss: 0.9409040411313375 Validation Loss: 1.3409632444381714\n",
      "Epoch 1174: Training Loss: 0.9386617342631022 Validation Loss: 1.3393968343734741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1175: Training Loss: 0.937649687131246 Validation Loss: 1.339369773864746\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1176: Training Loss: 0.9373878439267477 Validation Loss: 1.3381770849227905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1177: Training Loss: 0.9368669788042704 Validation Loss: 1.3373061418533325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1178: Training Loss: 0.9367238481839498 Validation Loss: 1.3380061388015747\n",
      "Epoch 1179: Training Loss: 0.9360628724098206 Validation Loss: 1.3380508422851562\n",
      "Epoch 1180: Training Loss: 0.9356909195582072 Validation Loss: 1.337231159210205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1181: Training Loss: 0.9338506658871969 Validation Loss: 1.3357340097427368\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1182: Training Loss: 0.9325052499771118 Validation Loss: 1.3348640203475952\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1183: Training Loss: 0.9322535395622253 Validation Loss: 1.3338614702224731\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1184: Training Loss: 0.9319340387980143 Validation Loss: 1.333850622177124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1185: Training Loss: 0.9310540358225504 Validation Loss: 1.3326892852783203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1186: Training Loss: 0.9317282438278198 Validation Loss: 1.3326787948608398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1187: Training Loss: 0.9299497604370117 Validation Loss: 1.3328297138214111\n",
      "Epoch 1188: Training Loss: 0.92893519004186 Validation Loss: 1.3323633670806885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1189: Training Loss: 0.9286903937657675 Validation Loss: 1.3303298950195312\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1190: Training Loss: 0.9265444874763489 Validation Loss: 1.329831600189209\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1191: Training Loss: 0.9258476694424947 Validation Loss: 1.330163598060608\n",
      "Epoch 1192: Training Loss: 0.9270368218421936 Validation Loss: 1.330063819885254\n",
      "Epoch 1193: Training Loss: 0.9269424279530843 Validation Loss: 1.3309366703033447\n",
      "Epoch 1194: Training Loss: 0.9254677494366964 Validation Loss: 1.32958984375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1195: Training Loss: 0.9247795144716898 Validation Loss: 1.3284128904342651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1196: Training Loss: 0.9235110481580099 Validation Loss: 1.3269870281219482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1197: Training Loss: 0.9230444629987081 Validation Loss: 1.32627272605896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1198: Training Loss: 0.9226949016253153 Validation Loss: 1.325646996498108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1199: Training Loss: 0.9212884306907654 Validation Loss: 1.3249170780181885\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1200: Training Loss: 0.9202277263005575 Validation Loss: 1.324263095855713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1201: Training Loss: 0.9213798443476359 Validation Loss: 1.3250185251235962\n",
      "Epoch 1202: Training Loss: 0.9189157287279764 Validation Loss: 1.3246300220489502\n",
      "Epoch 1203: Training Loss: 0.9211980899175009 Validation Loss: 1.3238403797149658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1204: Training Loss: 0.9183008074760437 Validation Loss: 1.3230023384094238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1205: Training Loss: 0.9169062773386637 Validation Loss: 1.3219897747039795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1206: Training Loss: 0.9165775378545126 Validation Loss: 1.3212330341339111\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1207: Training Loss: 0.91527392466863 Validation Loss: 1.3212473392486572\n",
      "Epoch 1208: Training Loss: 0.9158757328987122 Validation Loss: 1.3213874101638794\n",
      "Epoch 1209: Training Loss: 0.9156671166419983 Validation Loss: 1.3201028108596802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1210: Training Loss: 0.9139265219370524 Validation Loss: 1.319047212600708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1211: Training Loss: 0.9132391810417175 Validation Loss: 1.3184763193130493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1212: Training Loss: 0.9125153621037801 Validation Loss: 1.318885087966919\n",
      "Epoch 1213: Training Loss: 0.9115574955940247 Validation Loss: 1.3188260793685913\n",
      "Epoch 1214: Training Loss: 0.9098027547200521 Validation Loss: 1.317667841911316\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1215: Training Loss: 0.9091151555379232 Validation Loss: 1.3164364099502563\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1216: Training Loss: 0.908748726050059 Validation Loss: 1.3157299757003784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1217: Training Loss: 0.9083842039108276 Validation Loss: 1.3162778615951538\n",
      "Epoch 1218: Training Loss: 0.908437450726827 Validation Loss: 1.315520167350769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1219: Training Loss: 0.9104233582814535 Validation Loss: 1.3152662515640259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1220: Training Loss: 0.9062369465827942 Validation Loss: 1.3148480653762817\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1221: Training Loss: 0.9063387115796407 Validation Loss: 1.3132117986679077\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1222: Training Loss: 0.9049089352289835 Validation Loss: 1.3134595155715942\n",
      "Epoch 1223: Training Loss: 0.9063586592674255 Validation Loss: 1.3125900030136108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1224: Training Loss: 0.9035210808118185 Validation Loss: 1.311561942100525\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1225: Training Loss: 0.9044233759244283 Validation Loss: 1.3113471269607544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1226: Training Loss: 0.9022212823232015 Validation Loss: 1.3121562004089355\n",
      "Epoch 1227: Training Loss: 0.9032236735026041 Validation Loss: 1.31148362159729\n",
      "Epoch 1228: Training Loss: 0.9025851885477701 Validation Loss: 1.3101577758789062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1229: Training Loss: 0.9011411070823669 Validation Loss: 1.3087387084960938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1230: Training Loss: 0.8997147480646769 Validation Loss: 1.3071917295455933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1231: Training Loss: 0.8994538187980652 Validation Loss: 1.3072724342346191\n",
      "Epoch 1232: Training Loss: 0.8977248271306356 Validation Loss: 1.3080717325210571\n",
      "Epoch 1233: Training Loss: 0.8983457485834757 Validation Loss: 1.3087331056594849\n",
      "Epoch 1234: Training Loss: 0.8989593187967936 Validation Loss: 1.3084427118301392\n",
      "Epoch 1235: Training Loss: 0.8976199626922607 Validation Loss: 1.3073829412460327\n",
      "Epoch 1236: Training Loss: 0.8957911928494772 Validation Loss: 1.3053433895111084\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1237: Training Loss: 0.8957952260971069 Validation Loss: 1.3041861057281494\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1238: Training Loss: 0.894468347231547 Validation Loss: 1.3039861917495728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1239: Training Loss: 0.8936328291893005 Validation Loss: 1.3039329051971436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1240: Training Loss: 0.8931397596995035 Validation Loss: 1.303558349609375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1241: Training Loss: 0.8929052551587423 Validation Loss: 1.3026947975158691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1242: Training Loss: 0.8942598501841227 Validation Loss: 1.302314043045044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1243: Training Loss: 0.891447106997172 Validation Loss: 1.3014289140701294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1244: Training Loss: 0.8902804454167684 Validation Loss: 1.3003402948379517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1245: Training Loss: 0.8897005319595337 Validation Loss: 1.2997925281524658\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1246: Training Loss: 0.8892959157625834 Validation Loss: 1.3002104759216309\n",
      "Epoch 1247: Training Loss: 0.8878004749615988 Validation Loss: 1.3003973960876465\n",
      "Epoch 1248: Training Loss: 0.8879051804542542 Validation Loss: 1.3000807762145996\n",
      "Epoch 1249: Training Loss: 0.8869789242744446 Validation Loss: 1.299467921257019\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1250: Training Loss: 0.8861694733301798 Validation Loss: 1.2981947660446167\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1251: Training Loss: 0.8855577707290649 Validation Loss: 1.2973880767822266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1252: Training Loss: 0.8854623238245646 Validation Loss: 1.297304391860962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1253: Training Loss: 0.8837762475013733 Validation Loss: 1.2968013286590576\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1254: Training Loss: 0.8843717773755392 Validation Loss: 1.2965501546859741\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1255: Training Loss: 0.8844770391782125 Validation Loss: 1.2964403629302979\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1256: Training Loss: 0.8834455609321594 Validation Loss: 1.2955909967422485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1257: Training Loss: 0.8830506602923075 Validation Loss: 1.2941036224365234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1258: Training Loss: 0.8838346799214681 Validation Loss: 1.293927788734436\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1259: Training Loss: 0.8788426121075948 Validation Loss: 1.2943202257156372\n",
      "Epoch 1260: Training Loss: 0.8807660937309265 Validation Loss: 1.2945255041122437\n",
      "Epoch 1261: Training Loss: 0.8813724517822266 Validation Loss: 1.2933562994003296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1262: Training Loss: 0.8786359429359436 Validation Loss: 1.2923524379730225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1263: Training Loss: 0.877403974533081 Validation Loss: 1.2914559841156006\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1264: Training Loss: 0.8787945111592611 Validation Loss: 1.2911826372146606\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1265: Training Loss: 0.8785934845606486 Validation Loss: 1.2905272245407104\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1266: Training Loss: 0.8745872179667155 Validation Loss: 1.2899563312530518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1267: Training Loss: 0.876790185769399 Validation Loss: 1.2895140647888184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1268: Training Loss: 0.8738648494084676 Validation Loss: 1.2890297174453735\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1269: Training Loss: 0.8767085274060568 Validation Loss: 1.2885390520095825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1270: Training Loss: 0.873649537563324 Validation Loss: 1.2875699996948242\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1271: Training Loss: 0.8722608089447021 Validation Loss: 1.2867765426635742\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1272: Training Loss: 0.8729745149612427 Validation Loss: 1.2866852283477783\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1273: Training Loss: 0.8712703585624695 Validation Loss: 1.2868845462799072\n",
      "Epoch 1274: Training Loss: 0.8712929089864095 Validation Loss: 1.2854963541030884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1275: Training Loss: 0.8699630498886108 Validation Loss: 1.2861239910125732\n",
      "Epoch 1276: Training Loss: 0.8709635535875956 Validation Loss: 1.2856980562210083\n",
      "Epoch 1277: Training Loss: 0.8664854367574056 Validation Loss: 1.284816026687622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1278: Training Loss: 0.8681663473447164 Validation Loss: 1.2833560705184937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1279: Training Loss: 0.8666942914326986 Validation Loss: 1.283111810684204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1280: Training Loss: 0.8670312960942587 Validation Loss: 1.2835726737976074\n",
      "Epoch 1281: Training Loss: 0.866303543249766 Validation Loss: 1.2837105989456177\n",
      "Epoch 1282: Training Loss: 0.8648021618525187 Validation Loss: 1.283610224723816\n",
      "Epoch 1283: Training Loss: 0.8650906085968018 Validation Loss: 1.2823585271835327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1284: Training Loss: 0.8641611933708191 Validation Loss: 1.281280279159546\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1285: Training Loss: 0.8655261397361755 Validation Loss: 1.2803537845611572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1286: Training Loss: 0.863249401251475 Validation Loss: 1.2801361083984375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1287: Training Loss: 0.8640388051668803 Validation Loss: 1.2795417308807373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1288: Training Loss: 0.861666758855184 Validation Loss: 1.2791391611099243\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1289: Training Loss: 0.8618199229240417 Validation Loss: 1.2785784006118774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1290: Training Loss: 0.8611215949058533 Validation Loss: 1.2790004014968872\n",
      "Epoch 1291: Training Loss: 0.8603903849919637 Validation Loss: 1.2781682014465332\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1292: Training Loss: 0.8595099051793417 Validation Loss: 1.2782632112503052\n",
      "Epoch 1293: Training Loss: 0.8607110579808553 Validation Loss: 1.277403712272644\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1294: Training Loss: 0.8578945398330688 Validation Loss: 1.275928020477295\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1295: Training Loss: 0.8563328385353088 Validation Loss: 1.2749199867248535\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1296: Training Loss: 0.858338475227356 Validation Loss: 1.2732943296432495\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1297: Training Loss: 0.8572855194409689 Validation Loss: 1.2735886573791504\n",
      "Epoch 1298: Training Loss: 0.8572575251261393 Validation Loss: 1.2739607095718384\n",
      "Epoch 1299: Training Loss: 0.8556968172391256 Validation Loss: 1.2741663455963135\n",
      "Epoch 1300: Training Loss: 0.8541171749432882 Validation Loss: 1.2743802070617676\n",
      "Epoch 1301: Training Loss: 0.8551338911056519 Validation Loss: 1.273439884185791\n",
      "Epoch 1302: Training Loss: 0.8522608677546183 Validation Loss: 1.2734675407409668\n",
      "Epoch 1303: Training Loss: 0.8503994146982828 Validation Loss: 1.273008942604065\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1304: Training Loss: 0.851410706837972 Validation Loss: 1.2725462913513184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1305: Training Loss: 0.8510076602300009 Validation Loss: 1.271297574043274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1306: Training Loss: 0.8507445255915324 Validation Loss: 1.270100712776184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1307: Training Loss: 0.8492222229639689 Validation Loss: 1.2694344520568848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1308: Training Loss: 0.8500117262204488 Validation Loss: 1.269395351409912\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1309: Training Loss: 0.8483588298161825 Validation Loss: 1.2687957286834717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1310: Training Loss: 0.8480284015337626 Validation Loss: 1.2691619396209717\n",
      "Epoch 1311: Training Loss: 0.8473438024520874 Validation Loss: 1.2691857814788818\n",
      "Epoch 1312: Training Loss: 0.8463296294212341 Validation Loss: 1.2681117057800293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1313: Training Loss: 0.8456835945447286 Validation Loss: 1.267330288887024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1314: Training Loss: 0.8453778425852457 Validation Loss: 1.2657238245010376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1315: Training Loss: 0.8441673517227173 Validation Loss: 1.2652167081832886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1316: Training Loss: 0.8447079658508301 Validation Loss: 1.2649487257003784\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1317: Training Loss: 0.8442100286483765 Validation Loss: 1.2658025026321411\n",
      "Epoch 1318: Training Loss: 0.8439268271128336 Validation Loss: 1.2651710510253906\n",
      "Epoch 1319: Training Loss: 0.8421651919682821 Validation Loss: 1.2637717723846436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1320: Training Loss: 0.844110349814097 Validation Loss: 1.2633757591247559\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1321: Training Loss: 0.8421051899592081 Validation Loss: 1.2631837129592896\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1322: Training Loss: 0.840005119641622 Validation Loss: 1.2635202407836914\n",
      "Epoch 1323: Training Loss: 0.8395985166231791 Validation Loss: 1.2631999254226685\n",
      "Epoch 1324: Training Loss: 0.8390783468882242 Validation Loss: 1.2631404399871826\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1325: Training Loss: 0.8389638463656107 Validation Loss: 1.262255072593689\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1326: Training Loss: 0.8378649155298868 Validation Loss: 1.262118935585022\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1327: Training Loss: 0.8376391331354777 Validation Loss: 1.2611327171325684\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1328: Training Loss: 0.838586668173472 Validation Loss: 1.2604966163635254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1329: Training Loss: 0.8362259268760681 Validation Loss: 1.258867859840393\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1330: Training Loss: 0.8356374303499857 Validation Loss: 1.2585928440093994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1331: Training Loss: 0.8353768984476725 Validation Loss: 1.2586182355880737\n",
      "Epoch 1332: Training Loss: 0.8343275189399719 Validation Loss: 1.2581818103790283\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1333: Training Loss: 0.8339486718177795 Validation Loss: 1.2585349082946777\n",
      "Epoch 1334: Training Loss: 0.8335973024368286 Validation Loss: 1.256738543510437\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1335: Training Loss: 0.8347907861073812 Validation Loss: 1.2561094760894775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1336: Training Loss: 0.8326113422711691 Validation Loss: 1.256130576133728\n",
      "Epoch 1337: Training Loss: 0.831642727057139 Validation Loss: 1.255721092224121\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1338: Training Loss: 0.8307491739590963 Validation Loss: 1.2550252676010132\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1339: Training Loss: 0.8299335638682047 Validation Loss: 1.2553973197937012\n",
      "Epoch 1340: Training Loss: 0.8332869211832682 Validation Loss: 1.2548739910125732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1341: Training Loss: 0.8269126216570536 Validation Loss: 1.2541594505310059\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1342: Training Loss: 0.8286149700482687 Validation Loss: 1.2535276412963867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1343: Training Loss: 0.8289113243420919 Validation Loss: 1.2523828744888306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1344: Training Loss: 0.8272219101587931 Validation Loss: 1.251895546913147\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1345: Training Loss: 0.8261775970458984 Validation Loss: 1.252225637435913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1346: Training Loss: 0.8295769890149435 Validation Loss: 1.2534798383712769\n",
      "Epoch 1347: Training Loss: 0.8254521489143372 Validation Loss: 1.2528165578842163\n",
      "Epoch 1348: Training Loss: 0.8258874217669169 Validation Loss: 1.252007246017456\n",
      "Epoch 1349: Training Loss: 0.8250025113423666 Validation Loss: 1.2513152360916138\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1350: Training Loss: 0.8233140707015991 Validation Loss: 1.250374674797058\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1351: Training Loss: 0.8233515818913778 Validation Loss: 1.2499351501464844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1352: Training Loss: 0.8251412510871887 Validation Loss: 1.2498987913131714\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1353: Training Loss: 0.8219941457112631 Validation Loss: 1.2496944665908813\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1354: Training Loss: 0.8224064509073893 Validation Loss: 1.2486169338226318\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1355: Training Loss: 0.820364753405253 Validation Loss: 1.2476515769958496\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1356: Training Loss: 0.8195118506749471 Validation Loss: 1.246478796005249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1357: Training Loss: 0.8192852735519409 Validation Loss: 1.2462280988693237\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1358: Training Loss: 0.8190658291180929 Validation Loss: 1.246081829071045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1359: Training Loss: 0.8182684580485026 Validation Loss: 1.2461684942245483\n",
      "Epoch 1360: Training Loss: 0.8183837532997131 Validation Loss: 1.2463449239730835\n",
      "Epoch 1361: Training Loss: 0.8183475136756897 Validation Loss: 1.2464020252227783\n",
      "Epoch 1362: Training Loss: 0.8152835567792257 Validation Loss: 1.2451303005218506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1363: Training Loss: 0.815157949924469 Validation Loss: 1.2437925338745117\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1364: Training Loss: 0.8152700066566467 Validation Loss: 1.2438032627105713\n",
      "Epoch 1365: Training Loss: 0.8146199385325114 Validation Loss: 1.2434208393096924\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1366: Training Loss: 0.8148564696311951 Validation Loss: 1.2436609268188477\n",
      "Epoch 1367: Training Loss: 0.8126400510470072 Validation Loss: 1.243412733078003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1368: Training Loss: 0.8125456770261129 Validation Loss: 1.2436423301696777\n",
      "Epoch 1369: Training Loss: 0.8143086433410645 Validation Loss: 1.2431411743164062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1370: Training Loss: 0.8120113213857015 Validation Loss: 1.2414251565933228\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1371: Training Loss: 0.8111566305160522 Validation Loss: 1.2401076555252075\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1372: Training Loss: 0.8105563322703043 Validation Loss: 1.2393696308135986\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1373: Training Loss: 0.811306357383728 Validation Loss: 1.2395731210708618\n",
      "Epoch 1374: Training Loss: 0.8111733396848043 Validation Loss: 1.2396694421768188\n",
      "Epoch 1375: Training Loss: 0.8091851870218912 Validation Loss: 1.239537239074707\n",
      "Epoch 1376: Training Loss: 0.8091854651769003 Validation Loss: 1.2395739555358887\n",
      "Epoch 1377: Training Loss: 0.8084066311518351 Validation Loss: 1.2391011714935303\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1378: Training Loss: 0.8069198131561279 Validation Loss: 1.2382705211639404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1379: Training Loss: 0.8080213268597921 Validation Loss: 1.2378325462341309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1380: Training Loss: 0.8063752055168152 Validation Loss: 1.237859845161438\n",
      "Epoch 1381: Training Loss: 0.8066918253898621 Validation Loss: 1.2367204427719116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1382: Training Loss: 0.8089673717816671 Validation Loss: 1.2364599704742432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1383: Training Loss: 0.8053390781084696 Validation Loss: 1.2364304065704346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1384: Training Loss: 0.8022538423538208 Validation Loss: 1.2351880073547363\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1385: Training Loss: 0.8035032351811727 Validation Loss: 1.2350823879241943\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1386: Training Loss: 0.8030317823092142 Validation Loss: 1.2347960472106934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1387: Training Loss: 0.8024142980575562 Validation Loss: 1.2341630458831787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1388: Training Loss: 0.8015279173851013 Validation Loss: 1.232886552810669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1389: Training Loss: 0.8011377851168314 Validation Loss: 1.2324453592300415\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1390: Training Loss: 0.8020686109860738 Validation Loss: 1.2330174446105957\n",
      "Epoch 1391: Training Loss: 0.8007911642392477 Validation Loss: 1.2321748733520508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1392: Training Loss: 0.7989322543144226 Validation Loss: 1.2333711385726929\n",
      "Epoch 1393: Training Loss: 0.7999118566513062 Validation Loss: 1.2330398559570312\n",
      "Epoch 1394: Training Loss: 0.7977272868156433 Validation Loss: 1.2310746908187866\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1395: Training Loss: 0.8001105785369873 Validation Loss: 1.229567527770996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1396: Training Loss: 0.797576109568278 Validation Loss: 1.2300889492034912\n",
      "Epoch 1397: Training Loss: 0.7966622114181519 Validation Loss: 1.2305186986923218\n",
      "Epoch 1398: Training Loss: 0.7963205178578695 Validation Loss: 1.2304060459136963\n",
      "Epoch 1399: Training Loss: 0.7944482962290446 Validation Loss: 1.2296106815338135\n",
      "Epoch 1400: Training Loss: 0.7948317329088846 Validation Loss: 1.2291252613067627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1401: Training Loss: 0.7952781716982523 Validation Loss: 1.22761869430542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1402: Training Loss: 0.7931473255157471 Validation Loss: 1.2267446517944336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1403: Training Loss: 0.7929250597953796 Validation Loss: 1.2266018390655518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1404: Training Loss: 0.7926432092984518 Validation Loss: 1.2263027429580688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1405: Training Loss: 0.7918024659156799 Validation Loss: 1.2268011569976807\n",
      "Epoch 1406: Training Loss: 0.7910969853401184 Validation Loss: 1.2260029315948486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1407: Training Loss: 0.790986180305481 Validation Loss: 1.2258142232894897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1408: Training Loss: 0.791442076365153 Validation Loss: 1.2262426614761353\n",
      "Epoch 1409: Training Loss: 0.7896136244138082 Validation Loss: 1.22707998752594\n",
      "Epoch 1410: Training Loss: 0.7906700770060221 Validation Loss: 1.2250288724899292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1411: Training Loss: 0.7897131443023682 Validation Loss: 1.2236125469207764\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1412: Training Loss: 0.7877095739046732 Validation Loss: 1.2216880321502686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1413: Training Loss: 0.7875722845395406 Validation Loss: 1.220881462097168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1414: Training Loss: 0.7874553998311361 Validation Loss: 1.2219946384429932\n",
      "Epoch 1415: Training Loss: 0.7867730061213175 Validation Loss: 1.224092721939087\n",
      "Epoch 1416: Training Loss: 0.7865186929702759 Validation Loss: 1.2242738008499146\n",
      "Epoch 1417: Training Loss: 0.7853483955065409 Validation Loss: 1.2233493328094482\n",
      "Epoch 1418: Training Loss: 0.7849656144777933 Validation Loss: 1.2221347093582153\n",
      "Epoch 1419: Training Loss: 0.78619917233785 Validation Loss: 1.2196308374404907\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1420: Training Loss: 0.7841429710388184 Validation Loss: 1.2189868688583374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1421: Training Loss: 0.7824483116467794 Validation Loss: 1.2190048694610596\n",
      "Epoch 1422: Training Loss: 0.7852062781651815 Validation Loss: 1.2193801403045654\n",
      "Epoch 1423: Training Loss: 0.7816973527272543 Validation Loss: 1.2196201086044312\n",
      "Epoch 1424: Training Loss: 0.7812304496765137 Validation Loss: 1.218976616859436\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1425: Training Loss: 0.780712882677714 Validation Loss: 1.2194794416427612\n",
      "Epoch 1426: Training Loss: 0.7819159428278605 Validation Loss: 1.2191568613052368\n",
      "Epoch 1427: Training Loss: 0.779923141002655 Validation Loss: 1.2175724506378174\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1428: Training Loss: 0.7797418832778931 Validation Loss: 1.2171376943588257\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1429: Training Loss: 0.7785244186719259 Validation Loss: 1.2162928581237793\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1430: Training Loss: 0.7775729099909464 Validation Loss: 1.2159035205841064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1431: Training Loss: 0.7795557578404745 Validation Loss: 1.2161024808883667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1432: Training Loss: 0.7769492467244467 Validation Loss: 1.2153681516647339\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1433: Training Loss: 0.7766475280125936 Validation Loss: 1.21592378616333\n",
      "Epoch 1434: Training Loss: 0.7754455804824829 Validation Loss: 1.215064287185669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1435: Training Loss: 0.7754429578781128 Validation Loss: 1.2138034105300903\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1436: Training Loss: 0.774881104628245 Validation Loss: 1.2127176523208618\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1437: Training Loss: 0.7742289106051127 Validation Loss: 1.2128735780715942\n",
      "Epoch 1438: Training Loss: 0.7735806703567505 Validation Loss: 1.2134588956832886\n",
      "Epoch 1439: Training Loss: 0.7760199904441833 Validation Loss: 1.212904453277588\n",
      "Epoch 1440: Training Loss: 0.7733838558197021 Validation Loss: 1.212719440460205\n",
      "Epoch 1441: Training Loss: 0.7727808554967245 Validation Loss: 1.2128888368606567\n",
      "Epoch 1442: Training Loss: 0.7714274128278097 Validation Loss: 1.2124196290969849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1443: Training Loss: 0.7727887630462646 Validation Loss: 1.2127044200897217\n",
      "Epoch 1444: Training Loss: 0.7708585858345032 Validation Loss: 1.2110768556594849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1445: Training Loss: 0.7706606586774191 Validation Loss: 1.2106267213821411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1446: Training Loss: 0.771492600440979 Validation Loss: 1.2097160816192627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1447: Training Loss: 0.7693405946095785 Validation Loss: 1.2093874216079712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1448: Training Loss: 0.7689669926961263 Validation Loss: 1.2099248170852661\n",
      "Epoch 1449: Training Loss: 0.768251379330953 Validation Loss: 1.2090798616409302\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1450: Training Loss: 0.7699527541796366 Validation Loss: 1.2084909677505493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1451: Training Loss: 0.7673924962679545 Validation Loss: 1.207453727722168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1452: Training Loss: 0.766426165898641 Validation Loss: 1.2081480026245117\n",
      "Epoch 1453: Training Loss: 0.765222450097402 Validation Loss: 1.2087292671203613\n",
      "Epoch 1454: Training Loss: 0.7648609081904093 Validation Loss: 1.2079490423202515\n",
      "Epoch 1455: Training Loss: 0.7645473877588908 Validation Loss: 1.2060621976852417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1456: Training Loss: 0.764232337474823 Validation Loss: 1.2055302858352661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1457: Training Loss: 0.7658334573109945 Validation Loss: 1.204929232597351\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1458: Training Loss: 0.7634815176328024 Validation Loss: 1.2053091526031494\n",
      "Epoch 1459: Training Loss: 0.762583057085673 Validation Loss: 1.2054662704467773\n",
      "Epoch 1460: Training Loss: 0.7639111280441284 Validation Loss: 1.2055095434188843\n",
      "Epoch 1461: Training Loss: 0.7609696984291077 Validation Loss: 1.2049816846847534\n",
      "Epoch 1462: Training Loss: 0.7609856327374777 Validation Loss: 1.2038042545318604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1463: Training Loss: 0.7613546848297119 Validation Loss: 1.2034015655517578\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1464: Training Loss: 0.760745644569397 Validation Loss: 1.2028698921203613\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1465: Training Loss: 0.759890596071879 Validation Loss: 1.2020235061645508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1466: Training Loss: 0.7586989601453146 Validation Loss: 1.2020364999771118\n",
      "Epoch 1467: Training Loss: 0.7583949367205302 Validation Loss: 1.2025443315505981\n",
      "Epoch 1468: Training Loss: 0.757823129494985 Validation Loss: 1.20223867893219\n",
      "Epoch 1469: Training Loss: 0.7574703693389893 Validation Loss: 1.2019453048706055\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1470: Training Loss: 0.7568664153416952 Validation Loss: 1.2016208171844482\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1471: Training Loss: 0.7565788427988688 Validation Loss: 1.2004032135009766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1472: Training Loss: 0.7569764256477356 Validation Loss: 1.1990829706192017\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1473: Training Loss: 0.7555229663848877 Validation Loss: 1.198907732963562\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1474: Training Loss: 0.7554253538449606 Validation Loss: 1.1988940238952637\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1475: Training Loss: 0.7534732222557068 Validation Loss: 1.1987354755401611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1476: Training Loss: 0.7550872365633646 Validation Loss: 1.1991024017333984\n",
      "Epoch 1477: Training Loss: 0.7534432212511698 Validation Loss: 1.199093222618103\n",
      "Epoch 1478: Training Loss: 0.7529414296150208 Validation Loss: 1.1990193128585815\n",
      "Epoch 1479: Training Loss: 0.7522232135136923 Validation Loss: 1.1979984045028687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1480: Training Loss: 0.751686672369639 Validation Loss: 1.1970677375793457\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1481: Training Loss: 0.7505991657574972 Validation Loss: 1.1969794034957886\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1482: Training Loss: 0.750226636727651 Validation Loss: 1.1961373090744019\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1483: Training Loss: 0.7484635313351949 Validation Loss: 1.1959460973739624\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1484: Training Loss: 0.7497033874193827 Validation Loss: 1.1954232454299927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1485: Training Loss: 0.7505820393562317 Validation Loss: 1.1953741312026978\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1486: Training Loss: 0.7504712343215942 Validation Loss: 1.1953685283660889\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1487: Training Loss: 0.7499670584996542 Validation Loss: 1.1951700448989868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1488: Training Loss: 0.7493413885434469 Validation Loss: 1.1936546564102173\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1489: Training Loss: 0.7464235424995422 Validation Loss: 1.1932690143585205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1490: Training Loss: 0.749177873134613 Validation Loss: 1.1932955980300903\n",
      "Epoch 1491: Training Loss: 0.74602343638738 Validation Loss: 1.192729115486145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1492: Training Loss: 0.7467873493830363 Validation Loss: 1.1930714845657349\n",
      "Epoch 1493: Training Loss: 0.7446456551551819 Validation Loss: 1.1927939653396606\n",
      "Epoch 1494: Training Loss: 0.7437496582667033 Validation Loss: 1.1923195123672485\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1495: Training Loss: 0.7446453173955282 Validation Loss: 1.1926934719085693\n",
      "Epoch 1496: Training Loss: 0.7428335547447205 Validation Loss: 1.1923264265060425\n",
      "Epoch 1497: Training Loss: 0.7443526784578959 Validation Loss: 1.1910823583602905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1498: Training Loss: 0.7430483897527059 Validation Loss: 1.190544843673706\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1499: Training Loss: 0.741937259833018 Validation Loss: 1.1900047063827515\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1500: Training Loss: 0.7411208152770996 Validation Loss: 1.190316081047058\n",
      "Epoch 1501: Training Loss: 0.7409944732983907 Validation Loss: 1.1901419162750244\n",
      "Epoch 1502: Training Loss: 0.7409168481826782 Validation Loss: 1.188857078552246\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1503: Training Loss: 0.7394362886746725 Validation Loss: 1.1881499290466309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1504: Training Loss: 0.7372205853462219 Validation Loss: 1.1885615587234497\n",
      "Epoch 1505: Training Loss: 0.739011287689209 Validation Loss: 1.189152479171753\n",
      "Epoch 1506: Training Loss: 0.7381934523582458 Validation Loss: 1.1880791187286377\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1507: Training Loss: 0.7373690406481425 Validation Loss: 1.187816858291626\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1508: Training Loss: 0.7397768696149191 Validation Loss: 1.1876189708709717\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1509: Training Loss: 0.7362574338912964 Validation Loss: 1.1869515180587769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1510: Training Loss: 0.7394357721010844 Validation Loss: 1.1860977411270142\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1511: Training Loss: 0.7365524371465048 Validation Loss: 1.1849946975708008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1512: Training Loss: 0.7358475724856058 Validation Loss: 1.1851005554199219\n",
      "Epoch 1513: Training Loss: 0.7348952094713846 Validation Loss: 1.1853121519088745\n",
      "Epoch 1514: Training Loss: 0.7344866196314493 Validation Loss: 1.1848268508911133\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1515: Training Loss: 0.7349363565444946 Validation Loss: 1.1854301691055298\n",
      "Epoch 1516: Training Loss: 0.7337708473205566 Validation Loss: 1.1844924688339233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1517: Training Loss: 0.7328197161356608 Validation Loss: 1.1847180128097534\n",
      "Epoch 1518: Training Loss: 0.7349870800971985 Validation Loss: 1.183605670928955\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1519: Training Loss: 0.7328047752380371 Validation Loss: 1.182650089263916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1520: Training Loss: 0.7317737738291422 Validation Loss: 1.1822293996810913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 1521: Training Loss: 0.7316614786783854 Validation Loss: 1.182056188583374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1522: Training Loss: 0.7309145530064901 Validation Loss: 1.181928038597107\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1523: Training Loss: 0.7305442889531454 Validation Loss: 1.1815706491470337\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1524: Training Loss: 0.7303047378857931 Validation Loss: 1.1805472373962402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1525: Training Loss: 0.7295885682106018 Validation Loss: 1.180874228477478\n",
      "Epoch 1526: Training Loss: 0.7293533484141032 Validation Loss: 1.1812870502471924\n",
      "Epoch 1527: Training Loss: 0.7320794065793356 Validation Loss: 1.1807948350906372\n",
      "Epoch 1528: Training Loss: 0.7278655568758646 Validation Loss: 1.1811233758926392\n",
      "Epoch 1529: Training Loss: 0.7272753715515137 Validation Loss: 1.1804285049438477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1530: Training Loss: 0.7313246528307596 Validation Loss: 1.1799625158309937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1531: Training Loss: 0.7268870870272318 Validation Loss: 1.1789332628250122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1532: Training Loss: 0.7275377511978149 Validation Loss: 1.177963376045227\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1533: Training Loss: 0.724563201268514 Validation Loss: 1.1776373386383057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1534: Training Loss: 0.7245559493700663 Validation Loss: 1.1771715879440308\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1535: Training Loss: 0.7242697874704996 Validation Loss: 1.1780493259429932\n",
      "Epoch 1536: Training Loss: 0.7251596252123514 Validation Loss: 1.1779953241348267\n",
      "Epoch 1537: Training Loss: 0.7237566510836283 Validation Loss: 1.176999807357788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1538: Training Loss: 0.7239066362380981 Validation Loss: 1.1764887571334839\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1539: Training Loss: 0.7217442790667216 Validation Loss: 1.175310730934143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1540: Training Loss: 0.7223073045412699 Validation Loss: 1.1756560802459717\n",
      "Epoch 1541: Training Loss: 0.7208454608917236 Validation Loss: 1.175803303718567\n",
      "Epoch 1542: Training Loss: 0.7214936017990112 Validation Loss: 1.1755764484405518\n",
      "Epoch 1543: Training Loss: 0.720036784807841 Validation Loss: 1.1756815910339355\n",
      "Epoch 1544: Training Loss: 0.7189378539721171 Validation Loss: 1.1751537322998047\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1545: Training Loss: 0.7201423843701681 Validation Loss: 1.1750125885009766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1546: Training Loss: 0.7202872832616171 Validation Loss: 1.1747993230819702\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1547: Training Loss: 0.7182272672653198 Validation Loss: 1.17476487159729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1548: Training Loss: 0.7191211581230164 Validation Loss: 1.173880696296692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1549: Training Loss: 0.7183066209157308 Validation Loss: 1.1719553470611572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1550: Training Loss: 0.7175055543581644 Validation Loss: 1.171118974685669\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1551: Training Loss: 0.7160817384719849 Validation Loss: 1.171280026435852\n",
      "Epoch 1552: Training Loss: 0.7143405675888062 Validation Loss: 1.171126127243042\n",
      "Epoch 1553: Training Loss: 0.7151813507080078 Validation Loss: 1.1715549230575562\n",
      "Epoch 1554: Training Loss: 0.715155025323232 Validation Loss: 1.1718204021453857\n",
      "Epoch 1555: Training Loss: 0.7136305173238119 Validation Loss: 1.1713964939117432\n",
      "Epoch 1556: Training Loss: 0.7139801581700643 Validation Loss: 1.171006679534912\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1557: Training Loss: 0.7136540611584982 Validation Loss: 1.170685052871704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1558: Training Loss: 0.7152921160062155 Validation Loss: 1.1703563928604126\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1559: Training Loss: 0.7106354435284933 Validation Loss: 1.1702674627304077\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1560: Training Loss: 0.7122284770011902 Validation Loss: 1.1697274446487427\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1561: Training Loss: 0.7111953695615133 Validation Loss: 1.1686394214630127\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1562: Training Loss: 0.7114376028378805 Validation Loss: 1.1686654090881348\n",
      "Epoch 1563: Training Loss: 0.7124826510747274 Validation Loss: 1.1682031154632568\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1564: Training Loss: 0.7096742391586304 Validation Loss: 1.1682771444320679\n",
      "Epoch 1565: Training Loss: 0.7075858116149902 Validation Loss: 1.1676418781280518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1566: Training Loss: 0.70905601978302 Validation Loss: 1.1672563552856445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1567: Training Loss: 0.7080573836962382 Validation Loss: 1.167648196220398\n",
      "Epoch 1568: Training Loss: 0.7096764445304871 Validation Loss: 1.1668243408203125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1569: Training Loss: 0.7074116071065267 Validation Loss: 1.1657217741012573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1570: Training Loss: 0.706764300664266 Validation Loss: 1.1657720804214478\n",
      "Epoch 1571: Training Loss: 0.7059861620267233 Validation Loss: 1.1658533811569214\n",
      "Epoch 1572: Training Loss: 0.7063213785489401 Validation Loss: 1.165789008140564\n",
      "Epoch 1573: Training Loss: 0.7074016332626343 Validation Loss: 1.1654750108718872\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1574: Training Loss: 0.7062226931254069 Validation Loss: 1.1642396450042725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1575: Training Loss: 0.704938272635142 Validation Loss: 1.163686990737915\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1576: Training Loss: 0.705619235833486 Validation Loss: 1.1642895936965942\n",
      "Epoch 1577: Training Loss: 0.7038782437642416 Validation Loss: 1.164821982383728\n",
      "Epoch 1578: Training Loss: 0.7060516675313314 Validation Loss: 1.1645954847335815\n",
      "Epoch 1579: Training Loss: 0.7028417189915975 Validation Loss: 1.1632179021835327\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1580: Training Loss: 0.7025617758433024 Validation Loss: 1.1622673273086548\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1581: Training Loss: 0.7027356425921122 Validation Loss: 1.1610345840454102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1582: Training Loss: 0.7044735749562582 Validation Loss: 1.1610583066940308\n",
      "Epoch 1583: Training Loss: 0.7035304506619772 Validation Loss: 1.1624432802200317\n",
      "Epoch 1584: Training Loss: 0.7008117238680521 Validation Loss: 1.1617944240570068\n",
      "Epoch 1585: Training Loss: 0.700641413529714 Validation Loss: 1.16274893283844\n",
      "Epoch 1586: Training Loss: 0.701264480749766 Validation Loss: 1.1619244813919067\n",
      "Epoch 1587: Training Loss: 0.7012836138407389 Validation Loss: 1.1601730585098267\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1588: Training Loss: 0.6991256475448608 Validation Loss: 1.159090280532837\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1589: Training Loss: 0.6998884280522665 Validation Loss: 1.159227728843689\n",
      "Epoch 1590: Training Loss: 0.6985652844111124 Validation Loss: 1.15982186794281\n",
      "Epoch 1591: Training Loss: 0.6981344223022461 Validation Loss: 1.1591403484344482\n",
      "Epoch 1592: Training Loss: 0.696861982345581 Validation Loss: 1.1585429906845093\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1593: Training Loss: 0.6981505552927653 Validation Loss: 1.1584562063217163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1594: Training Loss: 0.6973931590716044 Validation Loss: 1.1585323810577393\n",
      "Epoch 1595: Training Loss: 0.6981852054595947 Validation Loss: 1.1599438190460205\n",
      "Epoch 1596: Training Loss: 0.6994953354199728 Validation Loss: 1.1587460041046143\n",
      "Epoch 1597: Training Loss: 0.6949503819147745 Validation Loss: 1.1578046083450317\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1598: Training Loss: 0.6938952803611755 Validation Loss: 1.1568790674209595\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1599: Training Loss: 0.6954610149065653 Validation Loss: 1.1565992832183838\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1600: Training Loss: 0.6937077641487122 Validation Loss: 1.1571877002716064\n",
      "Epoch 1601: Training Loss: 0.6956391533215841 Validation Loss: 1.1567373275756836\n",
      "Epoch 1602: Training Loss: 0.6921993891398112 Validation Loss: 1.156179428100586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1603: Training Loss: 0.6924402515093485 Validation Loss: 1.1555668115615845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1604: Training Loss: 0.6912621458371481 Validation Loss: 1.1546638011932373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1605: Training Loss: 0.6921326518058777 Validation Loss: 1.1543768644332886\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1606: Training Loss: 0.6909589370091757 Validation Loss: 1.153833031654358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1607: Training Loss: 0.6929677128791809 Validation Loss: 1.155009150505066\n",
      "Epoch 1608: Training Loss: 0.688745359579722 Validation Loss: 1.1548206806182861\n",
      "Epoch 1609: Training Loss: 0.6910939812660217 Validation Loss: 1.1543501615524292\n",
      "Epoch 1610: Training Loss: 0.6885617772738138 Validation Loss: 1.1545084714889526\n",
      "Epoch 1611: Training Loss: 0.6876775821050009 Validation Loss: 1.1533162593841553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1612: Training Loss: 0.6881085236867269 Validation Loss: 1.152126669883728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1613: Training Loss: 0.6868647138277689 Validation Loss: 1.1520085334777832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1614: Training Loss: 0.6861804922421774 Validation Loss: 1.1510766744613647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1615: Training Loss: 0.6861232916514078 Validation Loss: 1.1500834226608276\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1616: Training Loss: 0.6867766976356506 Validation Loss: 1.149794340133667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1617: Training Loss: 0.685071070988973 Validation Loss: 1.1499874591827393\n",
      "Epoch 1618: Training Loss: 0.6854736606280009 Validation Loss: 1.1504528522491455\n",
      "Epoch 1619: Training Loss: 0.685067613919576 Validation Loss: 1.1495555639266968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1620: Training Loss: 0.6844829122225443 Validation Loss: 1.1499284505844116\n",
      "Epoch 1621: Training Loss: 0.6847842335700989 Validation Loss: 1.1494560241699219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1622: Training Loss: 0.6831031441688538 Validation Loss: 1.1487295627593994\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1623: Training Loss: 0.6832744479179382 Validation Loss: 1.1479556560516357\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1624: Training Loss: 0.6828993757565817 Validation Loss: 1.1479549407958984\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1625: Training Loss: 0.6813204288482666 Validation Loss: 1.1478334665298462\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1626: Training Loss: 0.6811724503835043 Validation Loss: 1.1481726169586182\n",
      "Epoch 1627: Training Loss: 0.6812420686086019 Validation Loss: 1.1473252773284912\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1628: Training Loss: 0.6819125016530355 Validation Loss: 1.1461732387542725\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1629: Training Loss: 0.6794659495353699 Validation Loss: 1.1454310417175293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1630: Training Loss: 0.6800156037012736 Validation Loss: 1.14470636844635\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1631: Training Loss: 0.6801188985506693 Validation Loss: 1.1443864107131958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1632: Training Loss: 0.6781879663467407 Validation Loss: 1.1449296474456787\n",
      "Epoch 1633: Training Loss: 0.6798421144485474 Validation Loss: 1.1456263065338135\n",
      "Epoch 1634: Training Loss: 0.6783532301584879 Validation Loss: 1.1453872919082642\n",
      "Epoch 1635: Training Loss: 0.6770341793696085 Validation Loss: 1.1442419290542603\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1636: Training Loss: 0.6769505143165588 Validation Loss: 1.144782543182373\n",
      "Epoch 1637: Training Loss: 0.6764186024665833 Validation Loss: 1.1441833972930908\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1638: Training Loss: 0.6765695412953695 Validation Loss: 1.143584132194519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1639: Training Loss: 0.6773255864779154 Validation Loss: 1.1418964862823486\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1640: Training Loss: 0.6754972736040751 Validation Loss: 1.1415033340454102\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1641: Training Loss: 0.6740131576855978 Validation Loss: 1.1413421630859375\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1642: Training Loss: 0.6742178996404012 Validation Loss: 1.1427533626556396\n",
      "Epoch 1643: Training Loss: 0.6751342415809631 Validation Loss: 1.142574667930603\n",
      "Epoch 1644: Training Loss: 0.6735864480336508 Validation Loss: 1.141931176185608\n",
      "Epoch 1645: Training Loss: 0.6723714669545492 Validation Loss: 1.1413112878799438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1646: Training Loss: 0.6727970441182455 Validation Loss: 1.141157865524292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1647: Training Loss: 0.6717322071393331 Validation Loss: 1.1398941278457642\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1648: Training Loss: 0.6703905463218689 Validation Loss: 1.1393388509750366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1649: Training Loss: 0.6712484359741211 Validation Loss: 1.139283537864685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1650: Training Loss: 0.6697070995966593 Validation Loss: 1.1404505968093872\n",
      "Epoch 1651: Training Loss: 0.6706985433896383 Validation Loss: 1.1398322582244873\n",
      "Epoch 1652: Training Loss: 0.669762392838796 Validation Loss: 1.1390053033828735\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1653: Training Loss: 0.6694109241167704 Validation Loss: 1.1378867626190186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1654: Training Loss: 0.6684534351030985 Validation Loss: 1.1373224258422852\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1655: Training Loss: 0.6685966650644938 Validation Loss: 1.137872338294983\n",
      "Epoch 1656: Training Loss: 0.6685461203257242 Validation Loss: 1.1373717784881592\n",
      "Epoch 1657: Training Loss: 0.6682523886362711 Validation Loss: 1.1381030082702637\n",
      "Epoch 1658: Training Loss: 0.6692659258842468 Validation Loss: 1.1388940811157227\n",
      "Epoch 1659: Training Loss: 0.6663942138353983 Validation Loss: 1.1384344100952148\n",
      "Epoch 1660: Training Loss: 0.6682613492012024 Validation Loss: 1.1377918720245361\n",
      "Epoch 1661: Training Loss: 0.6646382212638855 Validation Loss: 1.1360315084457397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1662: Training Loss: 0.6645669937133789 Validation Loss: 1.135938048362732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1663: Training Loss: 0.6649828751881918 Validation Loss: 1.1354944705963135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1664: Training Loss: 0.6639289458592733 Validation Loss: 1.1351479291915894\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1665: Training Loss: 0.6646994948387146 Validation Loss: 1.1348209381103516\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1666: Training Loss: 0.6638916730880737 Validation Loss: 1.1338751316070557\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1667: Training Loss: 0.6640662550926208 Validation Loss: 1.1347427368164062\n",
      "Epoch 1668: Training Loss: 0.6631413896878561 Validation Loss: 1.1345864534378052\n",
      "Epoch 1669: Training Loss: 0.6616219878196716 Validation Loss: 1.1343480348587036\n",
      "Epoch 1670: Training Loss: 0.6633706092834473 Validation Loss: 1.1322970390319824\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1671: Training Loss: 0.6619148651758829 Validation Loss: 1.1319506168365479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1672: Training Loss: 0.6632273197174072 Validation Loss: 1.132752776145935\n",
      "Epoch 1673: Training Loss: 0.6606469750404358 Validation Loss: 1.1340943574905396\n",
      "Epoch 1674: Training Loss: 0.6603930393854777 Validation Loss: 1.1345481872558594\n",
      "Epoch 1675: Training Loss: 0.659128467241923 Validation Loss: 1.1332447528839111\n",
      "Epoch 1676: Training Loss: 0.6601359844207764 Validation Loss: 1.1310477256774902\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1677: Training Loss: 0.6600622137387594 Validation Loss: 1.1310651302337646\n",
      "Epoch 1678: Training Loss: 0.6595066785812378 Validation Loss: 1.130637526512146\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1679: Training Loss: 0.6585320432980856 Validation Loss: 1.1301665306091309\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1680: Training Loss: 0.6593165795008341 Validation Loss: 1.1298296451568604\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1681: Training Loss: 0.6574986775716146 Validation Loss: 1.129910945892334\n",
      "Epoch 1682: Training Loss: 0.6567864418029785 Validation Loss: 1.1304078102111816\n",
      "Epoch 1683: Training Loss: 0.6563925743103027 Validation Loss: 1.130817174911499\n",
      "Epoch 1684: Training Loss: 0.6566234628359476 Validation Loss: 1.1300435066223145\n",
      "Epoch 1685: Training Loss: 0.6556327144304911 Validation Loss: 1.129392147064209\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1686: Training Loss: 0.6550686756769816 Validation Loss: 1.1294031143188477\n",
      "Epoch 1687: Training Loss: 0.6546719471613566 Validation Loss: 1.1288822889328003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1688: Training Loss: 0.6547313729921976 Validation Loss: 1.1285327672958374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1689: Training Loss: 0.6559561292330424 Validation Loss: 1.1277176141738892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1690: Training Loss: 0.6537905136744181 Validation Loss: 1.1271848678588867\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1691: Training Loss: 0.6534335215886434 Validation Loss: 1.1277143955230713\n",
      "Epoch 1692: Training Loss: 0.6530022422472636 Validation Loss: 1.1266601085662842\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1693: Training Loss: 0.6541755398114523 Validation Loss: 1.1267318725585938\n",
      "Epoch 1694: Training Loss: 0.6521843870480856 Validation Loss: 1.1264616250991821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1695: Training Loss: 0.6525739034016927 Validation Loss: 1.1257401704788208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1696: Training Loss: 0.6510041554768881 Validation Loss: 1.1266472339630127\n",
      "Epoch 1697: Training Loss: 0.6503608624140421 Validation Loss: 1.1260502338409424\n",
      "Epoch 1698: Training Loss: 0.6509312589963278 Validation Loss: 1.1261144876480103\n",
      "Epoch 1699: Training Loss: 0.649506409962972 Validation Loss: 1.1264835596084595\n",
      "Epoch 1700: Training Loss: 0.6491622527440389 Validation Loss: 1.1256635189056396\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1701: Training Loss: 0.6477639079093933 Validation Loss: 1.1248064041137695\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1702: Training Loss: 0.6489226222038269 Validation Loss: 1.1242032051086426\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1703: Training Loss: 0.6484563151995341 Validation Loss: 1.1239737272262573\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1704: Training Loss: 0.648856520652771 Validation Loss: 1.1233705282211304\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1705: Training Loss: 0.6472857197125753 Validation Loss: 1.12367844581604\n",
      "Epoch 1706: Training Loss: 0.6464964946111044 Validation Loss: 1.1233259439468384\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1707: Training Loss: 0.6475891470909119 Validation Loss: 1.1239898204803467\n",
      "Epoch 1708: Training Loss: 0.6468060612678528 Validation Loss: 1.123459815979004\n",
      "Epoch 1709: Training Loss: 0.6454102198282877 Validation Loss: 1.12296724319458\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1710: Training Loss: 0.6475184957186381 Validation Loss: 1.1222648620605469\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1711: Training Loss: 0.6453643043835958 Validation Loss: 1.122006893157959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1712: Training Loss: 0.649716297785441 Validation Loss: 1.1210476160049438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1713: Training Loss: 0.642749031384786 Validation Loss: 1.1211888790130615\n",
      "Epoch 1714: Training Loss: 0.6438462138175964 Validation Loss: 1.1216883659362793\n",
      "Epoch 1715: Training Loss: 0.6425878405570984 Validation Loss: 1.1212501525878906\n",
      "Epoch 1716: Training Loss: 0.642751673857371 Validation Loss: 1.1203086376190186\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1717: Training Loss: 0.645026445388794 Validation Loss: 1.1206474304199219\n",
      "Epoch 1718: Training Loss: 0.6417154868443807 Validation Loss: 1.1201099157333374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1719: Training Loss: 0.6417985359827677 Validation Loss: 1.1200839281082153\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1720: Training Loss: 0.6425025264422098 Validation Loss: 1.1201808452606201\n",
      "Epoch 1721: Training Loss: 0.6398420532544454 Validation Loss: 1.1196612119674683\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1722: Training Loss: 0.6407194336255392 Validation Loss: 1.119153380393982\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1723: Training Loss: 0.6398575901985168 Validation Loss: 1.1192289590835571\n",
      "Epoch 1724: Training Loss: 0.6398043433825175 Validation Loss: 1.1180459260940552\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1725: Training Loss: 0.6389870246251425 Validation Loss: 1.1178646087646484\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1726: Training Loss: 0.6409521301587423 Validation Loss: 1.1173593997955322\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1727: Training Loss: 0.6379751761754354 Validation Loss: 1.118221640586853\n",
      "Epoch 1728: Training Loss: 0.6384007533391317 Validation Loss: 1.1187046766281128\n",
      "Epoch 1729: Training Loss: 0.6413520971934 Validation Loss: 1.1179780960083008\n",
      "Epoch 1730: Training Loss: 0.6378690600395203 Validation Loss: 1.1162112951278687\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1731: Training Loss: 0.6376417875289917 Validation Loss: 1.114656925201416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1732: Training Loss: 0.6374363700548807 Validation Loss: 1.1144605875015259\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1733: Training Loss: 0.6356956958770752 Validation Loss: 1.115822672843933\n",
      "Epoch 1734: Training Loss: 0.6354602575302124 Validation Loss: 1.116425633430481\n",
      "Epoch 1735: Training Loss: 0.6350868145624796 Validation Loss: 1.1161205768585205\n",
      "Epoch 1736: Training Loss: 0.6351509690284729 Validation Loss: 1.1157137155532837\n",
      "Epoch 1737: Training Loss: 0.633908728758494 Validation Loss: 1.1148388385772705\n",
      "Epoch 1738: Training Loss: 0.6338725884755453 Validation Loss: 1.1135677099227905\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1739: Training Loss: 0.6336002151171366 Validation Loss: 1.1144827604293823\n",
      "Epoch 1740: Training Loss: 0.6364603042602539 Validation Loss: 1.1146910190582275\n",
      "Epoch 1741: Training Loss: 0.6326146523157755 Validation Loss: 1.1143637895584106\n",
      "Epoch 1742: Training Loss: 0.6336161891619364 Validation Loss: 1.1149152517318726\n",
      "Epoch 1743: Training Loss: 0.6327829957008362 Validation Loss: 1.1142410039901733\n",
      "Epoch 1744: Training Loss: 0.6342738469441732 Validation Loss: 1.114047884941101\n",
      "Epoch 1745: Training Loss: 0.6320998867352804 Validation Loss: 1.1131144762039185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1746: Training Loss: 0.6296027700106303 Validation Loss: 1.1133036613464355\n",
      "Epoch 1747: Training Loss: 0.6303579211235046 Validation Loss: 1.111934781074524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1748: Training Loss: 0.6297175884246826 Validation Loss: 1.1112335920333862\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1749: Training Loss: 0.6304011344909668 Validation Loss: 1.1119223833084106\n",
      "Epoch 1750: Training Loss: 0.62966920932134 Validation Loss: 1.1118899583816528\n",
      "Epoch 1751: Training Loss: 0.6298840840657552 Validation Loss: 1.1117933988571167\n",
      "Epoch 1752: Training Loss: 0.6321908831596375 Validation Loss: 1.1106256246566772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1753: Training Loss: 0.6278413931528727 Validation Loss: 1.1111013889312744\n",
      "Epoch 1754: Training Loss: 0.6287290255228678 Validation Loss: 1.1119298934936523\n",
      "Epoch 1755: Training Loss: 0.6268724799156189 Validation Loss: 1.1102768182754517\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1756: Training Loss: 0.6268370946248373 Validation Loss: 1.1089617013931274\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1757: Training Loss: 0.6267682512601217 Validation Loss: 1.1081960201263428\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1758: Training Loss: 0.6258775591850281 Validation Loss: 1.1084710359573364\n",
      "Epoch 1759: Training Loss: 0.6263030370076498 Validation Loss: 1.1089015007019043\n",
      "Epoch 1760: Training Loss: 0.6250706116358439 Validation Loss: 1.110106348991394\n",
      "Epoch 1761: Training Loss: 0.6237422823905945 Validation Loss: 1.1097559928894043\n",
      "Epoch 1762: Training Loss: 0.624175230662028 Validation Loss: 1.1092069149017334\n",
      "Epoch 1763: Training Loss: 0.6240042448043823 Validation Loss: 1.1088908910751343\n",
      "Epoch 1764: Training Loss: 0.6253071626027426 Validation Loss: 1.1077344417572021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1765: Training Loss: 0.6240413188934326 Validation Loss: 1.1081011295318604\n",
      "Epoch 1766: Training Loss: 0.622919499874115 Validation Loss: 1.1076653003692627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1767: Training Loss: 0.6235897938410441 Validation Loss: 1.1077333688735962\n",
      "Epoch 1768: Training Loss: 0.6218826373418173 Validation Loss: 1.107439637184143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1769: Training Loss: 0.6229432225227356 Validation Loss: 1.1071447134017944\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1770: Training Loss: 0.6211276650428772 Validation Loss: 1.106694221496582\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1771: Training Loss: 0.6201291084289551 Validation Loss: 1.1064592599868774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1772: Training Loss: 0.6200517217318217 Validation Loss: 1.1051782369613647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1773: Training Loss: 0.6198523044586182 Validation Loss: 1.1051130294799805\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1774: Training Loss: 0.6203404466311137 Validation Loss: 1.1052796840667725\n",
      "Epoch 1775: Training Loss: 0.6216089924176534 Validation Loss: 1.105259656906128\n",
      "Epoch 1776: Training Loss: 0.6184560457865397 Validation Loss: 1.106013536453247\n",
      "Epoch 1777: Training Loss: 0.6210481723149618 Validation Loss: 1.1053236722946167\n",
      "Epoch 1778: Training Loss: 0.6181320945421854 Validation Loss: 1.1039718389511108\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1779: Training Loss: 0.6177218953768412 Validation Loss: 1.1041672229766846\n",
      "Epoch 1780: Training Loss: 0.617383619149526 Validation Loss: 1.103803277015686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1781: Training Loss: 0.6175893942515055 Validation Loss: 1.103319764137268\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1782: Training Loss: 0.6178939541180929 Validation Loss: 1.1032938957214355\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1783: Training Loss: 0.616584579149882 Validation Loss: 1.1037321090698242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1784: Training Loss: 0.6165089209874471 Validation Loss: 1.1034367084503174\n",
      "Epoch 1785: Training Loss: 0.6158905227979025 Validation Loss: 1.1033830642700195\n",
      "Epoch 1786: Training Loss: 0.6149364511171976 Validation Loss: 1.1033649444580078\n",
      "Epoch 1787: Training Loss: 0.6156792839368185 Validation Loss: 1.1024479866027832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1788: Training Loss: 0.6151867508888245 Validation Loss: 1.1024643182754517\n",
      "Epoch 1789: Training Loss: 0.6143678824106852 Validation Loss: 1.102152943611145\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1790: Training Loss: 0.6141589482625326 Validation Loss: 1.1007320880889893\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1791: Training Loss: 0.6134971976280212 Validation Loss: 1.100591778755188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1792: Training Loss: 0.6135305364926656 Validation Loss: 1.1011717319488525\n",
      "Epoch 1793: Training Loss: 0.6144412557284037 Validation Loss: 1.101489543914795\n",
      "Epoch 1794: Training Loss: 0.6120085120201111 Validation Loss: 1.101677417755127\n",
      "Epoch 1795: Training Loss: 0.6117790142695109 Validation Loss: 1.100150227546692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1796: Training Loss: 0.6122137506802877 Validation Loss: 1.0994707345962524\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1797: Training Loss: 0.6105281114578247 Validation Loss: 1.0997915267944336\n",
      "Epoch 1798: Training Loss: 0.6119949221611023 Validation Loss: 1.0996447801589966\n",
      "Epoch 1799: Training Loss: 0.6109053095181783 Validation Loss: 1.1002342700958252\n",
      "Epoch 1800: Training Loss: 0.6106343865394592 Validation Loss: 1.1003721952438354\n",
      "Epoch 1801: Training Loss: 0.6092981894810995 Validation Loss: 1.0999290943145752\n",
      "Epoch 1802: Training Loss: 0.6090746919314066 Validation Loss: 1.099395751953125\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1803: Training Loss: 0.6103489200274149 Validation Loss: 1.098467230796814\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1804: Training Loss: 0.6083158651987711 Validation Loss: 1.0984069108963013\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1805: Training Loss: 0.608101487159729 Validation Loss: 1.0969974994659424\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1806: Training Loss: 0.6074751416842142 Validation Loss: 1.0975919961929321\n",
      "Epoch 1807: Training Loss: 0.6078181266784668 Validation Loss: 1.0975539684295654\n",
      "Epoch 1808: Training Loss: 0.6074450612068176 Validation Loss: 1.0972118377685547\n",
      "Epoch 1809: Training Loss: 0.6068493525187174 Validation Loss: 1.0965783596038818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1810: Training Loss: 0.6067135532697042 Validation Loss: 1.09702730178833\n",
      "Epoch 1811: Training Loss: 0.6065593957901001 Validation Loss: 1.0978182554244995\n",
      "Epoch 1812: Training Loss: 0.6055745482444763 Validation Loss: 1.0971252918243408\n",
      "Epoch 1813: Training Loss: 0.6054704785346985 Validation Loss: 1.0965631008148193\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1814: Training Loss: 0.6043516794840494 Validation Loss: 1.095045804977417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1815: Training Loss: 0.605791707833608 Validation Loss: 1.0943158864974976\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1816: Training Loss: 0.6040759285291036 Validation Loss: 1.0949848890304565\n",
      "Epoch 1817: Training Loss: 0.6047612031300863 Validation Loss: 1.0960792303085327\n",
      "Epoch 1818: Training Loss: 0.6031132539113363 Validation Loss: 1.0962004661560059\n",
      "Epoch 1819: Training Loss: 0.603329062461853 Validation Loss: 1.0967241525650024\n",
      "Epoch 1820: Training Loss: 0.6027019421259562 Validation Loss: 1.0952237844467163\n",
      "Epoch 1821: Training Loss: 0.6026798089345297 Validation Loss: 1.094292402267456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1822: Training Loss: 0.6030439138412476 Validation Loss: 1.094083547592163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1823: Training Loss: 0.6016152600447336 Validation Loss: 1.0929887294769287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1824: Training Loss: 0.6016363898913065 Validation Loss: 1.0926666259765625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1825: Training Loss: 0.6007706721623739 Validation Loss: 1.0926114320755005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1826: Training Loss: 0.600802997748057 Validation Loss: 1.093371033668518\n",
      "Epoch 1827: Training Loss: 0.6013951102892557 Validation Loss: 1.093727707862854\n",
      "Epoch 1828: Training Loss: 0.5997827251752218 Validation Loss: 1.0931059122085571\n",
      "Epoch 1829: Training Loss: 0.5995319684346517 Validation Loss: 1.0923140048980713\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1830: Training Loss: 0.598554809888204 Validation Loss: 1.0910924673080444\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1831: Training Loss: 0.5984161694844564 Validation Loss: 1.0909383296966553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1832: Training Loss: 0.5984495282173157 Validation Loss: 1.0909909009933472\n",
      "Epoch 1833: Training Loss: 0.5980950395266215 Validation Loss: 1.0918248891830444\n",
      "Epoch 1834: Training Loss: 0.5971398949623108 Validation Loss: 1.0923447608947754\n",
      "Epoch 1835: Training Loss: 0.5975918173789978 Validation Loss: 1.0926467180252075\n",
      "Epoch 1836: Training Loss: 0.596847136815389 Validation Loss: 1.0926867723464966\n",
      "Epoch 1837: Training Loss: 0.5965399742126465 Validation Loss: 1.090838074684143\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1838: Training Loss: 0.5956974228223165 Validation Loss: 1.0897395610809326\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1839: Training Loss: 0.5948800245920817 Validation Loss: 1.090164065361023\n",
      "Epoch 1840: Training Loss: 0.5953747828801473 Validation Loss: 1.0899521112442017\n",
      "Epoch 1841: Training Loss: 0.5946163535118103 Validation Loss: 1.089860200881958\n",
      "Epoch 1842: Training Loss: 0.595499575138092 Validation Loss: 1.0889326333999634\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1843: Training Loss: 0.5948187510172526 Validation Loss: 1.0889116525650024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1844: Training Loss: 0.5936558643976847 Validation Loss: 1.0897098779678345\n",
      "Epoch 1845: Training Loss: 0.593508243560791 Validation Loss: 1.088253378868103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1846: Training Loss: 0.5930881102879842 Validation Loss: 1.087518334388733\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1847: Training Loss: 0.5930286049842834 Validation Loss: 1.0878541469573975\n",
      "Epoch 1848: Training Loss: 0.5921618342399597 Validation Loss: 1.0881327390670776\n",
      "Epoch 1849: Training Loss: 0.592745840549469 Validation Loss: 1.0881558656692505\n",
      "Epoch 1850: Training Loss: 0.5913173953692118 Validation Loss: 1.0889196395874023\n",
      "Epoch 1851: Training Loss: 0.5912571549415588 Validation Loss: 1.0887483358383179\n",
      "Epoch 1852: Training Loss: 0.5907870133717855 Validation Loss: 1.0886074304580688\n",
      "Epoch 1853: Training Loss: 0.5911763707796732 Validation Loss: 1.0882166624069214\n",
      "Epoch 1854: Training Loss: 0.5905153354008993 Validation Loss: 1.0874536037445068\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1855: Training Loss: 0.5907118121782938 Validation Loss: 1.086310863494873\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1856: Training Loss: 0.5889816482861837 Validation Loss: 1.085768699645996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1857: Training Loss: 0.5887096722920736 Validation Loss: 1.085282802581787\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1858: Training Loss: 0.5903399387995402 Validation Loss: 1.0854771137237549\n",
      "Epoch 1859: Training Loss: 0.5882185101509094 Validation Loss: 1.0855412483215332\n",
      "Epoch 1860: Training Loss: 0.5916882554690043 Validation Loss: 1.085788369178772\n",
      "Epoch 1861: Training Loss: 0.5875854889551798 Validation Loss: 1.0848171710968018\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1862: Training Loss: 0.5869730313618978 Validation Loss: 1.084633708000183\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1863: Training Loss: 0.5881041089693705 Validation Loss: 1.0850245952606201\n",
      "Epoch 1864: Training Loss: 0.5873665809631348 Validation Loss: 1.0855417251586914\n",
      "Epoch 1865: Training Loss: 0.5863555669784546 Validation Loss: 1.084714412689209\n",
      "Epoch 1866: Training Loss: 0.5859370231628418 Validation Loss: 1.0842781066894531\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1867: Training Loss: 0.5858503778775533 Validation Loss: 1.0840890407562256\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1868: Training Loss: 0.5868758161862692 Validation Loss: 1.0837801694869995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1869: Training Loss: 0.5848037600517273 Validation Loss: 1.084029197692871\n",
      "Epoch 1870: Training Loss: 0.5849012533823649 Validation Loss: 1.0834085941314697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1871: Training Loss: 0.5843406319618225 Validation Loss: 1.0835087299346924\n",
      "Epoch 1872: Training Loss: 0.583912173906962 Validation Loss: 1.0828906297683716\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1873: Training Loss: 0.5851358572642008 Validation Loss: 1.0826951265335083\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1874: Training Loss: 0.5837533473968506 Validation Loss: 1.0830800533294678\n",
      "Epoch 1875: Training Loss: 0.5826939741770426 Validation Loss: 1.0831536054611206\n",
      "Epoch 1876: Training Loss: 0.5824652711550394 Validation Loss: 1.082925796508789\n",
      "Epoch 1877: Training Loss: 0.5822155276934305 Validation Loss: 1.0819753408432007\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1878: Training Loss: 0.5834971268971761 Validation Loss: 1.0806845426559448\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1879: Training Loss: 0.584059993426005 Validation Loss: 1.0810322761535645\n",
      "Epoch 1880: Training Loss: 0.5810950795809428 Validation Loss: 1.0817979574203491\n",
      "Epoch 1881: Training Loss: 0.5804831584294637 Validation Loss: 1.0812952518463135\n",
      "Epoch 1882: Training Loss: 0.5804974834124247 Validation Loss: 1.0811163187026978\n",
      "Epoch 1883: Training Loss: 0.5809724926948547 Validation Loss: 1.081158995628357\n",
      "Epoch 1884: Training Loss: 0.5797979235649109 Validation Loss: 1.0808067321777344\n",
      "Epoch 1885: Training Loss: 0.5805591344833374 Validation Loss: 1.081182599067688\n",
      "Epoch 1886: Training Loss: 0.5798741380373637 Validation Loss: 1.080686330795288\n",
      "Epoch 1887: Training Loss: 0.5790310899416605 Validation Loss: 1.079022765159607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1888: Training Loss: 0.5798890391985575 Validation Loss: 1.0786182880401611\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1889: Training Loss: 0.5780965685844421 Validation Loss: 1.0795273780822754\n",
      "Epoch 1890: Training Loss: 0.5779354770978292 Validation Loss: 1.0799494981765747\n",
      "Epoch 1891: Training Loss: 0.5818946162859598 Validation Loss: 1.079590916633606\n",
      "Epoch 1892: Training Loss: 0.5775467356046041 Validation Loss: 1.0789884328842163\n",
      "Epoch 1893: Training Loss: 0.5777741471926371 Validation Loss: 1.077852725982666\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1894: Training Loss: 0.5764159162839254 Validation Loss: 1.0777697563171387\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1895: Training Loss: 0.5760693947474161 Validation Loss: 1.0783846378326416\n",
      "Epoch 1896: Training Loss: 0.5753241578737894 Validation Loss: 1.0781066417694092\n",
      "Epoch 1897: Training Loss: 0.5752060015996298 Validation Loss: 1.0781863927841187\n",
      "Epoch 1898: Training Loss: 0.5789253314336141 Validation Loss: 1.0775648355484009\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1899: Training Loss: 0.5768139958381653 Validation Loss: 1.076277494430542\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1900: Training Loss: 0.5742265979448954 Validation Loss: 1.0763027667999268\n",
      "Epoch 1901: Training Loss: 0.5745080709457397 Validation Loss: 1.075897455215454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1902: Training Loss: 0.5754402279853821 Validation Loss: 1.076709270477295\n",
      "Epoch 1903: Training Loss: 0.5734251538912455 Validation Loss: 1.077866554260254\n",
      "Epoch 1904: Training Loss: 0.5735344290733337 Validation Loss: 1.0778847932815552\n",
      "Epoch 1905: Training Loss: 0.5723916888237 Validation Loss: 1.0772688388824463\n",
      "Epoch 1906: Training Loss: 0.5721745093663534 Validation Loss: 1.075610637664795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1907: Training Loss: 0.5718722542126974 Validation Loss: 1.0748016834259033\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1908: Training Loss: 0.5727999806404114 Validation Loss: 1.0745316743850708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1909: Training Loss: 0.5718807578086853 Validation Loss: 1.075019359588623\n",
      "Epoch 1910: Training Loss: 0.5710634390513102 Validation Loss: 1.0754048824310303\n",
      "Epoch 1911: Training Loss: 0.5699891646703085 Validation Loss: 1.0754233598709106\n",
      "Epoch 1912: Training Loss: 0.5713523228963217 Validation Loss: 1.074988842010498\n",
      "Epoch 1913: Training Loss: 0.5725957155227661 Validation Loss: 1.0734734535217285\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1914: Training Loss: 0.5700498421986898 Validation Loss: 1.074347972869873\n",
      "Epoch 1915: Training Loss: 0.5690202514330546 Validation Loss: 1.0748933553695679\n",
      "Epoch 1916: Training Loss: 0.5694050590197245 Validation Loss: 1.0753507614135742\n",
      "Epoch 1917: Training Loss: 0.5693550109863281 Validation Loss: 1.0745261907577515\n",
      "Epoch 1918: Training Loss: 0.5697576204935709 Validation Loss: 1.073992371559143\n",
      "Epoch 1919: Training Loss: 0.5675080418586731 Validation Loss: 1.0729095935821533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1920: Training Loss: 0.5699122349421183 Validation Loss: 1.0721651315689087\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1921: Training Loss: 0.5664833188056946 Validation Loss: 1.0714200735092163\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1922: Training Loss: 0.5671722491582235 Validation Loss: 1.0715194940567017\n",
      "Epoch 1923: Training Loss: 0.5676158865292867 Validation Loss: 1.072245478630066\n",
      "Epoch 1924: Training Loss: 0.5660337209701538 Validation Loss: 1.0730068683624268\n",
      "Epoch 1925: Training Loss: 0.5651913285255432 Validation Loss: 1.0732998847961426\n",
      "Epoch 1926: Training Loss: 0.5663284659385681 Validation Loss: 1.0727813243865967\n",
      "Epoch 1927: Training Loss: 0.565420905749003 Validation Loss: 1.0724201202392578\n",
      "Epoch 1928: Training Loss: 0.5658128062884012 Validation Loss: 1.0715217590332031\n",
      "Epoch 1929: Training Loss: 0.5655940771102905 Validation Loss: 1.0706477165222168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1930: Training Loss: 0.5640470782915751 Validation Loss: 1.0703328847885132\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1931: Training Loss: 0.5645133058230082 Validation Loss: 1.0703039169311523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1932: Training Loss: 0.5646774371465048 Validation Loss: 1.0707814693450928\n",
      "Epoch 1933: Training Loss: 0.5622608065605164 Validation Loss: 1.0704991817474365\n",
      "Epoch 1934: Training Loss: 0.5628131429354349 Validation Loss: 1.0707118511199951\n",
      "Epoch 1935: Training Loss: 0.5623595913251241 Validation Loss: 1.0705907344818115\n",
      "Epoch 1936: Training Loss: 0.5625932614008585 Validation Loss: 1.0701396465301514\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1937: Training Loss: 0.5618916948636373 Validation Loss: 1.0698319673538208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1938: Training Loss: 0.56128990650177 Validation Loss: 1.0689167976379395\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1939: Training Loss: 0.5607590874036154 Validation Loss: 1.0689464807510376\n",
      "Epoch 1940: Training Loss: 0.5619809428850809 Validation Loss: 1.0697660446166992\n",
      "Epoch 1941: Training Loss: 0.5618344148000082 Validation Loss: 1.0695960521697998\n",
      "Epoch 1942: Training Loss: 0.5605255166689554 Validation Loss: 1.0689774751663208\n",
      "Epoch 1943: Training Loss: 0.559484620889028 Validation Loss: 1.0674595832824707\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1944: Training Loss: 0.5592835446198782 Validation Loss: 1.0671778917312622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1945: Training Loss: 0.5598622560501099 Validation Loss: 1.0686911344528198\n",
      "Epoch 1946: Training Loss: 0.5592385530471802 Validation Loss: 1.0678285360336304\n",
      "Epoch 1947: Training Loss: 0.5586545268694559 Validation Loss: 1.067676067352295\n",
      "Epoch 1948: Training Loss: 0.5594425797462463 Validation Loss: 1.067489743232727\n",
      "Epoch 1949: Training Loss: 0.557883103688558 Validation Loss: 1.0673284530639648\n",
      "Epoch 1950: Training Loss: 0.5569128394126892 Validation Loss: 1.067447304725647\n",
      "Epoch 1951: Training Loss: 0.557996412118276 Validation Loss: 1.0674279928207397\n",
      "Epoch 1952: Training Loss: 0.5570720434188843 Validation Loss: 1.067197322845459\n",
      "Epoch 1953: Training Loss: 0.5565238197644552 Validation Loss: 1.0662047863006592\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1954: Training Loss: 0.5567909876505533 Validation Loss: 1.0657106637954712\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1955: Training Loss: 0.5567991733551025 Validation Loss: 1.0653083324432373\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1956: Training Loss: 0.5553613305091858 Validation Loss: 1.066454291343689\n",
      "Epoch 1957: Training Loss: 0.5530412991841634 Validation Loss: 1.0669920444488525\n",
      "Epoch 1958: Training Loss: 0.5554586251576742 Validation Loss: 1.0665276050567627\n",
      "Epoch 1959: Training Loss: 0.5566777189572653 Validation Loss: 1.066781997680664\n",
      "Epoch 1960: Training Loss: 0.5563603639602661 Validation Loss: 1.0657966136932373\n",
      "Epoch 1961: Training Loss: 0.5549258589744568 Validation Loss: 1.0650230646133423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1962: Training Loss: 0.5537753701210022 Validation Loss: 1.0649813413619995\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1963: Training Loss: 0.5532891154289246 Validation Loss: 1.0645946264266968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1964: Training Loss: 0.5544812778631846 Validation Loss: 1.0632656812667847\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1965: Training Loss: 0.551975150903066 Validation Loss: 1.063281774520874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1966: Training Loss: 0.5525075395901998 Validation Loss: 1.0634641647338867\n",
      "Epoch 1967: Training Loss: 0.5526034434636434 Validation Loss: 1.0645629167556763\n",
      "Epoch 1968: Training Loss: 0.5520843267440796 Validation Loss: 1.0645774602890015\n",
      "Epoch 1969: Training Loss: 0.5533454815546671 Validation Loss: 1.064161777496338\n",
      "Epoch 1970: Training Loss: 0.5511769851048788 Validation Loss: 1.0626047849655151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1971: Training Loss: 0.551816721757253 Validation Loss: 1.0622398853302002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1972: Training Loss: 0.5517640511194865 Validation Loss: 1.0619025230407715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1973: Training Loss: 0.552026609579722 Validation Loss: 1.062124252319336\n",
      "Epoch 1974: Training Loss: 0.5501545468966166 Validation Loss: 1.0626296997070312\n",
      "Epoch 1975: Training Loss: 0.550324281056722 Validation Loss: 1.0629576444625854\n",
      "Epoch 1976: Training Loss: 0.5500666896502177 Validation Loss: 1.0627440214157104\n",
      "Epoch 1977: Training Loss: 0.5489497184753418 Validation Loss: 1.062896966934204\n",
      "Epoch 1978: Training Loss: 0.5486103296279907 Validation Loss: 1.0618430376052856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1979: Training Loss: 0.5495166977246603 Validation Loss: 1.0611473321914673\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1980: Training Loss: 0.5490145087242126 Validation Loss: 1.0610790252685547\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1981: Training Loss: 0.547970175743103 Validation Loss: 1.0611907243728638\n",
      "Epoch 1982: Training Loss: 0.5480205217997233 Validation Loss: 1.0613844394683838\n",
      "Epoch 1983: Training Loss: 0.5474783380826315 Validation Loss: 1.0618940591812134\n",
      "Epoch 1984: Training Loss: 0.5468956828117371 Validation Loss: 1.0609123706817627\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1985: Training Loss: 0.5468881130218506 Validation Loss: 1.0607033967971802\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1986: Training Loss: 0.5460393726825714 Validation Loss: 1.060985803604126\n",
      "Epoch 1987: Training Loss: 0.5474438766638438 Validation Loss: 1.0599284172058105\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1988: Training Loss: 0.5455713272094727 Validation Loss: 1.0603549480438232\n",
      "Epoch 1989: Training Loss: 0.5465631286303202 Validation Loss: 1.0606589317321777\n",
      "Epoch 1990: Training Loss: 0.5451164245605469 Validation Loss: 1.0599782466888428\n",
      "Epoch 1991: Training Loss: 0.5457046230634054 Validation Loss: 1.059635877609253\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1992: Training Loss: 0.54474276304245 Validation Loss: 1.0589945316314697\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1993: Training Loss: 0.5449149012565613 Validation Loss: 1.0598475933074951\n",
      "Epoch 1994: Training Loss: 0.5435591538747152 Validation Loss: 1.059991717338562\n",
      "Epoch 1995: Training Loss: 0.5472593108812968 Validation Loss: 1.0587023496627808\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1996: Training Loss: 0.5435936450958252 Validation Loss: 1.0576846599578857\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1997: Training Loss: 0.5423008998235067 Validation Loss: 1.0572011470794678\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 1998: Training Loss: 0.5430785417556763 Validation Loss: 1.0580118894577026\n",
      "Epoch 1999: Training Loss: 0.5416803558667501 Validation Loss: 1.0578850507736206\n",
      "Epoch 2000: Training Loss: 0.5436453223228455 Validation Loss: 1.0579622983932495\n",
      "Epoch 2001: Training Loss: 0.5415284434954325 Validation Loss: 1.0584083795547485\n",
      "Epoch 2002: Training Loss: 0.5410475730895996 Validation Loss: 1.0576895475387573\n",
      "Epoch 2003: Training Loss: 0.5411590933799744 Validation Loss: 1.057692527770996\n",
      "Epoch 2004: Training Loss: 0.5421727299690247 Validation Loss: 1.0570664405822754\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2005: Training Loss: 0.539186159769694 Validation Loss: 1.0571355819702148\n",
      "Epoch 2006: Training Loss: 0.5433774491151174 Validation Loss: 1.0569177865982056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2007: Training Loss: 0.5400698781013489 Validation Loss: 1.0571290254592896\n",
      "Epoch 2008: Training Loss: 0.5395685831705729 Validation Loss: 1.0572000741958618\n",
      "Epoch 2009: Training Loss: 0.5393034815788269 Validation Loss: 1.0571852922439575\n",
      "Epoch 2010: Training Loss: 0.5384603142738342 Validation Loss: 1.0570745468139648\n",
      "Epoch 2011: Training Loss: 0.538632313410441 Validation Loss: 1.0569276809692383\n",
      "Epoch 2012: Training Loss: 0.537272572517395 Validation Loss: 1.0561121702194214\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2013: Training Loss: 0.5384765267372131 Validation Loss: 1.056249976158142\n",
      "Epoch 2014: Training Loss: 0.5380644798278809 Validation Loss: 1.055044174194336\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2015: Training Loss: 0.5382071832815806 Validation Loss: 1.0544641017913818\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2016: Training Loss: 0.5364712377389272 Validation Loss: 1.054935336112976\n",
      "Epoch 2017: Training Loss: 0.5372781952222189 Validation Loss: 1.055918574333191\n",
      "Epoch 2018: Training Loss: 0.5376315613587698 Validation Loss: 1.05509614944458\n",
      "Epoch 2019: Training Loss: 0.5362275044123331 Validation Loss: 1.0540739297866821\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2020: Training Loss: 0.5362443327903748 Validation Loss: 1.0546848773956299\n",
      "Epoch 2021: Training Loss: 0.5375889639059702 Validation Loss: 1.0533766746520996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2022: Training Loss: 0.5344085792700449 Validation Loss: 1.053248405456543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2023: Training Loss: 0.5345624685287476 Validation Loss: 1.0535252094268799\n",
      "Epoch 2024: Training Loss: 0.536204973856608 Validation Loss: 1.0538685321807861\n",
      "Epoch 2025: Training Loss: 0.5340031385421753 Validation Loss: 1.0530531406402588\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2026: Training Loss: 0.533959706624349 Validation Loss: 1.052854061126709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2027: Training Loss: 0.532156636317571 Validation Loss: 1.0535825490951538\n",
      "Epoch 2028: Training Loss: 0.5340810517470042 Validation Loss: 1.0535494089126587\n",
      "Epoch 2029: Training Loss: 0.5348477959632874 Validation Loss: 1.053547739982605\n",
      "Epoch 2030: Training Loss: 0.5328168471654257 Validation Loss: 1.0526096820831299\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2031: Training Loss: 0.5328604976336161 Validation Loss: 1.0530459880828857\n",
      "Epoch 2032: Training Loss: 0.5304742952187856 Validation Loss: 1.0524729490280151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2033: Training Loss: 0.5323658386866251 Validation Loss: 1.0521554946899414\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2034: Training Loss: 0.5316861867904663 Validation Loss: 1.052127480506897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2035: Training Loss: 0.5317354599634806 Validation Loss: 1.0519057512283325\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2036: Training Loss: 0.5309799114863077 Validation Loss: 1.0518239736557007\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2037: Training Loss: 0.530520518620809 Validation Loss: 1.0516409873962402\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2038: Training Loss: 0.5316381255785624 Validation Loss: 1.0520073175430298\n",
      "Epoch 2039: Training Loss: 0.529951810836792 Validation Loss: 1.05135977268219\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2040: Training Loss: 0.530240128437678 Validation Loss: 1.0507348775863647\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2041: Training Loss: 0.5297793944676717 Validation Loss: 1.0502771139144897\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2042: Training Loss: 0.5284680525461832 Validation Loss: 1.0508028268814087\n",
      "Epoch 2043: Training Loss: 0.5285231272379557 Validation Loss: 1.0510364770889282\n",
      "Epoch 2044: Training Loss: 0.5300365388393402 Validation Loss: 1.0514930486679077\n",
      "Epoch 2045: Training Loss: 0.5282208124796549 Validation Loss: 1.0505914688110352\n",
      "Epoch 2046: Training Loss: 0.529718816280365 Validation Loss: 1.0495741367340088\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2047: Training Loss: 0.5288423399130503 Validation Loss: 1.0489025115966797\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2048: Training Loss: 0.5277367830276489 Validation Loss: 1.0493613481521606\n",
      "Epoch 2049: Training Loss: 0.5275988280773163 Validation Loss: 1.0500251054763794\n",
      "Epoch 2050: Training Loss: 0.527035117149353 Validation Loss: 1.0489319562911987\n",
      "Epoch 2051: Training Loss: 0.5293451646963755 Validation Loss: 1.0490179061889648\n",
      "Epoch 2052: Training Loss: 0.5277747412522634 Validation Loss: 1.0483886003494263\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2053: Training Loss: 0.5262442727883657 Validation Loss: 1.0482813119888306\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2054: Training Loss: 0.5274125635623932 Validation Loss: 1.0491561889648438\n",
      "Epoch 2055: Training Loss: 0.5264364977677664 Validation Loss: 1.0503525733947754\n",
      "Epoch 2056: Training Loss: 0.5240366359551748 Validation Loss: 1.050473690032959\n",
      "Epoch 2057: Training Loss: 0.5251037081082662 Validation Loss: 1.0486533641815186\n",
      "Epoch 2058: Training Loss: 0.5249357223510742 Validation Loss: 1.0466660261154175\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2059: Training Loss: 0.5248691240946451 Validation Loss: 1.0461739301681519\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2060: Training Loss: 0.5231499075889587 Validation Loss: 1.0465961694717407\n",
      "Epoch 2061: Training Loss: 0.5245461463928223 Validation Loss: 1.0478081703186035\n",
      "Epoch 2062: Training Loss: 0.5239147146542867 Validation Loss: 1.047957181930542\n",
      "Epoch 2063: Training Loss: 0.5226257642110189 Validation Loss: 1.0480926036834717\n",
      "Epoch 2064: Training Loss: 0.5227002501487732 Validation Loss: 1.048405647277832\n",
      "Epoch 2065: Training Loss: 0.5236139297485352 Validation Loss: 1.0477144718170166\n",
      "Epoch 2066: Training Loss: 0.5223979751269022 Validation Loss: 1.0468063354492188\n",
      "Epoch 2067: Training Loss: 0.5220056573549906 Validation Loss: 1.0463117361068726\n",
      "Epoch 2068: Training Loss: 0.5213955839474996 Validation Loss: 1.0463141202926636\n",
      "Epoch 2069: Training Loss: 0.5221882661183676 Validation Loss: 1.0463409423828125\n",
      "Epoch 2070: Training Loss: 0.5209967295328776 Validation Loss: 1.046148657798767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2071: Training Loss: 0.5219214757283529 Validation Loss: 1.0457369089126587\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2072: Training Loss: 0.5216050446033478 Validation Loss: 1.0455082654953003\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2073: Training Loss: 0.5224099854628245 Validation Loss: 1.0458823442459106\n",
      "Epoch 2074: Training Loss: 0.519679973522822 Validation Loss: 1.0448209047317505\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2075: Training Loss: 0.5199892421563467 Validation Loss: 1.0454093217849731\n",
      "Epoch 2076: Training Loss: 0.5197189549605051 Validation Loss: 1.0458557605743408\n",
      "Epoch 2077: Training Loss: 0.5195570190747579 Validation Loss: 1.04573655128479\n",
      "Epoch 2078: Training Loss: 0.5198085208733877 Validation Loss: 1.0453535318374634\n",
      "Epoch 2079: Training Loss: 0.5184688170750936 Validation Loss: 1.0450663566589355\n",
      "Epoch 2080: Training Loss: 0.5188366969426473 Validation Loss: 1.0447555780410767\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2081: Training Loss: 0.5184540748596191 Validation Loss: 1.0441854000091553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2082: Training Loss: 0.519030382235845 Validation Loss: 1.0436694622039795\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2083: Training Loss: 0.518120547135671 Validation Loss: 1.0437432527542114\n",
      "Epoch 2084: Training Loss: 0.5174449682235718 Validation Loss: 1.0429596900939941\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2085: Training Loss: 0.5170763929684957 Validation Loss: 1.0431815385818481\n",
      "Epoch 2086: Training Loss: 0.5166999598344167 Validation Loss: 1.0429661273956299\n",
      "Epoch 2087: Training Loss: 0.5155810415744781 Validation Loss: 1.0430378913879395\n",
      "Epoch 2088: Training Loss: 0.5167576769987742 Validation Loss: 1.0430749654769897\n",
      "Epoch 2089: Training Loss: 0.5148845613002777 Validation Loss: 1.0438647270202637\n",
      "Epoch 2090: Training Loss: 0.5149975717067719 Validation Loss: 1.0445020198822021\n",
      "Epoch 2091: Training Loss: 0.51650537053744 Validation Loss: 1.043351173400879\n",
      "Epoch 2092: Training Loss: 0.5160728096961975 Validation Loss: 1.0424165725708008\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2093: Training Loss: 0.5154843827088674 Validation Loss: 1.042067289352417\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2094: Training Loss: 0.5142269531885783 Validation Loss: 1.041613221168518\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2095: Training Loss: 0.5141105055809021 Validation Loss: 1.0426242351531982\n",
      "Epoch 2096: Training Loss: 0.5134892066319784 Validation Loss: 1.0425240993499756\n",
      "Epoch 2097: Training Loss: 0.5131974220275879 Validation Loss: 1.0425103902816772\n",
      "Epoch 2098: Training Loss: 0.5140367647012075 Validation Loss: 1.0432024002075195\n",
      "Epoch 2099: Training Loss: 0.5126746892929077 Validation Loss: 1.0424294471740723\n",
      "Epoch 2100: Training Loss: 0.5133446355660757 Validation Loss: 1.0412845611572266\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2101: Training Loss: 0.5124615132808685 Validation Loss: 1.0411360263824463\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2102: Training Loss: 0.5115750630696615 Validation Loss: 1.041569471359253\n",
      "Epoch 2103: Training Loss: 0.5128891368707021 Validation Loss: 1.0419045686721802\n",
      "Epoch 2104: Training Loss: 0.5112196902434031 Validation Loss: 1.04098379611969\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2105: Training Loss: 0.5113237599531809 Validation Loss: 1.0402063131332397\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2106: Training Loss: 0.511294811964035 Validation Loss: 1.039734959602356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2107: Training Loss: 0.510560154914856 Validation Loss: 1.0391722917556763\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2108: Training Loss: 0.5105126400788625 Validation Loss: 1.040089726448059\n",
      "Epoch 2109: Training Loss: 0.5092332859834036 Validation Loss: 1.0407075881958008\n",
      "Epoch 2110: Training Loss: 0.5101108253002167 Validation Loss: 1.0401923656463623\n",
      "Epoch 2111: Training Loss: 0.5120322604974111 Validation Loss: 1.0408858060836792\n",
      "Epoch 2112: Training Loss: 0.5091724296410879 Validation Loss: 1.0402519702911377\n",
      "Epoch 2113: Training Loss: 0.5091369946797689 Validation Loss: 1.039475440979004\n",
      "Epoch 2114: Training Loss: 0.5088827113310496 Validation Loss: 1.0380752086639404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2115: Training Loss: 0.5081247389316559 Validation Loss: 1.0380496978759766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2116: Training Loss: 0.5085070033868154 Validation Loss: 1.03892982006073\n",
      "Epoch 2117: Training Loss: 0.5083504617214203 Validation Loss: 1.0388317108154297\n",
      "Epoch 2118: Training Loss: 0.5076670149962107 Validation Loss: 1.0390735864639282\n",
      "Epoch 2119: Training Loss: 0.5074785550435384 Validation Loss: 1.0391920804977417\n",
      "Epoch 2120: Training Loss: 0.5080931782722473 Validation Loss: 1.0382450819015503\n",
      "Epoch 2121: Training Loss: 0.5067555209000906 Validation Loss: 1.037698745727539\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2122: Training Loss: 0.5083399216334025 Validation Loss: 1.0382081270217896\n",
      "Epoch 2123: Training Loss: 0.5084658066431681 Validation Loss: 1.038069486618042\n",
      "Epoch 2124: Training Loss: 0.5072862605253855 Validation Loss: 1.037262201309204\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2125: Training Loss: 0.505381445089976 Validation Loss: 1.038468599319458\n",
      "Epoch 2126: Training Loss: 0.5051819284756979 Validation Loss: 1.0384591817855835\n",
      "Epoch 2127: Training Loss: 0.505616287390391 Validation Loss: 1.0388836860656738\n",
      "Epoch 2128: Training Loss: 0.5038187106450399 Validation Loss: 1.038022518157959\n",
      "Epoch 2129: Training Loss: 0.5047605435053507 Validation Loss: 1.0374027490615845\n",
      "Epoch 2130: Training Loss: 0.504562497138977 Validation Loss: 1.0373269319534302\n",
      "Epoch 2131: Training Loss: 0.5037864446640015 Validation Loss: 1.036702275276184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2132: Training Loss: 0.5045074820518494 Validation Loss: 1.0365618467330933\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2133: Training Loss: 0.5033944745858511 Validation Loss: 1.0363103151321411\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2134: Training Loss: 0.5029924511909485 Validation Loss: 1.0368452072143555\n",
      "Epoch 2135: Training Loss: 0.5027832686901093 Validation Loss: 1.0368252992630005\n",
      "Epoch 2136: Training Loss: 0.5040815472602844 Validation Loss: 1.0370731353759766\n",
      "Epoch 2137: Training Loss: 0.5025847355524699 Validation Loss: 1.0368363857269287\n",
      "Epoch 2138: Training Loss: 0.502118190129598 Validation Loss: 1.0357351303100586\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2139: Training Loss: 0.5017598470052084 Validation Loss: 1.0350137948989868\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2140: Training Loss: 0.5026199916998545 Validation Loss: 1.0345730781555176\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2141: Training Loss: 0.5032405157883962 Validation Loss: 1.035597801208496\n",
      "Epoch 2142: Training Loss: 0.5007203618685404 Validation Loss: 1.0359455347061157\n",
      "Epoch 2143: Training Loss: 0.5001515845457712 Validation Loss: 1.0345078706741333\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2144: Training Loss: 0.4998623828093211 Validation Loss: 1.0350420475006104\n",
      "Epoch 2145: Training Loss: 0.5022812187671661 Validation Loss: 1.0357019901275635\n",
      "Epoch 2146: Training Loss: 0.5001388788223267 Validation Loss: 1.0347938537597656\n",
      "Epoch 2147: Training Loss: 0.500068744023641 Validation Loss: 1.0352588891983032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2148: Training Loss: 0.4992336829503377 Validation Loss: 1.034204125404358\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2149: Training Loss: 0.49803124864896137 Validation Loss: 1.0338231325149536\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2150: Training Loss: 0.5003679990768433 Validation Loss: 1.033913016319275\n",
      "Epoch 2151: Training Loss: 0.4988538424173991 Validation Loss: 1.0343666076660156\n",
      "Epoch 2152: Training Loss: 0.49977413813273114 Validation Loss: 1.0335495471954346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2153: Training Loss: 0.49932416280110675 Validation Loss: 1.0330522060394287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2154: Training Loss: 0.49813657999038696 Validation Loss: 1.0340452194213867\n",
      "Epoch 2155: Training Loss: 0.49721701939900714 Validation Loss: 1.03395676612854\n",
      "Epoch 2156: Training Loss: 0.498493750890096 Validation Loss: 1.0334558486938477\n",
      "Epoch 2157: Training Loss: 0.4979090988636017 Validation Loss: 1.0336320400238037\n",
      "Epoch 2158: Training Loss: 0.49666083852450055 Validation Loss: 1.0332362651824951\n",
      "Epoch 2159: Training Loss: 0.49808651208877563 Validation Loss: 1.0328283309936523\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2160: Training Loss: 0.49703986446062726 Validation Loss: 1.0326188802719116\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2161: Training Loss: 0.4954700867335002 Validation Loss: 1.0338313579559326\n",
      "Epoch 2162: Training Loss: 0.4948592285315196 Validation Loss: 1.0328365564346313\n",
      "Epoch 2163: Training Loss: 0.4962432583173116 Validation Loss: 1.0320508480072021\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2164: Training Loss: 0.4950321714083354 Validation Loss: 1.0313502550125122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2165: Training Loss: 0.49514949321746826 Validation Loss: 1.0314242839813232\n",
      "Epoch 2166: Training Loss: 0.4945320785045624 Validation Loss: 1.0316674709320068\n",
      "Epoch 2167: Training Loss: 0.49416399995485943 Validation Loss: 1.0317981243133545\n",
      "Epoch 2168: Training Loss: 0.49395081400871277 Validation Loss: 1.0318093299865723\n",
      "Epoch 2169: Training Loss: 0.49280346433321637 Validation Loss: 1.0315760374069214\n",
      "Epoch 2170: Training Loss: 0.49374441305796307 Validation Loss: 1.0311206579208374\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2171: Training Loss: 0.4933453897635142 Validation Loss: 1.0314239263534546\n",
      "Epoch 2172: Training Loss: 0.49365272124608356 Validation Loss: 1.0316381454467773\n",
      "Epoch 2173: Training Loss: 0.4932536582152049 Validation Loss: 1.030647873878479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2174: Training Loss: 0.49264933665593463 Validation Loss: 1.0303443670272827\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2175: Training Loss: 0.4926567077636719 Validation Loss: 1.03037428855896\n",
      "Epoch 2176: Training Loss: 0.4909922977288564 Validation Loss: 1.0308629274368286\n",
      "Epoch 2177: Training Loss: 0.49335580070813495 Validation Loss: 1.030360460281372\n",
      "Epoch 2178: Training Loss: 0.4910092055797577 Validation Loss: 1.030349850654602\n",
      "Epoch 2179: Training Loss: 0.49156100551287335 Validation Loss: 1.030685305595398\n",
      "Epoch 2180: Training Loss: 0.4908642868200938 Validation Loss: 1.0310238599777222\n",
      "Epoch 2181: Training Loss: 0.4912342429161072 Validation Loss: 1.03042471408844\n",
      "Epoch 2182: Training Loss: 0.4910134474436442 Validation Loss: 1.0290541648864746\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2183: Training Loss: 0.4916960696379344 Validation Loss: 1.028605341911316\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2184: Training Loss: 0.4899272720019023 Validation Loss: 1.0295888185501099\n",
      "Epoch 2185: Training Loss: 0.4916130006313324 Validation Loss: 1.0292452573776245\n",
      "Epoch 2186: Training Loss: 0.4910753667354584 Validation Loss: 1.029091477394104\n",
      "Epoch 2187: Training Loss: 0.489687184492747 Validation Loss: 1.0297468900680542\n",
      "Epoch 2188: Training Loss: 0.48815154035886127 Validation Loss: 1.0303077697753906\n",
      "Epoch 2189: Training Loss: 0.48843716581662494 Validation Loss: 1.0302249193191528\n",
      "Epoch 2190: Training Loss: 0.48826897144317627 Validation Loss: 1.0289459228515625\n",
      "Epoch 2191: Training Loss: 0.4882807234923045 Validation Loss: 1.027960181236267\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2192: Training Loss: 0.48787542184193927 Validation Loss: 1.0284953117370605\n",
      "Epoch 2193: Training Loss: 0.48763323823610943 Validation Loss: 1.0282058715820312\n",
      "Epoch 2194: Training Loss: 0.49435314536094666 Validation Loss: 1.028536319732666\n",
      "Epoch 2195: Training Loss: 0.488040695587794 Validation Loss: 1.0288788080215454\n",
      "Epoch 2196: Training Loss: 0.4863779942194621 Validation Loss: 1.0279163122177124\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2197: Training Loss: 0.4864128033320109 Validation Loss: 1.0273736715316772\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2198: Training Loss: 0.4875100453694661 Validation Loss: 1.0275040864944458\n",
      "Epoch 2199: Training Loss: 0.4869473874568939 Validation Loss: 1.0278490781784058\n",
      "Epoch 2200: Training Loss: 0.4867714047431946 Validation Loss: 1.0281789302825928\n",
      "Epoch 2201: Training Loss: 0.4863240619500478 Validation Loss: 1.0274102687835693\n",
      "Epoch 2202: Training Loss: 0.48565075794855755 Validation Loss: 1.02683687210083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2203: Training Loss: 0.48470671971638996 Validation Loss: 1.0262147188186646\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2204: Training Loss: 0.48720020055770874 Validation Loss: 1.0268988609313965\n",
      "Epoch 2205: Training Loss: 0.4841691255569458 Validation Loss: 1.026815414428711\n",
      "Epoch 2206: Training Loss: 0.4854215284188588 Validation Loss: 1.0260456800460815\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2207: Training Loss: 0.4850432475407918 Validation Loss: 1.0266318321228027\n",
      "Epoch 2208: Training Loss: 0.48402609427769977 Validation Loss: 1.0266759395599365\n",
      "Epoch 2209: Training Loss: 0.4831082820892334 Validation Loss: 1.0263880491256714\n",
      "Epoch 2210: Training Loss: 0.48355833689371747 Validation Loss: 1.0267162322998047\n",
      "Epoch 2211: Training Loss: 0.4831855793793996 Validation Loss: 1.0263636112213135\n",
      "Epoch 2212: Training Loss: 0.48335466782251996 Validation Loss: 1.0249342918395996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2213: Training Loss: 0.484715203444163 Validation Loss: 1.0246878862380981\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2214: Training Loss: 0.4821295936902364 Validation Loss: 1.0252227783203125\n",
      "Epoch 2215: Training Loss: 0.48210633794466656 Validation Loss: 1.026193380355835\n",
      "Epoch 2216: Training Loss: 0.480899045864741 Validation Loss: 1.026128888130188\n",
      "Epoch 2217: Training Loss: 0.48183026909828186 Validation Loss: 1.025864839553833\n",
      "Epoch 2218: Training Loss: 0.48114155729611713 Validation Loss: 1.0243074893951416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2219: Training Loss: 0.48195236921310425 Validation Loss: 1.0243573188781738\n",
      "Epoch 2220: Training Loss: 0.4819541772206624 Validation Loss: 1.025288462638855\n",
      "Epoch 2221: Training Loss: 0.4802280267079671 Validation Loss: 1.02559232711792\n",
      "Epoch 2222: Training Loss: 0.481904278198878 Validation Loss: 1.0251675844192505\n",
      "Epoch 2223: Training Loss: 0.48000669479370117 Validation Loss: 1.024448275566101\n",
      "Epoch 2224: Training Loss: 0.4795716305573781 Validation Loss: 1.0232678651809692\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2225: Training Loss: 0.4798143009344737 Validation Loss: 1.023264765739441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2226: Training Loss: 0.4790411988894145 Validation Loss: 1.0229026079177856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2227: Training Loss: 0.47968191901842755 Validation Loss: 1.0228592157363892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2228: Training Loss: 0.47865939140319824 Validation Loss: 1.0232367515563965\n",
      "Epoch 2229: Training Loss: 0.47844242056210834 Validation Loss: 1.024137258529663\n",
      "Epoch 2230: Training Loss: 0.47992075483004254 Validation Loss: 1.0240994691848755\n",
      "Epoch 2231: Training Loss: 0.48093899091084796 Validation Loss: 1.0236490964889526\n",
      "Epoch 2232: Training Loss: 0.4789371192455292 Validation Loss: 1.023676872253418\n",
      "Epoch 2233: Training Loss: 0.47908636927604675 Validation Loss: 1.023215413093567\n",
      "Epoch 2234: Training Loss: 0.47763686378796893 Validation Loss: 1.023459792137146\n",
      "Epoch 2235: Training Loss: 0.4769442180792491 Validation Loss: 1.0229462385177612\n",
      "Epoch 2236: Training Loss: 0.4761398732662201 Validation Loss: 1.0222539901733398\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2237: Training Loss: 0.4768771330515544 Validation Loss: 1.0227267742156982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2238: Training Loss: 0.47834614912668866 Validation Loss: 1.0220388174057007\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2239: Training Loss: 0.47602113087972003 Validation Loss: 1.0223283767700195\n",
      "Epoch 2240: Training Loss: 0.47495728731155396 Validation Loss: 1.0226565599441528\n",
      "Epoch 2241: Training Loss: 0.4765201012293498 Validation Loss: 1.0222285985946655\n",
      "Epoch 2242: Training Loss: 0.4747871557871501 Validation Loss: 1.0221889019012451\n",
      "Epoch 2243: Training Loss: 0.4756147861480713 Validation Loss: 1.0215678215026855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2244: Training Loss: 0.4749471644560496 Validation Loss: 1.0214831829071045\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2245: Training Loss: 0.4744572937488556 Validation Loss: 1.0214694738388062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2246: Training Loss: 0.47457432746887207 Validation Loss: 1.0213136672973633\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2247: Training Loss: 0.47386788328488666 Validation Loss: 1.0218729972839355\n",
      "Epoch 2248: Training Loss: 0.47591476639111835 Validation Loss: 1.0218708515167236\n",
      "Epoch 2249: Training Loss: 0.4734449088573456 Validation Loss: 1.0216679573059082\n",
      "Epoch 2250: Training Loss: 0.4733591576417287 Validation Loss: 1.0210245847702026\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2251: Training Loss: 0.4724495013554891 Validation Loss: 1.020261526107788\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2252: Training Loss: 0.47381611665089923 Validation Loss: 1.019935131072998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2253: Training Loss: 0.473007470369339 Validation Loss: 1.0194660425186157\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2254: Training Loss: 0.4721649984518687 Validation Loss: 1.019526720046997\n",
      "Epoch 2255: Training Loss: 0.4713467061519623 Validation Loss: 1.020890474319458\n",
      "Epoch 2256: Training Loss: 0.4715263942877452 Validation Loss: 1.0207041501998901\n",
      "Epoch 2257: Training Loss: 0.47130805253982544 Validation Loss: 1.020601749420166\n",
      "Epoch 2258: Training Loss: 0.4718646506468455 Validation Loss: 1.0202420949935913\n",
      "Epoch 2259: Training Loss: 0.47122252980868023 Validation Loss: 1.0204240083694458\n",
      "Epoch 2260: Training Loss: 0.47107138236363727 Validation Loss: 1.020855188369751\n",
      "Epoch 2261: Training Loss: 0.470528503259023 Validation Loss: 1.0193262100219727\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2262: Training Loss: 0.47041745980580646 Validation Loss: 1.019399881362915\n",
      "Epoch 2263: Training Loss: 0.4718448023001353 Validation Loss: 1.0201414823532104\n",
      "Epoch 2264: Training Loss: 0.4695114294687907 Validation Loss: 1.0192228555679321\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2265: Training Loss: 0.46965651710828143 Validation Loss: 1.0191187858581543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2266: Training Loss: 0.46975472569465637 Validation Loss: 1.019331693649292\n",
      "Epoch 2267: Training Loss: 0.4689825475215912 Validation Loss: 1.0181728601455688\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2268: Training Loss: 0.4697766403357188 Validation Loss: 1.0182368755340576\n",
      "Epoch 2269: Training Loss: 0.4679556091626485 Validation Loss: 1.0184497833251953\n",
      "Epoch 2270: Training Loss: 0.4683289925257365 Validation Loss: 1.017848253250122\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2271: Training Loss: 0.4693986773490906 Validation Loss: 1.0182312726974487\n",
      "Epoch 2272: Training Loss: 0.4691117505232493 Validation Loss: 1.0199288129806519\n",
      "Epoch 2273: Training Loss: 0.4676823318004608 Validation Loss: 1.0204756259918213\n",
      "Epoch 2274: Training Loss: 0.46968694527943927 Validation Loss: 1.0192986726760864\n",
      "Epoch 2275: Training Loss: 0.4675522843996684 Validation Loss: 1.0190131664276123\n",
      "Epoch 2276: Training Loss: 0.46588940421740216 Validation Loss: 1.0179539918899536\n",
      "Epoch 2277: Training Loss: 0.46640779574712116 Validation Loss: 1.0171332359313965\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2278: Training Loss: 0.46629340449968976 Validation Loss: 1.0169930458068848\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2279: Training Loss: 0.46591058373451233 Validation Loss: 1.0176769495010376\n",
      "Epoch 2280: Training Loss: 0.465914120276769 Validation Loss: 1.0179046392440796\n",
      "Epoch 2281: Training Loss: 0.4650428593158722 Validation Loss: 1.0168095827102661\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2282: Training Loss: 0.46573033928871155 Validation Loss: 1.0177698135375977\n",
      "Epoch 2283: Training Loss: 0.4668154815832774 Validation Loss: 1.0171399116516113\n",
      "Epoch 2284: Training Loss: 0.46462459365526837 Validation Loss: 1.0171467065811157\n",
      "Epoch 2285: Training Loss: 0.4642506142457326 Validation Loss: 1.0163427591323853\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2286: Training Loss: 0.4656764765580495 Validation Loss: 1.0160220861434937\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2287: Training Loss: 0.4664239188035329 Validation Loss: 1.0161545276641846\n",
      "Epoch 2288: Training Loss: 0.4675578971703847 Validation Loss: 1.0170581340789795\n",
      "Epoch 2289: Training Loss: 0.46423275272051495 Validation Loss: 1.0172135829925537\n",
      "Epoch 2290: Training Loss: 0.4676145613193512 Validation Loss: 1.018072247505188\n",
      "Epoch 2291: Training Loss: 0.46341519554456073 Validation Loss: 1.0167568922042847\n",
      "Epoch 2292: Training Loss: 0.46290789047876996 Validation Loss: 1.0162831544876099\n",
      "Epoch 2293: Training Loss: 0.46186644832293194 Validation Loss: 1.016052007675171\n",
      "Epoch 2294: Training Loss: 0.46310606598854065 Validation Loss: 1.016126036643982\n",
      "Epoch 2295: Training Loss: 0.46295349796613056 Validation Loss: 1.0168395042419434\n",
      "Epoch 2296: Training Loss: 0.46229122082392377 Validation Loss: 1.016094446182251\n",
      "Epoch 2297: Training Loss: 0.4619954824447632 Validation Loss: 1.0153456926345825\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2298: Training Loss: 0.4616650144259135 Validation Loss: 1.0151526927947998\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2299: Training Loss: 0.4613965650399526 Validation Loss: 1.0150974988937378\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2300: Training Loss: 0.4612252712249756 Validation Loss: 1.0148727893829346\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2301: Training Loss: 0.4608115057150523 Validation Loss: 1.014649748802185\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2302: Training Loss: 0.4610586067040761 Validation Loss: 1.0146455764770508\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2303: Training Loss: 0.46153273185094196 Validation Loss: 1.0146018266677856\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2304: Training Loss: 0.46284053723017377 Validation Loss: 1.0148000717163086\n",
      "Epoch 2305: Training Loss: 0.4606616993745168 Validation Loss: 1.0152180194854736\n",
      "Epoch 2306: Training Loss: 0.4597211480140686 Validation Loss: 1.0146137475967407\n",
      "Epoch 2307: Training Loss: 0.46018393834431964 Validation Loss: 1.014886498451233\n",
      "Epoch 2308: Training Loss: 0.4597012996673584 Validation Loss: 1.0141911506652832\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2309: Training Loss: 0.4590967396895091 Validation Loss: 1.0147918462753296\n",
      "Epoch 2310: Training Loss: 0.4587640662988027 Validation Loss: 1.014106035232544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2311: Training Loss: 0.45953338344891864 Validation Loss: 1.0133837461471558\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2312: Training Loss: 0.45827363928159076 Validation Loss: 1.0124530792236328\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2313: Training Loss: 0.45860658089319867 Validation Loss: 1.0131735801696777\n",
      "Epoch 2314: Training Loss: 0.4579211672147115 Validation Loss: 1.013122797012329\n",
      "Epoch 2315: Training Loss: 0.45835284392038983 Validation Loss: 1.0137802362442017\n",
      "Epoch 2316: Training Loss: 0.45762622356414795 Validation Loss: 1.0146268606185913\n",
      "Epoch 2317: Training Loss: 0.4569348096847534 Validation Loss: 1.0146491527557373\n",
      "Epoch 2318: Training Loss: 0.45760873953501385 Validation Loss: 1.013433575630188\n",
      "Epoch 2319: Training Loss: 0.456528107325236 Validation Loss: 1.0123567581176758\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2320: Training Loss: 0.45648111899693805 Validation Loss: 1.0122008323669434\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2321: Training Loss: 0.45671071608861286 Validation Loss: 1.0127187967300415\n",
      "Epoch 2322: Training Loss: 0.45828848083813983 Validation Loss: 1.0130640268325806\n",
      "Epoch 2323: Training Loss: 0.45590460300445557 Validation Loss: 1.012730598449707\n",
      "Epoch 2324: Training Loss: 0.4570021629333496 Validation Loss: 1.0128660202026367\n",
      "Epoch 2325: Training Loss: 0.45733632644017536 Validation Loss: 1.0136358737945557\n",
      "Epoch 2326: Training Loss: 0.45517881711324054 Validation Loss: 1.0128751993179321\n",
      "Epoch 2327: Training Loss: 0.4549790620803833 Validation Loss: 1.0122894048690796\n",
      "Epoch 2328: Training Loss: 0.4548263947168986 Validation Loss: 1.0116050243377686\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2329: Training Loss: 0.4552258352438609 Validation Loss: 1.0110795497894287\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2330: Training Loss: 0.45408982038497925 Validation Loss: 1.0111002922058105\n",
      "Epoch 2331: Training Loss: 0.45345444480578107 Validation Loss: 1.0112059116363525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2332: Training Loss: 0.45350850621859234 Validation Loss: 1.010813593864441\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2333: Training Loss: 0.454389363527298 Validation Loss: 1.0108946561813354\n",
      "Epoch 2334: Training Loss: 0.4529848098754883 Validation Loss: 1.0117119550704956\n",
      "Epoch 2335: Training Loss: 0.45330024758974713 Validation Loss: 1.0116791725158691\n",
      "Epoch 2336: Training Loss: 0.45279603203137714 Validation Loss: 1.0113850831985474\n",
      "Epoch 2337: Training Loss: 0.4533005754152934 Validation Loss: 1.0109742879867554\n",
      "Epoch 2338: Training Loss: 0.4532468815644582 Validation Loss: 1.0111902952194214\n",
      "Epoch 2339: Training Loss: 0.45242713888486225 Validation Loss: 1.0113511085510254\n",
      "Epoch 2340: Training Loss: 0.45197967688242596 Validation Loss: 1.0115498304367065\n",
      "Epoch 2341: Training Loss: 0.4518369634946187 Validation Loss: 1.0117321014404297\n",
      "Epoch 2342: Training Loss: 0.4523376723130544 Validation Loss: 1.0110242366790771\n",
      "Epoch 2343: Training Loss: 0.4509631594022115 Validation Loss: 1.0107874870300293\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2344: Training Loss: 0.45151832699775696 Validation Loss: 1.009537935256958\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2345: Training Loss: 0.452437162399292 Validation Loss: 1.0090668201446533\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2346: Training Loss: 0.45190879702568054 Validation Loss: 1.0098289251327515\n",
      "Epoch 2347: Training Loss: 0.44923506180445355 Validation Loss: 1.0102660655975342\n",
      "Epoch 2348: Training Loss: 0.4500597318013509 Validation Loss: 1.009750247001648\n",
      "Epoch 2349: Training Loss: 0.4514385561148326 Validation Loss: 1.0103524923324585\n",
      "Epoch 2350: Training Loss: 0.4514903128147125 Validation Loss: 1.0097894668579102\n",
      "Epoch 2351: Training Loss: 0.4496247073014577 Validation Loss: 1.0099124908447266\n",
      "Epoch 2352: Training Loss: 0.4496287206808726 Validation Loss: 1.0104551315307617\n",
      "Epoch 2353: Training Loss: 0.44840672612190247 Validation Loss: 1.0103785991668701\n",
      "Epoch 2354: Training Loss: 0.4486871262391408 Validation Loss: 1.0102630853652954\n",
      "Epoch 2355: Training Loss: 0.448763112227122 Validation Loss: 1.0089441537857056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2356: Training Loss: 0.448906143506368 Validation Loss: 1.0085963010787964\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2357: Training Loss: 0.44779325524965924 Validation Loss: 1.0085577964782715\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2358: Training Loss: 0.4474569857120514 Validation Loss: 1.0084068775177002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2359: Training Loss: 0.44679754972457886 Validation Loss: 1.0078256130218506\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2360: Training Loss: 0.4475219150384267 Validation Loss: 1.0075868368148804\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2361: Training Loss: 0.44703251123428345 Validation Loss: 1.0085386037826538\n",
      "Epoch 2362: Training Loss: 0.4469309051831563 Validation Loss: 1.0086699724197388\n",
      "Epoch 2363: Training Loss: 0.446698397397995 Validation Loss: 1.0089306831359863\n",
      "Epoch 2364: Training Loss: 0.4464777608712514 Validation Loss: 1.0086780786514282\n",
      "Epoch 2365: Training Loss: 0.4466166893641154 Validation Loss: 1.00934636592865\n",
      "Epoch 2366: Training Loss: 0.44556934634844464 Validation Loss: 1.0083674192428589\n",
      "Epoch 2367: Training Loss: 0.446344792842865 Validation Loss: 1.0084142684936523\n",
      "Epoch 2368: Training Loss: 0.4465359052022298 Validation Loss: 1.0090869665145874\n",
      "Epoch 2369: Training Loss: 0.4458613495031993 Validation Loss: 1.008743166923523\n",
      "Epoch 2370: Training Loss: 0.44597577055295307 Validation Loss: 1.0075287818908691\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2371: Training Loss: 0.4451569815476735 Validation Loss: 1.0075299739837646\n",
      "Epoch 2372: Training Loss: 0.44475021958351135 Validation Loss: 1.0075913667678833\n",
      "Epoch 2373: Training Loss: 0.44606825709342957 Validation Loss: 1.00606107711792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2374: Training Loss: 0.4451608459154765 Validation Loss: 1.0061633586883545\n",
      "Epoch 2375: Training Loss: 0.443647176027298 Validation Loss: 1.007045030593872\n",
      "Epoch 2376: Training Loss: 0.4436458945274353 Validation Loss: 1.0069361925125122\n",
      "Epoch 2377: Training Loss: 0.4431452751159668 Validation Loss: 1.0066369771957397\n",
      "Epoch 2378: Training Loss: 0.44356661041577655 Validation Loss: 1.0066537857055664\n",
      "Epoch 2379: Training Loss: 0.4426417648792267 Validation Loss: 1.0065199136734009\n",
      "Epoch 2380: Training Loss: 0.4423312544822693 Validation Loss: 1.0073518753051758\n",
      "Epoch 2381: Training Loss: 0.4432574013868968 Validation Loss: 1.006983995437622\n",
      "Epoch 2382: Training Loss: 0.4428311189015706 Validation Loss: 1.006435513496399\n",
      "Epoch 2383: Training Loss: 0.4426769216855367 Validation Loss: 1.005927324295044\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2384: Training Loss: 0.4417933225631714 Validation Loss: 1.0060877799987793\n",
      "Epoch 2385: Training Loss: 0.4412112236022949 Validation Loss: 1.0058963298797607\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2386: Training Loss: 0.4416261812051137 Validation Loss: 1.0059192180633545\n",
      "Epoch 2387: Training Loss: 0.4416652321815491 Validation Loss: 1.0058188438415527\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2388: Training Loss: 0.4415281613667806 Validation Loss: 1.0064369440078735\n",
      "Epoch 2389: Training Loss: 0.4411601424217224 Validation Loss: 1.005595326423645\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2390: Training Loss: 0.44118690490722656 Validation Loss: 1.0059840679168701\n",
      "Epoch 2391: Training Loss: 0.44081010421117145 Validation Loss: 1.005513072013855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2392: Training Loss: 0.4409448504447937 Validation Loss: 1.0058091878890991\n",
      "Epoch 2393: Training Loss: 0.4419952829678853 Validation Loss: 1.0055344104766846\n",
      "Epoch 2394: Training Loss: 0.4421081741650899 Validation Loss: 1.0061209201812744\n",
      "Epoch 2395: Training Loss: 0.44083162148793537 Validation Loss: 1.0056612491607666\n",
      "Epoch 2396: Training Loss: 0.43899447719256085 Validation Loss: 1.0047410726547241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2397: Training Loss: 0.43920307358105976 Validation Loss: 1.0041717290878296\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2398: Training Loss: 0.43899543086687726 Validation Loss: 1.0049540996551514\n",
      "Epoch 2399: Training Loss: 0.4399205942948659 Validation Loss: 1.004902720451355\n",
      "Epoch 2400: Training Loss: 0.4386349717775981 Validation Loss: 1.004885196685791\n",
      "Epoch 2401: Training Loss: 0.4380413591861725 Validation Loss: 1.0043374300003052\n",
      "Epoch 2402: Training Loss: 0.43822787205378216 Validation Loss: 1.004528522491455\n",
      "Epoch 2403: Training Loss: 0.43736785650253296 Validation Loss: 1.0045981407165527\n",
      "Epoch 2404: Training Loss: 0.440168430407842 Validation Loss: 1.0055800676345825\n",
      "Epoch 2405: Training Loss: 0.43788371483484906 Validation Loss: 1.0042787790298462\n",
      "Epoch 2406: Training Loss: 0.43694637219111127 Validation Loss: 1.003979206085205\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2407: Training Loss: 0.43780901034673053 Validation Loss: 1.003566026687622\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2408: Training Loss: 0.4376632173856099 Validation Loss: 1.0034116506576538\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2409: Training Loss: 0.43786153197288513 Validation Loss: 1.0031523704528809\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2410: Training Loss: 0.43920525908470154 Validation Loss: 1.0041571855545044\n",
      "Epoch 2411: Training Loss: 0.43823949495951336 Validation Loss: 1.0043612718582153\n",
      "Epoch 2412: Training Loss: 0.43637119730313617 Validation Loss: 1.0040066242218018\n",
      "Epoch 2413: Training Loss: 0.43583496411641437 Validation Loss: 1.003717303276062\n",
      "Epoch 2414: Training Loss: 0.4365844925244649 Validation Loss: 1.0032237768173218\n",
      "Epoch 2415: Training Loss: 0.43541940053304035 Validation Loss: 1.0031059980392456\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2416: Training Loss: 0.4356054365634918 Validation Loss: 1.0040980577468872\n",
      "Epoch 2417: Training Loss: 0.43520853916804 Validation Loss: 1.0033044815063477\n",
      "Epoch 2418: Training Loss: 0.4344245990117391 Validation Loss: 1.0029189586639404\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2419: Training Loss: 0.4360378086566925 Validation Loss: 1.0028177499771118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2420: Training Loss: 0.4331599275271098 Validation Loss: 1.0026170015335083\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2421: Training Loss: 0.43520236015319824 Validation Loss: 1.0020555257797241\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2422: Training Loss: 0.43547184268633526 Validation Loss: 1.0021533966064453\n",
      "Epoch 2423: Training Loss: 0.4346167743206024 Validation Loss: 1.0028884410858154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2424: Training Loss: 0.4342745939890544 Validation Loss: 1.003230094909668\n",
      "Epoch 2425: Training Loss: 0.43571901321411133 Validation Loss: 1.0037795305252075\n",
      "Epoch 2426: Training Loss: 0.4324639638264974 Validation Loss: 1.0024700164794922\n",
      "Epoch 2427: Training Loss: 0.4346799651781718 Validation Loss: 1.00159752368927\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2428: Training Loss: 0.4335382084051768 Validation Loss: 1.000852108001709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2429: Training Loss: 0.4335791567961375 Validation Loss: 1.0012397766113281\n",
      "Epoch 2430: Training Loss: 0.43200825651486713 Validation Loss: 1.0012670755386353\n",
      "Epoch 2431: Training Loss: 0.43156469861666363 Validation Loss: 1.0014609098434448\n",
      "Epoch 2432: Training Loss: 0.4328995943069458 Validation Loss: 1.0019326210021973\n",
      "Epoch 2433: Training Loss: 0.43132583300272626 Validation Loss: 1.0019382238388062\n",
      "Epoch 2434: Training Loss: 0.4325169424215953 Validation Loss: 1.0013357400894165\n",
      "Epoch 2435: Training Loss: 0.4307546118895213 Validation Loss: 1.0008876323699951\n",
      "Epoch 2436: Training Loss: 0.43187103668848675 Validation Loss: 1.0003787279129028\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2437: Training Loss: 0.43079936504364014 Validation Loss: 1.0008649826049805\n",
      "Epoch 2438: Training Loss: 0.4304455518722534 Validation Loss: 1.0006619691848755\n",
      "Epoch 2439: Training Loss: 0.4316624899705251 Validation Loss: 1.0007961988449097\n",
      "Epoch 2440: Training Loss: 0.4299418131510417 Validation Loss: 1.0013025999069214\n",
      "Epoch 2441: Training Loss: 0.4297231435775757 Validation Loss: 1.0012013912200928\n",
      "Epoch 2442: Training Loss: 0.4293601910273234 Validation Loss: 1.0022450685501099\n",
      "Epoch 2443: Training Loss: 0.42986785372098285 Validation Loss: 1.0027168989181519\n",
      "Epoch 2444: Training Loss: 0.42955100536346436 Validation Loss: 1.0014175176620483\n",
      "Epoch 2445: Training Loss: 0.42909382780392963 Validation Loss: 1.0008124113082886\n",
      "Epoch 2446: Training Loss: 0.4292878309885661 Validation Loss: 1.0000816583633423\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2447: Training Loss: 0.4295474886894226 Validation Loss: 0.9997681975364685\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2448: Training Loss: 0.42863428592681885 Validation Loss: 1.0002278089523315\n",
      "Epoch 2449: Training Loss: 0.42852269609769184 Validation Loss: 1.0005794763565063\n",
      "Epoch 2450: Training Loss: 0.4279300471146901 Validation Loss: 1.0003591775894165\n",
      "Epoch 2451: Training Loss: 0.42805753151575726 Validation Loss: 0.9997170567512512\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2452: Training Loss: 0.42697927355766296 Validation Loss: 0.9996060132980347\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2453: Training Loss: 0.42766717076301575 Validation Loss: 0.9991897344589233\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2454: Training Loss: 0.4269750813643138 Validation Loss: 0.9993557929992676\n",
      "Epoch 2455: Training Loss: 0.42771197358767193 Validation Loss: 1.0001647472381592\n",
      "Epoch 2456: Training Loss: 0.4288765986760457 Validation Loss: 1.0005455017089844\n",
      "Epoch 2457: Training Loss: 0.4267115791638692 Validation Loss: 1.0003831386566162\n",
      "Epoch 2458: Training Loss: 0.42473875482877094 Validation Loss: 0.9997246265411377\n",
      "Epoch 2459: Training Loss: 0.4266100029150645 Validation Loss: 0.9984647631645203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2460: Training Loss: 0.4269035855929057 Validation Loss: 0.9986016750335693\n",
      "Epoch 2461: Training Loss: 0.4264000554879506 Validation Loss: 0.9983820915222168\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2462: Training Loss: 0.42709432045618695 Validation Loss: 0.9985657930374146\n",
      "Epoch 2463: Training Loss: 0.427217294772466 Validation Loss: 0.9988991022109985\n",
      "Epoch 2464: Training Loss: 0.42563096682230633 Validation Loss: 0.9986380934715271\n",
      "Epoch 2465: Training Loss: 0.42573726177215576 Validation Loss: 0.998656153678894\n",
      "Epoch 2466: Training Loss: 0.4248417317867279 Validation Loss: 0.9993540644645691\n",
      "Epoch 2467: Training Loss: 0.4252401491006215 Validation Loss: 0.9991326928138733\n",
      "Epoch 2468: Training Loss: 0.4237086872259776 Validation Loss: 0.9995115995407104\n",
      "Epoch 2469: Training Loss: 0.42422063151995343 Validation Loss: 0.9984055757522583\n",
      "Epoch 2470: Training Loss: 0.4240388770898183 Validation Loss: 0.9978657364845276\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2471: Training Loss: 0.42436978220939636 Validation Loss: 0.9980387091636658\n",
      "Epoch 2472: Training Loss: 0.4237568974494934 Validation Loss: 0.998151957988739\n",
      "Epoch 2473: Training Loss: 0.42452622453371686 Validation Loss: 0.9977902173995972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2474: Training Loss: 0.4230372707049052 Validation Loss: 0.9983470439910889\n",
      "Epoch 2475: Training Loss: 0.42285176118214923 Validation Loss: 0.9986985921859741\n",
      "Epoch 2476: Training Loss: 0.4225931167602539 Validation Loss: 0.9984724521636963\n",
      "Epoch 2477: Training Loss: 0.4235294957955678 Validation Loss: 0.9979956746101379\n",
      "Epoch 2478: Training Loss: 0.4227875272432963 Validation Loss: 0.9973839521408081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2479: Training Loss: 0.42218145728111267 Validation Loss: 0.9976311326026917\n",
      "Epoch 2480: Training Loss: 0.42346563935279846 Validation Loss: 0.9971553087234497\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2481: Training Loss: 0.42333895961443585 Validation Loss: 0.9971196055412292\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2482: Training Loss: 0.4216443598270416 Validation Loss: 0.9971722960472107\n",
      "Epoch 2483: Training Loss: 0.4227493306001027 Validation Loss: 0.9976649284362793\n",
      "Epoch 2484: Training Loss: 0.42097344994544983 Validation Loss: 0.9973859786987305\n",
      "Epoch 2485: Training Loss: 0.42118382453918457 Validation Loss: 0.9968714118003845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2486: Training Loss: 0.4222669303417206 Validation Loss: 0.9979018568992615\n",
      "Epoch 2487: Training Loss: 0.4216561516125997 Validation Loss: 0.9974196553230286\n",
      "Epoch 2488: Training Loss: 0.4200872977574666 Validation Loss: 0.9971532821655273\n",
      "Epoch 2489: Training Loss: 0.41951820254325867 Validation Loss: 0.9968228340148926\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2490: Training Loss: 0.42006903886795044 Validation Loss: 0.9967054128646851\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2491: Training Loss: 0.4196303188800812 Validation Loss: 0.996366560459137\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2492: Training Loss: 0.41975008447964984 Validation Loss: 0.9957809448242188\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2493: Training Loss: 0.41982849438985187 Validation Loss: 0.9957376718521118\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2494: Training Loss: 0.419882466395696 Validation Loss: 0.9951975345611572\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2495: Training Loss: 0.41890448331832886 Validation Loss: 0.9956742525100708\n",
      "Epoch 2496: Training Loss: 0.41689276695251465 Validation Loss: 0.9957238435745239\n",
      "Epoch 2497: Training Loss: 0.41831405957539874 Validation Loss: 0.9958176612854004\n",
      "Epoch 2498: Training Loss: 0.41792458295822144 Validation Loss: 0.99537593126297\n",
      "Epoch 2499: Training Loss: 0.41959009567896527 Validation Loss: 0.9959121942520142\n",
      "Epoch 2500: Training Loss: 0.4184867839018504 Validation Loss: 0.9964405298233032\n",
      "Epoch 2501: Training Loss: 0.41765086849530536 Validation Loss: 0.9965358376502991\n",
      "Epoch 2502: Training Loss: 0.4172830482323964 Validation Loss: 0.9966139793395996\n",
      "Epoch 2503: Training Loss: 0.4172681470712026 Validation Loss: 0.9965330958366394\n",
      "Epoch 2504: Training Loss: 0.41704320907592773 Validation Loss: 0.9958794713020325\n",
      "Epoch 2505: Training Loss: 0.417339950799942 Validation Loss: 0.9955353736877441\n",
      "Epoch 2506: Training Loss: 0.41675040125846863 Validation Loss: 0.9947007298469543\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2507: Training Loss: 0.4168397784233093 Validation Loss: 0.9943963289260864\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2508: Training Loss: 0.41756367683410645 Validation Loss: 0.994790256023407\n",
      "Epoch 2509: Training Loss: 0.41610434651374817 Validation Loss: 0.9948833584785461\n",
      "Epoch 2510: Training Loss: 0.4161859452724457 Validation Loss: 0.995670735836029\n",
      "Epoch 2511: Training Loss: 0.4176759521166484 Validation Loss: 0.9947201609611511\n",
      "Epoch 2512: Training Loss: 0.41580914457639057 Validation Loss: 0.9951967597007751\n",
      "Epoch 2513: Training Loss: 0.41512975096702576 Validation Loss: 0.9952951073646545\n",
      "Epoch 2514: Training Loss: 0.4152924418449402 Validation Loss: 0.994051456451416\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2515: Training Loss: 0.416138897339503 Validation Loss: 0.9937418103218079\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2516: Training Loss: 0.41655747095743817 Validation Loss: 0.9940963983535767\n",
      "Epoch 2517: Training Loss: 0.41453177730242413 Validation Loss: 0.9937991499900818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2518: Training Loss: 0.41453073422114056 Validation Loss: 0.994325578212738\n",
      "Epoch 2519: Training Loss: 0.4142388204733531 Validation Loss: 0.9946523308753967\n",
      "Epoch 2520: Training Loss: 0.4146180550257365 Validation Loss: 0.9944537281990051\n",
      "Epoch 2521: Training Loss: 0.4160732130209605 Validation Loss: 0.994556725025177\n",
      "Epoch 2522: Training Loss: 0.4142265518506368 Validation Loss: 0.9942089915275574\n",
      "Epoch 2523: Training Loss: 0.4157133797804515 Validation Loss: 0.9942147135734558\n",
      "Epoch 2524: Training Loss: 0.4137146870295207 Validation Loss: 0.9942753314971924\n",
      "Epoch 2525: Training Loss: 0.41305189331372577 Validation Loss: 0.9942721128463745\n",
      "Epoch 2526: Training Loss: 0.4121483365694682 Validation Loss: 0.9937875270843506\n",
      "Epoch 2527: Training Loss: 0.4127745032310486 Validation Loss: 0.9940770864486694\n",
      "Epoch 2528: Training Loss: 0.41291608413060504 Validation Loss: 0.9937003254890442\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2529: Training Loss: 0.41369758049647015 Validation Loss: 0.9932611584663391\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2530: Training Loss: 0.4122922122478485 Validation Loss: 0.9938009977340698\n",
      "Epoch 2531: Training Loss: 0.4122570753097534 Validation Loss: 0.9939945340156555\n",
      "Epoch 2532: Training Loss: 0.4124695360660553 Validation Loss: 0.9940043091773987\n",
      "Epoch 2533: Training Loss: 0.4111163914203644 Validation Loss: 0.9929953217506409\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2534: Training Loss: 0.4121238589286804 Validation Loss: 0.9927160143852234\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2535: Training Loss: 0.4120550751686096 Validation Loss: 0.9927554726600647\n",
      "Epoch 2536: Training Loss: 0.41086889306704205 Validation Loss: 0.9930967092514038\n",
      "Epoch 2537: Training Loss: 0.41200045744578045 Validation Loss: 0.9929489493370056\n",
      "Epoch 2538: Training Loss: 0.41222862402598065 Validation Loss: 0.9925121665000916\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2539: Training Loss: 0.4102003475030263 Validation Loss: 0.992883026599884\n",
      "Epoch 2540: Training Loss: 0.41030187408129376 Validation Loss: 0.99325031042099\n",
      "Epoch 2541: Training Loss: 0.410706490278244 Validation Loss: 0.9927387833595276\n",
      "Epoch 2542: Training Loss: 0.41032936175664264 Validation Loss: 0.9925365447998047\n",
      "Epoch 2543: Training Loss: 0.4098350604375203 Validation Loss: 0.9916725754737854\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2544: Training Loss: 0.40929893652598065 Validation Loss: 0.9918216466903687\n",
      "Epoch 2545: Training Loss: 0.40945881605148315 Validation Loss: 0.9925265908241272\n",
      "Epoch 2546: Training Loss: 0.41041891773541767 Validation Loss: 0.9919361472129822\n",
      "Epoch 2547: Training Loss: 0.40823455651601154 Validation Loss: 0.9923220872879028\n",
      "Epoch 2548: Training Loss: 0.4103415111700694 Validation Loss: 0.9926478862762451\n",
      "Epoch 2549: Training Loss: 0.4083009958267212 Validation Loss: 0.9935175776481628\n",
      "Epoch 2550: Training Loss: 0.40806107719739276 Validation Loss: 0.9920628666877747\n",
      "Epoch 2551: Training Loss: 0.4089433252811432 Validation Loss: 0.9920542240142822\n",
      "Epoch 2552: Training Loss: 0.40789949893951416 Validation Loss: 0.9908369779586792\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2553: Training Loss: 0.4080715278784434 Validation Loss: 0.9905222654342651\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2554: Training Loss: 0.4070676763852437 Validation Loss: 0.991430401802063\n",
      "Epoch 2555: Training Loss: 0.4079442123572032 Validation Loss: 0.9918929934501648\n",
      "Epoch 2556: Training Loss: 0.4074690540631612 Validation Loss: 0.9919759035110474\n",
      "Epoch 2557: Training Loss: 0.40747763713200885 Validation Loss: 0.992107629776001\n",
      "Epoch 2558: Training Loss: 0.4060514469941457 Validation Loss: 0.991570770740509\n",
      "Epoch 2559: Training Loss: 0.40700268745422363 Validation Loss: 0.990873396396637\n",
      "Epoch 2560: Training Loss: 0.4066833754380544 Validation Loss: 0.990740954875946\n",
      "Epoch 2561: Training Loss: 0.4073521097501119 Validation Loss: 0.990181565284729\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2562: Training Loss: 0.40592097242673236 Validation Loss: 0.990690290927887\n",
      "Epoch 2563: Training Loss: 0.40462204813957214 Validation Loss: 0.9916213750839233\n",
      "Epoch 2564: Training Loss: 0.40480483571688336 Validation Loss: 0.991931140422821\n",
      "Epoch 2565: Training Loss: 0.4058206081390381 Validation Loss: 0.9908982515335083\n",
      "Epoch 2566: Training Loss: 0.4051334460576375 Validation Loss: 0.9904704689979553\n",
      "Epoch 2567: Training Loss: 0.4053977330525716 Validation Loss: 0.9906385540962219\n",
      "Epoch 2568: Training Loss: 0.40514861543973285 Validation Loss: 0.9898099303245544\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2569: Training Loss: 0.403989444176356 Validation Loss: 0.9894501566886902\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2570: Training Loss: 0.4049005111058553 Validation Loss: 0.9889156222343445\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2571: Training Loss: 0.4054841995239258 Validation Loss: 0.9895384907722473\n",
      "Epoch 2572: Training Loss: 0.40426899989446 Validation Loss: 0.9891150593757629\n",
      "Epoch 2573: Training Loss: 0.40387803316116333 Validation Loss: 0.9897058010101318\n",
      "Epoch 2574: Training Loss: 0.4041048189004262 Validation Loss: 0.9904890656471252\n",
      "Epoch 2575: Training Loss: 0.40326716502507526 Validation Loss: 0.9909853935241699\n",
      "Epoch 2576: Training Loss: 0.40352999170621234 Validation Loss: 0.9907863736152649\n",
      "Epoch 2577: Training Loss: 0.40294018387794495 Validation Loss: 0.9905371069908142\n",
      "Epoch 2578: Training Loss: 0.4029490252335866 Validation Loss: 0.9898986220359802\n",
      "Epoch 2579: Training Loss: 0.40282132228215534 Validation Loss: 0.9895102977752686\n",
      "Epoch 2580: Training Loss: 0.40428626537323 Validation Loss: 0.9895380735397339\n",
      "Epoch 2581: Training Loss: 0.40217988689740497 Validation Loss: 0.9897935390472412\n",
      "Epoch 2582: Training Loss: 0.40212251742680866 Validation Loss: 0.9887924194335938\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2583: Training Loss: 0.40159304936726886 Validation Loss: 0.9901110529899597\n",
      "Epoch 2584: Training Loss: 0.4018527865409851 Validation Loss: 0.9901854395866394\n",
      "Epoch 2585: Training Loss: 0.4012616773446401 Validation Loss: 0.9897399544715881\n",
      "Epoch 2586: Training Loss: 0.4031420151392619 Validation Loss: 0.988923192024231\n",
      "Epoch 2587: Training Loss: 0.4026947319507599 Validation Loss: 0.9884510040283203\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2588: Training Loss: 0.4008243183294932 Validation Loss: 0.9881719946861267\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2589: Training Loss: 0.4010832607746124 Validation Loss: 0.9886559844017029\n",
      "Epoch 2590: Training Loss: 0.4011431435743968 Validation Loss: 0.9889177083969116\n",
      "Epoch 2591: Training Loss: 0.40024760365486145 Validation Loss: 0.9892027974128723\n",
      "Epoch 2592: Training Loss: 0.40043099721272785 Validation Loss: 0.9896175861358643\n",
      "Epoch 2593: Training Loss: 0.400558739900589 Validation Loss: 0.9898800253868103\n",
      "Epoch 2594: Training Loss: 0.4008756677309672 Validation Loss: 0.9897664785385132\n",
      "Epoch 2595: Training Loss: 0.4000564118226369 Validation Loss: 0.9889810681343079\n",
      "Epoch 2596: Training Loss: 0.39963360627492267 Validation Loss: 0.9880198240280151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2597: Training Loss: 0.4004409611225128 Validation Loss: 0.987115740776062\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2598: Training Loss: 0.39932551980018616 Validation Loss: 0.9872382879257202\n",
      "Epoch 2599: Training Loss: 0.39887042840321857 Validation Loss: 0.9867254495620728\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2600: Training Loss: 0.3989660441875458 Validation Loss: 0.9878146648406982\n",
      "Epoch 2601: Training Loss: 0.4000797172387441 Validation Loss: 0.9878904819488525\n",
      "Epoch 2602: Training Loss: 0.39923136432965595 Validation Loss: 0.9884910583496094\n",
      "Epoch 2603: Training Loss: 0.400671382745107 Validation Loss: 0.9885419011116028\n",
      "Epoch 2604: Training Loss: 0.39830827713012695 Validation Loss: 0.9890667200088501\n",
      "Epoch 2605: Training Loss: 0.39801936348279315 Validation Loss: 0.9876325130462646\n",
      "Epoch 2606: Training Loss: 0.3979653020699819 Validation Loss: 0.9867755174636841\n",
      "Epoch 2607: Training Loss: 0.3976408739884694 Validation Loss: 0.9876917600631714\n",
      "Epoch 2608: Training Loss: 0.3984559377034505 Validation Loss: 0.9875890612602234\n",
      "Epoch 2609: Training Loss: 0.3978629906972249 Validation Loss: 0.9875480532646179\n",
      "Epoch 2610: Training Loss: 0.3972535530726115 Validation Loss: 0.9880297183990479\n",
      "Epoch 2611: Training Loss: 0.3963485062122345 Validation Loss: 0.9886719584465027\n",
      "Epoch 2612: Training Loss: 0.39674440026283264 Validation Loss: 0.987652599811554\n",
      "Epoch 2613: Training Loss: 0.3961696724096934 Validation Loss: 0.9873157739639282\n",
      "Epoch 2614: Training Loss: 0.3965873718261719 Validation Loss: 0.9867483973503113\n",
      "Epoch 2615: Training Loss: 0.3964984317620595 Validation Loss: 0.9864078760147095\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2616: Training Loss: 0.3959618906180064 Validation Loss: 0.9871617555618286\n",
      "Epoch 2617: Training Loss: 0.396745761235555 Validation Loss: 0.9875741600990295\n",
      "Epoch 2618: Training Loss: 0.3964182635148366 Validation Loss: 0.9868637323379517\n",
      "Epoch 2619: Training Loss: 0.39573782682418823 Validation Loss: 0.9870862364768982\n",
      "Epoch 2620: Training Loss: 0.3948337336381276 Validation Loss: 0.9866750836372375\n",
      "Epoch 2621: Training Loss: 0.39559097091356915 Validation Loss: 0.9873687028884888\n",
      "Epoch 2622: Training Loss: 0.3941945632298787 Validation Loss: 0.9870219230651855\n",
      "Epoch 2623: Training Loss: 0.3944193124771118 Validation Loss: 0.9862344861030579\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2624: Training Loss: 0.3976036210854848 Validation Loss: 0.9858496189117432\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2625: Training Loss: 0.3985048532485962 Validation Loss: 0.985662579536438\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2626: Training Loss: 0.3938511312007904 Validation Loss: 0.9858232736587524\n",
      "Epoch 2627: Training Loss: 0.395050048828125 Validation Loss: 0.9859731197357178\n",
      "Epoch 2628: Training Loss: 0.3943934738636017 Validation Loss: 0.9865636229515076\n",
      "Epoch 2629: Training Loss: 0.39307387669881183 Validation Loss: 0.9872827529907227\n",
      "Epoch 2630: Training Loss: 0.3941168586413066 Validation Loss: 0.9876124858856201\n",
      "Epoch 2631: Training Loss: 0.3938248058160146 Validation Loss: 0.9873805642127991\n",
      "Epoch 2632: Training Loss: 0.39311109979947406 Validation Loss: 0.987302303314209\n",
      "Epoch 2633: Training Loss: 0.3928031027317047 Validation Loss: 0.9856467247009277\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2634: Training Loss: 0.39377662539482117 Validation Loss: 0.9858578443527222\n",
      "Epoch 2635: Training Loss: 0.3942621151606242 Validation Loss: 0.9857422709465027\n",
      "Epoch 2636: Training Loss: 0.3922553062438965 Validation Loss: 0.9856276512145996\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2637: Training Loss: 0.39239083727200824 Validation Loss: 0.9855592846870422\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2638: Training Loss: 0.3919575810432434 Validation Loss: 0.9854896664619446\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2639: Training Loss: 0.391921987136205 Validation Loss: 0.9851111173629761\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2640: Training Loss: 0.39367727438608807 Validation Loss: 0.9855455160140991\n",
      "Epoch 2641: Training Loss: 0.39136846860249835 Validation Loss: 0.9860878586769104\n",
      "Epoch 2642: Training Loss: 0.39125282565752667 Validation Loss: 0.9859022498130798\n",
      "Epoch 2643: Training Loss: 0.39273427923520404 Validation Loss: 0.9850504398345947\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2644: Training Loss: 0.39088743925094604 Validation Loss: 0.9848629832267761\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2645: Training Loss: 0.390663206577301 Validation Loss: 0.984192430973053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2646: Training Loss: 0.3912267287572225 Validation Loss: 0.9843589067459106\n",
      "Epoch 2647: Training Loss: 0.39022671182950336 Validation Loss: 0.9848288297653198\n",
      "Epoch 2648: Training Loss: 0.3909912208716075 Validation Loss: 0.9854303002357483\n",
      "Epoch 2649: Training Loss: 0.39023251334826153 Validation Loss: 0.9855737686157227\n",
      "Epoch 2650: Training Loss: 0.3900782068570455 Validation Loss: 0.9860208034515381\n",
      "Epoch 2651: Training Loss: 0.3917934497197469 Validation Loss: 0.9851972460746765\n",
      "Epoch 2652: Training Loss: 0.38995001713434857 Validation Loss: 0.9848735928535461\n",
      "Epoch 2653: Training Loss: 0.39219753940900165 Validation Loss: 0.9849318861961365\n",
      "Epoch 2654: Training Loss: 0.3893048167228699 Validation Loss: 0.9852811694145203\n",
      "Epoch 2655: Training Loss: 0.3886306385199229 Validation Loss: 0.9846476912498474\n",
      "Epoch 2656: Training Loss: 0.3897651632626851 Validation Loss: 0.9845812320709229\n",
      "Epoch 2657: Training Loss: 0.3898711899916331 Validation Loss: 0.9850395917892456\n",
      "Epoch 2658: Training Loss: 0.38851218422253925 Validation Loss: 0.985245406627655\n",
      "Epoch 2659: Training Loss: 0.3881469964981079 Validation Loss: 0.9851154088973999\n",
      "Epoch 2660: Training Loss: 0.388528714577357 Validation Loss: 0.9848315119743347\n",
      "Epoch 2661: Training Loss: 0.3887588083744049 Validation Loss: 0.9845338463783264\n",
      "Epoch 2662: Training Loss: 0.3884640634059906 Validation Loss: 0.9835415482521057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2663: Training Loss: 0.3875955442587535 Validation Loss: 0.9836112856864929\n",
      "Epoch 2664: Training Loss: 0.3880206346511841 Validation Loss: 0.9844906330108643\n",
      "Epoch 2665: Training Loss: 0.3873356878757477 Validation Loss: 0.9845470786094666\n",
      "Epoch 2666: Training Loss: 0.38688870271046955 Validation Loss: 0.9844174981117249\n",
      "Epoch 2667: Training Loss: 0.3870251675446828 Validation Loss: 0.9841929078102112\n",
      "Epoch 2668: Training Loss: 0.38801130652427673 Validation Loss: 0.9839935898780823\n",
      "Epoch 2669: Training Loss: 0.38869767387708026 Validation Loss: 0.9834161996841431\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2670: Training Loss: 0.38695525129636127 Validation Loss: 0.983293890953064\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2671: Training Loss: 0.3859444359938304 Validation Loss: 0.9829379320144653\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2672: Training Loss: 0.3858505388100942 Validation Loss: 0.9828645586967468\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2673: Training Loss: 0.38646430770556134 Validation Loss: 0.9828216433525085\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2674: Training Loss: 0.3860902587572734 Validation Loss: 0.9835600852966309\n",
      "Epoch 2675: Training Loss: 0.38653043905893963 Validation Loss: 0.983124315738678\n",
      "Epoch 2676: Training Loss: 0.3854776819547017 Validation Loss: 0.9840764999389648\n",
      "Epoch 2677: Training Loss: 0.3873978555202484 Validation Loss: 0.9837784171104431\n",
      "Epoch 2678: Training Loss: 0.3853680491447449 Validation Loss: 0.9839920997619629\n",
      "Epoch 2679: Training Loss: 0.3856963813304901 Validation Loss: 0.9837720394134521\n",
      "Epoch 2680: Training Loss: 0.38509315252304077 Validation Loss: 0.9834549427032471\n",
      "Epoch 2681: Training Loss: 0.3849477271238963 Validation Loss: 0.9830975532531738\n",
      "Epoch 2682: Training Loss: 0.3845807611942291 Validation Loss: 0.9832672476768494\n",
      "Epoch 2683: Training Loss: 0.3842118481794993 Validation Loss: 0.9826292991638184\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2684: Training Loss: 0.38512460390726727 Validation Loss: 0.9828280210494995\n",
      "Epoch 2685: Training Loss: 0.38437867164611816 Validation Loss: 0.9827355146408081\n",
      "Epoch 2686: Training Loss: 0.38407037655512494 Validation Loss: 0.9831196069717407\n",
      "Epoch 2687: Training Loss: 0.38393251101175946 Validation Loss: 0.9835039377212524\n",
      "Epoch 2688: Training Loss: 0.38367099563280743 Validation Loss: 0.9836457371711731\n",
      "Epoch 2689: Training Loss: 0.3841038942337036 Validation Loss: 0.9829815626144409\n",
      "Epoch 2690: Training Loss: 0.3830123444398244 Validation Loss: 0.9825693368911743\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2691: Training Loss: 0.382915735244751 Validation Loss: 0.982223629951477\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2692: Training Loss: 0.3845182458559672 Validation Loss: 0.9815420508384705\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2693: Training Loss: 0.38258864482243854 Validation Loss: 0.9813067317008972\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2694: Training Loss: 0.38213494420051575 Validation Loss: 0.9812130331993103\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2695: Training Loss: 0.38239459196726483 Validation Loss: 0.9811751842498779\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2696: Training Loss: 0.38617948691050213 Validation Loss: 0.9812744855880737\n",
      "Epoch 2697: Training Loss: 0.38366661469141644 Validation Loss: 0.9821151494979858\n",
      "Epoch 2698: Training Loss: 0.38188207149505615 Validation Loss: 0.9822640419006348\n",
      "Epoch 2699: Training Loss: 0.381099005540212 Validation Loss: 0.9829198122024536\n",
      "Epoch 2700: Training Loss: 0.3809988300005595 Validation Loss: 0.9832630157470703\n",
      "Epoch 2701: Training Loss: 0.3816128472487132 Validation Loss: 0.9828049540519714\n",
      "Epoch 2702: Training Loss: 0.382906973361969 Validation Loss: 0.9816693663597107\n",
      "Epoch 2703: Training Loss: 0.3804129163424174 Validation Loss: 0.9811349511146545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2704: Training Loss: 0.3868500292301178 Validation Loss: 0.9808533787727356\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2705: Training Loss: 0.38042140007019043 Validation Loss: 0.9813370108604431\n",
      "Epoch 2706: Training Loss: 0.3791484336058299 Validation Loss: 0.9818633198738098\n",
      "Epoch 2707: Training Loss: 0.3801770508289337 Validation Loss: 0.98220294713974\n",
      "Epoch 2708: Training Loss: 0.3800143202145894 Validation Loss: 0.9822821617126465\n",
      "Epoch 2709: Training Loss: 0.37985460956891376 Validation Loss: 0.9819784760475159\n",
      "Epoch 2710: Training Loss: 0.3793472647666931 Validation Loss: 0.9812719225883484\n",
      "Epoch 2711: Training Loss: 0.3796726167201996 Validation Loss: 0.9807985424995422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Checkpoint Saved into PATH\n",
      "Epoch 2712: Training Loss: 0.37988389531771344 Validation Loss: 0.9807831048965454\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2713: Training Loss: 0.3801266551017761 Validation Loss: 0.9805834293365479\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2714: Training Loss: 0.38009437918663025 Validation Loss: 0.9809002876281738\n",
      "Epoch 2715: Training Loss: 0.37863625089327496 Validation Loss: 0.9802602529525757\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2716: Training Loss: 0.3782782554626465 Validation Loss: 0.9810522794723511\n",
      "Epoch 2717: Training Loss: 0.37845619519551593 Validation Loss: 0.9808688163757324\n",
      "Epoch 2718: Training Loss: 0.3786879777908325 Validation Loss: 0.9807118773460388\n",
      "Epoch 2719: Training Loss: 0.3789536754290263 Validation Loss: 0.9811066389083862\n",
      "Epoch 2720: Training Loss: 0.3774125079313914 Validation Loss: 0.9809139966964722\n",
      "Epoch 2721: Training Loss: 0.3779049714406331 Validation Loss: 0.9812746644020081\n",
      "Epoch 2722: Training Loss: 0.3779328167438507 Validation Loss: 0.9802929759025574\n",
      "Epoch 2723: Training Loss: 0.37859484553337097 Validation Loss: 0.9801084995269775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2724: Training Loss: 0.3774370054403941 Validation Loss: 0.9796527624130249\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2725: Training Loss: 0.3778652747472127 Validation Loss: 0.9800323247909546\n",
      "Epoch 2726: Training Loss: 0.37620176871617633 Validation Loss: 0.9806836843490601\n",
      "Epoch 2727: Training Loss: 0.3767780164877574 Validation Loss: 0.9804559350013733\n",
      "Epoch 2728: Training Loss: 0.37657695015271503 Validation Loss: 0.9813516736030579\n",
      "Epoch 2729: Training Loss: 0.37680160999298096 Validation Loss: 0.981515109539032\n",
      "Epoch 2730: Training Loss: 0.37629271546999615 Validation Loss: 0.9805946946144104\n",
      "Epoch 2731: Training Loss: 0.3766554494698842 Validation Loss: 0.9805443286895752\n",
      "Epoch 2732: Training Loss: 0.37567052245140076 Validation Loss: 0.9804065227508545\n",
      "Epoch 2733: Training Loss: 0.37581008672714233 Validation Loss: 0.9799764156341553\n",
      "Epoch 2734: Training Loss: 0.37620089451471966 Validation Loss: 0.9796410202980042\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2735: Training Loss: 0.3754950861136119 Validation Loss: 0.9790647029876709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2736: Training Loss: 0.3747466305891673 Validation Loss: 0.9792253971099854\n",
      "Epoch 2737: Training Loss: 0.3765467703342438 Validation Loss: 0.979456901550293\n",
      "Epoch 2738: Training Loss: 0.37605416774749756 Validation Loss: 0.979627251625061\n",
      "Epoch 2739: Training Loss: 0.37432466944058734 Validation Loss: 0.9796950817108154\n",
      "Epoch 2740: Training Loss: 0.3747275769710541 Validation Loss: 0.9791691303253174\n",
      "Epoch 2741: Training Loss: 0.3746291796366374 Validation Loss: 0.9794980883598328\n",
      "Epoch 2742: Training Loss: 0.3740147451559703 Validation Loss: 0.9796925783157349\n",
      "Epoch 2743: Training Loss: 0.3740565776824951 Validation Loss: 0.9795899987220764\n",
      "Epoch 2744: Training Loss: 0.37634599208831787 Validation Loss: 0.9789304733276367\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2745: Training Loss: 0.3733556767304738 Validation Loss: 0.9794725775718689\n",
      "Epoch 2746: Training Loss: 0.37471717596054077 Validation Loss: 0.9797534942626953\n",
      "Epoch 2747: Training Loss: 0.37385539213816327 Validation Loss: 0.9797616600990295\n",
      "Epoch 2748: Training Loss: 0.37385202447573346 Validation Loss: 0.9797868132591248\n",
      "Epoch 2749: Training Loss: 0.3730250994364421 Validation Loss: 0.979262113571167\n",
      "Epoch 2750: Training Loss: 0.37295133868853253 Validation Loss: 0.9790253639221191\n",
      "Epoch 2751: Training Loss: 0.3733520805835724 Validation Loss: 0.9784987568855286\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2752: Training Loss: 0.3726840317249298 Validation Loss: 0.9779735803604126\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2753: Training Loss: 0.3726768692334493 Validation Loss: 0.9778547286987305\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2754: Training Loss: 0.3744162718454997 Validation Loss: 0.9782404899597168\n",
      "Epoch 2755: Training Loss: 0.37281713883082074 Validation Loss: 0.9780061841011047\n",
      "Epoch 2756: Training Loss: 0.37192972501118976 Validation Loss: 0.9785892367362976\n",
      "Epoch 2757: Training Loss: 0.3727149963378906 Validation Loss: 0.9793620705604553\n",
      "Epoch 2758: Training Loss: 0.3712674876054128 Validation Loss: 0.9794155359268188\n",
      "Epoch 2759: Training Loss: 0.3725874125957489 Validation Loss: 0.9796561002731323\n",
      "Epoch 2760: Training Loss: 0.3713673750559489 Validation Loss: 0.9790209531784058\n",
      "Epoch 2761: Training Loss: 0.37468481063842773 Validation Loss: 0.9793474674224854\n",
      "Epoch 2762: Training Loss: 0.37156402071317035 Validation Loss: 0.9786364436149597\n",
      "Epoch 2763: Training Loss: 0.37206167976061505 Validation Loss: 0.9786227941513062\n",
      "Epoch 2764: Training Loss: 0.37187347809473675 Validation Loss: 0.9776932001113892\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2765: Training Loss: 0.3716332217057546 Validation Loss: 0.9781867861747742\n",
      "Epoch 2766: Training Loss: 0.3702849547068278 Validation Loss: 0.9774007797241211\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2767: Training Loss: 0.37001578013102215 Validation Loss: 0.9781555533409119\n",
      "Epoch 2768: Training Loss: 0.37131304542223614 Validation Loss: 0.9780666828155518\n",
      "Epoch 2769: Training Loss: 0.3698282539844513 Validation Loss: 0.9775362014770508\n",
      "Epoch 2770: Training Loss: 0.36980851491292316 Validation Loss: 0.9785493016242981\n",
      "Epoch 2771: Training Loss: 0.36965890725453693 Validation Loss: 0.9789612293243408\n",
      "Epoch 2772: Training Loss: 0.37106621265411377 Validation Loss: 0.9784303307533264\n",
      "Epoch 2773: Training Loss: 0.3685705363750458 Validation Loss: 0.977898359298706\n",
      "Epoch 2774: Training Loss: 0.36940455436706543 Validation Loss: 0.9777195453643799\n",
      "Epoch 2775: Training Loss: 0.36895906925201416 Validation Loss: 0.9775828123092651\n",
      "Epoch 2776: Training Loss: 0.36907702684402466 Validation Loss: 0.9772026538848877\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2777: Training Loss: 0.36880192160606384 Validation Loss: 0.9772539734840393\n",
      "Epoch 2778: Training Loss: 0.3686907887458801 Validation Loss: 0.9773526787757874\n",
      "Epoch 2779: Training Loss: 0.36849072575569153 Validation Loss: 0.9774782657623291\n",
      "Epoch 2780: Training Loss: 0.36759312947591144 Validation Loss: 0.9779545068740845\n",
      "Epoch 2781: Training Loss: 0.36809036135673523 Validation Loss: 0.9786131978034973\n",
      "Epoch 2782: Training Loss: 0.3677252133687337 Validation Loss: 0.9781395792961121\n",
      "Epoch 2783: Training Loss: 0.3678588966528575 Validation Loss: 0.9774934649467468\n",
      "Epoch 2784: Training Loss: 0.36911843220392865 Validation Loss: 0.9768990278244019\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2785: Training Loss: 0.3685128887494405 Validation Loss: 0.9767755270004272\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2786: Training Loss: 0.3674050768216451 Validation Loss: 0.9771685600280762\n",
      "Epoch 2787: Training Loss: 0.3674612541993459 Validation Loss: 0.9769799113273621\n",
      "Epoch 2788: Training Loss: 0.3673450748125712 Validation Loss: 0.9767573475837708\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2789: Training Loss: 0.36654895544052124 Validation Loss: 0.9762407541275024\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2790: Training Loss: 0.36656033992767334 Validation Loss: 0.975891649723053\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2791: Training Loss: 0.36576129992802936 Validation Loss: 0.9754505753517151\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2792: Training Loss: 0.36681318283081055 Validation Loss: 0.9764029383659363\n",
      "Epoch 2793: Training Loss: 0.3661506275335948 Validation Loss: 0.9765439033508301\n",
      "Epoch 2794: Training Loss: 0.3666474421819051 Validation Loss: 0.9771701693534851\n",
      "Epoch 2795: Training Loss: 0.3653842906157176 Validation Loss: 0.9774764776229858\n",
      "Epoch 2796: Training Loss: 0.36578695972760517 Validation Loss: 0.9777469635009766\n",
      "Epoch 2797: Training Loss: 0.3656949798266093 Validation Loss: 0.97762531042099\n",
      "Epoch 2798: Training Loss: 0.3665205041567485 Validation Loss: 0.9776862859725952\n",
      "Epoch 2799: Training Loss: 0.3679117163022359 Validation Loss: 0.9774638414382935\n",
      "Epoch 2800: Training Loss: 0.36583112676938373 Validation Loss: 0.9764877557754517\n",
      "Epoch 2801: Training Loss: 0.364663302898407 Validation Loss: 0.9756194353103638\n",
      "Epoch 2802: Training Loss: 0.3649689058462779 Validation Loss: 0.9758278727531433\n",
      "Epoch 2803: Training Loss: 0.36447537938753766 Validation Loss: 0.9760547876358032\n",
      "Epoch 2804: Training Loss: 0.36482449372609455 Validation Loss: 0.9760976433753967\n",
      "Epoch 2805: Training Loss: 0.3645746012528737 Validation Loss: 0.9765488505363464\n",
      "Epoch 2806: Training Loss: 0.3658893307050069 Validation Loss: 0.9768986701965332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2807: Training Loss: 0.3641083737214406 Validation Loss: 0.9764625430107117\n",
      "Epoch 2808: Training Loss: 0.36398130655288696 Validation Loss: 0.9749356508255005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2809: Training Loss: 0.36388160785039264 Validation Loss: 0.9747922420501709\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2810: Training Loss: 0.3640466233094533 Validation Loss: 0.9756001830101013\n",
      "Epoch 2811: Training Loss: 0.3622540533542633 Validation Loss: 0.975703239440918\n",
      "Epoch 2812: Training Loss: 0.3633520205815633 Validation Loss: 0.9756814241409302\n",
      "Epoch 2813: Training Loss: 0.36316083868344623 Validation Loss: 0.9758659601211548\n",
      "Epoch 2814: Training Loss: 0.3651764988899231 Validation Loss: 0.9757543802261353\n",
      "Epoch 2815: Training Loss: 0.3645118673642476 Validation Loss: 0.976166307926178\n",
      "Epoch 2816: Training Loss: 0.36293262243270874 Validation Loss: 0.9759370684623718\n",
      "Epoch 2817: Training Loss: 0.3627462486426036 Validation Loss: 0.9757415652275085\n",
      "Epoch 2818: Training Loss: 0.36256494124730426 Validation Loss: 0.9757871031761169\n",
      "Epoch 2819: Training Loss: 0.3620547254880269 Validation Loss: 0.9760025143623352\n",
      "Epoch 2820: Training Loss: 0.3618963559468587 Validation Loss: 0.9749972224235535\n",
      "Epoch 2821: Training Loss: 0.3622807363669078 Validation Loss: 0.9752548933029175\n",
      "Epoch 2822: Training Loss: 0.3617997368176778 Validation Loss: 0.9760286808013916\n",
      "Epoch 2823: Training Loss: 0.36161185304323834 Validation Loss: 0.9751860499382019\n",
      "Epoch 2824: Training Loss: 0.3612365126609802 Validation Loss: 0.9747654795646667\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2825: Training Loss: 0.3608880837758382 Validation Loss: 0.9754108786582947\n",
      "Epoch 2826: Training Loss: 0.3620462715625763 Validation Loss: 0.9756374359130859\n",
      "Epoch 2827: Training Loss: 0.3612794280052185 Validation Loss: 0.9761435389518738\n",
      "Epoch 2828: Training Loss: 0.36009731888771057 Validation Loss: 0.9761601686477661\n",
      "Epoch 2829: Training Loss: 0.3602493306001027 Validation Loss: 0.9758875370025635\n",
      "Epoch 2830: Training Loss: 0.360569308201472 Validation Loss: 0.9757713675498962\n",
      "Epoch 2831: Training Loss: 0.36166877547899884 Validation Loss: 0.9752434492111206\n",
      "Epoch 2832: Training Loss: 0.3601752519607544 Validation Loss: 0.974732518196106\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2833: Training Loss: 0.36135952671368915 Validation Loss: 0.9740006327629089\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2834: Training Loss: 0.35983139276504517 Validation Loss: 0.9737038612365723\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2835: Training Loss: 0.35975082715352374 Validation Loss: 0.9734019637107849\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2836: Training Loss: 0.3600911299387614 Validation Loss: 0.9738656878471375\n",
      "Epoch 2837: Training Loss: 0.3593640824158986 Validation Loss: 0.9742388725280762\n",
      "Epoch 2838: Training Loss: 0.3586657643318176 Validation Loss: 0.9751987457275391\n",
      "Epoch 2839: Training Loss: 0.358930766582489 Validation Loss: 0.9756473302841187\n",
      "Epoch 2840: Training Loss: 0.3597537676493327 Validation Loss: 0.9756605625152588\n",
      "Epoch 2841: Training Loss: 0.3594394524892171 Validation Loss: 0.9755514860153198\n",
      "Epoch 2842: Training Loss: 0.3586610456307729 Validation Loss: 0.9741514325141907\n",
      "Epoch 2843: Training Loss: 0.35835156838099164 Validation Loss: 0.9738711714744568\n",
      "Epoch 2844: Training Loss: 0.3581520915031433 Validation Loss: 0.9741083979606628\n",
      "Epoch 2845: Training Loss: 0.35848108927408856 Validation Loss: 0.9743544459342957\n",
      "Epoch 2846: Training Loss: 0.35864681005477905 Validation Loss: 0.9751441478729248\n",
      "Epoch 2847: Training Loss: 0.35802937547365826 Validation Loss: 0.9745228886604309\n",
      "Epoch 2848: Training Loss: 0.35779039065043133 Validation Loss: 0.9751590490341187\n",
      "Epoch 2849: Training Loss: 0.3569010595480601 Validation Loss: 0.9751066565513611\n",
      "Epoch 2850: Training Loss: 0.35682738820711773 Validation Loss: 0.9744611978530884\n",
      "Epoch 2851: Training Loss: 0.3570185899734497 Validation Loss: 0.9731974601745605\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2852: Training Loss: 0.36004579067230225 Validation Loss: 0.9729761481285095\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2853: Training Loss: 0.3568672835826874 Validation Loss: 0.9730842113494873\n",
      "Epoch 2854: Training Loss: 0.357049028078715 Validation Loss: 0.9732046723365784\n",
      "Epoch 2855: Training Loss: 0.3584236204624176 Validation Loss: 0.9744463562965393\n",
      "Epoch 2856: Training Loss: 0.3559931218624115 Validation Loss: 0.9751847982406616\n",
      "Epoch 2857: Training Loss: 0.35620776812235516 Validation Loss: 0.9751405119895935\n",
      "Epoch 2858: Training Loss: 0.3577272097269694 Validation Loss: 0.9748605489730835\n",
      "Epoch 2859: Training Loss: 0.3571039040883382 Validation Loss: 0.9746090173721313\n",
      "Epoch 2860: Training Loss: 0.3565123975276947 Validation Loss: 0.9741382598876953\n",
      "Epoch 2861: Training Loss: 0.355340709288915 Validation Loss: 0.9730132818222046\n",
      "Epoch 2862: Training Loss: 0.35680073499679565 Validation Loss: 0.97349613904953\n",
      "Epoch 2863: Training Loss: 0.35567571719487506 Validation Loss: 0.9727569818496704\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2864: Training Loss: 0.355066974957784 Validation Loss: 0.9726671576499939\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2865: Training Loss: 0.35534173250198364 Validation Loss: 0.9730988144874573\n",
      "Epoch 2866: Training Loss: 0.35552804668744403 Validation Loss: 0.9735130071640015\n",
      "Epoch 2867: Training Loss: 0.3548438449700673 Validation Loss: 0.9743447303771973\n",
      "Epoch 2868: Training Loss: 0.3551364839076996 Validation Loss: 0.9744551777839661\n",
      "Epoch 2869: Training Loss: 0.3553617497285207 Validation Loss: 0.9742487668991089\n",
      "Epoch 2870: Training Loss: 0.35750094056129456 Validation Loss: 0.9731878042221069\n",
      "Epoch 2871: Training Loss: 0.3541992704073588 Validation Loss: 0.9724871516227722\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2872: Training Loss: 0.35441507895787555 Validation Loss: 0.9736666679382324\n",
      "Epoch 2873: Training Loss: 0.35378242532412213 Validation Loss: 0.9735456705093384\n",
      "Epoch 2874: Training Loss: 0.35447293519973755 Validation Loss: 0.9729549288749695\n",
      "Epoch 2875: Training Loss: 0.35365889469782513 Validation Loss: 0.9739983677864075\n",
      "Epoch 2876: Training Loss: 0.3534167806307475 Validation Loss: 0.9740509986877441\n",
      "Epoch 2877: Training Loss: 0.3529019355773926 Validation Loss: 0.9742619395256042\n",
      "Epoch 2878: Training Loss: 0.3530977666378021 Validation Loss: 0.9735695719718933\n",
      "Epoch 2879: Training Loss: 0.3527972102165222 Validation Loss: 0.9738702178001404\n",
      "Epoch 2880: Training Loss: 0.3521967927614848 Validation Loss: 0.9726316928863525\n",
      "Epoch 2881: Training Loss: 0.3532218138376872 Validation Loss: 0.9731305241584778\n",
      "Epoch 2882: Training Loss: 0.3525087038675944 Validation Loss: 0.9734501242637634\n",
      "Epoch 2883: Training Loss: 0.3517669240633647 Validation Loss: 0.973054826259613\n",
      "Epoch 2884: Training Loss: 0.35221025347709656 Validation Loss: 0.973028838634491\n",
      "Epoch 2885: Training Loss: 0.3543691833813985 Validation Loss: 0.9727186560630798\n",
      "Epoch 2886: Training Loss: 0.35273100932439166 Validation Loss: 0.9721722602844238\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2887: Training Loss: 0.35142310460408527 Validation Loss: 0.9728614091873169\n",
      "Epoch 2888: Training Loss: 0.35170794526735943 Validation Loss: 0.973551332950592\n",
      "Epoch 2889: Training Loss: 0.35158541798591614 Validation Loss: 0.9723920822143555\n",
      "Epoch 2890: Training Loss: 0.351052463054657 Validation Loss: 0.9717937111854553\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2891: Training Loss: 0.3515331943829854 Validation Loss: 0.9725267887115479\n",
      "Epoch 2892: Training Loss: 0.35033469398816425 Validation Loss: 0.9725598692893982\n",
      "Epoch 2893: Training Loss: 0.3508408268292745 Validation Loss: 0.972413182258606\n",
      "Epoch 2894: Training Loss: 0.351349671681722 Validation Loss: 0.9725438356399536\n",
      "Epoch 2895: Training Loss: 0.3528614044189453 Validation Loss: 0.9725956320762634\n",
      "Epoch 2896: Training Loss: 0.3508027493953705 Validation Loss: 0.9729072451591492\n",
      "Epoch 2897: Training Loss: 0.34999887148539227 Validation Loss: 0.9722281098365784\n",
      "Epoch 2898: Training Loss: 0.35064177711804706 Validation Loss: 0.9721661806106567\n",
      "Epoch 2899: Training Loss: 0.3506334225336711 Validation Loss: 0.9726573824882507\n",
      "Epoch 2900: Training Loss: 0.3508516649405162 Validation Loss: 0.9724551439285278\n",
      "Epoch 2901: Training Loss: 0.3496874272823334 Validation Loss: 0.9716246128082275\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2902: Training Loss: 0.3507168193658193 Validation Loss: 0.9711388349533081\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2903: Training Loss: 0.3504897753397624 Validation Loss: 0.9715049862861633\n",
      "Epoch 2904: Training Loss: 0.34938280781110126 Validation Loss: 0.9716064929962158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2905: Training Loss: 0.3489006757736206 Validation Loss: 0.9724336266517639\n",
      "Epoch 2906: Training Loss: 0.3501046101252238 Validation Loss: 0.972756028175354\n",
      "Epoch 2907: Training Loss: 0.349337637424469 Validation Loss: 0.9728097915649414\n",
      "Epoch 2908: Training Loss: 0.34928203622500104 Validation Loss: 0.9725112318992615\n",
      "Epoch 2909: Training Loss: 0.34880881508191425 Validation Loss: 0.9717260599136353\n",
      "Epoch 2910: Training Loss: 0.34915674726168316 Validation Loss: 0.9712367653846741\n",
      "Epoch 2911: Training Loss: 0.34760592381159466 Validation Loss: 0.9713818430900574\n",
      "Epoch 2912: Training Loss: 0.3492366075515747 Validation Loss: 0.9707539677619934\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2913: Training Loss: 0.34766661127408344 Validation Loss: 0.9708480834960938\n",
      "Epoch 2914: Training Loss: 0.34791627526283264 Validation Loss: 0.9714205861091614\n",
      "Epoch 2915: Training Loss: 0.34803494811058044 Validation Loss: 0.9717506170272827\n",
      "Epoch 2916: Training Loss: 0.34728245933850604 Validation Loss: 0.9720508456230164\n",
      "Epoch 2917: Training Loss: 0.3474825421969096 Validation Loss: 0.9709404110908508\n",
      "Epoch 2918: Training Loss: 0.3474026918411255 Validation Loss: 0.9702112674713135\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2919: Training Loss: 0.3477145930131276 Validation Loss: 0.9702420830726624\n",
      "Epoch 2920: Training Loss: 0.3476215700308482 Validation Loss: 0.9710081815719604\n",
      "Epoch 2921: Training Loss: 0.3465964098771413 Validation Loss: 0.9713737368583679\n",
      "Epoch 2922: Training Loss: 0.34662647048632306 Validation Loss: 0.9720414280891418\n",
      "Epoch 2923: Training Loss: 0.34820930163065594 Validation Loss: 0.9726439118385315\n",
      "Epoch 2924: Training Loss: 0.34639166792233783 Validation Loss: 0.9726042747497559\n",
      "Epoch 2925: Training Loss: 0.3463437755902608 Validation Loss: 0.9716722965240479\n",
      "Epoch 2926: Training Loss: 0.3463486234347026 Validation Loss: 0.9716551303863525\n",
      "Epoch 2927: Training Loss: 0.3453340331713359 Validation Loss: 0.9700764417648315\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2928: Training Loss: 0.34576257069905597 Validation Loss: 0.9707106947898865\n",
      "Epoch 2929: Training Loss: 0.3453351557254791 Validation Loss: 0.9699159860610962\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2930: Training Loss: 0.345041424036026 Validation Loss: 0.969817042350769\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2931: Training Loss: 0.3481849630673726 Validation Loss: 0.9706531167030334\n",
      "Epoch 2932: Training Loss: 0.3448283076286316 Validation Loss: 0.9700314998626709\n",
      "Epoch 2933: Training Loss: 0.3443145453929901 Validation Loss: 0.9698944687843323\n",
      "Epoch 2934: Training Loss: 0.3457412123680115 Validation Loss: 0.9697345495223999\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2935: Training Loss: 0.3455202480157216 Validation Loss: 0.9704627990722656\n",
      "Epoch 2936: Training Loss: 0.3452312449614207 Validation Loss: 0.9707353711128235\n",
      "Epoch 2937: Training Loss: 0.3446686863899231 Validation Loss: 0.9709949493408203\n",
      "Epoch 2938: Training Loss: 0.34456316630045575 Validation Loss: 0.9708081483840942\n",
      "Epoch 2939: Training Loss: 0.3441638946533203 Validation Loss: 0.9708693027496338\n",
      "Epoch 2940: Training Loss: 0.34430516759554547 Validation Loss: 0.9715486168861389\n",
      "Epoch 2941: Training Loss: 0.3444446126619975 Validation Loss: 0.9706537127494812\n",
      "Epoch 2942: Training Loss: 0.3440103828907013 Validation Loss: 0.9712172150611877\n",
      "Epoch 2943: Training Loss: 0.3434912661711375 Validation Loss: 0.9708508253097534\n",
      "Epoch 2944: Training Loss: 0.3437406122684479 Validation Loss: 0.9711525440216064\n",
      "Epoch 2945: Training Loss: 0.34464073181152344 Validation Loss: 0.9710323214530945\n",
      "Epoch 2946: Training Loss: 0.34417953093846637 Validation Loss: 0.9708338379859924\n",
      "Epoch 2947: Training Loss: 0.3465263446172078 Validation Loss: 0.9714307188987732\n",
      "Epoch 2948: Training Loss: 0.3438948293526967 Validation Loss: 0.9708970785140991\n",
      "Epoch 2949: Training Loss: 0.3429803450902303 Validation Loss: 0.9710441827774048\n",
      "Epoch 2950: Training Loss: 0.3439086178938548 Validation Loss: 0.9707179069519043\n",
      "Epoch 2951: Training Loss: 0.34267277518908185 Validation Loss: 0.9705908298492432\n",
      "Epoch 2952: Training Loss: 0.3433678448200226 Validation Loss: 0.9705879092216492\n",
      "Epoch 2953: Training Loss: 0.3450140655040741 Validation Loss: 0.9700815677642822\n",
      "Epoch 2954: Training Loss: 0.3426160017649333 Validation Loss: 0.9692283272743225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2955: Training Loss: 0.3450833062330882 Validation Loss: 0.9694320559501648\n",
      "Epoch 2956: Training Loss: 0.3426334857940674 Validation Loss: 0.9685211181640625\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2957: Training Loss: 0.34199321269989014 Validation Loss: 0.9692537188529968\n",
      "Epoch 2958: Training Loss: 0.3446095486481984 Validation Loss: 0.9705909490585327\n",
      "Epoch 2959: Training Loss: 0.3414418896039327 Validation Loss: 0.9711663722991943\n",
      "Epoch 2960: Training Loss: 0.341224600871404 Validation Loss: 0.9708654284477234\n",
      "Epoch 2961: Training Loss: 0.34208373228708905 Validation Loss: 0.9705836176872253\n",
      "Epoch 2962: Training Loss: 0.34155841668446857 Validation Loss: 0.9698112607002258\n",
      "Epoch 2963: Training Loss: 0.34155377745628357 Validation Loss: 0.9695488214492798\n",
      "Epoch 2964: Training Loss: 0.34084179997444153 Validation Loss: 0.9692283272743225\n",
      "Epoch 2965: Training Loss: 0.34087254603703815 Validation Loss: 0.9687359929084778\n",
      "Epoch 2966: Training Loss: 0.3411724964777629 Validation Loss: 0.9693124294281006\n",
      "Epoch 2967: Training Loss: 0.34058406949043274 Validation Loss: 0.9696677327156067\n",
      "Epoch 2968: Training Loss: 0.3397003213564555 Validation Loss: 0.9695907235145569\n",
      "Epoch 2969: Training Loss: 0.34001752734184265 Validation Loss: 0.9701715111732483\n",
      "Epoch 2970: Training Loss: 0.3411137064297994 Validation Loss: 0.9707288146018982\n",
      "Epoch 2971: Training Loss: 0.34037121136983234 Validation Loss: 0.9708250761032104\n",
      "Epoch 2972: Training Loss: 0.34178826212882996 Validation Loss: 0.9700790047645569\n",
      "Epoch 2973: Training Loss: 0.3397132058938344 Validation Loss: 0.9695290923118591\n",
      "Epoch 2974: Training Loss: 0.3397608995437622 Validation Loss: 0.969343364238739\n",
      "Epoch 2975: Training Loss: 0.33930566906929016 Validation Loss: 0.9695369601249695\n",
      "Epoch 2976: Training Loss: 0.33948694666226703 Validation Loss: 0.9693892002105713\n",
      "Epoch 2977: Training Loss: 0.33900657296180725 Validation Loss: 0.969344973564148\n",
      "Epoch 2978: Training Loss: 0.3384542961915334 Validation Loss: 0.9689785838127136\n",
      "Epoch 2979: Training Loss: 0.3392838140328725 Validation Loss: 0.96891188621521\n",
      "Epoch 2980: Training Loss: 0.3382529417673747 Validation Loss: 0.9689868688583374\n",
      "Epoch 2981: Training Loss: 0.3390421171983083 Validation Loss: 0.9688460826873779\n",
      "Epoch 2982: Training Loss: 0.3385263482729594 Validation Loss: 0.9689880609512329\n",
      "Epoch 2983: Training Loss: 0.33876295884450275 Validation Loss: 0.9684221148490906\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2984: Training Loss: 0.34041932225227356 Validation Loss: 0.9683747887611389\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2985: Training Loss: 0.3387978474299113 Validation Loss: 0.9685644507408142\n",
      "Epoch 2986: Training Loss: 0.33842872579892475 Validation Loss: 0.9695549607276917\n",
      "Epoch 2987: Training Loss: 0.33744098742802936 Validation Loss: 0.970059871673584\n",
      "Epoch 2988: Training Loss: 0.3374889095624288 Validation Loss: 0.969787061214447\n",
      "Epoch 2989: Training Loss: 0.33741068840026855 Validation Loss: 0.9694474339485168\n",
      "Epoch 2990: Training Loss: 0.33714545766512555 Validation Loss: 0.9690588116645813\n",
      "Epoch 2991: Training Loss: 0.3378807604312897 Validation Loss: 0.9687561392784119\n",
      "Epoch 2992: Training Loss: 0.3375315268834432 Validation Loss: 0.9691371917724609\n",
      "Epoch 2993: Training Loss: 0.33751170833905536 Validation Loss: 0.9684978127479553\n",
      "Epoch 2994: Training Loss: 0.3373841842015584 Validation Loss: 0.967992901802063\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2995: Training Loss: 0.3365019162495931 Validation Loss: 0.967778742313385\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 2996: Training Loss: 0.33689919114112854 Validation Loss: 0.9680944085121155\n",
      "Epoch 2997: Training Loss: 0.33579447865486145 Validation Loss: 0.9682857990264893\n",
      "Epoch 2998: Training Loss: 0.3377131223678589 Validation Loss: 0.9691583514213562\n",
      "Epoch 2999: Training Loss: 0.3359309732913971 Validation Loss: 0.9687724113464355\n",
      "Epoch 3000: Training Loss: 0.33606961369514465 Validation Loss: 0.9686001539230347\n",
      "Epoch 3001: Training Loss: 0.33643917242685956 Validation Loss: 0.968549370765686\n",
      "Epoch 3002: Training Loss: 0.3358592788378398 Validation Loss: 0.9687235355377197\n",
      "Epoch 3003: Training Loss: 0.3350799282391866 Validation Loss: 0.9691431522369385\n",
      "Epoch 3004: Training Loss: 0.33592947324117023 Validation Loss: 0.9688624739646912\n",
      "Epoch 3005: Training Loss: 0.33805785576502484 Validation Loss: 0.9678599238395691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3006: Training Loss: 0.3351978162924449 Validation Loss: 0.9684983491897583\n",
      "Epoch 3007: Training Loss: 0.33447176218032837 Validation Loss: 0.9684857726097107\n",
      "Epoch 3008: Training Loss: 0.3346046606699626 Validation Loss: 0.9675860404968262\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3009: Training Loss: 0.3345047930876414 Validation Loss: 0.9680337309837341\n",
      "Epoch 3010: Training Loss: 0.3346789975961049 Validation Loss: 0.9678661823272705\n",
      "Epoch 3011: Training Loss: 0.33528051773707074 Validation Loss: 0.9686604738235474\n",
      "Epoch 3012: Training Loss: 0.33433390657107037 Validation Loss: 0.9682227373123169\n",
      "Epoch 3013: Training Loss: 0.33486618598302204 Validation Loss: 0.9684298038482666\n",
      "Epoch 3014: Training Loss: 0.3339528938134511 Validation Loss: 0.9680340886116028\n",
      "Epoch 3015: Training Loss: 0.3342184325059255 Validation Loss: 0.9683377742767334\n",
      "Epoch 3016: Training Loss: 0.3355072538057963 Validation Loss: 0.9683859944343567\n",
      "Epoch 3017: Training Loss: 0.3358994821707408 Validation Loss: 0.9678378701210022\n",
      "Epoch 3018: Training Loss: 0.334014226992925 Validation Loss: 0.9678884744644165\n",
      "Epoch 3019: Training Loss: 0.33477463324864704 Validation Loss: 0.9685859084129333\n",
      "Epoch 3020: Training Loss: 0.3334845503171285 Validation Loss: 0.9682735800743103\n",
      "Epoch 3021: Training Loss: 0.3333502511183421 Validation Loss: 0.9679118394851685\n",
      "Epoch 3022: Training Loss: 0.33342215418815613 Validation Loss: 0.9673486948013306\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3023: Training Loss: 0.3334054450194041 Validation Loss: 0.9672197103500366\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3024: Training Loss: 0.33268290758132935 Validation Loss: 0.9675767421722412\n",
      "Epoch 3025: Training Loss: 0.3326635956764221 Validation Loss: 0.9680852293968201\n",
      "Epoch 3026: Training Loss: 0.33372623721758526 Validation Loss: 0.968144416809082\n",
      "Epoch 3027: Training Loss: 0.3325445254643758 Validation Loss: 0.9684386253356934\n",
      "Epoch 3028: Training Loss: 0.3319168786207835 Validation Loss: 0.9684367179870605\n",
      "Epoch 3029: Training Loss: 0.33260778586069745 Validation Loss: 0.968035876750946\n",
      "Epoch 3030: Training Loss: 0.332348754008611 Validation Loss: 0.9678966403007507\n",
      "Epoch 3031: Training Loss: 0.33276866873105365 Validation Loss: 0.9674545526504517\n",
      "Epoch 3032: Training Loss: 0.33152573307355243 Validation Loss: 0.967866837978363\n",
      "Epoch 3033: Training Loss: 0.3317941625912984 Validation Loss: 0.9675721526145935\n",
      "Epoch 3034: Training Loss: 0.33256171147028607 Validation Loss: 0.9671115875244141\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3035: Training Loss: 0.3312581976254781 Validation Loss: 0.9669408798217773\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3036: Training Loss: 0.33079389731089276 Validation Loss: 0.9682714939117432\n",
      "Epoch 3037: Training Loss: 0.3311263918876648 Validation Loss: 0.9686192870140076\n",
      "Epoch 3038: Training Loss: 0.33208730816841125 Validation Loss: 0.9683985710144043\n",
      "Epoch 3039: Training Loss: 0.3308331271012624 Validation Loss: 0.9680591225624084\n",
      "Epoch 3040: Training Loss: 0.33022504051526386 Validation Loss: 0.9671614766120911\n",
      "Epoch 3041: Training Loss: 0.3301797111829122 Validation Loss: 0.9670851230621338\n",
      "Epoch 3042: Training Loss: 0.330628901720047 Validation Loss: 0.9672664999961853\n",
      "Epoch 3043: Training Loss: 0.329611857732137 Validation Loss: 0.9666385650634766\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3044: Training Loss: 0.3299972315629323 Validation Loss: 0.9668254256248474\n",
      "Epoch 3045: Training Loss: 0.3298391004403432 Validation Loss: 0.9667710065841675\n",
      "Epoch 3046: Training Loss: 0.3303849895795186 Validation Loss: 0.9665849804878235\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3047: Training Loss: 0.33015555143356323 Validation Loss: 0.9678566455841064\n",
      "Epoch 3048: Training Loss: 0.3301440676053365 Validation Loss: 0.9683632254600525\n",
      "Epoch 3049: Training Loss: 0.32979966203371686 Validation Loss: 0.9674904346466064\n",
      "Epoch 3050: Training Loss: 0.3293212254842122 Validation Loss: 0.967063307762146\n",
      "Epoch 3051: Training Loss: 0.32952049374580383 Validation Loss: 0.9670426249504089\n",
      "Epoch 3052: Training Loss: 0.3292859693368276 Validation Loss: 0.9666194319725037\n",
      "Epoch 3053: Training Loss: 0.3294260899225871 Validation Loss: 0.9665190577507019\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3054: Training Loss: 0.3288585344950358 Validation Loss: 0.9665166735649109\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3055: Training Loss: 0.3299982448418935 Validation Loss: 0.9671701788902283\n",
      "Epoch 3056: Training Loss: 0.328731248776118 Validation Loss: 0.9666098356246948\n",
      "Epoch 3057: Training Loss: 0.32908186316490173 Validation Loss: 0.9665877223014832\n",
      "Epoch 3058: Training Loss: 0.32922760645548504 Validation Loss: 0.9668852686882019\n",
      "Epoch 3059: Training Loss: 0.329726646343867 Validation Loss: 0.9667032361030579\n",
      "Epoch 3060: Training Loss: 0.32785557707150775 Validation Loss: 0.9670267701148987\n",
      "Epoch 3061: Training Loss: 0.32881126801172894 Validation Loss: 0.9670402407646179\n",
      "Epoch 3062: Training Loss: 0.327992707490921 Validation Loss: 0.9666730165481567\n",
      "Epoch 3063: Training Loss: 0.3274153669675191 Validation Loss: 0.9661805629730225\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3064: Training Loss: 0.3281711935997009 Validation Loss: 0.9662274122238159\n",
      "Epoch 3065: Training Loss: 0.3292722702026367 Validation Loss: 0.9665801525115967\n",
      "Epoch 3066: Training Loss: 0.3277423679828644 Validation Loss: 0.9663570523262024\n",
      "Epoch 3067: Training Loss: 0.32760729392369586 Validation Loss: 0.9661242961883545\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3068: Training Loss: 0.3269999821980794 Validation Loss: 0.965962290763855\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3069: Training Loss: 0.32726139823595685 Validation Loss: 0.9655500650405884\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3070: Training Loss: 0.3269408345222473 Validation Loss: 0.9662376642227173\n",
      "Epoch 3071: Training Loss: 0.3264052172501882 Validation Loss: 0.9659644961357117\n",
      "Epoch 3072: Training Loss: 0.3270060122013092 Validation Loss: 0.966145932674408\n",
      "Epoch 3073: Training Loss: 0.3268338143825531 Validation Loss: 0.9670059084892273\n",
      "Epoch 3074: Training Loss: 0.3264700373013814 Validation Loss: 0.9668324589729309\n",
      "Epoch 3075: Training Loss: 0.3269723455111186 Validation Loss: 0.9665895104408264\n",
      "Epoch 3076: Training Loss: 0.3264511624972026 Validation Loss: 0.9664640426635742\n",
      "Epoch 3077: Training Loss: 0.3259859085083008 Validation Loss: 0.9659996628761292\n",
      "Epoch 3078: Training Loss: 0.32567426562309265 Validation Loss: 0.9652622938156128\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3079: Training Loss: 0.3257927695910136 Validation Loss: 0.9653344750404358\n",
      "Epoch 3080: Training Loss: 0.32532914479573566 Validation Loss: 0.9660081267356873\n",
      "Epoch 3081: Training Loss: 0.3255675832430522 Validation Loss: 0.9661952257156372\n",
      "Epoch 3082: Training Loss: 0.32507039109865826 Validation Loss: 0.9661781787872314\n",
      "Epoch 3083: Training Loss: 0.3246544102827708 Validation Loss: 0.9659850001335144\n",
      "Epoch 3084: Training Loss: 0.3254964252312978 Validation Loss: 0.9663389325141907\n",
      "Epoch 3085: Training Loss: 0.32489291826883954 Validation Loss: 0.9659606218338013\n",
      "Epoch 3086: Training Loss: 0.3247221112251282 Validation Loss: 0.9658639430999756\n",
      "Epoch 3087: Training Loss: 0.3244905471801758 Validation Loss: 0.9657933712005615\n",
      "Epoch 3088: Training Loss: 0.32601575056711835 Validation Loss: 0.9658614993095398\n",
      "Epoch 3089: Training Loss: 0.32471731305122375 Validation Loss: 0.9659913182258606\n",
      "Epoch 3090: Training Loss: 0.3242838482062022 Validation Loss: 0.9657772779464722\n",
      "Epoch 3091: Training Loss: 0.3244011700153351 Validation Loss: 0.9657192230224609\n",
      "Epoch 3092: Training Loss: 0.3249937395254771 Validation Loss: 0.9661305546760559\n",
      "Epoch 3093: Training Loss: 0.32521435618400574 Validation Loss: 0.965964674949646\n",
      "Epoch 3094: Training Loss: 0.325155367453893 Validation Loss: 0.9663153290748596\n",
      "Epoch 3095: Training Loss: 0.3240618606408437 Validation Loss: 0.966241717338562\n",
      "Epoch 3096: Training Loss: 0.32359729210535687 Validation Loss: 0.9661099910736084\n",
      "Epoch 3097: Training Loss: 0.3238113224506378 Validation Loss: 0.965510904788971\n",
      "Epoch 3098: Training Loss: 0.32392483949661255 Validation Loss: 0.9658315777778625\n",
      "Epoch 3099: Training Loss: 0.3235308925310771 Validation Loss: 0.9653050899505615\n",
      "Epoch 3100: Training Loss: 0.32294856508572894 Validation Loss: 0.9655889868736267\n",
      "Epoch 3101: Training Loss: 0.32269195715586346 Validation Loss: 0.9657577872276306\n",
      "Epoch 3102: Training Loss: 0.3227579593658447 Validation Loss: 0.9655468463897705\n",
      "Epoch 3103: Training Loss: 0.3225748340288798 Validation Loss: 0.9655250310897827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3104: Training Loss: 0.3231710195541382 Validation Loss: 0.9652630686759949\n",
      "Epoch 3105: Training Loss: 0.32293564081192017 Validation Loss: 0.965862512588501\n",
      "Epoch 3106: Training Loss: 0.32247260212898254 Validation Loss: 0.9656332731246948\n",
      "Epoch 3107: Training Loss: 0.32223353783289593 Validation Loss: 0.9651058316230774\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3108: Training Loss: 0.32236000895500183 Validation Loss: 0.9648383259773254\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3109: Training Loss: 0.32182801763216656 Validation Loss: 0.965425431728363\n",
      "Epoch 3110: Training Loss: 0.3221456507841746 Validation Loss: 0.9655051231384277\n",
      "Epoch 3111: Training Loss: 0.32225319743156433 Validation Loss: 0.9655722975730896\n",
      "Epoch 3112: Training Loss: 0.32148606578509015 Validation Loss: 0.9661478400230408\n",
      "Epoch 3113: Training Loss: 0.32040510574976605 Validation Loss: 0.9659754037857056\n",
      "Epoch 3114: Training Loss: 0.321026474237442 Validation Loss: 0.9653788208961487\n",
      "Epoch 3115: Training Loss: 0.3204985062281291 Validation Loss: 0.9647996425628662\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3116: Training Loss: 0.3204292853673299 Validation Loss: 0.9643155336380005\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3117: Training Loss: 0.32111313939094543 Validation Loss: 0.9646593928337097\n",
      "Epoch 3118: Training Loss: 0.3231502175331116 Validation Loss: 0.9655500650405884\n",
      "Epoch 3119: Training Loss: 0.32055890560150146 Validation Loss: 0.9663036465644836\n",
      "Epoch 3120: Training Loss: 0.3204701642195384 Validation Loss: 0.9663393497467041\n",
      "Epoch 3121: Training Loss: 0.3207510809103648 Validation Loss: 0.9663068056106567\n",
      "Epoch 3122: Training Loss: 0.3200162649154663 Validation Loss: 0.9660617113113403\n",
      "Epoch 3123: Training Loss: 0.31949523091316223 Validation Loss: 0.9657571315765381\n",
      "Epoch 3124: Training Loss: 0.3200944860776265 Validation Loss: 0.9654141664505005\n",
      "Epoch 3125: Training Loss: 0.3205265402793884 Validation Loss: 0.9642812013626099\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3126: Training Loss: 0.32081446051597595 Validation Loss: 0.964175283908844\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3127: Training Loss: 0.3209120531876882 Validation Loss: 0.9639583230018616\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3128: Training Loss: 0.3192090690135956 Validation Loss: 0.964411735534668\n",
      "Epoch 3129: Training Loss: 0.3211524188518524 Validation Loss: 0.9640160799026489\n",
      "Epoch 3130: Training Loss: 0.319928507010142 Validation Loss: 0.9641543030738831\n",
      "Epoch 3131: Training Loss: 0.31900088985761005 Validation Loss: 0.9650474190711975\n",
      "Epoch 3132: Training Loss: 0.31986290216445923 Validation Loss: 0.9652879238128662\n",
      "Epoch 3133: Training Loss: 0.31943119565645856 Validation Loss: 0.9658131003379822\n",
      "Epoch 3134: Training Loss: 0.31896404425303143 Validation Loss: 0.9661312103271484\n",
      "Epoch 3135: Training Loss: 0.31947410106658936 Validation Loss: 0.9661920070648193\n",
      "Epoch 3136: Training Loss: 0.31869598229726154 Validation Loss: 0.9653485417366028\n",
      "Epoch 3137: Training Loss: 0.31798768043518066 Validation Loss: 0.9647027850151062\n",
      "Epoch 3138: Training Loss: 0.3186129927635193 Validation Loss: 0.9640394449234009\n",
      "Epoch 3139: Training Loss: 0.318203071753184 Validation Loss: 0.9629606008529663\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3140: Training Loss: 0.3175068298975627 Validation Loss: 0.9633527994155884\n",
      "Epoch 3141: Training Loss: 0.3177439868450165 Validation Loss: 0.9640865921974182\n",
      "Epoch 3142: Training Loss: 0.3172937234242757 Validation Loss: 0.9643944501876831\n",
      "Epoch 3143: Training Loss: 0.3183637460072835 Validation Loss: 0.9649536609649658\n",
      "Epoch 3144: Training Loss: 0.3187262713909149 Validation Loss: 0.9649290442466736\n",
      "Epoch 3145: Training Loss: 0.3172057767709096 Validation Loss: 0.9654029607772827\n",
      "Epoch 3146: Training Loss: 0.3174290855725606 Validation Loss: 0.9647365808486938\n",
      "Epoch 3147: Training Loss: 0.3166545828183492 Validation Loss: 0.965451717376709\n",
      "Epoch 3148: Training Loss: 0.3175433576107025 Validation Loss: 0.9647904634475708\n",
      "Epoch 3149: Training Loss: 0.31724892059961957 Validation Loss: 0.9639446139335632\n",
      "Epoch 3150: Training Loss: 0.31642090280850727 Validation Loss: 0.9639536738395691\n",
      "Epoch 3151: Training Loss: 0.31824564933776855 Validation Loss: 0.9644212126731873\n",
      "Epoch 3152: Training Loss: 0.3165628711382548 Validation Loss: 0.9649245738983154\n",
      "Epoch 3153: Training Loss: 0.31688069303830463 Validation Loss: 0.965137243270874\n",
      "Epoch 3154: Training Loss: 0.3171740174293518 Validation Loss: 0.9653424024581909\n",
      "Epoch 3155: Training Loss: 0.31674957275390625 Validation Loss: 0.9645372033119202\n",
      "Epoch 3156: Training Loss: 0.3159224291642507 Validation Loss: 0.9636703133583069\n",
      "Epoch 3157: Training Loss: 0.31620418032010394 Validation Loss: 0.9637378454208374\n",
      "Epoch 3158: Training Loss: 0.31609665354092914 Validation Loss: 0.9642819762229919\n",
      "Epoch 3159: Training Loss: 0.31633710861206055 Validation Loss: 0.9643338322639465\n",
      "Epoch 3160: Training Loss: 0.31567524870236713 Validation Loss: 0.9642069339752197\n",
      "Epoch 3161: Training Loss: 0.31549803415934247 Validation Loss: 0.9650315642356873\n",
      "Epoch 3162: Training Loss: 0.3160332242647807 Validation Loss: 0.9650915861129761\n",
      "Epoch 3163: Training Loss: 0.31529226899147034 Validation Loss: 0.9641004204750061\n",
      "Epoch 3164: Training Loss: 0.31545905272165936 Validation Loss: 0.963632345199585\n",
      "Epoch 3165: Training Loss: 0.316464364528656 Validation Loss: 0.9630446434020996\n",
      "Epoch 3166: Training Loss: 0.31580813725789386 Validation Loss: 0.9631909132003784\n",
      "Epoch 3167: Training Loss: 0.31498714288075763 Validation Loss: 0.9635420441627502\n",
      "Epoch 3168: Training Loss: 0.31447197993596393 Validation Loss: 0.9637905359268188\n",
      "Epoch 3169: Training Loss: 0.3142344355583191 Validation Loss: 0.9633280634880066\n",
      "Epoch 3170: Training Loss: 0.31418176492055255 Validation Loss: 0.9634062051773071\n",
      "Epoch 3171: Training Loss: 0.3141368627548218 Validation Loss: 0.9637075662612915\n",
      "Epoch 3172: Training Loss: 0.31413020690282184 Validation Loss: 0.9641982913017273\n",
      "Epoch 3173: Training Loss: 0.31415267785390216 Validation Loss: 0.9646462798118591\n",
      "Epoch 3174: Training Loss: 0.31425289312998456 Validation Loss: 0.965132474899292\n",
      "Epoch 3175: Training Loss: 0.31433911124865216 Validation Loss: 0.9648619890213013\n",
      "Epoch 3176: Training Loss: 0.31329875191052753 Validation Loss: 0.9645468592643738\n",
      "Epoch 3177: Training Loss: 0.3137771983941396 Validation Loss: 0.9639377593994141\n",
      "Epoch 3178: Training Loss: 0.3132839600245158 Validation Loss: 0.963489294052124\n",
      "Epoch 3179: Training Loss: 0.3131023148695628 Validation Loss: 0.963553249835968\n",
      "Epoch 3180: Training Loss: 0.3143350879351298 Validation Loss: 0.9638556241989136\n",
      "Epoch 3181: Training Loss: 0.3133545418580373 Validation Loss: 0.9634108543395996\n",
      "Epoch 3182: Training Loss: 0.3129609326521556 Validation Loss: 0.9629805684089661\n",
      "Epoch 3183: Training Loss: 0.3129102090994517 Validation Loss: 0.9627793431282043\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3184: Training Loss: 0.3139786620934804 Validation Loss: 0.9629701972007751\n",
      "Epoch 3185: Training Loss: 0.3123277525107066 Validation Loss: 0.9632284641265869\n",
      "Epoch 3186: Training Loss: 0.31241077184677124 Validation Loss: 0.9633201956748962\n",
      "Epoch 3187: Training Loss: 0.31230082114537555 Validation Loss: 0.9630928635597229\n",
      "Epoch 3188: Training Loss: 0.3124694724877675 Validation Loss: 0.9634488821029663\n",
      "Epoch 3189: Training Loss: 0.3121516704559326 Validation Loss: 0.9631769061088562\n",
      "Epoch 3190: Training Loss: 0.3126225769519806 Validation Loss: 0.964326798915863\n",
      "Epoch 3191: Training Loss: 0.31184308727582294 Validation Loss: 0.9634433388710022\n",
      "Epoch 3192: Training Loss: 0.3120366930961609 Validation Loss: 0.9629717469215393\n",
      "Epoch 3193: Training Loss: 0.3121252457300822 Validation Loss: 0.9629461169242859\n",
      "Epoch 3194: Training Loss: 0.3109699885050456 Validation Loss: 0.9628974795341492\n",
      "Epoch 3195: Training Loss: 0.31238065163294476 Validation Loss: 0.9630683660507202\n",
      "Epoch 3196: Training Loss: 0.3154633790254593 Validation Loss: 0.9637773633003235\n",
      "Epoch 3197: Training Loss: 0.3113400141398112 Validation Loss: 0.9636510014533997\n",
      "Epoch 3198: Training Loss: 0.31113534172375995 Validation Loss: 0.9642462730407715\n",
      "Epoch 3199: Training Loss: 0.31041817863782245 Validation Loss: 0.9643243551254272\n",
      "Epoch 3200: Training Loss: 0.31284667054812115 Validation Loss: 0.9641982913017273\n",
      "Epoch 3201: Training Loss: 0.31059781710306805 Validation Loss: 0.9641168117523193\n",
      "Epoch 3202: Training Loss: 0.31035786867141724 Validation Loss: 0.9637797474861145\n",
      "Epoch 3203: Training Loss: 0.31043585141499835 Validation Loss: 0.9638029932975769\n",
      "Epoch 3204: Training Loss: 0.3123551309108734 Validation Loss: 0.9628552794456482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3205: Training Loss: 0.3102230727672577 Validation Loss: 0.9627377986907959\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3206: Training Loss: 0.3106425901254018 Validation Loss: 0.9623013138771057\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3207: Training Loss: 0.31241655349731445 Validation Loss: 0.9626888632774353\n",
      "Epoch 3208: Training Loss: 0.30968328317006427 Validation Loss: 0.9625561237335205\n",
      "Epoch 3209: Training Loss: 0.31044430534044903 Validation Loss: 0.9626505374908447\n",
      "Epoch 3210: Training Loss: 0.30921144286791485 Validation Loss: 0.9628399014472961\n",
      "Epoch 3211: Training Loss: 0.30944205323855084 Validation Loss: 0.9629068374633789\n",
      "Epoch 3212: Training Loss: 0.3102648854255676 Validation Loss: 0.9638421535491943\n",
      "Epoch 3213: Training Loss: 0.30891961852709454 Validation Loss: 0.9637219905853271\n",
      "Epoch 3214: Training Loss: 0.30895987153053284 Validation Loss: 0.9632943868637085\n",
      "Epoch 3215: Training Loss: 0.30891287326812744 Validation Loss: 0.9627332091331482\n",
      "Epoch 3216: Training Loss: 0.3087795078754425 Validation Loss: 0.9625638723373413\n",
      "Epoch 3217: Training Loss: 0.3090588450431824 Validation Loss: 0.9628536105155945\n",
      "Epoch 3218: Training Loss: 0.3086034854253133 Validation Loss: 0.9630133509635925\n",
      "Epoch 3219: Training Loss: 0.3091222246487935 Validation Loss: 0.9626154899597168\n",
      "Epoch 3220: Training Loss: 0.3080962896347046 Validation Loss: 0.9625176787376404\n",
      "Epoch 3221: Training Loss: 0.30828357736269635 Validation Loss: 0.9628680348396301\n",
      "Epoch 3222: Training Loss: 0.30862762530644733 Validation Loss: 0.9625132083892822\n",
      "Epoch 3223: Training Loss: 0.3082185685634613 Validation Loss: 0.963781476020813\n",
      "Epoch 3224: Training Loss: 0.3076540728410085 Validation Loss: 0.9633777737617493\n",
      "Epoch 3225: Training Loss: 0.3075263500213623 Validation Loss: 0.963517963886261\n",
      "Epoch 3226: Training Loss: 0.30827702085177106 Validation Loss: 0.9632277488708496\n",
      "Epoch 3227: Training Loss: 0.3072381814320882 Validation Loss: 0.9633261561393738\n",
      "Epoch 3228: Training Loss: 0.30669281880060834 Validation Loss: 0.9633871912956238\n",
      "Epoch 3229: Training Loss: 0.3073476354281108 Validation Loss: 0.9625473022460938\n",
      "Epoch 3230: Training Loss: 0.30805496374766034 Validation Loss: 0.9622570276260376\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3231: Training Loss: 0.3077916204929352 Validation Loss: 0.9622491598129272\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3232: Training Loss: 0.30642707149187726 Validation Loss: 0.9624919891357422\n",
      "Epoch 3233: Training Loss: 0.3077644209067027 Validation Loss: 0.9626436233520508\n",
      "Epoch 3234: Training Loss: 0.3067336281140645 Validation Loss: 0.9624869227409363\n",
      "Epoch 3235: Training Loss: 0.30764858921368915 Validation Loss: 0.963193953037262\n",
      "Epoch 3236: Training Loss: 0.3072093228499095 Validation Loss: 0.963429868221283\n",
      "Epoch 3237: Training Loss: 0.3115438222885132 Validation Loss: 0.9633753299713135\n",
      "Epoch 3238: Training Loss: 0.30677104989687604 Validation Loss: 0.9632542133331299\n",
      "Epoch 3239: Training Loss: 0.3060717085997264 Validation Loss: 0.962235689163208\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3240: Training Loss: 0.30664001901944477 Validation Loss: 0.9618995785713196\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3241: Training Loss: 0.3051364024480184 Validation Loss: 0.9618998169898987\n",
      "Epoch 3242: Training Loss: 0.30554138620694477 Validation Loss: 0.9622931480407715\n",
      "Epoch 3243: Training Loss: 0.30613940954208374 Validation Loss: 0.9629690647125244\n",
      "Epoch 3244: Training Loss: 0.30563894907633465 Validation Loss: 0.9625142812728882\n",
      "Epoch 3245: Training Loss: 0.305426965157191 Validation Loss: 0.9622372388839722\n",
      "Epoch 3246: Training Loss: 0.30552545189857483 Validation Loss: 0.9628463387489319\n",
      "Epoch 3247: Training Loss: 0.30517881115277606 Validation Loss: 0.9625699520111084\n",
      "Epoch 3248: Training Loss: 0.3061344524224599 Validation Loss: 0.962628185749054\n",
      "Epoch 3249: Training Loss: 0.3058382471402486 Validation Loss: 0.9621416926383972\n",
      "Epoch 3250: Training Loss: 0.30505367120107013 Validation Loss: 0.9621341824531555\n",
      "Epoch 3251: Training Loss: 0.30457383394241333 Validation Loss: 0.9620890617370605\n",
      "Epoch 3252: Training Loss: 0.30476269125938416 Validation Loss: 0.962958812713623\n",
      "Epoch 3253: Training Loss: 0.3050425450007121 Validation Loss: 0.9637522101402283\n",
      "Epoch 3254: Training Loss: 0.3044297893842061 Validation Loss: 0.9630898237228394\n",
      "Epoch 3255: Training Loss: 0.30494511127471924 Validation Loss: 0.9624790549278259\n",
      "Epoch 3256: Training Loss: 0.30461540818214417 Validation Loss: 0.961554765701294\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3257: Training Loss: 0.3049995203812917 Validation Loss: 0.9610885977745056\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3258: Training Loss: 0.30417678753534955 Validation Loss: 0.961750328540802\n",
      "Epoch 3259: Training Loss: 0.30529242753982544 Validation Loss: 0.9618709087371826\n",
      "Epoch 3260: Training Loss: 0.303556223710378 Validation Loss: 0.9622877836227417\n",
      "Epoch 3261: Training Loss: 0.30373790860176086 Validation Loss: 0.9625828266143799\n",
      "Epoch 3262: Training Loss: 0.30425138274828595 Validation Loss: 0.962771475315094\n",
      "Epoch 3263: Training Loss: 0.3035825987656911 Validation Loss: 0.9625173807144165\n",
      "Epoch 3264: Training Loss: 0.3049711088339488 Validation Loss: 0.9618515372276306\n",
      "Epoch 3265: Training Loss: 0.30378753940264386 Validation Loss: 0.9620773196220398\n",
      "Epoch 3266: Training Loss: 0.3035590449968974 Validation Loss: 0.9616198539733887\n",
      "Epoch 3267: Training Loss: 0.30316121379534405 Validation Loss: 0.961564838886261\n",
      "Epoch 3268: Training Loss: 0.3026914894580841 Validation Loss: 0.9616264700889587\n",
      "Epoch 3269: Training Loss: 0.30344433585802716 Validation Loss: 0.9615536332130432\n",
      "Epoch 3270: Training Loss: 0.3026592632134755 Validation Loss: 0.9625335335731506\n",
      "Epoch 3271: Training Loss: 0.3056652247905731 Validation Loss: 0.9623966813087463\n",
      "Epoch 3272: Training Loss: 0.30249473452568054 Validation Loss: 0.962124228477478\n",
      "Epoch 3273: Training Loss: 0.30345656474431354 Validation Loss: 0.962730884552002\n",
      "Epoch 3274: Training Loss: 0.3021768132845561 Validation Loss: 0.9623376727104187\n",
      "Epoch 3275: Training Loss: 0.30296723047892254 Validation Loss: 0.9624579548835754\n",
      "Epoch 3276: Training Loss: 0.30259652932484943 Validation Loss: 0.9631748795509338\n",
      "Epoch 3277: Training Loss: 0.30218470096588135 Validation Loss: 0.9628368616104126\n",
      "Epoch 3278: Training Loss: 0.30227067073186237 Validation Loss: 0.9621946215629578\n",
      "Epoch 3279: Training Loss: 0.3015637497107188 Validation Loss: 0.9612249732017517\n",
      "Epoch 3280: Training Loss: 0.30082035064697266 Validation Loss: 0.9612218737602234\n",
      "Epoch 3281: Training Loss: 0.3021554748217265 Validation Loss: 0.9611403942108154\n",
      "Epoch 3282: Training Loss: 0.3021478255589803 Validation Loss: 0.961543083190918\n",
      "Epoch 3283: Training Loss: 0.30298235019048053 Validation Loss: 0.9613234400749207\n",
      "Epoch 3284: Training Loss: 0.3010670443375905 Validation Loss: 0.9615239500999451\n",
      "Epoch 3285: Training Loss: 0.3018225034077962 Validation Loss: 0.9614737033843994\n",
      "Epoch 3286: Training Loss: 0.3009115854899089 Validation Loss: 0.9618875980377197\n",
      "Epoch 3287: Training Loss: 0.30051013827323914 Validation Loss: 0.961471676826477\n",
      "Epoch 3288: Training Loss: 0.30058854818344116 Validation Loss: 0.9617770314216614\n",
      "Epoch 3289: Training Loss: 0.3011707365512848 Validation Loss: 0.9616492390632629\n",
      "Epoch 3290: Training Loss: 0.29992708563804626 Validation Loss: 0.9618346691131592\n",
      "Epoch 3291: Training Loss: 0.30268730719884235 Validation Loss: 0.9617426991462708\n",
      "Epoch 3292: Training Loss: 0.2996998131275177 Validation Loss: 0.9625335335731506\n",
      "Epoch 3293: Training Loss: 0.30031277736028034 Validation Loss: 0.9621060490608215\n",
      "Epoch 3294: Training Loss: 0.30052463213602704 Validation Loss: 0.9622579216957092\n",
      "Epoch 3295: Training Loss: 0.3002270956834157 Validation Loss: 0.9611227512359619\n",
      "Epoch 3296: Training Loss: 0.30188316106796265 Validation Loss: 0.9605640769004822\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3297: Training Loss: 0.3000775674978892 Validation Loss: 0.9609586596488953\n",
      "Epoch 3298: Training Loss: 0.29989123344421387 Validation Loss: 0.9621204733848572\n",
      "Epoch 3299: Training Loss: 0.29947354396184284 Validation Loss: 0.9621480703353882\n",
      "Epoch 3300: Training Loss: 0.29990612467130023 Validation Loss: 0.962640106678009\n",
      "Epoch 3301: Training Loss: 0.29894838730494183 Validation Loss: 0.9623880386352539\n",
      "Epoch 3302: Training Loss: 0.2991935610771179 Validation Loss: 0.961937665939331\n",
      "Epoch 3303: Training Loss: 0.2993396421273549 Validation Loss: 0.9613140821456909\n",
      "Epoch 3304: Training Loss: 0.29968172311782837 Validation Loss: 0.9604899883270264\n",
      "New Checkpoint Saved into PATH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3305: Training Loss: 0.2991009255250295 Validation Loss: 0.9603332877159119\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3306: Training Loss: 0.2988276580969493 Validation Loss: 0.961062490940094\n",
      "Epoch 3307: Training Loss: 0.29952770471572876 Validation Loss: 0.9620011448860168\n",
      "Epoch 3308: Training Loss: 0.2980557481447856 Validation Loss: 0.9618582725524902\n",
      "Epoch 3309: Training Loss: 0.2996092935403188 Validation Loss: 0.9617539644241333\n",
      "Epoch 3310: Training Loss: 0.2984745303789775 Validation Loss: 0.9615909457206726\n",
      "Epoch 3311: Training Loss: 0.29918646812438965 Validation Loss: 0.9618027210235596\n",
      "Epoch 3312: Training Loss: 0.2981055478254954 Validation Loss: 0.96119624376297\n",
      "Epoch 3313: Training Loss: 0.2988755702972412 Validation Loss: 0.9607034921646118\n",
      "Epoch 3314: Training Loss: 0.2974053919315338 Validation Loss: 0.9612785577774048\n",
      "Epoch 3315: Training Loss: 0.29765268166859943 Validation Loss: 0.9607079029083252\n",
      "Epoch 3316: Training Loss: 0.29745548963546753 Validation Loss: 0.9607366919517517\n",
      "Epoch 3317: Training Loss: 0.29756973187128705 Validation Loss: 0.9610469341278076\n",
      "Epoch 3318: Training Loss: 0.29702738920847577 Validation Loss: 0.96145099401474\n",
      "Epoch 3319: Training Loss: 0.29760323961575824 Validation Loss: 0.9615071415901184\n",
      "Epoch 3320: Training Loss: 0.2971494098504384 Validation Loss: 0.9617099761962891\n",
      "Epoch 3321: Training Loss: 0.2969689965248108 Validation Loss: 0.9617542624473572\n",
      "Epoch 3322: Training Loss: 0.29763486981391907 Validation Loss: 0.9613508582115173\n",
      "Epoch 3323: Training Loss: 0.29677867889404297 Validation Loss: 0.9606651663780212\n",
      "Epoch 3324: Training Loss: 0.296470711628596 Validation Loss: 0.9616538286209106\n",
      "Epoch 3325: Training Loss: 0.2965630392233531 Validation Loss: 0.9619552493095398\n",
      "Epoch 3326: Training Loss: 0.29648588101069134 Validation Loss: 0.961508572101593\n",
      "Epoch 3327: Training Loss: 0.2969178855419159 Validation Loss: 0.961295485496521\n",
      "Epoch 3328: Training Loss: 0.2967035671075185 Validation Loss: 0.9610541462898254\n",
      "Epoch 3329: Training Loss: 0.29862164457639057 Validation Loss: 0.961628258228302\n",
      "Epoch 3330: Training Loss: 0.2961078683535258 Validation Loss: 0.9611281752586365\n",
      "Epoch 3331: Training Loss: 0.2971751391887665 Validation Loss: 0.9614625573158264\n",
      "Epoch 3332: Training Loss: 0.2958901524543762 Validation Loss: 0.9604741930961609\n",
      "Epoch 3333: Training Loss: 0.2955902616182963 Validation Loss: 0.9600771069526672\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3334: Training Loss: 0.2962123354276021 Validation Loss: 0.9602831602096558\n",
      "Epoch 3335: Training Loss: 0.29618552327156067 Validation Loss: 0.9597413539886475\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3336: Training Loss: 0.2952841619650523 Validation Loss: 0.960374116897583\n",
      "Epoch 3337: Training Loss: 0.29620487491289776 Validation Loss: 0.9608554840087891\n",
      "Epoch 3338: Training Loss: 0.2954562306404114 Validation Loss: 0.9613140821456909\n",
      "Epoch 3339: Training Loss: 0.29537923137346905 Validation Loss: 0.9615478515625\n",
      "Epoch 3340: Training Loss: 0.29596416155497235 Validation Loss: 0.9621864557266235\n",
      "Epoch 3341: Training Loss: 0.2950627605120341 Validation Loss: 0.9619389176368713\n",
      "Epoch 3342: Training Loss: 0.2956175406773885 Validation Loss: 0.9612128734588623\n",
      "Epoch 3343: Training Loss: 0.29577470819155377 Validation Loss: 0.9603316783905029\n",
      "Epoch 3344: Training Loss: 0.2942468225955963 Validation Loss: 0.9598991274833679\n",
      "Epoch 3345: Training Loss: 0.29476138949394226 Validation Loss: 0.9600587487220764\n",
      "Epoch 3346: Training Loss: 0.29439373811086017 Validation Loss: 0.9606322646141052\n",
      "Epoch 3347: Training Loss: 0.2946255604426066 Validation Loss: 0.9608572721481323\n",
      "Epoch 3348: Training Loss: 0.2957700590292613 Validation Loss: 0.9612443447113037\n",
      "Epoch 3349: Training Loss: 0.29554522037506104 Validation Loss: 0.9615103006362915\n",
      "Epoch 3350: Training Loss: 0.2945503195126851 Validation Loss: 0.9615213871002197\n",
      "Epoch 3351: Training Loss: 0.2943639854590098 Validation Loss: 0.9615234136581421\n",
      "Epoch 3352: Training Loss: 0.2952239016691844 Validation Loss: 0.9617003798484802\n",
      "Epoch 3353: Training Loss: 0.29302677512168884 Validation Loss: 0.9615921378135681\n",
      "Epoch 3354: Training Loss: 0.2942196528116862 Validation Loss: 0.960532009601593\n",
      "Epoch 3355: Training Loss: 0.2932935357093811 Validation Loss: 0.9601835608482361\n",
      "Epoch 3356: Training Loss: 0.29297785957654315 Validation Loss: 0.9597883224487305\n",
      "Epoch 3357: Training Loss: 0.29359960556030273 Validation Loss: 0.9601194262504578\n",
      "Epoch 3358: Training Loss: 0.295723021030426 Validation Loss: 0.9602484107017517\n",
      "Epoch 3359: Training Loss: 0.29324469963709515 Validation Loss: 0.9613139629364014\n",
      "Epoch 3360: Training Loss: 0.2933870752652486 Validation Loss: 0.9612922668457031\n",
      "Epoch 3361: Training Loss: 0.2934591770172119 Validation Loss: 0.9615309834480286\n",
      "Epoch 3362: Training Loss: 0.2928256392478943 Validation Loss: 0.9608907699584961\n",
      "Epoch 3363: Training Loss: 0.2925471564133962 Validation Loss: 0.9607020616531372\n",
      "Epoch 3364: Training Loss: 0.29328227043151855 Validation Loss: 0.9601541757583618\n",
      "Epoch 3365: Training Loss: 0.29221707582473755 Validation Loss: 0.9602358341217041\n",
      "Epoch 3366: Training Loss: 0.2923672596613566 Validation Loss: 0.9606083631515503\n",
      "Epoch 3367: Training Loss: 0.2928025523821513 Validation Loss: 0.9602611660957336\n",
      "Epoch 3368: Training Loss: 0.2930128276348114 Validation Loss: 0.960353434085846\n",
      "Epoch 3369: Training Loss: 0.29191842675209045 Validation Loss: 0.960162878036499\n",
      "Epoch 3370: Training Loss: 0.29244659344355267 Validation Loss: 0.9606126546859741\n",
      "Epoch 3371: Training Loss: 0.2920900881290436 Validation Loss: 0.9609790444374084\n",
      "Epoch 3372: Training Loss: 0.2922758460044861 Validation Loss: 0.9610287547111511\n",
      "Epoch 3373: Training Loss: 0.2924882372220357 Validation Loss: 0.9603760242462158\n",
      "Epoch 3374: Training Loss: 0.2914352019627889 Validation Loss: 0.9601876139640808\n",
      "Epoch 3375: Training Loss: 0.2921759784221649 Validation Loss: 0.9609251618385315\n",
      "Epoch 3376: Training Loss: 0.2918716371059418 Validation Loss: 0.9609830975532532\n",
      "Epoch 3377: Training Loss: 0.2918561100959778 Validation Loss: 0.9601817727088928\n",
      "Epoch 3378: Training Loss: 0.2910825312137604 Validation Loss: 0.9605873227119446\n",
      "Epoch 3379: Training Loss: 0.29077548782030743 Validation Loss: 0.9609116315841675\n",
      "Epoch 3380: Training Loss: 0.29096325238545734 Validation Loss: 0.9608750939369202\n",
      "Epoch 3381: Training Loss: 0.29196224610010785 Validation Loss: 0.9614969491958618\n",
      "Epoch 3382: Training Loss: 0.29106826583544415 Validation Loss: 0.9616190195083618\n",
      "Epoch 3383: Training Loss: 0.29033859570821124 Validation Loss: 0.9605491161346436\n",
      "Epoch 3384: Training Loss: 0.29043782750765484 Validation Loss: 0.9601517915725708\n",
      "Epoch 3385: Training Loss: 0.2905186017354329 Validation Loss: 0.9601383209228516\n",
      "Epoch 3386: Training Loss: 0.2901173233985901 Validation Loss: 0.9597769379615784\n",
      "Epoch 3387: Training Loss: 0.29065295060475665 Validation Loss: 0.959646999835968\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3388: Training Loss: 0.2901989320913951 Validation Loss: 0.9592236876487732\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3389: Training Loss: 0.28981829682985943 Validation Loss: 0.9593850374221802\n",
      "Epoch 3390: Training Loss: 0.2908298472563426 Validation Loss: 0.9597329497337341\n",
      "Epoch 3391: Training Loss: 0.28950223326683044 Validation Loss: 0.9605945348739624\n",
      "Epoch 3392: Training Loss: 0.2915563682715098 Validation Loss: 0.9601880311965942\n",
      "Epoch 3393: Training Loss: 0.28944236040115356 Validation Loss: 0.960152268409729\n",
      "Epoch 3394: Training Loss: 0.29023729761441547 Validation Loss: 0.9601249098777771\n",
      "Epoch 3395: Training Loss: 0.28938932220141095 Validation Loss: 0.9597862958908081\n",
      "Epoch 3396: Training Loss: 0.2890896300474803 Validation Loss: 0.9598484635353088\n",
      "Epoch 3397: Training Loss: 0.2913818260033925 Validation Loss: 0.9607193470001221\n",
      "Epoch 3398: Training Loss: 0.28898417949676514 Validation Loss: 0.9603861570358276\n",
      "Epoch 3399: Training Loss: 0.2894599636395772 Validation Loss: 0.9611334800720215\n",
      "Epoch 3400: Training Loss: 0.28891319036483765 Validation Loss: 0.960993766784668\n",
      "Epoch 3401: Training Loss: 0.28953812023003894 Validation Loss: 0.9606574177742004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3402: Training Loss: 0.28933549920717877 Validation Loss: 0.9603096842765808\n",
      "Epoch 3403: Training Loss: 0.28889543811480206 Validation Loss: 0.9596344828605652\n",
      "Epoch 3404: Training Loss: 0.2872665921847026 Validation Loss: 0.9598237872123718\n",
      "Epoch 3405: Training Loss: 0.28835559884707135 Validation Loss: 0.9598606824874878\n",
      "Epoch 3406: Training Loss: 0.289547324180603 Validation Loss: 0.9605877995491028\n",
      "Epoch 3407: Training Loss: 0.2881477177143097 Validation Loss: 0.9607484936714172\n",
      "Epoch 3408: Training Loss: 0.2882414956887563 Validation Loss: 0.9609382748603821\n",
      "Epoch 3409: Training Loss: 0.2885158856709798 Validation Loss: 0.9606354236602783\n",
      "Epoch 3410: Training Loss: 0.2876747449239095 Validation Loss: 0.9597781896591187\n",
      "Epoch 3411: Training Loss: 0.28759684165318805 Validation Loss: 0.9594427943229675\n",
      "Epoch 3412: Training Loss: 0.28816621502240497 Validation Loss: 0.9597822427749634\n",
      "Epoch 3413: Training Loss: 0.2878205676873525 Validation Loss: 0.9594514966011047\n",
      "Epoch 3414: Training Loss: 0.28742849826812744 Validation Loss: 0.9597285389900208\n",
      "Epoch 3415: Training Loss: 0.2872077425320943 Validation Loss: 0.9603893756866455\n",
      "Epoch 3416: Training Loss: 0.28760960698127747 Validation Loss: 0.9603090286254883\n",
      "Epoch 3417: Training Loss: 0.2865769664446513 Validation Loss: 0.9605472087860107\n",
      "Epoch 3418: Training Loss: 0.28796206911404926 Validation Loss: 0.9607408046722412\n",
      "Epoch 3419: Training Loss: 0.2869679133097331 Validation Loss: 0.9604786038398743\n",
      "Epoch 3420: Training Loss: 0.2865443329016368 Validation Loss: 0.9604241847991943\n",
      "Epoch 3421: Training Loss: 0.288267840941747 Validation Loss: 0.9601002931594849\n",
      "Epoch 3422: Training Loss: 0.2866855164368947 Validation Loss: 0.9597648978233337\n",
      "Epoch 3423: Training Loss: 0.2864327629407247 Validation Loss: 0.9600763916969299\n",
      "Epoch 3424: Training Loss: 0.28646225730578107 Validation Loss: 0.9597187638282776\n",
      "Epoch 3425: Training Loss: 0.28662625948588055 Validation Loss: 0.9595447182655334\n",
      "Epoch 3426: Training Loss: 0.28600671887397766 Validation Loss: 0.9597777724266052\n",
      "Epoch 3427: Training Loss: 0.2861051062742869 Validation Loss: 0.9600059986114502\n",
      "Epoch 3428: Training Loss: 0.285881370306015 Validation Loss: 0.9601994752883911\n",
      "Epoch 3429: Training Loss: 0.28576816121737164 Validation Loss: 0.9604131579399109\n",
      "Epoch 3430: Training Loss: 0.2876381427049637 Validation Loss: 0.9600513577461243\n",
      "Epoch 3431: Training Loss: 0.28539780775705975 Validation Loss: 0.9603284001350403\n",
      "Epoch 3432: Training Loss: 0.28546756505966187 Validation Loss: 0.9595635533332825\n",
      "Epoch 3433: Training Loss: 0.28589316209157306 Validation Loss: 0.9593515396118164\n",
      "Epoch 3434: Training Loss: 0.2852322558561961 Validation Loss: 0.9594861268997192\n",
      "Epoch 3435: Training Loss: 0.28575244545936584 Validation Loss: 0.9604015946388245\n",
      "Epoch 3436: Training Loss: 0.28555232286453247 Validation Loss: 0.9603329300880432\n",
      "Epoch 3437: Training Loss: 0.28603431582450867 Validation Loss: 0.9596261382102966\n",
      "Epoch 3438: Training Loss: 0.2868148585160573 Validation Loss: 0.9595498442649841\n",
      "Epoch 3439: Training Loss: 0.2851967513561249 Validation Loss: 0.9595180153846741\n",
      "Epoch 3440: Training Loss: 0.2855651179949443 Validation Loss: 0.9594780802726746\n",
      "Epoch 3441: Training Loss: 0.2853505114714305 Validation Loss: 0.960023820400238\n",
      "Epoch 3442: Training Loss: 0.2849220037460327 Validation Loss: 0.9600316286087036\n",
      "Epoch 3443: Training Loss: 0.2849461833635966 Validation Loss: 0.9603500366210938\n",
      "Epoch 3444: Training Loss: 0.285001074274381 Validation Loss: 0.960050106048584\n",
      "Epoch 3445: Training Loss: 0.2846363087495168 Validation Loss: 0.9598045349121094\n",
      "Epoch 3446: Training Loss: 0.28486596544583637 Validation Loss: 0.9599339962005615\n",
      "Epoch 3447: Training Loss: 0.2840968469778697 Validation Loss: 0.9589903354644775\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3448: Training Loss: 0.28416399161020917 Validation Loss: 0.958480179309845\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3449: Training Loss: 0.28357914090156555 Validation Loss: 0.9587801694869995\n",
      "Epoch 3450: Training Loss: 0.28415559728940326 Validation Loss: 0.9590859413146973\n",
      "Epoch 3451: Training Loss: 0.28420667846997577 Validation Loss: 0.9594859480857849\n",
      "Epoch 3452: Training Loss: 0.2836860617001851 Validation Loss: 0.9609004259109497\n",
      "Epoch 3453: Training Loss: 0.2841354012489319 Validation Loss: 0.960729718208313\n",
      "Epoch 3454: Training Loss: 0.2842038571834564 Validation Loss: 0.9608364105224609\n",
      "Epoch 3455: Training Loss: 0.28439997633298236 Validation Loss: 0.9605953693389893\n",
      "Epoch 3456: Training Loss: 0.2830127775669098 Validation Loss: 0.9604355692863464\n",
      "Epoch 3457: Training Loss: 0.28286022941271466 Validation Loss: 0.9598021507263184\n",
      "Epoch 3458: Training Loss: 0.28285809357961017 Validation Loss: 0.9592840075492859\n",
      "Epoch 3459: Training Loss: 0.28374214967091876 Validation Loss: 0.959732174873352\n",
      "Epoch 3460: Training Loss: 0.28302016854286194 Validation Loss: 0.9600008726119995\n",
      "Epoch 3461: Training Loss: 0.2827409009138743 Validation Loss: 0.9602405428886414\n",
      "Epoch 3462: Training Loss: 0.28248422344525653 Validation Loss: 0.9603349566459656\n",
      "Epoch 3463: Training Loss: 0.2829650441805522 Validation Loss: 0.959848165512085\n",
      "Epoch 3464: Training Loss: 0.28272923827171326 Validation Loss: 0.9601516127586365\n",
      "Epoch 3465: Training Loss: 0.28373871246973675 Validation Loss: 0.9590921998023987\n",
      "Epoch 3466: Training Loss: 0.2821749250094096 Validation Loss: 0.9588016867637634\n",
      "Epoch 3467: Training Loss: 0.28434059023857117 Validation Loss: 0.9587869048118591\n",
      "Epoch 3468: Training Loss: 0.2822319467862447 Validation Loss: 0.958556592464447\n",
      "Epoch 3469: Training Loss: 0.2818532884120941 Validation Loss: 0.9581372737884521\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3470: Training Loss: 0.281800776720047 Validation Loss: 0.9592689275741577\n",
      "Epoch 3471: Training Loss: 0.28165217240651447 Validation Loss: 0.9597880244255066\n",
      "Epoch 3472: Training Loss: 0.28167834877967834 Validation Loss: 0.960390031337738\n",
      "Epoch 3473: Training Loss: 0.28198692202568054 Validation Loss: 0.960699737071991\n",
      "Epoch 3474: Training Loss: 0.2819436987241109 Validation Loss: 0.9602059125900269\n",
      "Epoch 3475: Training Loss: 0.28151912490526837 Validation Loss: 0.9598820209503174\n",
      "Epoch 3476: Training Loss: 0.2809288402398427 Validation Loss: 0.9590682983398438\n",
      "Epoch 3477: Training Loss: 0.2815919021765391 Validation Loss: 0.959195613861084\n",
      "Epoch 3478: Training Loss: 0.2812725404898326 Validation Loss: 0.959845781326294\n",
      "Epoch 3479: Training Loss: 0.2809872229894002 Validation Loss: 0.9599260091781616\n",
      "Epoch 3480: Training Loss: 0.2812915841738383 Validation Loss: 0.9597838521003723\n",
      "Epoch 3481: Training Loss: 0.28102805217107135 Validation Loss: 0.95933598279953\n",
      "Epoch 3482: Training Loss: 0.28078489502271015 Validation Loss: 0.9589155316352844\n",
      "Epoch 3483: Training Loss: 0.2803639868895213 Validation Loss: 0.9592443704605103\n",
      "Epoch 3484: Training Loss: 0.2798525094985962 Validation Loss: 0.9598502516746521\n",
      "Epoch 3485: Training Loss: 0.2817973891894023 Validation Loss: 0.9597905278205872\n",
      "Epoch 3486: Training Loss: 0.28063031037648517 Validation Loss: 0.9593873023986816\n",
      "Epoch 3487: Training Loss: 0.2804735203584035 Validation Loss: 0.9598512053489685\n",
      "Epoch 3488: Training Loss: 0.28059878945350647 Validation Loss: 0.9599263072013855\n",
      "Epoch 3489: Training Loss: 0.27999353408813477 Validation Loss: 0.960496723651886\n",
      "Epoch 3490: Training Loss: 0.28039003908634186 Validation Loss: 0.9599019289016724\n",
      "Epoch 3491: Training Loss: 0.2797616422176361 Validation Loss: 0.9593667984008789\n",
      "Epoch 3492: Training Loss: 0.2798147300879161 Validation Loss: 0.9595596194267273\n",
      "Epoch 3493: Training Loss: 0.27955342332522076 Validation Loss: 0.960001528263092\n",
      "Epoch 3494: Training Loss: 0.28067347407341003 Validation Loss: 0.9600843191146851\n",
      "Epoch 3495: Training Loss: 0.27943726380666095 Validation Loss: 0.9595767259597778\n",
      "Epoch 3496: Training Loss: 0.2793309688568115 Validation Loss: 0.959555447101593\n",
      "Epoch 3497: Training Loss: 0.2794238229592641 Validation Loss: 0.9598021507263184\n",
      "Epoch 3498: Training Loss: 0.2793544928232829 Validation Loss: 0.9595381021499634\n",
      "Epoch 3499: Training Loss: 0.27843979994455975 Validation Loss: 0.9597654342651367\n",
      "Epoch 3500: Training Loss: 0.278551588455836 Validation Loss: 0.9593797326087952\n",
      "Epoch 3501: Training Loss: 0.27894513805707294 Validation Loss: 0.9589601159095764\n",
      "Epoch 3502: Training Loss: 0.2797635595003764 Validation Loss: 0.9586226940155029\n",
      "Epoch 3503: Training Loss: 0.2790595094362895 Validation Loss: 0.9581902027130127\n",
      "Epoch 3504: Training Loss: 0.27905502915382385 Validation Loss: 0.9586621522903442\n",
      "Epoch 3505: Training Loss: 0.2793707052866618 Validation Loss: 0.9588048458099365\n",
      "Epoch 3506: Training Loss: 0.2787436942259471 Validation Loss: 0.9591079950332642\n",
      "Epoch 3507: Training Loss: 0.27884329358736676 Validation Loss: 0.9601747989654541\n",
      "Epoch 3508: Training Loss: 0.2785099546114604 Validation Loss: 0.9603959918022156\n",
      "Epoch 3509: Training Loss: 0.2784852186838786 Validation Loss: 0.9606017470359802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3510: Training Loss: 0.27935009201367694 Validation Loss: 0.9601718783378601\n",
      "Epoch 3511: Training Loss: 0.27788634101549786 Validation Loss: 0.9595836400985718\n",
      "Epoch 3512: Training Loss: 0.2782456874847412 Validation Loss: 0.9586032032966614\n",
      "Epoch 3513: Training Loss: 0.2778704762458801 Validation Loss: 0.9577625393867493\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3514: Training Loss: 0.2784618039925893 Validation Loss: 0.9584336876869202\n",
      "Epoch 3515: Training Loss: 0.2774990399678548 Validation Loss: 0.9588804244995117\n",
      "Epoch 3516: Training Loss: 0.27758902311325073 Validation Loss: 0.9595449566841125\n",
      "Epoch 3517: Training Loss: 0.27722208698590595 Validation Loss: 0.9592176079750061\n",
      "Epoch 3518: Training Loss: 0.2775539954503377 Validation Loss: 0.9587134718894958\n",
      "Epoch 3519: Training Loss: 0.2776071031888326 Validation Loss: 0.9590469002723694\n",
      "Epoch 3520: Training Loss: 0.2780974606672923 Validation Loss: 0.959475040435791\n",
      "Epoch 3521: Training Loss: 0.2770100732644399 Validation Loss: 0.9598491191864014\n",
      "Epoch 3522: Training Loss: 0.2773029953241348 Validation Loss: 0.9600483775138855\n",
      "Epoch 3523: Training Loss: 0.2768672506014506 Validation Loss: 0.9602435827255249\n",
      "Epoch 3524: Training Loss: 0.2783852865298589 Validation Loss: 0.9603708982467651\n",
      "Epoch 3525: Training Loss: 0.2766091227531433 Validation Loss: 0.9601564407348633\n",
      "Epoch 3526: Training Loss: 0.2763118048508962 Validation Loss: 0.9601516723632812\n",
      "Epoch 3527: Training Loss: 0.2767959137757619 Validation Loss: 0.9596510529518127\n",
      "Epoch 3528: Training Loss: 0.2761942545572917 Validation Loss: 0.9589060544967651\n",
      "Epoch 3529: Training Loss: 0.2775682657957077 Validation Loss: 0.9586679339408875\n",
      "Epoch 3530: Training Loss: 0.27621719737847644 Validation Loss: 0.9586976766586304\n",
      "Epoch 3531: Training Loss: 0.2771337578694026 Validation Loss: 0.95872563123703\n",
      "Epoch 3532: Training Loss: 0.2761320620775223 Validation Loss: 0.9589260220527649\n",
      "Epoch 3533: Training Loss: 0.27578474084536236 Validation Loss: 0.9591602087020874\n",
      "Epoch 3534: Training Loss: 0.27603880564371747 Validation Loss: 0.9584237933158875\n",
      "Epoch 3535: Training Loss: 0.2764444351196289 Validation Loss: 0.958835780620575\n",
      "Epoch 3536: Training Loss: 0.2751114269097646 Validation Loss: 0.9596184492111206\n",
      "Epoch 3537: Training Loss: 0.27642900745073956 Validation Loss: 0.9593903422355652\n",
      "Epoch 3538: Training Loss: 0.27689548830191296 Validation Loss: 0.9594211578369141\n",
      "Epoch 3539: Training Loss: 0.2753685216108958 Validation Loss: 0.9593605399131775\n",
      "Epoch 3540: Training Loss: 0.2754584352175395 Validation Loss: 0.9588719010353088\n",
      "Epoch 3541: Training Loss: 0.2764156460762024 Validation Loss: 0.9585985541343689\n",
      "Epoch 3542: Training Loss: 0.2747996747493744 Validation Loss: 0.9586314558982849\n",
      "Epoch 3543: Training Loss: 0.274816632270813 Validation Loss: 0.9586111307144165\n",
      "Epoch 3544: Training Loss: 0.27507131298383075 Validation Loss: 0.9597703218460083\n",
      "Epoch 3545: Training Loss: 0.2745922307173411 Validation Loss: 0.9597601294517517\n",
      "Epoch 3546: Training Loss: 0.2748001118501027 Validation Loss: 0.960378110408783\n",
      "Epoch 3547: Training Loss: 0.27461666862169903 Validation Loss: 0.9596840739250183\n",
      "Epoch 3548: Training Loss: 0.2746159831682841 Validation Loss: 0.9594334959983826\n",
      "Epoch 3549: Training Loss: 0.2751534581184387 Validation Loss: 0.9587447643280029\n",
      "Epoch 3550: Training Loss: 0.27540743350982666 Validation Loss: 0.9589775204658508\n",
      "Epoch 3551: Training Loss: 0.27559833725293476 Validation Loss: 0.9589998126029968\n",
      "Epoch 3552: Training Loss: 0.27484289805094403 Validation Loss: 0.9588055610656738\n",
      "Epoch 3553: Training Loss: 0.2744555672009786 Validation Loss: 0.9587670564651489\n",
      "Epoch 3554: Training Loss: 0.27478156487147015 Validation Loss: 0.9590005874633789\n",
      "Epoch 3555: Training Loss: 0.27358244856198627 Validation Loss: 0.9591081738471985\n",
      "Epoch 3556: Training Loss: 0.27396639188130695 Validation Loss: 0.9592477679252625\n",
      "Epoch 3557: Training Loss: 0.2734200358390808 Validation Loss: 0.9586967825889587\n",
      "Epoch 3558: Training Loss: 0.27352939049402875 Validation Loss: 0.9589548707008362\n",
      "Epoch 3559: Training Loss: 0.2744794934988022 Validation Loss: 0.9592047929763794\n",
      "Epoch 3560: Training Loss: 0.2738034129142761 Validation Loss: 0.958652138710022\n",
      "Epoch 3561: Training Loss: 0.272404208779335 Validation Loss: 0.9586824774742126\n",
      "Epoch 3562: Training Loss: 0.2731263339519501 Validation Loss: 0.9588198065757751\n",
      "Epoch 3563: Training Loss: 0.27366532882054645 Validation Loss: 0.9585965275764465\n",
      "Epoch 3564: Training Loss: 0.2727983097235362 Validation Loss: 0.9588865041732788\n",
      "Epoch 3565: Training Loss: 0.27388065059979755 Validation Loss: 0.9589956402778625\n",
      "Epoch 3566: Training Loss: 0.2734239995479584 Validation Loss: 0.9586228132247925\n",
      "Epoch 3567: Training Loss: 0.27483780185381573 Validation Loss: 0.9585717916488647\n",
      "Epoch 3568: Training Loss: 0.2726157506306966 Validation Loss: 0.9587486982345581\n",
      "Epoch 3569: Training Loss: 0.27256930867830914 Validation Loss: 0.9589179754257202\n",
      "Epoch 3570: Training Loss: 0.2724601924419403 Validation Loss: 0.959878146648407\n",
      "Epoch 3571: Training Loss: 0.2725636015335719 Validation Loss: 0.9593754410743713\n",
      "Epoch 3572: Training Loss: 0.27201037108898163 Validation Loss: 0.9593777060508728\n",
      "Epoch 3573: Training Loss: 0.27286232511202496 Validation Loss: 0.9594058990478516\n",
      "Epoch 3574: Training Loss: 0.2749800483385722 Validation Loss: 0.9600887894630432\n",
      "Epoch 3575: Training Loss: 0.27253753940264386 Validation Loss: 0.9602785706520081\n",
      "Epoch 3576: Training Loss: 0.2721174756685893 Validation Loss: 0.9600493311882019\n",
      "Epoch 3577: Training Loss: 0.2717199424902598 Validation Loss: 0.9595471024513245\n",
      "Epoch 3578: Training Loss: 0.272434800863266 Validation Loss: 0.9589107632637024\n",
      "Epoch 3579: Training Loss: 0.2720843056837718 Validation Loss: 0.9581179618835449\n",
      "Epoch 3580: Training Loss: 0.27128611008326214 Validation Loss: 0.9582019448280334\n",
      "Epoch 3581: Training Loss: 0.27220741907755536 Validation Loss: 0.9581512212753296\n",
      "Epoch 3582: Training Loss: 0.27144359548886615 Validation Loss: 0.9591199159622192\n",
      "Epoch 3583: Training Loss: 0.2727964172760646 Validation Loss: 0.9587385058403015\n",
      "Epoch 3584: Training Loss: 0.27098843455314636 Validation Loss: 0.9586951732635498\n",
      "Epoch 3585: Training Loss: 0.2712347110112508 Validation Loss: 0.9590266942977905\n",
      "Epoch 3586: Training Loss: 0.27089067300160724 Validation Loss: 0.959003746509552\n",
      "Epoch 3587: Training Loss: 0.27051975329717 Validation Loss: 0.9589016437530518\n",
      "Epoch 3588: Training Loss: 0.2720711827278137 Validation Loss: 0.9587078094482422\n",
      "Epoch 3589: Training Loss: 0.27146456638971966 Validation Loss: 0.959235429763794\n",
      "Epoch 3590: Training Loss: 0.2708841562271118 Validation Loss: 0.9597883820533752\n",
      "Epoch 3591: Training Loss: 0.2705109417438507 Validation Loss: 0.9598492980003357\n",
      "Epoch 3592: Training Loss: 0.2709384759267171 Validation Loss: 0.9595009088516235\n",
      "Epoch 3593: Training Loss: 0.2701977292696635 Validation Loss: 0.9586049914360046\n",
      "Epoch 3594: Training Loss: 0.27011128266652423 Validation Loss: 0.9580037593841553\n",
      "Epoch 3595: Training Loss: 0.27033380667368573 Validation Loss: 0.9584847092628479\n",
      "Epoch 3596: Training Loss: 0.27071185906728107 Validation Loss: 0.9585748314857483\n",
      "Epoch 3597: Training Loss: 0.27015385031700134 Validation Loss: 0.9585636258125305\n",
      "Epoch 3598: Training Loss: 0.26989325881004333 Validation Loss: 0.9585299491882324\n",
      "Epoch 3599: Training Loss: 0.27061207095781964 Validation Loss: 0.9585899710655212\n",
      "Epoch 3600: Training Loss: 0.26957641045252484 Validation Loss: 0.9592903852462769\n",
      "Epoch 3601: Training Loss: 0.27038846413294476 Validation Loss: 0.9591864347457886\n",
      "Epoch 3602: Training Loss: 0.2693978746732076 Validation Loss: 0.9589559435844421\n",
      "Epoch 3603: Training Loss: 0.27039791146914166 Validation Loss: 0.9588565230369568\n",
      "Epoch 3604: Training Loss: 0.2690374553203583 Validation Loss: 0.9592725038528442\n",
      "Epoch 3605: Training Loss: 0.26924747228622437 Validation Loss: 0.9590536952018738\n",
      "Epoch 3606: Training Loss: 0.2691207528114319 Validation Loss: 0.9589412212371826\n",
      "Epoch 3607: Training Loss: 0.2686018645763397 Validation Loss: 0.9590442180633545\n",
      "Epoch 3608: Training Loss: 0.26963604986667633 Validation Loss: 0.9590093493461609\n",
      "Epoch 3609: Training Loss: 0.2702616552511851 Validation Loss: 0.9584317803382874\n",
      "Epoch 3610: Training Loss: 0.2687987983226776 Validation Loss: 0.9588345289230347\n",
      "Epoch 3611: Training Loss: 0.26910288135210675 Validation Loss: 0.9590440392494202\n",
      "Epoch 3612: Training Loss: 0.2690812398989995 Validation Loss: 0.9590736031532288\n",
      "Epoch 3613: Training Loss: 0.2690071811278661 Validation Loss: 0.9587976336479187\n",
      "Epoch 3614: Training Loss: 0.26814723014831543 Validation Loss: 0.9591413736343384\n",
      "Epoch 3615: Training Loss: 0.2685251583655675 Validation Loss: 0.958904504776001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3616: Training Loss: 0.2683033347129822 Validation Loss: 0.9581556916236877\n",
      "Epoch 3617: Training Loss: 0.26772377888361615 Validation Loss: 0.9580151438713074\n",
      "Epoch 3618: Training Loss: 0.2681419203678767 Validation Loss: 0.9585719108581543\n",
      "Epoch 3619: Training Loss: 0.26863042016824085 Validation Loss: 0.9592827558517456\n",
      "Epoch 3620: Training Loss: 0.26759399473667145 Validation Loss: 0.9594702124595642\n",
      "Epoch 3621: Training Loss: 0.26784806450208026 Validation Loss: 0.959433376789093\n",
      "Epoch 3622: Training Loss: 0.2683362166086833 Validation Loss: 0.9591366052627563\n",
      "Epoch 3623: Training Loss: 0.2676678200562795 Validation Loss: 0.9594842195510864\n",
      "Epoch 3624: Training Loss: 0.26751116414864856 Validation Loss: 0.9593924880027771\n",
      "Epoch 3625: Training Loss: 0.2677765488624573 Validation Loss: 0.9591667056083679\n",
      "Epoch 3626: Training Loss: 0.2677153100570043 Validation Loss: 0.9590719938278198\n",
      "Epoch 3627: Training Loss: 0.2674318154652913 Validation Loss: 0.9589031934738159\n",
      "Epoch 3628: Training Loss: 0.2675031820933024 Validation Loss: 0.9589385390281677\n",
      "Epoch 3629: Training Loss: 0.26767292122046155 Validation Loss: 0.9588749408721924\n",
      "Epoch 3630: Training Loss: 0.2672966818014781 Validation Loss: 0.9593797922134399\n",
      "Epoch 3631: Training Loss: 0.2672897030909856 Validation Loss: 0.9585767984390259\n",
      "Epoch 3632: Training Loss: 0.2674598793188731 Validation Loss: 0.9585939049720764\n",
      "Epoch 3633: Training Loss: 0.26825017233689624 Validation Loss: 0.9581213593482971\n",
      "Epoch 3634: Training Loss: 0.2691646267970403 Validation Loss: 0.9589672088623047\n",
      "Epoch 3635: Training Loss: 0.2670833468437195 Validation Loss: 0.9588475227355957\n",
      "Epoch 3636: Training Loss: 0.2664338946342468 Validation Loss: 0.9596756100654602\n",
      "Epoch 3637: Training Loss: 0.267269566655159 Validation Loss: 0.9594948291778564\n",
      "Epoch 3638: Training Loss: 0.26695170998573303 Validation Loss: 0.9587118029594421\n",
      "Epoch 3639: Training Loss: 0.2679017235835393 Validation Loss: 0.9582992196083069\n",
      "Epoch 3640: Training Loss: 0.26588815947373706 Validation Loss: 0.9582014679908752\n",
      "Epoch 3641: Training Loss: 0.266038715839386 Validation Loss: 0.9584280848503113\n",
      "Epoch 3642: Training Loss: 0.26705120007197064 Validation Loss: 0.9582546949386597\n",
      "Epoch 3643: Training Loss: 0.2663794656594594 Validation Loss: 0.9589190483093262\n",
      "Epoch 3644: Training Loss: 0.26584088802337646 Validation Loss: 0.9592107534408569\n",
      "Epoch 3645: Training Loss: 0.2664954463640849 Validation Loss: 0.9590294361114502\n",
      "Epoch 3646: Training Loss: 0.2663513918717702 Validation Loss: 0.9588735103607178\n",
      "Epoch 3647: Training Loss: 0.2663046022256215 Validation Loss: 0.9583027958869934\n",
      "Epoch 3648: Training Loss: 0.26596414546171826 Validation Loss: 0.9584938883781433\n",
      "Epoch 3649: Training Loss: 0.26566025118033093 Validation Loss: 0.9580429196357727\n",
      "Epoch 3650: Training Loss: 0.26545385519663495 Validation Loss: 0.958254337310791\n",
      "Epoch 3651: Training Loss: 0.26525357365608215 Validation Loss: 0.9587679505348206\n",
      "Epoch 3652: Training Loss: 0.2655719021956126 Validation Loss: 0.9585628509521484\n",
      "Epoch 3653: Training Loss: 0.26550352573394775 Validation Loss: 0.9586390256881714\n",
      "Epoch 3654: Training Loss: 0.26517338554064435 Validation Loss: 0.9591017365455627\n",
      "Epoch 3655: Training Loss: 0.26532162229220074 Validation Loss: 0.9593537449836731\n",
      "Epoch 3656: Training Loss: 0.2650957852602005 Validation Loss: 0.9594570994377136\n",
      "Epoch 3657: Training Loss: 0.2644951244195302 Validation Loss: 0.9593823552131653\n",
      "Epoch 3658: Training Loss: 0.2639606793721517 Validation Loss: 0.9590919613838196\n",
      "Epoch 3659: Training Loss: 0.2645021726687749 Validation Loss: 0.9591010212898254\n",
      "Epoch 3660: Training Loss: 0.26392360031604767 Validation Loss: 0.9587902426719666\n",
      "Epoch 3661: Training Loss: 0.26486629247665405 Validation Loss: 0.9587442278862\n",
      "Epoch 3662: Training Loss: 0.2647786537806193 Validation Loss: 0.9580557346343994\n",
      "Epoch 3663: Training Loss: 0.2644022156794866 Validation Loss: 0.9578924775123596\n",
      "Epoch 3664: Training Loss: 0.2647188901901245 Validation Loss: 0.9579301476478577\n",
      "Epoch 3665: Training Loss: 0.2646170953909556 Validation Loss: 0.9591031074523926\n",
      "Epoch 3666: Training Loss: 0.26491815348466236 Validation Loss: 0.959412693977356\n",
      "Epoch 3667: Training Loss: 0.26469481984774273 Validation Loss: 0.9598236680030823\n",
      "Epoch 3668: Training Loss: 0.2640303572018941 Validation Loss: 0.9595412611961365\n",
      "Epoch 3669: Training Loss: 0.26430682341257733 Validation Loss: 0.9588244557380676\n",
      "Epoch 3670: Training Loss: 0.2636861900488536 Validation Loss: 0.9583537578582764\n",
      "Epoch 3671: Training Loss: 0.2632354497909546 Validation Loss: 0.9592514038085938\n",
      "Epoch 3672: Training Loss: 0.2635248601436615 Validation Loss: 0.9585354328155518\n",
      "Epoch 3673: Training Loss: 0.26376281181971234 Validation Loss: 0.9578262567520142\n",
      "Epoch 3674: Training Loss: 0.2632611195246379 Validation Loss: 0.958111584186554\n",
      "Epoch 3675: Training Loss: 0.26372671624024707 Validation Loss: 0.9579696655273438\n",
      "Epoch 3676: Training Loss: 0.26365797718365985 Validation Loss: 0.9582447409629822\n",
      "Epoch 3677: Training Loss: 0.26550152401129407 Validation Loss: 0.9590451717376709\n",
      "Epoch 3678: Training Loss: 0.2637961010138194 Validation Loss: 0.9587935209274292\n",
      "Epoch 3679: Training Loss: 0.26336217919985455 Validation Loss: 0.9593647122383118\n",
      "Epoch 3680: Training Loss: 0.26362064480781555 Validation Loss: 0.9591012597084045\n",
      "Epoch 3681: Training Loss: 0.2615649650494258 Validation Loss: 0.9593673944473267\n",
      "Epoch 3682: Training Loss: 0.262770156065623 Validation Loss: 0.9588459730148315\n",
      "Epoch 3683: Training Loss: 0.26227350036303204 Validation Loss: 0.958501398563385\n",
      "Epoch 3684: Training Loss: 0.26291751861572266 Validation Loss: 0.9591086506843567\n",
      "Epoch 3685: Training Loss: 0.2626672287782033 Validation Loss: 0.9599388241767883\n",
      "Epoch 3686: Training Loss: 0.2623576720555623 Validation Loss: 0.9599444270133972\n",
      "Epoch 3687: Training Loss: 0.2621333599090576 Validation Loss: 0.959213137626648\n",
      "Epoch 3688: Training Loss: 0.26286694904168445 Validation Loss: 0.9583960175514221\n",
      "Epoch 3689: Training Loss: 0.26196502645810443 Validation Loss: 0.957781970500946\n",
      "Epoch 3690: Training Loss: 0.2621799310048421 Validation Loss: 0.9578235149383545\n",
      "Epoch 3691: Training Loss: 0.263187696536382 Validation Loss: 0.9586907029151917\n",
      "Epoch 3692: Training Loss: 0.26238615810871124 Validation Loss: 0.9590646028518677\n",
      "Epoch 3693: Training Loss: 0.26299728949864704 Validation Loss: 0.9592658877372742\n",
      "Epoch 3694: Training Loss: 0.2611793130636215 Validation Loss: 0.9593483805656433\n",
      "Epoch 3695: Training Loss: 0.26362089316050213 Validation Loss: 0.9587603211402893\n",
      "Epoch 3696: Training Loss: 0.261613463362058 Validation Loss: 0.958536684513092\n",
      "Epoch 3697: Training Loss: 0.26228173573811847 Validation Loss: 0.958914577960968\n",
      "Epoch 3698: Training Loss: 0.26116377115249634 Validation Loss: 0.9590862393379211\n",
      "Epoch 3699: Training Loss: 0.26222800215085346 Validation Loss: 0.959555447101593\n",
      "Epoch 3700: Training Loss: 0.2611708790063858 Validation Loss: 0.9594962000846863\n",
      "Epoch 3701: Training Loss: 0.2606002241373062 Validation Loss: 0.9595085382461548\n",
      "Epoch 3702: Training Loss: 0.26097654302914935 Validation Loss: 0.9593486189842224\n",
      "Epoch 3703: Training Loss: 0.26066816846529645 Validation Loss: 0.9595361351966858\n",
      "Epoch 3704: Training Loss: 0.2617311278978984 Validation Loss: 0.9594585299491882\n",
      "Epoch 3705: Training Loss: 0.26186850170294446 Validation Loss: 0.958723247051239\n",
      "Epoch 3706: Training Loss: 0.2612529496351878 Validation Loss: 0.9579501748085022\n",
      "Epoch 3707: Training Loss: 0.26066914200782776 Validation Loss: 0.957603931427002\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3708: Training Loss: 0.26036402583122253 Validation Loss: 0.9577287435531616\n",
      "Epoch 3709: Training Loss: 0.2610651950041453 Validation Loss: 0.9575815796852112\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3710: Training Loss: 0.2615724503993988 Validation Loss: 0.9582800269126892\n",
      "Epoch 3711: Training Loss: 0.26037423809369403 Validation Loss: 0.9590426087379456\n",
      "Epoch 3712: Training Loss: 0.2601836423079173 Validation Loss: 0.9587501883506775\n",
      "Epoch 3713: Training Loss: 0.2600255161523819 Validation Loss: 0.9590938091278076\n",
      "Epoch 3714: Training Loss: 0.25997405250867206 Validation Loss: 0.959650993347168\n",
      "Epoch 3715: Training Loss: 0.25989867250124615 Validation Loss: 0.9594944715499878\n",
      "Epoch 3716: Training Loss: 0.2594512403011322 Validation Loss: 0.9597831964492798\n",
      "Epoch 3717: Training Loss: 0.26012231409549713 Validation Loss: 0.9593144655227661\n",
      "Epoch 3718: Training Loss: 0.26018599172433216 Validation Loss: 0.9581473469734192\n",
      "Epoch 3719: Training Loss: 0.25971506039301556 Validation Loss: 0.9581993818283081\n",
      "Epoch 3720: Training Loss: 0.25961435834566754 Validation Loss: 0.9583824872970581\n",
      "Epoch 3721: Training Loss: 0.25954849521319073 Validation Loss: 0.958587646484375\n",
      "Epoch 3722: Training Loss: 0.25920868913332623 Validation Loss: 0.9587352871894836\n",
      "Epoch 3723: Training Loss: 0.2601213951905568 Validation Loss: 0.9588050246238708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3724: Training Loss: 0.2591678301493327 Validation Loss: 0.9586840867996216\n",
      "Epoch 3725: Training Loss: 0.25956552227338153 Validation Loss: 0.9587939977645874\n",
      "Epoch 3726: Training Loss: 0.2590225785970688 Validation Loss: 0.959026575088501\n",
      "Epoch 3727: Training Loss: 0.2584397147099177 Validation Loss: 0.9590864181518555\n",
      "Epoch 3728: Training Loss: 0.2593718369801839 Validation Loss: 0.958869218826294\n",
      "Epoch 3729: Training Loss: 0.25889116525650024 Validation Loss: 0.9590527415275574\n",
      "Epoch 3730: Training Loss: 0.25870996713638306 Validation Loss: 0.9590257406234741\n",
      "Epoch 3731: Training Loss: 0.2583469698826472 Validation Loss: 0.9592363834381104\n",
      "Epoch 3732: Training Loss: 0.2586643050114314 Validation Loss: 0.9588665962219238\n",
      "Epoch 3733: Training Loss: 0.25825562079747516 Validation Loss: 0.9584527611732483\n",
      "Epoch 3734: Training Loss: 0.25789904594421387 Validation Loss: 0.9584212899208069\n",
      "Epoch 3735: Training Loss: 0.2598838458458583 Validation Loss: 0.9580076336860657\n",
      "Epoch 3736: Training Loss: 0.25875135759512585 Validation Loss: 0.9579609632492065\n",
      "Epoch 3737: Training Loss: 0.2590119043986003 Validation Loss: 0.9580095410346985\n",
      "Epoch 3738: Training Loss: 0.2580032100280126 Validation Loss: 0.958070695400238\n",
      "Epoch 3739: Training Loss: 0.2577456732590993 Validation Loss: 0.9584200382232666\n",
      "Epoch 3740: Training Loss: 0.25760746995608014 Validation Loss: 0.9586760401725769\n",
      "Epoch 3741: Training Loss: 0.2578992694616318 Validation Loss: 0.9586045145988464\n",
      "Epoch 3742: Training Loss: 0.2575074185927709 Validation Loss: 0.9586056470870972\n",
      "Epoch 3743: Training Loss: 0.25868692994117737 Validation Loss: 0.9591262936592102\n",
      "Epoch 3744: Training Loss: 0.2576878269513448 Validation Loss: 0.9583399891853333\n",
      "Epoch 3745: Training Loss: 0.25867776075998944 Validation Loss: 0.9582527279853821\n",
      "Epoch 3746: Training Loss: 0.25709327061971027 Validation Loss: 0.9584121704101562\n",
      "Epoch 3747: Training Loss: 0.25701050957043964 Validation Loss: 0.9588196277618408\n",
      "Epoch 3748: Training Loss: 0.25819778939088184 Validation Loss: 0.958845317363739\n",
      "Epoch 3749: Training Loss: 0.25700926780700684 Validation Loss: 0.9589025974273682\n",
      "Epoch 3750: Training Loss: 0.2568828413883845 Validation Loss: 0.9590847492218018\n",
      "Epoch 3751: Training Loss: 0.25687605142593384 Validation Loss: 0.9598280787467957\n",
      "Epoch 3752: Training Loss: 0.2570906380812327 Validation Loss: 0.9599606394767761\n",
      "Epoch 3753: Training Loss: 0.2569097379843394 Validation Loss: 0.9597553014755249\n",
      "Epoch 3754: Training Loss: 0.2568534016609192 Validation Loss: 0.9592043161392212\n",
      "Epoch 3755: Training Loss: 0.2564188341299693 Validation Loss: 0.9585321545600891\n",
      "Epoch 3756: Training Loss: 0.2585354745388031 Validation Loss: 0.9577856063842773\n",
      "Epoch 3757: Training Loss: 0.2566064993540446 Validation Loss: 0.9578068256378174\n",
      "Epoch 3758: Training Loss: 0.25661883254845935 Validation Loss: 0.9572514891624451\n",
      "New Checkpoint Saved into PATH\n",
      "Epoch 3759: Training Loss: 0.2561707595984141 Validation Loss: 0.9579511880874634\n",
      "Epoch 3760: Training Loss: 0.25586920976638794 Validation Loss: 0.9581815600395203\n",
      "Epoch 3761: Training Loss: 0.25568952163060504 Validation Loss: 0.9588351845741272\n",
      "Epoch 3762: Training Loss: 0.25661228597164154 Validation Loss: 0.9593626260757446\n",
      "Epoch 3763: Training Loss: 0.25648149847984314 Validation Loss: 0.9597699046134949\n",
      "Epoch 3764: Training Loss: 0.25604183475176495 Validation Loss: 0.9596222639083862\n",
      "Epoch 3765: Training Loss: 0.2546534985303879 Validation Loss: 0.9598318338394165\n",
      "Epoch 3766: Training Loss: 0.25588828325271606 Validation Loss: 0.9594553709030151\n",
      "Epoch 3767: Training Loss: 0.256669282913208 Validation Loss: 0.959054172039032\n",
      "Epoch 3768: Training Loss: 0.25548545519510907 Validation Loss: 0.9583024382591248\n",
      "Epoch 3769: Training Loss: 0.2557610472043355 Validation Loss: 0.9584914445877075\n",
      "Epoch 3770: Training Loss: 0.2554051826397578 Validation Loss: 0.958068311214447\n",
      "Epoch 3771: Training Loss: 0.25495645403862 Validation Loss: 0.9585819244384766\n",
      "Epoch 3772: Training Loss: 0.255676989754041 Validation Loss: 0.9580543637275696\n",
      "Epoch 3773: Training Loss: 0.25516020754973096 Validation Loss: 0.9578573107719421\n",
      "Epoch 3774: Training Loss: 0.25501860678195953 Validation Loss: 0.9586461186408997\n",
      "Epoch 3775: Training Loss: 0.2549679676691691 Validation Loss: 0.9587916731834412\n",
      "Epoch 3776: Training Loss: 0.25508210559686023 Validation Loss: 0.9592549800872803\n",
      "Epoch 3777: Training Loss: 0.255496879418691 Validation Loss: 0.9588600397109985\n",
      "Epoch 3778: Training Loss: 0.25482680400212604 Validation Loss: 0.9583501219749451\n",
      "Epoch 3779: Training Loss: 0.25566085676352185 Validation Loss: 0.9586378931999207\n",
      "Epoch 3780: Training Loss: 0.2548503627379735 Validation Loss: 0.958402693271637\n",
      "Epoch 3781: Training Loss: 0.2546929369370143 Validation Loss: 0.9591729640960693\n",
      "Epoch 3782: Training Loss: 0.2549632837375005 Validation Loss: 0.9593698382377625\n",
      "Epoch 3783: Training Loss: 0.2546245604753494 Validation Loss: 0.958698034286499\n",
      "Epoch 3784: Training Loss: 0.25483350952466327 Validation Loss: 0.9580729603767395\n",
      "Epoch 3785: Training Loss: 0.25444206098715466 Validation Loss: 0.9581810832023621\n",
      "Epoch 3786: Training Loss: 0.254045565923055 Validation Loss: 0.9580427408218384\n",
      "Epoch 3787: Training Loss: 0.2544766416152318 Validation Loss: 0.9574606418609619\n",
      "Epoch 3788: Training Loss: 0.254188597202301 Validation Loss: 0.9585492014884949\n",
      "Epoch 3789: Training Loss: 0.25462817152341205 Validation Loss: 0.9596149325370789\n",
      "Epoch 3790: Training Loss: 0.2534934977690379 Validation Loss: 0.9593743085861206\n",
      "Epoch 3791: Training Loss: 0.25430236756801605 Validation Loss: 0.9600648880004883\n",
      "Epoch 3792: Training Loss: 0.2536996155977249 Validation Loss: 0.9597780704498291\n",
      "Epoch 3793: Training Loss: 0.25303640961647034 Validation Loss: 0.9594500660896301\n",
      "Epoch 3794: Training Loss: 0.25484425326188404 Validation Loss: 0.9586369395256042\n",
      "Epoch 3795: Training Loss: 0.2532717039187749 Validation Loss: 0.9584189653396606\n",
      "Epoch 3796: Training Loss: 0.25265923142433167 Validation Loss: 0.9577917456626892\n",
      "Epoch 3797: Training Loss: 0.25288158158461255 Validation Loss: 0.9585811495780945\n",
      "Epoch 3798: Training Loss: 0.25349076588948566 Validation Loss: 0.9585096836090088\n",
      "Epoch 3799: Training Loss: 0.2537619223197301 Validation Loss: 0.958219587802887\n",
      "Epoch 3800: Training Loss: 0.25307142237822217 Validation Loss: 0.9583958387374878\n",
      "Epoch 3801: Training Loss: 0.2526826560497284 Validation Loss: 0.9584152102470398\n",
      "Epoch 3802: Training Loss: 0.25319303572177887 Validation Loss: 0.9590044617652893\n",
      "Epoch 3803: Training Loss: 0.25302618245283764 Validation Loss: 0.9587604403495789\n",
      "Epoch 3804: Training Loss: 0.25271621346473694 Validation Loss: 0.9590053558349609\n",
      "Epoch 3805: Training Loss: 0.25345396002133685 Validation Loss: 0.9585016965866089\n",
      "Epoch 3806: Training Loss: 0.2526344458262126 Validation Loss: 0.9585270881652832\n",
      "Epoch 3807: Training Loss: 0.25259362657864887 Validation Loss: 0.958092451095581\n",
      "Epoch 3808: Training Loss: 0.2530951003233592 Validation Loss: 0.9581491351127625\n",
      "Epoch 3809: Training Loss: 0.25247249007225037 Validation Loss: 0.958755612373352\n",
      "Epoch 3810: Training Loss: 0.2520472506682078 Validation Loss: 0.9586517214775085\n",
      "Epoch 3811: Training Loss: 0.253519207239151 Validation Loss: 0.9592130780220032\n",
      "Epoch 3812: Training Loss: 0.2527873367071152 Validation Loss: 0.9598162770271301\n",
      "Epoch 3813: Training Loss: 0.2527276227871577 Validation Loss: 0.95981764793396\n",
      "Epoch 3814: Training Loss: 0.2521404027938843 Validation Loss: 0.9589899778366089\n",
      "Epoch 3815: Training Loss: 0.252124826113383 Validation Loss: 0.9582832455635071\n",
      "Epoch 3816: Training Loss: 0.25193289419015247 Validation Loss: 0.9581081867218018\n",
      "Epoch 3817: Training Loss: 0.2517993201812108 Validation Loss: 0.958484411239624\n",
      "Epoch 3818: Training Loss: 0.25213288764158887 Validation Loss: 0.9585607051849365\n",
      "Epoch 3819: Training Loss: 0.2512907385826111 Validation Loss: 0.9586885571479797\n",
      "Epoch 3820: Training Loss: 0.2513413280248642 Validation Loss: 0.9584970474243164\n",
      "Epoch 3821: Training Loss: 0.2511199067036311 Validation Loss: 0.9592288732528687\n",
      "Epoch 3822: Training Loss: 0.25117892523606616 Validation Loss: 0.9591495990753174\n",
      "Epoch 3823: Training Loss: 0.2522389888763428 Validation Loss: 0.9591836333274841\n",
      "Epoch 3824: Training Loss: 0.2511008381843567 Validation Loss: 0.9591930508613586\n",
      "Epoch 3825: Training Loss: 0.2506767561038335 Validation Loss: 0.9588809013366699\n",
      "Epoch 3826: Training Loss: 0.25102560222148895 Validation Loss: 0.9586681723594666\n",
      "Epoch 3827: Training Loss: 0.25098317364851636 Validation Loss: 0.9586478471755981\n",
      "Epoch 3828: Training Loss: 0.2508243570725123 Validation Loss: 0.9586125612258911\n",
      "Epoch 3829: Training Loss: 0.25127435723940533 Validation Loss: 0.9585340619087219\n",
      "Epoch 3830: Training Loss: 0.25056687494119007 Validation Loss: 0.9581527709960938\n",
      "Epoch 3831: Training Loss: 0.25060103336970013 Validation Loss: 0.957859456539154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3832: Training Loss: 0.2511868526538213 Validation Loss: 0.9578637480735779\n",
      "Epoch 3833: Training Loss: 0.251376748085022 Validation Loss: 0.9583061933517456\n",
      "Epoch 3834: Training Loss: 0.2502579639355342 Validation Loss: 0.9585999846458435\n",
      "Epoch 3835: Training Loss: 0.2515242099761963 Validation Loss: 0.9589651226997375\n",
      "Epoch 3836: Training Loss: 0.25033343334992725 Validation Loss: 0.9590253829956055\n",
      "Epoch 3837: Training Loss: 0.25003423790136975 Validation Loss: 0.9590674638748169\n",
      "Epoch 3838: Training Loss: 0.24950173497200012 Validation Loss: 0.9588850736618042\n",
      "Epoch 3839: Training Loss: 0.24982571601867676 Validation Loss: 0.958961546421051\n",
      "Epoch 3840: Training Loss: 0.24989016354084015 Validation Loss: 0.9592017531394958\n",
      "Epoch 3841: Training Loss: 0.2498522698879242 Validation Loss: 0.9591317772865295\n",
      "Epoch 3842: Training Loss: 0.2503657291332881 Validation Loss: 0.958824634552002\n",
      "Epoch 3843: Training Loss: 0.24932233492533365 Validation Loss: 0.9590848088264465\n",
      "Epoch 3844: Training Loss: 0.24913667142391205 Validation Loss: 0.9587782621383667\n",
      "Epoch 3845: Training Loss: 0.24966150522232056 Validation Loss: 0.9586867690086365\n",
      "Epoch 3846: Training Loss: 0.2494389017422994 Validation Loss: 0.9594219923019409\n",
      "Epoch 3847: Training Loss: 0.24937990804513296 Validation Loss: 0.9601753950119019\n",
      "Epoch 3848: Training Loss: 0.24935325980186462 Validation Loss: 0.9598348140716553\n",
      "Epoch 3849: Training Loss: 0.24969907104969025 Validation Loss: 0.9591680765151978\n",
      "Epoch 3850: Training Loss: 0.249246617158254 Validation Loss: 0.957770049571991\n",
      "Epoch 3851: Training Loss: 0.24926264584064484 Validation Loss: 0.9576263427734375\n",
      "Epoch 3852: Training Loss: 0.25029892722765607 Validation Loss: 0.9584820866584778\n",
      "Epoch 3853: Training Loss: 0.24880665043989816 Validation Loss: 0.958956778049469\n",
      "Epoch 3854: Training Loss: 0.24966733157634735 Validation Loss: 0.9586576223373413\n",
      "Epoch 3855: Training Loss: 0.2508612225453059 Validation Loss: 0.9588974118232727\n",
      "Epoch 3856: Training Loss: 0.24862192074457803 Validation Loss: 0.9587429761886597\n",
      "Epoch 3857: Training Loss: 0.24854314823945364 Validation Loss: 0.9590848088264465\n",
      "Epoch 3858: Training Loss: 0.2493570993343989 Validation Loss: 0.9594286680221558\n",
      "Epoch 3859: Training Loss: 0.2475797732671102 Validation Loss: 0.9595134258270264\n",
      "Epoch 3860: Training Loss: 0.248274564743042 Validation Loss: 0.9595241546630859\n",
      "Epoch 3861: Training Loss: 0.24854015310605368 Validation Loss: 0.9592878818511963\n",
      "Epoch 3862: Training Loss: 0.24870421985785165 Validation Loss: 0.958952009677887\n",
      "Epoch 3863: Training Loss: 0.2489811380704244 Validation Loss: 0.9584574699401855\n",
      "Epoch 3864: Training Loss: 0.24765393137931824 Validation Loss: 0.9585066437721252\n",
      "Epoch 3865: Training Loss: 0.248627041776975 Validation Loss: 0.9587972164154053\n",
      "Epoch 3866: Training Loss: 0.24866928656895956 Validation Loss: 0.9585583209991455\n",
      "Epoch 3867: Training Loss: 0.2477124184370041 Validation Loss: 0.9587106704711914\n",
      "Epoch 3868: Training Loss: 0.24867803851763406 Validation Loss: 0.9584192037582397\n",
      "Epoch 3869: Training Loss: 0.2486264556646347 Validation Loss: 0.9589771628379822\n",
      "Epoch 3870: Training Loss: 0.247170219818751 Validation Loss: 0.9585868716239929\n",
      "Epoch 3871: Training Loss: 0.24749397238095602 Validation Loss: 0.9592007994651794\n",
      "Epoch 3872: Training Loss: 0.2474368413289388 Validation Loss: 0.9589096307754517\n",
      "Epoch 3873: Training Loss: 0.247946431239446 Validation Loss: 0.9591014981269836\n",
      "Epoch 3874: Training Loss: 0.24796468516190848 Validation Loss: 0.9596132040023804\n",
      "Epoch 3875: Training Loss: 0.2472864786783854 Validation Loss: 0.9589051008224487\n",
      "Epoch 3876: Training Loss: 0.2470526397228241 Validation Loss: 0.9592191576957703\n",
      "Epoch 3877: Training Loss: 0.24670558671156564 Validation Loss: 0.9593319892883301\n",
      "Epoch 3878: Training Loss: 0.24662555754184723 Validation Loss: 0.9589113593101501\n",
      "Epoch 3879: Training Loss: 0.24650553862253824 Validation Loss: 0.9594143033027649\n",
      "Epoch 3880: Training Loss: 0.2469982107480367 Validation Loss: 0.9592373371124268\n",
      "Epoch 3881: Training Loss: 0.24677656590938568 Validation Loss: 0.9593632221221924\n",
      "Epoch 3882: Training Loss: 0.2473605473836263 Validation Loss: 0.9595506191253662\n",
      "Epoch 3883: Training Loss: 0.24674351513385773 Validation Loss: 0.9592337608337402\n",
      "Epoch 3884: Training Loss: 0.24695314963658652 Validation Loss: 0.9597758054733276\n",
      "Epoch 3885: Training Loss: 0.24627374609311423 Validation Loss: 0.9595114588737488\n",
      "Epoch 3886: Training Loss: 0.246519202987353 Validation Loss: 0.9592828750610352\n",
      "Epoch 3887: Training Loss: 0.2468843162059784 Validation Loss: 0.9591293931007385\n",
      "Epoch 3888: Training Loss: 0.24575472374757132 Validation Loss: 0.9582577347755432\n",
      "Epoch 3889: Training Loss: 0.2473963350057602 Validation Loss: 0.9574881196022034\n",
      "Epoch 3890: Training Loss: 0.24701985716819763 Validation Loss: 0.9576940536499023\n",
      "Epoch 3891: Training Loss: 0.24649721384048462 Validation Loss: 0.9579917788505554\n",
      "Epoch 3892: Training Loss: 0.24573110044002533 Validation Loss: 0.9586354494094849\n",
      "Epoch 3893: Training Loss: 0.2464506278435389 Validation Loss: 0.959455668926239\n",
      "Epoch 3894: Training Loss: 0.2467035303513209 Validation Loss: 0.9600285887718201\n",
      "Epoch 3895: Training Loss: 0.24713229636351267 Validation Loss: 0.959552526473999\n",
      "Epoch 3896: Training Loss: 0.24760915835698447 Validation Loss: 0.9597578048706055\n",
      "Epoch 3897: Training Loss: 0.2465908924738566 Validation Loss: 0.9602033495903015\n",
      "Epoch 3898: Training Loss: 0.24568417171637216 Validation Loss: 0.9601117372512817\n",
      "Epoch 3899: Training Loss: 0.2455095797777176 Validation Loss: 0.9591466784477234\n",
      "Epoch 3900: Training Loss: 0.24460415045420328 Validation Loss: 0.9591231346130371\n",
      "Epoch 3901: Training Loss: 0.2448537548383077 Validation Loss: 0.958767831325531\n",
      "Epoch 3902: Training Loss: 0.24551066756248474 Validation Loss: 0.9588977098464966\n",
      "Epoch 3903: Training Loss: 0.2451807459195455 Validation Loss: 0.958806037902832\n",
      "Epoch 3904: Training Loss: 0.24502278864383698 Validation Loss: 0.9592244625091553\n",
      "Epoch 3905: Training Loss: 0.24668156603972116 Validation Loss: 0.9594082832336426\n",
      "Epoch 3906: Training Loss: 0.24464468161265054 Validation Loss: 0.9589295387268066\n",
      "Epoch 3907: Training Loss: 0.24587832391262054 Validation Loss: 0.958989679813385\n",
      "Epoch 3908: Training Loss: 0.24549363553524017 Validation Loss: 0.9585461616516113\n",
      "Epoch 3909: Training Loss: 0.24471336603164673 Validation Loss: 0.9583641886711121\n",
      "Epoch 3910: Training Loss: 0.24558566510677338 Validation Loss: 0.9591652750968933\n",
      "Epoch 3911: Training Loss: 0.24429739514986673 Validation Loss: 0.9593923091888428\n",
      "Epoch 3912: Training Loss: 0.2445656011501948 Validation Loss: 0.9591014981269836\n",
      "Epoch 3913: Training Loss: 0.2444603443145752 Validation Loss: 0.9585246443748474\n",
      "Epoch 3914: Training Loss: 0.24482440451780954 Validation Loss: 0.9586465954780579\n",
      "Epoch 3915: Training Loss: 0.2447355886300405 Validation Loss: 0.9589948058128357\n",
      "Epoch 3916: Training Loss: 0.2443662037452062 Validation Loss: 0.9588111639022827\n",
      "Epoch 3917: Training Loss: 0.24397310117880502 Validation Loss: 0.9586412310600281\n",
      "Epoch 3918: Training Loss: 0.2437323033809662 Validation Loss: 0.9584944844245911\n",
      "Epoch 3919: Training Loss: 0.24477588633696237 Validation Loss: 0.9589260220527649\n",
      "Epoch 3920: Training Loss: 0.24480647842089334 Validation Loss: 0.9586830735206604\n",
      "Epoch 3921: Training Loss: 0.24493201076984406 Validation Loss: 0.95884108543396\n",
      "Epoch 3922: Training Loss: 0.2438249240318934 Validation Loss: 0.9592960476875305\n",
      "Epoch 3923: Training Loss: 0.24375768999258676 Validation Loss: 0.9605795741081238\n",
      "Epoch 3924: Training Loss: 0.2436779886484146 Validation Loss: 0.9605828523635864\n",
      "Epoch 3925: Training Loss: 0.2438836693763733 Validation Loss: 0.9597208499908447\n",
      "Epoch 3926: Training Loss: 0.24357103804747263 Validation Loss: 0.9592358469963074\n",
      "Epoch 3927: Training Loss: 0.2439101388057073 Validation Loss: 0.959216833114624\n",
      "Epoch 3928: Training Loss: 0.24454961717128754 Validation Loss: 0.9591100811958313\n",
      "Epoch 3929: Training Loss: 0.2428451975186666 Validation Loss: 0.9588116407394409\n",
      "Epoch 3930: Training Loss: 0.2429941644271215 Validation Loss: 0.9590413570404053\n",
      "Epoch 3931: Training Loss: 0.24415511389573416 Validation Loss: 0.9588364958763123\n",
      "Epoch 3932: Training Loss: 0.24372974038124084 Validation Loss: 0.9587525725364685\n",
      "Epoch 3933: Training Loss: 0.24261416991551718 Validation Loss: 0.9585645794868469\n",
      "Epoch 3934: Training Loss: 0.24402470390001932 Validation Loss: 0.9582277536392212\n",
      "Epoch 3935: Training Loss: 0.24292062719662985 Validation Loss: 0.958243727684021\n",
      "Epoch 3936: Training Loss: 0.2426860680182775 Validation Loss: 0.959006667137146\n",
      "Epoch 3937: Training Loss: 0.2430228590965271 Validation Loss: 0.9596928954124451\n",
      "Epoch 3938: Training Loss: 0.24279649058977762 Validation Loss: 0.9596468210220337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3939: Training Loss: 0.24225756029287973 Validation Loss: 0.9592369794845581\n",
      "Epoch 3940: Training Loss: 0.24215133488178253 Validation Loss: 0.9586136341094971\n",
      "Epoch 3941: Training Loss: 0.24353380501270294 Validation Loss: 0.9597164988517761\n",
      "Epoch 3942: Training Loss: 0.24293779333432516 Validation Loss: 0.959947407245636\n",
      "Epoch 3943: Training Loss: 0.24328542749087015 Validation Loss: 0.9597386121749878\n",
      "Epoch 3944: Training Loss: 0.24311025937398276 Validation Loss: 0.9597535133361816\n",
      "Epoch 3945: Training Loss: 0.2420036643743515 Validation Loss: 0.959801435470581\n",
      "Epoch 3946: Training Loss: 0.2420317679643631 Validation Loss: 0.9600234627723694\n",
      "Epoch 3947: Training Loss: 0.24161254862944284 Validation Loss: 0.9591542482376099\n",
      "Epoch 3948: Training Loss: 0.24163362880547842 Validation Loss: 0.9584702253341675\n",
      "Epoch 3949: Training Loss: 0.2415992021560669 Validation Loss: 0.9581813812255859\n",
      "Epoch 3950: Training Loss: 0.24219278494517008 Validation Loss: 0.9584044814109802\n",
      "Epoch 3951: Training Loss: 0.24153722325960794 Validation Loss: 0.9586142897605896\n",
      "Epoch 3952: Training Loss: 0.24137679735819498 Validation Loss: 0.959041178226471\n",
      "Epoch 3953: Training Loss: 0.24236955245335898 Validation Loss: 0.9592840075492859\n",
      "Epoch 3954: Training Loss: 0.24143874645233154 Validation Loss: 0.9598320126533508\n",
      "Epoch 3955: Training Loss: 0.24207788705825806 Validation Loss: 0.9597344398498535\n",
      "Epoch 3956: Training Loss: 0.24162884056568146 Validation Loss: 0.9593551158905029\n",
      "Epoch 3957: Training Loss: 0.2404403289159139 Validation Loss: 0.9593213796615601\n",
      "Epoch 3958: Training Loss: 0.24113847812016806 Validation Loss: 0.958301842212677\n",
      "Epoch 3959: Training Loss: 0.24093948801358542 Validation Loss: 0.9580177068710327\n",
      "Epoch 3960: Training Loss: 0.24083726108074188 Validation Loss: 0.9585243463516235\n",
      "Epoch 3961: Training Loss: 0.2412923127412796 Validation Loss: 0.9585027098655701\n",
      "Epoch 3962: Training Loss: 0.24093959232171377 Validation Loss: 0.9592376947402954\n",
      "Epoch 3963: Training Loss: 0.24108795821666718 Validation Loss: 0.9591416120529175\n",
      "Epoch 3964: Training Loss: 0.24289710323015848 Validation Loss: 0.9597254991531372\n",
      "Epoch 3965: Training Loss: 0.2408382991949717 Validation Loss: 0.9596952795982361\n",
      "Epoch 3966: Training Loss: 0.24109757443269095 Validation Loss: 0.9598664045333862\n",
      "Epoch 3967: Training Loss: 0.24015323321024576 Validation Loss: 0.9593717455863953\n",
      "Epoch 3968: Training Loss: 0.24032454192638397 Validation Loss: 0.9592453837394714\n",
      "Epoch 3969: Training Loss: 0.24076266586780548 Validation Loss: 0.9585668444633484\n",
      "Epoch 3970: Training Loss: 0.24082301060358682 Validation Loss: 0.9588343501091003\n",
      "Epoch 3971: Training Loss: 0.24026475846767426 Validation Loss: 0.9592260718345642\n",
      "Epoch 3972: Training Loss: 0.2401122252146403 Validation Loss: 0.9595829844474792\n",
      "Epoch 3973: Training Loss: 0.24013669788837433 Validation Loss: 0.9597892761230469\n",
      "Epoch 3974: Training Loss: 0.24023854732513428 Validation Loss: 0.9597078561782837\n",
      "Epoch 3975: Training Loss: 0.24071497221787772 Validation Loss: 0.9598948359489441\n",
      "Epoch 3976: Training Loss: 0.24054748813311258 Validation Loss: 0.9595977663993835\n",
      "Epoch 3977: Training Loss: 0.2416499654452006 Validation Loss: 0.9601374268531799\n",
      "Epoch 3978: Training Loss: 0.23920413851737976 Validation Loss: 0.9596904516220093\n",
      "Epoch 3979: Training Loss: 0.24020188053448996 Validation Loss: 0.9593605399131775\n",
      "Epoch 3980: Training Loss: 0.24023133516311646 Validation Loss: 0.9583857655525208\n",
      "Epoch 3981: Training Loss: 0.2397686243057251 Validation Loss: 0.9586756229400635\n",
      "Epoch 3982: Training Loss: 0.24000599483648935 Validation Loss: 0.9593679308891296\n",
      "Epoch 3983: Training Loss: 0.23987979193528494 Validation Loss: 0.9593188166618347\n",
      "Epoch 3984: Training Loss: 0.24017339448134103 Validation Loss: 0.9596319794654846\n",
      "Epoch 3985: Training Loss: 0.239473690589269 Validation Loss: 0.9598731398582458\n",
      "Epoch 3986: Training Loss: 0.23928086459636688 Validation Loss: 0.9598234295845032\n",
      "Epoch 3987: Training Loss: 0.2393043984969457 Validation Loss: 0.9594033360481262\n",
      "Epoch 3988: Training Loss: 0.2401789128780365 Validation Loss: 0.9594340324401855\n",
      "Epoch 3989: Training Loss: 0.23865618805090585 Validation Loss: 0.9596005082130432\n",
      "Epoch 3990: Training Loss: 0.2396479994058609 Validation Loss: 0.959808886051178\n",
      "Epoch 3991: Training Loss: 0.23821605245272318 Validation Loss: 0.9595939517021179\n",
      "Epoch 3992: Training Loss: 0.23881724973519644 Validation Loss: 0.9592258930206299\n",
      "Epoch 3993: Training Loss: 0.23868978520234427 Validation Loss: 0.9589473009109497\n",
      "Epoch 3994: Training Loss: 0.23872062067190805 Validation Loss: 0.9591543674468994\n",
      "Epoch 3995: Training Loss: 0.23853602508703867 Validation Loss: 0.9593321084976196\n",
      "Epoch 3996: Training Loss: 0.2392502079407374 Validation Loss: 0.9597220420837402\n",
      "Epoch 3997: Training Loss: 0.23859178523222604 Validation Loss: 0.959475040435791\n",
      "Epoch 3998: Training Loss: 0.23865032196044922 Validation Loss: 0.9592291712760925\n",
      "Epoch 3999: Training Loss: 0.238916277885437 Validation Loss: 0.959380567073822\n",
      "Epoch 4000: Training Loss: 0.23857327302296957 Validation Loss: 0.9592671394348145\n",
      "Epoch 4001: Training Loss: 0.23842170337835947 Validation Loss: 0.959377646446228\n",
      "Epoch 4002: Training Loss: 0.23784270385901132 Validation Loss: 0.959993839263916\n",
      "Epoch 4003: Training Loss: 0.2392438898483912 Validation Loss: 0.9599565863609314\n",
      "Epoch 4004: Training Loss: 0.23955318331718445 Validation Loss: 0.9601148962974548\n",
      "Epoch 4005: Training Loss: 0.239055464665095 Validation Loss: 0.9601249098777771\n",
      "Epoch 4006: Training Loss: 0.23760360479354858 Validation Loss: 0.9604524374008179\n",
      "Epoch 4007: Training Loss: 0.23776842653751373 Validation Loss: 0.9601210355758667\n",
      "Epoch 4008: Training Loss: 0.23714925348758698 Validation Loss: 0.9599965810775757\n",
      "Epoch 4009: Training Loss: 0.23704072336355844 Validation Loss: 0.9594449400901794\n",
      "Epoch 4010: Training Loss: 0.23732377588748932 Validation Loss: 0.9594839215278625\n",
      "Epoch 4011: Training Loss: 0.23766164481639862 Validation Loss: 0.9591001868247986\n",
      "Epoch 4012: Training Loss: 0.23732839028040567 Validation Loss: 0.9594070911407471\n",
      "Epoch 4013: Training Loss: 0.23744763930638632 Validation Loss: 0.9591279029846191\n",
      "Epoch 4014: Training Loss: 0.2398802936077118 Validation Loss: 0.959353506565094\n",
      "Epoch 4015: Training Loss: 0.23697626094023386 Validation Loss: 0.9590268731117249\n",
      "Epoch 4016: Training Loss: 0.2369006872177124 Validation Loss: 0.9593687057495117\n",
      "Epoch 4017: Training Loss: 0.2378771702448527 Validation Loss: 0.95982426404953\n",
      "Epoch 4018: Training Loss: 0.2370386322339376 Validation Loss: 0.9599549770355225\n",
      "Epoch 4019: Training Loss: 0.23730701208114624 Validation Loss: 0.9601287245750427\n",
      "Epoch 4020: Training Loss: 0.23790723085403442 Validation Loss: 0.9601160883903503\n",
      "Epoch 4021: Training Loss: 0.23692676424980164 Validation Loss: 0.9601731896400452\n",
      "Epoch 4022: Training Loss: 0.23771580557028452 Validation Loss: 0.9598810076713562\n",
      "Epoch 4023: Training Loss: 0.23881966372330984 Validation Loss: 0.9596908092498779\n",
      "Epoch 4024: Training Loss: 0.23660850524902344 Validation Loss: 0.959017276763916\n",
      "Epoch 4025: Training Loss: 0.23630506793657938 Validation Loss: 0.9585725665092468\n",
      "Epoch 4026: Training Loss: 0.2378980815410614 Validation Loss: 0.9584900140762329\n",
      "Epoch 4027: Training Loss: 0.2362474004427592 Validation Loss: 0.9591346383094788\n",
      "Epoch 4028: Training Loss: 0.2369991640249888 Validation Loss: 0.959088146686554\n",
      "Epoch 4029: Training Loss: 0.23632858196894327 Validation Loss: 0.9592781066894531\n",
      "Epoch 4030: Training Loss: 0.23620691895484924 Validation Loss: 0.9594123363494873\n",
      "Epoch 4031: Training Loss: 0.23592906693617502 Validation Loss: 0.9596027135848999\n",
      "Epoch 4032: Training Loss: 0.2359880954027176 Validation Loss: 0.9593998193740845\n",
      "Epoch 4033: Training Loss: 0.235861008365949 Validation Loss: 0.9598807692527771\n",
      "Epoch 4034: Training Loss: 0.2354936550060908 Validation Loss: 0.9600253701210022\n",
      "Epoch 4035: Training Loss: 0.23579551776250204 Validation Loss: 0.9600452780723572\n",
      "Epoch 4036: Training Loss: 0.23639431099096933 Validation Loss: 0.9607770442962646\n",
      "Epoch 4037: Training Loss: 0.23590332766373953 Validation Loss: 0.9606970548629761\n",
      "Epoch 4038: Training Loss: 0.2363542765378952 Validation Loss: 0.9610844254493713\n",
      "Epoch 4039: Training Loss: 0.23575608928998312 Validation Loss: 0.9607764482498169\n",
      "Epoch 4040: Training Loss: 0.23574882745742798 Validation Loss: 0.9605013132095337\n",
      "Epoch 4041: Training Loss: 0.2363397777080536 Validation Loss: 0.96006178855896\n",
      "Epoch 4042: Training Loss: 0.23464535176753998 Validation Loss: 0.9596319198608398\n",
      "Epoch 4043: Training Loss: 0.23496932288010916 Validation Loss: 0.9598685503005981\n",
      "Epoch 4044: Training Loss: 0.23512324690818787 Validation Loss: 0.9596186280250549\n",
      "Epoch 4045: Training Loss: 0.23497341573238373 Validation Loss: 0.9588987231254578\n",
      "Epoch 4046: Training Loss: 0.23509275913238525 Validation Loss: 0.9591016173362732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4047: Training Loss: 0.23451468348503113 Validation Loss: 0.9596157073974609\n",
      "Epoch 4048: Training Loss: 0.23520839711030325 Validation Loss: 0.9596859812736511\n",
      "Epoch 4049: Training Loss: 0.23483794927597046 Validation Loss: 0.9599912762641907\n",
      "Epoch 4050: Training Loss: 0.2348643740018209 Validation Loss: 0.9600304365158081\n",
      "Epoch 4051: Training Loss: 0.23460868000984192 Validation Loss: 0.9603067636489868\n",
      "Epoch 4052: Training Loss: 0.2346685528755188 Validation Loss: 0.960631251335144\n",
      "Epoch 4053: Training Loss: 0.2357507050037384 Validation Loss: 0.9603242874145508\n",
      "Epoch 4054: Training Loss: 0.23421228925387064 Validation Loss: 0.9599072337150574\n",
      "Epoch 4055: Training Loss: 0.2342656354109446 Validation Loss: 0.9593597054481506\n",
      "Epoch 4056: Training Loss: 0.23429565131664276 Validation Loss: 0.9590334892272949\n",
      "Epoch 4057: Training Loss: 0.23441260556379953 Validation Loss: 0.9592790603637695\n",
      "Epoch 4058: Training Loss: 0.23417841891447702 Validation Loss: 0.959503173828125\n",
      "Epoch 4059: Training Loss: 0.2341668357451757 Validation Loss: 0.9604398608207703\n",
      "Epoch 4060: Training Loss: 0.23449075718720755 Validation Loss: 0.9605599045753479\n",
      "Epoch 4061: Training Loss: 0.23459961513678232 Validation Loss: 0.9601070284843445\n",
      "Epoch 4062: Training Loss: 0.23431308070818582 Validation Loss: 0.9601662158966064\n",
      "Epoch 4063: Training Loss: 0.23471503953138986 Validation Loss: 0.9601452350616455\n",
      "Epoch 4064: Training Loss: 0.23387478788693747 Validation Loss: 0.9598040580749512\n",
      "Epoch 4065: Training Loss: 0.23371081550916037 Validation Loss: 0.959308922290802\n",
      "Epoch 4066: Training Loss: 0.23473413288593292 Validation Loss: 0.959540069103241\n",
      "Epoch 4067: Training Loss: 0.23430650432904562 Validation Loss: 0.9596678614616394\n",
      "Epoch 4068: Training Loss: 0.23292721807956696 Validation Loss: 0.9604347348213196\n",
      "Epoch 4069: Training Loss: 0.2335972785949707 Validation Loss: 0.960198163986206\n",
      "Epoch 4070: Training Loss: 0.23325454692045847 Validation Loss: 0.9605705142021179\n",
      "Epoch 4071: Training Loss: 0.23360798259576163 Validation Loss: 0.961095929145813\n",
      "Epoch 4072: Training Loss: 0.23434780538082123 Validation Loss: 0.9613344669342041\n",
      "Epoch 4073: Training Loss: 0.23314092556635538 Validation Loss: 0.9611014127731323\n",
      "Epoch 4074: Training Loss: 0.23313058416048685 Validation Loss: 0.9607334733009338\n",
      "Epoch 4075: Training Loss: 0.23374628027280173 Validation Loss: 0.9602475762367249\n",
      "Epoch 4076: Training Loss: 0.23374048372109732 Validation Loss: 0.9595915079116821\n",
      "Epoch 4077: Training Loss: 0.233682190378507 Validation Loss: 0.9595111012458801\n",
      "Epoch 4078: Training Loss: 0.23303978641827902 Validation Loss: 0.9598640203475952\n",
      "Epoch 4079: Training Loss: 0.23279410103956857 Validation Loss: 0.9600406289100647\n",
      "Epoch 4080: Training Loss: 0.2342550257841746 Validation Loss: 0.9604616761207581\n",
      "Epoch 4081: Training Loss: 0.23503310978412628 Validation Loss: 0.960392415523529\n",
      "Epoch 4082: Training Loss: 0.23292571802934012 Validation Loss: 0.9605008959770203\n",
      "Epoch 4083: Training Loss: 0.2330110420783361 Validation Loss: 0.9597278237342834\n",
      "Epoch 4084: Training Loss: 0.23410843312740326 Validation Loss: 0.9598450064659119\n",
      "Epoch 4085: Training Loss: 0.2323212722937266 Validation Loss: 0.9602504968643188\n",
      "Epoch 4086: Training Loss: 0.23291023075580597 Validation Loss: 0.959707498550415\n",
      "Epoch 4087: Training Loss: 0.23238181074460348 Validation Loss: 0.9601464867591858\n",
      "Epoch 4088: Training Loss: 0.23240656157334647 Validation Loss: 0.9606131911277771\n",
      "Epoch 4089: Training Loss: 0.2322814961274465 Validation Loss: 0.960938572883606\n",
      "Epoch 4090: Training Loss: 0.23272227247556052 Validation Loss: 0.9609565734863281\n",
      "Epoch 4091: Training Loss: 0.23167064289251962 Validation Loss: 0.9608792662620544\n",
      "Epoch 4092: Training Loss: 0.23230129480361938 Validation Loss: 0.9607858657836914\n",
      "Epoch 4093: Training Loss: 0.23165330290794373 Validation Loss: 0.9599834084510803\n",
      "Epoch 4094: Training Loss: 0.23290087282657623 Validation Loss: 0.9596988558769226\n",
      "Epoch 4095: Training Loss: 0.23159562548001608 Validation Loss: 0.9597451686859131\n",
      "Epoch 4096: Training Loss: 0.2323965181907018 Validation Loss: 0.9597914218902588\n",
      "Epoch 4097: Training Loss: 0.23208443820476532 Validation Loss: 0.9600678086280823\n",
      "Epoch 4098: Training Loss: 0.23242596288522085 Validation Loss: 0.9604092836380005\n",
      "Epoch 4099: Training Loss: 0.23136503994464874 Validation Loss: 0.9603039026260376\n",
      "Epoch 4100: Training Loss: 0.2314441204071045 Validation Loss: 0.9602300524711609\n",
      "Epoch 4101: Training Loss: 0.23237840831279755 Validation Loss: 0.9606053233146667\n",
      "Epoch 4102: Training Loss: 0.23170185585816702 Validation Loss: 0.96122145652771\n",
      "Epoch 4103: Training Loss: 0.2316680202881495 Validation Loss: 0.9610511064529419\n",
      "Epoch 4104: Training Loss: 0.23271262645721436 Validation Loss: 0.9609248042106628\n",
      "Epoch 4105: Training Loss: 0.23105197648207346 Validation Loss: 0.961199939250946\n",
      "Epoch 4106: Training Loss: 0.23081514736016592 Validation Loss: 0.9612672924995422\n",
      "Epoch 4107: Training Loss: 0.2314144174257914 Validation Loss: 0.9611495733261108\n",
      "Epoch 4108: Training Loss: 0.2311455855766932 Validation Loss: 0.9602276682853699\n",
      "Epoch 4109: Training Loss: 0.2310429165760676 Validation Loss: 0.9597955346107483\n",
      "Epoch 4110: Training Loss: 0.23112739622592926 Validation Loss: 0.9598098397254944\n",
      "Epoch 4111: Training Loss: 0.23094984889030457 Validation Loss: 0.960252583026886\n",
      "Epoch 4112: Training Loss: 0.23121376832326254 Validation Loss: 0.9609590172767639\n",
      "Epoch 4113: Training Loss: 0.23037444551785788 Validation Loss: 0.9605908393859863\n",
      "Epoch 4114: Training Loss: 0.230515256524086 Validation Loss: 0.9608380794525146\n",
      "Epoch 4115: Training Loss: 0.23035057882467905 Validation Loss: 0.9606861472129822\n",
      "Epoch 4116: Training Loss: 0.2304734836022059 Validation Loss: 0.9604982733726501\n",
      "Epoch 4117: Training Loss: 0.2297065705060959 Validation Loss: 0.9608025550842285\n",
      "Epoch 4118: Training Loss: 0.2303502062956492 Validation Loss: 0.9610207080841064\n",
      "Epoch 4119: Training Loss: 0.23092537124951681 Validation Loss: 0.9602956771850586\n",
      "Epoch 4120: Training Loss: 0.23106274008750916 Validation Loss: 0.9606219530105591\n",
      "Epoch 4121: Training Loss: 0.2299217482407888 Validation Loss: 0.9603614211082458\n",
      "Epoch 4122: Training Loss: 0.2315966784954071 Validation Loss: 0.9607074856758118\n",
      "Epoch 4123: Training Loss: 0.2301928550004959 Validation Loss: 0.9605070948600769\n",
      "Epoch 4124: Training Loss: 0.22956661880016327 Validation Loss: 0.960602879524231\n",
      "Epoch 4125: Training Loss: 0.23101595540841421 Validation Loss: 0.9606423377990723\n",
      "Epoch 4126: Training Loss: 0.22951433062553406 Validation Loss: 0.9605964422225952\n",
      "Epoch 4127: Training Loss: 0.2295485039552053 Validation Loss: 0.9604769945144653\n",
      "Epoch 4128: Training Loss: 0.23014667630195618 Validation Loss: 0.9602479338645935\n",
      "Epoch 4129: Training Loss: 0.22970029215017954 Validation Loss: 0.960241973400116\n",
      "Epoch 4130: Training Loss: 0.22955446938673654 Validation Loss: 0.9608569741249084\n",
      "Epoch 4131: Training Loss: 0.22891321778297424 Validation Loss: 0.9613397717475891\n",
      "Epoch 4132: Training Loss: 0.23039243618647257 Validation Loss: 0.9605410099029541\n",
      "Epoch 4133: Training Loss: 0.22969462474187216 Validation Loss: 0.9606051445007324\n",
      "Epoch 4134: Training Loss: 0.22971801459789276 Validation Loss: 0.9607045650482178\n",
      "Epoch 4135: Training Loss: 0.22907644510269165 Validation Loss: 0.9610790014266968\n",
      "Epoch 4136: Training Loss: 0.22905727724234262 Validation Loss: 0.960862398147583\n",
      "Epoch 4137: Training Loss: 0.22957657277584076 Validation Loss: 0.9604809880256653\n",
      "Epoch 4138: Training Loss: 0.2288575420777003 Validation Loss: 0.9611243605613708\n",
      "Epoch 4139: Training Loss: 0.22886061668395996 Validation Loss: 0.9610318541526794\n",
      "Epoch 4140: Training Loss: 0.2287081480026245 Validation Loss: 0.9610078930854797\n",
      "Epoch 4141: Training Loss: 0.22926101088523865 Validation Loss: 0.960495114326477\n",
      "Epoch 4142: Training Loss: 0.22844959795475006 Validation Loss: 0.9603753089904785\n",
      "Epoch 4143: Training Loss: 0.2285092423359553 Validation Loss: 0.9603567719459534\n",
      "Epoch 4144: Training Loss: 0.22777424256006876 Validation Loss: 0.9605496525764465\n",
      "Epoch 4145: Training Loss: 0.22909168899059296 Validation Loss: 0.9609717726707458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4146: Training Loss: 0.22881007691224417 Validation Loss: 0.9606793522834778\n",
      "Epoch 4147: Training Loss: 0.22888396183649698 Validation Loss: 0.9606871008872986\n",
      "Epoch 4148: Training Loss: 0.22909492254257202 Validation Loss: 0.9605879187583923\n",
      "Epoch 4149: Training Loss: 0.22800389428933462 Validation Loss: 0.9610817432403564\n",
      "Epoch 4150: Training Loss: 0.2280701994895935 Validation Loss: 0.9608498811721802\n",
      "Epoch 4151: Training Loss: 0.2282017469406128 Validation Loss: 0.9607490301132202\n",
      "Epoch 4152: Training Loss: 0.22826887170473734 Validation Loss: 0.9608328938484192\n",
      "Epoch 4153: Training Loss: 0.2280501425266266 Validation Loss: 0.9610769152641296\n",
      "Epoch 4154: Training Loss: 0.22838752965132395 Validation Loss: 0.9616415500640869\n",
      "Epoch 4155: Training Loss: 0.22778541843096414 Validation Loss: 0.9613314270973206\n",
      "Epoch 4156: Training Loss: 0.22833735247453055 Validation Loss: 0.9608238935470581\n",
      "Epoch 4157: Training Loss: 0.2286835511525472 Validation Loss: 0.9602223634719849\n",
      "Epoch 4158: Training Loss: 0.22871472438176474 Validation Loss: 0.9602693915367126\n",
      "Epoch 4159: Training Loss: 0.22790617247422537 Validation Loss: 0.9606983661651611\n",
      "Epoch 4160: Training Loss: 0.22764915227890015 Validation Loss: 0.961493968963623\n",
      "Epoch 4161: Training Loss: 0.22762816150983176 Validation Loss: 0.9612303972244263\n",
      "Epoch 4162: Training Loss: 0.22879308462142944 Validation Loss: 0.9616959095001221\n",
      "Epoch 4163: Training Loss: 0.22832106053829193 Validation Loss: 0.9619788527488708\n",
      "Epoch 4164: Training Loss: 0.22799492379029593 Validation Loss: 0.9611404538154602\n",
      "Epoch 4165: Training Loss: 0.22712474564711252 Validation Loss: 0.9609196782112122\n",
      "Epoch 4166: Training Loss: 0.2273040364185969 Validation Loss: 0.9613192677497864\n",
      "Epoch 4167: Training Loss: 0.227191631992658 Validation Loss: 0.9615321159362793\n",
      "Epoch 4168: Training Loss: 0.226811612645785 Validation Loss: 0.9609370231628418\n",
      "Epoch 4169: Training Loss: 0.22709146638711294 Validation Loss: 0.9606038331985474\n",
      "Epoch 4170: Training Loss: 0.22685936590035757 Validation Loss: 0.9605424404144287\n",
      "Epoch 4171: Training Loss: 0.22725916902224222 Validation Loss: 0.9602484107017517\n",
      "Epoch 4172: Training Loss: 0.227113276720047 Validation Loss: 0.9601115584373474\n",
      "Epoch 4173: Training Loss: 0.22673545281092325 Validation Loss: 0.9611181616783142\n",
      "Epoch 4174: Training Loss: 0.2262593905131022 Validation Loss: 0.9620281457901001\n",
      "Epoch 4175: Training Loss: 0.22661728163560232 Validation Loss: 0.9615258574485779\n",
      "Epoch 4176: Training Loss: 0.22629141807556152 Validation Loss: 0.9615795612335205\n",
      "Epoch 4177: Training Loss: 0.22619844476381937 Validation Loss: 0.9613409042358398\n",
      "Epoch 4178: Training Loss: 0.226648211479187 Validation Loss: 0.9615565538406372\n",
      "Epoch 4179: Training Loss: 0.22630319992701212 Validation Loss: 0.9613288640975952\n",
      "Epoch 4180: Training Loss: 0.2265601853529612 Validation Loss: 0.9603773355484009\n",
      "Epoch 4181: Training Loss: 0.22615468502044678 Validation Loss: 0.9601136445999146\n",
      "Epoch 4182: Training Loss: 0.2266177088022232 Validation Loss: 0.9609324932098389\n",
      "Epoch 4183: Training Loss: 0.22619682550430298 Validation Loss: 0.9611049294471741\n",
      "Epoch 4184: Training Loss: 0.22664815187454224 Validation Loss: 0.9610995054244995\n",
      "Epoch 4185: Training Loss: 0.22541948159535727 Validation Loss: 0.9610629677772522\n",
      "Epoch 4186: Training Loss: 0.22593726217746735 Validation Loss: 0.9615013599395752\n",
      "Epoch 4187: Training Loss: 0.22600185871124268 Validation Loss: 0.9614976048469543\n",
      "Epoch 4188: Training Loss: 0.22571349143981934 Validation Loss: 0.9619629383087158\n",
      "Epoch 4189: Training Loss: 0.22624912361303964 Validation Loss: 0.9617510437965393\n",
      "Epoch 4190: Training Loss: 0.22568513453006744 Validation Loss: 0.9617171883583069\n",
      "Epoch 4191: Training Loss: 0.22561525305112204 Validation Loss: 0.9611343145370483\n",
      "Epoch 4192: Training Loss: 0.22527618209520975 Validation Loss: 0.9616901278495789\n",
      "Epoch 4193: Training Loss: 0.22494739790757498 Validation Loss: 0.9618017673492432\n",
      "Epoch 4194: Training Loss: 0.2255178689956665 Validation Loss: 0.9618948101997375\n",
      "Epoch 4195: Training Loss: 0.22562704483668009 Validation Loss: 0.9617977738380432\n",
      "Epoch 4196: Training Loss: 0.22501024107138315 Validation Loss: 0.9616482257843018\n",
      "Epoch 4197: Training Loss: 0.22522918383280435 Validation Loss: 0.9615896940231323\n",
      "Epoch 4198: Training Loss: 0.22557559112707773 Validation Loss: 0.9613432884216309\n",
      "Epoch 4199: Training Loss: 0.22528142233689627 Validation Loss: 0.9613816142082214\n",
      "Epoch 4200: Training Loss: 0.2256777435541153 Validation Loss: 0.9608548879623413\n",
      "Epoch 4201: Training Loss: 0.22767175237337747 Validation Loss: 0.9604758620262146\n",
      "Epoch 4202: Training Loss: 0.22493462761243185 Validation Loss: 0.9606333374977112\n",
      "Epoch 4203: Training Loss: 0.225168839097023 Validation Loss: 0.961119532585144\n",
      "Epoch 4204: Training Loss: 0.22474643091360727 Validation Loss: 0.9619891047477722\n",
      "Epoch 4205: Training Loss: 0.2247829536596934 Validation Loss: 0.9619593024253845\n",
      "Epoch 4206: Training Loss: 0.22535543143749237 Validation Loss: 0.9617557525634766\n",
      "Epoch 4207: Training Loss: 0.22459862132867178 Validation Loss: 0.9613777995109558\n",
      "Epoch 4208: Training Loss: 0.2248319685459137 Validation Loss: 0.9618547558784485\n",
      "Epoch 4209: Training Loss: 0.2244448115428289 Validation Loss: 0.961645245552063\n",
      "Epoch 4210: Training Loss: 0.22389272848765054 Validation Loss: 0.9614424109458923\n",
      "Epoch 4211: Training Loss: 0.22533272206783295 Validation Loss: 0.961921751499176\n",
      "Epoch 4212: Training Loss: 0.22431475420792898 Validation Loss: 0.9620037078857422\n",
      "Epoch 4213: Training Loss: 0.22454189757506052 Validation Loss: 0.9619628190994263\n",
      "Epoch 4214: Training Loss: 0.22450158496697745 Validation Loss: 0.9619845151901245\n",
      "Epoch 4215: Training Loss: 0.2245053599278132 Validation Loss: 0.9620586037635803\n",
      "Epoch 4216: Training Loss: 0.22410103181997934 Validation Loss: 0.9617571830749512\n",
      "Epoch 4217: Training Loss: 0.2240520417690277 Validation Loss: 0.9619244933128357\n",
      "Epoch 4218: Training Loss: 0.22447439034779867 Validation Loss: 0.9615932703018188\n",
      "Epoch 4219: Training Loss: 0.22389345367749533 Validation Loss: 0.9613744616508484\n",
      "Epoch 4220: Training Loss: 0.2243187576532364 Validation Loss: 0.9619015455245972\n",
      "Epoch 4221: Training Loss: 0.22428605953852335 Validation Loss: 0.9620192646980286\n",
      "Epoch 4222: Training Loss: 0.22429876029491425 Validation Loss: 0.9619886875152588\n",
      "Epoch 4223: Training Loss: 0.22339666386445364 Validation Loss: 0.9612777233123779\n",
      "Epoch 4224: Training Loss: 0.22441946466763815 Validation Loss: 0.961227536201477\n",
      "Epoch 4225: Training Loss: 0.2232354680697123 Validation Loss: 0.9612777233123779\n",
      "Epoch 4226: Training Loss: 0.22345462441444397 Validation Loss: 0.9614667892456055\n",
      "Epoch 4227: Training Loss: 0.2233294794956843 Validation Loss: 0.9614112973213196\n",
      "Epoch 4228: Training Loss: 0.22321416437625885 Validation Loss: 0.9608157277107239\n",
      "Epoch 4229: Training Loss: 0.2246991048256556 Validation Loss: 0.9613770246505737\n",
      "Epoch 4230: Training Loss: 0.2229015032450358 Validation Loss: 0.9622045755386353\n",
      "Epoch 4231: Training Loss: 0.22351861000061035 Validation Loss: 0.9624738693237305\n",
      "Epoch 4232: Training Loss: 0.22353531420230865 Validation Loss: 0.963038980960846\n",
      "Epoch 4233: Training Loss: 0.22292946775754294 Validation Loss: 0.9618297219276428\n",
      "Epoch 4234: Training Loss: 0.22228447596232095 Validation Loss: 0.9617437124252319\n",
      "Epoch 4235: Training Loss: 0.2239294151465098 Validation Loss: 0.9615890979766846\n",
      "Epoch 4236: Training Loss: 0.2234409600496292 Validation Loss: 0.9614223837852478\n",
      "Epoch 4237: Training Loss: 0.22276232639948526 Validation Loss: 0.9615152478218079\n",
      "Epoch 4238: Training Loss: 0.22331705689430237 Validation Loss: 0.9617396593093872\n",
      "Epoch 4239: Training Loss: 0.22207185129324594 Validation Loss: 0.9614827036857605\n",
      "Epoch 4240: Training Loss: 0.2226005345582962 Validation Loss: 0.9614987969398499\n",
      "Epoch 4241: Training Loss: 0.22355422377586365 Validation Loss: 0.9611157178878784\n",
      "Epoch 4242: Training Loss: 0.22312147418657938 Validation Loss: 0.9608849287033081\n",
      "Epoch 4243: Training Loss: 0.22248555719852448 Validation Loss: 0.9611530900001526\n",
      "Epoch 4244: Training Loss: 0.22259563207626343 Validation Loss: 0.9615443348884583\n",
      "Epoch 4245: Training Loss: 0.2221963107585907 Validation Loss: 0.9623952507972717\n",
      "Epoch 4246: Training Loss: 0.22269205749034882 Validation Loss: 0.9626050591468811\n",
      "Epoch 4247: Training Loss: 0.2218594253063202 Validation Loss: 0.9626404047012329\n",
      "Epoch 4248: Training Loss: 0.2222550610701243 Validation Loss: 0.9625162482261658\n",
      "Epoch 4249: Training Loss: 0.22201107442378998 Validation Loss: 0.9620234370231628\n",
      "Epoch 4250: Training Loss: 0.22203104197978973 Validation Loss: 0.9618399143218994\n",
      "Epoch 4251: Training Loss: 0.22323001424471536 Validation Loss: 0.9622735381126404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4252: Training Loss: 0.22187680502732596 Validation Loss: 0.9623231291770935\n",
      "Epoch 4253: Training Loss: 0.22331846257050833 Validation Loss: 0.9623433351516724\n",
      "Epoch 4254: Training Loss: 0.22087602317333221 Validation Loss: 0.9623335599899292\n",
      "Epoch 4255: Training Loss: 0.22185434897740683 Validation Loss: 0.962390661239624\n",
      "Epoch 4256: Training Loss: 0.2243987818559011 Validation Loss: 0.9623990058898926\n",
      "Epoch 4257: Training Loss: 0.2226443886756897 Validation Loss: 0.9624718427658081\n",
      "Epoch 4258: Training Loss: 0.22153338293234506 Validation Loss: 0.9620316028594971\n",
      "Epoch 4259: Training Loss: 0.22193430860837302 Validation Loss: 0.9615497589111328\n",
      "Epoch 4260: Training Loss: 0.22238067289193472 Validation Loss: 0.9622731804847717\n",
      "Epoch 4261: Training Loss: 0.2213255912065506 Validation Loss: 0.9623489379882812\n",
      "Epoch 4262: Training Loss: 0.2216314673423767 Validation Loss: 0.9621836543083191\n",
      "Epoch 4263: Training Loss: 0.22121352950731912 Validation Loss: 0.9618184566497803\n",
      "Epoch 4264: Training Loss: 0.2209919293721517 Validation Loss: 0.9612150192260742\n",
      "Epoch 4265: Training Loss: 0.22249269982179007 Validation Loss: 0.9613178372383118\n",
      "Epoch 4266: Training Loss: 0.2209445188442866 Validation Loss: 0.9620476365089417\n",
      "Epoch 4267: Training Loss: 0.22107045352458954 Validation Loss: 0.9621080160140991\n",
      "Epoch 4268: Training Loss: 0.22051737705866495 Validation Loss: 0.9624219536781311\n",
      "Epoch 4269: Training Loss: 0.22099958856900534 Validation Loss: 0.9626373648643494\n",
      "Epoch 4270: Training Loss: 0.22136135399341583 Validation Loss: 0.9624682664871216\n",
      "Epoch 4271: Training Loss: 0.2209998071193695 Validation Loss: 0.962900698184967\n",
      "Epoch 4272: Training Loss: 0.22127623359362283 Validation Loss: 0.9624767899513245\n",
      "Epoch 4273: Training Loss: 0.22146126131216684 Validation Loss: 0.9629856944084167\n",
      "Epoch 4274: Training Loss: 0.2216279258330663 Validation Loss: 0.9625683426856995\n",
      "Epoch 4275: Training Loss: 0.2202449490626653 Validation Loss: 0.9625633955001831\n",
      "Epoch 4276: Training Loss: 0.22051316996415457 Validation Loss: 0.9624114632606506\n",
      "Epoch 4277: Training Loss: 0.2197186996539434 Validation Loss: 0.9620964527130127\n",
      "Epoch 4278: Training Loss: 0.2207609365383784 Validation Loss: 0.9625396132469177\n",
      "Epoch 4279: Training Loss: 0.22127929826577505 Validation Loss: 0.9629891514778137\n",
      "Epoch 4280: Training Loss: 0.22118152181307474 Validation Loss: 0.9633245468139648\n",
      "Epoch 4281: Training Loss: 0.22134831051031748 Validation Loss: 0.9624201059341431\n",
      "Epoch 4282: Training Loss: 0.21993200480937958 Validation Loss: 0.9624826908111572\n",
      "Epoch 4283: Training Loss: 0.21994072695573172 Validation Loss: 0.9622989892959595\n",
      "Epoch 4284: Training Loss: 0.2205743690331777 Validation Loss: 0.9617066979408264\n",
      "Epoch 4285: Training Loss: 0.2216265251239141 Validation Loss: 0.9618199467658997\n",
      "Epoch 4286: Training Loss: 0.21947559714317322 Validation Loss: 0.9617362022399902\n",
      "Epoch 4287: Training Loss: 0.21997306744257608 Validation Loss: 0.9616973400115967\n",
      "Epoch 4288: Training Loss: 0.21996577084064484 Validation Loss: 0.9620218873023987\n",
      "Epoch 4289: Training Loss: 0.2199976791938146 Validation Loss: 0.9628046154975891\n",
      "Epoch 4290: Training Loss: 0.21972680588563284 Validation Loss: 0.962947428226471\n",
      "Epoch 4291: Training Loss: 0.21950890123844147 Validation Loss: 0.9627554416656494\n",
      "Epoch 4292: Training Loss: 0.2199034740527471 Validation Loss: 0.9625189304351807\n",
      "Epoch 4293: Training Loss: 0.22178030014038086 Validation Loss: 0.9623337984085083\n",
      "Epoch 4294: Training Loss: 0.22043443719546 Validation Loss: 0.9619219303131104\n",
      "Epoch 4295: Training Loss: 0.21917855242888132 Validation Loss: 0.9622625112533569\n",
      "Epoch 4296: Training Loss: 0.21915479997793832 Validation Loss: 0.963161826133728\n",
      "Epoch 4297: Training Loss: 0.21914601822694144 Validation Loss: 0.96291583776474\n",
      "Epoch 4298: Training Loss: 0.21828489502271017 Validation Loss: 0.962833821773529\n",
      "Epoch 4299: Training Loss: 0.2191266268491745 Validation Loss: 0.9625199437141418\n",
      "Epoch 4300: Training Loss: 0.21971515814463297 Validation Loss: 0.9623022675514221\n",
      "Epoch 4301: Training Loss: 0.2189655750989914 Validation Loss: 0.9621477723121643\n",
      "Epoch 4302: Training Loss: 0.21929619709650675 Validation Loss: 0.9633121490478516\n",
      "Epoch 4303: Training Loss: 0.21887005865573883 Validation Loss: 0.963041365146637\n",
      "Epoch 4304: Training Loss: 0.21918952961762747 Validation Loss: 0.9631841778755188\n",
      "Epoch 4305: Training Loss: 0.21837055683135986 Validation Loss: 0.9633305072784424\n",
      "Epoch 4306: Training Loss: 0.21979568898677826 Validation Loss: 0.9631431102752686\n",
      "Epoch 4307: Training Loss: 0.21908332407474518 Validation Loss: 0.9633023738861084\n",
      "Epoch 4308: Training Loss: 0.21902717649936676 Validation Loss: 0.9641430377960205\n",
      "Epoch 4309: Training Loss: 0.21849597493807474 Validation Loss: 0.964087724685669\n",
      "Epoch 4310: Training Loss: 0.2185171147187551 Validation Loss: 0.9636427760124207\n",
      "Epoch 4311: Training Loss: 0.21822391947110495 Validation Loss: 0.9632318019866943\n",
      "Epoch 4312: Training Loss: 0.2184149573246638 Validation Loss: 0.9633610844612122\n",
      "Epoch 4313: Training Loss: 0.21807491282622019 Validation Loss: 0.9625173211097717\n",
      "Epoch 4314: Training Loss: 0.217878058552742 Validation Loss: 0.9624150395393372\n",
      "Epoch 4315: Training Loss: 0.2187537501255671 Validation Loss: 0.9629555344581604\n",
      "Epoch 4316: Training Loss: 0.21820548176765442 Validation Loss: 0.9622016549110413\n",
      "Epoch 4317: Training Loss: 0.21818025906880698 Validation Loss: 0.9615849256515503\n",
      "Epoch 4318: Training Loss: 0.21813353399435678 Validation Loss: 0.9623182415962219\n",
      "Epoch 4319: Training Loss: 0.21825118362903595 Validation Loss: 0.9622750878334045\n",
      "Epoch 4320: Training Loss: 0.21842694282531738 Validation Loss: 0.9626153111457825\n",
      "Epoch 4321: Training Loss: 0.21790558099746704 Validation Loss: 0.9626839756965637\n",
      "Epoch 4322: Training Loss: 0.21786917746067047 Validation Loss: 0.963518500328064\n",
      "Epoch 4323: Training Loss: 0.21776852011680603 Validation Loss: 0.9638519883155823\n",
      "Epoch 4324: Training Loss: 0.21762767434120178 Validation Loss: 0.9640855193138123\n",
      "Epoch 4325: Training Loss: 0.21748308340708414 Validation Loss: 0.963489294052124\n",
      "Epoch 4326: Training Loss: 0.21743171413739523 Validation Loss: 0.9632194638252258\n",
      "Epoch 4327: Training Loss: 0.21787355343500772 Validation Loss: 0.9635590314865112\n",
      "Epoch 4328: Training Loss: 0.21763323744138083 Validation Loss: 0.9633193612098694\n",
      "Epoch 4329: Training Loss: 0.21744225919246674 Validation Loss: 0.9632315039634705\n",
      "Epoch 4330: Training Loss: 0.21757451196511587 Validation Loss: 0.9630001783370972\n",
      "Epoch 4331: Training Loss: 0.21717702349027 Validation Loss: 0.9633414149284363\n",
      "Epoch 4332: Training Loss: 0.2171173095703125 Validation Loss: 0.9631174206733704\n",
      "Epoch 4333: Training Loss: 0.21696188052495322 Validation Loss: 0.9624366164207458\n",
      "Epoch 4334: Training Loss: 0.21691106259822845 Validation Loss: 0.9624151587486267\n",
      "Epoch 4335: Training Loss: 0.2168256789445877 Validation Loss: 0.9617781639099121\n",
      "Epoch 4336: Training Loss: 0.21790894865989685 Validation Loss: 0.9624521136283875\n",
      "Epoch 4337: Training Loss: 0.21683442095915476 Validation Loss: 0.9634872078895569\n",
      "Epoch 4338: Training Loss: 0.21675501763820648 Validation Loss: 0.963376522064209\n",
      "Epoch 4339: Training Loss: 0.217841570576032 Validation Loss: 0.963558554649353\n",
      "Epoch 4340: Training Loss: 0.2167783478895823 Validation Loss: 0.9642595648765564\n",
      "Epoch 4341: Training Loss: 0.21656850973765054 Validation Loss: 0.9638734459877014\n",
      "Epoch 4342: Training Loss: 0.21712082624435425 Validation Loss: 0.9633508324623108\n",
      "Epoch 4343: Training Loss: 0.2165759801864624 Validation Loss: 0.9639085531234741\n",
      "Epoch 4344: Training Loss: 0.21669067442417145 Validation Loss: 0.964095950126648\n",
      "Epoch 4345: Training Loss: 0.21625921626885733 Validation Loss: 0.9639381170272827\n",
      "Epoch 4346: Training Loss: 0.21631175776322684 Validation Loss: 0.963280439376831\n",
      "Epoch 4347: Training Loss: 0.21774390836556753 Validation Loss: 0.9631573557853699\n",
      "Epoch 4348: Training Loss: 0.216494952638944 Validation Loss: 0.9634512066841125\n",
      "Epoch 4349: Training Loss: 0.21586845318476358 Validation Loss: 0.9631299376487732\n",
      "Epoch 4350: Training Loss: 0.2164908548196157 Validation Loss: 0.9628462195396423\n",
      "Epoch 4351: Training Loss: 0.21576018631458282 Validation Loss: 0.9634498357772827\n",
      "Epoch 4352: Training Loss: 0.2163017839193344 Validation Loss: 0.963616132736206\n",
      "Epoch 4353: Training Loss: 0.21669302880764008 Validation Loss: 0.9634403586387634\n",
      "Epoch 4354: Training Loss: 0.21597174803415933 Validation Loss: 0.9635439515113831\n",
      "Epoch 4355: Training Loss: 0.21638154983520508 Validation Loss: 0.9637011885643005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4356: Training Loss: 0.21589762965838113 Validation Loss: 0.9638675451278687\n",
      "Epoch 4357: Training Loss: 0.21564513444900513 Validation Loss: 0.9641276597976685\n",
      "Epoch 4358: Training Loss: 0.21523440380891165 Validation Loss: 0.9641841053962708\n",
      "Epoch 4359: Training Loss: 0.2162521630525589 Validation Loss: 0.9646711349487305\n",
      "Epoch 4360: Training Loss: 0.21569239099820456 Validation Loss: 0.9643158316612244\n",
      "Epoch 4361: Training Loss: 0.21550401051839194 Validation Loss: 0.9641004204750061\n",
      "Epoch 4362: Training Loss: 0.21607578297456106 Validation Loss: 0.9639002084732056\n",
      "Epoch 4363: Training Loss: 0.21576968332131705 Validation Loss: 0.9635061025619507\n",
      "Epoch 4364: Training Loss: 0.2157807250817617 Validation Loss: 0.963248610496521\n",
      "Epoch 4365: Training Loss: 0.2157169133424759 Validation Loss: 0.9637570977210999\n",
      "Epoch 4366: Training Loss: 0.21547692020734152 Validation Loss: 0.9635038375854492\n",
      "Epoch 4367: Training Loss: 0.21536552409331003 Validation Loss: 0.9634478688240051\n",
      "Epoch 4368: Training Loss: 0.21476987500985464 Validation Loss: 0.9634472131729126\n",
      "Epoch 4369: Training Loss: 0.21534732977549234 Validation Loss: 0.9634439945220947\n",
      "Epoch 4370: Training Loss: 0.21436105171839395 Validation Loss: 0.9627273678779602\n",
      "Epoch 4371: Training Loss: 0.2154009292523066 Validation Loss: 0.963129997253418\n",
      "Epoch 4372: Training Loss: 0.2148162623246511 Validation Loss: 0.9629408121109009\n",
      "Epoch 4373: Training Loss: 0.21550618608792624 Validation Loss: 0.9633235335350037\n",
      "Epoch 4374: Training Loss: 0.21553446849187216 Validation Loss: 0.9640524387359619\n",
      "Epoch 4375: Training Loss: 0.21468728284041086 Validation Loss: 0.964560329914093\n",
      "Epoch 4376: Training Loss: 0.21487543980280557 Validation Loss: 0.9646092057228088\n",
      "Epoch 4377: Training Loss: 0.21461622913678488 Validation Loss: 0.9646117687225342\n",
      "Epoch 4378: Training Loss: 0.21513336896896362 Validation Loss: 0.9642379283905029\n",
      "Epoch 4379: Training Loss: 0.21444148818651834 Validation Loss: 0.9638118743896484\n",
      "Epoch 4380: Training Loss: 0.2146909534931183 Validation Loss: 0.9634579420089722\n",
      "Epoch 4381: Training Loss: 0.2144177109003067 Validation Loss: 0.9630283713340759\n",
      "Epoch 4382: Training Loss: 0.2146267145872116 Validation Loss: 0.9632646441459656\n",
      "Epoch 4383: Training Loss: 0.21423893173535666 Validation Loss: 0.9633504748344421\n",
      "Epoch 4384: Training Loss: 0.2141389548778534 Validation Loss: 0.9638956189155579\n",
      "Epoch 4385: Training Loss: 0.2151709347963333 Validation Loss: 0.9642220139503479\n",
      "Epoch 4386: Training Loss: 0.21455085277557373 Validation Loss: 0.9648647308349609\n",
      "Epoch 4387: Training Loss: 0.21434320509433746 Validation Loss: 0.9647559523582458\n",
      "Epoch 4388: Training Loss: 0.21359951297442117 Validation Loss: 0.9648752212524414\n",
      "Epoch 4389: Training Loss: 0.2140626609325409 Validation Loss: 0.9648371338844299\n",
      "Epoch 4390: Training Loss: 0.21429446836312613 Validation Loss: 0.9646763205528259\n",
      "Epoch 4391: Training Loss: 0.2138361632823944 Validation Loss: 0.964654266834259\n",
      "Epoch 4392: Training Loss: 0.21493162214756012 Validation Loss: 0.9645471572875977\n",
      "Epoch 4393: Training Loss: 0.21459372838338217 Validation Loss: 0.9647098183631897\n",
      "Epoch 4394: Training Loss: 0.21374345819155374 Validation Loss: 0.9646153450012207\n",
      "Epoch 4395: Training Loss: 0.21426529188950857 Validation Loss: 0.9646013379096985\n",
      "Epoch 4396: Training Loss: 0.21397864321867624 Validation Loss: 0.9639266729354858\n",
      "Epoch 4397: Training Loss: 0.21328801413377127 Validation Loss: 0.9637272953987122\n",
      "Epoch 4398: Training Loss: 0.2135992000500361 Validation Loss: 0.9632051587104797\n",
      "Epoch 4399: Training Loss: 0.21343440314133963 Validation Loss: 0.9639051556587219\n",
      "Epoch 4400: Training Loss: 0.21348280707995096 Validation Loss: 0.9643496870994568\n",
      "Epoch 4401: Training Loss: 0.21325410405794779 Validation Loss: 0.9641756415367126\n",
      "Epoch 4402: Training Loss: 0.21352198719978333 Validation Loss: 0.9645472764968872\n",
      "Epoch 4403: Training Loss: 0.21396207312742868 Validation Loss: 0.9639862179756165\n",
      "Epoch 4404: Training Loss: 0.21285807092984518 Validation Loss: 0.9644340872764587\n",
      "Epoch 4405: Training Loss: 0.21307273209095 Validation Loss: 0.9646031260490417\n",
      "Epoch 4406: Training Loss: 0.21302874386310577 Validation Loss: 0.9648334980010986\n",
      "Epoch 4407: Training Loss: 0.21317189931869507 Validation Loss: 0.9639562368392944\n",
      "Epoch 4408: Training Loss: 0.21317827701568604 Validation Loss: 0.9639692306518555\n",
      "Epoch 4409: Training Loss: 0.21273610492547354 Validation Loss: 0.9639250636100769\n",
      "Epoch 4410: Training Loss: 0.21338769793510437 Validation Loss: 0.9646707773208618\n",
      "Epoch 4411: Training Loss: 0.2135126292705536 Validation Loss: 0.9648666381835938\n",
      "Epoch 4412: Training Loss: 0.21232593059539795 Validation Loss: 0.9647073745727539\n",
      "Epoch 4413: Training Loss: 0.21265642841657004 Validation Loss: 0.9646914005279541\n",
      "Epoch 4414: Training Loss: 0.21358810365200043 Validation Loss: 0.9652299880981445\n",
      "Epoch 4415: Training Loss: 0.2124988834063212 Validation Loss: 0.9651435613632202\n",
      "Epoch 4416: Training Loss: 0.21335901816685995 Validation Loss: 0.9651926755905151\n",
      "Epoch 4417: Training Loss: 0.21381189425786337 Validation Loss: 0.9649706482887268\n",
      "Epoch 4418: Training Loss: 0.21337891618410745 Validation Loss: 0.964768648147583\n",
      "Epoch 4419: Training Loss: 0.2116240163644155 Validation Loss: 0.9645489454269409\n",
      "Epoch 4420: Training Loss: 0.2117429624001185 Validation Loss: 0.9643623232841492\n",
      "Epoch 4421: Training Loss: 0.21222219367822012 Validation Loss: 0.9647442698478699\n",
      "Epoch 4422: Training Loss: 0.21231945355733237 Validation Loss: 0.9651160836219788\n",
      "Epoch 4423: Training Loss: 0.2117893546819687 Validation Loss: 0.9652585387229919\n",
      "Epoch 4424: Training Loss: 0.21200974782307944 Validation Loss: 0.9655305743217468\n",
      "Epoch 4425: Training Loss: 0.211650679508845 Validation Loss: 0.9652695059776306\n",
      "Epoch 4426: Training Loss: 0.21168971558411917 Validation Loss: 0.9648557901382446\n",
      "Epoch 4427: Training Loss: 0.2118376543124517 Validation Loss: 0.9647026062011719\n",
      "Epoch 4428: Training Loss: 0.21237739423910776 Validation Loss: 0.9645359516143799\n",
      "Epoch 4429: Training Loss: 0.21171476940313974 Validation Loss: 0.964560329914093\n",
      "Epoch 4430: Training Loss: 0.2116988847653071 Validation Loss: 0.964024543762207\n",
      "Epoch 4431: Training Loss: 0.21172755459944406 Validation Loss: 0.9645598530769348\n",
      "Epoch 4432: Training Loss: 0.21177803973356882 Validation Loss: 0.96486896276474\n",
      "Epoch 4433: Training Loss: 0.21244360009829202 Validation Loss: 0.9647759199142456\n",
      "Epoch 4434: Training Loss: 0.21187488238016763 Validation Loss: 0.9643509984016418\n",
      "Epoch 4435: Training Loss: 0.2113465517759323 Validation Loss: 0.965181291103363\n",
      "Epoch 4436: Training Loss: 0.21138518551985422 Validation Loss: 0.9655817747116089\n",
      "Epoch 4437: Training Loss: 0.21181179583072662 Validation Loss: 0.9650750756263733\n",
      "Epoch 4438: Training Loss: 0.2111977239449819 Validation Loss: 0.9652023911476135\n",
      "Epoch 4439: Training Loss: 0.2112436145544052 Validation Loss: 0.964660108089447\n",
      "Epoch 4440: Training Loss: 0.21057334542274475 Validation Loss: 0.964664101600647\n",
      "Epoch 4441: Training Loss: 0.21111581722895303 Validation Loss: 0.9644685387611389\n",
      "Epoch 4442: Training Loss: 0.21088507771492004 Validation Loss: 0.9650296568870544\n",
      "Epoch 4443: Training Loss: 0.2116830994685491 Validation Loss: 0.9648881554603577\n",
      "Epoch 4444: Training Loss: 0.21193955838680267 Validation Loss: 0.9653444886207581\n",
      "Epoch 4445: Training Loss: 0.2108784168958664 Validation Loss: 0.9654409885406494\n",
      "Epoch 4446: Training Loss: 0.21201745669047037 Validation Loss: 0.965075671672821\n",
      "Epoch 4447: Training Loss: 0.21119700372219086 Validation Loss: 0.9652364253997803\n",
      "Epoch 4448: Training Loss: 0.21078953643639883 Validation Loss: 0.9651582837104797\n",
      "Epoch 4449: Training Loss: 0.21157681941986084 Validation Loss: 0.9650077223777771\n",
      "Epoch 4450: Training Loss: 0.21058405935764313 Validation Loss: 0.9646733999252319\n",
      "Epoch 4451: Training Loss: 0.21084156135718027 Validation Loss: 0.9642491340637207\n",
      "Epoch 4452: Training Loss: 0.2121148556470871 Validation Loss: 0.9648786783218384\n",
      "Epoch 4453: Training Loss: 0.21101815005143484 Validation Loss: 0.964503824710846\n",
      "Epoch 4454: Training Loss: 0.21033867200215658 Validation Loss: 0.9648358821868896\n",
      "Epoch 4455: Training Loss: 0.21080713470776877 Validation Loss: 0.9650915861129761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4456: Training Loss: 0.21022859712441763 Validation Loss: 0.9646983742713928\n",
      "Epoch 4457: Training Loss: 0.21067067980766296 Validation Loss: 0.964423656463623\n",
      "Epoch 4458: Training Loss: 0.21139515936374664 Validation Loss: 0.9647106528282166\n",
      "Epoch 4459: Training Loss: 0.21187212566534677 Validation Loss: 0.965055525302887\n",
      "Epoch 4460: Training Loss: 0.21068158249060312 Validation Loss: 0.9656591415405273\n",
      "Epoch 4461: Training Loss: 0.2101428210735321 Validation Loss: 0.9654216766357422\n",
      "Epoch 4462: Training Loss: 0.2098521739244461 Validation Loss: 0.965207040309906\n",
      "Epoch 4463: Training Loss: 0.21091522773106894 Validation Loss: 0.9654454588890076\n",
      "Epoch 4464: Training Loss: 0.20983788867791495 Validation Loss: 0.9657111167907715\n",
      "Epoch 4465: Training Loss: 0.21084285775820413 Validation Loss: 0.965794026851654\n",
      "Epoch 4466: Training Loss: 0.2103711167971293 Validation Loss: 0.9659225344657898\n",
      "Epoch 4467: Training Loss: 0.20966952542463937 Validation Loss: 0.9661825299263\n",
      "Epoch 4468: Training Loss: 0.20948482553164163 Validation Loss: 0.9655874371528625\n",
      "Epoch 4469: Training Loss: 0.20982606212298074 Validation Loss: 0.9648900628089905\n",
      "Epoch 4470: Training Loss: 0.21023303270339966 Validation Loss: 0.9649789333343506\n",
      "Epoch 4471: Training Loss: 0.20943091809749603 Validation Loss: 0.9651971459388733\n",
      "Epoch 4472: Training Loss: 0.21019275983174643 Validation Loss: 0.9650039672851562\n",
      "Epoch 4473: Training Loss: 0.2097779462734858 Validation Loss: 0.9658331871032715\n",
      "Epoch 4474: Training Loss: 0.20921801527341208 Validation Loss: 0.9664624333381653\n",
      "Epoch 4475: Training Loss: 0.20922920604546866 Validation Loss: 0.9665138721466064\n",
      "Epoch 4476: Training Loss: 0.20953894158204397 Validation Loss: 0.9661475419998169\n",
      "Epoch 4477: Training Loss: 0.20938600599765778 Validation Loss: 0.9657827615737915\n",
      "Epoch 4478: Training Loss: 0.20946922401587167 Validation Loss: 0.965861976146698\n",
      "Epoch 4479: Training Loss: 0.2107169677813848 Validation Loss: 0.9652994871139526\n",
      "Epoch 4480: Training Loss: 0.20885499318440756 Validation Loss: 0.9658014178276062\n",
      "Epoch 4481: Training Loss: 0.2109023481607437 Validation Loss: 0.9659157991409302\n",
      "Epoch 4482: Training Loss: 0.20973019301891327 Validation Loss: 0.9655246734619141\n",
      "Epoch 4483: Training Loss: 0.2093387891848882 Validation Loss: 0.9656947255134583\n",
      "Epoch 4484: Training Loss: 0.20870265364646912 Validation Loss: 0.966012179851532\n",
      "Epoch 4485: Training Loss: 0.2089274823665619 Validation Loss: 0.966008186340332\n",
      "Epoch 4486: Training Loss: 0.20867068072160086 Validation Loss: 0.9662872552871704\n",
      "Epoch 4487: Training Loss: 0.20906193554401398 Validation Loss: 0.9661254286766052\n",
      "Epoch 4488: Training Loss: 0.2086782306432724 Validation Loss: 0.9668356776237488\n",
      "Epoch 4489: Training Loss: 0.20839098592599234 Validation Loss: 0.9662843942642212\n",
      "Epoch 4490: Training Loss: 0.20792322357495627 Validation Loss: 0.9657941460609436\n",
      "Epoch 4491: Training Loss: 0.20842239757378897 Validation Loss: 0.9656361937522888\n",
      "Epoch 4492: Training Loss: 0.20893722275892893 Validation Loss: 0.965190589427948\n",
      "Epoch 4493: Training Loss: 0.20845070978005728 Validation Loss: 0.9660134315490723\n",
      "Epoch 4494: Training Loss: 0.20846515397230783 Validation Loss: 0.9659481644630432\n",
      "Epoch 4495: Training Loss: 0.2079992095629374 Validation Loss: 0.9661985635757446\n",
      "Epoch 4496: Training Loss: 0.20800132552782694 Validation Loss: 0.9658608436584473\n",
      "Epoch 4497: Training Loss: 0.20924236377080283 Validation Loss: 0.9660224318504333\n",
      "Epoch 4498: Training Loss: 0.20900983115037283 Validation Loss: 0.965434730052948\n",
      "Epoch 4499: Training Loss: 0.20818168421586355 Validation Loss: 0.9656039476394653\n",
      "Epoch 4500: Training Loss: 0.2077286640803019 Validation Loss: 0.9663550853729248\n",
      "Epoch 4501: Training Loss: 0.2083930422862371 Validation Loss: 0.965960681438446\n",
      "Epoch 4502: Training Loss: 0.20799735685189566 Validation Loss: 0.9656755924224854\n",
      "Epoch 4503: Training Loss: 0.20839192966620126 Validation Loss: 0.9647563695907593\n",
      "Epoch 4504: Training Loss: 0.20892558495203653 Validation Loss: 0.9654029011726379\n",
      "Epoch 4505: Training Loss: 0.20750340322653452 Validation Loss: 0.9658561944961548\n",
      "Epoch 4506: Training Loss: 0.2077852338552475 Validation Loss: 0.9662633538246155\n",
      "Epoch 4507: Training Loss: 0.20743552843729654 Validation Loss: 0.9664151072502136\n",
      "Epoch 4508: Training Loss: 0.2078243593374888 Validation Loss: 0.9666603207588196\n",
      "Epoch 4509: Training Loss: 0.20735121270020804 Validation Loss: 0.9664678573608398\n",
      "Epoch 4510: Training Loss: 0.20681077241897583 Validation Loss: 0.9667132496833801\n",
      "Epoch 4511: Training Loss: 0.20751920839150748 Validation Loss: 0.966772198677063\n",
      "Epoch 4512: Training Loss: 0.20747226476669312 Validation Loss: 0.9660618901252747\n",
      "Epoch 4513: Training Loss: 0.2073329289754232 Validation Loss: 0.9663210511207581\n",
      "Epoch 4514: Training Loss: 0.207608496149381 Validation Loss: 0.96610426902771\n",
      "Epoch 4515: Training Loss: 0.2074266622463862 Validation Loss: 0.9665833711624146\n",
      "Epoch 4516: Training Loss: 0.207494447628657 Validation Loss: 0.96634840965271\n",
      "Epoch 4517: Training Loss: 0.20795209209124246 Validation Loss: 0.9660334587097168\n",
      "Epoch 4518: Training Loss: 0.2071766455968221 Validation Loss: 0.9669548273086548\n",
      "Epoch 4519: Training Loss: 0.20763582487901053 Validation Loss: 0.9670588374137878\n",
      "Epoch 4520: Training Loss: 0.2071026712656021 Validation Loss: 0.9666846990585327\n",
      "Epoch 4521: Training Loss: 0.2070720543464025 Validation Loss: 0.9668378233909607\n",
      "Epoch 4522: Training Loss: 0.20691334207852682 Validation Loss: 0.9670653939247131\n",
      "Epoch 4523: Training Loss: 0.20688540736834207 Validation Loss: 0.966502845287323\n",
      "Epoch 4524: Training Loss: 0.20707881450653076 Validation Loss: 0.9664024710655212\n",
      "Epoch 4525: Training Loss: 0.2063102126121521 Validation Loss: 0.9663800001144409\n",
      "Epoch 4526: Training Loss: 0.20738726357618967 Validation Loss: 0.9668454527854919\n",
      "Epoch 4527: Training Loss: 0.20672799150149027 Validation Loss: 0.9669922590255737\n",
      "Epoch 4528: Training Loss: 0.20704368750254312 Validation Loss: 0.9674041867256165\n",
      "Epoch 4529: Training Loss: 0.20670911173025766 Validation Loss: 0.9672314524650574\n",
      "Epoch 4530: Training Loss: 0.20633100966612497 Validation Loss: 0.9673546552658081\n",
      "Epoch 4531: Training Loss: 0.2085388203461965 Validation Loss: 0.9668803215026855\n",
      "Epoch 4532: Training Loss: 0.20659032464027405 Validation Loss: 0.9667322635650635\n",
      "Epoch 4533: Training Loss: 0.2075572411219279 Validation Loss: 0.9668582677841187\n",
      "Epoch 4534: Training Loss: 0.2061080038547516 Validation Loss: 0.9668099880218506\n",
      "Epoch 4535: Training Loss: 0.20597834388415018 Validation Loss: 0.9671403169631958\n",
      "Epoch 4536: Training Loss: 0.20619613925615946 Validation Loss: 0.9667002558708191\n",
      "Epoch 4537: Training Loss: 0.20569094518820444 Validation Loss: 0.9666756391525269\n",
      "Epoch 4538: Training Loss: 0.2067961444457372 Validation Loss: 0.966511070728302\n",
      "Epoch 4539: Training Loss: 0.20595927039782205 Validation Loss: 0.9667358994483948\n",
      "Epoch 4540: Training Loss: 0.20571690301100412 Validation Loss: 0.9674633741378784\n",
      "Epoch 4541: Training Loss: 0.2056832859913508 Validation Loss: 0.9676219820976257\n",
      "Epoch 4542: Training Loss: 0.2053954005241394 Validation Loss: 0.9671832919120789\n",
      "Epoch 4543: Training Loss: 0.20650538305441538 Validation Loss: 0.96727055311203\n",
      "Epoch 4544: Training Loss: 0.20513173937797546 Validation Loss: 0.9667880535125732\n",
      "Epoch 4545: Training Loss: 0.20671837031841278 Validation Loss: 0.9661741852760315\n",
      "Epoch 4546: Training Loss: 0.2052939385175705 Validation Loss: 0.9664644598960876\n",
      "Epoch 4547: Training Loss: 0.2055009106794993 Validation Loss: 0.9662954211235046\n",
      "Epoch 4548: Training Loss: 0.20554227630297342 Validation Loss: 0.9670533537864685\n",
      "Epoch 4549: Training Loss: 0.2056517998377482 Validation Loss: 0.9670079946517944\n",
      "Epoch 4550: Training Loss: 0.20561003684997559 Validation Loss: 0.9665222764015198\n",
      "Epoch 4551: Training Loss: 0.20622229079405466 Validation Loss: 0.9666116237640381\n",
      "Epoch 4552: Training Loss: 0.20599321524302164 Validation Loss: 0.9668176770210266\n",
      "Epoch 4553: Training Loss: 0.20561211307843527 Validation Loss: 0.9667848348617554\n",
      "Epoch 4554: Training Loss: 0.20602722465991974 Validation Loss: 0.9673416018486023\n",
      "Epoch 4555: Training Loss: 0.2055012583732605 Validation Loss: 0.967477023601532\n",
      "Epoch 4556: Training Loss: 0.2050786813100179 Validation Loss: 0.9671911597251892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4557: Training Loss: 0.20529378950595856 Validation Loss: 0.9670740962028503\n",
      "Epoch 4558: Training Loss: 0.20504213869571686 Validation Loss: 0.9674096703529358\n",
      "Epoch 4559: Training Loss: 0.20476247866948447 Validation Loss: 0.9673586487770081\n",
      "Epoch 4560: Training Loss: 0.20506306986014047 Validation Loss: 0.9670825600624084\n",
      "Epoch 4561: Training Loss: 0.20564574003219604 Validation Loss: 0.9667261242866516\n",
      "Epoch 4562: Training Loss: 0.20411903659502664 Validation Loss: 0.9662641286849976\n",
      "Epoch 4563: Training Loss: 0.20528607567151388 Validation Loss: 0.9665506482124329\n",
      "Epoch 4564: Training Loss: 0.2046460211277008 Validation Loss: 0.9674588441848755\n",
      "Epoch 4565: Training Loss: 0.2046677271525065 Validation Loss: 0.9676234722137451\n",
      "Epoch 4566: Training Loss: 0.2043720285097758 Validation Loss: 0.9677520990371704\n",
      "Epoch 4567: Training Loss: 0.20595358312129974 Validation Loss: 0.9673014879226685\n",
      "Epoch 4568: Training Loss: 0.20433070758978525 Validation Loss: 0.9675867557525635\n",
      "Epoch 4569: Training Loss: 0.20391105115413666 Validation Loss: 0.9673661589622498\n",
      "Epoch 4570: Training Loss: 0.20438491304715475 Validation Loss: 0.9674996137619019\n",
      "Epoch 4571: Training Loss: 0.20542957385381064 Validation Loss: 0.9677123427391052\n",
      "Epoch 4572: Training Loss: 0.20428631206353506 Validation Loss: 0.9673200845718384\n",
      "Epoch 4573: Training Loss: 0.2044695963462194 Validation Loss: 0.9670998454093933\n",
      "Epoch 4574: Training Loss: 0.20448155204455057 Validation Loss: 0.9676329493522644\n",
      "Epoch 4575: Training Loss: 0.2040777305761973 Validation Loss: 0.9675037860870361\n",
      "Epoch 4576: Training Loss: 0.2047061175107956 Validation Loss: 0.9673143625259399\n",
      "Epoch 4577: Training Loss: 0.2042128990093867 Validation Loss: 0.9676475524902344\n",
      "Epoch 4578: Training Loss: 0.2037882556517919 Validation Loss: 0.9666765332221985\n",
      "Epoch 4579: Training Loss: 0.20392712950706482 Validation Loss: 0.9664475321769714\n",
      "Epoch 4580: Training Loss: 0.20380297799905142 Validation Loss: 0.9673578143119812\n",
      "Epoch 4581: Training Loss: 0.20393801232179007 Validation Loss: 0.9668838381767273\n",
      "Epoch 4582: Training Loss: 0.20422901213169098 Validation Loss: 0.9674837589263916\n",
      "Epoch 4583: Training Loss: 0.20463458697001138 Validation Loss: 0.967922568321228\n",
      "Epoch 4584: Training Loss: 0.20374884208043417 Validation Loss: 0.9674179553985596\n",
      "Epoch 4585: Training Loss: 0.20380054414272308 Validation Loss: 0.9675922989845276\n",
      "Epoch 4586: Training Loss: 0.20346699158350626 Validation Loss: 0.9675315022468567\n",
      "Epoch 4587: Training Loss: 0.20345902939637503 Validation Loss: 0.9673664569854736\n",
      "Epoch 4588: Training Loss: 0.2036538620789846 Validation Loss: 0.9674156904220581\n",
      "Epoch 4589: Training Loss: 0.20325501759847006 Validation Loss: 0.9675962924957275\n",
      "Epoch 4590: Training Loss: 0.2024981031815211 Validation Loss: 0.9681457877159119\n",
      "Epoch 4591: Training Loss: 0.20403668781121573 Validation Loss: 0.9681665897369385\n",
      "Epoch 4592: Training Loss: 0.20334195097287497 Validation Loss: 0.9689285755157471\n",
      "Epoch 4593: Training Loss: 0.20380664865175882 Validation Loss: 0.9696768522262573\n",
      "Epoch 4594: Training Loss: 0.2032836675643921 Validation Loss: 0.9691176414489746\n",
      "Epoch 4595: Training Loss: 0.20369443794091543 Validation Loss: 0.9686483144760132\n",
      "Epoch 4596: Training Loss: 0.2034649501244227 Validation Loss: 0.9681578874588013\n",
      "Epoch 4597: Training Loss: 0.20297251145044962 Validation Loss: 0.9676105976104736\n",
      "Epoch 4598: Training Loss: 0.20369536181290945 Validation Loss: 0.9669771194458008\n",
      "Epoch 4599: Training Loss: 0.2028633107741674 Validation Loss: 0.9670836925506592\n",
      "Epoch 4600: Training Loss: 0.2034948766231537 Validation Loss: 0.9673178791999817\n",
      "Epoch 4601: Training Loss: 0.20268900195757547 Validation Loss: 0.9677992463111877\n",
      "Epoch 4602: Training Loss: 0.203057070573171 Validation Loss: 0.9676088094711304\n",
      "Epoch 4603: Training Loss: 0.2035414526859919 Validation Loss: 0.9679118394851685\n",
      "Epoch 4604: Training Loss: 0.20376929640769958 Validation Loss: 0.9679146409034729\n",
      "Epoch 4605: Training Loss: 0.20276827116807303 Validation Loss: 0.9678510427474976\n",
      "Epoch 4606: Training Loss: 0.20252529283364615 Validation Loss: 0.9678661227226257\n",
      "Epoch 4607: Training Loss: 0.2026465187470118 Validation Loss: 0.9683464169502258\n",
      "Epoch 4608: Training Loss: 0.20248505473136902 Validation Loss: 0.9686410427093506\n",
      "Epoch 4609: Training Loss: 0.2034358928600947 Validation Loss: 0.9679622650146484\n",
      "Epoch 4610: Training Loss: 0.20272980133692423 Validation Loss: 0.9682822227478027\n",
      "Epoch 4611: Training Loss: 0.2019397715727488 Validation Loss: 0.9684008359909058\n",
      "Epoch 4612: Training Loss: 0.2022023250659307 Validation Loss: 0.9685666561126709\n",
      "Epoch 4613: Training Loss: 0.20171924928824106 Validation Loss: 0.9689266085624695\n",
      "Epoch 4614: Training Loss: 0.20321164528528848 Validation Loss: 0.9683255553245544\n",
      "Epoch 4615: Training Loss: 0.20411909619967142 Validation Loss: 0.9686264395713806\n",
      "Epoch 4616: Training Loss: 0.20253551999727884 Validation Loss: 0.9684102535247803\n",
      "Epoch 4617: Training Loss: 0.2016740341981252 Validation Loss: 0.9685443043708801\n",
      "Epoch 4618: Training Loss: 0.2017215092976888 Validation Loss: 0.968752384185791\n",
      "Epoch 4619: Training Loss: 0.20182772477467856 Validation Loss: 0.9689018726348877\n",
      "Epoch 4620: Training Loss: 0.20198859771092734 Validation Loss: 0.9687422513961792\n",
      "Epoch 4621: Training Loss: 0.2024024873971939 Validation Loss: 0.9679918885231018\n",
      "Epoch 4622: Training Loss: 0.2013462334871292 Validation Loss: 0.96812504529953\n",
      "Epoch 4623: Training Loss: 0.20151921113332114 Validation Loss: 0.968383252620697\n",
      "Epoch 4624: Training Loss: 0.20173093676567078 Validation Loss: 0.9684737920761108\n",
      "Epoch 4625: Training Loss: 0.20212495823701224 Validation Loss: 0.9682270288467407\n",
      "Epoch 4626: Training Loss: 0.20237074295679727 Validation Loss: 0.9685496091842651\n",
      "Epoch 4627: Training Loss: 0.20154975851376852 Validation Loss: 0.9682684540748596\n",
      "Epoch 4628: Training Loss: 0.20152069628238678 Validation Loss: 0.9687830805778503\n",
      "Epoch 4629: Training Loss: 0.20162246624628702 Validation Loss: 0.9686644673347473\n",
      "Epoch 4630: Training Loss: 0.20154527326424918 Validation Loss: 0.9687519073486328\n",
      "Epoch 4631: Training Loss: 0.20136274894078574 Validation Loss: 0.9686943292617798\n",
      "Epoch 4632: Training Loss: 0.20194911460081735 Validation Loss: 0.9682570695877075\n",
      "Epoch 4633: Training Loss: 0.20146712164084116 Validation Loss: 0.9684359431266785\n",
      "Epoch 4634: Training Loss: 0.2019818772872289 Validation Loss: 0.9684449434280396\n",
      "Epoch 4635: Training Loss: 0.20136988162994385 Validation Loss: 0.9688876271247864\n",
      "Epoch 4636: Training Loss: 0.20109692215919495 Validation Loss: 0.9686194658279419\n",
      "Epoch 4637: Training Loss: 0.20294460157553354 Validation Loss: 0.9695958495140076\n",
      "Epoch 4638: Training Loss: 0.20147750775019327 Validation Loss: 0.9694729447364807\n",
      "Epoch 4639: Training Loss: 0.20122691492239633 Validation Loss: 0.96907639503479\n",
      "Epoch 4640: Training Loss: 0.20152048766613007 Validation Loss: 0.9690423011779785\n",
      "Epoch 4641: Training Loss: 0.2014577587445577 Validation Loss: 0.9684210419654846\n",
      "Epoch 4642: Training Loss: 0.20064293841520944 Validation Loss: 0.9683327674865723\n",
      "Epoch 4643: Training Loss: 0.20076811810334524 Validation Loss: 0.9687682390213013\n",
      "Epoch 4644: Training Loss: 0.20068819324175516 Validation Loss: 0.9689001441001892\n",
      "Epoch 4645: Training Loss: 0.2003354380528132 Validation Loss: 0.969377338886261\n",
      "Epoch 4646: Training Loss: 0.2006954848766327 Validation Loss: 0.9686877131462097\n",
      "Epoch 4647: Training Loss: 0.20057673752307892 Validation Loss: 0.9681462049484253\n",
      "Epoch 4648: Training Loss: 0.20090627670288086 Validation Loss: 0.9683395028114319\n",
      "Epoch 4649: Training Loss: 0.20015102624893188 Validation Loss: 0.9686313271522522\n",
      "Epoch 4650: Training Loss: 0.20087974766890207 Validation Loss: 0.9687636494636536\n",
      "Epoch 4651: Training Loss: 0.2002376914024353 Validation Loss: 0.9693326354026794\n",
      "Epoch 4652: Training Loss: 0.20027982195218405 Validation Loss: 0.9700654745101929\n",
      "Epoch 4653: Training Loss: 0.20053082704544067 Validation Loss: 0.9698833227157593\n",
      "Epoch 4654: Training Loss: 0.2012820690870285 Validation Loss: 0.9696955680847168\n",
      "Epoch 4655: Training Loss: 0.20007866621017456 Validation Loss: 0.9690544009208679\n",
      "Epoch 4656: Training Loss: 0.20036515593528748 Validation Loss: 0.9692994356155396\n",
      "Epoch 4657: Training Loss: 0.20056202510992685 Validation Loss: 0.9691699147224426\n",
      "Epoch 4658: Training Loss: 0.2007168581088384 Validation Loss: 0.9690700173377991\n",
      "Epoch 4659: Training Loss: 0.20044135053952536 Validation Loss: 0.968641996383667\n",
      "Epoch 4660: Training Loss: 0.20013903578122458 Validation Loss: 0.9696491956710815\n",
      "Epoch 4661: Training Loss: 0.20033080875873566 Validation Loss: 0.9693284034729004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4662: Training Loss: 0.19996614257494608 Validation Loss: 0.9691476225852966\n",
      "Epoch 4663: Training Loss: 0.19974694152673086 Validation Loss: 0.969508707523346\n",
      "Epoch 4664: Training Loss: 0.1998403569062551 Validation Loss: 0.9695011973381042\n",
      "Epoch 4665: Training Loss: 0.20041936139265695 Validation Loss: 0.9695687294006348\n",
      "Epoch 4666: Training Loss: 0.2000123212734858 Validation Loss: 0.9695566892623901\n",
      "Epoch 4667: Training Loss: 0.19962802032629648 Validation Loss: 0.9696173071861267\n",
      "Epoch 4668: Training Loss: 0.19957013924916586 Validation Loss: 0.9693576097488403\n",
      "Epoch 4669: Training Loss: 0.1994389295578003 Validation Loss: 0.9692379832267761\n",
      "Epoch 4670: Training Loss: 0.2000486304362615 Validation Loss: 0.9688927531242371\n",
      "Epoch 4671: Training Loss: 0.1994395653406779 Validation Loss: 0.9686533808708191\n",
      "Epoch 4672: Training Loss: 0.19958087801933289 Validation Loss: 0.9682206511497498\n",
      "Epoch 4673: Training Loss: 0.19943255186080933 Validation Loss: 0.9687716960906982\n",
      "Epoch 4674: Training Loss: 0.1993419031302134 Validation Loss: 0.9692647457122803\n",
      "Epoch 4675: Training Loss: 0.19936159253120422 Validation Loss: 0.9698706269264221\n",
      "Epoch 4676: Training Loss: 0.19927861293156943 Validation Loss: 0.9704554080963135\n",
      "Epoch 4677: Training Loss: 0.2000960906346639 Validation Loss: 0.9705991744995117\n",
      "Epoch 4678: Training Loss: 0.199625626206398 Validation Loss: 0.9707865715026855\n",
      "Epoch 4679: Training Loss: 0.19920145471890768 Validation Loss: 0.9704883694648743\n",
      "Epoch 4680: Training Loss: 0.19900621473789215 Validation Loss: 0.9696888327598572\n",
      "Epoch 4681: Training Loss: 0.19881685574849448 Validation Loss: 0.9690380692481995\n",
      "Epoch 4682: Training Loss: 0.19873276352882385 Validation Loss: 0.9685810804367065\n",
      "Epoch 4683: Training Loss: 0.19793844719727835 Validation Loss: 0.9683505892753601\n",
      "Epoch 4684: Training Loss: 0.19916983942190805 Validation Loss: 0.9682983160018921\n",
      "Epoch 4685: Training Loss: 0.19881478945414224 Validation Loss: 0.9687305688858032\n",
      "Epoch 4686: Training Loss: 0.19984160363674164 Validation Loss: 0.9692753553390503\n",
      "Epoch 4687: Training Loss: 0.1984817534685135 Validation Loss: 0.9697461724281311\n",
      "Epoch 4688: Training Loss: 0.19984452426433563 Validation Loss: 0.9696387648582458\n",
      "Epoch 4689: Training Loss: 0.199113259712855 Validation Loss: 0.9696824550628662\n",
      "Epoch 4690: Training Loss: 0.1986358811457952 Validation Loss: 0.9704517722129822\n",
      "Epoch 4691: Training Loss: 0.19828753173351288 Validation Loss: 0.9702247381210327\n",
      "Epoch 4692: Training Loss: 0.19838752845923105 Validation Loss: 0.9697710275650024\n",
      "Epoch 4693: Training Loss: 0.19834163784980774 Validation Loss: 0.9700049757957458\n",
      "Epoch 4694: Training Loss: 0.19839100539684296 Validation Loss: 0.9700510501861572\n",
      "Epoch 4695: Training Loss: 0.1983583172162374 Validation Loss: 0.9707795977592468\n",
      "Epoch 4696: Training Loss: 0.19711301724116007 Validation Loss: 0.9708914756774902\n",
      "Epoch 4697: Training Loss: 0.19817540049552917 Validation Loss: 0.9704607129096985\n",
      "Epoch 4698: Training Loss: 0.19787338376045227 Validation Loss: 0.9696083664894104\n",
      "Epoch 4699: Training Loss: 0.19823352992534637 Validation Loss: 0.9693754315376282\n",
      "Epoch 4700: Training Loss: 0.19797049462795258 Validation Loss: 0.9694367051124573\n",
      "Epoch 4701: Training Loss: 0.19810298581918082 Validation Loss: 0.9696173667907715\n",
      "Epoch 4702: Training Loss: 0.1992028405268987 Validation Loss: 0.9695961475372314\n",
      "Epoch 4703: Training Loss: 0.19815872609615326 Validation Loss: 0.969585120677948\n",
      "Epoch 4704: Training Loss: 0.19789577523867288 Validation Loss: 0.969664990901947\n",
      "Epoch 4705: Training Loss: 0.19777589539686838 Validation Loss: 0.9694118499755859\n",
      "Epoch 4706: Training Loss: 0.1980188637971878 Validation Loss: 0.9708070158958435\n",
      "Epoch 4707: Training Loss: 0.1981817682584127 Validation Loss: 0.9704652428627014\n",
      "Epoch 4708: Training Loss: 0.19755633672078451 Validation Loss: 0.9705707430839539\n",
      "Epoch 4709: Training Loss: 0.19769797722498575 Validation Loss: 0.9710772037506104\n",
      "Epoch 4710: Training Loss: 0.19754675030708313 Validation Loss: 0.9707852602005005\n",
      "Epoch 4711: Training Loss: 0.19777879118919373 Validation Loss: 0.9705119132995605\n",
      "Epoch 4712: Training Loss: 0.19804597397645315 Validation Loss: 0.9706103801727295\n",
      "Epoch 4713: Training Loss: 0.20108170807361603 Validation Loss: 0.970922589302063\n",
      "Epoch 4714: Training Loss: 0.19772757589817047 Validation Loss: 0.9701006412506104\n",
      "Epoch 4715: Training Loss: 0.19759645561377207 Validation Loss: 0.9697186946868896\n",
      "Epoch 4716: Training Loss: 0.19729879995187125 Validation Loss: 0.970008909702301\n",
      "Epoch 4717: Training Loss: 0.19719637433687845 Validation Loss: 0.96993488073349\n",
      "Epoch 4718: Training Loss: 0.19784127175807953 Validation Loss: 0.9707245826721191\n",
      "Epoch 4719: Training Loss: 0.19685765107472739 Validation Loss: 0.9706820845603943\n",
      "Epoch 4720: Training Loss: 0.19667532046635947 Validation Loss: 0.9703792929649353\n",
      "Epoch 4721: Training Loss: 0.19779261449972788 Validation Loss: 0.9695050716400146\n",
      "Epoch 4722: Training Loss: 0.19752913216749826 Validation Loss: 0.9689190983772278\n",
      "Epoch 4723: Training Loss: 0.1968896985054016 Validation Loss: 0.9693006873130798\n",
      "Epoch 4724: Training Loss: 0.19735707839330038 Validation Loss: 0.9689407348632812\n",
      "Epoch 4725: Training Loss: 0.19685769577821097 Validation Loss: 0.9700912833213806\n",
      "Epoch 4726: Training Loss: 0.19687415659427643 Validation Loss: 0.970243513584137\n",
      "Epoch 4727: Training Loss: 0.19706234335899353 Validation Loss: 0.9695495963096619\n",
      "Epoch 4728: Training Loss: 0.1969402382771174 Validation Loss: 0.9699392914772034\n",
      "Epoch 4729: Training Loss: 0.19747744997342428 Validation Loss: 0.9699482321739197\n",
      "Epoch 4730: Training Loss: 0.19677802423636118 Validation Loss: 0.9708452224731445\n",
      "Epoch 4731: Training Loss: 0.1971018761396408 Validation Loss: 0.9712459444999695\n",
      "Epoch 4732: Training Loss: 0.19638230403264365 Validation Loss: 0.9707786440849304\n",
      "Epoch 4733: Training Loss: 0.19685194392999014 Validation Loss: 0.9711641669273376\n",
      "Epoch 4734: Training Loss: 0.1967185139656067 Validation Loss: 0.9715778231620789\n",
      "Epoch 4735: Training Loss: 0.19642150402069092 Validation Loss: 0.9711107611656189\n",
      "Epoch 4736: Training Loss: 0.19658577938874564 Validation Loss: 0.9713225960731506\n",
      "Epoch 4737: Training Loss: 0.19762654602527618 Validation Loss: 0.971517026424408\n",
      "Epoch 4738: Training Loss: 0.19630197683970133 Validation Loss: 0.9712432622909546\n",
      "Epoch 4739: Training Loss: 0.19585546851158142 Validation Loss: 0.9706211686134338\n",
      "Epoch 4740: Training Loss: 0.1961974302927653 Validation Loss: 0.9706403613090515\n",
      "Epoch 4741: Training Loss: 0.1963979701201121 Validation Loss: 0.9708869457244873\n",
      "Epoch 4742: Training Loss: 0.1970370610555013 Validation Loss: 0.9706211686134338\n",
      "Epoch 4743: Training Loss: 0.1957920640707016 Validation Loss: 0.9708991050720215\n",
      "Epoch 4744: Training Loss: 0.19607648253440857 Validation Loss: 0.9707754850387573\n",
      "Epoch 4745: Training Loss: 0.1961104025443395 Validation Loss: 0.9702022075653076\n",
      "Epoch 4746: Training Loss: 0.1959882229566574 Validation Loss: 0.9700936079025269\n",
      "Epoch 4747: Training Loss: 0.195921724041303 Validation Loss: 0.9701330661773682\n",
      "Epoch 4748: Training Loss: 0.19589624802271524 Validation Loss: 0.9705383777618408\n",
      "Epoch 4749: Training Loss: 0.19619553287823996 Validation Loss: 0.9708017706871033\n",
      "Epoch 4750: Training Loss: 0.19519541164239249 Validation Loss: 0.9710414409637451\n",
      "Epoch 4751: Training Loss: 0.1961326003074646 Validation Loss: 0.9703406095504761\n",
      "Epoch 4752: Training Loss: 0.19600940744082132 Validation Loss: 0.9704164266586304\n",
      "Epoch 4753: Training Loss: 0.1965603232383728 Validation Loss: 0.970374584197998\n",
      "Epoch 4754: Training Loss: 0.1963293453057607 Validation Loss: 0.9710538983345032\n",
      "Epoch 4755: Training Loss: 0.19593271613121033 Validation Loss: 0.9717490077018738\n",
      "Epoch 4756: Training Loss: 0.19663150111834207 Validation Loss: 0.9715657830238342\n",
      "Epoch 4757: Training Loss: 0.1956986685593923 Validation Loss: 0.9719080924987793\n",
      "Epoch 4758: Training Loss: 0.19536087910334268 Validation Loss: 0.972210168838501\n",
      "Epoch 4759: Training Loss: 0.19469200571378073 Validation Loss: 0.9716689586639404\n",
      "Epoch 4760: Training Loss: 0.1955540875593821 Validation Loss: 0.9719282984733582\n",
      "Epoch 4761: Training Loss: 0.19539400935173035 Validation Loss: 0.9718733429908752\n",
      "Epoch 4762: Training Loss: 0.19497249027093252 Validation Loss: 0.9708240628242493\n",
      "Epoch 4763: Training Loss: 0.19600495199362436 Validation Loss: 0.9710050225257874\n",
      "Epoch 4764: Training Loss: 0.19511430958906809 Validation Loss: 0.9706982970237732\n",
      "Epoch 4765: Training Loss: 0.19476930300394693 Validation Loss: 0.9707097411155701\n",
      "Epoch 4766: Training Loss: 0.19530932108561197 Validation Loss: 0.971014142036438\n",
      "Epoch 4767: Training Loss: 0.1947928269704183 Validation Loss: 0.9719111323356628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4768: Training Loss: 0.19491558273633322 Validation Loss: 0.9711655378341675\n",
      "Epoch 4769: Training Loss: 0.19474330047766367 Validation Loss: 0.9710258841514587\n",
      "Epoch 4770: Training Loss: 0.1940245678027471 Validation Loss: 0.9711347818374634\n",
      "Epoch 4771: Training Loss: 0.1945717732111613 Validation Loss: 0.9716275930404663\n",
      "Epoch 4772: Training Loss: 0.19458640615145364 Validation Loss: 0.971987783908844\n",
      "Epoch 4773: Training Loss: 0.19562876224517822 Validation Loss: 0.9719821810722351\n",
      "Epoch 4774: Training Loss: 0.1945075144370397 Validation Loss: 0.972399115562439\n",
      "Epoch 4775: Training Loss: 0.19560924172401428 Validation Loss: 0.9723548293113708\n",
      "Epoch 4776: Training Loss: 0.19452779491742453 Validation Loss: 0.9721844792366028\n",
      "Epoch 4777: Training Loss: 0.1953865389029185 Validation Loss: 0.9722987413406372\n",
      "Epoch 4778: Training Loss: 0.19434966643651327 Validation Loss: 0.9724066257476807\n",
      "Epoch 4779: Training Loss: 0.19488617281119028 Validation Loss: 0.9722733497619629\n",
      "Epoch 4780: Training Loss: 0.1943782518307368 Validation Loss: 0.971674919128418\n",
      "Epoch 4781: Training Loss: 0.193960835536321 Validation Loss: 0.9710233211517334\n",
      "Epoch 4782: Training Loss: 0.1941191703081131 Validation Loss: 0.9705231189727783\n",
      "Epoch 4783: Training Loss: 0.19499856233596802 Validation Loss: 0.9709569215774536\n",
      "Epoch 4784: Training Loss: 0.1940875401099523 Validation Loss: 0.9713122248649597\n",
      "Epoch 4785: Training Loss: 0.1945777436097463 Validation Loss: 0.9713265895843506\n",
      "Epoch 4786: Training Loss: 0.19517391423384348 Validation Loss: 0.9725593328475952\n",
      "Epoch 4787: Training Loss: 0.19452021519343057 Validation Loss: 0.9737075567245483\n",
      "Epoch 4788: Training Loss: 0.19371800621350607 Validation Loss: 0.9740790128707886\n",
      "Epoch 4789: Training Loss: 0.1943016598622004 Validation Loss: 0.9739035367965698\n",
      "Epoch 4790: Training Loss: 0.1958190699418386 Validation Loss: 0.9738109111785889\n",
      "Epoch 4791: Training Loss: 0.19446049630641937 Validation Loss: 0.9727033972740173\n",
      "Epoch 4792: Training Loss: 0.19378962616125742 Validation Loss: 0.9720728397369385\n",
      "Epoch 4793: Training Loss: 0.193844144543012 Validation Loss: 0.9706582427024841\n",
      "Epoch 4794: Training Loss: 0.19384528199831644 Validation Loss: 0.9703773260116577\n",
      "Epoch 4795: Training Loss: 0.19375894963741302 Validation Loss: 0.9702957272529602\n",
      "Epoch 4796: Training Loss: 0.19271126886208853 Validation Loss: 0.9716087579727173\n",
      "Epoch 4797: Training Loss: 0.19463457663853964 Validation Loss: 0.9720169901847839\n",
      "Epoch 4798: Training Loss: 0.1959844926993052 Validation Loss: 0.9716934561729431\n",
      "Epoch 4799: Training Loss: 0.19360815982023874 Validation Loss: 0.9715266227722168\n",
      "Epoch 4800: Training Loss: 0.19342543681462607 Validation Loss: 0.9719511270523071\n",
      "Epoch 4801: Training Loss: 0.19414410988489786 Validation Loss: 0.9722844362258911\n",
      "Epoch 4802: Training Loss: 0.19366387029488882 Validation Loss: 0.9724771976470947\n",
      "Epoch 4803: Training Loss: 0.19321063657601675 Validation Loss: 0.9726518988609314\n",
      "Epoch 4804: Training Loss: 0.19324249029159546 Validation Loss: 0.9721771478652954\n",
      "Epoch 4805: Training Loss: 0.19480262200037637 Validation Loss: 0.972185492515564\n",
      "Epoch 4806: Training Loss: 0.1925707310438156 Validation Loss: 0.9722617864608765\n",
      "Epoch 4807: Training Loss: 0.1937481015920639 Validation Loss: 0.9718616008758545\n",
      "Epoch 4808: Training Loss: 0.19309664765993753 Validation Loss: 0.9724427461624146\n",
      "Epoch 4809: Training Loss: 0.19286289314428964 Validation Loss: 0.9724563956260681\n",
      "Epoch 4810: Training Loss: 0.193071777621905 Validation Loss: 0.9730587005615234\n",
      "Epoch 4811: Training Loss: 0.19286720951398215 Validation Loss: 0.9729844331741333\n",
      "Epoch 4812: Training Loss: 0.19318407773971558 Validation Loss: 0.9728628993034363\n",
      "Epoch 4813: Training Loss: 0.19288883606592813 Validation Loss: 0.9719979166984558\n",
      "Epoch 4814: Training Loss: 0.19290165603160858 Validation Loss: 0.9714477062225342\n",
      "Epoch 4815: Training Loss: 0.19299406309922537 Validation Loss: 0.971713662147522\n",
      "Epoch 4816: Training Loss: 0.19332056740919748 Validation Loss: 0.972296953201294\n",
      "Epoch 4817: Training Loss: 0.19300326704978943 Validation Loss: 0.9728880524635315\n",
      "Epoch 4818: Training Loss: 0.1926748255888621 Validation Loss: 0.9731380939483643\n",
      "Epoch 4819: Training Loss: 0.19250909984111786 Validation Loss: 0.9726223945617676\n",
      "Epoch 4820: Training Loss: 0.1929607888062795 Validation Loss: 0.9730503559112549\n",
      "Epoch 4821: Training Loss: 0.19268587231636047 Validation Loss: 0.972502589225769\n",
      "Epoch 4822: Training Loss: 0.19261876245339712 Validation Loss: 0.9729505777359009\n",
      "Epoch 4823: Training Loss: 0.19256005187829336 Validation Loss: 0.9729882478713989\n",
      "Epoch 4824: Training Loss: 0.1928037703037262 Validation Loss: 0.9732532501220703\n",
      "Epoch 4825: Training Loss: 0.19223118821779886 Validation Loss: 0.9732227921485901\n",
      "Epoch 4826: Training Loss: 0.19143226742744446 Validation Loss: 0.9734411239624023\n",
      "Epoch 4827: Training Loss: 0.19284208118915558 Validation Loss: 0.9727615118026733\n",
      "Epoch 4828: Training Loss: 0.19331519305706024 Validation Loss: 0.973335862159729\n",
      "Epoch 4829: Training Loss: 0.1924302081267039 Validation Loss: 0.9721612334251404\n",
      "Epoch 4830: Training Loss: 0.19213188191254935 Validation Loss: 0.9718729853630066\n",
      "Epoch 4831: Training Loss: 0.1918277939160665 Validation Loss: 0.9725778102874756\n",
      "Epoch 4832: Training Loss: 0.19220930337905884 Validation Loss: 0.9720490574836731\n",
      "Epoch 4833: Training Loss: 0.19218067824840546 Validation Loss: 0.972163200378418\n",
      "Epoch 4834: Training Loss: 0.19192624588807425 Validation Loss: 0.9726498126983643\n",
      "Epoch 4835: Training Loss: 0.19269510606924692 Validation Loss: 0.9730204939842224\n",
      "Epoch 4836: Training Loss: 0.19177628556887308 Validation Loss: 0.9733853340148926\n",
      "Epoch 4837: Training Loss: 0.19292605916659036 Validation Loss: 0.9734571576118469\n",
      "Epoch 4838: Training Loss: 0.19182970623175302 Validation Loss: 0.972903311252594\n",
      "Epoch 4839: Training Loss: 0.19258188704649606 Validation Loss: 0.9727845788002014\n",
      "Epoch 4840: Training Loss: 0.1915984849135081 Validation Loss: 0.9730547070503235\n",
      "Epoch 4841: Training Loss: 0.19180086255073547 Validation Loss: 0.9733421206474304\n",
      "Epoch 4842: Training Loss: 0.1922110617160797 Validation Loss: 0.9735287427902222\n",
      "Epoch 4843: Training Loss: 0.19324111938476562 Validation Loss: 0.9740279316902161\n",
      "Epoch 4844: Training Loss: 0.191598042845726 Validation Loss: 0.9737672209739685\n",
      "Epoch 4845: Training Loss: 0.19148428241411844 Validation Loss: 0.9733132719993591\n",
      "Epoch 4846: Training Loss: 0.1917532483736674 Validation Loss: 0.9734013080596924\n",
      "Epoch 4847: Training Loss: 0.1923374483982722 Validation Loss: 0.9733273983001709\n",
      "Epoch 4848: Training Loss: 0.19162289798259735 Validation Loss: 0.9728745222091675\n",
      "Epoch 4849: Training Loss: 0.19119282563527426 Validation Loss: 0.9725498557090759\n",
      "Epoch 4850: Training Loss: 0.19286605715751648 Validation Loss: 0.9726034998893738\n",
      "Epoch 4851: Training Loss: 0.19125275810559592 Validation Loss: 0.9726936221122742\n",
      "Epoch 4852: Training Loss: 0.19164412717024484 Validation Loss: 0.9736313819885254\n",
      "Epoch 4853: Training Loss: 0.1913140912850698 Validation Loss: 0.9736400246620178\n",
      "Epoch 4854: Training Loss: 0.19077912966410318 Validation Loss: 0.9738026857376099\n",
      "Epoch 4855: Training Loss: 0.191364586353302 Validation Loss: 0.9739489555358887\n",
      "Epoch 4856: Training Loss: 0.1910803218682607 Validation Loss: 0.9737513661384583\n",
      "Epoch 4857: Training Loss: 0.1904310683409373 Validation Loss: 0.9735182523727417\n",
      "Epoch 4858: Training Loss: 0.1907526801029841 Validation Loss: 0.9737869501113892\n",
      "Epoch 4859: Training Loss: 0.19081124663352966 Validation Loss: 0.9736427068710327\n",
      "Epoch 4860: Training Loss: 0.19149420658747354 Validation Loss: 0.9732572436332703\n",
      "Epoch 4861: Training Loss: 0.19093873103459677 Validation Loss: 0.972599983215332\n",
      "Epoch 4862: Training Loss: 0.19079333543777466 Validation Loss: 0.9729771018028259\n",
      "Epoch 4863: Training Loss: 0.19074039657910666 Validation Loss: 0.9730244278907776\n",
      "Epoch 4864: Training Loss: 0.1916919598976771 Validation Loss: 0.9732983112335205\n",
      "Epoch 4865: Training Loss: 0.19089657068252563 Validation Loss: 0.9736014008522034\n",
      "Epoch 4866: Training Loss: 0.18992657959461212 Validation Loss: 0.9740925431251526\n",
      "Epoch 4867: Training Loss: 0.19051110247770944 Validation Loss: 0.973576545715332\n",
      "Epoch 4868: Training Loss: 0.19039925932884216 Validation Loss: 0.9739121198654175\n",
      "Epoch 4869: Training Loss: 0.19296352565288544 Validation Loss: 0.973730206489563\n",
      "Epoch 4870: Training Loss: 0.1912408322095871 Validation Loss: 0.9739055037498474\n",
      "Epoch 4871: Training Loss: 0.19051386415958405 Validation Loss: 0.9744203686714172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4872: Training Loss: 0.18994487325350443 Validation Loss: 0.9741253852844238\n",
      "Epoch 4873: Training Loss: 0.19031251470247904 Validation Loss: 0.9737199544906616\n",
      "Epoch 4874: Training Loss: 0.19108034173647562 Validation Loss: 0.9734499454498291\n",
      "Epoch 4875: Training Loss: 0.1908094584941864 Validation Loss: 0.9734807014465332\n",
      "Epoch 4876: Training Loss: 0.1897443930308024 Validation Loss: 0.9735767841339111\n",
      "Epoch 4877: Training Loss: 0.19043220579624176 Validation Loss: 0.9737091660499573\n",
      "Epoch 4878: Training Loss: 0.1906857987244924 Validation Loss: 0.9735607504844666\n",
      "Epoch 4879: Training Loss: 0.1909846564133962 Validation Loss: 0.9736999869346619\n",
      "Epoch 4880: Training Loss: 0.19073855380217233 Validation Loss: 0.9740891456604004\n",
      "Epoch 4881: Training Loss: 0.1900415321191152 Validation Loss: 0.9740862846374512\n",
      "Epoch 4882: Training Loss: 0.1904620627562205 Validation Loss: 0.9738989472389221\n",
      "Epoch 4883: Training Loss: 0.1903825749953588 Validation Loss: 0.9734511971473694\n",
      "Epoch 4884: Training Loss: 0.1900742600361506 Validation Loss: 0.9738687872886658\n",
      "Epoch 4885: Training Loss: 0.189730371038119 Validation Loss: 0.9738829135894775\n",
      "Epoch 4886: Training Loss: 0.1897311806678772 Validation Loss: 0.9743368029594421\n",
      "Epoch 4887: Training Loss: 0.18953914940357208 Validation Loss: 0.9746417999267578\n",
      "Epoch 4888: Training Loss: 0.18982100983460745 Validation Loss: 0.9748241901397705\n",
      "Epoch 4889: Training Loss: 0.18960964679718018 Validation Loss: 0.9744856953620911\n",
      "Epoch 4890: Training Loss: 0.19028597076733908 Validation Loss: 0.973669707775116\n",
      "Epoch 4891: Training Loss: 0.1906298796335856 Validation Loss: 0.9740625619888306\n",
      "Epoch 4892: Training Loss: 0.19024337331453958 Validation Loss: 0.9743377566337585\n",
      "Epoch 4893: Training Loss: 0.18964889645576477 Validation Loss: 0.9742088317871094\n",
      "Epoch 4894: Training Loss: 0.1896106352408727 Validation Loss: 0.9741496443748474\n",
      "Epoch 4895: Training Loss: 0.18904741605122885 Validation Loss: 0.9731701612472534\n",
      "Epoch 4896: Training Loss: 0.18932637572288513 Validation Loss: 0.9734122157096863\n",
      "Epoch 4897: Training Loss: 0.1896159996589025 Validation Loss: 0.9734817147254944\n",
      "Epoch 4898: Training Loss: 0.18959140280882517 Validation Loss: 0.974073588848114\n",
      "Epoch 4899: Training Loss: 0.18915418783823648 Validation Loss: 0.9735614061355591\n",
      "Epoch 4900: Training Loss: 0.1891497770945231 Validation Loss: 0.9739314913749695\n",
      "Epoch 4901: Training Loss: 0.19021952152252197 Validation Loss: 0.9744060039520264\n",
      "Epoch 4902: Training Loss: 0.18911142150561014 Validation Loss: 0.9744880795478821\n",
      "Epoch 4903: Training Loss: 0.18945336838563284 Validation Loss: 0.9741254448890686\n",
      "Epoch 4904: Training Loss: 0.19001465539137521 Validation Loss: 0.9736649394035339\n",
      "Epoch 4905: Training Loss: 0.18878506124019623 Validation Loss: 0.9739755988121033\n",
      "Epoch 4906: Training Loss: 0.18968998889128366 Validation Loss: 0.9741030931472778\n",
      "Epoch 4907: Training Loss: 0.18898809949556986 Validation Loss: 0.9748457670211792\n",
      "Epoch 4908: Training Loss: 0.18887548645337424 Validation Loss: 0.9741400480270386\n",
      "Epoch 4909: Training Loss: 0.18907578786214194 Validation Loss: 0.9740716814994812\n",
      "Epoch 4910: Training Loss: 0.1887092043956121 Validation Loss: 0.974870502948761\n",
      "Epoch 4911: Training Loss: 0.18863501151402792 Validation Loss: 0.9752953052520752\n",
      "Epoch 4912: Training Loss: 0.18948760131994882 Validation Loss: 0.9756717085838318\n",
      "Epoch 4913: Training Loss: 0.18840723236401877 Validation Loss: 0.9756704568862915\n",
      "Epoch 4914: Training Loss: 0.18876130878925323 Validation Loss: 0.9755498766899109\n",
      "Epoch 4915: Training Loss: 0.18899422387282053 Validation Loss: 0.9755977988243103\n",
      "Epoch 4916: Training Loss: 0.18822303414344788 Validation Loss: 0.9752107262611389\n",
      "Epoch 4917: Training Loss: 0.18839542071024576 Validation Loss: 0.9749760627746582\n",
      "Epoch 4918: Training Loss: 0.18810505171616873 Validation Loss: 0.9744906425476074\n",
      "Epoch 4919: Training Loss: 0.18829297025998434 Validation Loss: 0.9744486808776855\n",
      "Epoch 4920: Training Loss: 0.18815901378790537 Validation Loss: 0.9742855429649353\n",
      "Epoch 4921: Training Loss: 0.18919243415196738 Validation Loss: 0.9742451310157776\n",
      "Epoch 4922: Training Loss: 0.18976397812366486 Validation Loss: 0.9740880727767944\n",
      "Epoch 4923: Training Loss: 0.18877512216567993 Validation Loss: 0.9739153981208801\n",
      "Epoch 4924: Training Loss: 0.18835904200871786 Validation Loss: 0.9746087789535522\n",
      "Epoch 4925: Training Loss: 0.18844023843606314 Validation Loss: 0.9746769070625305\n",
      "Epoch 4926: Training Loss: 0.18862930436929068 Validation Loss: 0.9753560423851013\n",
      "Epoch 4927: Training Loss: 0.1878694941600164 Validation Loss: 0.9750198125839233\n",
      "Epoch 4928: Training Loss: 0.18891428411006927 Validation Loss: 0.9752565026283264\n",
      "Epoch 4929: Training Loss: 0.18786375224590302 Validation Loss: 0.9755733013153076\n",
      "Epoch 4930: Training Loss: 0.18919984499613443 Validation Loss: 0.9757159948348999\n",
      "Epoch 4931: Training Loss: 0.1879339118798574 Validation Loss: 0.9758290648460388\n",
      "Epoch 4932: Training Loss: 0.18839955826600394 Validation Loss: 0.9756540656089783\n",
      "Epoch 4933: Training Loss: 0.18771270910898843 Validation Loss: 0.9752859473228455\n",
      "Epoch 4934: Training Loss: 0.1879531741142273 Validation Loss: 0.9745654463768005\n",
      "Epoch 4935: Training Loss: 0.18756475547949472 Validation Loss: 0.9740994572639465\n",
      "Epoch 4936: Training Loss: 0.18763457735379538 Validation Loss: 0.9741812348365784\n",
      "Epoch 4937: Training Loss: 0.18750344713528952 Validation Loss: 0.974294126033783\n",
      "Epoch 4938: Training Loss: 0.18753225107987723 Validation Loss: 0.9740570783615112\n",
      "Epoch 4939: Training Loss: 0.1880907416343689 Validation Loss: 0.9749715328216553\n",
      "Epoch 4940: Training Loss: 0.18719232082366943 Validation Loss: 0.9751619696617126\n",
      "Epoch 4941: Training Loss: 0.18773747980594635 Validation Loss: 0.9752052426338196\n",
      "Epoch 4942: Training Loss: 0.18805750707785288 Validation Loss: 0.9757139682769775\n",
      "Epoch 4943: Training Loss: 0.18810831507047018 Validation Loss: 0.9757888913154602\n",
      "Epoch 4944: Training Loss: 0.18714738885561624 Validation Loss: 0.9758974313735962\n",
      "Epoch 4945: Training Loss: 0.18775588274002075 Validation Loss: 0.9762299656867981\n",
      "Epoch 4946: Training Loss: 0.18732817471027374 Validation Loss: 0.9756277799606323\n",
      "Epoch 4947: Training Loss: 0.18723806738853455 Validation Loss: 0.9753681421279907\n",
      "Epoch 4948: Training Loss: 0.1870675583680471 Validation Loss: 0.9751073122024536\n",
      "Epoch 4949: Training Loss: 0.18707426885763803 Validation Loss: 0.975222110748291\n",
      "Epoch 4950: Training Loss: 0.18744311233361563 Validation Loss: 0.9752978086471558\n",
      "Epoch 4951: Training Loss: 0.18640993535518646 Validation Loss: 0.9762250185012817\n",
      "Epoch 4952: Training Loss: 0.1868052730957667 Validation Loss: 0.9763745069503784\n",
      "Epoch 4953: Training Loss: 0.18699589371681213 Validation Loss: 0.9765142202377319\n",
      "Epoch 4954: Training Loss: 0.18688110013802847 Validation Loss: 0.9759901762008667\n",
      "Epoch 4955: Training Loss: 0.18663722276687622 Validation Loss: 0.9752640128135681\n",
      "Epoch 4956: Training Loss: 0.18746905525525412 Validation Loss: 0.9749130606651306\n",
      "Epoch 4957: Training Loss: 0.18675029277801514 Validation Loss: 0.9750630259513855\n",
      "Epoch 4958: Training Loss: 0.18712042272090912 Validation Loss: 0.9749564528465271\n",
      "Epoch 4959: Training Loss: 0.1873070846001307 Validation Loss: 0.9752404093742371\n",
      "Epoch 4960: Training Loss: 0.18778728445370993 Validation Loss: 0.9759924411773682\n",
      "Epoch 4961: Training Loss: 0.18701856831709543 Validation Loss: 0.9764654040336609\n",
      "Epoch 4962: Training Loss: 0.18673744797706604 Validation Loss: 0.9763405323028564\n",
      "Epoch 4963: Training Loss: 0.18633069594701132 Validation Loss: 0.9770687222480774\n",
      "Epoch 4964: Training Loss: 0.18667044738928476 Validation Loss: 0.977375328540802\n",
      "Epoch 4965: Training Loss: 0.18716725707054138 Validation Loss: 0.9768676161766052\n",
      "Epoch 4966: Training Loss: 0.18670440713564554 Validation Loss: 0.9763917922973633\n",
      "Epoch 4967: Training Loss: 0.1870810091495514 Validation Loss: 0.9754008650779724\n",
      "Epoch 4968: Training Loss: 0.18625027438004813 Validation Loss: 0.9755132794380188\n",
      "Epoch 4969: Training Loss: 0.1861907790104548 Validation Loss: 0.9762306213378906\n",
      "Epoch 4970: Training Loss: 0.1867468257745107 Validation Loss: 0.9757949709892273\n",
      "Epoch 4971: Training Loss: 0.18644801278909048 Validation Loss: 0.9760200381278992\n",
      "Epoch 4972: Training Loss: 0.18720236917336783 Validation Loss: 0.9764617681503296\n",
      "Epoch 4973: Training Loss: 0.18613063295682272 Validation Loss: 0.9765493273735046\n",
      "Epoch 4974: Training Loss: 0.18607816596825918 Validation Loss: 0.9762520790100098\n",
      "Epoch 4975: Training Loss: 0.18628362317879996 Validation Loss: 0.9760031700134277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4976: Training Loss: 0.18596688409646353 Validation Loss: 0.9754274487495422\n",
      "Epoch 4977: Training Loss: 0.18747523427009583 Validation Loss: 0.9755432605743408\n",
      "Epoch 4978: Training Loss: 0.1862320750951767 Validation Loss: 0.9764580726623535\n",
      "Epoch 4979: Training Loss: 0.18610738714536032 Validation Loss: 0.9763860106468201\n",
      "Epoch 4980: Training Loss: 0.18628387649854025 Validation Loss: 0.9762035012245178\n",
      "Epoch 4981: Training Loss: 0.18624228239059448 Validation Loss: 0.9771446585655212\n",
      "Epoch 4982: Training Loss: 0.18558792769908905 Validation Loss: 0.9776142835617065\n",
      "Epoch 4983: Training Loss: 0.18579533696174622 Validation Loss: 0.9777098894119263\n",
      "Epoch 4984: Training Loss: 0.185795396566391 Validation Loss: 0.9771651029586792\n",
      "Epoch 4985: Training Loss: 0.18542946875095367 Validation Loss: 0.9764080047607422\n",
      "Epoch 4986: Training Loss: 0.18632225692272186 Validation Loss: 0.9766263365745544\n",
      "Epoch 4987: Training Loss: 0.18523644904295603 Validation Loss: 0.9758565425872803\n",
      "Epoch 4988: Training Loss: 0.18585369984308878 Validation Loss: 0.9757860898971558\n",
      "Epoch 4989: Training Loss: 0.1872310241063436 Validation Loss: 0.9756847023963928\n",
      "Epoch 4990: Training Loss: 0.18563109636306763 Validation Loss: 0.9755173921585083\n",
      "Epoch 4991: Training Loss: 0.185456782579422 Validation Loss: 0.9756028056144714\n",
      "Epoch 4992: Training Loss: 0.18617744743824005 Validation Loss: 0.9757682681083679\n",
      "Epoch 4993: Training Loss: 0.18552220364411673 Validation Loss: 0.9768856763839722\n",
      "Epoch 4994: Training Loss: 0.18635796507199606 Validation Loss: 0.9779143929481506\n",
      "Epoch 4995: Training Loss: 0.18521798153718314 Validation Loss: 0.9774527549743652\n",
      "Epoch 4996: Training Loss: 0.18506230413913727 Validation Loss: 0.9768403172492981\n",
      "Epoch 4997: Training Loss: 0.18682118753592172 Validation Loss: 0.9769434928894043\n",
      "Epoch 4998: Training Loss: 0.18512943387031555 Validation Loss: 0.9765015244483948\n",
      "Epoch 4999: Training Loss: 0.18499370416005453 Validation Loss: 0.9768807291984558\n",
      "Epoch 5000: Training Loss: 0.18568633993466696 Validation Loss: 0.9767149090766907\n",
      "Epoch 5001: Training Loss: 0.18437600135803223 Validation Loss: 0.9771656394004822\n",
      "Epoch 5002: Training Loss: 0.185086523493131 Validation Loss: 0.9768550395965576\n",
      "Epoch 5003: Training Loss: 0.18618731697400412 Validation Loss: 0.9766331911087036\n",
      "Epoch 5004: Training Loss: 0.18510870138804117 Validation Loss: 0.9767000079154968\n",
      "Epoch 5005: Training Loss: 0.18529890477657318 Validation Loss: 0.9765399098396301\n",
      "Epoch 5006: Training Loss: 0.1844843626022339 Validation Loss: 0.9768228530883789\n",
      "Epoch 5007: Training Loss: 0.1847865084807078 Validation Loss: 0.9765962958335876\n",
      "Epoch 5008: Training Loss: 0.1854016731182734 Validation Loss: 0.976984977722168\n",
      "Epoch 5009: Training Loss: 0.18498678505420685 Validation Loss: 0.9770022034645081\n",
      "Epoch 5010: Training Loss: 0.1841858426729838 Validation Loss: 0.9775229692459106\n",
      "Epoch 5011: Training Loss: 0.18465870122114816 Validation Loss: 0.9777360558509827\n",
      "Epoch 5012: Training Loss: 0.18481294314066568 Validation Loss: 0.9771742820739746\n",
      "Epoch 5013: Training Loss: 0.1848028600215912 Validation Loss: 0.9769096374511719\n",
      "Epoch 5014: Training Loss: 0.18478190898895264 Validation Loss: 0.9769275784492493\n",
      "Epoch 5015: Training Loss: 0.1846581349770228 Validation Loss: 0.9767034649848938\n",
      "Epoch 5016: Training Loss: 0.18442650636037192 Validation Loss: 0.9763440489768982\n",
      "Epoch 5017: Training Loss: 0.184053565065066 Validation Loss: 0.9767556190490723\n",
      "Epoch 5018: Training Loss: 0.18442333738009134 Validation Loss: 0.9765589833259583\n",
      "Epoch 5019: Training Loss: 0.18409605820973715 Validation Loss: 0.9762887954711914\n",
      "Epoch 5020: Training Loss: 0.18488194048404694 Validation Loss: 0.9766737818717957\n",
      "Epoch 5021: Training Loss: 0.18410663803418478 Validation Loss: 0.9765366911888123\n",
      "Epoch 5022: Training Loss: 0.18435175716876984 Validation Loss: 0.9770097136497498\n",
      "Epoch 5023: Training Loss: 0.18400675058364868 Validation Loss: 0.9770225882530212\n",
      "Epoch 5024: Training Loss: 0.18366812666257223 Validation Loss: 0.9776942133903503\n",
      "Epoch 5025: Training Loss: 0.18422389527161917 Validation Loss: 0.9783505797386169\n",
      "Epoch 5026: Training Loss: 0.18407846490542093 Validation Loss: 0.9778701663017273\n",
      "Epoch 5027: Training Loss: 0.18405160307884216 Validation Loss: 0.9777230620384216\n",
      "Epoch 5028: Training Loss: 0.1838670422633489 Validation Loss: 0.9774684906005859\n",
      "Epoch 5029: Training Loss: 0.18474980195363364 Validation Loss: 0.9771319031715393\n",
      "Epoch 5030: Training Loss: 0.18397858242193857 Validation Loss: 0.977392315864563\n",
      "Epoch 5031: Training Loss: 0.18414126336574554 Validation Loss: 0.9774235486984253\n",
      "Epoch 5032: Training Loss: 0.18374503155549368 Validation Loss: 0.9776546955108643\n",
      "Epoch 5033: Training Loss: 0.18378321826457977 Validation Loss: 0.9776105880737305\n",
      "Epoch 5034: Training Loss: 0.18457265198230743 Validation Loss: 0.9782142639160156\n",
      "Epoch 5035: Training Loss: 0.18583468596140543 Validation Loss: 0.9780323505401611\n",
      "Epoch 5036: Training Loss: 0.18386869629224142 Validation Loss: 0.9780117869377136\n",
      "Epoch 5037: Training Loss: 0.18347838521003723 Validation Loss: 0.9778619408607483\n",
      "Epoch 5038: Training Loss: 0.18352826436360678 Validation Loss: 0.9774703979492188\n",
      "Epoch 5039: Training Loss: 0.18330961962540945 Validation Loss: 0.9781942963600159\n",
      "Epoch 5040: Training Loss: 0.18368240197499594 Validation Loss: 0.9776826500892639\n",
      "Epoch 5041: Training Loss: 0.18342561026414236 Validation Loss: 0.9769818186759949\n",
      "Epoch 5042: Training Loss: 0.18365840117136636 Validation Loss: 0.9766164422035217\n",
      "Epoch 5043: Training Loss: 0.1835134873787562 Validation Loss: 0.9769327044487\n",
      "Epoch 5044: Training Loss: 0.18303945163885751 Validation Loss: 0.9778599143028259\n",
      "Epoch 5045: Training Loss: 0.18407674133777618 Validation Loss: 0.9783909916877747\n",
      "Epoch 5046: Training Loss: 0.18370590607325235 Validation Loss: 0.9788975715637207\n",
      "Epoch 5047: Training Loss: 0.18259348968664804 Validation Loss: 0.9789692759513855\n",
      "Epoch 5048: Training Loss: 0.18303372462590536 Validation Loss: 0.9784536957740784\n",
      "Epoch 5049: Training Loss: 0.18319612244764963 Validation Loss: 0.978147029876709\n",
      "Epoch 5050: Training Loss: 0.18317421277364096 Validation Loss: 0.9779434204101562\n",
      "Epoch 5051: Training Loss: 0.1829162190357844 Validation Loss: 0.977967381477356\n",
      "Epoch 5052: Training Loss: 0.18357055882612863 Validation Loss: 0.9785832762718201\n",
      "Epoch 5053: Training Loss: 0.18281654516855875 Validation Loss: 0.9780104756355286\n",
      "Epoch 5054: Training Loss: 0.1829113612572352 Validation Loss: 0.9781061410903931\n",
      "Epoch 5055: Training Loss: 0.18350827197233835 Validation Loss: 0.978592574596405\n",
      "Epoch 5056: Training Loss: 0.18290921052296957 Validation Loss: 0.9783932566642761\n",
      "Epoch 5057: Training Loss: 0.18278635541598 Validation Loss: 0.9786898493766785\n",
      "Epoch 5058: Training Loss: 0.1828955908616384 Validation Loss: 0.9785811901092529\n",
      "Epoch 5059: Training Loss: 0.18286298712094626 Validation Loss: 0.9782663583755493\n",
      "Epoch 5060: Training Loss: 0.1826994220415751 Validation Loss: 0.9785351753234863\n",
      "Epoch 5061: Training Loss: 0.18271404504776 Validation Loss: 0.977921187877655\n",
      "Epoch 5062: Training Loss: 0.18318228423595428 Validation Loss: 0.9784469604492188\n",
      "Epoch 5063: Training Loss: 0.18265541394551596 Validation Loss: 0.9783855080604553\n",
      "Epoch 5064: Training Loss: 0.18293298284212747 Validation Loss: 0.9782613515853882\n",
      "Epoch 5065: Training Loss: 0.18323304255803427 Validation Loss: 0.9778306484222412\n",
      "Epoch 5066: Training Loss: 0.18202073375384012 Validation Loss: 0.9779189229011536\n",
      "Epoch 5067: Training Loss: 0.18398254613081613 Validation Loss: 0.9777716994285583\n",
      "Epoch 5068: Training Loss: 0.1825624257326126 Validation Loss: 0.978753387928009\n",
      "Epoch 5069: Training Loss: 0.18254743019739786 Validation Loss: 0.9787446856498718\n",
      "Epoch 5070: Training Loss: 0.18206308285395303 Validation Loss: 0.9791996479034424\n",
      "Epoch 5071: Training Loss: 0.18283953269322714 Validation Loss: 0.979204535484314\n",
      "Epoch 5072: Training Loss: 0.18237012128035227 Validation Loss: 0.9782896041870117\n",
      "Epoch 5073: Training Loss: 0.18200399478276572 Validation Loss: 0.9783635139465332\n",
      "Epoch 5074: Training Loss: 0.1816006749868393 Validation Loss: 0.9780365228652954\n",
      "Epoch 5075: Training Loss: 0.1821278582016627 Validation Loss: 0.9784744381904602\n",
      "Epoch 5076: Training Loss: 0.1815799425045649 Validation Loss: 0.9788657426834106\n",
      "Epoch 5077: Training Loss: 0.1840874453385671 Validation Loss: 0.9795424342155457\n",
      "Epoch 5078: Training Loss: 0.182145024339358 Validation Loss: 0.9795708060264587\n",
      "Epoch 5079: Training Loss: 0.1818387359380722 Validation Loss: 0.9792977571487427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5080: Training Loss: 0.18201562762260437 Validation Loss: 0.9789058566093445\n",
      "Epoch 5081: Training Loss: 0.18179044127464294 Validation Loss: 0.978644073009491\n",
      "Epoch 5082: Training Loss: 0.182202418645223 Validation Loss: 0.9783932566642761\n",
      "Epoch 5083: Training Loss: 0.1815583606561025 Validation Loss: 0.9779237508773804\n",
      "Epoch 5084: Training Loss: 0.18165252109368643 Validation Loss: 0.9778834581375122\n",
      "Epoch 5085: Training Loss: 0.1821619321902593 Validation Loss: 0.9780594706535339\n",
      "Epoch 5086: Training Loss: 0.18134831885496774 Validation Loss: 0.9781526327133179\n",
      "Epoch 5087: Training Loss: 0.18152869244416556 Validation Loss: 0.9792774319648743\n",
      "Epoch 5088: Training Loss: 0.18172950049241385 Validation Loss: 0.979797899723053\n",
      "Epoch 5089: Training Loss: 0.1817043274641037 Validation Loss: 0.9800683259963989\n",
      "Epoch 5090: Training Loss: 0.18252526223659515 Validation Loss: 0.9803597927093506\n",
      "Epoch 5091: Training Loss: 0.18165918191274008 Validation Loss: 0.9792655110359192\n",
      "Epoch 5092: Training Loss: 0.181352565685908 Validation Loss: 0.9790398478507996\n",
      "Epoch 5093: Training Loss: 0.18180364867051443 Validation Loss: 0.9791221022605896\n",
      "Epoch 5094: Training Loss: 0.18122043212254843 Validation Loss: 0.9784238934516907\n",
      "Epoch 5095: Training Loss: 0.18137938777605692 Validation Loss: 0.9785842299461365\n",
      "Epoch 5096: Training Loss: 0.1815973569949468 Validation Loss: 0.9786691069602966\n",
      "Epoch 5097: Training Loss: 0.1816305865844091 Validation Loss: 0.979384183883667\n",
      "Epoch 5098: Training Loss: 0.18123960494995117 Validation Loss: 0.9788781404495239\n",
      "Epoch 5099: Training Loss: 0.180868461728096 Validation Loss: 0.9786289930343628\n",
      "Epoch 5100: Training Loss: 0.18119597434997559 Validation Loss: 0.978791356086731\n",
      "Epoch 5101: Training Loss: 0.18194095293680826 Validation Loss: 0.9791697859764099\n",
      "Epoch 5102: Training Loss: 0.18092111746470133 Validation Loss: 0.9791098833084106\n",
      "Epoch 5103: Training Loss: 0.18218390146891275 Validation Loss: 0.9793869853019714\n",
      "Epoch 5104: Training Loss: 0.18071256577968597 Validation Loss: 0.97911536693573\n",
      "Epoch 5105: Training Loss: 0.18068146705627441 Validation Loss: 0.9790457487106323\n",
      "Epoch 5106: Training Loss: 0.1809290647506714 Validation Loss: 0.9796847105026245\n",
      "Epoch 5107: Training Loss: 0.1807274321715037 Validation Loss: 0.9799925088882446\n",
      "Epoch 5108: Training Loss: 0.18061068654060364 Validation Loss: 0.9796515107154846\n",
      "Epoch 5109: Training Loss: 0.18225818375746408 Validation Loss: 0.9798290133476257\n",
      "Epoch 5110: Training Loss: 0.1803538054227829 Validation Loss: 0.9790881276130676\n",
      "Epoch 5111: Training Loss: 0.18087300658226013 Validation Loss: 0.9787510633468628\n",
      "Epoch 5112: Training Loss: 0.18190957605838776 Validation Loss: 0.9779515862464905\n",
      "Epoch 5113: Training Loss: 0.18052682280540466 Validation Loss: 0.9791833758354187\n",
      "Epoch 5114: Training Loss: 0.18043061097462973 Validation Loss: 0.9793903231620789\n",
      "Epoch 5115: Training Loss: 0.18110749125480652 Validation Loss: 0.9797836542129517\n",
      "Epoch 5116: Training Loss: 0.1805355300505956 Validation Loss: 0.9791182279586792\n",
      "Epoch 5117: Training Loss: 0.18054487307866415 Validation Loss: 0.9793745875358582\n",
      "Epoch 5118: Training Loss: 0.18051066994667053 Validation Loss: 0.9799808263778687\n",
      "Epoch 5119: Training Loss: 0.180339977145195 Validation Loss: 0.9796289801597595\n",
      "Epoch 5120: Training Loss: 0.180424764752388 Validation Loss: 0.9798657894134521\n",
      "Epoch 5121: Training Loss: 0.18033971885840097 Validation Loss: 0.980059027671814\n",
      "Epoch 5122: Training Loss: 0.18064030011494955 Validation Loss: 0.979867696762085\n",
      "Epoch 5123: Training Loss: 0.18223878741264343 Validation Loss: 0.9798795580863953\n",
      "Epoch 5124: Training Loss: 0.18056809902191162 Validation Loss: 0.9795131683349609\n",
      "Epoch 5125: Training Loss: 0.18017706274986267 Validation Loss: 0.9787924885749817\n",
      "Epoch 5126: Training Loss: 0.18009512623151144 Validation Loss: 0.9791147112846375\n",
      "Epoch 5127: Training Loss: 0.18000458677609762 Validation Loss: 0.9790377020835876\n",
      "Epoch 5128: Training Loss: 0.1805574744939804 Validation Loss: 0.9799274206161499\n",
      "Epoch 5129: Training Loss: 0.18021302421887717 Validation Loss: 0.9799408912658691\n",
      "Epoch 5130: Training Loss: 0.18038122355937958 Validation Loss: 0.9797733426094055\n",
      "Epoch 5131: Training Loss: 0.17997903128465018 Validation Loss: 0.9807800650596619\n",
      "Epoch 5132: Training Loss: 0.17996483047803244 Validation Loss: 0.9811185598373413\n",
      "Epoch 5133: Training Loss: 0.17985835174719492 Validation Loss: 0.980913519859314\n",
      "Epoch 5134: Training Loss: 0.181126207113266 Validation Loss: 0.9799965023994446\n",
      "Epoch 5135: Training Loss: 0.17971670627593994 Validation Loss: 0.979427695274353\n",
      "Epoch 5136: Training Loss: 0.1801470766464869 Validation Loss: 0.9795215725898743\n",
      "Epoch 5137: Training Loss: 0.18038955827554068 Validation Loss: 0.9800208806991577\n",
      "Epoch 5138: Training Loss: 0.18013417720794678 Validation Loss: 0.9799810647964478\n",
      "Epoch 5139: Training Loss: 0.1796377201875051 Validation Loss: 0.980355978012085\n",
      "Epoch 5140: Training Loss: 0.17957156399885812 Validation Loss: 0.9806333184242249\n",
      "Epoch 5141: Training Loss: 0.1799324999252955 Validation Loss: 0.9806323647499084\n",
      "Epoch 5142: Training Loss: 0.18021388351917267 Validation Loss: 0.9804598093032837\n",
      "Epoch 5143: Training Loss: 0.17952628433704376 Validation Loss: 0.980093240737915\n",
      "Epoch 5144: Training Loss: 0.17899930973847708 Validation Loss: 0.9810123443603516\n",
      "Epoch 5145: Training Loss: 0.17937164505322775 Validation Loss: 0.9802890419960022\n",
      "Epoch 5146: Training Loss: 0.18019247551759085 Validation Loss: 0.9807594418525696\n",
      "Epoch 5147: Training Loss: 0.179416557153066 Validation Loss: 0.980161726474762\n",
      "Epoch 5148: Training Loss: 0.1795856406291326 Validation Loss: 0.9803043603897095\n",
      "Epoch 5149: Training Loss: 0.17953993876775107 Validation Loss: 0.9803115725517273\n",
      "Epoch 5150: Training Loss: 0.17871810992558798 Validation Loss: 0.9803999066352844\n",
      "Epoch 5151: Training Loss: 0.17939161757628122 Validation Loss: 0.9812621474266052\n",
      "Epoch 5152: Training Loss: 0.17935210963090262 Validation Loss: 0.9813726544380188\n",
      "Epoch 5153: Training Loss: 0.17861666778723398 Validation Loss: 0.9807279706001282\n",
      "Epoch 5154: Training Loss: 0.1789632042249044 Validation Loss: 0.9801241755485535\n",
      "Epoch 5155: Training Loss: 0.17891362806161246 Validation Loss: 0.9803274273872375\n",
      "Epoch 5156: Training Loss: 0.18084633350372314 Validation Loss: 0.9803564548492432\n",
      "Epoch 5157: Training Loss: 0.17952264348665872 Validation Loss: 0.9804770946502686\n",
      "Epoch 5158: Training Loss: 0.1789658566315969 Validation Loss: 0.9806042909622192\n",
      "Epoch 5159: Training Loss: 0.17947318653265634 Validation Loss: 0.9805025458335876\n",
      "Epoch 5160: Training Loss: 0.17889785766601562 Validation Loss: 0.9800035953521729\n",
      "Epoch 5161: Training Loss: 0.1793439437945684 Validation Loss: 0.9797770977020264\n",
      "Epoch 5162: Training Loss: 0.17874941726525626 Validation Loss: 0.9801439046859741\n",
      "Epoch 5163: Training Loss: 0.1788845956325531 Validation Loss: 0.9807859063148499\n",
      "Epoch 5164: Training Loss: 0.17870546380678812 Validation Loss: 0.9815575480461121\n",
      "Epoch 5165: Training Loss: 0.1796308010816574 Validation Loss: 0.9817417860031128\n",
      "Epoch 5166: Training Loss: 0.17947149773438772 Validation Loss: 0.9819098711013794\n",
      "Epoch 5167: Training Loss: 0.17791065076986948 Validation Loss: 0.9813117384910583\n",
      "Epoch 5168: Training Loss: 0.1784907877445221 Validation Loss: 0.9803943634033203\n",
      "Epoch 5169: Training Loss: 0.178482914964358 Validation Loss: 0.980212390422821\n",
      "Epoch 5170: Training Loss: 0.17811483641465506 Validation Loss: 0.9795637130737305\n",
      "Epoch 5171: Training Loss: 0.1788661777973175 Validation Loss: 0.9799095988273621\n",
      "Epoch 5172: Training Loss: 0.178761954108874 Validation Loss: 0.9810057878494263\n",
      "Epoch 5173: Training Loss: 0.17900566260019937 Validation Loss: 0.9811906218528748\n",
      "Epoch 5174: Training Loss: 0.17872072259585062 Validation Loss: 0.9814690351486206\n",
      "Epoch 5175: Training Loss: 0.17858744661013284 Validation Loss: 0.9820982813835144\n",
      "Epoch 5176: Training Loss: 0.1788786749045054 Validation Loss: 0.9823824167251587\n",
      "Epoch 5177: Training Loss: 0.17827332019805908 Validation Loss: 0.9818724393844604\n",
      "Epoch 5178: Training Loss: 0.1783180038134257 Validation Loss: 0.9821398854255676\n",
      "Epoch 5179: Training Loss: 0.17831050356229147 Validation Loss: 0.9813722968101501\n",
      "Epoch 5180: Training Loss: 0.1791413426399231 Validation Loss: 0.9812551140785217\n",
      "Epoch 5181: Training Loss: 0.178737739721934 Validation Loss: 0.9812154769897461\n",
      "Epoch 5182: Training Loss: 0.1778590828180313 Validation Loss: 0.9807291030883789\n",
      "Epoch 5183: Training Loss: 0.1782406916220983 Validation Loss: 0.9812304973602295\n",
      "Epoch 5184: Training Loss: 0.17774825791517893 Validation Loss: 0.9813152551651001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5185: Training Loss: 0.17801254987716675 Validation Loss: 0.9817295074462891\n",
      "Epoch 5186: Training Loss: 0.17809725801150003 Validation Loss: 0.9819469451904297\n",
      "Epoch 5187: Training Loss: 0.177714005112648 Validation Loss: 0.982029378414154\n",
      "Epoch 5188: Training Loss: 0.177581787109375 Validation Loss: 0.9818910956382751\n",
      "Epoch 5189: Training Loss: 0.178775688012441 Validation Loss: 0.9816036820411682\n",
      "Epoch 5190: Training Loss: 0.17784867187341055 Validation Loss: 0.9819815754890442\n",
      "Epoch 5191: Training Loss: 0.1775429199139277 Validation Loss: 0.9819552898406982\n",
      "Epoch 5192: Training Loss: 0.1778379132350286 Validation Loss: 0.9812312126159668\n",
      "Epoch 5193: Training Loss: 0.1776423156261444 Validation Loss: 0.9811933040618896\n",
      "Epoch 5194: Training Loss: 0.1776649703582128 Validation Loss: 0.9807260036468506\n",
      "Epoch 5195: Training Loss: 0.17877714335918427 Validation Loss: 0.9810008406639099\n",
      "Epoch 5196: Training Loss: 0.17750955124696097 Validation Loss: 0.9817944765090942\n",
      "Epoch 5197: Training Loss: 0.17761255304018655 Validation Loss: 0.9821321964263916\n",
      "Epoch 5198: Training Loss: 0.17783541977405548 Validation Loss: 0.981645941734314\n",
      "Epoch 5199: Training Loss: 0.17721318701903024 Validation Loss: 0.9817788004875183\n",
      "Epoch 5200: Training Loss: 0.17796972890694937 Validation Loss: 0.9825302958488464\n",
      "Epoch 5201: Training Loss: 0.17692057291666666 Validation Loss: 0.9821722507476807\n",
      "Epoch 5202: Training Loss: 0.17748631040255228 Validation Loss: 0.9815925359725952\n",
      "Epoch 5203: Training Loss: 0.17720604439576468 Validation Loss: 0.9815120100975037\n",
      "Epoch 5204: Training Loss: 0.17758538822333017 Validation Loss: 0.981995165348053\n",
      "Epoch 5205: Training Loss: 0.17693722744782767 Validation Loss: 0.9818674921989441\n",
      "Epoch 5206: Training Loss: 0.17694975435733795 Validation Loss: 0.9821842908859253\n",
      "Epoch 5207: Training Loss: 0.17769197622934976 Validation Loss: 0.9826170802116394\n",
      "Epoch 5208: Training Loss: 0.17741161584854126 Validation Loss: 0.981951117515564\n",
      "Epoch 5209: Training Loss: 0.1765055557092031 Validation Loss: 0.982386589050293\n",
      "Epoch 5210: Training Loss: 0.1780817856391271 Validation Loss: 0.9819175601005554\n",
      "Epoch 5211: Training Loss: 0.17707157135009766 Validation Loss: 0.982469379901886\n",
      "Epoch 5212: Training Loss: 0.176982710758845 Validation Loss: 0.9820860028266907\n",
      "Epoch 5213: Training Loss: 0.17723553876082102 Validation Loss: 0.9814426302909851\n",
      "Epoch 5214: Training Loss: 0.17654205362002054 Validation Loss: 0.9816135764122009\n",
      "Epoch 5215: Training Loss: 0.17771483957767487 Validation Loss: 0.9820945262908936\n",
      "Epoch 5216: Training Loss: 0.17712338268756866 Validation Loss: 0.9822757840156555\n",
      "Epoch 5217: Training Loss: 0.17692289253075918 Validation Loss: 0.9820541739463806\n",
      "Epoch 5218: Training Loss: 0.17793473601341248 Validation Loss: 0.9820069074630737\n",
      "Epoch 5219: Training Loss: 0.1772842804590861 Validation Loss: 0.9827978610992432\n",
      "Epoch 5220: Training Loss: 0.1764528751373291 Validation Loss: 0.98299241065979\n",
      "Epoch 5221: Training Loss: 0.17631501456101736 Validation Loss: 0.9835559725761414\n",
      "Epoch 5222: Training Loss: 0.17691492040952048 Validation Loss: 0.9832568168640137\n",
      "Epoch 5223: Training Loss: 0.17652596533298492 Validation Loss: 0.9829167723655701\n",
      "Epoch 5224: Training Loss: 0.17679005364576975 Validation Loss: 0.9826757907867432\n",
      "Epoch 5225: Training Loss: 0.17689307034015656 Validation Loss: 0.9823840260505676\n",
      "Epoch 5226: Training Loss: 0.17753966152668 Validation Loss: 0.9823410511016846\n",
      "Epoch 5227: Training Loss: 0.17678876221179962 Validation Loss: 0.9819738864898682\n",
      "Epoch 5228: Training Loss: 0.17645348608493805 Validation Loss: 0.9825811386108398\n",
      "Epoch 5229: Training Loss: 0.176930233836174 Validation Loss: 0.981755256652832\n",
      "Epoch 5230: Training Loss: 0.17637083431084952 Validation Loss: 0.9821541905403137\n",
      "Epoch 5231: Training Loss: 0.17609070241451263 Validation Loss: 0.9822324514389038\n",
      "Epoch 5232: Training Loss: 0.176987553636233 Validation Loss: 0.981799304485321\n",
      "Epoch 5233: Training Loss: 0.17644673585891724 Validation Loss: 0.9823692440986633\n",
      "Epoch 5234: Training Loss: 0.1762821525335312 Validation Loss: 0.9824154376983643\n",
      "Epoch 5235: Training Loss: 0.17631387213865915 Validation Loss: 0.9823586940765381\n",
      "Epoch 5236: Training Loss: 0.17634201049804688 Validation Loss: 0.9824396967887878\n",
      "Epoch 5237: Training Loss: 0.1765248974164327 Validation Loss: 0.9821451306343079\n",
      "Epoch 5238: Training Loss: 0.17671680450439453 Validation Loss: 0.9827378392219543\n",
      "Epoch 5239: Training Loss: 0.17657113075256348 Validation Loss: 0.982636034488678\n",
      "Epoch 5240: Training Loss: 0.17644663155078888 Validation Loss: 0.982700526714325\n",
      "Epoch 5241: Training Loss: 0.17608895897865295 Validation Loss: 0.9829211831092834\n",
      "Epoch 5242: Training Loss: 0.1765576402346293 Validation Loss: 0.983288049697876\n",
      "Epoch 5243: Training Loss: 0.17570808033148447 Validation Loss: 0.9834262132644653\n",
      "Epoch 5244: Training Loss: 0.17679054041703543 Validation Loss: 0.9835381507873535\n",
      "Epoch 5245: Training Loss: 0.1764969676733017 Validation Loss: 0.9835087656974792\n",
      "Epoch 5246: Training Loss: 0.17568292717138925 Validation Loss: 0.9831072688102722\n",
      "Epoch 5247: Training Loss: 0.1761278659105301 Validation Loss: 0.9834079146385193\n",
      "Epoch 5248: Training Loss: 0.175694872935613 Validation Loss: 0.9832935333251953\n",
      "Epoch 5249: Training Loss: 0.1758878082036972 Validation Loss: 0.9837868213653564\n",
      "Epoch 5250: Training Loss: 0.17553706467151642 Validation Loss: 0.9842016100883484\n",
      "Epoch 5251: Training Loss: 0.1756447603305181 Validation Loss: 0.983802855014801\n",
      "Epoch 5252: Training Loss: 0.17565927902857462 Validation Loss: 0.9836881160736084\n",
      "Epoch 5253: Training Loss: 0.17593599359194437 Validation Loss: 0.9829716086387634\n",
      "Epoch 5254: Training Loss: 0.17524084945519766 Validation Loss: 0.9821811318397522\n",
      "Epoch 5255: Training Loss: 0.17581307391325632 Validation Loss: 0.9820267558097839\n",
      "Epoch 5256: Training Loss: 0.1755808542172114 Validation Loss: 0.9823204874992371\n",
      "Epoch 5257: Training Loss: 0.1754663735628128 Validation Loss: 0.9828969836235046\n",
      "Epoch 5258: Training Loss: 0.17527357737223306 Validation Loss: 0.9831162691116333\n",
      "Epoch 5259: Training Loss: 0.17532741526762644 Validation Loss: 0.9832131266593933\n",
      "Epoch 5260: Training Loss: 0.17519988119602203 Validation Loss: 0.9830161929130554\n",
      "Epoch 5261: Training Loss: 0.1756510784228643 Validation Loss: 0.982417106628418\n",
      "Epoch 5262: Training Loss: 0.17530672748883566 Validation Loss: 0.9824549555778503\n",
      "Epoch 5263: Training Loss: 0.1755266785621643 Validation Loss: 0.9833813905715942\n",
      "Epoch 5264: Training Loss: 0.1750518431266149 Validation Loss: 0.9840947985649109\n",
      "Epoch 5265: Training Loss: 0.1750413328409195 Validation Loss: 0.9845318794250488\n",
      "Epoch 5266: Training Loss: 0.17501328388849893 Validation Loss: 0.9841985106468201\n",
      "Epoch 5267: Training Loss: 0.17498050133387247 Validation Loss: 0.9838281273841858\n",
      "Epoch 5268: Training Loss: 0.17517472306887308 Validation Loss: 0.9831523895263672\n",
      "Epoch 5269: Training Loss: 0.17503188053766885 Validation Loss: 0.983502209186554\n",
      "Epoch 5270: Training Loss: 0.17422412832578024 Validation Loss: 0.9841358065605164\n",
      "Epoch 5271: Training Loss: 0.17525681853294373 Validation Loss: 0.9841611981391907\n",
      "Epoch 5272: Training Loss: 0.17481924096743265 Validation Loss: 0.9846749305725098\n",
      "Epoch 5273: Training Loss: 0.17558909952640533 Validation Loss: 0.9846097230911255\n",
      "Epoch 5274: Training Loss: 0.17484723528226218 Validation Loss: 0.9835969805717468\n",
      "Epoch 5275: Training Loss: 0.17454861104488373 Validation Loss: 0.9834343791007996\n",
      "Epoch 5276: Training Loss: 0.17476706206798553 Validation Loss: 0.9828239679336548\n",
      "Epoch 5277: Training Loss: 0.17487353086471558 Validation Loss: 0.9832760691642761\n",
      "Epoch 5278: Training Loss: 0.17497242987155914 Validation Loss: 0.9840065240859985\n",
      "Epoch 5279: Training Loss: 0.17503435413042703 Validation Loss: 0.9845456480979919\n",
      "Epoch 5280: Training Loss: 0.1746386686960856 Validation Loss: 0.9843281507492065\n",
      "Epoch 5281: Training Loss: 0.1759486198425293 Validation Loss: 0.9837086200714111\n",
      "Epoch 5282: Training Loss: 0.17412373423576355 Validation Loss: 0.9837519526481628\n",
      "Epoch 5283: Training Loss: 0.1741706281900406 Validation Loss: 0.9835814833641052\n",
      "Epoch 5284: Training Loss: 0.17442061503728232 Validation Loss: 0.9836337566375732\n",
      "Epoch 5285: Training Loss: 0.17468591034412384 Validation Loss: 0.9838662147521973\n",
      "Epoch 5286: Training Loss: 0.1739970544974009 Validation Loss: 0.9839292168617249\n",
      "Epoch 5287: Training Loss: 0.17446653544902802 Validation Loss: 0.984289824962616\n",
      "Epoch 5288: Training Loss: 0.17505238950252533 Validation Loss: 0.9840561151504517\n",
      "Epoch 5289: Training Loss: 0.17442625761032104 Validation Loss: 0.983539879322052\n",
      "Epoch 5290: Training Loss: 0.17389952143033346 Validation Loss: 0.9847631454467773\n",
      "Epoch 5291: Training Loss: 0.1744767427444458 Validation Loss: 0.9846729040145874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5292: Training Loss: 0.17384830117225647 Validation Loss: 0.9845994114875793\n",
      "Epoch 5293: Training Loss: 0.17475947240988413 Validation Loss: 0.9838364720344543\n",
      "Epoch 5294: Training Loss: 0.17409967879454294 Validation Loss: 0.9833947420120239\n",
      "Epoch 5295: Training Loss: 0.1751112441221873 Validation Loss: 0.9832231402397156\n",
      "Epoch 5296: Training Loss: 0.17378579080104828 Validation Loss: 0.9838460087776184\n",
      "Epoch 5297: Training Loss: 0.17529086768627167 Validation Loss: 0.9838466048240662\n",
      "Epoch 5298: Training Loss: 0.17381375034650168 Validation Loss: 0.9842529296875\n",
      "Epoch 5299: Training Loss: 0.17393440504868826 Validation Loss: 0.9852673411369324\n",
      "Epoch 5300: Training Loss: 0.17365255455176035 Validation Loss: 0.9844431281089783\n",
      "Epoch 5301: Training Loss: 0.1737414946158727 Validation Loss: 0.9835832118988037\n",
      "Epoch 5302: Training Loss: 0.17353825271129608 Validation Loss: 0.9836673140525818\n",
      "Epoch 5303: Training Loss: 0.17387468616167703 Validation Loss: 0.9841432571411133\n",
      "Epoch 5304: Training Loss: 0.1738216131925583 Validation Loss: 0.9841432571411133\n",
      "Epoch 5305: Training Loss: 0.17344177265961966 Validation Loss: 0.9842981696128845\n",
      "Epoch 5306: Training Loss: 0.17341239750385284 Validation Loss: 0.9846645593643188\n",
      "Epoch 5307: Training Loss: 0.1734726925690969 Validation Loss: 0.9843565225601196\n",
      "Epoch 5308: Training Loss: 0.17363292475541434 Validation Loss: 0.984801709651947\n",
      "Epoch 5309: Training Loss: 0.17416865626970926 Validation Loss: 0.9850151538848877\n",
      "Epoch 5310: Training Loss: 0.1738433837890625 Validation Loss: 0.9848023653030396\n",
      "Epoch 5311: Training Loss: 0.17403283218542734 Validation Loss: 0.9843050837516785\n",
      "Epoch 5312: Training Loss: 0.17365279297033945 Validation Loss: 0.9852562546730042\n",
      "Epoch 5313: Training Loss: 0.17340772847334543 Validation Loss: 0.9851223230361938\n",
      "Epoch 5314: Training Loss: 0.1741427779197693 Validation Loss: 0.9855514168739319\n",
      "Epoch 5315: Training Loss: 0.17438338696956635 Validation Loss: 0.9858049154281616\n",
      "Epoch 5316: Training Loss: 0.17402570943037668 Validation Loss: 0.9852943420410156\n",
      "Epoch 5317: Training Loss: 0.17360831797122955 Validation Loss: 0.9853847622871399\n",
      "Epoch 5318: Training Loss: 0.17348564664522806 Validation Loss: 0.9846851825714111\n",
      "Epoch 5319: Training Loss: 0.17342063784599304 Validation Loss: 0.9845129251480103\n",
      "Epoch 5320: Training Loss: 0.17288786669572195 Validation Loss: 0.9850105047225952\n",
      "Epoch 5321: Training Loss: 0.1734507977962494 Validation Loss: 0.9851125478744507\n",
      "Epoch 5322: Training Loss: 0.17378332217534384 Validation Loss: 0.9853804707527161\n",
      "Epoch 5323: Training Loss: 0.17346037924289703 Validation Loss: 0.9852091670036316\n",
      "Epoch 5324: Training Loss: 0.1733869065841039 Validation Loss: 0.9855820536613464\n",
      "Epoch 5325: Training Loss: 0.17286773025989532 Validation Loss: 0.9852504730224609\n",
      "Epoch 5326: Training Loss: 0.17317517598470053 Validation Loss: 0.9846131801605225\n",
      "Epoch 5327: Training Loss: 0.17304832736651102 Validation Loss: 0.9850887656211853\n",
      "Epoch 5328: Training Loss: 0.1731931815544764 Validation Loss: 0.9849585890769958\n",
      "Epoch 5329: Training Loss: 0.17418760557969412 Validation Loss: 0.9844574332237244\n",
      "Epoch 5330: Training Loss: 0.17276219526926676 Validation Loss: 0.984460175037384\n",
      "Epoch 5331: Training Loss: 0.17305074632167816 Validation Loss: 0.9847981929779053\n",
      "Epoch 5332: Training Loss: 0.1727383186419805 Validation Loss: 0.9846907258033752\n",
      "Epoch 5333: Training Loss: 0.17259564499060312 Validation Loss: 0.9848706722259521\n",
      "Epoch 5334: Training Loss: 0.17336389422416687 Validation Loss: 0.9843658208847046\n",
      "Epoch 5335: Training Loss: 0.17298178871472678 Validation Loss: 0.9857220649719238\n",
      "Epoch 5336: Training Loss: 0.17260176440080008 Validation Loss: 0.9858009815216064\n",
      "Epoch 5337: Training Loss: 0.17259890337785086 Validation Loss: 0.9853968024253845\n",
      "Epoch 5338: Training Loss: 0.17348461349805197 Validation Loss: 0.98610919713974\n",
      "Epoch 5339: Training Loss: 0.1738409847021103 Validation Loss: 0.9855561256408691\n",
      "Epoch 5340: Training Loss: 0.17224301397800446 Validation Loss: 0.984488844871521\n",
      "Epoch 5341: Training Loss: 0.17255011200904846 Validation Loss: 0.9849327206611633\n",
      "Epoch 5342: Training Loss: 0.17338728408018747 Validation Loss: 0.9849737882614136\n",
      "Epoch 5343: Training Loss: 0.17236617704232535 Validation Loss: 0.9849159121513367\n",
      "Epoch 5344: Training Loss: 0.17206726968288422 Validation Loss: 0.9857025146484375\n",
      "Epoch 5345: Training Loss: 0.17202691237131754 Validation Loss: 0.9858636260032654\n",
      "Epoch 5346: Training Loss: 0.1721326063076655 Validation Loss: 0.98630291223526\n",
      "Epoch 5347: Training Loss: 0.172259251276652 Validation Loss: 0.9860320091247559\n",
      "Epoch 5348: Training Loss: 0.17211313048998514 Validation Loss: 0.9859485626220703\n",
      "Epoch 5349: Training Loss: 0.17238624393939972 Validation Loss: 0.9860103726387024\n",
      "Epoch 5350: Training Loss: 0.17207961777846018 Validation Loss: 0.9862393736839294\n",
      "Epoch 5351: Training Loss: 0.17167875667413077 Validation Loss: 0.9856547117233276\n",
      "Epoch 5352: Training Loss: 0.1719987690448761 Validation Loss: 0.9854558706283569\n",
      "Epoch 5353: Training Loss: 0.17224398255348206 Validation Loss: 0.9850200414657593\n",
      "Epoch 5354: Training Loss: 0.1723315715789795 Validation Loss: 0.9851298332214355\n",
      "Epoch 5355: Training Loss: 0.17204699416955313 Validation Loss: 0.9851627945899963\n",
      "Epoch 5356: Training Loss: 0.17187472184499106 Validation Loss: 0.985162615776062\n",
      "Epoch 5357: Training Loss: 0.17174985508124033 Validation Loss: 0.9854820966720581\n",
      "Epoch 5358: Training Loss: 0.17174358169237772 Validation Loss: 0.9856494069099426\n",
      "Epoch 5359: Training Loss: 0.17179638147354126 Validation Loss: 0.986416220664978\n",
      "Epoch 5360: Training Loss: 0.1722091188033422 Validation Loss: 0.9856805205345154\n",
      "Epoch 5361: Training Loss: 0.17162028948465982 Validation Loss: 0.9858955144882202\n",
      "Epoch 5362: Training Loss: 0.17169570922851562 Validation Loss: 0.9863484501838684\n",
      "Epoch 5363: Training Loss: 0.17193292081356049 Validation Loss: 0.9865923523902893\n",
      "Epoch 5364: Training Loss: 0.17168574035167694 Validation Loss: 0.9866601228713989\n",
      "Epoch 5365: Training Loss: 0.17283485333124796 Validation Loss: 0.9863412380218506\n",
      "Epoch 5366: Training Loss: 0.1718736837307612 Validation Loss: 0.9864052534103394\n",
      "Epoch 5367: Training Loss: 0.17170541981856027 Validation Loss: 0.9858412742614746\n",
      "Epoch 5368: Training Loss: 0.1714137444893519 Validation Loss: 0.9855246543884277\n",
      "Epoch 5369: Training Loss: 0.17194080849488577 Validation Loss: 0.986218273639679\n",
      "Epoch 5370: Training Loss: 0.17172817389170328 Validation Loss: 0.9867357015609741\n",
      "Epoch 5371: Training Loss: 0.17169434328873953 Validation Loss: 0.9869791865348816\n",
      "Epoch 5372: Training Loss: 0.1722105344136556 Validation Loss: 0.986638069152832\n",
      "Epoch 5373: Training Loss: 0.17149615287780762 Validation Loss: 0.9865575432777405\n",
      "Epoch 5374: Training Loss: 0.17195058862368265 Validation Loss: 0.9867593050003052\n",
      "Epoch 5375: Training Loss: 0.1716447522242864 Validation Loss: 0.986515998840332\n",
      "Epoch 5376: Training Loss: 0.17219420274098715 Validation Loss: 0.9870744943618774\n",
      "Epoch 5377: Training Loss: 0.17131895323594412 Validation Loss: 0.9876227974891663\n",
      "Epoch 5378: Training Loss: 0.17279967168966928 Validation Loss: 0.9875799417495728\n",
      "Epoch 5379: Training Loss: 0.1710012604792913 Validation Loss: 0.9873651266098022\n",
      "Epoch 5380: Training Loss: 0.17095469931761423 Validation Loss: 0.9863314628601074\n",
      "Epoch 5381: Training Loss: 0.17102505266666412 Validation Loss: 0.9858928918838501\n",
      "Epoch 5382: Training Loss: 0.1710276852051417 Validation Loss: 0.9860134124755859\n",
      "Epoch 5383: Training Loss: 0.17170511682828268 Validation Loss: 0.9854633212089539\n",
      "Epoch 5384: Training Loss: 0.1705892632404963 Validation Loss: 0.986208975315094\n",
      "Epoch 5385: Training Loss: 0.17094563941160837 Validation Loss: 0.9861495494842529\n",
      "Epoch 5386: Training Loss: 0.17120452721913657 Validation Loss: 0.9856842756271362\n",
      "Epoch 5387: Training Loss: 0.17066227893034616 Validation Loss: 0.9862253069877625\n",
      "Epoch 5388: Training Loss: 0.17097929616769156 Validation Loss: 0.987050473690033\n",
      "Epoch 5389: Training Loss: 0.17128983636697134 Validation Loss: 0.9870327711105347\n",
      "Epoch 5390: Training Loss: 0.17101446290810904 Validation Loss: 0.9872244596481323\n",
      "Epoch 5391: Training Loss: 0.17028222481409708 Validation Loss: 0.9871161580085754\n",
      "Epoch 5392: Training Loss: 0.17089482148488364 Validation Loss: 0.9868178963661194\n",
      "Epoch 5393: Training Loss: 0.17102450132369995 Validation Loss: 0.9863253831863403\n",
      "Epoch 5394: Training Loss: 0.1704832265774409 Validation Loss: 0.9868152737617493\n",
      "Epoch 5395: Training Loss: 0.17025749385356903 Validation Loss: 0.9870632290840149\n",
      "Epoch 5396: Training Loss: 0.17027327418327332 Validation Loss: 0.9869871139526367\n",
      "Epoch 5397: Training Loss: 0.17008166015148163 Validation Loss: 0.986872673034668\n",
      "Epoch 5398: Training Loss: 0.17059223353862762 Validation Loss: 0.9868056178092957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5399: Training Loss: 0.17068340380986533 Validation Loss: 0.9871386885643005\n",
      "Epoch 5400: Training Loss: 0.17031586170196533 Validation Loss: 0.9868032336235046\n",
      "Epoch 5401: Training Loss: 0.17044097681840262 Validation Loss: 0.9871907830238342\n",
      "Epoch 5402: Training Loss: 0.17050725718339285 Validation Loss: 0.9873866438865662\n",
      "Epoch 5403: Training Loss: 0.1709183007478714 Validation Loss: 0.9877468943595886\n",
      "Epoch 5404: Training Loss: 0.17033849159876505 Validation Loss: 0.9875916242599487\n",
      "Epoch 5405: Training Loss: 0.17025559147198996 Validation Loss: 0.9874642491340637\n",
      "Epoch 5406: Training Loss: 0.17078598837057749 Validation Loss: 0.9869489073753357\n",
      "Epoch 5407: Training Loss: 0.17053303122520447 Validation Loss: 0.9867368340492249\n",
      "Epoch 5408: Training Loss: 0.17094954351584116 Validation Loss: 0.98661208152771\n",
      "Epoch 5409: Training Loss: 0.16996934513250986 Validation Loss: 0.9869274497032166\n",
      "Epoch 5410: Training Loss: 0.16992472608884177 Validation Loss: 0.9877332448959351\n",
      "Epoch 5411: Training Loss: 0.17021863659222922 Validation Loss: 0.9879274368286133\n",
      "Epoch 5412: Training Loss: 0.17009700338045755 Validation Loss: 0.9880369901657104\n",
      "Epoch 5413: Training Loss: 0.17069983979066214 Validation Loss: 0.9876840710639954\n",
      "Epoch 5414: Training Loss: 0.17036327719688416 Validation Loss: 0.9871544241905212\n",
      "Epoch 5415: Training Loss: 0.16998269657293955 Validation Loss: 0.9864166975021362\n",
      "Epoch 5416: Training Loss: 0.17087414860725403 Validation Loss: 0.986469566822052\n",
      "Epoch 5417: Training Loss: 0.1695991506179174 Validation Loss: 0.9865586757659912\n",
      "Epoch 5418: Training Loss: 0.17025755842526755 Validation Loss: 0.9870943427085876\n",
      "Epoch 5419: Training Loss: 0.17060274382432303 Validation Loss: 0.9871871471405029\n",
      "Epoch 5420: Training Loss: 0.16940002143383026 Validation Loss: 0.9873687028884888\n",
      "Epoch 5421: Training Loss: 0.17002175251642862 Validation Loss: 0.9875807762145996\n",
      "Epoch 5422: Training Loss: 0.1695879946152369 Validation Loss: 0.9885948896408081\n",
      "Epoch 5423: Training Loss: 0.16963863869508108 Validation Loss: 0.9889049530029297\n",
      "Epoch 5424: Training Loss: 0.17056176563103995 Validation Loss: 0.988090991973877\n",
      "Epoch 5425: Training Loss: 0.16974957287311554 Validation Loss: 0.9873306751251221\n",
      "Epoch 5426: Training Loss: 0.16955832143624625 Validation Loss: 0.9878134727478027\n",
      "Epoch 5427: Training Loss: 0.1696528991063436 Validation Loss: 0.9875128865242004\n",
      "Epoch 5428: Training Loss: 0.16959054271380106 Validation Loss: 0.9873384833335876\n",
      "Epoch 5429: Training Loss: 0.16950191060702005 Validation Loss: 0.9874300956726074\n",
      "Epoch 5430: Training Loss: 0.1692797988653183 Validation Loss: 0.9870548844337463\n",
      "Epoch 5431: Training Loss: 0.16974729796250662 Validation Loss: 0.9878364205360413\n",
      "Epoch 5432: Training Loss: 0.17024091382821402 Validation Loss: 0.9880746603012085\n",
      "Epoch 5433: Training Loss: 0.17049489418665567 Validation Loss: 0.988591194152832\n",
      "Epoch 5434: Training Loss: 0.16939003268877664 Validation Loss: 0.9884787201881409\n",
      "Epoch 5435: Training Loss: 0.1695326268672943 Validation Loss: 0.9891283512115479\n",
      "Epoch 5436: Training Loss: 0.16954837242762247 Validation Loss: 0.988520622253418\n",
      "Epoch 5437: Training Loss: 0.16926907499631247 Validation Loss: 0.9881256818771362\n",
      "Epoch 5438: Training Loss: 0.1691103974978129 Validation Loss: 0.9878290891647339\n",
      "Epoch 5439: Training Loss: 0.16852785646915436 Validation Loss: 0.987790584564209\n",
      "Epoch 5440: Training Loss: 0.16918131212393442 Validation Loss: 0.9876046180725098\n",
      "Epoch 5441: Training Loss: 0.16938399771849313 Validation Loss: 0.9885071516036987\n",
      "Epoch 5442: Training Loss: 0.16914837062358856 Validation Loss: 0.9883033633232117\n",
      "Epoch 5443: Training Loss: 0.1688212106625239 Validation Loss: 0.9881971478462219\n",
      "Epoch 5444: Training Loss: 0.1689550777276357 Validation Loss: 0.9879769086837769\n",
      "Epoch 5445: Training Loss: 0.16896207630634308 Validation Loss: 0.9875614643096924\n",
      "Epoch 5446: Training Loss: 0.16895759602387747 Validation Loss: 0.9879763126373291\n",
      "Epoch 5447: Training Loss: 0.16883247594038645 Validation Loss: 0.9881024360656738\n",
      "Epoch 5448: Training Loss: 0.16895676652590433 Validation Loss: 0.9881447553634644\n",
      "Epoch 5449: Training Loss: 0.1691172868013382 Validation Loss: 0.9884769916534424\n",
      "Epoch 5450: Training Loss: 0.16873704890410104 Validation Loss: 0.9877007603645325\n",
      "Epoch 5451: Training Loss: 0.16976207991441092 Validation Loss: 0.9875374436378479\n",
      "Epoch 5452: Training Loss: 0.16824943820635477 Validation Loss: 0.9883106350898743\n",
      "Epoch 5453: Training Loss: 0.16897013783454895 Validation Loss: 0.9889757037162781\n",
      "Epoch 5454: Training Loss: 0.1684850404659907 Validation Loss: 0.9892422556877136\n",
      "Epoch 5455: Training Loss: 0.16864451269308725 Validation Loss: 0.9890556931495667\n",
      "Epoch 5456: Training Loss: 0.1686110645532608 Validation Loss: 0.9885927438735962\n",
      "Epoch 5457: Training Loss: 0.1687625696261724 Validation Loss: 0.9881166815757751\n",
      "Epoch 5458: Training Loss: 0.16894778609275818 Validation Loss: 0.9877265691757202\n",
      "Epoch 5459: Training Loss: 0.16846699515978494 Validation Loss: 0.9882616996765137\n",
      "Epoch 5460: Training Loss: 0.1684736212094625 Validation Loss: 0.9892356395721436\n",
      "Epoch 5461: Training Loss: 0.16879871984322867 Validation Loss: 0.9895877242088318\n",
      "Epoch 5462: Training Loss: 0.16841121017932892 Validation Loss: 0.9897826910018921\n",
      "Epoch 5463: Training Loss: 0.16921313107013702 Validation Loss: 0.9890284538269043\n",
      "Epoch 5464: Training Loss: 0.16898491978645325 Validation Loss: 0.9893980026245117\n",
      "Epoch 5465: Training Loss: 0.16858546435832977 Validation Loss: 0.9892150163650513\n",
      "Epoch 5466: Training Loss: 0.16839338342348734 Validation Loss: 0.9889103770256042\n",
      "Epoch 5467: Training Loss: 0.1681890736023585 Validation Loss: 0.9889326691627502\n",
      "Epoch 5468: Training Loss: 0.16826781630516052 Validation Loss: 0.9886694550514221\n",
      "Epoch 5469: Training Loss: 0.16866658131281534 Validation Loss: 0.9885807037353516\n",
      "Epoch 5470: Training Loss: 0.16809560855229697 Validation Loss: 0.9888290762901306\n",
      "Epoch 5471: Training Loss: 0.16793603201707205 Validation Loss: 0.9894904494285583\n",
      "Epoch 5472: Training Loss: 0.16850660741329193 Validation Loss: 0.989577054977417\n",
      "Epoch 5473: Training Loss: 0.1681697964668274 Validation Loss: 0.9895548820495605\n",
      "Epoch 5474: Training Loss: 0.16778194904327393 Validation Loss: 0.9892463088035583\n",
      "Epoch 5475: Training Loss: 0.16806109746297201 Validation Loss: 0.9894647598266602\n",
      "Epoch 5476: Training Loss: 0.16939018666744232 Validation Loss: 0.9889582991600037\n",
      "Epoch 5477: Training Loss: 0.16791199147701263 Validation Loss: 0.9882566928863525\n",
      "Epoch 5478: Training Loss: 0.16923825442790985 Validation Loss: 0.9887489676475525\n",
      "Epoch 5479: Training Loss: 0.16862498223781586 Validation Loss: 0.989510178565979\n",
      "Epoch 5480: Training Loss: 0.16785494983196259 Validation Loss: 0.9896076917648315\n",
      "Epoch 5481: Training Loss: 0.168118084470431 Validation Loss: 0.9894251227378845\n",
      "Epoch 5482: Training Loss: 0.16780023276805878 Validation Loss: 0.9897783398628235\n",
      "Epoch 5483: Training Loss: 0.16804335017999014 Validation Loss: 0.9896337985992432\n",
      "Epoch 5484: Training Loss: 0.1674258808294932 Validation Loss: 0.989727795124054\n",
      "Epoch 5485: Training Loss: 0.16794668634732565 Validation Loss: 0.9902207255363464\n",
      "Epoch 5486: Training Loss: 0.16782820224761963 Validation Loss: 0.9900909662246704\n",
      "Epoch 5487: Training Loss: 0.1675857206185659 Validation Loss: 0.989168643951416\n",
      "Epoch 5488: Training Loss: 0.16763056814670563 Validation Loss: 0.9887701869010925\n",
      "Epoch 5489: Training Loss: 0.1673757533232371 Validation Loss: 0.9882491230964661\n",
      "Epoch 5490: Training Loss: 0.1679962674776713 Validation Loss: 0.9886290431022644\n",
      "Epoch 5491: Training Loss: 0.16771077116330466 Validation Loss: 0.9886642694473267\n",
      "Epoch 5492: Training Loss: 0.16776827474435171 Validation Loss: 0.9881637096405029\n",
      "Epoch 5493: Training Loss: 0.16764824092388153 Validation Loss: 0.9891203045845032\n",
      "Epoch 5494: Training Loss: 0.1681070327758789 Validation Loss: 0.9903548359870911\n",
      "Epoch 5495: Training Loss: 0.16788022220134735 Validation Loss: 0.9907540678977966\n",
      "Epoch 5496: Training Loss: 0.16744508842627207 Validation Loss: 0.9903359413146973\n",
      "Epoch 5497: Training Loss: 0.16771884262561798 Validation Loss: 0.9899554252624512\n",
      "Epoch 5498: Training Loss: 0.167401651541392 Validation Loss: 0.9892090559005737\n",
      "Epoch 5499: Training Loss: 0.1675045241912206 Validation Loss: 0.9898587465286255\n",
      "Epoch 5500: Training Loss: 0.16694964468479156 Validation Loss: 0.9898264408111572\n",
      "Epoch 5501: Training Loss: 0.16767866909503937 Validation Loss: 0.9904580116271973\n",
      "Epoch 5502: Training Loss: 0.1671206752459208 Validation Loss: 0.9904767870903015\n",
      "Epoch 5503: Training Loss: 0.16705954571564993 Validation Loss: 0.9906221032142639\n",
      "Epoch 5504: Training Loss: 0.1675104151169459 Validation Loss: 0.9899721145629883\n",
      "Epoch 5505: Training Loss: 0.16701207061608633 Validation Loss: 0.989765465259552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5506: Training Loss: 0.16701137026151022 Validation Loss: 0.9896942973136902\n",
      "Epoch 5507: Training Loss: 0.16693726678689322 Validation Loss: 0.9894443154335022\n",
      "Epoch 5508: Training Loss: 0.1672199865182241 Validation Loss: 0.9893373847007751\n",
      "Epoch 5509: Training Loss: 0.1677831361691157 Validation Loss: 0.9898480772972107\n",
      "Epoch 5510: Training Loss: 0.1676927109559377 Validation Loss: 0.9900487661361694\n",
      "Epoch 5511: Training Loss: 0.16689776877562204 Validation Loss: 0.9899322986602783\n",
      "Epoch 5512: Training Loss: 0.1668021927277247 Validation Loss: 0.9903755187988281\n",
      "Epoch 5513: Training Loss: 0.16694805522759756 Validation Loss: 0.9904329180717468\n",
      "Epoch 5514: Training Loss: 0.16671082377433777 Validation Loss: 0.9902815818786621\n",
      "Epoch 5515: Training Loss: 0.16703220705191293 Validation Loss: 0.9904996752738953\n",
      "Epoch 5516: Training Loss: 0.16730195780595145 Validation Loss: 0.9899585843086243\n",
      "Epoch 5517: Training Loss: 0.1671378711859385 Validation Loss: 0.9896668195724487\n",
      "Epoch 5518: Training Loss: 0.16599549849828085 Validation Loss: 0.9897587895393372\n",
      "Epoch 5519: Training Loss: 0.16693812112013498 Validation Loss: 0.9907795786857605\n",
      "Epoch 5520: Training Loss: 0.16693051159381866 Validation Loss: 0.9903726577758789\n",
      "Epoch 5521: Training Loss: 0.16653947532176971 Validation Loss: 0.9907752275466919\n",
      "Epoch 5522: Training Loss: 0.16651958227157593 Validation Loss: 0.9904513955116272\n",
      "Epoch 5523: Training Loss: 0.16650386154651642 Validation Loss: 0.9904267191886902\n",
      "Epoch 5524: Training Loss: 0.16632546484470367 Validation Loss: 0.9901916980743408\n",
      "Epoch 5525: Training Loss: 0.16645188132921854 Validation Loss: 0.9910351037979126\n",
      "Epoch 5526: Training Loss: 0.16784112652142844 Validation Loss: 0.9906811118125916\n",
      "Epoch 5527: Training Loss: 0.16660084823767343 Validation Loss: 0.9907163977622986\n",
      "Epoch 5528: Training Loss: 0.1664307713508606 Validation Loss: 0.9907642006874084\n",
      "Epoch 5529: Training Loss: 0.16627796490987143 Validation Loss: 0.9902313351631165\n",
      "Epoch 5530: Training Loss: 0.16649352510770163 Validation Loss: 0.9899914860725403\n",
      "Epoch 5531: Training Loss: 0.16648392379283905 Validation Loss: 0.9904252290725708\n",
      "Epoch 5532: Training Loss: 0.16679290433724722 Validation Loss: 0.9912570118904114\n",
      "Epoch 5533: Training Loss: 0.1669544130563736 Validation Loss: 0.9911473989486694\n",
      "Epoch 5534: Training Loss: 0.16612042983373007 Validation Loss: 0.9909186959266663\n",
      "Epoch 5535: Training Loss: 0.16768347720305124 Validation Loss: 0.9907991290092468\n",
      "Epoch 5536: Training Loss: 0.1666103055079778 Validation Loss: 0.9906164407730103\n",
      "Epoch 5537: Training Loss: 0.1660799185434977 Validation Loss: 0.9915904998779297\n",
      "Epoch 5538: Training Loss: 0.16591669619083405 Validation Loss: 0.991037905216217\n",
      "Epoch 5539: Training Loss: 0.16615363955497742 Validation Loss: 0.9916155338287354\n",
      "Epoch 5540: Training Loss: 0.1656845659017563 Validation Loss: 0.9907154440879822\n",
      "Epoch 5541: Training Loss: 0.1655437151590983 Validation Loss: 0.9905559420585632\n",
      "Epoch 5542: Training Loss: 0.16600685318311056 Validation Loss: 0.9910255074501038\n",
      "Epoch 5543: Training Loss: 0.165584127108256 Validation Loss: 0.9916801452636719\n",
      "Epoch 5544: Training Loss: 0.1663071165482203 Validation Loss: 0.991611659526825\n",
      "Epoch 5545: Training Loss: 0.16573352118333182 Validation Loss: 0.9916629791259766\n",
      "Epoch 5546: Training Loss: 0.1659699777762095 Validation Loss: 0.9913113117218018\n",
      "Epoch 5547: Training Loss: 0.16568522651990256 Validation Loss: 0.9910886883735657\n",
      "Epoch 5548: Training Loss: 0.16543914874394736 Validation Loss: 0.990362823009491\n",
      "Epoch 5549: Training Loss: 0.16630931695302328 Validation Loss: 0.990079939365387\n",
      "Epoch 5550: Training Loss: 0.16540620724360147 Validation Loss: 0.9906443953514099\n",
      "Epoch 5551: Training Loss: 0.16609139740467072 Validation Loss: 0.9906289577484131\n",
      "Epoch 5552: Training Loss: 0.165596937139829 Validation Loss: 0.9916069507598877\n",
      "Epoch 5553: Training Loss: 0.16594910124937692 Validation Loss: 0.9919618368148804\n",
      "Epoch 5554: Training Loss: 0.1655746897061666 Validation Loss: 0.9926016330718994\n",
      "Epoch 5555: Training Loss: 0.16598962247371674 Validation Loss: 0.9925886392593384\n",
      "Epoch 5556: Training Loss: 0.1651668002208074 Validation Loss: 0.9913427829742432\n",
      "Epoch 5557: Training Loss: 0.16600360969702402 Validation Loss: 0.9914760589599609\n",
      "Epoch 5558: Training Loss: 0.1658091644446055 Validation Loss: 0.9914388656616211\n",
      "Epoch 5559: Training Loss: 0.16538571814695993 Validation Loss: 0.9911895990371704\n",
      "Epoch 5560: Training Loss: 0.16517969965934753 Validation Loss: 0.9909003973007202\n",
      "Epoch 5561: Training Loss: 0.1657219131787618 Validation Loss: 0.9912075996398926\n",
      "Epoch 5562: Training Loss: 0.1653474966684977 Validation Loss: 0.9912639856338501\n",
      "Epoch 5563: Training Loss: 0.1652797410885493 Validation Loss: 0.9921207427978516\n",
      "Epoch 5564: Training Loss: 0.16527928908665976 Validation Loss: 0.9922033548355103\n",
      "Epoch 5565: Training Loss: 0.16554219524065653 Validation Loss: 0.9916014075279236\n",
      "Epoch 5566: Training Loss: 0.1654357115427653 Validation Loss: 0.991287887096405\n",
      "Epoch 5567: Training Loss: 0.1651784727970759 Validation Loss: 0.9917249083518982\n",
      "Epoch 5568: Training Loss: 0.1655659278233846 Validation Loss: 0.991349995136261\n",
      "Epoch 5569: Training Loss: 0.16535726686318716 Validation Loss: 0.9911405444145203\n",
      "Epoch 5570: Training Loss: 0.1652242342631022 Validation Loss: 0.9910225868225098\n",
      "Epoch 5571: Training Loss: 0.16553184390068054 Validation Loss: 0.9913268685340881\n",
      "Epoch 5572: Training Loss: 0.16625888645648956 Validation Loss: 0.9911776185035706\n",
      "Epoch 5573: Training Loss: 0.16506233314673105 Validation Loss: 0.9919553399085999\n",
      "Epoch 5574: Training Loss: 0.16507264971733093 Validation Loss: 0.991703987121582\n",
      "Epoch 5575: Training Loss: 0.16488326092561087 Validation Loss: 0.9913394451141357\n",
      "Epoch 5576: Training Loss: 0.16493740181128183 Validation Loss: 0.9911360740661621\n",
      "Epoch 5577: Training Loss: 0.1651920278867086 Validation Loss: 0.9910992980003357\n",
      "Epoch 5578: Training Loss: 0.16456347207228342 Validation Loss: 0.991580069065094\n",
      "Epoch 5579: Training Loss: 0.1653813769419988 Validation Loss: 0.9921342730522156\n",
      "Epoch 5580: Training Loss: 0.16625542442003885 Validation Loss: 0.9923785328865051\n",
      "Epoch 5581: Training Loss: 0.1648019403219223 Validation Loss: 0.9921703338623047\n",
      "Epoch 5582: Training Loss: 0.1647606094678243 Validation Loss: 0.9917884469032288\n",
      "Epoch 5583: Training Loss: 0.1655690222978592 Validation Loss: 0.9922758340835571\n",
      "Epoch 5584: Training Loss: 0.1653773138920466 Validation Loss: 0.992526113986969\n",
      "Epoch 5585: Training Loss: 0.1653751234213511 Validation Loss: 0.9929966330528259\n",
      "Epoch 5586: Training Loss: 0.1642608493566513 Validation Loss: 0.9935154914855957\n",
      "Epoch 5587: Training Loss: 0.1643002132574717 Validation Loss: 0.9932755827903748\n",
      "Epoch 5588: Training Loss: 0.16519008576869965 Validation Loss: 0.9928116798400879\n",
      "Epoch 5589: Training Loss: 0.16425021986166635 Validation Loss: 0.9926559329032898\n",
      "Epoch 5590: Training Loss: 0.16484074791272482 Validation Loss: 0.9926104545593262\n",
      "Epoch 5591: Training Loss: 0.16468253235022226 Validation Loss: 0.9917791485786438\n",
      "Epoch 5592: Training Loss: 0.1645395259062449 Validation Loss: 0.9924395084381104\n",
      "Epoch 5593: Training Loss: 0.1646465907494227 Validation Loss: 0.9926336407661438\n",
      "Epoch 5594: Training Loss: 0.16555520395437875 Validation Loss: 0.9927520155906677\n",
      "Epoch 5595: Training Loss: 0.16437287628650665 Validation Loss: 0.9919635653495789\n",
      "Epoch 5596: Training Loss: 0.16459936400254568 Validation Loss: 0.9921543002128601\n",
      "Epoch 5597: Training Loss: 0.1653629938761393 Validation Loss: 0.9919667840003967\n",
      "Epoch 5598: Training Loss: 0.16415178775787354 Validation Loss: 0.9920080304145813\n",
      "Epoch 5599: Training Loss: 0.1644301563501358 Validation Loss: 0.9925617575645447\n",
      "Epoch 5600: Training Loss: 0.16356446842352548 Validation Loss: 0.9935266971588135\n",
      "Epoch 5601: Training Loss: 0.16388167440891266 Validation Loss: 0.9935564398765564\n",
      "Epoch 5602: Training Loss: 0.16333246231079102 Validation Loss: 0.9938732385635376\n",
      "Epoch 5603: Training Loss: 0.16372614105542502 Validation Loss: 0.9933078289031982\n",
      "Epoch 5604: Training Loss: 0.16427427530288696 Validation Loss: 0.9928643703460693\n",
      "Epoch 5605: Training Loss: 0.1641464481751124 Validation Loss: 0.9934433102607727\n",
      "Epoch 5606: Training Loss: 0.16410214205582938 Validation Loss: 0.9929549098014832\n",
      "Epoch 5607: Training Loss: 0.16528876622517905 Validation Loss: 0.9929706454277039\n",
      "Epoch 5608: Training Loss: 0.16341506441434225 Validation Loss: 0.9925692677497864\n",
      "Epoch 5609: Training Loss: 0.1636515607436498 Validation Loss: 0.9936891198158264\n",
      "Epoch 5610: Training Loss: 0.16473332544167837 Validation Loss: 0.9935113787651062\n",
      "Epoch 5611: Training Loss: 0.16396311422189078 Validation Loss: 0.9926719069480896\n",
      "Epoch 5612: Training Loss: 0.16365715861320496 Validation Loss: 0.9925822615623474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5613: Training Loss: 0.16446839769681296 Validation Loss: 0.9926931262016296\n",
      "Epoch 5614: Training Loss: 0.1635750929514567 Validation Loss: 0.9930731058120728\n",
      "Epoch 5615: Training Loss: 0.16355426609516144 Validation Loss: 0.9934832453727722\n",
      "Epoch 5616: Training Loss: 0.1639176905155182 Validation Loss: 0.993501603603363\n",
      "Epoch 5617: Training Loss: 0.1637138972679774 Validation Loss: 0.9934626221656799\n",
      "Epoch 5618: Training Loss: 0.16376161575317383 Validation Loss: 0.9934272766113281\n",
      "Epoch 5619: Training Loss: 0.1630223741134008 Validation Loss: 0.9930717945098877\n",
      "Epoch 5620: Training Loss: 0.16357306639353433 Validation Loss: 0.992827832698822\n",
      "Epoch 5621: Training Loss: 0.16349523762861887 Validation Loss: 0.9934238791465759\n",
      "Epoch 5622: Training Loss: 0.1639930953582128 Validation Loss: 0.9930331707000732\n",
      "Epoch 5623: Training Loss: 0.16318832337856293 Validation Loss: 0.9940955638885498\n",
      "Epoch 5624: Training Loss: 0.1633158673842748 Validation Loss: 0.9941045045852661\n",
      "Epoch 5625: Training Loss: 0.1638321429491043 Validation Loss: 0.9940762519836426\n",
      "Epoch 5626: Training Loss: 0.16350910067558289 Validation Loss: 0.9939699769020081\n",
      "Epoch 5627: Training Loss: 0.1633018602927526 Validation Loss: 0.992884635925293\n",
      "Epoch 5628: Training Loss: 0.16337164243062338 Validation Loss: 0.9924183487892151\n",
      "Epoch 5629: Training Loss: 0.1630693326393763 Validation Loss: 0.9921224117279053\n",
      "Epoch 5630: Training Loss: 0.16326076785723367 Validation Loss: 0.9926037192344666\n",
      "Epoch 5631: Training Loss: 0.16341775159041086 Validation Loss: 0.993733823299408\n",
      "Epoch 5632: Training Loss: 0.16342246532440186 Validation Loss: 0.9939484596252441\n",
      "Epoch 5633: Training Loss: 0.16303209960460663 Validation Loss: 0.993732750415802\n",
      "Epoch 5634: Training Loss: 0.16329174737135568 Validation Loss: 0.9938744902610779\n",
      "Epoch 5635: Training Loss: 0.1653114010890325 Validation Loss: 0.9936152696609497\n",
      "Epoch 5636: Training Loss: 0.16312092542648315 Validation Loss: 0.9937487244606018\n",
      "Epoch 5637: Training Loss: 0.1630837321281433 Validation Loss: 0.9933744668960571\n",
      "Epoch 5638: Training Loss: 0.16451375683148703 Validation Loss: 0.9934493899345398\n",
      "Epoch 5639: Training Loss: 0.16312782963116965 Validation Loss: 0.9931772351264954\n",
      "Epoch 5640: Training Loss: 0.16286013027032217 Validation Loss: 0.9930886626243591\n",
      "Epoch 5641: Training Loss: 0.1630544513463974 Validation Loss: 0.9933430552482605\n",
      "Epoch 5642: Training Loss: 0.16307715574900308 Validation Loss: 0.9937862753868103\n",
      "Epoch 5643: Training Loss: 0.16366400321324667 Validation Loss: 0.9941715598106384\n",
      "Epoch 5644: Training Loss: 0.16283189256985983 Validation Loss: 0.9942938089370728\n",
      "Epoch 5645: Training Loss: 0.16236709554990134 Validation Loss: 0.9939848184585571\n",
      "Epoch 5646: Training Loss: 0.16287857790788016 Validation Loss: 0.9941744208335876\n",
      "Epoch 5647: Training Loss: 0.16563908755779266 Validation Loss: 0.9952666759490967\n",
      "Epoch 5648: Training Loss: 0.16386318703492483 Validation Loss: 0.9954299330711365\n",
      "Epoch 5649: Training Loss: 0.16315743823846182 Validation Loss: 0.9952817559242249\n",
      "Epoch 5650: Training Loss: 0.16313023368517557 Validation Loss: 0.9951516389846802\n",
      "Epoch 5651: Training Loss: 0.1624272664388021 Validation Loss: 0.9942576885223389\n",
      "Epoch 5652: Training Loss: 0.16264483332633972 Validation Loss: 0.9936144948005676\n",
      "Epoch 5653: Training Loss: 0.16318794588247934 Validation Loss: 0.9931519031524658\n",
      "Epoch 5654: Training Loss: 0.16366965572039285 Validation Loss: 0.9927972555160522\n",
      "Epoch 5655: Training Loss: 0.16307265559832254 Validation Loss: 0.9938716292381287\n",
      "Epoch 5656: Training Loss: 0.16352801024913788 Validation Loss: 0.9944603443145752\n",
      "Epoch 5657: Training Loss: 0.16257298986117044 Validation Loss: 0.9951989650726318\n",
      "Epoch 5658: Training Loss: 0.16231802602609 Validation Loss: 0.9946954846382141\n",
      "Epoch 5659: Training Loss: 0.16229690114657083 Validation Loss: 0.9941250681877136\n",
      "Epoch 5660: Training Loss: 0.16272890071074167 Validation Loss: 0.9940813183784485\n",
      "Epoch 5661: Training Loss: 0.1626738359530767 Validation Loss: 0.9943103790283203\n",
      "Epoch 5662: Training Loss: 0.16245116790135702 Validation Loss: 0.9943633079528809\n",
      "Epoch 5663: Training Loss: 0.16221913695335388 Validation Loss: 0.9949342012405396\n",
      "Epoch 5664: Training Loss: 0.1624562293291092 Validation Loss: 0.9944095015525818\n",
      "Epoch 5665: Training Loss: 0.16248628993829092 Validation Loss: 0.9941869974136353\n",
      "Epoch 5666: Training Loss: 0.1624616583188375 Validation Loss: 0.9940834045410156\n",
      "Epoch 5667: Training Loss: 0.1624936560789744 Validation Loss: 0.9939193725585938\n",
      "Epoch 5668: Training Loss: 0.16258621712525687 Validation Loss: 0.9944097399711609\n",
      "Epoch 5669: Training Loss: 0.16310681402683258 Validation Loss: 0.9951362609863281\n",
      "Epoch 5670: Training Loss: 0.16185093422730765 Validation Loss: 0.995049774646759\n",
      "Epoch 5671: Training Loss: 0.16265288492043814 Validation Loss: 0.9954408407211304\n",
      "Epoch 5672: Training Loss: 0.16205051044623056 Validation Loss: 0.9952384829521179\n",
      "Epoch 5673: Training Loss: 0.16218577325344086 Validation Loss: 0.9955095648765564\n",
      "Epoch 5674: Training Loss: 0.16192394495010376 Validation Loss: 0.9955682754516602\n",
      "Epoch 5675: Training Loss: 0.16226166983445486 Validation Loss: 0.9955141544342041\n",
      "Epoch 5676: Training Loss: 0.1621535668770472 Validation Loss: 0.9946649670600891\n",
      "Epoch 5677: Training Loss: 0.1615328242381414 Validation Loss: 0.9943493008613586\n",
      "Epoch 5678: Training Loss: 0.1618071347475052 Validation Loss: 0.9948053956031799\n",
      "Epoch 5679: Training Loss: 0.16117285192012787 Validation Loss: 0.9945910573005676\n",
      "Epoch 5680: Training Loss: 0.1619052787621816 Validation Loss: 0.9948425889015198\n",
      "Epoch 5681: Training Loss: 0.16159937779108682 Validation Loss: 0.9948890805244446\n",
      "Epoch 5682: Training Loss: 0.16165104508399963 Validation Loss: 0.9950823783874512\n",
      "Epoch 5683: Training Loss: 0.1616385579109192 Validation Loss: 0.9946285486221313\n",
      "Epoch 5684: Training Loss: 0.162365992863973 Validation Loss: 0.9953935742378235\n",
      "Epoch 5685: Training Loss: 0.16189482808113098 Validation Loss: 0.9953386783599854\n",
      "Epoch 5686: Training Loss: 0.16127514839172363 Validation Loss: 0.9958478808403015\n",
      "Epoch 5687: Training Loss: 0.16146114468574524 Validation Loss: 0.9953237771987915\n",
      "Epoch 5688: Training Loss: 0.16185670097668967 Validation Loss: 0.9953495860099792\n",
      "Epoch 5689: Training Loss: 0.16162744164466858 Validation Loss: 0.994891345500946\n",
      "Epoch 5690: Training Loss: 0.16174829006195068 Validation Loss: 0.9959250092506409\n",
      "Epoch 5691: Training Loss: 0.16173446675141653 Validation Loss: 0.9953095316886902\n",
      "Epoch 5692: Training Loss: 0.16140933831532797 Validation Loss: 0.9952095746994019\n",
      "Epoch 5693: Training Loss: 0.161780113975207 Validation Loss: 0.9952906370162964\n",
      "Epoch 5694: Training Loss: 0.16152939200401306 Validation Loss: 0.9946485161781311\n",
      "Epoch 5695: Training Loss: 0.16170031328996023 Validation Loss: 0.994515061378479\n",
      "Epoch 5696: Training Loss: 0.16093145310878754 Validation Loss: 0.9941704273223877\n",
      "Epoch 5697: Training Loss: 0.16166126231352487 Validation Loss: 0.99500572681427\n",
      "Epoch 5698: Training Loss: 0.1615414321422577 Validation Loss: 0.9951572418212891\n",
      "Epoch 5699: Training Loss: 0.16110189259052277 Validation Loss: 0.9966646432876587\n",
      "Epoch 5700: Training Loss: 0.16135803858439127 Validation Loss: 0.9962785243988037\n",
      "Epoch 5701: Training Loss: 0.16208813587824503 Validation Loss: 0.9958762526512146\n",
      "Epoch 5702: Training Loss: 0.16184121867020926 Validation Loss: 0.9955345988273621\n",
      "Epoch 5703: Training Loss: 0.1608788768450419 Validation Loss: 0.9961640238761902\n",
      "Epoch 5704: Training Loss: 0.16074330111344656 Validation Loss: 0.996073842048645\n",
      "Epoch 5705: Training Loss: 0.16084076960881552 Validation Loss: 0.9961923956871033\n",
      "Epoch 5706: Training Loss: 0.16212433079878488 Validation Loss: 0.996177613735199\n",
      "Epoch 5707: Training Loss: 0.16061842938264212 Validation Loss: 0.9959851503372192\n",
      "Epoch 5708: Training Loss: 0.16190743446350098 Validation Loss: 0.9967796802520752\n",
      "Epoch 5709: Training Loss: 0.161721204717954 Validation Loss: 0.9969953894615173\n",
      "Epoch 5710: Training Loss: 0.16088775297005972 Validation Loss: 0.9962870478630066\n",
      "Epoch 5711: Training Loss: 0.16079367200533548 Validation Loss: 0.9956102967262268\n",
      "Epoch 5712: Training Loss: 0.16088227927684784 Validation Loss: 0.9948709607124329\n",
      "Epoch 5713: Training Loss: 0.1610005001227061 Validation Loss: 0.9952492117881775\n",
      "Epoch 5714: Training Loss: 0.16291331748167673 Validation Loss: 0.995465874671936\n",
      "Epoch 5715: Training Loss: 0.16060333450635275 Validation Loss: 0.9954089522361755\n",
      "Epoch 5716: Training Loss: 0.16138823827107748 Validation Loss: 0.9957137703895569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5717: Training Loss: 0.16062352557977042 Validation Loss: 0.9958071112632751\n",
      "Epoch 5718: Training Loss: 0.16075000166893005 Validation Loss: 0.9958948493003845\n",
      "Epoch 5719: Training Loss: 0.16080949703852335 Validation Loss: 0.9960219264030457\n",
      "Epoch 5720: Training Loss: 0.16047402719656625 Validation Loss: 0.9965011477470398\n",
      "Epoch 5721: Training Loss: 0.16119113067785898 Validation Loss: 0.9961284399032593\n",
      "Epoch 5722: Training Loss: 0.1606683830420176 Validation Loss: 0.9959012866020203\n",
      "Epoch 5723: Training Loss: 0.16084146996339163 Validation Loss: 0.9956659078598022\n",
      "Epoch 5724: Training Loss: 0.16035713255405426 Validation Loss: 0.9960669875144958\n",
      "Epoch 5725: Training Loss: 0.16038409372170767 Validation Loss: 0.9967852830886841\n",
      "Epoch 5726: Training Loss: 0.16049762070178986 Validation Loss: 0.9968536496162415\n",
      "Epoch 5727: Training Loss: 0.16024806598822275 Validation Loss: 0.99692702293396\n",
      "Epoch 5728: Training Loss: 0.16028501590092978 Validation Loss: 0.9969150424003601\n",
      "Epoch 5729: Training Loss: 0.16045472025871277 Validation Loss: 0.9964779615402222\n",
      "Epoch 5730: Training Loss: 0.16080403824647269 Validation Loss: 0.9963520169258118\n",
      "Epoch 5731: Training Loss: 0.16095732152462006 Validation Loss: 0.9962077736854553\n",
      "Epoch 5732: Training Loss: 0.1605448772509893 Validation Loss: 0.9958712458610535\n",
      "Epoch 5733: Training Loss: 0.1606314480304718 Validation Loss: 0.996492862701416\n",
      "Epoch 5734: Training Loss: 0.16040669878323874 Validation Loss: 0.9972642660140991\n",
      "Epoch 5735: Training Loss: 0.16101556519667307 Validation Loss: 0.9967226982116699\n",
      "Epoch 5736: Training Loss: 0.16000765562057495 Validation Loss: 0.9967371225357056\n",
      "Epoch 5737: Training Loss: 0.1599406599998474 Validation Loss: 0.9970389008522034\n",
      "Epoch 5738: Training Loss: 0.16007550557454428 Validation Loss: 0.9966511726379395\n",
      "Epoch 5739: Training Loss: 0.16119449337323508 Validation Loss: 0.9969189167022705\n",
      "Epoch 5740: Training Loss: 0.16016075511773428 Validation Loss: 0.9960923194885254\n",
      "Epoch 5741: Training Loss: 0.1599739988644918 Validation Loss: 0.9962254762649536\n",
      "Epoch 5742: Training Loss: 0.16020209093888602 Validation Loss: 0.9964187145233154\n",
      "Epoch 5743: Training Loss: 0.16107173264026642 Validation Loss: 0.9963244199752808\n",
      "Epoch 5744: Training Loss: 0.1589076668024063 Validation Loss: 0.9966665506362915\n",
      "Epoch 5745: Training Loss: 0.16005892058213553 Validation Loss: 0.9965593814849854\n",
      "Epoch 5746: Training Loss: 0.1598017861445745 Validation Loss: 0.9964751601219177\n",
      "Epoch 5747: Training Loss: 0.16017991304397583 Validation Loss: 0.9963963031768799\n",
      "Epoch 5748: Training Loss: 0.1594719191392263 Validation Loss: 0.9964914321899414\n",
      "Epoch 5749: Training Loss: 0.16109671195348105 Validation Loss: 0.9967575073242188\n",
      "Epoch 5750: Training Loss: 0.16059468686580658 Validation Loss: 0.9978968501091003\n",
      "Epoch 5751: Training Loss: 0.1598158081372579 Validation Loss: 0.9983238577842712\n",
      "Epoch 5752: Training Loss: 0.1603604257106781 Validation Loss: 0.9971808195114136\n",
      "Epoch 5753: Training Loss: 0.15977923572063446 Validation Loss: 0.997214674949646\n",
      "Epoch 5754: Training Loss: 0.16002664963404337 Validation Loss: 0.9965193271636963\n",
      "Epoch 5755: Training Loss: 0.16064909100532532 Validation Loss: 0.9965109825134277\n",
      "Epoch 5756: Training Loss: 0.15947291254997253 Validation Loss: 0.9964883327484131\n",
      "Epoch 5757: Training Loss: 0.15947498877843222 Validation Loss: 0.9967623353004456\n",
      "Epoch 5758: Training Loss: 0.15983850012222925 Validation Loss: 0.9968681931495667\n",
      "Epoch 5759: Training Loss: 0.15992827216784158 Validation Loss: 0.9974508285522461\n",
      "Epoch 5760: Training Loss: 0.1593823085228602 Validation Loss: 0.9969171285629272\n",
      "Epoch 5761: Training Loss: 0.15971887608369192 Validation Loss: 0.9967899322509766\n",
      "Epoch 5762: Training Loss: 0.15967110792795816 Validation Loss: 0.9970804452896118\n",
      "Epoch 5763: Training Loss: 0.15982692937056223 Validation Loss: 0.9972802996635437\n",
      "Epoch 5764: Training Loss: 0.15931256612141928 Validation Loss: 0.9969179034233093\n",
      "Epoch 5765: Training Loss: 0.1596105992794037 Validation Loss: 0.9974806308746338\n",
      "Epoch 5766: Training Loss: 0.15920670330524445 Validation Loss: 0.9977850914001465\n",
      "Epoch 5767: Training Loss: 0.1591376562913259 Validation Loss: 0.998221218585968\n",
      "Epoch 5768: Training Loss: 0.15944325427214304 Validation Loss: 0.9981591701507568\n",
      "Epoch 5769: Training Loss: 0.15979930758476257 Validation Loss: 0.9977492690086365\n",
      "Epoch 5770: Training Loss: 0.15947753190994263 Validation Loss: 0.9977098107337952\n",
      "Epoch 5771: Training Loss: 0.16071128845214844 Validation Loss: 0.9970777034759521\n",
      "Epoch 5772: Training Loss: 0.16044004758199057 Validation Loss: 0.9971746206283569\n",
      "Epoch 5773: Training Loss: 0.1592650612195333 Validation Loss: 0.9980250597000122\n",
      "Epoch 5774: Training Loss: 0.15924727420012155 Validation Loss: 0.9984122514724731\n",
      "Epoch 5775: Training Loss: 0.15965592861175537 Validation Loss: 0.9984714388847351\n",
      "Epoch 5776: Training Loss: 0.1591030160586039 Validation Loss: 0.9986404180526733\n",
      "Epoch 5777: Training Loss: 0.15908450384934744 Validation Loss: 0.9986277222633362\n",
      "Epoch 5778: Training Loss: 0.15911460916201273 Validation Loss: 0.9983475208282471\n",
      "Epoch 5779: Training Loss: 0.15888693432013193 Validation Loss: 0.9981002807617188\n",
      "Epoch 5780: Training Loss: 0.15961271027723947 Validation Loss: 0.9981545209884644\n",
      "Epoch 5781: Training Loss: 0.15890456239382425 Validation Loss: 0.9982560873031616\n",
      "Epoch 5782: Training Loss: 0.15893721083799997 Validation Loss: 0.9984480142593384\n",
      "Epoch 5783: Training Loss: 0.15915334224700928 Validation Loss: 0.9979943037033081\n",
      "Epoch 5784: Training Loss: 0.15880295634269714 Validation Loss: 0.997752845287323\n",
      "Epoch 5785: Training Loss: 0.1586704800526301 Validation Loss: 0.9976792335510254\n",
      "Epoch 5786: Training Loss: 0.15871774156888327 Validation Loss: 0.9977962970733643\n",
      "Epoch 5787: Training Loss: 0.15900560716787973 Validation Loss: 0.9979490637779236\n",
      "Epoch 5788: Training Loss: 0.15898439288139343 Validation Loss: 0.9973006844520569\n",
      "Epoch 5789: Training Loss: 0.1585845798254013 Validation Loss: 0.9981330037117004\n",
      "Epoch 5790: Training Loss: 0.15938538312911987 Validation Loss: 0.9983410835266113\n",
      "Epoch 5791: Training Loss: 0.15861867864926657 Validation Loss: 0.9986710548400879\n",
      "Epoch 5792: Training Loss: 0.16017299890518188 Validation Loss: 0.9987696409225464\n",
      "Epoch 5793: Training Loss: 0.1583813081185023 Validation Loss: 0.9977639317512512\n",
      "Epoch 5794: Training Loss: 0.1585436910390854 Validation Loss: 0.9980685114860535\n",
      "Epoch 5795: Training Loss: 0.15885368982950845 Validation Loss: 0.9975096583366394\n",
      "Epoch 5796: Training Loss: 0.158022070924441 Validation Loss: 0.9979903697967529\n",
      "Epoch 5797: Training Loss: 0.1586795598268509 Validation Loss: 0.9976963400840759\n",
      "Epoch 5798: Training Loss: 0.1583586484193802 Validation Loss: 0.9983700513839722\n",
      "Epoch 5799: Training Loss: 0.15873031814893088 Validation Loss: 0.999349057674408\n",
      "Epoch 5800: Training Loss: 0.1590314656496048 Validation Loss: 0.9986245036125183\n",
      "Epoch 5801: Training Loss: 0.15817306439081827 Validation Loss: 0.9983017444610596\n",
      "Epoch 5802: Training Loss: 0.1583003451426824 Validation Loss: 0.9986599087715149\n",
      "Epoch 5803: Training Loss: 0.15825359026590982 Validation Loss: 0.9990347027778625\n",
      "Epoch 5804: Training Loss: 0.15846232573191324 Validation Loss: 0.99941486120224\n",
      "Epoch 5805: Training Loss: 0.15854973097642264 Validation Loss: 0.9993677139282227\n",
      "Epoch 5806: Training Loss: 0.1588797022898992 Validation Loss: 0.9990269541740417\n",
      "Epoch 5807: Training Loss: 0.15811187525590262 Validation Loss: 0.9984931349754333\n",
      "Epoch 5808: Training Loss: 0.1580807218949 Validation Loss: 0.9986153841018677\n",
      "Epoch 5809: Training Loss: 0.15897932648658752 Validation Loss: 0.998552143573761\n",
      "Epoch 5810: Training Loss: 0.1579934706290563 Validation Loss: 0.9986893534660339\n",
      "Epoch 5811: Training Loss: 0.15822296837965646 Validation Loss: 0.9985659122467041\n",
      "Epoch 5812: Training Loss: 0.15796016156673431 Validation Loss: 0.9987995028495789\n",
      "Epoch 5813: Training Loss: 0.1579110473394394 Validation Loss: 0.9984754920005798\n",
      "Epoch 5814: Training Loss: 0.157939612865448 Validation Loss: 0.998418390750885\n",
      "Epoch 5815: Training Loss: 0.15813564757506052 Validation Loss: 0.9986225366592407\n",
      "Epoch 5816: Training Loss: 0.15852205455303192 Validation Loss: 0.9993972182273865\n",
      "Epoch 5817: Training Loss: 0.15818029642105103 Validation Loss: 1.0000300407409668\n",
      "Epoch 5818: Training Loss: 0.15812555452187857 Validation Loss: 0.9995993375778198\n",
      "Epoch 5819: Training Loss: 0.1581624448299408 Validation Loss: 0.9993330240249634\n",
      "Epoch 5820: Training Loss: 0.15842702984809875 Validation Loss: 0.9996724724769592\n",
      "Epoch 5821: Training Loss: 0.15809219082196554 Validation Loss: 0.9992542862892151\n",
      "Epoch 5822: Training Loss: 0.15796301265557608 Validation Loss: 0.9996201992034912\n",
      "Epoch 5823: Training Loss: 0.15837586422761282 Validation Loss: 0.9994417428970337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5824: Training Loss: 0.15771598120530447 Validation Loss: 0.9990843534469604\n",
      "Epoch 5825: Training Loss: 0.15762032071749368 Validation Loss: 0.9991546869277954\n",
      "Epoch 5826: Training Loss: 0.15773790578047434 Validation Loss: 0.9982193112373352\n",
      "Epoch 5827: Training Loss: 0.15832953651746115 Validation Loss: 0.9982343912124634\n",
      "Epoch 5828: Training Loss: 0.15790683527787527 Validation Loss: 0.9985280632972717\n",
      "Epoch 5829: Training Loss: 0.15815595785776773 Validation Loss: 0.9987667798995972\n",
      "Epoch 5830: Training Loss: 0.15774070223172507 Validation Loss: 0.9989825487136841\n",
      "Epoch 5831: Training Loss: 0.15749028821786246 Validation Loss: 0.9989566802978516\n",
      "Epoch 5832: Training Loss: 0.1575555553038915 Validation Loss: 0.9997978806495667\n",
      "Epoch 5833: Training Loss: 0.15769855181376138 Validation Loss: 1.0001182556152344\n",
      "Epoch 5834: Training Loss: 0.15746224919954935 Validation Loss: 0.9999050498008728\n",
      "Epoch 5835: Training Loss: 0.15766392151514688 Validation Loss: 0.9998822212219238\n",
      "Epoch 5836: Training Loss: 0.15884770452976227 Validation Loss: 0.9995990991592407\n",
      "Epoch 5837: Training Loss: 0.15703273812929788 Validation Loss: 0.9995774626731873\n",
      "Epoch 5838: Training Loss: 0.15728950997193655 Validation Loss: 0.9996634721755981\n",
      "Epoch 5839: Training Loss: 0.15701389809449515 Validation Loss: 0.9995434880256653\n",
      "Epoch 5840: Training Loss: 0.15679488082726797 Validation Loss: 0.9992316961288452\n",
      "Epoch 5841: Training Loss: 0.15671688318252563 Validation Loss: 0.9994109869003296\n",
      "Epoch 5842: Training Loss: 0.15745139122009277 Validation Loss: 0.9995833039283752\n",
      "Epoch 5843: Training Loss: 0.15723973512649536 Validation Loss: 0.9993132948875427\n",
      "Epoch 5844: Training Loss: 0.15711230039596558 Validation Loss: 0.9997047185897827\n",
      "Epoch 5845: Training Loss: 0.15730472902456918 Validation Loss: 1.0000780820846558\n",
      "Epoch 5846: Training Loss: 0.15735488633314768 Validation Loss: 1.0002704858779907\n",
      "Epoch 5847: Training Loss: 0.15754434466362 Validation Loss: 0.9999133348464966\n",
      "Epoch 5848: Training Loss: 0.1575614015261332 Validation Loss: 0.9989625215530396\n",
      "Epoch 5849: Training Loss: 0.15712349116802216 Validation Loss: 0.9993808269500732\n",
      "Epoch 5850: Training Loss: 0.15741180876890817 Validation Loss: 0.9994948506355286\n",
      "Epoch 5851: Training Loss: 0.1578246901432673 Validation Loss: 0.9997113943099976\n",
      "Epoch 5852: Training Loss: 0.15783985455830893 Validation Loss: 1.000326156616211\n",
      "Epoch 5853: Training Loss: 0.15757838388284048 Validation Loss: 1.0002373456954956\n",
      "Epoch 5854: Training Loss: 0.1577734649181366 Validation Loss: 1.0007128715515137\n",
      "Epoch 5855: Training Loss: 0.15747709572315216 Validation Loss: 1.0004907846450806\n",
      "Epoch 5856: Training Loss: 0.1567311187585195 Validation Loss: 1.0008018016815186\n",
      "Epoch 5857: Training Loss: 0.15698924660682678 Validation Loss: 1.0014801025390625\n",
      "Epoch 5858: Training Loss: 0.15719829499721527 Validation Loss: 1.0009878873825073\n",
      "Epoch 5859: Training Loss: 0.15667655070622763 Validation Loss: 1.0007911920547485\n",
      "Epoch 5860: Training Loss: 0.1567703535159429 Validation Loss: 0.9999351501464844\n",
      "Epoch 5861: Training Loss: 0.15692038337389627 Validation Loss: 0.999920129776001\n",
      "Epoch 5862: Training Loss: 0.15658608078956604 Validation Loss: 0.9997662901878357\n",
      "Epoch 5863: Training Loss: 0.1569724828004837 Validation Loss: 0.9996897578239441\n",
      "Epoch 5864: Training Loss: 0.1569527139266332 Validation Loss: 0.9999136924743652\n",
      "Epoch 5865: Training Loss: 0.15669575333595276 Validation Loss: 1.0006844997406006\n",
      "Epoch 5866: Training Loss: 0.156995619336764 Validation Loss: 1.0004619359970093\n",
      "Epoch 5867: Training Loss: 0.15750931451718012 Validation Loss: 1.000228762626648\n",
      "Epoch 5868: Training Loss: 0.1563613216082255 Validation Loss: 0.9997286200523376\n",
      "Epoch 5869: Training Loss: 0.1567943592866262 Validation Loss: 0.9995250701904297\n",
      "Epoch 5870: Training Loss: 0.15657642483711243 Validation Loss: 0.999234676361084\n",
      "Epoch 5871: Training Loss: 0.1586365575591723 Validation Loss: 0.999725878238678\n",
      "Epoch 5872: Training Loss: 0.1571370760599772 Validation Loss: 1.000423550605774\n",
      "Epoch 5873: Training Loss: 0.15670178333918253 Validation Loss: 1.0003242492675781\n",
      "Epoch 5874: Training Loss: 0.1562866767247518 Validation Loss: 1.0005502700805664\n",
      "Epoch 5875: Training Loss: 0.1569263438383738 Validation Loss: 1.0007320642471313\n",
      "Epoch 5876: Training Loss: 0.1563935230175654 Validation Loss: 1.0012097358703613\n",
      "Epoch 5877: Training Loss: 0.15736369788646698 Validation Loss: 1.002050518989563\n",
      "Epoch 5878: Training Loss: 0.1560763716697693 Validation Loss: 1.0018588304519653\n",
      "Epoch 5879: Training Loss: 0.15656428535779318 Validation Loss: 1.0015608072280884\n",
      "Epoch 5880: Training Loss: 0.15652529895305634 Validation Loss: 1.0014920234680176\n",
      "Epoch 5881: Training Loss: 0.15638664861520132 Validation Loss: 1.0016952753067017\n",
      "Epoch 5882: Training Loss: 0.1562043527762095 Validation Loss: 1.0009280443191528\n",
      "Epoch 5883: Training Loss: 0.15568080047766367 Validation Loss: 1.0005799531936646\n",
      "Epoch 5884: Training Loss: 0.15653515607118607 Validation Loss: 1.000133991241455\n",
      "Epoch 5885: Training Loss: 0.15630750358104706 Validation Loss: 0.9999929666519165\n",
      "Epoch 5886: Training Loss: 0.1562849928935369 Validation Loss: 1.0009026527404785\n",
      "Epoch 5887: Training Loss: 0.15620257953802744 Validation Loss: 1.0012004375457764\n",
      "Epoch 5888: Training Loss: 0.15691814571619034 Validation Loss: 1.0013397932052612\n",
      "Epoch 5889: Training Loss: 0.15587210655212402 Validation Loss: 1.0008697509765625\n",
      "Epoch 5890: Training Loss: 0.1567748263478279 Validation Loss: 1.0010358095169067\n",
      "Epoch 5891: Training Loss: 0.15625356634457907 Validation Loss: 1.0007483959197998\n",
      "Epoch 5892: Training Loss: 0.15777356922626495 Validation Loss: 1.000244379043579\n",
      "Epoch 5893: Training Loss: 0.1557606558005015 Validation Loss: 1.0011221170425415\n",
      "Epoch 5894: Training Loss: 0.15654700001080832 Validation Loss: 1.001501202583313\n",
      "Epoch 5895: Training Loss: 0.15584921836853027 Validation Loss: 1.0018595457077026\n",
      "Epoch 5896: Training Loss: 0.1560411403576533 Validation Loss: 1.001652717590332\n",
      "Epoch 5897: Training Loss: 0.15646919111410776 Validation Loss: 1.0025360584259033\n",
      "Epoch 5898: Training Loss: 0.1559356302022934 Validation Loss: 1.0020592212677002\n",
      "Epoch 5899: Training Loss: 0.1556361218293508 Validation Loss: 1.0020521879196167\n",
      "Epoch 5900: Training Loss: 0.1560276597738266 Validation Loss: 1.0009928941726685\n",
      "Epoch 5901: Training Loss: 0.15639382104078928 Validation Loss: 1.0000101327896118\n",
      "Epoch 5902: Training Loss: 0.1560796101888021 Validation Loss: 1.0001097917556763\n",
      "Epoch 5903: Training Loss: 0.15593078235785165 Validation Loss: 1.0005265474319458\n",
      "Epoch 5904: Training Loss: 0.15606330831845602 Validation Loss: 1.0011016130447388\n",
      "Epoch 5905: Training Loss: 0.15553938845793405 Validation Loss: 1.0017330646514893\n",
      "Epoch 5906: Training Loss: 0.15543411175409952 Validation Loss: 1.001723051071167\n",
      "Epoch 5907: Training Loss: 0.15571186443169913 Validation Loss: 1.000612497329712\n",
      "Epoch 5908: Training Loss: 0.15603605409463248 Validation Loss: 1.0014896392822266\n",
      "Epoch 5909: Training Loss: 0.15627977748711905 Validation Loss: 1.00191068649292\n",
      "Epoch 5910: Training Loss: 0.15521049002806345 Validation Loss: 1.0026836395263672\n",
      "Epoch 5911: Training Loss: 0.15554081896940866 Validation Loss: 1.0025585889816284\n",
      "Epoch 5912: Training Loss: 0.15511413415273032 Validation Loss: 1.00143301486969\n",
      "Epoch 5913: Training Loss: 0.15513470768928528 Validation Loss: 1.001976728439331\n",
      "Epoch 5914: Training Loss: 0.1551420787970225 Validation Loss: 1.0014902353286743\n",
      "Epoch 5915: Training Loss: 0.15528941651185355 Validation Loss: 1.001128911972046\n",
      "Epoch 5916: Training Loss: 0.15665393571058908 Validation Loss: 1.0007288455963135\n",
      "Epoch 5917: Training Loss: 0.15504576762517294 Validation Loss: 1.0018208026885986\n",
      "Epoch 5918: Training Loss: 0.15480393171310425 Validation Loss: 1.0025033950805664\n",
      "Epoch 5919: Training Loss: 0.15508748094240823 Validation Loss: 1.0029551982879639\n",
      "Epoch 5920: Training Loss: 0.15525561074415842 Validation Loss: 1.0024919509887695\n",
      "Epoch 5921: Training Loss: 0.15482799212137857 Validation Loss: 1.0022633075714111\n",
      "Epoch 5922: Training Loss: 0.15573124587535858 Validation Loss: 1.0018359422683716\n",
      "Epoch 5923: Training Loss: 0.15578295787175497 Validation Loss: 1.0018821954727173\n",
      "Epoch 5924: Training Loss: 0.15522625048955283 Validation Loss: 1.0025442838668823\n",
      "Epoch 5925: Training Loss: 0.15482416252295175 Validation Loss: 1.003016471862793\n",
      "Epoch 5926: Training Loss: 0.15431581685940424 Validation Loss: 1.0024900436401367\n",
      "Epoch 5927: Training Loss: 0.15565724670886993 Validation Loss: 1.0017261505126953\n",
      "Epoch 5928: Training Loss: 0.15572063128153482 Validation Loss: 1.0011862516403198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5929: Training Loss: 0.15569738547007242 Validation Loss: 1.0011898279190063\n",
      "Epoch 5930: Training Loss: 0.15529789527257284 Validation Loss: 1.0009803771972656\n",
      "Epoch 5931: Training Loss: 0.1552506536245346 Validation Loss: 1.0011545419692993\n",
      "Epoch 5932: Training Loss: 0.15558217962582907 Validation Loss: 1.001674771308899\n",
      "Epoch 5933: Training Loss: 0.15493585666020712 Validation Loss: 1.0026756525039673\n",
      "Epoch 5934: Training Loss: 0.15475983917713165 Validation Loss: 1.0027899742126465\n",
      "Epoch 5935: Training Loss: 0.15498127539952597 Validation Loss: 1.0033705234527588\n",
      "Epoch 5936: Training Loss: 0.15493304034074148 Validation Loss: 1.0030803680419922\n",
      "Epoch 5937: Training Loss: 0.1545305997133255 Validation Loss: 1.0023999214172363\n",
      "Epoch 5938: Training Loss: 0.15533776581287384 Validation Loss: 1.0019313097000122\n",
      "Epoch 5939: Training Loss: 0.1547701209783554 Validation Loss: 1.0015885829925537\n",
      "Epoch 5940: Training Loss: 0.1548192799091339 Validation Loss: 1.0016121864318848\n",
      "Epoch 5941: Training Loss: 0.1547826329867045 Validation Loss: 1.002175211906433\n",
      "Epoch 5942: Training Loss: 0.15426302949587503 Validation Loss: 1.0022697448730469\n",
      "Epoch 5943: Training Loss: 0.15464679400126138 Validation Loss: 1.0030686855316162\n",
      "Epoch 5944: Training Loss: 0.15471643706162772 Validation Loss: 1.0035700798034668\n",
      "Epoch 5945: Training Loss: 0.15482732156912485 Validation Loss: 1.0029515027999878\n",
      "Epoch 5946: Training Loss: 0.15641348560651144 Validation Loss: 1.0019594430923462\n",
      "Epoch 5947: Training Loss: 0.15476531783739725 Validation Loss: 1.0027201175689697\n",
      "Epoch 5948: Training Loss: 0.15488776564598083 Validation Loss: 1.0032399892807007\n",
      "Epoch 5949: Training Loss: 0.15480306247870126 Validation Loss: 1.0031222105026245\n",
      "Epoch 5950: Training Loss: 0.15449615816275278 Validation Loss: 1.0028221607208252\n",
      "Epoch 5951: Training Loss: 0.15478619933128357 Validation Loss: 1.0022451877593994\n",
      "Epoch 5952: Training Loss: 0.15477088590463003 Validation Loss: 1.00276517868042\n",
      "Epoch 5953: Training Loss: 0.15465731422106424 Validation Loss: 1.0027562379837036\n",
      "Epoch 5954: Training Loss: 0.15479847292105356 Validation Loss: 1.0026686191558838\n",
      "Epoch 5955: Training Loss: 0.1541904161373774 Validation Loss: 1.003119945526123\n",
      "Epoch 5956: Training Loss: 0.15433602035045624 Validation Loss: 1.0033782720565796\n",
      "Epoch 5957: Training Loss: 0.15507439275582632 Validation Loss: 1.0027333498001099\n",
      "Epoch 5958: Training Loss: 0.15408269067605337 Validation Loss: 1.0028705596923828\n",
      "Epoch 5959: Training Loss: 0.1537021944920222 Validation Loss: 1.0030243396759033\n",
      "Epoch 5960: Training Loss: 0.15410774449507395 Validation Loss: 1.0031378269195557\n",
      "Epoch 5961: Training Loss: 0.15402242044607797 Validation Loss: 1.0030877590179443\n",
      "Epoch 5962: Training Loss: 0.15411228934923807 Validation Loss: 1.0031362771987915\n",
      "Epoch 5963: Training Loss: 0.15398454666137695 Validation Loss: 1.0035662651062012\n",
      "Epoch 5964: Training Loss: 0.15444114804267883 Validation Loss: 1.0036085844039917\n",
      "Epoch 5965: Training Loss: 0.15417437255382538 Validation Loss: 1.0032421350479126\n",
      "Epoch 5966: Training Loss: 0.154182568192482 Validation Loss: 1.0037606954574585\n",
      "Epoch 5967: Training Loss: 0.15369186798731485 Validation Loss: 1.0039423704147339\n",
      "Epoch 5968: Training Loss: 0.15419869124889374 Validation Loss: 1.0043342113494873\n",
      "Epoch 5969: Training Loss: 0.1540302832921346 Validation Loss: 1.0034505128860474\n",
      "Epoch 5970: Training Loss: 0.15366977949937186 Validation Loss: 1.0031784772872925\n",
      "Epoch 5971: Training Loss: 0.1538851112127304 Validation Loss: 1.0033345222473145\n",
      "Epoch 5972: Training Loss: 0.1537361890077591 Validation Loss: 1.0030088424682617\n",
      "Epoch 5973: Training Loss: 0.15389219423135123 Validation Loss: 1.002903938293457\n",
      "Epoch 5974: Training Loss: 0.15396648148695627 Validation Loss: 1.0031565427780151\n",
      "Epoch 5975: Training Loss: 0.15409131348133087 Validation Loss: 1.0029246807098389\n",
      "Epoch 5976: Training Loss: 0.1540145923693975 Validation Loss: 1.003686785697937\n",
      "Epoch 5977: Training Loss: 0.1545243263244629 Validation Loss: 1.002832293510437\n",
      "Epoch 5978: Training Loss: 0.15391836067040762 Validation Loss: 1.0030921697616577\n",
      "Epoch 5979: Training Loss: 0.15378640095392862 Validation Loss: 1.003645896911621\n",
      "Epoch 5980: Training Loss: 0.15398731331030527 Validation Loss: 1.0039699077606201\n",
      "Epoch 5981: Training Loss: 0.1537915567557017 Validation Loss: 1.003394365310669\n",
      "Epoch 5982: Training Loss: 0.1527059773604075 Validation Loss: 1.004015326499939\n",
      "Epoch 5983: Training Loss: 0.1538012574116389 Validation Loss: 1.0043959617614746\n",
      "Epoch 5984: Training Loss: 0.15391687552134195 Validation Loss: 1.0042511224746704\n",
      "Epoch 5985: Training Loss: 0.1539401262998581 Validation Loss: 1.004990816116333\n",
      "Epoch 5986: Training Loss: 0.1538291573524475 Validation Loss: 1.0053861141204834\n",
      "Epoch 5987: Training Loss: 0.15427357951800028 Validation Loss: 1.0050088167190552\n",
      "Epoch 5988: Training Loss: 0.15342543025811514 Validation Loss: 1.0041626691818237\n",
      "Epoch 5989: Training Loss: 0.1535618007183075 Validation Loss: 1.0036431550979614\n",
      "Epoch 5990: Training Loss: 0.15336559216181436 Validation Loss: 1.0027164220809937\n",
      "Epoch 5991: Training Loss: 0.1531862368186315 Validation Loss: 1.002589225769043\n",
      "Epoch 5992: Training Loss: 0.15336615343888602 Validation Loss: 1.0039715766906738\n",
      "Epoch 5993: Training Loss: 0.1537028799454371 Validation Loss: 1.0041098594665527\n",
      "Epoch 5994: Training Loss: 0.1534801870584488 Validation Loss: 1.0034316778182983\n",
      "Epoch 5995: Training Loss: 0.15342183411121368 Validation Loss: 1.0029804706573486\n",
      "Epoch 5996: Training Loss: 0.15366274615128836 Validation Loss: 1.0034799575805664\n",
      "Epoch 5997: Training Loss: 0.15319488942623138 Validation Loss: 1.0037405490875244\n",
      "Epoch 5998: Training Loss: 0.15334019561608633 Validation Loss: 1.0033601522445679\n",
      "Epoch 5999: Training Loss: 0.15342338879903158 Validation Loss: 1.0038987398147583\n",
      "Epoch 6000: Training Loss: 0.1531300644079844 Validation Loss: 1.003814458847046\n",
      "Epoch 6001: Training Loss: 0.1531547506650289 Validation Loss: 1.003943681716919\n",
      "Epoch 6002: Training Loss: 0.15321312844753265 Validation Loss: 1.0042271614074707\n",
      "Epoch 6003: Training Loss: 0.15301073590914407 Validation Loss: 1.0038620233535767\n",
      "Epoch 6004: Training Loss: 0.1530771553516388 Validation Loss: 1.003642201423645\n",
      "Epoch 6005: Training Loss: 0.1529786984125773 Validation Loss: 1.0035587549209595\n",
      "Epoch 6006: Training Loss: 0.1539464294910431 Validation Loss: 1.0036284923553467\n",
      "Epoch 6007: Training Loss: 0.15391687800486883 Validation Loss: 1.0039359331130981\n",
      "Epoch 6008: Training Loss: 0.15287386377652487 Validation Loss: 1.0040371417999268\n",
      "Epoch 6009: Training Loss: 0.15292172133922577 Validation Loss: 1.0038880109786987\n",
      "Epoch 6010: Training Loss: 0.15273521343866983 Validation Loss: 1.003832221031189\n",
      "Epoch 6011: Training Loss: 0.15315288801987967 Validation Loss: 1.0045109987258911\n",
      "Epoch 6012: Training Loss: 0.15358146528402963 Validation Loss: 1.0053194761276245\n",
      "Epoch 6013: Training Loss: 0.15255301694075266 Validation Loss: 1.0051981210708618\n",
      "Epoch 6014: Training Loss: 0.1530142774184545 Validation Loss: 1.005316138267517\n",
      "Epoch 6015: Training Loss: 0.15269672870635986 Validation Loss: 1.0049872398376465\n",
      "Epoch 6016: Training Loss: 0.1529327630996704 Validation Loss: 1.0045422315597534\n",
      "Epoch 6017: Training Loss: 0.1527384122212728 Validation Loss: 1.0047794580459595\n",
      "Epoch 6018: Training Loss: 0.15259230633576712 Validation Loss: 1.0047478675842285\n",
      "Epoch 6019: Training Loss: 0.152835746606191 Validation Loss: 1.0053683519363403\n",
      "Epoch 6020: Training Loss: 0.1528503249088923 Validation Loss: 1.0060666799545288\n",
      "Epoch 6021: Training Loss: 0.1529427021741867 Validation Loss: 1.0053552389144897\n",
      "Epoch 6022: Training Loss: 0.15246564149856567 Validation Loss: 1.0049434900283813\n",
      "Epoch 6023: Training Loss: 0.15338894973198572 Validation Loss: 1.0050984621047974\n",
      "Epoch 6024: Training Loss: 0.15281632045904794 Validation Loss: 1.0049229860305786\n",
      "Epoch 6025: Training Loss: 0.15265055994192758 Validation Loss: 1.005554437637329\n",
      "Epoch 6026: Training Loss: 0.15269896388053894 Validation Loss: 1.0051823854446411\n",
      "Epoch 6027: Training Loss: 0.15278686583042145 Validation Loss: 1.0048421621322632\n",
      "Epoch 6028: Training Loss: 0.15243049959341684 Validation Loss: 1.0049670934677124\n",
      "Epoch 6029: Training Loss: 0.15242746969064078 Validation Loss: 1.0054196119308472\n",
      "Epoch 6030: Training Loss: 0.153403510649999 Validation Loss: 1.0055938959121704\n",
      "Epoch 6031: Training Loss: 0.15282265345255533 Validation Loss: 1.0052387714385986\n",
      "Epoch 6032: Training Loss: 0.15295044084390005 Validation Loss: 1.0049091577529907\n",
      "Epoch 6033: Training Loss: 0.1525447964668274 Validation Loss: 1.0050292015075684\n",
      "Epoch 6034: Training Loss: 0.15264704326788583 Validation Loss: 1.0052227973937988\n",
      "Epoch 6035: Training Loss: 0.15243830780188242 Validation Loss: 1.0052543878555298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6036: Training Loss: 0.1520919899145762 Validation Loss: 1.0057806968688965\n",
      "Epoch 6037: Training Loss: 0.152262344956398 Validation Loss: 1.0058382749557495\n",
      "Epoch 6038: Training Loss: 0.1522177110115687 Validation Loss: 1.0059967041015625\n",
      "Epoch 6039: Training Loss: 0.15226112802823386 Validation Loss: 1.0062575340270996\n",
      "Epoch 6040: Training Loss: 0.15262756745020548 Validation Loss: 1.0060631036758423\n",
      "Epoch 6041: Training Loss: 0.15227014819780985 Validation Loss: 1.0066895484924316\n",
      "Epoch 6042: Training Loss: 0.152171790599823 Validation Loss: 1.0061756372451782\n",
      "Epoch 6043: Training Loss: 0.15213488539059958 Validation Loss: 1.0057246685028076\n",
      "Epoch 6044: Training Loss: 0.15224626660346985 Validation Loss: 1.0058153867721558\n",
      "Epoch 6045: Training Loss: 0.15222911536693573 Validation Loss: 1.0055208206176758\n",
      "Epoch 6046: Training Loss: 0.15252453088760376 Validation Loss: 1.0050097703933716\n",
      "Epoch 6047: Training Loss: 0.15240796903769174 Validation Loss: 1.0055732727050781\n",
      "Epoch 6048: Training Loss: 0.1524899254242579 Validation Loss: 1.0057176351547241\n",
      "Epoch 6049: Training Loss: 0.15203288942575455 Validation Loss: 1.0050631761550903\n",
      "Epoch 6050: Training Loss: 0.15187562505404154 Validation Loss: 1.0052502155303955\n",
      "Epoch 6051: Training Loss: 0.15229667723178864 Validation Loss: 1.0047518014907837\n",
      "Epoch 6052: Training Loss: 0.15182475745677948 Validation Loss: 1.0046180486679077\n",
      "Epoch 6053: Training Loss: 0.15223459402720133 Validation Loss: 1.0049628019332886\n",
      "Epoch 6054: Training Loss: 0.15191452701886496 Validation Loss: 1.0059894323349\n",
      "Epoch 6055: Training Loss: 0.15132878224054971 Validation Loss: 1.0065326690673828\n",
      "Epoch 6056: Training Loss: 0.15159592032432556 Validation Loss: 1.0058643817901611\n",
      "Epoch 6057: Training Loss: 0.15150596698125204 Validation Loss: 1.005345344543457\n",
      "Epoch 6058: Training Loss: 0.15158693492412567 Validation Loss: 1.0056565999984741\n",
      "Epoch 6059: Training Loss: 0.1518917679786682 Validation Loss: 1.0053786039352417\n",
      "Epoch 6060: Training Loss: 0.1515544056892395 Validation Loss: 1.0059434175491333\n",
      "Epoch 6061: Training Loss: 0.15142892797787985 Validation Loss: 1.006657361984253\n",
      "Epoch 6062: Training Loss: 0.15153184036413828 Validation Loss: 1.0067713260650635\n",
      "Epoch 6063: Training Loss: 0.15161197384198508 Validation Loss: 1.0063673257827759\n",
      "Epoch 6064: Training Loss: 0.15141192575295767 Validation Loss: 1.005759835243225\n",
      "Epoch 6065: Training Loss: 0.15172312160332999 Validation Loss: 1.0058575868606567\n",
      "Epoch 6066: Training Loss: 0.1519331435362498 Validation Loss: 1.0059574842453003\n",
      "Epoch 6067: Training Loss: 0.15146616101264954 Validation Loss: 1.0059353113174438\n",
      "Epoch 6068: Training Loss: 0.15172702074050903 Validation Loss: 1.0065668821334839\n",
      "Epoch 6069: Training Loss: 0.15120250980059305 Validation Loss: 1.0074632167816162\n",
      "Epoch 6070: Training Loss: 0.15161900222301483 Validation Loss: 1.0074458122253418\n",
      "Epoch 6071: Training Loss: 0.15129661560058594 Validation Loss: 1.0067353248596191\n",
      "Epoch 6072: Training Loss: 0.1515519618988037 Validation Loss: 1.0066522359848022\n",
      "Epoch 6073: Training Loss: 0.1511473407347997 Validation Loss: 1.0060091018676758\n",
      "Epoch 6074: Training Loss: 0.15156257152557373 Validation Loss: 1.0061017274856567\n",
      "Epoch 6075: Training Loss: 0.1515813022851944 Validation Loss: 1.0064921379089355\n",
      "Epoch 6076: Training Loss: 0.1505865901708603 Validation Loss: 1.0067291259765625\n",
      "Epoch 6077: Training Loss: 0.1512224624554316 Validation Loss: 1.0067261457443237\n",
      "Epoch 6078: Training Loss: 0.15294198940197626 Validation Loss: 1.006460189819336\n",
      "Epoch 6079: Training Loss: 0.15158268809318542 Validation Loss: 1.006392478942871\n",
      "Epoch 6080: Training Loss: 0.15085706611474356 Validation Loss: 1.0060044527053833\n",
      "Epoch 6081: Training Loss: 0.15102907518545786 Validation Loss: 1.006880283355713\n",
      "Epoch 6082: Training Loss: 0.15059980750083923 Validation Loss: 1.0073164701461792\n",
      "Epoch 6083: Training Loss: 0.15157508850097656 Validation Loss: 1.0068869590759277\n",
      "Epoch 6084: Training Loss: 0.15133142471313477 Validation Loss: 1.0081321001052856\n",
      "Epoch 6085: Training Loss: 0.15080508589744568 Validation Loss: 1.0075061321258545\n",
      "Epoch 6086: Training Loss: 0.15097380677858988 Validation Loss: 1.007767677307129\n",
      "Epoch 6087: Training Loss: 0.15125002960364023 Validation Loss: 1.0072989463806152\n",
      "Epoch 6088: Training Loss: 0.15094534556070963 Validation Loss: 1.007272481918335\n",
      "Epoch 6089: Training Loss: 0.15084118644396463 Validation Loss: 1.0070031881332397\n",
      "Epoch 6090: Training Loss: 0.15108703076839447 Validation Loss: 1.0066077709197998\n",
      "Epoch 6091: Training Loss: 0.1511071672042211 Validation Loss: 1.0065358877182007\n",
      "Epoch 6092: Training Loss: 0.1509649952252706 Validation Loss: 1.006905198097229\n",
      "Epoch 6093: Training Loss: 0.15135488907496134 Validation Loss: 1.0073587894439697\n",
      "Epoch 6094: Training Loss: 0.15084137519200644 Validation Loss: 1.0073603391647339\n",
      "Epoch 6095: Training Loss: 0.15098672608534494 Validation Loss: 1.0069249868392944\n",
      "Epoch 6096: Training Loss: 0.1506907840569814 Validation Loss: 1.007474422454834\n",
      "Epoch 6097: Training Loss: 0.15078546603520712 Validation Loss: 1.0068399906158447\n",
      "Epoch 6098: Training Loss: 0.1506660431623459 Validation Loss: 1.006807804107666\n",
      "Epoch 6099: Training Loss: 0.15079892675081888 Validation Loss: 1.0073535442352295\n",
      "Epoch 6100: Training Loss: 0.1509589304526647 Validation Loss: 1.0074818134307861\n",
      "Epoch 6101: Training Loss: 0.15057699382305145 Validation Loss: 1.0077235698699951\n",
      "Epoch 6102: Training Loss: 0.15053799748420715 Validation Loss: 1.0064207315444946\n",
      "Epoch 6103: Training Loss: 0.15157566467920938 Validation Loss: 1.0068705081939697\n",
      "Epoch 6104: Training Loss: 0.1507529765367508 Validation Loss: 1.007840871810913\n",
      "Epoch 6105: Training Loss: 0.15077009797096252 Validation Loss: 1.0082292556762695\n",
      "Epoch 6106: Training Loss: 0.15078100562095642 Validation Loss: 1.008184552192688\n",
      "Epoch 6107: Training Loss: 0.15044997135798135 Validation Loss: 1.0080524682998657\n",
      "Epoch 6108: Training Loss: 0.15120761593182883 Validation Loss: 1.0069884061813354\n",
      "Epoch 6109: Training Loss: 0.1508996586004893 Validation Loss: 1.0067640542984009\n",
      "Epoch 6110: Training Loss: 0.15136357645193735 Validation Loss: 1.0067404508590698\n",
      "Epoch 6111: Training Loss: 0.15055408577124277 Validation Loss: 1.0069252252578735\n",
      "Epoch 6112: Training Loss: 0.15096157789230347 Validation Loss: 1.0074368715286255\n",
      "Epoch 6113: Training Loss: 0.15033913652102152 Validation Loss: 1.007379174232483\n",
      "Epoch 6114: Training Loss: 0.15012710789839426 Validation Loss: 1.0075933933258057\n",
      "Epoch 6115: Training Loss: 0.15089792013168335 Validation Loss: 1.007746934890747\n",
      "Epoch 6116: Training Loss: 0.1509228547414144 Validation Loss: 1.0088260173797607\n",
      "Epoch 6117: Training Loss: 0.15026167531808218 Validation Loss: 1.0085068941116333\n",
      "Epoch 6118: Training Loss: 0.15041305124759674 Validation Loss: 1.0087083578109741\n",
      "Epoch 6119: Training Loss: 0.15045964221159616 Validation Loss: 1.0087484121322632\n",
      "Epoch 6120: Training Loss: 0.15065857271353403 Validation Loss: 1.0083372592926025\n",
      "Epoch 6121: Training Loss: 0.15034550925095877 Validation Loss: 1.0074131488800049\n",
      "Epoch 6122: Training Loss: 0.1501078854004542 Validation Loss: 1.0074493885040283\n",
      "Epoch 6123: Training Loss: 0.15007719894250235 Validation Loss: 1.0070714950561523\n",
      "Epoch 6124: Training Loss: 0.15033074220021567 Validation Loss: 1.0075352191925049\n",
      "Epoch 6125: Training Loss: 0.15015265345573425 Validation Loss: 1.0085294246673584\n",
      "Epoch 6126: Training Loss: 0.1500274439652761 Validation Loss: 1.009397268295288\n",
      "Epoch 6127: Training Loss: 0.15104595323403677 Validation Loss: 1.0090700387954712\n",
      "Epoch 6128: Training Loss: 0.15053760508696237 Validation Loss: 1.0081039667129517\n",
      "Epoch 6129: Training Loss: 0.1501226226488749 Validation Loss: 1.007585048675537\n",
      "Epoch 6130: Training Loss: 0.15012015153964361 Validation Loss: 1.007768988609314\n",
      "Epoch 6131: Training Loss: 0.1497552196184794 Validation Loss: 1.0080220699310303\n",
      "Epoch 6132: Training Loss: 0.15077169239521027 Validation Loss: 1.007835865020752\n",
      "Epoch 6133: Training Loss: 0.15075106918811798 Validation Loss: 1.0085251331329346\n",
      "Epoch 6134: Training Loss: 0.15002188086509705 Validation Loss: 1.0084940195083618\n",
      "Epoch 6135: Training Loss: 0.14997597535451254 Validation Loss: 1.0084973573684692\n",
      "Epoch 6136: Training Loss: 0.15060968697071075 Validation Loss: 1.0085850954055786\n",
      "Epoch 6137: Training Loss: 0.14973315596580505 Validation Loss: 1.0080817937850952\n",
      "Epoch 6138: Training Loss: 0.14976360400517783 Validation Loss: 1.0092474222183228\n",
      "Epoch 6139: Training Loss: 0.15000328421592712 Validation Loss: 1.0093908309936523\n",
      "Epoch 6140: Training Loss: 0.15000114341576895 Validation Loss: 1.0086085796356201\n",
      "Epoch 6141: Training Loss: 0.15003200868765512 Validation Loss: 1.0079976320266724\n",
      "Epoch 6142: Training Loss: 0.15043689807256064 Validation Loss: 1.007969856262207\n",
      "Epoch 6143: Training Loss: 0.15013903876145682 Validation Loss: 1.0085487365722656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6144: Training Loss: 0.1497977077960968 Validation Loss: 1.0089364051818848\n",
      "Epoch 6145: Training Loss: 0.14904466271400452 Validation Loss: 1.0088304281234741\n",
      "Epoch 6146: Training Loss: 0.149602472782135 Validation Loss: 1.0093660354614258\n",
      "Epoch 6147: Training Loss: 0.1504936714967092 Validation Loss: 1.0091471672058105\n",
      "Epoch 6148: Training Loss: 0.14953174193700156 Validation Loss: 1.0083770751953125\n",
      "Epoch 6149: Training Loss: 0.14937535425027212 Validation Loss: 1.0090723037719727\n",
      "Epoch 6150: Training Loss: 0.14966052770614624 Validation Loss: 1.0091661214828491\n",
      "Epoch 6151: Training Loss: 0.14993550876776376 Validation Loss: 1.0094327926635742\n",
      "Epoch 6152: Training Loss: 0.14982353150844574 Validation Loss: 1.009155511856079\n",
      "Epoch 6153: Training Loss: 0.1491160492102305 Validation Loss: 1.0093327760696411\n",
      "Epoch 6154: Training Loss: 0.14973277846972147 Validation Loss: 1.0088534355163574\n",
      "Epoch 6155: Training Loss: 0.14942365884780884 Validation Loss: 1.0086238384246826\n",
      "Epoch 6156: Training Loss: 0.14963717261950174 Validation Loss: 1.0085442066192627\n",
      "Epoch 6157: Training Loss: 0.1492657164732615 Validation Loss: 1.0088626146316528\n",
      "Epoch 6158: Training Loss: 0.15015019476413727 Validation Loss: 1.009148359298706\n",
      "Epoch 6159: Training Loss: 0.14891028900941214 Validation Loss: 1.009543776512146\n",
      "Epoch 6160: Training Loss: 0.14948882659276327 Validation Loss: 1.008914589881897\n",
      "Epoch 6161: Training Loss: 0.14955970644950867 Validation Loss: 1.0081136226654053\n",
      "Epoch 6162: Training Loss: 0.14888487259546915 Validation Loss: 1.0084664821624756\n",
      "Epoch 6163: Training Loss: 0.1491935153802236 Validation Loss: 1.008989691734314\n",
      "Epoch 6164: Training Loss: 0.14938909312089285 Validation Loss: 1.0087525844573975\n",
      "Epoch 6165: Training Loss: 0.14837784816821417 Validation Loss: 1.0093425512313843\n",
      "Epoch 6166: Training Loss: 0.14913501342137656 Validation Loss: 1.0097908973693848\n",
      "Epoch 6167: Training Loss: 0.1490140507618586 Validation Loss: 1.009994626045227\n",
      "Epoch 6168: Training Loss: 0.1490623007218043 Validation Loss: 1.010202407836914\n",
      "Epoch 6169: Training Loss: 0.14938759307066599 Validation Loss: 1.0097016096115112\n",
      "Epoch 6170: Training Loss: 0.14917625486850739 Validation Loss: 1.0083398818969727\n",
      "Epoch 6171: Training Loss: 0.1492913564046224 Validation Loss: 1.0081621408462524\n",
      "Epoch 6172: Training Loss: 0.149000883102417 Validation Loss: 1.0083187818527222\n",
      "Epoch 6173: Training Loss: 0.1488825430472692 Validation Loss: 1.0087032318115234\n",
      "Epoch 6174: Training Loss: 0.14892506102720895 Validation Loss: 1.0096789598464966\n",
      "Epoch 6175: Training Loss: 0.14910373588403067 Validation Loss: 1.0097098350524902\n",
      "Epoch 6176: Training Loss: 0.1487837334473928 Validation Loss: 1.010502815246582\n",
      "Epoch 6177: Training Loss: 0.1487635374069214 Validation Loss: 1.0099525451660156\n",
      "Epoch 6178: Training Loss: 0.1484635422627131 Validation Loss: 1.0097782611846924\n",
      "Epoch 6179: Training Loss: 0.14828723669052124 Validation Loss: 1.0101817846298218\n",
      "Epoch 6180: Training Loss: 0.14857804775238037 Validation Loss: 1.0104331970214844\n",
      "Epoch 6181: Training Loss: 0.14796448747316995 Validation Loss: 1.0102524757385254\n",
      "Epoch 6182: Training Loss: 0.1490579048792521 Validation Loss: 1.010895848274231\n",
      "Epoch 6183: Training Loss: 0.14899206161499023 Validation Loss: 1.009687900543213\n",
      "Epoch 6184: Training Loss: 0.1487564891576767 Validation Loss: 1.0092408657073975\n",
      "Epoch 6185: Training Loss: 0.14905633529027304 Validation Loss: 1.0086240768432617\n",
      "Epoch 6186: Training Loss: 0.14837743838628134 Validation Loss: 1.008731484413147\n",
      "Epoch 6187: Training Loss: 0.14848491549491882 Validation Loss: 1.0098087787628174\n",
      "Epoch 6188: Training Loss: 0.148577610651652 Validation Loss: 1.0100938081741333\n",
      "Epoch 6189: Training Loss: 0.14821816980838776 Validation Loss: 1.0111922025680542\n",
      "Epoch 6190: Training Loss: 0.14849497129519781 Validation Loss: 1.0101567506790161\n",
      "Epoch 6191: Training Loss: 0.14886447290579477 Validation Loss: 1.0102691650390625\n",
      "Epoch 6192: Training Loss: 0.14840401212374368 Validation Loss: 1.0102989673614502\n",
      "Epoch 6193: Training Loss: 0.14958939949671426 Validation Loss: 1.0100687742233276\n",
      "Epoch 6194: Training Loss: 0.1487310230731964 Validation Loss: 1.0101710557937622\n",
      "Epoch 6195: Training Loss: 0.14913753171761832 Validation Loss: 1.0101096630096436\n",
      "Epoch 6196: Training Loss: 0.1484156275788943 Validation Loss: 1.0102615356445312\n",
      "Epoch 6197: Training Loss: 0.1482522189617157 Validation Loss: 1.0101304054260254\n",
      "Epoch 6198: Training Loss: 0.14856583873430887 Validation Loss: 1.0103938579559326\n",
      "Epoch 6199: Training Loss: 0.14880937337875366 Validation Loss: 1.0108839273452759\n",
      "Epoch 6200: Training Loss: 0.14831909040609995 Validation Loss: 1.0115889310836792\n",
      "Epoch 6201: Training Loss: 0.14937442044417062 Validation Loss: 1.0107609033584595\n",
      "Epoch 6202: Training Loss: 0.14836696286996207 Validation Loss: 1.0099338293075562\n",
      "Epoch 6203: Training Loss: 0.14831271767616272 Validation Loss: 1.0098233222961426\n",
      "Epoch 6204: Training Loss: 0.14780665934085846 Validation Loss: 1.0097320079803467\n",
      "Epoch 6205: Training Loss: 0.14811482032140097 Validation Loss: 1.0097931623458862\n",
      "Epoch 6206: Training Loss: 0.1482908328374227 Validation Loss: 1.0099810361862183\n",
      "Epoch 6207: Training Loss: 0.1483307679494222 Validation Loss: 1.0107085704803467\n",
      "Epoch 6208: Training Loss: 0.14806534349918365 Validation Loss: 1.0103930234909058\n",
      "Epoch 6209: Training Loss: 0.14817306399345398 Validation Loss: 1.0109111070632935\n",
      "Epoch 6210: Training Loss: 0.1482193817694982 Validation Loss: 1.0118509531021118\n",
      "Epoch 6211: Training Loss: 0.14809087415536246 Validation Loss: 1.011285424232483\n",
      "Epoch 6212: Training Loss: 0.14865154027938843 Validation Loss: 1.0108892917633057\n",
      "Epoch 6213: Training Loss: 0.14862511803706488 Validation Loss: 1.010920524597168\n",
      "Epoch 6214: Training Loss: 0.1480443775653839 Validation Loss: 1.0104808807373047\n",
      "Epoch 6215: Training Loss: 0.14835921426614126 Validation Loss: 1.011038064956665\n",
      "Epoch 6216: Training Loss: 0.14819152156511942 Validation Loss: 1.010947585105896\n",
      "Epoch 6217: Training Loss: 0.14798848827679953 Validation Loss: 1.0113439559936523\n",
      "Epoch 6218: Training Loss: 0.14841230710347494 Validation Loss: 1.0108000040054321\n",
      "Epoch 6219: Training Loss: 0.14805370072523752 Validation Loss: 1.0114432573318481\n",
      "Epoch 6220: Training Loss: 0.14773573478062949 Validation Loss: 1.010581374168396\n",
      "Epoch 6221: Training Loss: 0.14777424931526184 Validation Loss: 1.0107173919677734\n",
      "Epoch 6222: Training Loss: 0.14803364872932434 Validation Loss: 1.0105173587799072\n",
      "Epoch 6223: Training Loss: 0.14807962874571481 Validation Loss: 1.0105537176132202\n",
      "Epoch 6224: Training Loss: 0.1477367083231608 Validation Loss: 1.0105456113815308\n",
      "Epoch 6225: Training Loss: 0.1475553661584854 Validation Loss: 1.0109050273895264\n",
      "Epoch 6226: Training Loss: 0.14793642858664194 Validation Loss: 1.0113811492919922\n",
      "Epoch 6227: Training Loss: 0.14821501076221466 Validation Loss: 1.0109758377075195\n",
      "Epoch 6228: Training Loss: 0.14775646726290384 Validation Loss: 1.0107781887054443\n",
      "Epoch 6229: Training Loss: 0.14760381976763406 Validation Loss: 1.0101131200790405\n",
      "Epoch 6230: Training Loss: 0.147416482369105 Validation Loss: 1.0107133388519287\n",
      "Epoch 6231: Training Loss: 0.14771191775798798 Validation Loss: 1.0108733177185059\n",
      "Epoch 6232: Training Loss: 0.14775793751080832 Validation Loss: 1.0116506814956665\n",
      "Epoch 6233: Training Loss: 0.14814806481202444 Validation Loss: 1.012312412261963\n",
      "Epoch 6234: Training Loss: 0.14773511389891306 Validation Loss: 1.0117405652999878\n",
      "Epoch 6235: Training Loss: 0.1474784811337789 Validation Loss: 1.0108826160430908\n",
      "Epoch 6236: Training Loss: 0.14761650562286377 Validation Loss: 1.011200189590454\n",
      "Epoch 6237: Training Loss: 0.14743012686570486 Validation Loss: 1.011109709739685\n",
      "Epoch 6238: Training Loss: 0.14756307502587637 Validation Loss: 1.0115991830825806\n",
      "Epoch 6239: Training Loss: 0.14734070499738058 Validation Loss: 1.0121021270751953\n",
      "Epoch 6240: Training Loss: 0.1474537452061971 Validation Loss: 1.0122164487838745\n",
      "Epoch 6241: Training Loss: 0.1474790871143341 Validation Loss: 1.0117571353912354\n",
      "Epoch 6242: Training Loss: 0.1476852223277092 Validation Loss: 1.0112740993499756\n",
      "Epoch 6243: Training Loss: 0.14843534926573435 Validation Loss: 1.0102843046188354\n",
      "Epoch 6244: Training Loss: 0.14724074800809225 Validation Loss: 1.0101114511489868\n",
      "Epoch 6245: Training Loss: 0.14801892141501108 Validation Loss: 1.010200023651123\n",
      "Epoch 6246: Training Loss: 0.14753341674804688 Validation Loss: 1.0103163719177246\n",
      "Epoch 6247: Training Loss: 0.14660471181074777 Validation Loss: 1.0112571716308594\n",
      "Epoch 6248: Training Loss: 0.14747423181931177 Validation Loss: 1.0119705200195312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6249: Training Loss: 0.14823783934116364 Validation Loss: 1.0130391120910645\n",
      "Epoch 6250: Training Loss: 0.14729237059752145 Validation Loss: 1.012975811958313\n",
      "Epoch 6251: Training Loss: 0.14811508854230246 Validation Loss: 1.013110876083374\n",
      "Epoch 6252: Training Loss: 0.14736081659793854 Validation Loss: 1.0120526552200317\n",
      "Epoch 6253: Training Loss: 0.14710023999214172 Validation Loss: 1.0121158361434937\n",
      "Epoch 6254: Training Loss: 0.14736270407835642 Validation Loss: 1.0120903253555298\n",
      "Epoch 6255: Training Loss: 0.14701160788536072 Validation Loss: 1.0132958889007568\n",
      "Epoch 6256: Training Loss: 0.14701735973358154 Validation Loss: 1.013309121131897\n",
      "Epoch 6257: Training Loss: 0.14700079957644144 Validation Loss: 1.0129470825195312\n",
      "Epoch 6258: Training Loss: 0.1469879945119222 Validation Loss: 1.012460708618164\n",
      "Epoch 6259: Training Loss: 0.14672781030337015 Validation Loss: 1.0119510889053345\n",
      "Epoch 6260: Training Loss: 0.14720394710699716 Validation Loss: 1.012006402015686\n",
      "Epoch 6261: Training Loss: 0.14682715634504953 Validation Loss: 1.0113515853881836\n",
      "Epoch 6262: Training Loss: 0.1475392853220304 Validation Loss: 1.0112775564193726\n",
      "Epoch 6263: Training Loss: 0.1471499353647232 Validation Loss: 1.01187264919281\n",
      "Epoch 6264: Training Loss: 0.14718387027581534 Validation Loss: 1.012001633644104\n",
      "Epoch 6265: Training Loss: 0.14733014504114786 Validation Loss: 1.011699914932251\n",
      "Epoch 6266: Training Loss: 0.14730955163637796 Validation Loss: 1.0118556022644043\n",
      "Epoch 6267: Training Loss: 0.1469616194566091 Validation Loss: 1.0124166011810303\n",
      "Epoch 6268: Training Loss: 0.1465697636206945 Validation Loss: 1.0123882293701172\n",
      "Epoch 6269: Training Loss: 0.14654328425725302 Validation Loss: 1.0120198726654053\n",
      "Epoch 6270: Training Loss: 0.14658696949481964 Validation Loss: 1.0124229192733765\n",
      "Epoch 6271: Training Loss: 0.14673874278863272 Validation Loss: 1.0129872560501099\n",
      "Epoch 6272: Training Loss: 0.14622636636098227 Validation Loss: 1.0125929117202759\n",
      "Epoch 6273: Training Loss: 0.14707068602244058 Validation Loss: 1.0122302770614624\n",
      "Epoch 6274: Training Loss: 0.14670355121294656 Validation Loss: 1.0124783515930176\n",
      "Epoch 6275: Training Loss: 0.14654302100340524 Validation Loss: 1.0125659704208374\n",
      "Epoch 6276: Training Loss: 0.1464842607577642 Validation Loss: 1.0129809379577637\n",
      "Epoch 6277: Training Loss: 0.14726664374272028 Validation Loss: 1.0127155780792236\n",
      "Epoch 6278: Training Loss: 0.14745273689428964 Validation Loss: 1.012345552444458\n",
      "Epoch 6279: Training Loss: 0.14663227399190268 Validation Loss: 1.0125007629394531\n",
      "Epoch 6280: Training Loss: 0.14725671708583832 Validation Loss: 1.012727975845337\n",
      "Epoch 6281: Training Loss: 0.14711837967236838 Validation Loss: 1.0126886367797852\n",
      "Epoch 6282: Training Loss: 0.14698461691538492 Validation Loss: 1.0122137069702148\n",
      "Epoch 6283: Training Loss: 0.14639411369959512 Validation Loss: 1.0124281644821167\n",
      "Epoch 6284: Training Loss: 0.14601550002892813 Validation Loss: 1.0126296281814575\n",
      "Epoch 6285: Training Loss: 0.14632372558116913 Validation Loss: 1.0130478143692017\n",
      "Epoch 6286: Training Loss: 0.14625253280003866 Validation Loss: 1.0132869482040405\n",
      "Epoch 6287: Training Loss: 0.14639214674631754 Validation Loss: 1.0138607025146484\n",
      "Epoch 6288: Training Loss: 0.14620208243529 Validation Loss: 1.0140851736068726\n",
      "Epoch 6289: Training Loss: 0.1462834576765696 Validation Loss: 1.0147590637207031\n",
      "Epoch 6290: Training Loss: 0.1463655779759089 Validation Loss: 1.0145692825317383\n",
      "Epoch 6291: Training Loss: 0.14635002116362253 Validation Loss: 1.0144695043563843\n",
      "Epoch 6292: Training Loss: 0.14717605461676916 Validation Loss: 1.013654112815857\n",
      "Epoch 6293: Training Loss: 0.14655145506064096 Validation Loss: 1.0125478506088257\n",
      "Epoch 6294: Training Loss: 0.14681672801574072 Validation Loss: 1.0121891498565674\n",
      "Epoch 6295: Training Loss: 0.14686376849810281 Validation Loss: 1.0129276514053345\n",
      "Epoch 6296: Training Loss: 0.14609748125076294 Validation Loss: 1.0133966207504272\n",
      "Epoch 6297: Training Loss: 0.14620453615983328 Validation Loss: 1.0132336616516113\n",
      "Epoch 6298: Training Loss: 0.1461596886316935 Validation Loss: 1.0130951404571533\n",
      "Epoch 6299: Training Loss: 0.14675012230873108 Validation Loss: 1.012993574142456\n",
      "Epoch 6300: Training Loss: 0.1461497445901235 Validation Loss: 1.013227105140686\n",
      "Epoch 6301: Training Loss: 0.14745411525170007 Validation Loss: 1.0128037929534912\n",
      "Epoch 6302: Training Loss: 0.14595993359883627 Validation Loss: 1.0123928785324097\n",
      "Epoch 6303: Training Loss: 0.14587834477424622 Validation Loss: 1.0125981569290161\n",
      "Epoch 6304: Training Loss: 0.14631713926792145 Validation Loss: 1.013214349746704\n",
      "Epoch 6305: Training Loss: 0.1462950905164083 Validation Loss: 1.013932466506958\n",
      "Epoch 6306: Training Loss: 0.14589294294516245 Validation Loss: 1.0137897729873657\n",
      "Epoch 6307: Training Loss: 0.14600755274295807 Validation Loss: 1.013908863067627\n",
      "Epoch 6308: Training Loss: 0.1463815967241923 Validation Loss: 1.0142250061035156\n",
      "Epoch 6309: Training Loss: 0.14582745730876923 Validation Loss: 1.0145870447158813\n",
      "Epoch 6310: Training Loss: 0.14637550711631775 Validation Loss: 1.0133171081542969\n",
      "Epoch 6311: Training Loss: 0.14589899281660715 Validation Loss: 1.0135201215744019\n",
      "Epoch 6312: Training Loss: 0.14704347401857376 Validation Loss: 1.0134878158569336\n",
      "Epoch 6313: Training Loss: 0.14590340356032053 Validation Loss: 1.0136849880218506\n",
      "Epoch 6314: Training Loss: 0.1461703528960546 Validation Loss: 1.0138452053070068\n",
      "Epoch 6315: Training Loss: 0.14719476799170175 Validation Loss: 1.0136650800704956\n",
      "Epoch 6316: Training Loss: 0.14564023911952972 Validation Loss: 1.0139068365097046\n",
      "Epoch 6317: Training Loss: 0.14571444690227509 Validation Loss: 1.0136851072311401\n",
      "Epoch 6318: Training Loss: 0.14567465583483377 Validation Loss: 1.0135778188705444\n",
      "Epoch 6319: Training Loss: 0.1458283563454946 Validation Loss: 1.0131100416183472\n",
      "Epoch 6320: Training Loss: 0.1457668791214625 Validation Loss: 1.0132673978805542\n",
      "Epoch 6321: Training Loss: 0.14521293342113495 Validation Loss: 1.0134731531143188\n",
      "Epoch 6322: Training Loss: 0.145826518535614 Validation Loss: 1.0140153169631958\n",
      "Epoch 6323: Training Loss: 0.14586066206296286 Validation Loss: 1.0139223337173462\n",
      "Epoch 6324: Training Loss: 0.14579479893048605 Validation Loss: 1.014276146888733\n",
      "Epoch 6325: Training Loss: 0.1453214387098948 Validation Loss: 1.013877272605896\n",
      "Epoch 6326: Training Loss: 0.14633576075236002 Validation Loss: 1.0145432949066162\n",
      "Epoch 6327: Training Loss: 0.1454065591096878 Validation Loss: 1.0149136781692505\n",
      "Epoch 6328: Training Loss: 0.14503765602906546 Validation Loss: 1.015652060508728\n",
      "Epoch 6329: Training Loss: 0.14542690416177115 Validation Loss: 1.0145374536514282\n",
      "Epoch 6330: Training Loss: 0.14421029885609946 Validation Loss: 1.0139684677124023\n",
      "Epoch 6331: Training Loss: 0.14512670040130615 Validation Loss: 1.0137180089950562\n",
      "Epoch 6332: Training Loss: 0.14529894789059958 Validation Loss: 1.0135869979858398\n",
      "Epoch 6333: Training Loss: 0.14534708857536316 Validation Loss: 1.0141230821609497\n",
      "Epoch 6334: Training Loss: 0.1449650451540947 Validation Loss: 1.0141023397445679\n",
      "Epoch 6335: Training Loss: 0.14521009226640066 Validation Loss: 1.0144059658050537\n",
      "Epoch 6336: Training Loss: 0.14513406654198965 Validation Loss: 1.0145106315612793\n",
      "Epoch 6337: Training Loss: 0.144985760251681 Validation Loss: 1.0149025917053223\n",
      "Epoch 6338: Training Loss: 0.14541377127170563 Validation Loss: 1.0147011280059814\n",
      "Epoch 6339: Training Loss: 0.14568622907002768 Validation Loss: 1.014182209968567\n",
      "Epoch 6340: Training Loss: 0.14524195591608682 Validation Loss: 1.0149651765823364\n",
      "Epoch 6341: Training Loss: 0.14512240886688232 Validation Loss: 1.0143743753433228\n",
      "Epoch 6342: Training Loss: 0.14517673353354135 Validation Loss: 1.0138514041900635\n",
      "Epoch 6343: Training Loss: 0.1449170559644699 Validation Loss: 1.0143905878067017\n",
      "Epoch 6344: Training Loss: 0.14512576659520468 Validation Loss: 1.0145010948181152\n",
      "Epoch 6345: Training Loss: 0.14495933800935745 Validation Loss: 1.0148433446884155\n",
      "Epoch 6346: Training Loss: 0.14501444747050604 Validation Loss: 1.0147466659545898\n",
      "Epoch 6347: Training Loss: 0.1455261210600535 Validation Loss: 1.015005111694336\n",
      "Epoch 6348: Training Loss: 0.14482455948988596 Validation Loss: 1.015376091003418\n",
      "Epoch 6349: Training Loss: 0.14488722880681357 Validation Loss: 1.0158274173736572\n",
      "Epoch 6350: Training Loss: 0.14565841853618622 Validation Loss: 1.0153586864471436\n",
      "Epoch 6351: Training Loss: 0.1448744833469391 Validation Loss: 1.0157798528671265\n",
      "Epoch 6352: Training Loss: 0.14583746592203775 Validation Loss: 1.0153114795684814\n",
      "Epoch 6353: Training Loss: 0.14525785793860754 Validation Loss: 1.0146516561508179\n",
      "Epoch 6354: Training Loss: 0.14488073686758676 Validation Loss: 1.014693260192871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6355: Training Loss: 0.14472981790701547 Validation Loss: 1.014703392982483\n",
      "Epoch 6356: Training Loss: 0.14468936125437418 Validation Loss: 1.0149188041687012\n",
      "Epoch 6357: Training Loss: 0.14507662753264108 Validation Loss: 1.0149096250534058\n",
      "Epoch 6358: Training Loss: 0.14478468398253122 Validation Loss: 1.014641284942627\n",
      "Epoch 6359: Training Loss: 0.14476618667443594 Validation Loss: 1.0145666599273682\n",
      "Epoch 6360: Training Loss: 0.14471569657325745 Validation Loss: 1.0158544778823853\n",
      "Epoch 6361: Training Loss: 0.14509640137354532 Validation Loss: 1.0157009363174438\n",
      "Epoch 6362: Training Loss: 0.14458403488000235 Validation Loss: 1.0154486894607544\n",
      "Epoch 6363: Training Loss: 0.14533268908659616 Validation Loss: 1.0149410963058472\n",
      "Epoch 6364: Training Loss: 0.14479057242472967 Validation Loss: 1.0151453018188477\n",
      "Epoch 6365: Training Loss: 0.14546448985735574 Validation Loss: 1.015690803527832\n",
      "Epoch 6366: Training Loss: 0.14481800297896066 Validation Loss: 1.0155792236328125\n",
      "Epoch 6367: Training Loss: 0.1450681040684382 Validation Loss: 1.0162546634674072\n",
      "Epoch 6368: Training Loss: 0.14455589155356088 Validation Loss: 1.0162965059280396\n",
      "Epoch 6369: Training Loss: 0.14478459457556406 Validation Loss: 1.0168043375015259\n",
      "Epoch 6370: Training Loss: 0.14460159838199615 Validation Loss: 1.0160573720932007\n",
      "Epoch 6371: Training Loss: 0.14445501565933228 Validation Loss: 1.0154446363449097\n",
      "Epoch 6372: Training Loss: 0.1446027308702469 Validation Loss: 1.0154298543930054\n",
      "Epoch 6373: Training Loss: 0.144808828830719 Validation Loss: 1.0153002738952637\n",
      "Epoch 6374: Training Loss: 0.14550460626681647 Validation Loss: 1.0152205228805542\n",
      "Epoch 6375: Training Loss: 0.14453386763731638 Validation Loss: 1.015628457069397\n",
      "Epoch 6376: Training Loss: 0.14425015449523926 Validation Loss: 1.015739917755127\n",
      "Epoch 6377: Training Loss: 0.14451016982396445 Validation Loss: 1.0154545307159424\n",
      "Epoch 6378: Training Loss: 0.14511306583881378 Validation Loss: 1.0162581205368042\n",
      "Epoch 6379: Training Loss: 0.1443507174650828 Validation Loss: 1.0153037309646606\n",
      "Epoch 6380: Training Loss: 0.14446299771467844 Validation Loss: 1.0150506496429443\n",
      "Epoch 6381: Training Loss: 0.1449104150136312 Validation Loss: 1.0159897804260254\n",
      "Epoch 6382: Training Loss: 0.14514057338237762 Validation Loss: 1.016278624534607\n",
      "Epoch 6383: Training Loss: 0.14513562619686127 Validation Loss: 1.016270637512207\n",
      "Epoch 6384: Training Loss: 0.14470971127351126 Validation Loss: 1.016290545463562\n",
      "Epoch 6385: Training Loss: 0.14445922275384268 Validation Loss: 1.0154469013214111\n",
      "Epoch 6386: Training Loss: 0.144240602850914 Validation Loss: 1.0161058902740479\n",
      "Epoch 6387: Training Loss: 0.14417734245459238 Validation Loss: 1.016383171081543\n",
      "Epoch 6388: Training Loss: 0.14536465207735697 Validation Loss: 1.0167768001556396\n",
      "Epoch 6389: Training Loss: 0.1450807973742485 Validation Loss: 1.0171191692352295\n",
      "Epoch 6390: Training Loss: 0.14420156677563986 Validation Loss: 1.0163195133209229\n",
      "Epoch 6391: Training Loss: 0.1444721668958664 Validation Loss: 1.0161925554275513\n",
      "Epoch 6392: Training Loss: 0.14455288648605347 Validation Loss: 1.0162843465805054\n",
      "Epoch 6393: Training Loss: 0.14382201433181763 Validation Loss: 1.0164135694503784\n",
      "Epoch 6394: Training Loss: 0.14371385176976523 Validation Loss: 1.0166414976119995\n",
      "Epoch 6395: Training Loss: 0.14388624330361685 Validation Loss: 1.0167990922927856\n",
      "Epoch 6396: Training Loss: 0.14424368739128113 Validation Loss: 1.0165226459503174\n",
      "Epoch 6397: Training Loss: 0.1445884332060814 Validation Loss: 1.0170334577560425\n",
      "Epoch 6398: Training Loss: 0.14427395164966583 Validation Loss: 1.016855239868164\n",
      "Epoch 6399: Training Loss: 0.1437766303618749 Validation Loss: 1.0171892642974854\n",
      "Epoch 6400: Training Loss: 0.14440768460432687 Validation Loss: 1.015872597694397\n",
      "Epoch 6401: Training Loss: 0.1436267470320066 Validation Loss: 1.0168766975402832\n",
      "Epoch 6402: Training Loss: 0.14384683966636658 Validation Loss: 1.0164917707443237\n",
      "Epoch 6403: Training Loss: 0.14398492872714996 Validation Loss: 1.0166295766830444\n",
      "Epoch 6404: Training Loss: 0.14351906875769296 Validation Loss: 1.0163676738739014\n",
      "Epoch 6405: Training Loss: 0.14433715492486954 Validation Loss: 1.016005039215088\n",
      "Epoch 6406: Training Loss: 0.14360247055689493 Validation Loss: 1.0161727666854858\n",
      "Epoch 6407: Training Loss: 0.1434867481390635 Validation Loss: 1.0170339345932007\n",
      "Epoch 6408: Training Loss: 0.14376618961493173 Validation Loss: 1.0164356231689453\n",
      "Epoch 6409: Training Loss: 0.14384358127911887 Validation Loss: 1.0164010524749756\n",
      "Epoch 6410: Training Loss: 0.14353046814600626 Validation Loss: 1.0162053108215332\n",
      "Epoch 6411: Training Loss: 0.14360197385152182 Validation Loss: 1.016517996788025\n",
      "Epoch 6412: Training Loss: 0.14349148670832315 Validation Loss: 1.016626000404358\n",
      "Epoch 6413: Training Loss: 0.14357281724611917 Validation Loss: 1.017518401145935\n",
      "Epoch 6414: Training Loss: 0.1443543160955111 Validation Loss: 1.017592191696167\n",
      "Epoch 6415: Training Loss: 0.14390428364276886 Validation Loss: 1.0168163776397705\n",
      "Epoch 6416: Training Loss: 0.1436675488948822 Validation Loss: 1.0165331363677979\n",
      "Epoch 6417: Training Loss: 0.14371709525585175 Validation Loss: 1.0165786743164062\n",
      "Epoch 6418: Training Loss: 0.1438817878564199 Validation Loss: 1.0162798166275024\n",
      "Epoch 6419: Training Loss: 0.1432560384273529 Validation Loss: 1.0162580013275146\n",
      "Epoch 6420: Training Loss: 0.14464816699425379 Validation Loss: 1.0160495042800903\n",
      "Epoch 6421: Training Loss: 0.14353721340497336 Validation Loss: 1.016973853111267\n",
      "Epoch 6422: Training Loss: 0.14352344473203024 Validation Loss: 1.0168163776397705\n",
      "Epoch 6423: Training Loss: 0.1432033826907476 Validation Loss: 1.0169142484664917\n",
      "Epoch 6424: Training Loss: 0.14321904877821603 Validation Loss: 1.016658902168274\n",
      "Epoch 6425: Training Loss: 0.14400246739387512 Validation Loss: 1.0169196128845215\n",
      "Epoch 6426: Training Loss: 0.1432653417189916 Validation Loss: 1.0173858404159546\n",
      "Epoch 6427: Training Loss: 0.14331299563248953 Validation Loss: 1.016756296157837\n",
      "Epoch 6428: Training Loss: 0.14332150916258493 Validation Loss: 1.0175107717514038\n",
      "Epoch 6429: Training Loss: 0.14348635077476501 Validation Loss: 1.0175222158432007\n",
      "Epoch 6430: Training Loss: 0.14346325397491455 Validation Loss: 1.0172802209854126\n",
      "Epoch 6431: Training Loss: 0.14367487033208212 Validation Loss: 1.0168180465698242\n",
      "Epoch 6432: Training Loss: 0.14411593476931253 Validation Loss: 1.0173604488372803\n",
      "Epoch 6433: Training Loss: 0.1437047322591146 Validation Loss: 1.018143653869629\n",
      "Epoch 6434: Training Loss: 0.14326947927474976 Validation Loss: 1.018214225769043\n",
      "Epoch 6435: Training Loss: 0.14352746804555258 Validation Loss: 1.0179588794708252\n",
      "Epoch 6436: Training Loss: 0.14284822344779968 Validation Loss: 1.017655849456787\n",
      "Epoch 6437: Training Loss: 0.14334253470102945 Validation Loss: 1.017647624015808\n",
      "Epoch 6438: Training Loss: 0.14325368404388428 Validation Loss: 1.0177152156829834\n",
      "Epoch 6439: Training Loss: 0.1430433988571167 Validation Loss: 1.0174285173416138\n",
      "Epoch 6440: Training Loss: 0.14379908641179404 Validation Loss: 1.0186853408813477\n",
      "Epoch 6441: Training Loss: 0.14317150910695395 Validation Loss: 1.01797616481781\n",
      "Epoch 6442: Training Loss: 0.14313971002896628 Validation Loss: 1.0172128677368164\n",
      "Epoch 6443: Training Loss: 0.1430946389834086 Validation Loss: 1.017517328262329\n",
      "Epoch 6444: Training Loss: 0.14331141114234924 Validation Loss: 1.01786470413208\n",
      "Epoch 6445: Training Loss: 0.14313857754071554 Validation Loss: 1.0181965827941895\n",
      "Epoch 6446: Training Loss: 0.14356659104426703 Validation Loss: 1.0180673599243164\n",
      "Epoch 6447: Training Loss: 0.14353152612845102 Validation Loss: 1.0176372528076172\n",
      "Epoch 6448: Training Loss: 0.1437409594655037 Validation Loss: 1.0184277296066284\n",
      "Epoch 6449: Training Loss: 0.14280527333418527 Validation Loss: 1.0177961587905884\n",
      "Epoch 6450: Training Loss: 0.14324912677208582 Validation Loss: 1.018175721168518\n",
      "Epoch 6451: Training Loss: 0.14349287003278732 Validation Loss: 1.018577218055725\n",
      "Epoch 6452: Training Loss: 0.14323127766450247 Validation Loss: 1.0181046724319458\n",
      "Epoch 6453: Training Loss: 0.14278090000152588 Validation Loss: 1.018458366394043\n",
      "Epoch 6454: Training Loss: 0.14397933085759482 Validation Loss: 1.018005609512329\n",
      "Epoch 6455: Training Loss: 0.14290534456570944 Validation Loss: 1.0180273056030273\n",
      "Epoch 6456: Training Loss: 0.14284624656041464 Validation Loss: 1.0178749561309814\n",
      "Epoch 6457: Training Loss: 0.14219742516676584 Validation Loss: 1.0179963111877441\n",
      "Epoch 6458: Training Loss: 0.14219096302986145 Validation Loss: 1.0176655054092407\n",
      "Epoch 6459: Training Loss: 0.14331855376561484 Validation Loss: 1.0185307264328003\n",
      "Epoch 6460: Training Loss: 0.14284151792526245 Validation Loss: 1.0192631483078003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6461: Training Loss: 0.14278411368529 Validation Loss: 1.0192300081253052\n",
      "Epoch 6462: Training Loss: 0.14283679177363715 Validation Loss: 1.018837332725525\n",
      "Epoch 6463: Training Loss: 0.1424600382645925 Validation Loss: 1.0189956426620483\n",
      "Epoch 6464: Training Loss: 0.14257089296976724 Validation Loss: 1.0184656381607056\n",
      "Epoch 6465: Training Loss: 0.14276432991027832 Validation Loss: 1.0178500413894653\n",
      "Epoch 6466: Training Loss: 0.14286704609791437 Validation Loss: 1.0178489685058594\n",
      "Epoch 6467: Training Loss: 0.14442259321610132 Validation Loss: 1.0178619623184204\n",
      "Epoch 6468: Training Loss: 0.1427522450685501 Validation Loss: 1.0179957151412964\n",
      "Epoch 6469: Training Loss: 0.14264248559872308 Validation Loss: 1.0183777809143066\n",
      "Epoch 6470: Training Loss: 0.1435414726535479 Validation Loss: 1.018941879272461\n",
      "Epoch 6471: Training Loss: 0.14267862836519876 Validation Loss: 1.0189563035964966\n",
      "Epoch 6472: Training Loss: 0.14230706791083017 Validation Loss: 1.018913745880127\n",
      "Epoch 6473: Training Loss: 0.14196980992952982 Validation Loss: 1.0183104276657104\n",
      "Epoch 6474: Training Loss: 0.1435320327679316 Validation Loss: 1.0182944536209106\n",
      "Epoch 6475: Training Loss: 0.14214973648389181 Validation Loss: 1.018772840499878\n",
      "Epoch 6476: Training Loss: 0.14142445226510367 Validation Loss: 1.0184234380722046\n",
      "Epoch 6477: Training Loss: 0.1419212818145752 Validation Loss: 1.0189653635025024\n",
      "Epoch 6478: Training Loss: 0.1421850472688675 Validation Loss: 1.0195503234863281\n",
      "Epoch 6479: Training Loss: 0.1424441784620285 Validation Loss: 1.0198034048080444\n",
      "Epoch 6480: Training Loss: 0.1426576574643453 Validation Loss: 1.019416332244873\n",
      "Epoch 6481: Training Loss: 0.14209025104840597 Validation Loss: 1.0191775560379028\n",
      "Epoch 6482: Training Loss: 0.14309817552566528 Validation Loss: 1.019800066947937\n",
      "Epoch 6483: Training Loss: 0.14207594096660614 Validation Loss: 1.0191000699996948\n",
      "Epoch 6484: Training Loss: 0.14246821403503418 Validation Loss: 1.0184258222579956\n",
      "Epoch 6485: Training Loss: 0.1428563247124354 Validation Loss: 1.0187400579452515\n",
      "Epoch 6486: Training Loss: 0.142560842136542 Validation Loss: 1.0191936492919922\n",
      "Epoch 6487: Training Loss: 0.14209194978078207 Validation Loss: 1.0194966793060303\n",
      "Epoch 6488: Training Loss: 0.14222105840841928 Validation Loss: 1.0192826986312866\n",
      "Epoch 6489: Training Loss: 0.14179089417060217 Validation Loss: 1.0192049741744995\n",
      "Epoch 6490: Training Loss: 0.14205650985240936 Validation Loss: 1.0183515548706055\n",
      "Epoch 6491: Training Loss: 0.1419911781946818 Validation Loss: 1.0183863639831543\n",
      "Epoch 6492: Training Loss: 0.1420209010442098 Validation Loss: 1.0188130140304565\n",
      "Epoch 6493: Training Loss: 0.1417564650376638 Validation Loss: 1.0190216302871704\n",
      "Epoch 6494: Training Loss: 0.14231214423974356 Validation Loss: 1.018924355506897\n",
      "Epoch 6495: Training Loss: 0.14182102183500925 Validation Loss: 1.019095778465271\n",
      "Epoch 6496: Training Loss: 0.14224562545617422 Validation Loss: 1.0188379287719727\n",
      "Epoch 6497: Training Loss: 0.14179699619611105 Validation Loss: 1.0193599462509155\n",
      "Epoch 6498: Training Loss: 0.1416318118572235 Validation Loss: 1.0202175378799438\n",
      "Epoch 6499: Training Loss: 0.1423600489894549 Validation Loss: 1.0204696655273438\n",
      "Epoch 6500: Training Loss: 0.14174828926722208 Validation Loss: 1.0208574533462524\n",
      "Epoch 6501: Training Loss: 0.1420591821273168 Validation Loss: 1.0206944942474365\n",
      "Epoch 6502: Training Loss: 0.14162061115105948 Validation Loss: 1.020268201828003\n",
      "Epoch 6503: Training Loss: 0.1415426234404246 Validation Loss: 1.0196408033370972\n",
      "Epoch 6504: Training Loss: 0.14213760693868002 Validation Loss: 1.0194664001464844\n",
      "Epoch 6505: Training Loss: 0.1421361118555069 Validation Loss: 1.0189512968063354\n",
      "Epoch 6506: Training Loss: 0.14201664924621582 Validation Loss: 1.0191142559051514\n",
      "Epoch 6507: Training Loss: 0.1417331943909327 Validation Loss: 1.0186774730682373\n",
      "Epoch 6508: Training Loss: 0.14179094632466635 Validation Loss: 1.019197702407837\n",
      "Epoch 6509: Training Loss: 0.14155292014280954 Validation Loss: 1.0190448760986328\n",
      "Epoch 6510: Training Loss: 0.14253983894983926 Validation Loss: 1.0201572179794312\n",
      "Epoch 6511: Training Loss: 0.14194258550802866 Validation Loss: 1.0203818082809448\n",
      "Epoch 6512: Training Loss: 0.14198638995488486 Validation Loss: 1.0197136402130127\n",
      "Epoch 6513: Training Loss: 0.14156360427538553 Validation Loss: 1.0200855731964111\n",
      "Epoch 6514: Training Loss: 0.14159893492857614 Validation Loss: 1.0204365253448486\n",
      "Epoch 6515: Training Loss: 0.14169896145661673 Validation Loss: 1.0205919742584229\n",
      "Epoch 6516: Training Loss: 0.1412508636713028 Validation Loss: 1.020113229751587\n",
      "Epoch 6517: Training Loss: 0.14112662772337595 Validation Loss: 1.0203962326049805\n",
      "Epoch 6518: Training Loss: 0.14129568139712015 Validation Loss: 1.0202313661575317\n",
      "Epoch 6519: Training Loss: 0.14166533946990967 Validation Loss: 1.0206472873687744\n",
      "Epoch 6520: Training Loss: 0.14167986313501993 Validation Loss: 1.020009994506836\n",
      "Epoch 6521: Training Loss: 0.14143096407254538 Validation Loss: 1.019684910774231\n",
      "Epoch 6522: Training Loss: 0.14144095281759897 Validation Loss: 1.0195045471191406\n",
      "Epoch 6523: Training Loss: 0.14183306942383447 Validation Loss: 1.0187324285507202\n",
      "Epoch 6524: Training Loss: 0.14133537809054056 Validation Loss: 1.0195430517196655\n",
      "Epoch 6525: Training Loss: 0.14173934857050577 Validation Loss: 1.020598292350769\n",
      "Epoch 6526: Training Loss: 0.14183040459950766 Validation Loss: 1.0210497379302979\n",
      "Epoch 6527: Training Loss: 0.1427831326921781 Validation Loss: 1.0212758779525757\n",
      "Epoch 6528: Training Loss: 0.14131900171438852 Validation Loss: 1.020442247390747\n",
      "Epoch 6529: Training Loss: 0.14110263685385385 Validation Loss: 1.0192478895187378\n",
      "Epoch 6530: Training Loss: 0.1414064566294352 Validation Loss: 1.0193061828613281\n",
      "Epoch 6531: Training Loss: 0.14144195864597955 Validation Loss: 1.0193498134613037\n",
      "Epoch 6532: Training Loss: 0.14133811990420023 Validation Loss: 1.0200021266937256\n",
      "Epoch 6533: Training Loss: 0.14133774737517038 Validation Loss: 1.0206152200698853\n",
      "Epoch 6534: Training Loss: 0.1414784938097 Validation Loss: 1.020472764968872\n",
      "Epoch 6535: Training Loss: 0.1412542164325714 Validation Loss: 1.0211777687072754\n",
      "Epoch 6536: Training Loss: 0.14141422510147095 Validation Loss: 1.0213671922683716\n",
      "Epoch 6537: Training Loss: 0.14145440608263016 Validation Loss: 1.0207204818725586\n",
      "Epoch 6538: Training Loss: 0.14137611786524454 Validation Loss: 1.0208265781402588\n",
      "Epoch 6539: Training Loss: 0.14104307691256204 Validation Loss: 1.0207929611206055\n",
      "Epoch 6540: Training Loss: 0.14121556530396143 Validation Loss: 1.0201817750930786\n",
      "Epoch 6541: Training Loss: 0.14191331962744394 Validation Loss: 1.0198941230773926\n",
      "Epoch 6542: Training Loss: 0.14110382397969565 Validation Loss: 1.0196919441223145\n",
      "Epoch 6543: Training Loss: 0.1409356246391932 Validation Loss: 1.0203489065170288\n",
      "Epoch 6544: Training Loss: 0.14106623083353043 Validation Loss: 1.0206750631332397\n",
      "Epoch 6545: Training Loss: 0.14035742978254953 Validation Loss: 1.0204120874404907\n",
      "Epoch 6546: Training Loss: 0.14070657392342886 Validation Loss: 1.021461009979248\n",
      "Epoch 6547: Training Loss: 0.14151491721471152 Validation Loss: 1.0217101573944092\n",
      "Epoch 6548: Training Loss: 0.1407999445994695 Validation Loss: 1.0213165283203125\n",
      "Epoch 6549: Training Loss: 0.14131984611352286 Validation Loss: 1.0205656290054321\n",
      "Epoch 6550: Training Loss: 0.14083755513032278 Validation Loss: 1.0202879905700684\n",
      "Epoch 6551: Training Loss: 0.14108172059059143 Validation Loss: 1.0206916332244873\n",
      "Epoch 6552: Training Loss: 0.14052613576253256 Validation Loss: 1.0206319093704224\n",
      "Epoch 6553: Training Loss: 0.1414585510889689 Validation Loss: 1.0203803777694702\n",
      "Epoch 6554: Training Loss: 0.14007826149463654 Validation Loss: 1.0207711458206177\n",
      "Epoch 6555: Training Loss: 0.14082329471906027 Validation Loss: 1.0214828252792358\n",
      "Epoch 6556: Training Loss: 0.14035077889760336 Validation Loss: 1.0216844081878662\n",
      "Epoch 6557: Training Loss: 0.14072402318318686 Validation Loss: 1.021917462348938\n",
      "Epoch 6558: Training Loss: 0.14072161416212717 Validation Loss: 1.020750641822815\n",
      "Epoch 6559: Training Loss: 0.14070325593153635 Validation Loss: 1.021130919456482\n",
      "Epoch 6560: Training Loss: 0.1415184885263443 Validation Loss: 1.0214732885360718\n",
      "Epoch 6561: Training Loss: 0.1411644915739695 Validation Loss: 1.020574927330017\n",
      "Epoch 6562: Training Loss: 0.14038501679897308 Validation Loss: 1.019776701927185\n",
      "Epoch 6563: Training Loss: 0.14057233929634094 Validation Loss: 1.0204943418502808\n",
      "Epoch 6564: Training Loss: 0.14063725372155508 Validation Loss: 1.020805835723877\n",
      "Epoch 6565: Training Loss: 0.1407508005698522 Validation Loss: 1.0214155912399292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6566: Training Loss: 0.14073050518830618 Validation Loss: 1.0219844579696655\n",
      "Epoch 6567: Training Loss: 0.14107480396827063 Validation Loss: 1.022413730621338\n",
      "Epoch 6568: Training Loss: 0.14045938849449158 Validation Loss: 1.0229078531265259\n",
      "Epoch 6569: Training Loss: 0.14123527705669403 Validation Loss: 1.0217918157577515\n",
      "Epoch 6570: Training Loss: 0.140941192706426 Validation Loss: 1.0211684703826904\n",
      "Epoch 6571: Training Loss: 0.14030478646357855 Validation Loss: 1.0211247205734253\n",
      "Epoch 6572: Training Loss: 0.1404253343741099 Validation Loss: 1.0219227075576782\n",
      "Epoch 6573: Training Loss: 0.1401772846778234 Validation Loss: 1.0221407413482666\n",
      "Epoch 6574: Training Loss: 0.140786940852801 Validation Loss: 1.0210676193237305\n",
      "Epoch 6575: Training Loss: 0.14023787279923758 Validation Loss: 1.0198242664337158\n",
      "Epoch 6576: Training Loss: 0.1415929744640986 Validation Loss: 1.0193582773208618\n",
      "Epoch 6577: Training Loss: 0.1408716986576716 Validation Loss: 1.0195674896240234\n",
      "Epoch 6578: Training Loss: 0.14042150974273682 Validation Loss: 1.0206857919692993\n",
      "Epoch 6579: Training Loss: 0.1403699517250061 Validation Loss: 1.021812081336975\n",
      "Epoch 6580: Training Loss: 0.1403943250576655 Validation Loss: 1.0217033624649048\n",
      "Epoch 6581: Training Loss: 0.14033997058868408 Validation Loss: 1.0215038061141968\n",
      "Epoch 6582: Training Loss: 0.1403879076242447 Validation Loss: 1.0215858221054077\n",
      "Epoch 6583: Training Loss: 0.1398958464463552 Validation Loss: 1.0209131240844727\n",
      "Epoch 6584: Training Loss: 0.14007319509983063 Validation Loss: 1.0216022729873657\n",
      "Epoch 6585: Training Loss: 0.14048465341329575 Validation Loss: 1.022255301475525\n",
      "Epoch 6586: Training Loss: 0.1403520703315735 Validation Loss: 1.0217968225479126\n",
      "Epoch 6587: Training Loss: 0.13989297052224478 Validation Loss: 1.021386742591858\n",
      "Epoch 6588: Training Loss: 0.1402442306280136 Validation Loss: 1.0216721296310425\n",
      "Epoch 6589: Training Loss: 0.14008733381827673 Validation Loss: 1.021157145500183\n",
      "Epoch 6590: Training Loss: 0.13990436494350433 Validation Loss: 1.0215705633163452\n",
      "Epoch 6591: Training Loss: 0.14006218810876211 Validation Loss: 1.0221508741378784\n",
      "Epoch 6592: Training Loss: 0.1402429218093554 Validation Loss: 1.0216126441955566\n",
      "Epoch 6593: Training Loss: 0.14018643895785013 Validation Loss: 1.0217794179916382\n",
      "Epoch 6594: Training Loss: 0.13994921743869781 Validation Loss: 1.0223535299301147\n",
      "Epoch 6595: Training Loss: 0.1396916707356771 Validation Loss: 1.0222969055175781\n",
      "Epoch 6596: Training Loss: 0.14060173432032266 Validation Loss: 1.0223897695541382\n",
      "Epoch 6597: Training Loss: 0.1398273209730784 Validation Loss: 1.0227090120315552\n",
      "Epoch 6598: Training Loss: 0.14042040457328162 Validation Loss: 1.0230693817138672\n",
      "Epoch 6599: Training Loss: 0.14000744620958963 Validation Loss: 1.0228197574615479\n",
      "Epoch 6600: Training Loss: 0.1396883875131607 Validation Loss: 1.022749900817871\n",
      "Epoch 6601: Training Loss: 0.13984216252962747 Validation Loss: 1.0230802297592163\n",
      "Epoch 6602: Training Loss: 0.1405120069781939 Validation Loss: 1.0231845378875732\n",
      "Epoch 6603: Training Loss: 0.13942124942938486 Validation Loss: 1.0231397151947021\n",
      "Epoch 6604: Training Loss: 0.14020436008771262 Validation Loss: 1.022859811782837\n",
      "Epoch 6605: Training Loss: 0.13948944956064224 Validation Loss: 1.0233157873153687\n",
      "Epoch 6606: Training Loss: 0.1402555281917254 Validation Loss: 1.0229448080062866\n",
      "Epoch 6607: Training Loss: 0.13995365798473358 Validation Loss: 1.022655725479126\n",
      "Epoch 6608: Training Loss: 0.1395342449347178 Validation Loss: 1.022019386291504\n",
      "Epoch 6609: Training Loss: 0.13969584802786508 Validation Loss: 1.02236008644104\n",
      "Epoch 6610: Training Loss: 0.13969979186852774 Validation Loss: 1.0225920677185059\n",
      "Epoch 6611: Training Loss: 0.1390464057524999 Validation Loss: 1.0228689908981323\n",
      "Epoch 6612: Training Loss: 0.13989433646202087 Validation Loss: 1.0226103067398071\n",
      "Epoch 6613: Training Loss: 0.13936383028825125 Validation Loss: 1.0224121809005737\n",
      "Epoch 6614: Training Loss: 0.1395939141511917 Validation Loss: 1.0220351219177246\n",
      "Epoch 6615: Training Loss: 0.13941324253877005 Validation Loss: 1.0233045816421509\n",
      "Epoch 6616: Training Loss: 0.13931124905745187 Validation Loss: 1.0230951309204102\n",
      "Epoch 6617: Training Loss: 0.13963817059993744 Validation Loss: 1.0238001346588135\n",
      "Epoch 6618: Training Loss: 0.1390050450960795 Validation Loss: 1.0237016677856445\n",
      "Epoch 6619: Training Loss: 0.13930334647496542 Validation Loss: 1.023179531097412\n",
      "Epoch 6620: Training Loss: 0.1391728421052297 Validation Loss: 1.0231337547302246\n",
      "Epoch 6621: Training Loss: 0.14011971155802408 Validation Loss: 1.0231680870056152\n",
      "Epoch 6622: Training Loss: 0.13976304729779562 Validation Loss: 1.0237659215927124\n",
      "Epoch 6623: Training Loss: 0.13946972290674844 Validation Loss: 1.024357795715332\n",
      "Epoch 6624: Training Loss: 0.13956802586714426 Validation Loss: 1.024247169494629\n",
      "Epoch 6625: Training Loss: 0.1401682272553444 Validation Loss: 1.023736596107483\n",
      "Epoch 6626: Training Loss: 0.13930671910444895 Validation Loss: 1.023582935333252\n",
      "Epoch 6627: Training Loss: 0.13947020967801413 Validation Loss: 1.0238715410232544\n",
      "Epoch 6628: Training Loss: 0.13905348628759384 Validation Loss: 1.0233147144317627\n",
      "Epoch 6629: Training Loss: 0.1391627366344134 Validation Loss: 1.0230439901351929\n",
      "Epoch 6630: Training Loss: 0.13933829218149185 Validation Loss: 1.0233479738235474\n",
      "Epoch 6631: Training Loss: 0.1393278588851293 Validation Loss: 1.0238908529281616\n",
      "Epoch 6632: Training Loss: 0.13934509952863058 Validation Loss: 1.0239856243133545\n",
      "Epoch 6633: Training Loss: 0.139610988398393 Validation Loss: 1.0243046283721924\n",
      "Epoch 6634: Training Loss: 0.14040702333052954 Validation Loss: 1.0232833623886108\n",
      "Epoch 6635: Training Loss: 0.13922572135925293 Validation Loss: 1.023695468902588\n",
      "Epoch 6636: Training Loss: 0.1393020823597908 Validation Loss: 1.0238304138183594\n",
      "Epoch 6637: Training Loss: 0.14032157510519028 Validation Loss: 1.0240901708602905\n",
      "Epoch 6638: Training Loss: 0.13957820584376654 Validation Loss: 1.0247317552566528\n",
      "Epoch 6639: Training Loss: 0.13923366367816925 Validation Loss: 1.0248545408248901\n",
      "Epoch 6640: Training Loss: 0.1390445480744044 Validation Loss: 1.023726463317871\n",
      "Epoch 6641: Training Loss: 0.13885591427485147 Validation Loss: 1.0229541063308716\n",
      "Epoch 6642: Training Loss: 0.13934514919916788 Validation Loss: 1.022493600845337\n",
      "Epoch 6643: Training Loss: 0.1390285144249598 Validation Loss: 1.0222804546356201\n",
      "Epoch 6644: Training Loss: 0.13905443251132965 Validation Loss: 1.0231646299362183\n",
      "Epoch 6645: Training Loss: 0.13843131313721338 Validation Loss: 1.02407968044281\n",
      "Epoch 6646: Training Loss: 0.1391364981730779 Validation Loss: 1.0238347053527832\n",
      "Epoch 6647: Training Loss: 0.13915418336788812 Validation Loss: 1.023803949356079\n",
      "Epoch 6648: Training Loss: 0.13961164156595865 Validation Loss: 1.023154377937317\n",
      "Epoch 6649: Training Loss: 0.139462319513162 Validation Loss: 1.02400541305542\n",
      "Epoch 6650: Training Loss: 0.13866637150446573 Validation Loss: 1.0238037109375\n",
      "Epoch 6651: Training Loss: 0.13906515141328177 Validation Loss: 1.024662733078003\n",
      "Epoch 6652: Training Loss: 0.13884059091409048 Validation Loss: 1.0244848728179932\n",
      "Epoch 6653: Training Loss: 0.13929219047228494 Validation Loss: 1.0246809720993042\n",
      "Epoch 6654: Training Loss: 0.13902613520622253 Validation Loss: 1.0251753330230713\n",
      "Epoch 6655: Training Loss: 0.1387694925069809 Validation Loss: 1.0244807004928589\n",
      "Epoch 6656: Training Loss: 0.13872691492239633 Validation Loss: 1.0245734453201294\n",
      "Epoch 6657: Training Loss: 0.13901173075040182 Validation Loss: 1.02424955368042\n",
      "Epoch 6658: Training Loss: 0.13850531230370203 Validation Loss: 1.0243477821350098\n",
      "Epoch 6659: Training Loss: 0.13861938814322153 Validation Loss: 1.0243865251541138\n",
      "Epoch 6660: Training Loss: 0.1387363870938619 Validation Loss: 1.0250188112258911\n",
      "Epoch 6661: Training Loss: 0.13885865112145743 Validation Loss: 1.0252598524093628\n",
      "Epoch 6662: Training Loss: 0.13837241133054098 Validation Loss: 1.0250896215438843\n",
      "Epoch 6663: Training Loss: 0.1392409155766169 Validation Loss: 1.0250468254089355\n",
      "Epoch 6664: Training Loss: 0.1386591518918673 Validation Loss: 1.0248916149139404\n",
      "Epoch 6665: Training Loss: 0.1387308364113172 Validation Loss: 1.0241715908050537\n",
      "Epoch 6666: Training Loss: 0.139292041460673 Validation Loss: 1.023834228515625\n",
      "Epoch 6667: Training Loss: 0.1384396900733312 Validation Loss: 1.0240402221679688\n",
      "Epoch 6668: Training Loss: 0.13865424195925394 Validation Loss: 1.0240782499313354\n",
      "Epoch 6669: Training Loss: 0.13865854839483896 Validation Loss: 1.0246204137802124\n",
      "Epoch 6670: Training Loss: 0.13854665557543436 Validation Loss: 1.0252643823623657\n",
      "Epoch 6671: Training Loss: 0.13843770821889242 Validation Loss: 1.0248973369598389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6672: Training Loss: 0.1385023072361946 Validation Loss: 1.0252079963684082\n",
      "Epoch 6673: Training Loss: 0.13867313166459402 Validation Loss: 1.0248373746871948\n",
      "Epoch 6674: Training Loss: 0.13839562237262726 Validation Loss: 1.0246042013168335\n",
      "Epoch 6675: Training Loss: 0.13843431075414023 Validation Loss: 1.0243479013442993\n",
      "Epoch 6676: Training Loss: 0.13906200230121613 Validation Loss: 1.02460515499115\n",
      "Epoch 6677: Training Loss: 0.13837896287441254 Validation Loss: 1.0255950689315796\n",
      "Epoch 6678: Training Loss: 0.13870000342528024 Validation Loss: 1.0261355638504028\n",
      "Epoch 6679: Training Loss: 0.13833515346050262 Validation Loss: 1.025567650794983\n",
      "Epoch 6680: Training Loss: 0.13832266628742218 Validation Loss: 1.0248700380325317\n",
      "Epoch 6681: Training Loss: 0.13805132607618967 Validation Loss: 1.023992657661438\n",
      "Epoch 6682: Training Loss: 0.13862702250480652 Validation Loss: 1.0245836973190308\n",
      "Epoch 6683: Training Loss: 0.13784228761990866 Validation Loss: 1.0254433155059814\n",
      "Epoch 6684: Training Loss: 0.1382479965686798 Validation Loss: 1.025933861732483\n",
      "Epoch 6685: Training Loss: 0.13869509100914001 Validation Loss: 1.0257220268249512\n",
      "Epoch 6686: Training Loss: 0.13838668167591095 Validation Loss: 1.025100827217102\n",
      "Epoch 6687: Training Loss: 0.13846189777056375 Validation Loss: 1.0247915983200073\n",
      "Epoch 6688: Training Loss: 0.13840995480616888 Validation Loss: 1.0252710580825806\n",
      "Epoch 6689: Training Loss: 0.13858655591805777 Validation Loss: 1.0248912572860718\n",
      "Epoch 6690: Training Loss: 0.1397945980230967 Validation Loss: 1.0251359939575195\n",
      "Epoch 6691: Training Loss: 0.13845494389533997 Validation Loss: 1.0268809795379639\n",
      "Epoch 6692: Training Loss: 0.13786887129147848 Validation Loss: 1.0269964933395386\n",
      "Epoch 6693: Training Loss: 0.1377196510632833 Validation Loss: 1.0264356136322021\n",
      "Epoch 6694: Training Loss: 0.1382031043370565 Validation Loss: 1.0258573293685913\n",
      "Epoch 6695: Training Loss: 0.13825676838556925 Validation Loss: 1.0248408317565918\n",
      "Epoch 6696: Training Loss: 0.13791241745154062 Validation Loss: 1.024354100227356\n",
      "Epoch 6697: Training Loss: 0.13783826430638632 Validation Loss: 1.0251702070236206\n",
      "Epoch 6698: Training Loss: 0.13824904461701712 Validation Loss: 1.025405764579773\n",
      "Epoch 6699: Training Loss: 0.13921847442785898 Validation Loss: 1.0255578756332397\n",
      "Epoch 6700: Training Loss: 0.13804130256175995 Validation Loss: 1.0258336067199707\n",
      "Epoch 6701: Training Loss: 0.13780459016561508 Validation Loss: 1.0255458354949951\n",
      "Epoch 6702: Training Loss: 0.13891414056221643 Validation Loss: 1.0255581140518188\n",
      "Epoch 6703: Training Loss: 0.13834060728549957 Validation Loss: 1.025843858718872\n",
      "Epoch 6704: Training Loss: 0.13781581819057465 Validation Loss: 1.0252033472061157\n",
      "Epoch 6705: Training Loss: 0.13825398683547974 Validation Loss: 1.024984359741211\n",
      "Epoch 6706: Training Loss: 0.13783144454161325 Validation Loss: 1.0251160860061646\n",
      "Epoch 6707: Training Loss: 0.13810246189435324 Validation Loss: 1.0254204273223877\n",
      "Epoch 6708: Training Loss: 0.13795616974433264 Validation Loss: 1.026175856590271\n",
      "Epoch 6709: Training Loss: 0.1387848680218061 Validation Loss: 1.0264263153076172\n",
      "Epoch 6710: Training Loss: 0.13787164290746054 Validation Loss: 1.026489496231079\n",
      "Epoch 6711: Training Loss: 0.13763315478960672 Validation Loss: 1.0264790058135986\n",
      "Epoch 6712: Training Loss: 0.13763712346553802 Validation Loss: 1.0263924598693848\n",
      "Epoch 6713: Training Loss: 0.13767380515734354 Validation Loss: 1.0264545679092407\n",
      "Epoch 6714: Training Loss: 0.13785875091950098 Validation Loss: 1.026329755783081\n",
      "Epoch 6715: Training Loss: 0.13836021969715753 Validation Loss: 1.0258445739746094\n",
      "Epoch 6716: Training Loss: 0.13741393387317657 Validation Loss: 1.026172161102295\n",
      "Epoch 6717: Training Loss: 0.137699822584788 Validation Loss: 1.0262709856033325\n",
      "Epoch 6718: Training Loss: 0.1379868114988009 Validation Loss: 1.0264639854431152\n",
      "Epoch 6719: Training Loss: 0.13772214452425638 Validation Loss: 1.026634931564331\n",
      "Epoch 6720: Training Loss: 0.13800832629203796 Validation Loss: 1.0268253087997437\n",
      "Epoch 6721: Training Loss: 0.13726862519979477 Validation Loss: 1.0266927480697632\n",
      "Epoch 6722: Training Loss: 0.13785258928934732 Validation Loss: 1.026687502861023\n",
      "Epoch 6723: Training Loss: 0.1378410110870997 Validation Loss: 1.0262153148651123\n",
      "Epoch 6724: Training Loss: 0.13748683035373688 Validation Loss: 1.026408076286316\n",
      "Epoch 6725: Training Loss: 0.13835098842779794 Validation Loss: 1.02597177028656\n",
      "Epoch 6726: Training Loss: 0.13700697322686514 Validation Loss: 1.0261664390563965\n",
      "Epoch 6727: Training Loss: 0.13771074016888937 Validation Loss: 1.0262963771820068\n",
      "Epoch 6728: Training Loss: 0.13752115766207376 Validation Loss: 1.026018500328064\n",
      "Epoch 6729: Training Loss: 0.13750598827997842 Validation Loss: 1.0261472463607788\n",
      "Epoch 6730: Training Loss: 0.13757274548212686 Validation Loss: 1.0260469913482666\n",
      "Epoch 6731: Training Loss: 0.13838341335455576 Validation Loss: 1.0266427993774414\n",
      "Epoch 6732: Training Loss: 0.13848885893821716 Validation Loss: 1.026850938796997\n",
      "Epoch 6733: Training Loss: 0.13765631367762884 Validation Loss: 1.027186632156372\n",
      "Epoch 6734: Training Loss: 0.137297252813975 Validation Loss: 1.0260841846466064\n",
      "Epoch 6735: Training Loss: 0.13701879233121872 Validation Loss: 1.025518536567688\n",
      "Epoch 6736: Training Loss: 0.13778755813837051 Validation Loss: 1.0258064270019531\n",
      "Epoch 6737: Training Loss: 0.137146125237147 Validation Loss: 1.0264240503311157\n",
      "Epoch 6738: Training Loss: 0.13764440019925436 Validation Loss: 1.0264556407928467\n",
      "Epoch 6739: Training Loss: 0.13725923001766205 Validation Loss: 1.027437686920166\n",
      "Epoch 6740: Training Loss: 0.13754245142141977 Validation Loss: 1.0277212858200073\n",
      "Epoch 6741: Training Loss: 0.1366053099433581 Validation Loss: 1.0284452438354492\n",
      "Epoch 6742: Training Loss: 0.13717644413312277 Validation Loss: 1.028551697731018\n",
      "Epoch 6743: Training Loss: 0.1372949779033661 Validation Loss: 1.0287153720855713\n",
      "Epoch 6744: Training Loss: 0.13701369613409042 Validation Loss: 1.0282984972000122\n",
      "Epoch 6745: Training Loss: 0.13688886165618896 Validation Loss: 1.027388572692871\n",
      "Epoch 6746: Training Loss: 0.13732065508762994 Validation Loss: 1.0267484188079834\n",
      "Epoch 6747: Training Loss: 0.13703705867131552 Validation Loss: 1.0262622833251953\n",
      "Epoch 6748: Training Loss: 0.1375347524881363 Validation Loss: 1.025557518005371\n",
      "Epoch 6749: Training Loss: 0.1376810222864151 Validation Loss: 1.0260887145996094\n",
      "Epoch 6750: Training Loss: 0.1374813293417295 Validation Loss: 1.027143120765686\n",
      "Epoch 6751: Training Loss: 0.1371941218773524 Validation Loss: 1.027382254600525\n",
      "Epoch 6752: Training Loss: 0.13696836431821188 Validation Loss: 1.0272095203399658\n",
      "Epoch 6753: Training Loss: 0.13704054057598114 Validation Loss: 1.0265395641326904\n",
      "Epoch 6754: Training Loss: 0.13686639815568924 Validation Loss: 1.0258311033248901\n",
      "Epoch 6755: Training Loss: 0.13767285148302713 Validation Loss: 1.0263679027557373\n",
      "Epoch 6756: Training Loss: 0.13707026342550913 Validation Loss: 1.0274584293365479\n",
      "Epoch 6757: Training Loss: 0.13655022283395132 Validation Loss: 1.0279430150985718\n",
      "Epoch 6758: Training Loss: 0.13659125566482544 Validation Loss: 1.0279546976089478\n",
      "Epoch 6759: Training Loss: 0.13679290314515433 Validation Loss: 1.0279039144515991\n",
      "Epoch 6760: Training Loss: 0.13665016492207846 Validation Loss: 1.0285909175872803\n",
      "Epoch 6761: Training Loss: 0.13709569722414017 Validation Loss: 1.028362512588501\n",
      "Epoch 6762: Training Loss: 0.13796580582857132 Validation Loss: 1.0274083614349365\n",
      "Epoch 6763: Training Loss: 0.1366706738869349 Validation Loss: 1.0269854068756104\n",
      "Epoch 6764: Training Loss: 0.13668681184450784 Validation Loss: 1.0265848636627197\n",
      "Epoch 6765: Training Loss: 0.13683072725931802 Validation Loss: 1.0272903442382812\n",
      "Epoch 6766: Training Loss: 0.13646282504002252 Validation Loss: 1.0272880792617798\n",
      "Epoch 6767: Training Loss: 0.13679183026154837 Validation Loss: 1.027788758277893\n",
      "Epoch 6768: Training Loss: 0.13640383879343668 Validation Loss: 1.0282142162322998\n",
      "Epoch 6769: Training Loss: 0.13715550800164542 Validation Loss: 1.0272159576416016\n",
      "Epoch 6770: Training Loss: 0.13723290463288626 Validation Loss: 1.0266693830490112\n",
      "Epoch 6771: Training Loss: 0.1368643194437027 Validation Loss: 1.0278078317642212\n",
      "Epoch 6772: Training Loss: 0.1364931414524714 Validation Loss: 1.0281611680984497\n",
      "Epoch 6773: Training Loss: 0.1363571286201477 Validation Loss: 1.0280406475067139\n",
      "Epoch 6774: Training Loss: 0.13591080158948898 Validation Loss: 1.0281004905700684\n",
      "Epoch 6775: Training Loss: 0.13680116335550943 Validation Loss: 1.0284311771392822\n",
      "Epoch 6776: Training Loss: 0.1369361157218615 Validation Loss: 1.0286887884140015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6777: Training Loss: 0.13690718015034994 Validation Loss: 1.028255581855774\n",
      "Epoch 6778: Training Loss: 0.13636750479539236 Validation Loss: 1.0281000137329102\n",
      "Epoch 6779: Training Loss: 0.1364807883898417 Validation Loss: 1.027260661125183\n",
      "Epoch 6780: Training Loss: 0.13645332554976145 Validation Loss: 1.0275225639343262\n",
      "Epoch 6781: Training Loss: 0.13637953996658325 Validation Loss: 1.0277920961380005\n",
      "Epoch 6782: Training Loss: 0.1363700528939565 Validation Loss: 1.028943419456482\n",
      "Epoch 6783: Training Loss: 0.13617495199044546 Validation Loss: 1.0287647247314453\n",
      "Epoch 6784: Training Loss: 0.1361070399483045 Validation Loss: 1.0287039279937744\n",
      "Epoch 6785: Training Loss: 0.13630198935667673 Validation Loss: 1.02889883518219\n",
      "Epoch 6786: Training Loss: 0.1361833463112513 Validation Loss: 1.0285048484802246\n",
      "Epoch 6787: Training Loss: 0.1363080491622289 Validation Loss: 1.0288724899291992\n",
      "Epoch 6788: Training Loss: 0.13638917605082193 Validation Loss: 1.0283352136611938\n",
      "Epoch 6789: Training Loss: 0.13683079928159714 Validation Loss: 1.0286858081817627\n",
      "Epoch 6790: Training Loss: 0.1369185671210289 Validation Loss: 1.0289767980575562\n",
      "Epoch 6791: Training Loss: 0.1363678202033043 Validation Loss: 1.0287505388259888\n",
      "Epoch 6792: Training Loss: 0.136517862478892 Validation Loss: 1.0284135341644287\n",
      "Epoch 6793: Training Loss: 0.136414701739947 Validation Loss: 1.0283070802688599\n",
      "Epoch 6794: Training Loss: 0.13624474654595056 Validation Loss: 1.0285900831222534\n",
      "Epoch 6795: Training Loss: 0.13616013526916504 Validation Loss: 1.0282334089279175\n",
      "Epoch 6796: Training Loss: 0.13626901308695474 Validation Loss: 1.0281808376312256\n",
      "Epoch 6797: Training Loss: 0.13621983428796133 Validation Loss: 1.0281881093978882\n",
      "Epoch 6798: Training Loss: 0.13622028132279715 Validation Loss: 1.0284596681594849\n",
      "Epoch 6799: Training Loss: 0.13627386838197708 Validation Loss: 1.0286041498184204\n",
      "Epoch 6800: Training Loss: 0.13582601149876913 Validation Loss: 1.0289934873580933\n",
      "Epoch 6801: Training Loss: 0.13659831881523132 Validation Loss: 1.0295219421386719\n",
      "Epoch 6802: Training Loss: 0.13591848810513815 Validation Loss: 1.0296334028244019\n",
      "Epoch 6803: Training Loss: 0.13604511320590973 Validation Loss: 1.0283713340759277\n",
      "Epoch 6804: Training Loss: 0.13605446368455887 Validation Loss: 1.0276763439178467\n",
      "Epoch 6805: Training Loss: 0.13589300215244293 Validation Loss: 1.028233528137207\n",
      "Epoch 6806: Training Loss: 0.13605713844299316 Validation Loss: 1.0287214517593384\n",
      "Epoch 6807: Training Loss: 0.13588238755861917 Validation Loss: 1.0292062759399414\n",
      "Epoch 6808: Training Loss: 0.1358535165588061 Validation Loss: 1.0296976566314697\n",
      "Epoch 6809: Training Loss: 0.13575601081053415 Validation Loss: 1.0296865701675415\n",
      "Epoch 6810: Training Loss: 0.13675820330778757 Validation Loss: 1.030140995979309\n",
      "Epoch 6811: Training Loss: 0.13589822252591452 Validation Loss: 1.0297551155090332\n",
      "Epoch 6812: Training Loss: 0.13592212895552316 Validation Loss: 1.0296090841293335\n",
      "Epoch 6813: Training Loss: 0.13648087034622827 Validation Loss: 1.0287799835205078\n",
      "Epoch 6814: Training Loss: 0.13651457925637564 Validation Loss: 1.0292961597442627\n",
      "Epoch 6815: Training Loss: 0.13662301748991013 Validation Loss: 1.0288883447647095\n",
      "Epoch 6816: Training Loss: 0.13596184303363165 Validation Loss: 1.0283277034759521\n",
      "Epoch 6817: Training Loss: 0.13570275406042734 Validation Loss: 1.0281693935394287\n",
      "Epoch 6818: Training Loss: 0.13558914264043173 Validation Loss: 1.0283019542694092\n",
      "Epoch 6819: Training Loss: 0.13637781888246536 Validation Loss: 1.0291428565979004\n",
      "Epoch 6820: Training Loss: 0.13663483162721 Validation Loss: 1.0278757810592651\n",
      "Epoch 6821: Training Loss: 0.13561813781658807 Validation Loss: 1.028363823890686\n",
      "Epoch 6822: Training Loss: 0.13567250967025757 Validation Loss: 1.0301398038864136\n",
      "Epoch 6823: Training Loss: 0.13576190421978632 Validation Loss: 1.0304909944534302\n",
      "Epoch 6824: Training Loss: 0.135946291188399 Validation Loss: 1.0303641557693481\n",
      "Epoch 6825: Training Loss: 0.13551661868890127 Validation Loss: 1.0299241542816162\n",
      "Epoch 6826: Training Loss: 0.1362966224551201 Validation Loss: 1.02977454662323\n",
      "Epoch 6827: Training Loss: 0.13538148005803427 Validation Loss: 1.0298030376434326\n",
      "Epoch 6828: Training Loss: 0.13567322492599487 Validation Loss: 1.0306798219680786\n",
      "Epoch 6829: Training Loss: 0.13558726012706757 Validation Loss: 1.0301845073699951\n",
      "Epoch 6830: Training Loss: 0.1354780321319898 Validation Loss: 1.0302964448928833\n",
      "Epoch 6831: Training Loss: 0.1355070968468984 Validation Loss: 1.0304226875305176\n",
      "Epoch 6832: Training Loss: 0.13576303670803705 Validation Loss: 1.0305501222610474\n",
      "Epoch 6833: Training Loss: 0.13549899558226267 Validation Loss: 1.03065824508667\n",
      "Epoch 6834: Training Loss: 0.13572391867637634 Validation Loss: 1.0299042463302612\n",
      "Epoch 6835: Training Loss: 0.13562395672003427 Validation Loss: 1.0297054052352905\n",
      "Epoch 6836: Training Loss: 0.13532564540704092 Validation Loss: 1.0294344425201416\n",
      "Epoch 6837: Training Loss: 0.13547541201114655 Validation Loss: 1.0294002294540405\n",
      "Epoch 6838: Training Loss: 0.1354953572154045 Validation Loss: 1.0297423601150513\n",
      "Epoch 6839: Training Loss: 0.13527511556943259 Validation Loss: 1.029905915260315\n",
      "Epoch 6840: Training Loss: 0.13557019581397375 Validation Loss: 1.0303813219070435\n",
      "Epoch 6841: Training Loss: 0.13548530141512552 Validation Loss: 1.030548095703125\n",
      "Epoch 6842: Training Loss: 0.13534195721149445 Validation Loss: 1.0304685831069946\n",
      "Epoch 6843: Training Loss: 0.13541889687379202 Validation Loss: 1.0297776460647583\n",
      "Epoch 6844: Training Loss: 0.13554620742797852 Validation Loss: 1.0301483869552612\n",
      "Epoch 6845: Training Loss: 0.13533156861861548 Validation Loss: 1.0304540395736694\n",
      "Epoch 6846: Training Loss: 0.13542387137810388 Validation Loss: 1.030395746231079\n",
      "Epoch 6847: Training Loss: 0.13529269645611444 Validation Loss: 1.030173897743225\n",
      "Epoch 6848: Training Loss: 0.13539679845174155 Validation Loss: 1.030264973640442\n",
      "Epoch 6849: Training Loss: 0.13514557977517447 Validation Loss: 1.029862880706787\n",
      "Epoch 6850: Training Loss: 0.13534163435300192 Validation Loss: 1.0298030376434326\n",
      "Epoch 6851: Training Loss: 0.13559836645921072 Validation Loss: 1.029585361480713\n",
      "Epoch 6852: Training Loss: 0.13515510658423105 Validation Loss: 1.029801368713379\n",
      "Epoch 6853: Training Loss: 0.13537473479906717 Validation Loss: 1.030035376548767\n",
      "Epoch 6854: Training Loss: 0.13492033133904138 Validation Loss: 1.0296238660812378\n",
      "Epoch 6855: Training Loss: 0.13491639494895935 Validation Loss: 1.0297753810882568\n",
      "Epoch 6856: Training Loss: 0.13475261628627777 Validation Loss: 1.0295277833938599\n",
      "Epoch 6857: Training Loss: 0.13501620044310889 Validation Loss: 1.0300695896148682\n",
      "Epoch 6858: Training Loss: 0.13497011363506317 Validation Loss: 1.0310205221176147\n",
      "Epoch 6859: Training Loss: 0.1349350313345591 Validation Loss: 1.0311100482940674\n",
      "Epoch 6860: Training Loss: 0.13534808407227197 Validation Loss: 1.0312618017196655\n",
      "Epoch 6861: Training Loss: 0.13573526094357172 Validation Loss: 1.0307464599609375\n",
      "Epoch 6862: Training Loss: 0.13523426155249277 Validation Loss: 1.0304210186004639\n",
      "Epoch 6863: Training Loss: 0.13488313555717468 Validation Loss: 1.030444622039795\n",
      "Epoch 6864: Training Loss: 0.13481575747330984 Validation Loss: 1.0298553705215454\n",
      "Epoch 6865: Training Loss: 0.13518202801545462 Validation Loss: 1.0293843746185303\n",
      "Epoch 6866: Training Loss: 0.13515069584051767 Validation Loss: 1.0299023389816284\n",
      "Epoch 6867: Training Loss: 0.13460904359817505 Validation Loss: 1.029892086982727\n",
      "Epoch 6868: Training Loss: 0.13542580852905908 Validation Loss: 1.0302398204803467\n",
      "Epoch 6869: Training Loss: 0.13412998616695404 Validation Loss: 1.0304607152938843\n",
      "Epoch 6870: Training Loss: 0.13536602507034937 Validation Loss: 1.03169846534729\n",
      "Epoch 6871: Training Loss: 0.13565309345722198 Validation Loss: 1.031957983970642\n",
      "Epoch 6872: Training Loss: 0.13465439279874167 Validation Loss: 1.0317704677581787\n",
      "Epoch 6873: Training Loss: 0.13545168687899908 Validation Loss: 1.0320981740951538\n",
      "Epoch 6874: Training Loss: 0.13451484342416128 Validation Loss: 1.031751036643982\n",
      "Epoch 6875: Training Loss: 0.13492674132188162 Validation Loss: 1.0308737754821777\n",
      "Epoch 6876: Training Loss: 0.13427540163199106 Validation Loss: 1.030457615852356\n",
      "Epoch 6877: Training Loss: 0.1347285658121109 Validation Loss: 1.0304044485092163\n",
      "Epoch 6878: Training Loss: 0.13473365207513174 Validation Loss: 1.0316938161849976\n",
      "Epoch 6879: Training Loss: 0.13459408779939017 Validation Loss: 1.0312862396240234\n",
      "Epoch 6880: Training Loss: 0.13467422624429068 Validation Loss: 1.0315613746643066\n",
      "Epoch 6881: Training Loss: 0.13540791471799216 Validation Loss: 1.0318626165390015\n",
      "Epoch 6882: Training Loss: 0.13467615842819214 Validation Loss: 1.0311676263809204\n",
      "Epoch 6883: Training Loss: 0.13439977169036865 Validation Loss: 1.0316790342330933\n",
      "Epoch 6884: Training Loss: 0.13464449097712836 Validation Loss: 1.0322641134262085\n",
      "Epoch 6885: Training Loss: 0.13471063474814096 Validation Loss: 1.0325243473052979\n",
      "Epoch 6886: Training Loss: 0.13609771678845087 Validation Loss: 1.03183114528656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6887: Training Loss: 0.13453146318594614 Validation Loss: 1.0318800210952759\n",
      "Epoch 6888: Training Loss: 0.1348180224498113 Validation Loss: 1.0307413339614868\n",
      "Epoch 6889: Training Loss: 0.13454695791006088 Validation Loss: 1.0313689708709717\n",
      "Epoch 6890: Training Loss: 0.1343308761715889 Validation Loss: 1.0313740968704224\n",
      "Epoch 6891: Training Loss: 0.13403656830390295 Validation Loss: 1.0314801931381226\n",
      "Epoch 6892: Training Loss: 0.13462240993976593 Validation Loss: 1.0314111709594727\n",
      "Epoch 6893: Training Loss: 0.13440060863892236 Validation Loss: 1.0315278768539429\n",
      "Epoch 6894: Training Loss: 0.1340510000785192 Validation Loss: 1.0314751863479614\n",
      "Epoch 6895: Training Loss: 0.13523980726798376 Validation Loss: 1.0333127975463867\n",
      "Epoch 6896: Training Loss: 0.13443333158890405 Validation Loss: 1.0321424007415771\n",
      "Epoch 6897: Training Loss: 0.13467852771282196 Validation Loss: 1.0317957401275635\n",
      "Epoch 6898: Training Loss: 0.13421092927455902 Validation Loss: 1.0316847562789917\n",
      "Epoch 6899: Training Loss: 0.13413242250680923 Validation Loss: 1.0314174890518188\n",
      "Epoch 6900: Training Loss: 0.13433735569318137 Validation Loss: 1.0316321849822998\n",
      "Epoch 6901: Training Loss: 0.13497755924860635 Validation Loss: 1.0317753553390503\n",
      "Epoch 6902: Training Loss: 0.13421371082464853 Validation Loss: 1.0323935747146606\n",
      "Epoch 6903: Training Loss: 0.13483993212381998 Validation Loss: 1.0317496061325073\n",
      "Epoch 6904: Training Loss: 0.1341263602177302 Validation Loss: 1.0316331386566162\n",
      "Epoch 6905: Training Loss: 0.1343405544757843 Validation Loss: 1.0324184894561768\n",
      "Epoch 6906: Training Loss: 0.13441662738720575 Validation Loss: 1.0325788259506226\n",
      "Epoch 6907: Training Loss: 0.13407853990793228 Validation Loss: 1.0322517156600952\n",
      "Epoch 6908: Training Loss: 0.1351045990983645 Validation Loss: 1.032294511795044\n",
      "Epoch 6909: Training Loss: 0.13515876978635788 Validation Loss: 1.0325450897216797\n",
      "Epoch 6910: Training Loss: 0.13400523364543915 Validation Loss: 1.0326980352401733\n",
      "Epoch 6911: Training Loss: 0.134436363975207 Validation Loss: 1.0325541496276855\n",
      "Epoch 6912: Training Loss: 0.13414859275023142 Validation Loss: 1.0322829484939575\n",
      "Epoch 6913: Training Loss: 0.1340643266836802 Validation Loss: 1.0320253372192383\n",
      "Epoch 6914: Training Loss: 0.1341406429807345 Validation Loss: 1.0318154096603394\n",
      "Epoch 6915: Training Loss: 0.1340633382399877 Validation Loss: 1.0319933891296387\n",
      "Epoch 6916: Training Loss: 0.13440133134524027 Validation Loss: 1.0325959920883179\n",
      "Epoch 6917: Training Loss: 0.13405163089434305 Validation Loss: 1.0327948331832886\n",
      "Epoch 6918: Training Loss: 0.13413064430157343 Validation Loss: 1.0327119827270508\n",
      "Epoch 6919: Training Loss: 0.13430381069580713 Validation Loss: 1.0325541496276855\n",
      "Epoch 6920: Training Loss: 0.13400947054227194 Validation Loss: 1.0322175025939941\n",
      "Epoch 6921: Training Loss: 0.13392245024442673 Validation Loss: 1.032261610031128\n",
      "Epoch 6922: Training Loss: 0.13418507079283395 Validation Loss: 1.031578779220581\n",
      "Epoch 6923: Training Loss: 0.1334653546412786 Validation Loss: 1.0317386388778687\n",
      "Epoch 6924: Training Loss: 0.1335755487283071 Validation Loss: 1.0321191549301147\n",
      "Epoch 6925: Training Loss: 0.13388627022504807 Validation Loss: 1.0328896045684814\n",
      "Epoch 6926: Training Loss: 0.13386988639831543 Validation Loss: 1.03252112865448\n",
      "Epoch 6927: Training Loss: 0.13398494074741998 Validation Loss: 1.0325340032577515\n",
      "Epoch 6928: Training Loss: 0.13397172838449478 Validation Loss: 1.0327800512313843\n",
      "Epoch 6929: Training Loss: 0.1340250497063001 Validation Loss: 1.032579779624939\n",
      "Epoch 6930: Training Loss: 0.13439075648784637 Validation Loss: 1.0329880714416504\n",
      "Epoch 6931: Training Loss: 0.1342911347746849 Validation Loss: 1.0325415134429932\n",
      "Epoch 6932: Training Loss: 0.1343814805150032 Validation Loss: 1.0326050519943237\n",
      "Epoch 6933: Training Loss: 0.13306719809770584 Validation Loss: 1.0327690839767456\n",
      "Epoch 6934: Training Loss: 0.13363758226235709 Validation Loss: 1.03266441822052\n",
      "Epoch 6935: Training Loss: 0.1342656190196673 Validation Loss: 1.033093810081482\n",
      "Epoch 6936: Training Loss: 0.1336128736535708 Validation Loss: 1.0327441692352295\n",
      "Epoch 6937: Training Loss: 0.1336240073045095 Validation Loss: 1.0326913595199585\n",
      "Epoch 6938: Training Loss: 0.13339043160279593 Validation Loss: 1.0324897766113281\n",
      "Epoch 6939: Training Loss: 0.13481996953487396 Validation Loss: 1.033013105392456\n",
      "Epoch 6940: Training Loss: 0.13303325821956 Validation Loss: 1.0334573984146118\n",
      "Epoch 6941: Training Loss: 0.13386844595273337 Validation Loss: 1.0335885286331177\n",
      "Epoch 6942: Training Loss: 0.13439231862624487 Validation Loss: 1.0338391065597534\n",
      "Epoch 6943: Training Loss: 0.1343779315551122 Validation Loss: 1.0329060554504395\n",
      "Epoch 6944: Training Loss: 0.13344707588354746 Validation Loss: 1.0329034328460693\n",
      "Epoch 6945: Training Loss: 0.13328583041826883 Validation Loss: 1.033046841621399\n",
      "Epoch 6946: Training Loss: 0.13333853830893835 Validation Loss: 1.0334844589233398\n",
      "Epoch 6947: Training Loss: 0.1333493491013845 Validation Loss: 1.0334360599517822\n",
      "Epoch 6948: Training Loss: 0.13356428841749826 Validation Loss: 1.0338541269302368\n",
      "Epoch 6949: Training Loss: 0.1340425362189611 Validation Loss: 1.034044623374939\n",
      "Epoch 6950: Training Loss: 0.13327381014823914 Validation Loss: 1.0332226753234863\n",
      "Epoch 6951: Training Loss: 0.13349037369092306 Validation Loss: 1.0332399606704712\n",
      "Epoch 6952: Training Loss: 0.13376938303311667 Validation Loss: 1.0329415798187256\n",
      "Epoch 6953: Training Loss: 0.13333850602308908 Validation Loss: 1.0329080820083618\n",
      "Epoch 6954: Training Loss: 0.13367082426945368 Validation Loss: 1.033571481704712\n",
      "Epoch 6955: Training Loss: 0.13341545313596725 Validation Loss: 1.0332117080688477\n",
      "Epoch 6956: Training Loss: 0.13343160599470139 Validation Loss: 1.0335618257522583\n",
      "Epoch 6957: Training Loss: 0.13400246451298395 Validation Loss: 1.0337437391281128\n",
      "Epoch 6958: Training Loss: 0.13419391959905624 Validation Loss: 1.0344551801681519\n",
      "Epoch 6959: Training Loss: 0.13332068175077438 Validation Loss: 1.0343754291534424\n",
      "Epoch 6960: Training Loss: 0.13342900574207306 Validation Loss: 1.033495306968689\n",
      "Epoch 6961: Training Loss: 0.1334486852089564 Validation Loss: 1.033228874206543\n",
      "Epoch 6962: Training Loss: 0.1331795925895373 Validation Loss: 1.0332492589950562\n",
      "Epoch 6963: Training Loss: 0.13313896457354227 Validation Loss: 1.0340638160705566\n",
      "Epoch 6964: Training Loss: 0.1326534003019333 Validation Loss: 1.034429669380188\n",
      "Epoch 6965: Training Loss: 0.1331426352262497 Validation Loss: 1.0342907905578613\n",
      "Epoch 6966: Training Loss: 0.1331713249286016 Validation Loss: 1.03359055519104\n",
      "Epoch 6967: Training Loss: 0.1331565131743749 Validation Loss: 1.0334768295288086\n",
      "Epoch 6968: Training Loss: 0.13314868013064066 Validation Loss: 1.0337185859680176\n",
      "Epoch 6969: Training Loss: 0.13355215142170587 Validation Loss: 1.0344594717025757\n",
      "Epoch 6970: Training Loss: 0.1338425800204277 Validation Loss: 1.0344805717468262\n",
      "Epoch 6971: Training Loss: 0.132901298503081 Validation Loss: 1.0341562032699585\n",
      "Epoch 6972: Training Loss: 0.13340222338835397 Validation Loss: 1.0336556434631348\n",
      "Epoch 6973: Training Loss: 0.1342231606443723 Validation Loss: 1.0339947938919067\n",
      "Epoch 6974: Training Loss: 0.13318958630164465 Validation Loss: 1.0345207452774048\n",
      "Epoch 6975: Training Loss: 0.13292059302330017 Validation Loss: 1.0342000722885132\n",
      "Epoch 6976: Training Loss: 0.13270611315965652 Validation Loss: 1.0342211723327637\n",
      "Epoch 6977: Training Loss: 0.1327273646990458 Validation Loss: 1.0340648889541626\n",
      "Epoch 6978: Training Loss: 0.13359832018613815 Validation Loss: 1.0339478254318237\n",
      "Epoch 6979: Training Loss: 0.13299607237180075 Validation Loss: 1.033994197845459\n",
      "Epoch 6980: Training Loss: 0.13369995852311453 Validation Loss: 1.0338503122329712\n",
      "Epoch 6981: Training Loss: 0.13317124297221503 Validation Loss: 1.0343457460403442\n",
      "Epoch 6982: Training Loss: 0.13296370208263397 Validation Loss: 1.0344054698944092\n",
      "Epoch 6983: Training Loss: 0.13280268013477325 Validation Loss: 1.0338636636734009\n",
      "Epoch 6984: Training Loss: 0.13352000216643015 Validation Loss: 1.0342373847961426\n",
      "Epoch 6985: Training Loss: 0.13316975285609564 Validation Loss: 1.0340243577957153\n",
      "Epoch 6986: Training Loss: 0.1331822151939074 Validation Loss: 1.0338045358657837\n",
      "Epoch 6987: Training Loss: 0.1334237257639567 Validation Loss: 1.0340662002563477\n",
      "Epoch 6988: Training Loss: 0.13278555870056152 Validation Loss: 1.0344444513320923\n",
      "Epoch 6989: Training Loss: 0.13264617572228113 Validation Loss: 1.034774661064148\n",
      "Epoch 6990: Training Loss: 0.1322858730951945 Validation Loss: 1.035372257232666\n",
      "Epoch 6991: Training Loss: 0.1328775535027186 Validation Loss: 1.0356131792068481\n",
      "Epoch 6992: Training Loss: 0.1329429770509402 Validation Loss: 1.03483247756958\n",
      "Epoch 6993: Training Loss: 0.1326623558998108 Validation Loss: 1.034529447555542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6994: Training Loss: 0.13249585529168448 Validation Loss: 1.0340189933776855\n",
      "Epoch 6995: Training Loss: 0.1331616168220838 Validation Loss: 1.0336397886276245\n",
      "Epoch 6996: Training Loss: 0.13265334566434225 Validation Loss: 1.0348520278930664\n",
      "Epoch 6997: Training Loss: 0.13317106167475382 Validation Loss: 1.0346217155456543\n",
      "Epoch 6998: Training Loss: 0.1325675050417582 Validation Loss: 1.0352298021316528\n",
      "Epoch 6999: Training Loss: 0.132535919547081 Validation Loss: 1.0354840755462646\n",
      "Epoch 7000: Training Loss: 0.13225588450829187 Validation Loss: 1.034872055053711\n",
      "Epoch 7001: Training Loss: 0.13257677853107452 Validation Loss: 1.0353596210479736\n",
      "Epoch 7002: Training Loss: 0.13234628985325494 Validation Loss: 1.0350297689437866\n",
      "Epoch 7003: Training Loss: 0.1325512776772181 Validation Loss: 1.0351967811584473\n",
      "Epoch 7004: Training Loss: 0.13231669863065085 Validation Loss: 1.0355936288833618\n",
      "Epoch 7005: Training Loss: 0.13235252102216086 Validation Loss: 1.0357838869094849\n",
      "Epoch 7006: Training Loss: 0.13254699856042862 Validation Loss: 1.036200761795044\n",
      "Epoch 7007: Training Loss: 0.13265598565340042 Validation Loss: 1.0356531143188477\n",
      "Epoch 7008: Training Loss: 0.13195540507634482 Validation Loss: 1.0345042943954468\n",
      "Epoch 7009: Training Loss: 0.13339103758335114 Validation Loss: 1.0350186824798584\n",
      "Epoch 7010: Training Loss: 0.13285301129023233 Validation Loss: 1.0349949598312378\n",
      "Epoch 7011: Training Loss: 0.13254926105340323 Validation Loss: 1.0347676277160645\n",
      "Epoch 7012: Training Loss: 0.13235180576642355 Validation Loss: 1.0349787473678589\n",
      "Epoch 7013: Training Loss: 0.1327733819683393 Validation Loss: 1.0351334810256958\n",
      "Epoch 7014: Training Loss: 0.13198983917633691 Validation Loss: 1.034838080406189\n",
      "Epoch 7015: Training Loss: 0.13258564472198486 Validation Loss: 1.034676432609558\n",
      "Epoch 7016: Training Loss: 0.13209676494201025 Validation Loss: 1.0345991849899292\n",
      "Epoch 7017: Training Loss: 0.13237964610258737 Validation Loss: 1.034490942955017\n",
      "Epoch 7018: Training Loss: 0.13286074747641882 Validation Loss: 1.0347306728363037\n",
      "Epoch 7019: Training Loss: 0.1327303797006607 Validation Loss: 1.0354310274124146\n",
      "Epoch 7020: Training Loss: 0.1327901855111122 Validation Loss: 1.0361493825912476\n",
      "Epoch 7021: Training Loss: 0.13239056120316187 Validation Loss: 1.0351533889770508\n",
      "Epoch 7022: Training Loss: 0.13216273734966913 Validation Loss: 1.0354100465774536\n",
      "Epoch 7023: Training Loss: 0.1321377456188202 Validation Loss: 1.0353928804397583\n",
      "Epoch 7024: Training Loss: 0.13238133738438287 Validation Loss: 1.0340197086334229\n",
      "Epoch 7025: Training Loss: 0.13209229211012521 Validation Loss: 1.0348236560821533\n",
      "Epoch 7026: Training Loss: 0.13257265090942383 Validation Loss: 1.035506010055542\n",
      "Epoch 7027: Training Loss: 0.13209396849075952 Validation Loss: 1.0348998308181763\n",
      "Epoch 7028: Training Loss: 0.1317709063490232 Validation Loss: 1.0351123809814453\n",
      "Epoch 7029: Training Loss: 0.13204402228196463 Validation Loss: 1.0352253913879395\n",
      "Epoch 7030: Training Loss: 0.13250725467999777 Validation Loss: 1.035699486732483\n",
      "Epoch 7031: Training Loss: 0.13214750587940216 Validation Loss: 1.0358753204345703\n",
      "Epoch 7032: Training Loss: 0.13231260826190314 Validation Loss: 1.0357553958892822\n",
      "Epoch 7033: Training Loss: 0.13211455196142197 Validation Loss: 1.0362193584442139\n",
      "Epoch 7034: Training Loss: 0.13201196491718292 Validation Loss: 1.03662109375\n",
      "Epoch 7035: Training Loss: 0.13209321101506552 Validation Loss: 1.036306381225586\n",
      "Epoch 7036: Training Loss: 0.13238079100847244 Validation Loss: 1.035831093788147\n",
      "Epoch 7037: Training Loss: 0.13231933116912842 Validation Loss: 1.035513162612915\n",
      "Epoch 7038: Training Loss: 0.13197619219621023 Validation Loss: 1.0362305641174316\n",
      "Epoch 7039: Training Loss: 0.13194492956002554 Validation Loss: 1.035948395729065\n",
      "Epoch 7040: Training Loss: 0.13210700949033102 Validation Loss: 1.0361952781677246\n",
      "Epoch 7041: Training Loss: 0.1318432167172432 Validation Loss: 1.036942481994629\n",
      "Epoch 7042: Training Loss: 0.13186992208162943 Validation Loss: 1.0367953777313232\n",
      "Epoch 7043: Training Loss: 0.13267071545124054 Validation Loss: 1.0378843545913696\n",
      "Epoch 7044: Training Loss: 0.13231507440408072 Validation Loss: 1.0368520021438599\n",
      "Epoch 7045: Training Loss: 0.13161453853050867 Validation Loss: 1.0360820293426514\n",
      "Epoch 7046: Training Loss: 0.13169731199741364 Validation Loss: 1.035866379737854\n",
      "Epoch 7047: Training Loss: 0.13164611905813217 Validation Loss: 1.0361180305480957\n",
      "Epoch 7048: Training Loss: 0.13160754491885504 Validation Loss: 1.0367145538330078\n",
      "Epoch 7049: Training Loss: 0.13218722492456436 Validation Loss: 1.0365346670150757\n",
      "Epoch 7050: Training Loss: 0.13172862927118936 Validation Loss: 1.0370959043502808\n",
      "Epoch 7051: Training Loss: 0.13174764811992645 Validation Loss: 1.037567377090454\n",
      "Epoch 7052: Training Loss: 0.13168452183405557 Validation Loss: 1.0367339849472046\n",
      "Epoch 7053: Training Loss: 0.13250799725453058 Validation Loss: 1.0374116897583008\n",
      "Epoch 7054: Training Loss: 0.13191290448109308 Validation Loss: 1.0369490385055542\n",
      "Epoch 7055: Training Loss: 0.1315780927737554 Validation Loss: 1.036874771118164\n",
      "Epoch 7056: Training Loss: 0.1320656811197599 Validation Loss: 1.0365699529647827\n",
      "Epoch 7057: Training Loss: 0.13168229907751083 Validation Loss: 1.036051630973816\n",
      "Epoch 7058: Training Loss: 0.13156567762295404 Validation Loss: 1.0359052419662476\n",
      "Epoch 7059: Training Loss: 0.1315221885840098 Validation Loss: 1.0360488891601562\n",
      "Epoch 7060: Training Loss: 0.13137889405091605 Validation Loss: 1.0361664295196533\n",
      "Epoch 7061: Training Loss: 0.13176237543423971 Validation Loss: 1.0368587970733643\n",
      "Epoch 7062: Training Loss: 0.13174148400624594 Validation Loss: 1.0366135835647583\n",
      "Epoch 7063: Training Loss: 0.1318708062171936 Validation Loss: 1.0361802577972412\n",
      "Epoch 7064: Training Loss: 0.13135739167531332 Validation Loss: 1.0364807844161987\n",
      "Epoch 7065: Training Loss: 0.13145366807778677 Validation Loss: 1.0360976457595825\n",
      "Epoch 7066: Training Loss: 0.13165009766817093 Validation Loss: 1.036188006401062\n",
      "Epoch 7067: Training Loss: 0.13138402998447418 Validation Loss: 1.0363879203796387\n",
      "Epoch 7068: Training Loss: 0.13168549040953317 Validation Loss: 1.037346363067627\n",
      "Epoch 7069: Training Loss: 0.13126495977242789 Validation Loss: 1.0373427867889404\n",
      "Epoch 7070: Training Loss: 0.1309651111563047 Validation Loss: 1.037222146987915\n",
      "Epoch 7071: Training Loss: 0.1312571441133817 Validation Loss: 1.03706693649292\n",
      "Epoch 7072: Training Loss: 0.13176782925923666 Validation Loss: 1.0369685888290405\n",
      "Epoch 7073: Training Loss: 0.13142102708419165 Validation Loss: 1.0374157428741455\n",
      "Epoch 7074: Training Loss: 0.13192250827948251 Validation Loss: 1.0377590656280518\n",
      "Epoch 7075: Training Loss: 0.1313204417626063 Validation Loss: 1.0377910137176514\n",
      "Epoch 7076: Training Loss: 0.13119315107663473 Validation Loss: 1.0376485586166382\n",
      "Epoch 7077: Training Loss: 0.13148648291826248 Validation Loss: 1.0376405715942383\n",
      "Epoch 7078: Training Loss: 0.13201178361972174 Validation Loss: 1.0384159088134766\n",
      "Epoch 7079: Training Loss: 0.13152647018432617 Validation Loss: 1.038431167602539\n",
      "Epoch 7080: Training Loss: 0.13173480331897736 Validation Loss: 1.0386590957641602\n",
      "Epoch 7081: Training Loss: 0.13079364597797394 Validation Loss: 1.0382592678070068\n",
      "Epoch 7082: Training Loss: 0.13170413424571356 Validation Loss: 1.0384825468063354\n",
      "Epoch 7083: Training Loss: 0.13098042706648508 Validation Loss: 1.0375512838363647\n",
      "Epoch 7084: Training Loss: 0.13182604561249414 Validation Loss: 1.036901593208313\n",
      "Epoch 7085: Training Loss: 0.13107851644357046 Validation Loss: 1.0362372398376465\n",
      "Epoch 7086: Training Loss: 0.13208184142907461 Validation Loss: 1.0362091064453125\n",
      "Epoch 7087: Training Loss: 0.13134985665480295 Validation Loss: 1.0362045764923096\n",
      "Epoch 7088: Training Loss: 0.13160778830448785 Validation Loss: 1.0362049341201782\n",
      "Epoch 7089: Training Loss: 0.13124080995718637 Validation Loss: 1.0364307165145874\n",
      "Epoch 7090: Training Loss: 0.13134583830833435 Validation Loss: 1.0374525785446167\n",
      "Epoch 7091: Training Loss: 0.1312286158402761 Validation Loss: 1.0374977588653564\n",
      "Epoch 7092: Training Loss: 0.13117116192976633 Validation Loss: 1.0385537147521973\n",
      "Epoch 7093: Training Loss: 0.13106336692969003 Validation Loss: 1.0383986234664917\n",
      "Epoch 7094: Training Loss: 0.13118502497673035 Validation Loss: 1.0387018918991089\n",
      "Epoch 7095: Training Loss: 0.13123491406440735 Validation Loss: 1.038962483406067\n",
      "Epoch 7096: Training Loss: 0.130906251569589 Validation Loss: 1.038719654083252\n",
      "Epoch 7097: Training Loss: 0.13262142489353815 Validation Loss: 1.038143515586853\n",
      "Epoch 7098: Training Loss: 0.13124613463878632 Validation Loss: 1.037901759147644\n",
      "Epoch 7099: Training Loss: 0.13107196738322577 Validation Loss: 1.0381824970245361\n",
      "Epoch 7100: Training Loss: 0.1312162528435389 Validation Loss: 1.038140892982483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7101: Training Loss: 0.1311358908812205 Validation Loss: 1.0375375747680664\n",
      "Epoch 7102: Training Loss: 0.13136711716651917 Validation Loss: 1.0382040739059448\n",
      "Epoch 7103: Training Loss: 0.13100321094195047 Validation Loss: 1.0382530689239502\n",
      "Epoch 7104: Training Loss: 0.13141258309284845 Validation Loss: 1.0375393629074097\n",
      "Epoch 7105: Training Loss: 0.13111557314793268 Validation Loss: 1.0374650955200195\n",
      "Epoch 7106: Training Loss: 0.1307028904557228 Validation Loss: 1.037531852722168\n",
      "Epoch 7107: Training Loss: 0.13097809503475824 Validation Loss: 1.0380865335464478\n",
      "Epoch 7108: Training Loss: 0.13142812003691992 Validation Loss: 1.0373597145080566\n",
      "Epoch 7109: Training Loss: 0.13070032993952432 Validation Loss: 1.037406325340271\n",
      "Epoch 7110: Training Loss: 0.13099761307239532 Validation Loss: 1.0383965969085693\n",
      "Epoch 7111: Training Loss: 0.13064955919981003 Validation Loss: 1.0390665531158447\n",
      "Epoch 7112: Training Loss: 0.13070473819971085 Validation Loss: 1.0388119220733643\n",
      "Epoch 7113: Training Loss: 0.13068561255931854 Validation Loss: 1.0391298532485962\n",
      "Epoch 7114: Training Loss: 0.13093917071819305 Validation Loss: 1.0382152795791626\n",
      "Epoch 7115: Training Loss: 0.13067734986543655 Validation Loss: 1.0389763116836548\n",
      "Epoch 7116: Training Loss: 0.13137425730625787 Validation Loss: 1.0387022495269775\n",
      "Epoch 7117: Training Loss: 0.13098084181547165 Validation Loss: 1.0385159254074097\n",
      "Epoch 7118: Training Loss: 0.13061350087324777 Validation Loss: 1.0381088256835938\n",
      "Epoch 7119: Training Loss: 0.13101093719402948 Validation Loss: 1.038016438484192\n",
      "Epoch 7120: Training Loss: 0.13063320517539978 Validation Loss: 1.038541555404663\n",
      "Epoch 7121: Training Loss: 0.1304018646478653 Validation Loss: 1.0389100313186646\n",
      "Epoch 7122: Training Loss: 0.13088499506314596 Validation Loss: 1.0387229919433594\n",
      "Epoch 7123: Training Loss: 0.13049380977948508 Validation Loss: 1.0379849672317505\n",
      "Epoch 7124: Training Loss: 0.1309706469376882 Validation Loss: 1.0384454727172852\n",
      "Epoch 7125: Training Loss: 0.1309368833899498 Validation Loss: 1.0384238958358765\n",
      "Epoch 7126: Training Loss: 0.1304664413134257 Validation Loss: 1.0384162664413452\n",
      "Epoch 7127: Training Loss: 0.131061851978302 Validation Loss: 1.0395863056182861\n",
      "Epoch 7128: Training Loss: 0.13027838617563248 Validation Loss: 1.0394595861434937\n",
      "Epoch 7129: Training Loss: 0.130840169886748 Validation Loss: 1.0389958620071411\n",
      "Epoch 7130: Training Loss: 0.13037129739920297 Validation Loss: 1.0389940738677979\n",
      "Epoch 7131: Training Loss: 0.13074449946482977 Validation Loss: 1.0387815237045288\n",
      "Epoch 7132: Training Loss: 0.13046138236920038 Validation Loss: 1.0387039184570312\n",
      "Epoch 7133: Training Loss: 0.13090142359336218 Validation Loss: 1.0388137102127075\n",
      "Epoch 7134: Training Loss: 0.13078029453754425 Validation Loss: 1.0388017892837524\n",
      "Epoch 7135: Training Loss: 0.13032331069310507 Validation Loss: 1.0389090776443481\n",
      "Epoch 7136: Training Loss: 0.13022573788960776 Validation Loss: 1.0385791063308716\n",
      "Epoch 7137: Training Loss: 0.1308661326766014 Validation Loss: 1.0393532514572144\n",
      "Epoch 7138: Training Loss: 0.1303500533103943 Validation Loss: 1.0388191938400269\n",
      "Epoch 7139: Training Loss: 0.13023866961399713 Validation Loss: 1.0389286279678345\n",
      "Epoch 7140: Training Loss: 0.13044672956069311 Validation Loss: 1.0389782190322876\n",
      "Epoch 7141: Training Loss: 0.13149229685465494 Validation Loss: 1.0394794940948486\n",
      "Epoch 7142: Training Loss: 0.13036884119113287 Validation Loss: 1.0397052764892578\n",
      "Epoch 7143: Training Loss: 0.13055002441008887 Validation Loss: 1.0396658182144165\n",
      "Epoch 7144: Training Loss: 0.12988719095786413 Validation Loss: 1.0404726266860962\n",
      "Epoch 7145: Training Loss: 0.1297446663180987 Validation Loss: 1.0395996570587158\n",
      "Epoch 7146: Training Loss: 0.13013961911201477 Validation Loss: 1.0393801927566528\n",
      "Epoch 7147: Training Loss: 0.12980875869592032 Validation Loss: 1.0391956567764282\n",
      "Epoch 7148: Training Loss: 0.1299086958169937 Validation Loss: 1.0393699407577515\n",
      "Epoch 7149: Training Loss: 0.13010753691196442 Validation Loss: 1.0396541357040405\n",
      "Epoch 7150: Training Loss: 0.13064110527435938 Validation Loss: 1.0396785736083984\n",
      "Epoch 7151: Training Loss: 0.129974198838075 Validation Loss: 1.039665699005127\n",
      "Epoch 7152: Training Loss: 0.13042214512825012 Validation Loss: 1.0389783382415771\n",
      "Epoch 7153: Training Loss: 0.13046403725941977 Validation Loss: 1.0387499332427979\n",
      "Epoch 7154: Training Loss: 0.13120215634504953 Validation Loss: 1.038450002670288\n",
      "Epoch 7155: Training Loss: 0.12992106874783835 Validation Loss: 1.0390548706054688\n",
      "Epoch 7156: Training Loss: 0.13027347872654596 Validation Loss: 1.0397800207138062\n",
      "Epoch 7157: Training Loss: 0.1300549308458964 Validation Loss: 1.0391374826431274\n",
      "Epoch 7158: Training Loss: 0.12994472434123358 Validation Loss: 1.0396826267242432\n",
      "Epoch 7159: Training Loss: 0.1299473742643992 Validation Loss: 1.040563702583313\n",
      "Epoch 7160: Training Loss: 0.1299778347214063 Validation Loss: 1.0407263040542603\n",
      "Epoch 7161: Training Loss: 0.1298837512731552 Validation Loss: 1.0401932001113892\n",
      "Epoch 7162: Training Loss: 0.130842794974645 Validation Loss: 1.0396316051483154\n",
      "Epoch 7163: Training Loss: 0.1305982619524002 Validation Loss: 1.0398041009902954\n",
      "Epoch 7164: Training Loss: 0.12991472582022348 Validation Loss: 1.0401415824890137\n",
      "Epoch 7165: Training Loss: 0.13023908187945685 Validation Loss: 1.0397861003875732\n",
      "Epoch 7166: Training Loss: 0.12987985710302988 Validation Loss: 1.0399812459945679\n",
      "Epoch 7167: Training Loss: 0.129742793738842 Validation Loss: 1.0400747060775757\n",
      "Epoch 7168: Training Loss: 0.13032289346059164 Validation Loss: 1.0404435396194458\n",
      "Epoch 7169: Training Loss: 0.1302738537391027 Validation Loss: 1.0401588678359985\n",
      "Epoch 7170: Training Loss: 0.1300856719414393 Validation Loss: 1.0390491485595703\n",
      "Epoch 7171: Training Loss: 0.1297972227136294 Validation Loss: 1.040000319480896\n",
      "Epoch 7172: Training Loss: 0.1301150694489479 Validation Loss: 1.0405259132385254\n",
      "Epoch 7173: Training Loss: 0.12966351211071014 Validation Loss: 1.0405428409576416\n",
      "Epoch 7174: Training Loss: 0.12949432929356894 Validation Loss: 1.0408321619033813\n",
      "Epoch 7175: Training Loss: 0.1295881395538648 Validation Loss: 1.0402603149414062\n",
      "Epoch 7176: Training Loss: 0.12971283495426178 Validation Loss: 1.0404644012451172\n",
      "Epoch 7177: Training Loss: 0.1303292984763781 Validation Loss: 1.0403234958648682\n",
      "Epoch 7178: Training Loss: 0.13050575306018194 Validation Loss: 1.0405288934707642\n",
      "Epoch 7179: Training Loss: 0.12961110969384512 Validation Loss: 1.0407037734985352\n",
      "Epoch 7180: Training Loss: 0.12969458351532617 Validation Loss: 1.0405535697937012\n",
      "Epoch 7181: Training Loss: 0.1294407695531845 Validation Loss: 1.0402188301086426\n",
      "Epoch 7182: Training Loss: 0.1302051618695259 Validation Loss: 1.0405303239822388\n",
      "Epoch 7183: Training Loss: 0.12998835494120917 Validation Loss: 1.0408381223678589\n",
      "Epoch 7184: Training Loss: 0.12954932947953543 Validation Loss: 1.0411453247070312\n",
      "Epoch 7185: Training Loss: 0.1298335393269857 Validation Loss: 1.0402437448501587\n",
      "Epoch 7186: Training Loss: 0.13000570982694626 Validation Loss: 1.0402721166610718\n",
      "Epoch 7187: Training Loss: 0.12978549053271612 Validation Loss: 1.0398309230804443\n",
      "Epoch 7188: Training Loss: 0.12954095005989075 Validation Loss: 1.0404688119888306\n",
      "Epoch 7189: Training Loss: 0.12980875124533972 Validation Loss: 1.0407105684280396\n",
      "Epoch 7190: Training Loss: 0.12974743793408075 Validation Loss: 1.041567087173462\n",
      "Epoch 7191: Training Loss: 0.12964257349570593 Validation Loss: 1.0411161184310913\n",
      "Epoch 7192: Training Loss: 0.12965514262517294 Validation Loss: 1.0408614873886108\n",
      "Epoch 7193: Training Loss: 0.12939859926700592 Validation Loss: 1.0409016609191895\n",
      "Epoch 7194: Training Loss: 0.12928306063016257 Validation Loss: 1.0406551361083984\n",
      "Epoch 7195: Training Loss: 0.12985054155190787 Validation Loss: 1.0402297973632812\n",
      "Epoch 7196: Training Loss: 0.12963712960481644 Validation Loss: 1.0405523777008057\n",
      "Epoch 7197: Training Loss: 0.12937344113985697 Validation Loss: 1.0402030944824219\n",
      "Epoch 7198: Training Loss: 0.1294274926185608 Validation Loss: 1.0405534505844116\n",
      "Epoch 7199: Training Loss: 0.12931269903977713 Validation Loss: 1.0404645204544067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7200: Training Loss: 0.12995142241319022 Validation Loss: 1.0403950214385986\n",
      "Epoch 7201: Training Loss: 0.1295123447974523 Validation Loss: 1.0408645868301392\n",
      "Epoch 7202: Training Loss: 0.12918370962142944 Validation Loss: 1.0414005517959595\n",
      "Epoch 7203: Training Loss: 0.12962623437245688 Validation Loss: 1.0415186882019043\n",
      "Epoch 7204: Training Loss: 0.12939781695604324 Validation Loss: 1.0414928197860718\n",
      "Epoch 7205: Training Loss: 0.12926672399044037 Validation Loss: 1.041207194328308\n",
      "Epoch 7206: Training Loss: 0.1304596463839213 Validation Loss: 1.0411676168441772\n",
      "Epoch 7207: Training Loss: 0.12916159133116403 Validation Loss: 1.041113257408142\n",
      "Epoch 7208: Training Loss: 0.12913541992505392 Validation Loss: 1.0411781072616577\n",
      "Epoch 7209: Training Loss: 0.1292951131860415 Validation Loss: 1.0411694049835205\n",
      "Epoch 7210: Training Loss: 0.1293681338429451 Validation Loss: 1.04105806350708\n",
      "Epoch 7211: Training Loss: 0.12932276725769043 Validation Loss: 1.0417839288711548\n",
      "Epoch 7212: Training Loss: 0.12941833337148032 Validation Loss: 1.0416514873504639\n",
      "Epoch 7213: Training Loss: 0.12933604419231415 Validation Loss: 1.0418180227279663\n",
      "Epoch 7214: Training Loss: 0.1286133180061976 Validation Loss: 1.0412163734436035\n",
      "Epoch 7215: Training Loss: 0.12920934706926346 Validation Loss: 1.0418800115585327\n",
      "Epoch 7216: Training Loss: 0.12894130994876227 Validation Loss: 1.0419282913208008\n",
      "Epoch 7217: Training Loss: 0.12893997877836227 Validation Loss: 1.0426353216171265\n",
      "Epoch 7218: Training Loss: 0.12918691833813986 Validation Loss: 1.0422202348709106\n",
      "Epoch 7219: Training Loss: 0.12951735158761343 Validation Loss: 1.0419561862945557\n",
      "Epoch 7220: Training Loss: 0.12879285961389542 Validation Loss: 1.041736125946045\n",
      "Epoch 7221: Training Loss: 0.12893831233183542 Validation Loss: 1.040804386138916\n",
      "Epoch 7222: Training Loss: 0.12913755079110464 Validation Loss: 1.04118812084198\n",
      "Epoch 7223: Training Loss: 0.12893098841110864 Validation Loss: 1.0407906770706177\n",
      "Epoch 7224: Training Loss: 0.1291454608241717 Validation Loss: 1.040878176689148\n",
      "Epoch 7225: Training Loss: 0.128944493830204 Validation Loss: 1.0413966178894043\n",
      "Epoch 7226: Training Loss: 0.12868748356898627 Validation Loss: 1.0421679019927979\n",
      "Epoch 7227: Training Loss: 0.12850312888622284 Validation Loss: 1.0420721769332886\n",
      "Epoch 7228: Training Loss: 0.1286837806304296 Validation Loss: 1.0416982173919678\n",
      "Epoch 7229: Training Loss: 0.12874397883812586 Validation Loss: 1.042067289352417\n",
      "Epoch 7230: Training Loss: 0.1290902942419052 Validation Loss: 1.042325496673584\n",
      "Epoch 7231: Training Loss: 0.1288303335507711 Validation Loss: 1.0426291227340698\n",
      "Epoch 7232: Training Loss: 0.12887517114480337 Validation Loss: 1.042249321937561\n",
      "Epoch 7233: Training Loss: 0.12862759083509445 Validation Loss: 1.0419453382492065\n",
      "Epoch 7234: Training Loss: 0.12887178858121237 Validation Loss: 1.041365385055542\n",
      "Epoch 7235: Training Loss: 0.1286393329501152 Validation Loss: 1.0413254499435425\n",
      "Epoch 7236: Training Loss: 0.1289475386341413 Validation Loss: 1.040906310081482\n",
      "Epoch 7237: Training Loss: 0.12881314754486084 Validation Loss: 1.041497826576233\n",
      "Epoch 7238: Training Loss: 0.12873071928819022 Validation Loss: 1.042251706123352\n",
      "Epoch 7239: Training Loss: 0.1290001372496287 Validation Loss: 1.0427989959716797\n",
      "Epoch 7240: Training Loss: 0.12858373920122781 Validation Loss: 1.0429086685180664\n",
      "Epoch 7241: Training Loss: 0.12854582568009695 Validation Loss: 1.0431349277496338\n",
      "Epoch 7242: Training Loss: 0.12863023082415262 Validation Loss: 1.042593002319336\n",
      "Epoch 7243: Training Loss: 0.12907079607248306 Validation Loss: 1.0419162511825562\n",
      "Epoch 7244: Training Loss: 0.13016215711832047 Validation Loss: 1.0418347120285034\n",
      "Epoch 7245: Training Loss: 0.12906660387913385 Validation Loss: 1.0432814359664917\n",
      "Epoch 7246: Training Loss: 0.1293760985136032 Validation Loss: 1.043213129043579\n",
      "Epoch 7247: Training Loss: 0.1286242405573527 Validation Loss: 1.043460488319397\n",
      "Epoch 7248: Training Loss: 0.12849342823028564 Validation Loss: 1.0430172681808472\n",
      "Epoch 7249: Training Loss: 0.12911732494831085 Validation Loss: 1.043140172958374\n",
      "Epoch 7250: Training Loss: 0.12858039140701294 Validation Loss: 1.0426982641220093\n",
      "Epoch 7251: Training Loss: 0.12859114756186804 Validation Loss: 1.0424565076828003\n",
      "Epoch 7252: Training Loss: 0.12877180675665537 Validation Loss: 1.042342185974121\n",
      "Epoch 7253: Training Loss: 0.1284323905905088 Validation Loss: 1.0428826808929443\n",
      "Epoch 7254: Training Loss: 0.1286517232656479 Validation Loss: 1.0427888631820679\n",
      "Epoch 7255: Training Loss: 0.12883348762989044 Validation Loss: 1.0424988269805908\n",
      "Epoch 7256: Training Loss: 0.1284915084640185 Validation Loss: 1.0423082113265991\n",
      "Epoch 7257: Training Loss: 0.12810304760932922 Validation Loss: 1.0432325601577759\n",
      "Epoch 7258: Training Loss: 0.12875967224438986 Validation Loss: 1.043638825416565\n",
      "Epoch 7259: Training Loss: 0.12862209230661392 Validation Loss: 1.043502688407898\n",
      "Epoch 7260: Training Loss: 0.12836632877588272 Validation Loss: 1.0433969497680664\n",
      "Epoch 7261: Training Loss: 0.12836311757564545 Validation Loss: 1.0433144569396973\n",
      "Epoch 7262: Training Loss: 0.12901756167411804 Validation Loss: 1.0439032316207886\n",
      "Epoch 7263: Training Loss: 0.12847943852345148 Validation Loss: 1.0437989234924316\n",
      "Epoch 7264: Training Loss: 0.12841419875621796 Validation Loss: 1.042946696281433\n",
      "Epoch 7265: Training Loss: 0.12814778834581375 Validation Loss: 1.0427838563919067\n",
      "Epoch 7266: Training Loss: 0.12873358776172003 Validation Loss: 1.0428709983825684\n",
      "Epoch 7267: Training Loss: 0.1282330850760142 Validation Loss: 1.042473554611206\n",
      "Epoch 7268: Training Loss: 0.12834831327199936 Validation Loss: 1.0420482158660889\n",
      "Epoch 7269: Training Loss: 0.1283221816023191 Validation Loss: 1.0427409410476685\n",
      "Epoch 7270: Training Loss: 0.1291465163230896 Validation Loss: 1.0434547662734985\n",
      "Epoch 7271: Training Loss: 0.12807808071374893 Validation Loss: 1.042748212814331\n",
      "Epoch 7272: Training Loss: 0.12826046099265417 Validation Loss: 1.0417770147323608\n",
      "Epoch 7273: Training Loss: 0.12798998753229776 Validation Loss: 1.0422505140304565\n",
      "Epoch 7274: Training Loss: 0.12832491596539816 Validation Loss: 1.0429843664169312\n",
      "Epoch 7275: Training Loss: 0.12917368859052658 Validation Loss: 1.0433311462402344\n",
      "Epoch 7276: Training Loss: 0.1288891260822614 Validation Loss: 1.0440369844436646\n",
      "Epoch 7277: Training Loss: 0.1282773514588674 Validation Loss: 1.0443743467330933\n",
      "Epoch 7278: Training Loss: 0.12778951724370322 Validation Loss: 1.0443589687347412\n",
      "Epoch 7279: Training Loss: 0.12850811829169592 Validation Loss: 1.0440400838851929\n",
      "Epoch 7280: Training Loss: 0.12777685622374216 Validation Loss: 1.044163703918457\n",
      "Epoch 7281: Training Loss: 0.12809414913256964 Validation Loss: 1.0434892177581787\n",
      "Epoch 7282: Training Loss: 0.12809034436941147 Validation Loss: 1.0433429479599\n",
      "Epoch 7283: Training Loss: 0.12800520161787668 Validation Loss: 1.0438224077224731\n",
      "Epoch 7284: Training Loss: 0.12812347461779913 Validation Loss: 1.043967843055725\n",
      "Epoch 7285: Training Loss: 0.12897253533204397 Validation Loss: 1.0447696447372437\n",
      "Epoch 7286: Training Loss: 0.12800122052431107 Validation Loss: 1.044472098350525\n",
      "Epoch 7287: Training Loss: 0.12812937051057816 Validation Loss: 1.0442383289337158\n",
      "Epoch 7288: Training Loss: 0.12821967899799347 Validation Loss: 1.0443115234375\n",
      "Epoch 7289: Training Loss: 0.1280935655037562 Validation Loss: 1.0433050394058228\n",
      "Epoch 7290: Training Loss: 0.1277835617462794 Validation Loss: 1.0429877042770386\n",
      "Epoch 7291: Training Loss: 0.12822499871253967 Validation Loss: 1.0434842109680176\n",
      "Epoch 7292: Training Loss: 0.12757801016171774 Validation Loss: 1.0429424047470093\n",
      "Epoch 7293: Training Loss: 0.12794115394353867 Validation Loss: 1.0441681146621704\n",
      "Epoch 7294: Training Loss: 0.12827959408362707 Validation Loss: 1.04472017288208\n",
      "Epoch 7295: Training Loss: 0.12697604795296988 Validation Loss: 1.0446380376815796\n",
      "Epoch 7296: Training Loss: 0.12784073253472647 Validation Loss: 1.0446133613586426\n",
      "Epoch 7297: Training Loss: 0.1277981624007225 Validation Loss: 1.0442496538162231\n",
      "Epoch 7298: Training Loss: 0.1279994621872902 Validation Loss: 1.0444658994674683\n",
      "Epoch 7299: Training Loss: 0.12767301251490912 Validation Loss: 1.044025182723999\n",
      "Epoch 7300: Training Loss: 0.1281863898038864 Validation Loss: 1.0432649850845337\n",
      "Epoch 7301: Training Loss: 0.12781367699305216 Validation Loss: 1.0432178974151611\n",
      "Epoch 7302: Training Loss: 0.12777731070915857 Validation Loss: 1.0444389581680298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7303: Training Loss: 0.12784704566001892 Validation Loss: 1.0446724891662598\n",
      "Epoch 7304: Training Loss: 0.12761501719554266 Validation Loss: 1.0447041988372803\n",
      "Epoch 7305: Training Loss: 0.12794549514849982 Validation Loss: 1.0447309017181396\n",
      "Epoch 7306: Training Loss: 0.12796974182128906 Validation Loss: 1.0443224906921387\n",
      "Epoch 7307: Training Loss: 0.1270716960231463 Validation Loss: 1.0441981554031372\n",
      "Epoch 7308: Training Loss: 0.1274754504362742 Validation Loss: 1.044351577758789\n",
      "Epoch 7309: Training Loss: 0.1277936523159345 Validation Loss: 1.0437687635421753\n",
      "Epoch 7310: Training Loss: 0.1283226137359937 Validation Loss: 1.0440011024475098\n",
      "Epoch 7311: Training Loss: 0.12763757755359015 Validation Loss: 1.043925166130066\n",
      "Epoch 7312: Training Loss: 0.12739305198192596 Validation Loss: 1.0446462631225586\n",
      "Epoch 7313: Training Loss: 0.127912275493145 Validation Loss: 1.0443445444107056\n",
      "Epoch 7314: Training Loss: 0.12744557112455368 Validation Loss: 1.0434547662734985\n",
      "Epoch 7315: Training Loss: 0.12756924827893576 Validation Loss: 1.044113039970398\n",
      "Epoch 7316: Training Loss: 0.12757206708192825 Validation Loss: 1.0444988012313843\n",
      "Epoch 7317: Training Loss: 0.12841862688461939 Validation Loss: 1.045078158378601\n",
      "Epoch 7318: Training Loss: 0.12776114791631699 Validation Loss: 1.0455855131149292\n",
      "Epoch 7319: Training Loss: 0.12805111209551492 Validation Loss: 1.0448940992355347\n",
      "Epoch 7320: Training Loss: 0.12685865412155786 Validation Loss: 1.0446685552597046\n",
      "Epoch 7321: Training Loss: 0.12783054014046988 Validation Loss: 1.044488787651062\n",
      "Epoch 7322: Training Loss: 0.12757182866334915 Validation Loss: 1.0447441339492798\n",
      "Epoch 7323: Training Loss: 0.12749315798282623 Validation Loss: 1.0449601411819458\n",
      "Epoch 7324: Training Loss: 0.12801064302523932 Validation Loss: 1.0454680919647217\n",
      "Epoch 7325: Training Loss: 0.1272915651400884 Validation Loss: 1.0452858209609985\n",
      "Epoch 7326: Training Loss: 0.12731853872537613 Validation Loss: 1.0446542501449585\n",
      "Epoch 7327: Training Loss: 0.12774568051099777 Validation Loss: 1.0443673133850098\n",
      "Epoch 7328: Training Loss: 0.12732208023468652 Validation Loss: 1.044463038444519\n",
      "Epoch 7329: Training Loss: 0.12729443609714508 Validation Loss: 1.044321894645691\n",
      "Epoch 7330: Training Loss: 0.12762742737929025 Validation Loss: 1.0437822341918945\n",
      "Epoch 7331: Training Loss: 0.1273930643995603 Validation Loss: 1.044594645500183\n",
      "Epoch 7332: Training Loss: 0.1275013660391172 Validation Loss: 1.044837236404419\n",
      "Epoch 7333: Training Loss: 0.12727597852547964 Validation Loss: 1.044515609741211\n",
      "Epoch 7334: Training Loss: 0.12752753992875418 Validation Loss: 1.0445891618728638\n",
      "Epoch 7335: Training Loss: 0.1270838181177775 Validation Loss: 1.0449645519256592\n",
      "Epoch 7336: Training Loss: 0.127666637301445 Validation Loss: 1.0453150272369385\n",
      "Epoch 7337: Training Loss: 0.1275010754664739 Validation Loss: 1.0458152294158936\n",
      "Epoch 7338: Training Loss: 0.12745756655931473 Validation Loss: 1.045835018157959\n",
      "Epoch 7339: Training Loss: 0.12719431519508362 Validation Loss: 1.0459516048431396\n",
      "Epoch 7340: Training Loss: 0.12725307792425156 Validation Loss: 1.0466511249542236\n",
      "Epoch 7341: Training Loss: 0.12741793443759283 Validation Loss: 1.0463446378707886\n",
      "Epoch 7342: Training Loss: 0.12731054921944937 Validation Loss: 1.0456669330596924\n",
      "Epoch 7343: Training Loss: 0.1272788867354393 Validation Loss: 1.0454468727111816\n",
      "Epoch 7344: Training Loss: 0.12745769818623862 Validation Loss: 1.0449910163879395\n",
      "Epoch 7345: Training Loss: 0.12728618582089743 Validation Loss: 1.0451903343200684\n",
      "Epoch 7346: Training Loss: 0.12699675808350244 Validation Loss: 1.045339822769165\n",
      "Epoch 7347: Training Loss: 0.12715866168340048 Validation Loss: 1.0456639528274536\n",
      "Epoch 7348: Training Loss: 0.12720782061417898 Validation Loss: 1.0454047918319702\n",
      "Epoch 7349: Training Loss: 0.1273326501250267 Validation Loss: 1.0451726913452148\n",
      "Epoch 7350: Training Loss: 0.12703966349363327 Validation Loss: 1.0451081991195679\n",
      "Epoch 7351: Training Loss: 0.12684640288352966 Validation Loss: 1.0456805229187012\n",
      "Epoch 7352: Training Loss: 0.12733286867539087 Validation Loss: 1.045998454093933\n",
      "Epoch 7353: Training Loss: 0.1265945831934611 Validation Loss: 1.0460981130599976\n",
      "Epoch 7354: Training Loss: 0.12674574553966522 Validation Loss: 1.046262502670288\n",
      "Epoch 7355: Training Loss: 0.1270692547162374 Validation Loss: 1.0465503931045532\n",
      "Epoch 7356: Training Loss: 0.12684943030277887 Validation Loss: 1.046478033065796\n",
      "Epoch 7357: Training Loss: 0.12690197676420212 Validation Loss: 1.045813798904419\n",
      "Epoch 7358: Training Loss: 0.1269584447145462 Validation Loss: 1.0461862087249756\n",
      "Epoch 7359: Training Loss: 0.12789827833573023 Validation Loss: 1.046356201171875\n",
      "Epoch 7360: Training Loss: 0.12698296457529068 Validation Loss: 1.0458656549453735\n",
      "Epoch 7361: Training Loss: 0.12842968106269836 Validation Loss: 1.0457682609558105\n",
      "Epoch 7362: Training Loss: 0.12685181945562363 Validation Loss: 1.0460479259490967\n",
      "Epoch 7363: Training Loss: 0.12685825179020563 Validation Loss: 1.0457475185394287\n",
      "Epoch 7364: Training Loss: 0.12683253983656564 Validation Loss: 1.0460582971572876\n",
      "Epoch 7365: Training Loss: 0.12697881708542505 Validation Loss: 1.0466501712799072\n",
      "Epoch 7366: Training Loss: 0.1266812408963839 Validation Loss: 1.0462591648101807\n",
      "Epoch 7367: Training Loss: 0.12701870252688727 Validation Loss: 1.0465272665023804\n",
      "Epoch 7368: Training Loss: 0.12749741474787393 Validation Loss: 1.046558141708374\n",
      "Epoch 7369: Training Loss: 0.12726502368847528 Validation Loss: 1.0461766719818115\n",
      "Epoch 7370: Training Loss: 0.12699082990487418 Validation Loss: 1.0466140508651733\n",
      "Epoch 7371: Training Loss: 0.12670794626077017 Validation Loss: 1.046644926071167\n",
      "Epoch 7372: Training Loss: 0.12774123499790827 Validation Loss: 1.0464563369750977\n",
      "Epoch 7373: Training Loss: 0.12708157300949097 Validation Loss: 1.0461010932922363\n",
      "Epoch 7374: Training Loss: 0.12668924778699875 Validation Loss: 1.045947790145874\n",
      "Epoch 7375: Training Loss: 0.12723888953526816 Validation Loss: 1.0460548400878906\n",
      "Epoch 7376: Training Loss: 0.12744015455245972 Validation Loss: 1.0457583665847778\n",
      "Epoch 7377: Training Loss: 0.1268528401851654 Validation Loss: 1.0462167263031006\n",
      "Epoch 7378: Training Loss: 0.12699631601572037 Validation Loss: 1.0465248823165894\n",
      "Epoch 7379: Training Loss: 0.1271123563249906 Validation Loss: 1.0466177463531494\n",
      "Epoch 7380: Training Loss: 0.12693514674901962 Validation Loss: 1.046615719795227\n",
      "Epoch 7381: Training Loss: 0.1268578047553698 Validation Loss: 1.0465048551559448\n",
      "Epoch 7382: Training Loss: 0.12607148538033167 Validation Loss: 1.0464648008346558\n",
      "Epoch 7383: Training Loss: 0.12714453786611557 Validation Loss: 1.046863079071045\n",
      "Epoch 7384: Training Loss: 0.12654894590377808 Validation Loss: 1.0476866960525513\n",
      "Epoch 7385: Training Loss: 0.1266147866845131 Validation Loss: 1.047916054725647\n",
      "Epoch 7386: Training Loss: 0.12679274876912436 Validation Loss: 1.0474449396133423\n",
      "Epoch 7387: Training Loss: 0.12704031666119894 Validation Loss: 1.046642541885376\n",
      "Epoch 7388: Training Loss: 0.12626383701960245 Validation Loss: 1.0470213890075684\n",
      "Epoch 7389: Training Loss: 0.12718416502078375 Validation Loss: 1.0461575984954834\n",
      "Epoch 7390: Training Loss: 0.12657951563596725 Validation Loss: 1.0456095933914185\n",
      "Epoch 7391: Training Loss: 0.12660773346821466 Validation Loss: 1.0459781885147095\n",
      "Epoch 7392: Training Loss: 0.12674289445082346 Validation Loss: 1.0457286834716797\n",
      "Epoch 7393: Training Loss: 0.12654977291822433 Validation Loss: 1.0463156700134277\n",
      "Epoch 7394: Training Loss: 0.12643145769834518 Validation Loss: 1.0471625328063965\n",
      "Epoch 7395: Training Loss: 0.12705199420452118 Validation Loss: 1.0471311807632446\n",
      "Epoch 7396: Training Loss: 0.12703244388103485 Validation Loss: 1.0476776361465454\n",
      "Epoch 7397: Training Loss: 0.12634386122226715 Validation Loss: 1.0478529930114746\n",
      "Epoch 7398: Training Loss: 0.12625128527482352 Validation Loss: 1.0475778579711914\n",
      "Epoch 7399: Training Loss: 0.1265764832496643 Validation Loss: 1.047484040260315\n",
      "Epoch 7400: Training Loss: 0.12625343600908914 Validation Loss: 1.0466814041137695\n",
      "Epoch 7401: Training Loss: 0.12724501142899194 Validation Loss: 1.0468467473983765\n",
      "Epoch 7402: Training Loss: 0.12643354137738547 Validation Loss: 1.04712975025177\n",
      "Epoch 7403: Training Loss: 0.12628260751565298 Validation Loss: 1.0461242198944092\n",
      "Epoch 7404: Training Loss: 0.12635033329327902 Validation Loss: 1.0468525886535645\n",
      "Epoch 7405: Training Loss: 0.1258243297537168 Validation Loss: 1.046826958656311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7406: Training Loss: 0.12641308705012003 Validation Loss: 1.0473809242248535\n",
      "Epoch 7407: Training Loss: 0.12598690887292227 Validation Loss: 1.0475767850875854\n",
      "Epoch 7408: Training Loss: 0.1263738845785459 Validation Loss: 1.0478835105895996\n",
      "Epoch 7409: Training Loss: 0.1261151358485222 Validation Loss: 1.0483741760253906\n",
      "Epoch 7410: Training Loss: 0.1262842838962873 Validation Loss: 1.0477027893066406\n",
      "Epoch 7411: Training Loss: 0.12616287171840668 Validation Loss: 1.0475536584854126\n",
      "Epoch 7412: Training Loss: 0.12656081964572272 Validation Loss: 1.0473055839538574\n",
      "Epoch 7413: Training Loss: 0.12610778212547302 Validation Loss: 1.0469669103622437\n",
      "Epoch 7414: Training Loss: 0.12674600630998611 Validation Loss: 1.0470088720321655\n",
      "Epoch 7415: Training Loss: 0.12606042871872583 Validation Loss: 1.04705810546875\n",
      "Epoch 7416: Training Loss: 0.1259183312455813 Validation Loss: 1.0472850799560547\n",
      "Epoch 7417: Training Loss: 0.1259453942378362 Validation Loss: 1.0474716424942017\n",
      "Epoch 7418: Training Loss: 0.12640846520662308 Validation Loss: 1.0477057695388794\n",
      "Epoch 7419: Training Loss: 0.1262229656179746 Validation Loss: 1.0478616952896118\n",
      "Epoch 7420: Training Loss: 0.12568816294272742 Validation Loss: 1.0477979183197021\n",
      "Epoch 7421: Training Loss: 0.1268129050731659 Validation Loss: 1.0475997924804688\n",
      "Epoch 7422: Training Loss: 0.12587217489878336 Validation Loss: 1.0478332042694092\n",
      "Epoch 7423: Training Loss: 0.1261462147037188 Validation Loss: 1.048496961593628\n",
      "Epoch 7424: Training Loss: 0.12629217157761255 Validation Loss: 1.048041820526123\n",
      "Epoch 7425: Training Loss: 0.1258981997768084 Validation Loss: 1.0478154420852661\n",
      "Epoch 7426: Training Loss: 0.12745638191699982 Validation Loss: 1.0471960306167603\n",
      "Epoch 7427: Training Loss: 0.12596467385689417 Validation Loss: 1.0473008155822754\n",
      "Epoch 7428: Training Loss: 0.1262428934375445 Validation Loss: 1.0474567413330078\n",
      "Epoch 7429: Training Loss: 0.12593444188435873 Validation Loss: 1.0480889081954956\n",
      "Epoch 7430: Training Loss: 0.125909594198068 Validation Loss: 1.0486457347869873\n",
      "Epoch 7431: Training Loss: 0.1260883907477061 Validation Loss: 1.0483181476593018\n",
      "Epoch 7432: Training Loss: 0.1268471802274386 Validation Loss: 1.0479170083999634\n",
      "Epoch 7433: Training Loss: 0.1261711617310842 Validation Loss: 1.0475929975509644\n",
      "Epoch 7434: Training Loss: 0.12577975541353226 Validation Loss: 1.0472438335418701\n",
      "Epoch 7435: Training Loss: 0.12553752958774567 Validation Loss: 1.047610878944397\n",
      "Epoch 7436: Training Loss: 0.12563528617223105 Validation Loss: 1.0482161045074463\n",
      "Epoch 7437: Training Loss: 0.12584631890058517 Validation Loss: 1.0492407083511353\n",
      "Epoch 7438: Training Loss: 0.12574357291062674 Validation Loss: 1.0496879816055298\n",
      "Epoch 7439: Training Loss: 0.1258103201786677 Validation Loss: 1.0494242906570435\n",
      "Epoch 7440: Training Loss: 0.12579823533693948 Validation Loss: 1.0484288930892944\n",
      "Epoch 7441: Training Loss: 0.12537775685389838 Validation Loss: 1.0483676195144653\n",
      "Epoch 7442: Training Loss: 0.12580865621566772 Validation Loss: 1.048101544380188\n",
      "Epoch 7443: Training Loss: 0.12575448056062064 Validation Loss: 1.04819655418396\n",
      "Epoch 7444: Training Loss: 0.1257806271314621 Validation Loss: 1.0487818717956543\n",
      "Epoch 7445: Training Loss: 0.1261535088221232 Validation Loss: 1.0492632389068604\n",
      "Epoch 7446: Training Loss: 0.1255959396560987 Validation Loss: 1.0490092039108276\n",
      "Epoch 7447: Training Loss: 0.12561329454183578 Validation Loss: 1.0486812591552734\n",
      "Epoch 7448: Training Loss: 0.12531725068887076 Validation Loss: 1.0486161708831787\n",
      "Epoch 7449: Training Loss: 0.126717838148276 Validation Loss: 1.0483283996582031\n",
      "Epoch 7450: Training Loss: 0.12617044399182 Validation Loss: 1.048706293106079\n",
      "Epoch 7451: Training Loss: 0.12638657788435617 Validation Loss: 1.048933506011963\n",
      "Epoch 7452: Training Loss: 0.12571392705043158 Validation Loss: 1.0496881008148193\n",
      "Epoch 7453: Training Loss: 0.12540384630362192 Validation Loss: 1.0499743223190308\n",
      "Epoch 7454: Training Loss: 0.12556189050277075 Validation Loss: 1.0486472845077515\n",
      "Epoch 7455: Training Loss: 0.12540818999210993 Validation Loss: 1.0485427379608154\n",
      "Epoch 7456: Training Loss: 0.12614721804857254 Validation Loss: 1.0487830638885498\n",
      "Epoch 7457: Training Loss: 0.12544077883164087 Validation Loss: 1.0484883785247803\n",
      "Epoch 7458: Training Loss: 0.12655875583489737 Validation Loss: 1.0492196083068848\n",
      "Epoch 7459: Training Loss: 0.1258349890510241 Validation Loss: 1.050100564956665\n",
      "Epoch 7460: Training Loss: 0.12512305130561194 Validation Loss: 1.0499452352523804\n",
      "Epoch 7461: Training Loss: 0.12530162185430527 Validation Loss: 1.0497972965240479\n",
      "Epoch 7462: Training Loss: 0.12600246320168176 Validation Loss: 1.049416422843933\n",
      "Epoch 7463: Training Loss: 0.1265589396158854 Validation Loss: 1.049505591392517\n",
      "Epoch 7464: Training Loss: 0.12576310336589813 Validation Loss: 1.049761176109314\n",
      "Epoch 7465: Training Loss: 0.1259504755338033 Validation Loss: 1.0501446723937988\n",
      "Epoch 7466: Training Loss: 0.12512298425038657 Validation Loss: 1.0499168634414673\n",
      "Epoch 7467: Training Loss: 0.1251298983891805 Validation Loss: 1.0499531030654907\n",
      "Epoch 7468: Training Loss: 0.1248946264386177 Validation Loss: 1.0500460863113403\n",
      "Epoch 7469: Training Loss: 0.12570155411958694 Validation Loss: 1.0496177673339844\n",
      "Epoch 7470: Training Loss: 0.12508116165796915 Validation Loss: 1.0497770309448242\n",
      "Epoch 7471: Training Loss: 0.12535816431045532 Validation Loss: 1.048951506614685\n",
      "Epoch 7472: Training Loss: 0.12550225853919983 Validation Loss: 1.0487639904022217\n",
      "Epoch 7473: Training Loss: 0.12518450866142908 Validation Loss: 1.0497663021087646\n",
      "Epoch 7474: Training Loss: 0.12494421501954396 Validation Loss: 1.0503567457199097\n",
      "Epoch 7475: Training Loss: 0.12470185508330663 Validation Loss: 1.0501149892807007\n",
      "Epoch 7476: Training Loss: 0.12548873325188956 Validation Loss: 1.0499241352081299\n",
      "Epoch 7477: Training Loss: 0.12532206376393637 Validation Loss: 1.0501736402511597\n",
      "Epoch 7478: Training Loss: 0.12540438771247864 Validation Loss: 1.0496402978897095\n",
      "Epoch 7479: Training Loss: 0.12538810074329376 Validation Loss: 1.048022985458374\n",
      "Epoch 7480: Training Loss: 0.124879685540994 Validation Loss: 1.048696517944336\n",
      "Epoch 7481: Training Loss: 0.12559964756170908 Validation Loss: 1.0497658252716064\n",
      "Epoch 7482: Training Loss: 0.12479520589113235 Validation Loss: 1.0495805740356445\n",
      "Epoch 7483: Training Loss: 0.1254939486583074 Validation Loss: 1.049758791923523\n",
      "Epoch 7484: Training Loss: 0.12518846988677979 Validation Loss: 1.0489702224731445\n",
      "Epoch 7485: Training Loss: 0.12502208600441614 Validation Loss: 1.048697590827942\n",
      "Epoch 7486: Training Loss: 0.12554542471965155 Validation Loss: 1.0485886335372925\n",
      "Epoch 7487: Training Loss: 0.12517941743135452 Validation Loss: 1.0485899448394775\n",
      "Epoch 7488: Training Loss: 0.1260390281677246 Validation Loss: 1.0496987104415894\n",
      "Epoch 7489: Training Loss: 0.1250728170077006 Validation Loss: 1.0500588417053223\n",
      "Epoch 7490: Training Loss: 0.12492721279462178 Validation Loss: 1.0511841773986816\n",
      "Epoch 7491: Training Loss: 0.12504002700249353 Validation Loss: 1.0502336025238037\n",
      "Epoch 7492: Training Loss: 0.1252019206682841 Validation Loss: 1.0500248670578003\n",
      "Epoch 7493: Training Loss: 0.12501685321331024 Validation Loss: 1.049896240234375\n",
      "Epoch 7494: Training Loss: 0.12474211553732555 Validation Loss: 1.0499211549758911\n",
      "Epoch 7495: Training Loss: 0.12633158763249716 Validation Loss: 1.049428939819336\n",
      "Epoch 7496: Training Loss: 0.1249367247025172 Validation Loss: 1.0501576662063599\n",
      "Epoch 7497: Training Loss: 0.1251843422651291 Validation Loss: 1.051249623298645\n",
      "Epoch 7498: Training Loss: 0.1250995546579361 Validation Loss: 1.0513384342193604\n",
      "Epoch 7499: Training Loss: 0.12494320422410965 Validation Loss: 1.0510441064834595\n",
      "Epoch 7500: Training Loss: 0.12487799177567165 Validation Loss: 1.0494199991226196\n",
      "Epoch 7501: Training Loss: 0.1256110668182373 Validation Loss: 1.0492652654647827\n",
      "Epoch 7502: Training Loss: 0.12530275682608286 Validation Loss: 1.04951810836792\n",
      "Epoch 7503: Training Loss: 0.12514124810695648 Validation Loss: 1.0503814220428467\n",
      "Epoch 7504: Training Loss: 0.12502474834521612 Validation Loss: 1.0506523847579956\n",
      "Epoch 7505: Training Loss: 0.12515799204508463 Validation Loss: 1.0507934093475342\n",
      "Epoch 7506: Training Loss: 0.12532094617684683 Validation Loss: 1.0504767894744873\n",
      "Epoch 7507: Training Loss: 0.12481201440095901 Validation Loss: 1.0505030155181885\n",
      "Epoch 7508: Training Loss: 0.12523938218752542 Validation Loss: 1.0502787828445435\n",
      "Epoch 7509: Training Loss: 0.12500339498122534 Validation Loss: 1.0497064590454102\n",
      "Epoch 7510: Training Loss: 0.12452722589174907 Validation Loss: 1.0494632720947266\n",
      "Epoch 7511: Training Loss: 0.12475922952095668 Validation Loss: 1.0503458976745605\n",
      "Epoch 7512: Training Loss: 0.1247312327226003 Validation Loss: 1.05159330368042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7513: Training Loss: 0.12471528102954228 Validation Loss: 1.0509936809539795\n",
      "Epoch 7514: Training Loss: 0.12479430188735326 Validation Loss: 1.0511977672576904\n",
      "Epoch 7515: Training Loss: 0.12473302831252416 Validation Loss: 1.050870418548584\n",
      "Epoch 7516: Training Loss: 0.12499237805604935 Validation Loss: 1.0508794784545898\n",
      "Epoch 7517: Training Loss: 0.1246960312128067 Validation Loss: 1.0515345335006714\n",
      "Epoch 7518: Training Loss: 0.12470955650011699 Validation Loss: 1.0513979196548462\n",
      "Epoch 7519: Training Loss: 0.12486453851064046 Validation Loss: 1.0505443811416626\n",
      "Epoch 7520: Training Loss: 0.1246132751305898 Validation Loss: 1.0499861240386963\n",
      "Epoch 7521: Training Loss: 0.12467678139607112 Validation Loss: 1.0507525205612183\n",
      "Epoch 7522: Training Loss: 0.12455252806345622 Validation Loss: 1.0508562326431274\n",
      "Epoch 7523: Training Loss: 0.1252704660097758 Validation Loss: 1.051472544670105\n",
      "Epoch 7524: Training Loss: 0.1247022698322932 Validation Loss: 1.051401972770691\n",
      "Epoch 7525: Training Loss: 0.1245806689063708 Validation Loss: 1.051544189453125\n",
      "Epoch 7526: Training Loss: 0.12469050536553065 Validation Loss: 1.0519819259643555\n",
      "Epoch 7527: Training Loss: 0.1245851218700409 Validation Loss: 1.0516011714935303\n",
      "Epoch 7528: Training Loss: 0.12465845296780269 Validation Loss: 1.0519176721572876\n",
      "Epoch 7529: Training Loss: 0.1242263913154602 Validation Loss: 1.0515927076339722\n",
      "Epoch 7530: Training Loss: 0.12469245741764705 Validation Loss: 1.0507646799087524\n",
      "Epoch 7531: Training Loss: 0.1242281620701154 Validation Loss: 1.0512678623199463\n",
      "Epoch 7532: Training Loss: 0.12478861212730408 Validation Loss: 1.050824522972107\n",
      "Epoch 7533: Training Loss: 0.12438978751500447 Validation Loss: 1.0504120588302612\n",
      "Epoch 7534: Training Loss: 0.12460714081923167 Validation Loss: 1.0517765283584595\n",
      "Epoch 7535: Training Loss: 0.12509580701589584 Validation Loss: 1.0519603490829468\n",
      "Epoch 7536: Training Loss: 0.12483765681584676 Validation Loss: 1.051139235496521\n",
      "Epoch 7537: Training Loss: 0.12441310038169225 Validation Loss: 1.050440788269043\n",
      "Epoch 7538: Training Loss: 0.12421098351478577 Validation Loss: 1.051262378692627\n",
      "Epoch 7539: Training Loss: 0.1243972157438596 Validation Loss: 1.0516152381896973\n",
      "Epoch 7540: Training Loss: 0.12437777469555537 Validation Loss: 1.0511608123779297\n",
      "Epoch 7541: Training Loss: 0.12420868873596191 Validation Loss: 1.0511075258255005\n",
      "Epoch 7542: Training Loss: 0.12417489538590114 Validation Loss: 1.051118016242981\n",
      "Epoch 7543: Training Loss: 0.12433656056722005 Validation Loss: 1.0518546104431152\n",
      "Epoch 7544: Training Loss: 0.12456634640693665 Validation Loss: 1.052353024482727\n",
      "Epoch 7545: Training Loss: 0.1241100902358691 Validation Loss: 1.0529389381408691\n",
      "Epoch 7546: Training Loss: 0.12466107308864594 Validation Loss: 1.053324580192566\n",
      "Epoch 7547: Training Loss: 0.12433793892463048 Validation Loss: 1.0524065494537354\n",
      "Epoch 7548: Training Loss: 0.1242380365729332 Validation Loss: 1.0527573823928833\n",
      "Epoch 7549: Training Loss: 0.12464718520641327 Validation Loss: 1.0518152713775635\n",
      "Epoch 7550: Training Loss: 0.12479042013486226 Validation Loss: 1.0518254041671753\n",
      "Epoch 7551: Training Loss: 0.12458579738934834 Validation Loss: 1.0515114068984985\n",
      "Epoch 7552: Training Loss: 0.12444277107715607 Validation Loss: 1.0514190196990967\n",
      "Epoch 7553: Training Loss: 0.12411525845527649 Validation Loss: 1.051583170890808\n",
      "Epoch 7554: Training Loss: 0.12402495741844177 Validation Loss: 1.052136778831482\n",
      "Epoch 7555: Training Loss: 0.1238309492667516 Validation Loss: 1.0520689487457275\n",
      "Epoch 7556: Training Loss: 0.12410142024358113 Validation Loss: 1.0515117645263672\n",
      "Epoch 7557: Training Loss: 0.1249229833483696 Validation Loss: 1.0518587827682495\n",
      "Epoch 7558: Training Loss: 0.12396630893150966 Validation Loss: 1.051749587059021\n",
      "Epoch 7559: Training Loss: 0.12392356246709824 Validation Loss: 1.0523179769515991\n",
      "Epoch 7560: Training Loss: 0.12533817191918692 Validation Loss: 1.051938533782959\n",
      "Epoch 7561: Training Loss: 0.12506725390752158 Validation Loss: 1.0519862174987793\n",
      "Epoch 7562: Training Loss: 0.12373791386683781 Validation Loss: 1.0526057481765747\n",
      "Epoch 7563: Training Loss: 0.12497360507647197 Validation Loss: 1.05254065990448\n",
      "Epoch 7564: Training Loss: 0.1247499684492747 Validation Loss: 1.0523145198822021\n",
      "Epoch 7565: Training Loss: 0.12381243705749512 Validation Loss: 1.0523611307144165\n",
      "Epoch 7566: Training Loss: 0.12394880751768748 Validation Loss: 1.0525792837142944\n",
      "Epoch 7567: Training Loss: 0.12398970872163773 Validation Loss: 1.0528655052185059\n",
      "Epoch 7568: Training Loss: 0.12430156767368317 Validation Loss: 1.051722764968872\n",
      "Epoch 7569: Training Loss: 0.12451009452342987 Validation Loss: 1.0517702102661133\n",
      "Epoch 7570: Training Loss: 0.1239145000775655 Validation Loss: 1.052150011062622\n",
      "Epoch 7571: Training Loss: 0.1238867590824763 Validation Loss: 1.052098035812378\n",
      "Epoch 7572: Training Loss: 0.12391771624485652 Validation Loss: 1.0519580841064453\n",
      "Epoch 7573: Training Loss: 0.12376399586598079 Validation Loss: 1.052320957183838\n",
      "Epoch 7574: Training Loss: 0.12372338771820068 Validation Loss: 1.0521953105926514\n",
      "Epoch 7575: Training Loss: 0.12373137225707372 Validation Loss: 1.0523607730865479\n",
      "Epoch 7576: Training Loss: 0.12382824222246806 Validation Loss: 1.0523900985717773\n",
      "Epoch 7577: Training Loss: 0.12411268303791682 Validation Loss: 1.0527215003967285\n",
      "Epoch 7578: Training Loss: 0.12383193025986354 Validation Loss: 1.0536935329437256\n",
      "Epoch 7579: Training Loss: 0.12389792253573735 Validation Loss: 1.0538384914398193\n",
      "Epoch 7580: Training Loss: 0.12416934221982956 Validation Loss: 1.0526578426361084\n",
      "Epoch 7581: Training Loss: 0.1237056627869606 Validation Loss: 1.0528607368469238\n",
      "Epoch 7582: Training Loss: 0.12371551990509033 Validation Loss: 1.0529893636703491\n",
      "Epoch 7583: Training Loss: 0.1235034391283989 Validation Loss: 1.053218960762024\n",
      "Epoch 7584: Training Loss: 0.12379126499096553 Validation Loss: 1.0525492429733276\n",
      "Epoch 7585: Training Loss: 0.12367713699738185 Validation Loss: 1.0529510974884033\n",
      "Epoch 7586: Training Loss: 0.12359897047281265 Validation Loss: 1.0519850254058838\n",
      "Epoch 7587: Training Loss: 0.12429385880629222 Validation Loss: 1.052050232887268\n",
      "Epoch 7588: Training Loss: 0.12382631252209346 Validation Loss: 1.0528249740600586\n",
      "Epoch 7589: Training Loss: 0.12360070149103801 Validation Loss: 1.0538854598999023\n",
      "Epoch 7590: Training Loss: 0.12373484919468562 Validation Loss: 1.053531527519226\n",
      "Epoch 7591: Training Loss: 0.1232530027627945 Validation Loss: 1.05303955078125\n",
      "Epoch 7592: Training Loss: 0.12339745710293452 Validation Loss: 1.052311658859253\n",
      "Epoch 7593: Training Loss: 0.12346453219652176 Validation Loss: 1.0525113344192505\n",
      "Epoch 7594: Training Loss: 0.123814490934213 Validation Loss: 1.0531729459762573\n",
      "Epoch 7595: Training Loss: 0.12337090820074081 Validation Loss: 1.052182912826538\n",
      "Epoch 7596: Training Loss: 0.12349158277114232 Validation Loss: 1.052166223526001\n",
      "Epoch 7597: Training Loss: 0.12337237348159154 Validation Loss: 1.052193284034729\n",
      "Epoch 7598: Training Loss: 0.1238787720600764 Validation Loss: 1.0529221296310425\n",
      "Epoch 7599: Training Loss: 0.12400640298922856 Validation Loss: 1.0534428358078003\n",
      "Epoch 7600: Training Loss: 0.12433021018902461 Validation Loss: 1.0530849695205688\n",
      "Epoch 7601: Training Loss: 0.12344362835089366 Validation Loss: 1.0527129173278809\n",
      "Epoch 7602: Training Loss: 0.12361282110214233 Validation Loss: 1.052852749824524\n",
      "Epoch 7603: Training Loss: 0.12387190014123917 Validation Loss: 1.0536903142929077\n",
      "Epoch 7604: Training Loss: 0.12374106049537659 Validation Loss: 1.0538259744644165\n",
      "Epoch 7605: Training Loss: 0.12376662095387776 Validation Loss: 1.0540691614151\n",
      "Epoch 7606: Training Loss: 0.12365349878867467 Validation Loss: 1.0547693967819214\n",
      "Epoch 7607: Training Loss: 0.12342252085606258 Validation Loss: 1.0551339387893677\n",
      "Epoch 7608: Training Loss: 0.1239592507481575 Validation Loss: 1.0544723272323608\n",
      "Epoch 7609: Training Loss: 0.12338301539421082 Validation Loss: 1.053719162940979\n",
      "Epoch 7610: Training Loss: 0.12339546779791515 Validation Loss: 1.0535533428192139\n",
      "Epoch 7611: Training Loss: 0.123159259557724 Validation Loss: 1.0534276962280273\n",
      "Epoch 7612: Training Loss: 0.1235213428735733 Validation Loss: 1.0534205436706543\n",
      "Epoch 7613: Training Loss: 0.12325094391902287 Validation Loss: 1.054364800453186\n",
      "Epoch 7614: Training Loss: 0.12384939690430959 Validation Loss: 1.0546234846115112\n",
      "Epoch 7615: Training Loss: 0.12353350718816121 Validation Loss: 1.0545071363449097\n",
      "Epoch 7616: Training Loss: 0.12353343268235524 Validation Loss: 1.0535532236099243\n",
      "Epoch 7617: Training Loss: 0.12337561945120494 Validation Loss: 1.0534290075302124\n",
      "Epoch 7618: Training Loss: 0.12356607119242351 Validation Loss: 1.0533967018127441\n",
      "Epoch 7619: Training Loss: 0.1232752154270808 Validation Loss: 1.0532894134521484\n",
      "Epoch 7620: Training Loss: 0.12300994743903478 Validation Loss: 1.053347110748291\n",
      "Epoch 7621: Training Loss: 0.12323667357365291 Validation Loss: 1.0531635284423828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7622: Training Loss: 0.1234878624478976 Validation Loss: 1.0537954568862915\n",
      "Epoch 7623: Training Loss: 0.12299938996632893 Validation Loss: 1.054170846939087\n",
      "Epoch 7624: Training Loss: 0.12294862419366837 Validation Loss: 1.054263710975647\n",
      "Epoch 7625: Training Loss: 0.12337226172288258 Validation Loss: 1.0546106100082397\n",
      "Epoch 7626: Training Loss: 0.1242938016851743 Validation Loss: 1.0541061162948608\n",
      "Epoch 7627: Training Loss: 0.12286464869976044 Validation Loss: 1.0543556213378906\n",
      "Epoch 7628: Training Loss: 0.12337592740853627 Validation Loss: 1.05439293384552\n",
      "Epoch 7629: Training Loss: 0.12296098967393239 Validation Loss: 1.0548298358917236\n",
      "Epoch 7630: Training Loss: 0.12310290336608887 Validation Loss: 1.0548754930496216\n",
      "Epoch 7631: Training Loss: 0.12285520384709041 Validation Loss: 1.0543285608291626\n",
      "Epoch 7632: Training Loss: 0.12368213633696239 Validation Loss: 1.0545234680175781\n",
      "Epoch 7633: Training Loss: 0.1236439620455106 Validation Loss: 1.0541051626205444\n",
      "Epoch 7634: Training Loss: 0.12330878525972366 Validation Loss: 1.0545083284378052\n",
      "Epoch 7635: Training Loss: 0.12308577448129654 Validation Loss: 1.0549182891845703\n",
      "Epoch 7636: Training Loss: 0.12261983007192612 Validation Loss: 1.0544536113739014\n",
      "Epoch 7637: Training Loss: 0.12298447887102763 Validation Loss: 1.0541306734085083\n",
      "Epoch 7638: Training Loss: 0.12312124172846477 Validation Loss: 1.0534523725509644\n",
      "Epoch 7639: Training Loss: 0.12380209316809972 Validation Loss: 1.0538179874420166\n",
      "Epoch 7640: Training Loss: 0.12284208834171295 Validation Loss: 1.0536768436431885\n",
      "Epoch 7641: Training Loss: 0.12406474848588307 Validation Loss: 1.054085612297058\n",
      "Epoch 7642: Training Loss: 0.12279415378967921 Validation Loss: 1.0546441078186035\n",
      "Epoch 7643: Training Loss: 0.12327400594949722 Validation Loss: 1.0547503232955933\n",
      "Epoch 7644: Training Loss: 0.1225963830947876 Validation Loss: 1.0555020570755005\n",
      "Epoch 7645: Training Loss: 0.12406601011753082 Validation Loss: 1.0565630197525024\n",
      "Epoch 7646: Training Loss: 0.12359291811784108 Validation Loss: 1.057247281074524\n",
      "Epoch 7647: Training Loss: 0.12323251863320668 Validation Loss: 1.0563832521438599\n",
      "Epoch 7648: Training Loss: 0.12325303753217061 Validation Loss: 1.0553255081176758\n",
      "Epoch 7649: Training Loss: 0.12315696229537328 Validation Loss: 1.0539826154708862\n",
      "Epoch 7650: Training Loss: 0.12284111479918162 Validation Loss: 1.0546265840530396\n",
      "Epoch 7651: Training Loss: 0.12304804225762685 Validation Loss: 1.054858922958374\n",
      "Epoch 7652: Training Loss: 0.1228996217250824 Validation Loss: 1.0549126863479614\n",
      "Epoch 7653: Training Loss: 0.12273438026507695 Validation Loss: 1.0550835132598877\n",
      "Epoch 7654: Training Loss: 0.12315927942593892 Validation Loss: 1.0552042722702026\n",
      "Epoch 7655: Training Loss: 0.12287239730358124 Validation Loss: 1.0555766820907593\n",
      "Epoch 7656: Training Loss: 0.12330299119154613 Validation Loss: 1.0550435781478882\n",
      "Epoch 7657: Training Loss: 0.12276056905587514 Validation Loss: 1.053911566734314\n",
      "Epoch 7658: Training Loss: 0.12350512792666753 Validation Loss: 1.053794503211975\n",
      "Epoch 7659: Training Loss: 0.12280438095331192 Validation Loss: 1.0544894933700562\n",
      "Epoch 7660: Training Loss: 0.12385129928588867 Validation Loss: 1.0557937622070312\n",
      "Epoch 7661: Training Loss: 0.12295068055391312 Validation Loss: 1.055586814880371\n",
      "Epoch 7662: Training Loss: 0.12360803782939911 Validation Loss: 1.0557798147201538\n",
      "Epoch 7663: Training Loss: 0.12285112589597702 Validation Loss: 1.0555700063705444\n",
      "Epoch 7664: Training Loss: 0.12280080219109853 Validation Loss: 1.0547373294830322\n",
      "Epoch 7665: Training Loss: 0.12240466723839442 Validation Loss: 1.0552386045455933\n",
      "Epoch 7666: Training Loss: 0.12262618541717529 Validation Loss: 1.0552380084991455\n",
      "Epoch 7667: Training Loss: 0.12256018569072087 Validation Loss: 1.05515718460083\n",
      "Epoch 7668: Training Loss: 0.12267215798298518 Validation Loss: 1.0559241771697998\n",
      "Epoch 7669: Training Loss: 0.12255566318829854 Validation Loss: 1.0566530227661133\n",
      "Epoch 7670: Training Loss: 0.12320952365795772 Validation Loss: 1.0577199459075928\n",
      "Epoch 7671: Training Loss: 0.12288745741049449 Validation Loss: 1.0575042963027954\n",
      "Epoch 7672: Training Loss: 0.12268539766470592 Validation Loss: 1.0558240413665771\n",
      "Epoch 7673: Training Loss: 0.12236085037390391 Validation Loss: 1.0546088218688965\n",
      "Epoch 7674: Training Loss: 0.12274698416392009 Validation Loss: 1.0541486740112305\n",
      "Epoch 7675: Training Loss: 0.12238166977961858 Validation Loss: 1.0540072917938232\n",
      "Epoch 7676: Training Loss: 0.12220093111197154 Validation Loss: 1.0545912981033325\n",
      "Epoch 7677: Training Loss: 0.12351284921169281 Validation Loss: 1.0566372871398926\n",
      "Epoch 7678: Training Loss: 0.12287313242753346 Validation Loss: 1.0560591220855713\n",
      "Epoch 7679: Training Loss: 0.12242649743954341 Validation Loss: 1.0565094947814941\n",
      "Epoch 7680: Training Loss: 0.12264735500017802 Validation Loss: 1.056077480316162\n",
      "Epoch 7681: Training Loss: 0.12222622831662495 Validation Loss: 1.0554447174072266\n",
      "Epoch 7682: Training Loss: 0.12296639134486516 Validation Loss: 1.0561639070510864\n",
      "Epoch 7683: Training Loss: 0.12215649088223775 Validation Loss: 1.056557297706604\n",
      "Epoch 7684: Training Loss: 0.1237597440679868 Validation Loss: 1.0559206008911133\n",
      "Epoch 7685: Training Loss: 0.12238844484090805 Validation Loss: 1.0562995672225952\n",
      "Epoch 7686: Training Loss: 0.12236024687687556 Validation Loss: 1.0562852621078491\n",
      "Epoch 7687: Training Loss: 0.12256005158027013 Validation Loss: 1.05580735206604\n",
      "Epoch 7688: Training Loss: 0.12245817979176839 Validation Loss: 1.0551189184188843\n",
      "Epoch 7689: Training Loss: 0.12240342299143474 Validation Loss: 1.0557341575622559\n",
      "Epoch 7690: Training Loss: 0.1223025992512703 Validation Loss: 1.0559501647949219\n",
      "Epoch 7691: Training Loss: 0.12227268268664677 Validation Loss: 1.0566200017929077\n",
      "Epoch 7692: Training Loss: 0.1221534584959348 Validation Loss: 1.0564708709716797\n",
      "Epoch 7693: Training Loss: 0.12184662620226543 Validation Loss: 1.0555074214935303\n",
      "Epoch 7694: Training Loss: 0.12231656660636266 Validation Loss: 1.0555310249328613\n",
      "Epoch 7695: Training Loss: 0.12193494538466136 Validation Loss: 1.0554542541503906\n",
      "Epoch 7696: Training Loss: 0.12194591015577316 Validation Loss: 1.0548598766326904\n",
      "Epoch 7697: Training Loss: 0.12216182798147202 Validation Loss: 1.0558116436004639\n",
      "Epoch 7698: Training Loss: 0.1224593495329221 Validation Loss: 1.0558606386184692\n",
      "Epoch 7699: Training Loss: 0.1220069130261739 Validation Loss: 1.0558634996414185\n",
      "Epoch 7700: Training Loss: 0.122362586359183 Validation Loss: 1.0560084581375122\n",
      "Epoch 7701: Training Loss: 0.1220800851782163 Validation Loss: 1.055565357208252\n",
      "Epoch 7702: Training Loss: 0.12269774327675502 Validation Loss: 1.05617094039917\n",
      "Epoch 7703: Training Loss: 0.12278582155704498 Validation Loss: 1.0559172630310059\n",
      "Epoch 7704: Training Loss: 0.1222957322994868 Validation Loss: 1.0566458702087402\n",
      "Epoch 7705: Training Loss: 0.12203692644834518 Validation Loss: 1.0578422546386719\n",
      "Epoch 7706: Training Loss: 0.12249964227279027 Validation Loss: 1.0580308437347412\n",
      "Epoch 7707: Training Loss: 0.12211874375740688 Validation Loss: 1.0573172569274902\n",
      "Epoch 7708: Training Loss: 0.12240185091892879 Validation Loss: 1.0581185817718506\n",
      "Epoch 7709: Training Loss: 0.12231341501077016 Validation Loss: 1.0575134754180908\n",
      "Epoch 7710: Training Loss: 0.12184041490157445 Validation Loss: 1.0569756031036377\n",
      "Epoch 7711: Training Loss: 0.12233041475216548 Validation Loss: 1.0565308332443237\n",
      "Epoch 7712: Training Loss: 0.12218123426040013 Validation Loss: 1.0560482740402222\n",
      "Epoch 7713: Training Loss: 0.12207326292991638 Validation Loss: 1.0571681261062622\n",
      "Epoch 7714: Training Loss: 0.12215826660394669 Validation Loss: 1.0573272705078125\n",
      "Epoch 7715: Training Loss: 0.122059665620327 Validation Loss: 1.0573041439056396\n",
      "Epoch 7716: Training Loss: 0.12163842966159184 Validation Loss: 1.0567216873168945\n",
      "Epoch 7717: Training Loss: 0.12216528753439586 Validation Loss: 1.0578898191452026\n",
      "Epoch 7718: Training Loss: 0.12316333502531052 Validation Loss: 1.0573132038116455\n",
      "Epoch 7719: Training Loss: 0.12289007008075714 Validation Loss: 1.0570933818817139\n",
      "Epoch 7720: Training Loss: 0.12193526079257329 Validation Loss: 1.0566911697387695\n",
      "Epoch 7721: Training Loss: 0.12270757307608922 Validation Loss: 1.0562493801116943\n",
      "Epoch 7722: Training Loss: 0.12193665901819865 Validation Loss: 1.0569931268692017\n",
      "Epoch 7723: Training Loss: 0.12252250562111537 Validation Loss: 1.0578659772872925\n",
      "Epoch 7724: Training Loss: 0.12211096286773682 Validation Loss: 1.058790922164917\n",
      "Epoch 7725: Training Loss: 0.12217832853396733 Validation Loss: 1.0577868223190308\n",
      "Epoch 7726: Training Loss: 0.12189578264951706 Validation Loss: 1.056704044342041\n",
      "Epoch 7727: Training Loss: 0.12220546354850133 Validation Loss: 1.0561836957931519\n",
      "Epoch 7728: Training Loss: 0.12176670382420222 Validation Loss: 1.0569958686828613\n",
      "Epoch 7729: Training Loss: 0.12186535447835922 Validation Loss: 1.0575369596481323\n",
      "Epoch 7730: Training Loss: 0.12220604717731476 Validation Loss: 1.0574675798416138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7731: Training Loss: 0.12172751128673553 Validation Loss: 1.0573983192443848\n",
      "Epoch 7732: Training Loss: 0.1222562591234843 Validation Loss: 1.0577622652053833\n",
      "Epoch 7733: Training Loss: 0.12219304343064626 Validation Loss: 1.0576030015945435\n",
      "Epoch 7734: Training Loss: 0.12193081031243007 Validation Loss: 1.05743408203125\n",
      "Epoch 7735: Training Loss: 0.12159658720095952 Validation Loss: 1.0577962398529053\n",
      "Epoch 7736: Training Loss: 0.1216804360349973 Validation Loss: 1.0582472085952759\n",
      "Epoch 7737: Training Loss: 0.12246367583672206 Validation Loss: 1.0580750703811646\n",
      "Epoch 7738: Training Loss: 0.12153736998637517 Validation Loss: 1.0571115016937256\n",
      "Epoch 7739: Training Loss: 0.12163921445608139 Validation Loss: 1.0569106340408325\n",
      "Epoch 7740: Training Loss: 0.1216204861799876 Validation Loss: 1.0562095642089844\n",
      "Epoch 7741: Training Loss: 0.12176239242156346 Validation Loss: 1.0561317205429077\n",
      "Epoch 7742: Training Loss: 0.12165498981873195 Validation Loss: 1.0568138360977173\n",
      "Epoch 7743: Training Loss: 0.12215051800012589 Validation Loss: 1.0572326183319092\n",
      "Epoch 7744: Training Loss: 0.1223938415447871 Validation Loss: 1.0580357313156128\n",
      "Epoch 7745: Training Loss: 0.12166660527388255 Validation Loss: 1.0586246252059937\n",
      "Epoch 7746: Training Loss: 0.12156184017658234 Validation Loss: 1.0582308769226074\n",
      "Epoch 7747: Training Loss: 0.1216989482442538 Validation Loss: 1.0575343370437622\n",
      "Epoch 7748: Training Loss: 0.12204889704783757 Validation Loss: 1.0581821203231812\n",
      "Epoch 7749: Training Loss: 0.1221393272280693 Validation Loss: 1.0575802326202393\n",
      "Epoch 7750: Training Loss: 0.12197237213452657 Validation Loss: 1.0576273202896118\n",
      "Epoch 7751: Training Loss: 0.12136649588743846 Validation Loss: 1.0577402114868164\n",
      "Epoch 7752: Training Loss: 0.12142609059810638 Validation Loss: 1.0574887990951538\n",
      "Epoch 7753: Training Loss: 0.12165584415197372 Validation Loss: 1.057877779006958\n",
      "Epoch 7754: Training Loss: 0.12154888113339742 Validation Loss: 1.0583895444869995\n",
      "Epoch 7755: Training Loss: 0.1212313746412595 Validation Loss: 1.0581344366073608\n",
      "Epoch 7756: Training Loss: 0.1212473213672638 Validation Loss: 1.0578179359436035\n",
      "Epoch 7757: Training Loss: 0.12157193571329117 Validation Loss: 1.0581512451171875\n",
      "Epoch 7758: Training Loss: 0.1212173377474149 Validation Loss: 1.0585098266601562\n",
      "Epoch 7759: Training Loss: 0.12172490606705348 Validation Loss: 1.058927297592163\n",
      "Epoch 7760: Training Loss: 0.12189820657173793 Validation Loss: 1.058957815170288\n",
      "Epoch 7761: Training Loss: 0.12198389321565628 Validation Loss: 1.0585280656814575\n",
      "Epoch 7762: Training Loss: 0.12144903341929118 Validation Loss: 1.0588229894638062\n",
      "Epoch 7763: Training Loss: 0.12189388771851857 Validation Loss: 1.0588821172714233\n",
      "Epoch 7764: Training Loss: 0.1226046731074651 Validation Loss: 1.0574785470962524\n",
      "Epoch 7765: Training Loss: 0.12099409103393555 Validation Loss: 1.0577459335327148\n",
      "Epoch 7766: Training Loss: 0.12138662735621135 Validation Loss: 1.0586788654327393\n",
      "Epoch 7767: Training Loss: 0.12093078841765721 Validation Loss: 1.058428168296814\n",
      "Epoch 7768: Training Loss: 0.12190246085325877 Validation Loss: 1.05812406539917\n",
      "Epoch 7769: Training Loss: 0.121573140223821 Validation Loss: 1.0585331916809082\n",
      "Epoch 7770: Training Loss: 0.12148871272802353 Validation Loss: 1.0585262775421143\n",
      "Epoch 7771: Training Loss: 0.12139208614826202 Validation Loss: 1.0586844682693481\n",
      "Epoch 7772: Training Loss: 0.12133804460366567 Validation Loss: 1.058488130569458\n",
      "Epoch 7773: Training Loss: 0.12197967370351155 Validation Loss: 1.0582990646362305\n",
      "Epoch 7774: Training Loss: 0.12228125085433324 Validation Loss: 1.0584588050842285\n",
      "Epoch 7775: Training Loss: 0.12132069716850917 Validation Loss: 1.0589028596878052\n",
      "Epoch 7776: Training Loss: 0.12091317276159923 Validation Loss: 1.059069275856018\n",
      "Epoch 7777: Training Loss: 0.12194408724705379 Validation Loss: 1.0595430135726929\n",
      "Epoch 7778: Training Loss: 0.12154130140940349 Validation Loss: 1.0596071481704712\n",
      "Epoch 7779: Training Loss: 0.12072275578975677 Validation Loss: 1.0600999593734741\n",
      "Epoch 7780: Training Loss: 0.12101274977127711 Validation Loss: 1.059706687927246\n",
      "Epoch 7781: Training Loss: 0.12101335326830547 Validation Loss: 1.0600234270095825\n",
      "Epoch 7782: Training Loss: 0.1218167394399643 Validation Loss: 1.0596879720687866\n",
      "Epoch 7783: Training Loss: 0.12082909792661667 Validation Loss: 1.0600420236587524\n",
      "Epoch 7784: Training Loss: 0.12184908489386241 Validation Loss: 1.0594066381454468\n",
      "Epoch 7785: Training Loss: 0.12247015535831451 Validation Loss: 1.059667706489563\n",
      "Epoch 7786: Training Loss: 0.12140909085671107 Validation Loss: 1.0590415000915527\n",
      "Epoch 7787: Training Loss: 0.1211777776479721 Validation Loss: 1.0589982271194458\n",
      "Epoch 7788: Training Loss: 0.12100044637918472 Validation Loss: 1.0588122606277466\n",
      "Epoch 7789: Training Loss: 0.12077624350786209 Validation Loss: 1.0598379373550415\n",
      "Epoch 7790: Training Loss: 0.12148849914471309 Validation Loss: 1.060114860534668\n",
      "Epoch 7791: Training Loss: 0.12090266247590382 Validation Loss: 1.0596835613250732\n",
      "Epoch 7792: Training Loss: 0.12138156841198604 Validation Loss: 1.0589122772216797\n",
      "Epoch 7793: Training Loss: 0.12088742852210999 Validation Loss: 1.0588195323944092\n",
      "Epoch 7794: Training Loss: 0.12250478565692902 Validation Loss: 1.0585579872131348\n",
      "Epoch 7795: Training Loss: 0.12115916609764099 Validation Loss: 1.0596805810928345\n",
      "Epoch 7796: Training Loss: 0.12106521179278691 Validation Loss: 1.0601656436920166\n",
      "Epoch 7797: Training Loss: 0.1208332007129987 Validation Loss: 1.0599192380905151\n",
      "Epoch 7798: Training Loss: 0.12145526955525081 Validation Loss: 1.0599075555801392\n",
      "Epoch 7799: Training Loss: 0.1209867795308431 Validation Loss: 1.0594886541366577\n",
      "Epoch 7800: Training Loss: 0.12098267177740733 Validation Loss: 1.0596448183059692\n",
      "Epoch 7801: Training Loss: 0.12163214137156804 Validation Loss: 1.0587180852890015\n",
      "Epoch 7802: Training Loss: 0.12124340236186981 Validation Loss: 1.0592716932296753\n",
      "Epoch 7803: Training Loss: 0.12170523405075073 Validation Loss: 1.059255599975586\n",
      "Epoch 7804: Training Loss: 0.12120255827903748 Validation Loss: 1.0589566230773926\n",
      "Epoch 7805: Training Loss: 0.12183430542548497 Validation Loss: 1.0593458414077759\n",
      "Epoch 7806: Training Loss: 0.12044017761945724 Validation Loss: 1.0592619180679321\n",
      "Epoch 7807: Training Loss: 0.12136460343996684 Validation Loss: 1.0596296787261963\n",
      "Epoch 7808: Training Loss: 0.12085042397181193 Validation Loss: 1.0593518018722534\n",
      "Epoch 7809: Training Loss: 0.12075191487868626 Validation Loss: 1.059989333152771\n",
      "Epoch 7810: Training Loss: 0.12074174483617146 Validation Loss: 1.0600758790969849\n",
      "Epoch 7811: Training Loss: 0.12109686682621638 Validation Loss: 1.0595499277114868\n",
      "Epoch 7812: Training Loss: 0.12061030666033427 Validation Loss: 1.0590628385543823\n",
      "Epoch 7813: Training Loss: 0.12121450404326121 Validation Loss: 1.0594619512557983\n",
      "Epoch 7814: Training Loss: 0.1214606041709582 Validation Loss: 1.0593522787094116\n",
      "Epoch 7815: Training Loss: 0.12093807260195415 Validation Loss: 1.06035578250885\n",
      "Epoch 7816: Training Loss: 0.12063213686148326 Validation Loss: 1.0604333877563477\n",
      "Epoch 7817: Training Loss: 0.12100968013207118 Validation Loss: 1.060213565826416\n",
      "Epoch 7818: Training Loss: 0.12111547340949376 Validation Loss: 1.0595660209655762\n",
      "Epoch 7819: Training Loss: 0.12081598242123921 Validation Loss: 1.0593265295028687\n",
      "Epoch 7820: Training Loss: 0.12046991537014644 Validation Loss: 1.0590049028396606\n",
      "Epoch 7821: Training Loss: 0.12043541669845581 Validation Loss: 1.0597362518310547\n",
      "Epoch 7822: Training Loss: 0.12061057984828949 Validation Loss: 1.0604567527770996\n",
      "Epoch 7823: Training Loss: 0.12081602464119594 Validation Loss: 1.0603891611099243\n",
      "Epoch 7824: Training Loss: 0.12049472828706105 Validation Loss: 1.0610491037368774\n",
      "Epoch 7825: Training Loss: 0.12063092986742656 Validation Loss: 1.0609723329544067\n",
      "Epoch 7826: Training Loss: 0.1208169087767601 Validation Loss: 1.0611392259597778\n",
      "Epoch 7827: Training Loss: 0.12062534938255946 Validation Loss: 1.0599678754806519\n",
      "Epoch 7828: Training Loss: 0.12041349709033966 Validation Loss: 1.0602567195892334\n",
      "Epoch 7829: Training Loss: 0.12070292234420776 Validation Loss: 1.0598846673965454\n",
      "Epoch 7830: Training Loss: 0.12010818223158519 Validation Loss: 1.0606908798217773\n",
      "Epoch 7831: Training Loss: 0.12044232587019603 Validation Loss: 1.0606685876846313\n",
      "Epoch 7832: Training Loss: 0.12080549200375874 Validation Loss: 1.0598361492156982\n",
      "Epoch 7833: Training Loss: 0.12042684108018875 Validation Loss: 1.059529185295105\n",
      "Epoch 7834: Training Loss: 0.12085190663735072 Validation Loss: 1.0594720840454102\n",
      "Epoch 7835: Training Loss: 0.12073962390422821 Validation Loss: 1.0594720840454102\n",
      "Epoch 7836: Training Loss: 0.12064359585444133 Validation Loss: 1.0599502325057983\n",
      "Epoch 7837: Training Loss: 0.12110910316308339 Validation Loss: 1.0617977380752563\n",
      "Epoch 7838: Training Loss: 0.12029800315697987 Validation Loss: 1.062028169631958\n",
      "Epoch 7839: Training Loss: 0.1208242451151212 Validation Loss: 1.061866283416748\n",
      "Epoch 7840: Training Loss: 0.12080913285414378 Validation Loss: 1.0612590312957764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7841: Training Loss: 0.12031045307715733 Validation Loss: 1.0602633953094482\n",
      "Epoch 7842: Training Loss: 0.12009484320878983 Validation Loss: 1.0601084232330322\n",
      "Epoch 7843: Training Loss: 0.12013873706261317 Validation Loss: 1.0602037906646729\n",
      "Epoch 7844: Training Loss: 0.12045497447252274 Validation Loss: 1.0595449209213257\n",
      "Epoch 7845: Training Loss: 0.1204481174548467 Validation Loss: 1.0604171752929688\n",
      "Epoch 7846: Training Loss: 0.12021265178918839 Validation Loss: 1.0616909265518188\n",
      "Epoch 7847: Training Loss: 0.12050228317578633 Validation Loss: 1.0619347095489502\n",
      "Epoch 7848: Training Loss: 0.12039758016665776 Validation Loss: 1.0615922212600708\n",
      "Epoch 7849: Training Loss: 0.12007163216670354 Validation Loss: 1.060813546180725\n",
      "Epoch 7850: Training Loss: 0.1200379307071368 Validation Loss: 1.060291051864624\n",
      "Epoch 7851: Training Loss: 0.12030422190825145 Validation Loss: 1.0599943399429321\n",
      "Epoch 7852: Training Loss: 0.12028290579716365 Validation Loss: 1.0598431825637817\n",
      "Epoch 7853: Training Loss: 0.12046042581399281 Validation Loss: 1.0608998537063599\n",
      "Epoch 7854: Training Loss: 0.1204255223274231 Validation Loss: 1.0606826543807983\n",
      "Epoch 7855: Training Loss: 0.12041819095611572 Validation Loss: 1.061840295791626\n",
      "Epoch 7856: Training Loss: 0.12043788532416026 Validation Loss: 1.0624983310699463\n",
      "Epoch 7857: Training Loss: 0.12014536559581757 Validation Loss: 1.062685251235962\n",
      "Epoch 7858: Training Loss: 0.12012692044178645 Validation Loss: 1.06136953830719\n",
      "Epoch 7859: Training Loss: 0.12071956694126129 Validation Loss: 1.0610138177871704\n",
      "Epoch 7860: Training Loss: 0.1201988235116005 Validation Loss: 1.0604476928710938\n",
      "Epoch 7861: Training Loss: 0.12008064240217209 Validation Loss: 1.060951590538025\n",
      "Epoch 7862: Training Loss: 0.12014299134413402 Validation Loss: 1.060782790184021\n",
      "Epoch 7863: Training Loss: 0.12017129609982173 Validation Loss: 1.061344027519226\n",
      "Epoch 7864: Training Loss: 0.12017327795426051 Validation Loss: 1.0614850521087646\n",
      "Epoch 7865: Training Loss: 0.1201301043232282 Validation Loss: 1.061277985572815\n",
      "Epoch 7866: Training Loss: 0.12008297195037206 Validation Loss: 1.0614782571792603\n",
      "Epoch 7867: Training Loss: 0.1200726255774498 Validation Loss: 1.0608781576156616\n",
      "Epoch 7868: Training Loss: 0.12008582055568695 Validation Loss: 1.0610748529434204\n",
      "Epoch 7869: Training Loss: 0.12015274167060852 Validation Loss: 1.0619438886642456\n",
      "Epoch 7870: Training Loss: 0.12002824991941452 Validation Loss: 1.0613105297088623\n",
      "Epoch 7871: Training Loss: 0.12019900977611542 Validation Loss: 1.061346411705017\n",
      "Epoch 7872: Training Loss: 0.11985339969396591 Validation Loss: 1.0614529848098755\n",
      "Epoch 7873: Training Loss: 0.12011828770240147 Validation Loss: 1.0614678859710693\n",
      "Epoch 7874: Training Loss: 0.12042596936225891 Validation Loss: 1.0615075826644897\n",
      "Epoch 7875: Training Loss: 0.12013838936885197 Validation Loss: 1.0618053674697876\n",
      "Epoch 7876: Training Loss: 0.12050249179204305 Validation Loss: 1.0618435144424438\n",
      "Epoch 7877: Training Loss: 0.12013635784387589 Validation Loss: 1.061812162399292\n",
      "Epoch 7878: Training Loss: 0.12011139591534932 Validation Loss: 1.0622025728225708\n",
      "Epoch 7879: Training Loss: 0.12036632001399994 Validation Loss: 1.0618082284927368\n",
      "Epoch 7880: Training Loss: 0.12051511804262798 Validation Loss: 1.061484456062317\n",
      "Epoch 7881: Training Loss: 0.12038269639015198 Validation Loss: 1.0615019798278809\n",
      "Epoch 7882: Training Loss: 0.12080082545677821 Validation Loss: 1.0621590614318848\n",
      "Epoch 7883: Training Loss: 0.11987370749314626 Validation Loss: 1.0612126588821411\n",
      "Epoch 7884: Training Loss: 0.1206005613009135 Validation Loss: 1.0620288848876953\n",
      "Epoch 7885: Training Loss: 0.12070568899313609 Validation Loss: 1.0628516674041748\n",
      "Epoch 7886: Training Loss: 0.11999023209015529 Validation Loss: 1.0612552165985107\n",
      "Epoch 7887: Training Loss: 0.12013605733712514 Validation Loss: 1.061010718345642\n",
      "Epoch 7888: Training Loss: 0.11976631979147594 Validation Loss: 1.0617324113845825\n",
      "Epoch 7889: Training Loss: 0.12126138309637706 Validation Loss: 1.0628125667572021\n",
      "Epoch 7890: Training Loss: 0.11973016212383907 Validation Loss: 1.0630364418029785\n",
      "Epoch 7891: Training Loss: 0.12106337149937947 Validation Loss: 1.0635923147201538\n",
      "Epoch 7892: Training Loss: 0.11977919936180115 Validation Loss: 1.063018560409546\n",
      "Epoch 7893: Training Loss: 0.12001605828603108 Validation Loss: 1.062769889831543\n",
      "Epoch 7894: Training Loss: 0.11987591534852982 Validation Loss: 1.0623152256011963\n",
      "Epoch 7895: Training Loss: 0.12007926652828853 Validation Loss: 1.0620561838150024\n",
      "Epoch 7896: Training Loss: 0.11988138159116109 Validation Loss: 1.061476707458496\n",
      "Epoch 7897: Training Loss: 0.1199860821167628 Validation Loss: 1.0627118349075317\n",
      "Epoch 7898: Training Loss: 0.11965311070283254 Validation Loss: 1.0628129243850708\n",
      "Epoch 7899: Training Loss: 0.11953836927811305 Validation Loss: 1.0630384683609009\n",
      "Epoch 7900: Training Loss: 0.11983432869116466 Validation Loss: 1.062396764755249\n",
      "Epoch 7901: Training Loss: 0.11985861261685689 Validation Loss: 1.063008427619934\n",
      "Epoch 7902: Training Loss: 0.12019235640764236 Validation Loss: 1.0625975131988525\n",
      "Epoch 7903: Training Loss: 0.11974253505468369 Validation Loss: 1.0631704330444336\n",
      "Epoch 7904: Training Loss: 0.12019115686416626 Validation Loss: 1.0633069276809692\n",
      "Epoch 7905: Training Loss: 0.11942555507024129 Validation Loss: 1.0632402896881104\n",
      "Epoch 7906: Training Loss: 0.11931935946146648 Validation Loss: 1.0635526180267334\n",
      "Epoch 7907: Training Loss: 0.1204622561732928 Validation Loss: 1.0625381469726562\n",
      "Epoch 7908: Training Loss: 0.11962451785802841 Validation Loss: 1.0626450777053833\n",
      "Epoch 7909: Training Loss: 0.11958061655362447 Validation Loss: 1.0629453659057617\n",
      "Epoch 7910: Training Loss: 0.11931787679592769 Validation Loss: 1.063118577003479\n",
      "Epoch 7911: Training Loss: 0.11923271665970485 Validation Loss: 1.0634251832962036\n",
      "Epoch 7912: Training Loss: 0.12011021624008815 Validation Loss: 1.0637668371200562\n",
      "Epoch 7913: Training Loss: 0.12007395923137665 Validation Loss: 1.063307762145996\n",
      "Epoch 7914: Training Loss: 0.11908311893542607 Validation Loss: 1.0629072189331055\n",
      "Epoch 7915: Training Loss: 0.12182685732841492 Validation Loss: 1.0623376369476318\n",
      "Epoch 7916: Training Loss: 0.1193142260114352 Validation Loss: 1.0626106262207031\n",
      "Epoch 7917: Training Loss: 0.1197089006503423 Validation Loss: 1.0624032020568848\n",
      "Epoch 7918: Training Loss: 0.11984281490246455 Validation Loss: 1.0621544122695923\n",
      "Epoch 7919: Training Loss: 0.11967528859774272 Validation Loss: 1.0629197359085083\n",
      "Epoch 7920: Training Loss: 0.11950960506995519 Validation Loss: 1.0628374814987183\n",
      "Epoch 7921: Training Loss: 0.11958612749973933 Validation Loss: 1.0626569986343384\n",
      "Epoch 7922: Training Loss: 0.11940868695576985 Validation Loss: 1.0638034343719482\n",
      "Epoch 7923: Training Loss: 0.11888289203246434 Validation Loss: 1.0646159648895264\n",
      "Epoch 7924: Training Loss: 0.11962746580441792 Validation Loss: 1.0638964176177979\n",
      "Epoch 7925: Training Loss: 0.11928001791238785 Validation Loss: 1.0634604692459106\n",
      "Epoch 7926: Training Loss: 0.12036539614200592 Validation Loss: 1.0633931159973145\n",
      "Epoch 7927: Training Loss: 0.11930980036656062 Validation Loss: 1.0635098218917847\n",
      "Epoch 7928: Training Loss: 0.1195086141427358 Validation Loss: 1.0641372203826904\n",
      "Epoch 7929: Training Loss: 0.11979396889607112 Validation Loss: 1.0638601779937744\n",
      "Epoch 7930: Training Loss: 0.11910273631413777 Validation Loss: 1.063000202178955\n",
      "Epoch 7931: Training Loss: 0.12031824390093486 Validation Loss: 1.062070369720459\n",
      "Epoch 7932: Training Loss: 0.11942561715841293 Validation Loss: 1.062239170074463\n",
      "Epoch 7933: Training Loss: 0.11925246566534042 Validation Loss: 1.0627230405807495\n",
      "Epoch 7934: Training Loss: 0.11951777338981628 Validation Loss: 1.0636167526245117\n",
      "Epoch 7935: Training Loss: 0.11909348269303639 Validation Loss: 1.063970923423767\n",
      "Epoch 7936: Training Loss: 0.11934926857550938 Validation Loss: 1.0644350051879883\n",
      "Epoch 7937: Training Loss: 0.11951399842898051 Validation Loss: 1.0646824836730957\n",
      "Epoch 7938: Training Loss: 0.11918058743079503 Validation Loss: 1.064359188079834\n",
      "Epoch 7939: Training Loss: 0.11939764271179835 Validation Loss: 1.0632550716400146\n",
      "Epoch 7940: Training Loss: 0.1194295883178711 Validation Loss: 1.0631499290466309\n",
      "Epoch 7941: Training Loss: 0.11940143257379532 Validation Loss: 1.063217043876648\n",
      "Epoch 7942: Training Loss: 0.11912268896897633 Validation Loss: 1.0641839504241943\n",
      "Epoch 7943: Training Loss: 0.12028273443380992 Validation Loss: 1.0643819570541382\n",
      "Epoch 7944: Training Loss: 0.11842793474594752 Validation Loss: 1.0644760131835938\n",
      "Epoch 7945: Training Loss: 0.11932212859392166 Validation Loss: 1.0645085573196411\n",
      "Epoch 7946: Training Loss: 0.11938543617725372 Validation Loss: 1.0637707710266113\n",
      "Epoch 7947: Training Loss: 0.11934233456850052 Validation Loss: 1.0642985105514526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7948: Training Loss: 0.11938791970411937 Validation Loss: 1.0639530420303345\n",
      "Epoch 7949: Training Loss: 0.11918422083059947 Validation Loss: 1.06317138671875\n",
      "Epoch 7950: Training Loss: 0.11898338298002879 Validation Loss: 1.062936544418335\n",
      "Epoch 7951: Training Loss: 0.11993256211280823 Validation Loss: 1.0624921321868896\n",
      "Epoch 7952: Training Loss: 0.11873812973499298 Validation Loss: 1.0627847909927368\n",
      "Epoch 7953: Training Loss: 0.11921656628449757 Validation Loss: 1.0636351108551025\n",
      "Epoch 7954: Training Loss: 0.11962007731199265 Validation Loss: 1.0651366710662842\n",
      "Epoch 7955: Training Loss: 0.11967352032661438 Validation Loss: 1.0659562349319458\n",
      "Epoch 7956: Training Loss: 0.11907685548067093 Validation Loss: 1.0647436380386353\n",
      "Epoch 7957: Training Loss: 0.1188875362277031 Validation Loss: 1.0634007453918457\n",
      "Epoch 7958: Training Loss: 0.11921690652767818 Validation Loss: 1.062085747718811\n",
      "Epoch 7959: Training Loss: 0.11927968015273412 Validation Loss: 1.0627390146255493\n",
      "Epoch 7960: Training Loss: 0.11916612833738327 Validation Loss: 1.0645380020141602\n",
      "Epoch 7961: Training Loss: 0.11896078040202458 Validation Loss: 1.0665093660354614\n",
      "Epoch 7962: Training Loss: 0.11848139017820358 Validation Loss: 1.066284418106079\n",
      "Epoch 7963: Training Loss: 0.11940605938434601 Validation Loss: 1.0656040906906128\n",
      "Epoch 7964: Training Loss: 0.11956077069044113 Validation Loss: 1.0648064613342285\n",
      "Epoch 7965: Training Loss: 0.11909844477971394 Validation Loss: 1.064112663269043\n",
      "Epoch 7966: Training Loss: 0.11880030234654744 Validation Loss: 1.0645108222961426\n",
      "Epoch 7967: Training Loss: 0.1190099169810613 Validation Loss: 1.0660147666931152\n",
      "Epoch 7968: Training Loss: 0.11911919464667638 Validation Loss: 1.0651100873947144\n",
      "Epoch 7969: Training Loss: 0.11906899760166804 Validation Loss: 1.0647149085998535\n",
      "Epoch 7970: Training Loss: 0.11895954360564549 Validation Loss: 1.0638346672058105\n",
      "Epoch 7971: Training Loss: 0.1189309408267339 Validation Loss: 1.0635807514190674\n",
      "Epoch 7972: Training Loss: 0.11901319026947021 Validation Loss: 1.063836932182312\n",
      "Epoch 7973: Training Loss: 0.11902342985073726 Validation Loss: 1.0637840032577515\n",
      "Epoch 7974: Training Loss: 0.11918237308661143 Validation Loss: 1.0651147365570068\n",
      "Epoch 7975: Training Loss: 0.1187233179807663 Validation Loss: 1.0651580095291138\n",
      "Epoch 7976: Training Loss: 0.11896107345819473 Validation Loss: 1.0648505687713623\n",
      "Epoch 7977: Training Loss: 0.11871011058489482 Validation Loss: 1.065209150314331\n",
      "Epoch 7978: Training Loss: 0.11895868182182312 Validation Loss: 1.0644700527191162\n",
      "Epoch 7979: Training Loss: 0.11942142744859059 Validation Loss: 1.0644116401672363\n",
      "Epoch 7980: Training Loss: 0.11900891115268071 Validation Loss: 1.06449556350708\n",
      "Epoch 7981: Training Loss: 0.11964035034179688 Validation Loss: 1.063920497894287\n",
      "Epoch 7982: Training Loss: 0.11842356373866399 Validation Loss: 1.064573049545288\n",
      "Epoch 7983: Training Loss: 0.1186835120121638 Validation Loss: 1.0654447078704834\n",
      "Epoch 7984: Training Loss: 0.11907717088858287 Validation Loss: 1.065979242324829\n",
      "Epoch 7985: Training Loss: 0.11873338868220647 Validation Loss: 1.065565824508667\n",
      "Epoch 7986: Training Loss: 0.11842550585667293 Validation Loss: 1.0655871629714966\n",
      "Epoch 7987: Training Loss: 0.11863153427839279 Validation Loss: 1.0652756690979004\n",
      "Epoch 7988: Training Loss: 0.11862670133511226 Validation Loss: 1.0652731657028198\n",
      "Epoch 7989: Training Loss: 0.11950788895289104 Validation Loss: 1.0662366151809692\n",
      "Epoch 7990: Training Loss: 0.11850518981615703 Validation Loss: 1.0664931535720825\n",
      "Epoch 7991: Training Loss: 0.1189521923661232 Validation Loss: 1.066310167312622\n",
      "Epoch 7992: Training Loss: 0.11813990275065105 Validation Loss: 1.066298484802246\n",
      "Epoch 7993: Training Loss: 0.11881136645873387 Validation Loss: 1.065301775932312\n",
      "Epoch 7994: Training Loss: 0.11852576086918513 Validation Loss: 1.0650445222854614\n",
      "Epoch 7995: Training Loss: 0.11845408876736958 Validation Loss: 1.0643644332885742\n",
      "Epoch 7996: Training Loss: 0.11861785501241684 Validation Loss: 1.0642179250717163\n",
      "Epoch 7997: Training Loss: 0.11800221850474675 Validation Loss: 1.0640820264816284\n",
      "Epoch 7998: Training Loss: 0.1189750184615453 Validation Loss: 1.0637284517288208\n",
      "Epoch 7999: Training Loss: 0.11903634667396545 Validation Loss: 1.064752459526062\n",
      "Epoch 8000: Training Loss: 0.11943932125965755 Validation Loss: 1.066008448600769\n",
      "Epoch 8001: Training Loss: 0.11894199748833974 Validation Loss: 1.0666640996932983\n",
      "Epoch 8002: Training Loss: 0.11868154505888621 Validation Loss: 1.0662589073181152\n",
      "Epoch 8003: Training Loss: 0.11846572160720825 Validation Loss: 1.06584894657135\n",
      "Epoch 8004: Training Loss: 0.11854413151741028 Validation Loss: 1.0658397674560547\n",
      "Epoch 8005: Training Loss: 0.1175614669919014 Validation Loss: 1.0658217668533325\n",
      "Epoch 8006: Training Loss: 0.11901263395945232 Validation Loss: 1.065660834312439\n",
      "Epoch 8007: Training Loss: 0.11857575178146362 Validation Loss: 1.0661944150924683\n",
      "Epoch 8008: Training Loss: 0.11806746572256088 Validation Loss: 1.0661119222640991\n",
      "Epoch 8009: Training Loss: 0.1179661899805069 Validation Loss: 1.0661629438400269\n",
      "Epoch 8010: Training Loss: 0.11828740189472835 Validation Loss: 1.0668998956680298\n",
      "Epoch 8011: Training Loss: 0.11831346650918324 Validation Loss: 1.066468596458435\n",
      "Epoch 8012: Training Loss: 0.11817076057195663 Validation Loss: 1.0658321380615234\n",
      "Epoch 8013: Training Loss: 0.11804357419411342 Validation Loss: 1.065670132637024\n",
      "Epoch 8014: Training Loss: 0.11914765586455663 Validation Loss: 1.0652275085449219\n",
      "Epoch 8015: Training Loss: 0.11861233909924825 Validation Loss: 1.0655485391616821\n",
      "Epoch 8016: Training Loss: 0.11863492429256439 Validation Loss: 1.0674391984939575\n",
      "Epoch 8017: Training Loss: 0.11842676748832066 Validation Loss: 1.0673812627792358\n",
      "Epoch 8018: Training Loss: 0.11843922485907872 Validation Loss: 1.0662685632705688\n",
      "Epoch 8019: Training Loss: 0.11853078752756119 Validation Loss: 1.0653741359710693\n",
      "Epoch 8020: Training Loss: 0.11861933519442876 Validation Loss: 1.064815878868103\n",
      "Epoch 8021: Training Loss: 0.11822714904944102 Validation Loss: 1.0656986236572266\n",
      "Epoch 8022: Training Loss: 0.11760824670394261 Validation Loss: 1.0668913125991821\n",
      "Epoch 8023: Training Loss: 0.11813069383303325 Validation Loss: 1.0672990083694458\n",
      "Epoch 8024: Training Loss: 0.11889965583880742 Validation Loss: 1.0674179792404175\n",
      "Epoch 8025: Training Loss: 0.11767703294754028 Validation Loss: 1.0662025213241577\n",
      "Epoch 8026: Training Loss: 0.11778790007034938 Validation Loss: 1.065737247467041\n",
      "Epoch 8027: Training Loss: 0.11985842386881511 Validation Loss: 1.0666730403900146\n",
      "Epoch 8028: Training Loss: 0.11857464909553528 Validation Loss: 1.0671948194503784\n",
      "Epoch 8029: Training Loss: 0.11819048474232356 Validation Loss: 1.0671875476837158\n",
      "Epoch 8030: Training Loss: 0.11795172343651454 Validation Loss: 1.0666167736053467\n",
      "Epoch 8031: Training Loss: 0.1183649202187856 Validation Loss: 1.0661057233810425\n",
      "Epoch 8032: Training Loss: 0.1173350562651952 Validation Loss: 1.0656195878982544\n",
      "Epoch 8033: Training Loss: 0.118660402794679 Validation Loss: 1.0663951635360718\n",
      "Epoch 8034: Training Loss: 0.11819454530874889 Validation Loss: 1.0659369230270386\n",
      "Epoch 8035: Training Loss: 0.11828734477361043 Validation Loss: 1.0658783912658691\n",
      "Epoch 8036: Training Loss: 0.11803291241327922 Validation Loss: 1.0665831565856934\n",
      "Epoch 8037: Training Loss: 0.11828694989283879 Validation Loss: 1.0655986070632935\n",
      "Epoch 8038: Training Loss: 0.11816082398096721 Validation Loss: 1.0656940937042236\n",
      "Epoch 8039: Training Loss: 0.11799606680870056 Validation Loss: 1.0658270120620728\n",
      "Epoch 8040: Training Loss: 0.1179296871026357 Validation Loss: 1.0665688514709473\n",
      "Epoch 8041: Training Loss: 0.11839671929677327 Validation Loss: 1.0673967599868774\n",
      "Epoch 8042: Training Loss: 0.11899105707804362 Validation Loss: 1.0669996738433838\n",
      "Epoch 8043: Training Loss: 0.11807364970445633 Validation Loss: 1.0673630237579346\n",
      "Epoch 8044: Training Loss: 0.11777693529923756 Validation Loss: 1.0677571296691895\n",
      "Epoch 8045: Training Loss: 0.1180533915758133 Validation Loss: 1.0675855875015259\n",
      "Epoch 8046: Training Loss: 0.11819093426068623 Validation Loss: 1.0679550170898438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8047: Training Loss: 0.11859295517206192 Validation Loss: 1.0678937435150146\n",
      "Epoch 8048: Training Loss: 0.11779346565405528 Validation Loss: 1.0677725076675415\n",
      "Epoch 8049: Training Loss: 0.11783004303773244 Validation Loss: 1.068430781364441\n",
      "Epoch 8050: Training Loss: 0.11977233241001765 Validation Loss: 1.0679748058319092\n",
      "Epoch 8051: Training Loss: 0.11842357367277145 Validation Loss: 1.066964864730835\n",
      "Epoch 8052: Training Loss: 0.11747270077466965 Validation Loss: 1.0662975311279297\n",
      "Epoch 8053: Training Loss: 0.11759460965792339 Validation Loss: 1.0655932426452637\n",
      "Epoch 8054: Training Loss: 0.11795181532700856 Validation Loss: 1.0663537979125977\n",
      "Epoch 8055: Training Loss: 0.11781083544095357 Validation Loss: 1.0674986839294434\n",
      "Epoch 8056: Training Loss: 0.11789706101020177 Validation Loss: 1.0677223205566406\n",
      "Epoch 8057: Training Loss: 0.11766536037127177 Validation Loss: 1.0681946277618408\n",
      "Epoch 8058: Training Loss: 0.11782152950763702 Validation Loss: 1.0675965547561646\n",
      "Epoch 8059: Training Loss: 0.11802288144826889 Validation Loss: 1.0678662061691284\n",
      "Epoch 8060: Training Loss: 0.11870410044987996 Validation Loss: 1.0671266317367554\n",
      "Epoch 8061: Training Loss: 0.11815788596868515 Validation Loss: 1.0674288272857666\n",
      "Epoch 8062: Training Loss: 0.11850651850303014 Validation Loss: 1.0676758289337158\n",
      "Epoch 8063: Training Loss: 0.11837113400300343 Validation Loss: 1.0672351121902466\n",
      "Epoch 8064: Training Loss: 0.11762973417838414 Validation Loss: 1.0672123432159424\n",
      "Epoch 8065: Training Loss: 0.11796486377716064 Validation Loss: 1.0678389072418213\n",
      "Epoch 8066: Training Loss: 0.11750795940558116 Validation Loss: 1.0678051710128784\n",
      "Epoch 8067: Training Loss: 0.11792392532030742 Validation Loss: 1.0681676864624023\n",
      "Epoch 8068: Training Loss: 0.1175208364923795 Validation Loss: 1.0678378343582153\n",
      "Epoch 8069: Training Loss: 0.11773992081483205 Validation Loss: 1.0672098398208618\n",
      "Epoch 8070: Training Loss: 0.11760970205068588 Validation Loss: 1.067001461982727\n",
      "Epoch 8071: Training Loss: 0.11779046307007472 Validation Loss: 1.0669817924499512\n",
      "Epoch 8072: Training Loss: 0.11787805209557216 Validation Loss: 1.0668578147888184\n",
      "Epoch 8073: Training Loss: 0.11742370824019115 Validation Loss: 1.0675815343856812\n",
      "Epoch 8074: Training Loss: 0.11872479816277821 Validation Loss: 1.06900155544281\n",
      "Epoch 8075: Training Loss: 0.11763056615988414 Validation Loss: 1.0686233043670654\n",
      "Epoch 8076: Training Loss: 0.11777105927467346 Validation Loss: 1.0682101249694824\n",
      "Epoch 8077: Training Loss: 0.1173309435447057 Validation Loss: 1.067736029624939\n",
      "Epoch 8078: Training Loss: 0.11825451999902725 Validation Loss: 1.068161964416504\n",
      "Epoch 8079: Training Loss: 0.11838311702013016 Validation Loss: 1.0681569576263428\n",
      "Epoch 8080: Training Loss: 0.11740738650163014 Validation Loss: 1.0685241222381592\n",
      "Epoch 8081: Training Loss: 0.11787031839291255 Validation Loss: 1.068426489830017\n",
      "Epoch 8082: Training Loss: 0.11787360906600952 Validation Loss: 1.0687123537063599\n",
      "Epoch 8083: Training Loss: 0.1170615628361702 Validation Loss: 1.0685957670211792\n",
      "Epoch 8084: Training Loss: 0.11790280292431514 Validation Loss: 1.0674951076507568\n",
      "Epoch 8085: Training Loss: 0.1182199443380038 Validation Loss: 1.0673750638961792\n",
      "Epoch 8086: Training Loss: 0.11755149811506271 Validation Loss: 1.0680201053619385\n",
      "Epoch 8087: Training Loss: 0.1179199144244194 Validation Loss: 1.069456696510315\n",
      "Epoch 8088: Training Loss: 0.1177555372317632 Validation Loss: 1.0700966119766235\n",
      "Epoch 8089: Training Loss: 0.11732318500677745 Validation Loss: 1.0694917440414429\n",
      "Epoch 8090: Training Loss: 0.11752421408891678 Validation Loss: 1.0687611103057861\n",
      "Epoch 8091: Training Loss: 0.11728164553642273 Validation Loss: 1.0679467916488647\n",
      "Epoch 8092: Training Loss: 0.11756637195746104 Validation Loss: 1.0678354501724243\n",
      "Epoch 8093: Training Loss: 0.1175127203265826 Validation Loss: 1.0681895017623901\n",
      "Epoch 8094: Training Loss: 0.1172699232896169 Validation Loss: 1.0677263736724854\n",
      "Epoch 8095: Training Loss: 0.11748503893613815 Validation Loss: 1.0682551860809326\n",
      "Epoch 8096: Training Loss: 0.11746525516112645 Validation Loss: 1.0683082342147827\n",
      "Epoch 8097: Training Loss: 0.1174143875638644 Validation Loss: 1.068606972694397\n",
      "Epoch 8098: Training Loss: 0.11754778524239858 Validation Loss: 1.0683451890945435\n",
      "Epoch 8099: Training Loss: 0.11757624397675197 Validation Loss: 1.0684239864349365\n",
      "Epoch 8100: Training Loss: 0.11741950611273448 Validation Loss: 1.0680983066558838\n",
      "Epoch 8101: Training Loss: 0.11774704108635585 Validation Loss: 1.0673900842666626\n",
      "Epoch 8102: Training Loss: 0.1178223043680191 Validation Loss: 1.0672590732574463\n",
      "Epoch 8103: Training Loss: 0.11775567630926768 Validation Loss: 1.0676065683364868\n",
      "Epoch 8104: Training Loss: 0.11719002823034923 Validation Loss: 1.0688515901565552\n",
      "Epoch 8105: Training Loss: 0.11705029755830765 Validation Loss: 1.0702351331710815\n",
      "Epoch 8106: Training Loss: 0.11771401514609654 Validation Loss: 1.0713998079299927\n",
      "Epoch 8107: Training Loss: 0.11797793457905452 Validation Loss: 1.0703586339950562\n",
      "Epoch 8108: Training Loss: 0.11738384018341701 Validation Loss: 1.0689854621887207\n",
      "Epoch 8109: Training Loss: 0.1173904041449229 Validation Loss: 1.0676461458206177\n",
      "Epoch 8110: Training Loss: 0.11736958722273509 Validation Loss: 1.0668319463729858\n",
      "Epoch 8111: Training Loss: 0.11719811707735062 Validation Loss: 1.0678151845932007\n",
      "Epoch 8112: Training Loss: 0.11697221547365189 Validation Loss: 1.068798542022705\n",
      "Epoch 8113: Training Loss: 0.11724133292833964 Validation Loss: 1.0689918994903564\n",
      "Epoch 8114: Training Loss: 0.1172548308968544 Validation Loss: 1.0698472261428833\n",
      "Epoch 8115: Training Loss: 0.1182289645075798 Validation Loss: 1.0692917108535767\n",
      "Epoch 8116: Training Loss: 0.1177150160074234 Validation Loss: 1.068940281867981\n",
      "Epoch 8117: Training Loss: 0.11716202149788539 Validation Loss: 1.0689386129379272\n",
      "Epoch 8118: Training Loss: 0.11700094491243362 Validation Loss: 1.0681744813919067\n",
      "Epoch 8119: Training Loss: 0.11719431976477306 Validation Loss: 1.0684106349945068\n",
      "Epoch 8120: Training Loss: 0.11736345291137695 Validation Loss: 1.069193959236145\n",
      "Epoch 8121: Training Loss: 0.1172672708829244 Validation Loss: 1.069831132888794\n",
      "Epoch 8122: Training Loss: 0.11728888501723607 Validation Loss: 1.0694469213485718\n",
      "Epoch 8123: Training Loss: 0.11783166974782944 Validation Loss: 1.0693347454071045\n",
      "Epoch 8124: Training Loss: 0.1164988602201144 Validation Loss: 1.0687824487686157\n",
      "Epoch 8125: Training Loss: 0.11710990468660991 Validation Loss: 1.0690332651138306\n",
      "Epoch 8126: Training Loss: 0.11695109059413274 Validation Loss: 1.069654107093811\n",
      "Epoch 8127: Training Loss: 0.11699346452951431 Validation Loss: 1.069566249847412\n",
      "Epoch 8128: Training Loss: 0.11724776029586792 Validation Loss: 1.069882869720459\n",
      "Epoch 8129: Training Loss: 0.11693264544010162 Validation Loss: 1.069400668144226\n",
      "Epoch 8130: Training Loss: 0.117752306163311 Validation Loss: 1.0701801776885986\n",
      "Epoch 8131: Training Loss: 0.11711615572373073 Validation Loss: 1.0689531564712524\n",
      "Epoch 8132: Training Loss: 0.11692048360904057 Validation Loss: 1.06949782371521\n",
      "Epoch 8133: Training Loss: 0.11673492938280106 Validation Loss: 1.0695291757583618\n",
      "Epoch 8134: Training Loss: 0.11632170528173447 Validation Loss: 1.0698082447052002\n",
      "Epoch 8135: Training Loss: 0.11736935377120972 Validation Loss: 1.0695807933807373\n",
      "Epoch 8136: Training Loss: 0.11752566943566005 Validation Loss: 1.0702931880950928\n",
      "Epoch 8137: Training Loss: 0.1174472173055013 Validation Loss: 1.0707719326019287\n",
      "Epoch 8138: Training Loss: 0.11698509256045024 Validation Loss: 1.0703760385513306\n",
      "Epoch 8139: Training Loss: 0.11787132173776627 Validation Loss: 1.0699776411056519\n",
      "Epoch 8140: Training Loss: 0.11689965675274532 Validation Loss: 1.0691251754760742\n",
      "Epoch 8141: Training Loss: 0.1165980522831281 Validation Loss: 1.0691105127334595\n",
      "Epoch 8142: Training Loss: 0.11689663430054982 Validation Loss: 1.0704288482666016\n",
      "Epoch 8143: Training Loss: 0.11672156304121017 Validation Loss: 1.0702283382415771\n",
      "Epoch 8144: Training Loss: 0.11680632332960765 Validation Loss: 1.0699102878570557\n",
      "Epoch 8145: Training Loss: 0.11696569621562958 Validation Loss: 1.069631814956665\n",
      "Epoch 8146: Training Loss: 0.11719385037819545 Validation Loss: 1.0688751935958862\n",
      "Epoch 8147: Training Loss: 0.1169412409265836 Validation Loss: 1.0688825845718384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8148: Training Loss: 0.11660296966632207 Validation Loss: 1.069644808769226\n",
      "Epoch 8149: Training Loss: 0.11703186730543773 Validation Loss: 1.0706700086593628\n",
      "Epoch 8150: Training Loss: 0.11779150118430455 Validation Loss: 1.0711750984191895\n",
      "Epoch 8151: Training Loss: 0.11646616955598195 Validation Loss: 1.0707145929336548\n",
      "Epoch 8152: Training Loss: 0.1166690190633138 Validation Loss: 1.0703892707824707\n",
      "Epoch 8153: Training Loss: 0.11678308496872584 Validation Loss: 1.0700992345809937\n",
      "Epoch 8154: Training Loss: 0.11718674997488658 Validation Loss: 1.069188117980957\n",
      "Epoch 8155: Training Loss: 0.11647668729225795 Validation Loss: 1.0693964958190918\n",
      "Epoch 8156: Training Loss: 0.11663142840067546 Validation Loss: 1.0700430870056152\n",
      "Epoch 8157: Training Loss: 0.11725885172684987 Validation Loss: 1.0715097188949585\n",
      "Epoch 8158: Training Loss: 0.11710314949353536 Validation Loss: 1.0718402862548828\n",
      "Epoch 8159: Training Loss: 0.1168662856022517 Validation Loss: 1.0720438957214355\n",
      "Epoch 8160: Training Loss: 0.11946620792150497 Validation Loss: 1.0703283548355103\n",
      "Epoch 8161: Training Loss: 0.11694011837244034 Validation Loss: 1.0698164701461792\n",
      "Epoch 8162: Training Loss: 0.11693080763022105 Validation Loss: 1.0705254077911377\n",
      "Epoch 8163: Training Loss: 0.11653992036978404 Validation Loss: 1.0704309940338135\n",
      "Epoch 8164: Training Loss: 0.11655786881844203 Validation Loss: 1.0704659223556519\n",
      "Epoch 8165: Training Loss: 0.11782333999872208 Validation Loss: 1.0709024667739868\n",
      "Epoch 8166: Training Loss: 0.11779131243626277 Validation Loss: 1.0712796449661255\n",
      "Epoch 8167: Training Loss: 0.11659767478704453 Validation Loss: 1.0710645914077759\n",
      "Epoch 8168: Training Loss: 0.11670743425687154 Validation Loss: 1.0711520910263062\n",
      "Epoch 8169: Training Loss: 0.11647094537814458 Validation Loss: 1.0708765983581543\n",
      "Epoch 8170: Training Loss: 0.11645847310622533 Validation Loss: 1.070739984512329\n",
      "Epoch 8171: Training Loss: 0.11673997342586517 Validation Loss: 1.07099187374115\n",
      "Epoch 8172: Training Loss: 0.11685605347156525 Validation Loss: 1.0713653564453125\n",
      "Epoch 8173: Training Loss: 0.11650768170754115 Validation Loss: 1.0708287954330444\n",
      "Epoch 8174: Training Loss: 0.11634115626414616 Validation Loss: 1.0706522464752197\n",
      "Epoch 8175: Training Loss: 0.11651650567849477 Validation Loss: 1.070689082145691\n",
      "Epoch 8176: Training Loss: 0.11648800720771153 Validation Loss: 1.0699750185012817\n",
      "Epoch 8177: Training Loss: 0.11684018870194753 Validation Loss: 1.0705503225326538\n",
      "Epoch 8178: Training Loss: 0.116137000421683 Validation Loss: 1.070791244506836\n",
      "Epoch 8179: Training Loss: 0.11668728291988373 Validation Loss: 1.07087242603302\n",
      "Epoch 8180: Training Loss: 0.11659161001443863 Validation Loss: 1.0710183382034302\n",
      "Epoch 8181: Training Loss: 0.11664576828479767 Validation Loss: 1.0714163780212402\n",
      "Epoch 8182: Training Loss: 0.11644134918848674 Validation Loss: 1.0712954998016357\n",
      "Epoch 8183: Training Loss: 0.11685926715532939 Validation Loss: 1.0710716247558594\n",
      "Epoch 8184: Training Loss: 0.11677766591310501 Validation Loss: 1.070325255393982\n",
      "Epoch 8185: Training Loss: 0.11660237858692805 Validation Loss: 1.0709682703018188\n",
      "Epoch 8186: Training Loss: 0.11679468303918839 Validation Loss: 1.070461630821228\n",
      "Epoch 8187: Training Loss: 0.11631129930416743 Validation Loss: 1.0711030960083008\n",
      "Epoch 8188: Training Loss: 0.1162731796503067 Validation Loss: 1.0706143379211426\n",
      "Epoch 8189: Training Loss: 0.1163180743654569 Validation Loss: 1.0712823867797852\n",
      "Epoch 8190: Training Loss: 0.11621799816687901 Validation Loss: 1.0718536376953125\n",
      "Epoch 8191: Training Loss: 0.11702067404985428 Validation Loss: 1.0716524124145508\n",
      "Epoch 8192: Training Loss: 0.11650880177815755 Validation Loss: 1.0718132257461548\n",
      "Epoch 8193: Training Loss: 0.11643545577923457 Validation Loss: 1.0718741416931152\n",
      "Epoch 8194: Training Loss: 0.11626848081747691 Validation Loss: 1.071505069732666\n",
      "Epoch 8195: Training Loss: 0.11608512699604034 Validation Loss: 1.0710214376449585\n",
      "Epoch 8196: Training Loss: 0.11648073047399521 Validation Loss: 1.0712153911590576\n",
      "Epoch 8197: Training Loss: 0.11639125893513362 Validation Loss: 1.0702873468399048\n",
      "Epoch 8198: Training Loss: 0.11584465950727463 Validation Loss: 1.0704317092895508\n",
      "Epoch 8199: Training Loss: 0.1159432902932167 Validation Loss: 1.071433663368225\n",
      "Epoch 8200: Training Loss: 0.11621050039927165 Validation Loss: 1.0709900856018066\n",
      "Epoch 8201: Training Loss: 0.11653933177391688 Validation Loss: 1.0718364715576172\n",
      "Epoch 8202: Training Loss: 0.11616020401318868 Validation Loss: 1.0715898275375366\n",
      "Epoch 8203: Training Loss: 0.11638943602641423 Validation Loss: 1.0728826522827148\n",
      "Epoch 8204: Training Loss: 0.11612431704998016 Validation Loss: 1.0717138051986694\n",
      "Epoch 8205: Training Loss: 0.11648774147033691 Validation Loss: 1.0715770721435547\n",
      "Epoch 8206: Training Loss: 0.11621177693208058 Validation Loss: 1.0715186595916748\n",
      "Epoch 8207: Training Loss: 0.11589119583368301 Validation Loss: 1.071707844734192\n",
      "Epoch 8208: Training Loss: 0.11584018419186275 Validation Loss: 1.0710023641586304\n",
      "Epoch 8209: Training Loss: 0.1160737822453181 Validation Loss: 1.071090579032898\n",
      "Epoch 8210: Training Loss: 0.11629433184862137 Validation Loss: 1.0710647106170654\n",
      "Epoch 8211: Training Loss: 0.11598839362462361 Validation Loss: 1.0715094804763794\n",
      "Epoch 8212: Training Loss: 0.11590905984242757 Validation Loss: 1.0722843408584595\n",
      "Epoch 8213: Training Loss: 0.11602025479078293 Validation Loss: 1.0720903873443604\n",
      "Epoch 8214: Training Loss: 0.11617439736922582 Validation Loss: 1.0719274282455444\n",
      "Epoch 8215: Training Loss: 0.11612222343683243 Validation Loss: 1.0712600946426392\n",
      "Epoch 8216: Training Loss: 0.11677449693282445 Validation Loss: 1.0713558197021484\n",
      "Epoch 8217: Training Loss: 0.11631913234790166 Validation Loss: 1.070412516593933\n",
      "Epoch 8218: Training Loss: 0.11596233894427617 Validation Loss: 1.0708802938461304\n",
      "Epoch 8219: Training Loss: 0.11645162850618362 Validation Loss: 1.0712758302688599\n",
      "Epoch 8220: Training Loss: 0.11582341541846593 Validation Loss: 1.0721098184585571\n",
      "Epoch 8221: Training Loss: 0.11643366515636444 Validation Loss: 1.0718377828598022\n",
      "Epoch 8222: Training Loss: 0.11602465311686198 Validation Loss: 1.0715558528900146\n",
      "Epoch 8223: Training Loss: 0.11596359809239705 Validation Loss: 1.0718504190444946\n",
      "Epoch 8224: Training Loss: 0.11617812514305115 Validation Loss: 1.073158860206604\n",
      "Epoch 8225: Training Loss: 0.11625721802314122 Validation Loss: 1.073315143585205\n",
      "Epoch 8226: Training Loss: 0.11604546258846919 Validation Loss: 1.0727795362472534\n",
      "Epoch 8227: Training Loss: 0.11604516704877217 Validation Loss: 1.072942852973938\n",
      "Epoch 8228: Training Loss: 0.11627066880464554 Validation Loss: 1.0729749202728271\n",
      "Epoch 8229: Training Loss: 0.1162044753630956 Validation Loss: 1.073305368423462\n",
      "Epoch 8230: Training Loss: 0.11581082145373027 Validation Loss: 1.0732439756393433\n",
      "Epoch 8231: Training Loss: 0.11612405876318614 Validation Loss: 1.0731031894683838\n",
      "Epoch 8232: Training Loss: 0.11551925539970398 Validation Loss: 1.073160171508789\n",
      "Epoch 8233: Training Loss: 0.11599306017160416 Validation Loss: 1.07357656955719\n",
      "Epoch 8234: Training Loss: 0.11610706895589828 Validation Loss: 1.0720046758651733\n",
      "Epoch 8235: Training Loss: 0.11551777770121892 Validation Loss: 1.0719997882843018\n",
      "Epoch 8236: Training Loss: 0.11660567671060562 Validation Loss: 1.0717617273330688\n",
      "Epoch 8237: Training Loss: 0.11645579586426417 Validation Loss: 1.0726642608642578\n",
      "Epoch 8238: Training Loss: 0.11602773517370224 Validation Loss: 1.0722601413726807\n",
      "Epoch 8239: Training Loss: 0.11586761226256688 Validation Loss: 1.072493076324463\n",
      "Epoch 8240: Training Loss: 0.11580238988002141 Validation Loss: 1.0718661546707153\n",
      "Epoch 8241: Training Loss: 0.11543400585651398 Validation Loss: 1.0720934867858887\n",
      "Epoch 8242: Training Loss: 0.1157238706946373 Validation Loss: 1.0713880062103271\n",
      "Epoch 8243: Training Loss: 0.11578945318857829 Validation Loss: 1.07171630859375\n",
      "Epoch 8244: Training Loss: 0.11600830405950546 Validation Loss: 1.0714597702026367\n",
      "Epoch 8245: Training Loss: 0.1157193457086881 Validation Loss: 1.0717774629592896\n",
      "Epoch 8246: Training Loss: 0.11667451510826747 Validation Loss: 1.072996735572815\n",
      "Epoch 8247: Training Loss: 0.11578492820262909 Validation Loss: 1.072837471961975\n",
      "Epoch 8248: Training Loss: 0.11629394441843033 Validation Loss: 1.0735853910446167\n",
      "Epoch 8249: Training Loss: 0.11559391766786575 Validation Loss: 1.073269248008728\n",
      "Epoch 8250: Training Loss: 0.11545548836390178 Validation Loss: 1.073254942893982\n",
      "Epoch 8251: Training Loss: 0.11573875447114308 Validation Loss: 1.0735417604446411\n",
      "Epoch 8252: Training Loss: 0.11541930586099625 Validation Loss: 1.0727349519729614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8253: Training Loss: 0.11745098729928334 Validation Loss: 1.071911334991455\n",
      "Epoch 8254: Training Loss: 0.11496215065320332 Validation Loss: 1.0728282928466797\n",
      "Epoch 8255: Training Loss: 0.11598849296569824 Validation Loss: 1.0743789672851562\n",
      "Epoch 8256: Training Loss: 0.1159801110625267 Validation Loss: 1.0749648809432983\n",
      "Epoch 8257: Training Loss: 0.11580355962117513 Validation Loss: 1.0745922327041626\n",
      "Epoch 8258: Training Loss: 0.11561338106791179 Validation Loss: 1.0736353397369385\n",
      "Epoch 8259: Training Loss: 0.11581866691509883 Validation Loss: 1.0735697746276855\n",
      "Epoch 8260: Training Loss: 0.11568585286537807 Validation Loss: 1.073897123336792\n",
      "Epoch 8261: Training Loss: 0.1154826208949089 Validation Loss: 1.0732706785202026\n",
      "Epoch 8262: Training Loss: 0.116031547387441 Validation Loss: 1.0727343559265137\n",
      "Epoch 8263: Training Loss: 0.11562119424343109 Validation Loss: 1.0727882385253906\n",
      "Epoch 8264: Training Loss: 0.11553690334161122 Validation Loss: 1.0731056928634644\n",
      "Epoch 8265: Training Loss: 0.11536827931801479 Validation Loss: 1.0729544162750244\n",
      "Epoch 8266: Training Loss: 0.11530688156684239 Validation Loss: 1.0733603239059448\n",
      "Epoch 8267: Training Loss: 0.11563038577636083 Validation Loss: 1.072818398475647\n",
      "Epoch 8268: Training Loss: 0.11527257412672043 Validation Loss: 1.0732749700546265\n",
      "Epoch 8269: Training Loss: 0.11608392496903737 Validation Loss: 1.0733022689819336\n",
      "Epoch 8270: Training Loss: 0.11529986560344696 Validation Loss: 1.0732909440994263\n",
      "Epoch 8271: Training Loss: 0.11588917920986812 Validation Loss: 1.074002742767334\n",
      "Epoch 8272: Training Loss: 0.11503311743338902 Validation Loss: 1.0745726823806763\n",
      "Epoch 8273: Training Loss: 0.11537605275710423 Validation Loss: 1.0739047527313232\n",
      "Epoch 8274: Training Loss: 0.11533177644014359 Validation Loss: 1.0732542276382446\n",
      "Epoch 8275: Training Loss: 0.11550867309172948 Validation Loss: 1.0736775398254395\n",
      "Epoch 8276: Training Loss: 0.11531265079975128 Validation Loss: 1.0742082595825195\n",
      "Epoch 8277: Training Loss: 0.11689696957667668 Validation Loss: 1.0728389024734497\n",
      "Epoch 8278: Training Loss: 0.1160149375597636 Validation Loss: 1.07314133644104\n",
      "Epoch 8279: Training Loss: 0.11644525577624638 Validation Loss: 1.0737895965576172\n",
      "Epoch 8280: Training Loss: 0.1153710310657819 Validation Loss: 1.0739682912826538\n",
      "Epoch 8281: Training Loss: 0.11536463101704915 Validation Loss: 1.0743316411972046\n",
      "Epoch 8282: Training Loss: 0.11564745257298152 Validation Loss: 1.074973464012146\n",
      "Epoch 8283: Training Loss: 0.11495446662108104 Validation Loss: 1.0737969875335693\n",
      "Epoch 8284: Training Loss: 0.11537596583366394 Validation Loss: 1.0731228590011597\n",
      "Epoch 8285: Training Loss: 0.11602010081211726 Validation Loss: 1.0730280876159668\n",
      "Epoch 8286: Training Loss: 0.11556405077377956 Validation Loss: 1.0734211206436157\n",
      "Epoch 8287: Training Loss: 0.11479499687751134 Validation Loss: 1.073167324066162\n",
      "Epoch 8288: Training Loss: 0.11603615929683049 Validation Loss: 1.07328462600708\n",
      "Epoch 8289: Training Loss: 0.11566345393657684 Validation Loss: 1.0729109048843384\n",
      "Epoch 8290: Training Loss: 0.11573142309983571 Validation Loss: 1.0744563341140747\n",
      "Epoch 8291: Training Loss: 0.11503504713376363 Validation Loss: 1.075626015663147\n",
      "Epoch 8292: Training Loss: 0.11505697170893352 Validation Loss: 1.0755329132080078\n",
      "Epoch 8293: Training Loss: 0.11542209486166637 Validation Loss: 1.075028419494629\n",
      "Epoch 8294: Training Loss: 0.11487376193205516 Validation Loss: 1.0747597217559814\n",
      "Epoch 8295: Training Loss: 0.11549320071935654 Validation Loss: 1.0743598937988281\n",
      "Epoch 8296: Training Loss: 0.11507496486107509 Validation Loss: 1.0738935470581055\n",
      "Epoch 8297: Training Loss: 0.11516601343949635 Validation Loss: 1.0744853019714355\n",
      "Epoch 8298: Training Loss: 0.11504501104354858 Validation Loss: 1.0741381645202637\n",
      "Epoch 8299: Training Loss: 0.11499749372402827 Validation Loss: 1.0744162797927856\n",
      "Epoch 8300: Training Loss: 0.11519268155097961 Validation Loss: 1.0741965770721436\n",
      "Epoch 8301: Training Loss: 0.11519645154476166 Validation Loss: 1.0738162994384766\n",
      "Epoch 8302: Training Loss: 0.11540830135345459 Validation Loss: 1.0742762088775635\n",
      "Epoch 8303: Training Loss: 0.11661505450805028 Validation Loss: 1.0742876529693604\n",
      "Epoch 8304: Training Loss: 0.11501577993233998 Validation Loss: 1.074593424797058\n",
      "Epoch 8305: Training Loss: 0.11499401926994324 Validation Loss: 1.0745279788970947\n",
      "Epoch 8306: Training Loss: 0.11514803767204285 Validation Loss: 1.075148344039917\n",
      "Epoch 8307: Training Loss: 0.11511831482251485 Validation Loss: 1.075252652168274\n",
      "Epoch 8308: Training Loss: 0.11524941027164459 Validation Loss: 1.075946569442749\n",
      "Epoch 8309: Training Loss: 0.11569561312596004 Validation Loss: 1.075560450553894\n",
      "Epoch 8310: Training Loss: 0.11500644187132518 Validation Loss: 1.0754255056381226\n",
      "Epoch 8311: Training Loss: 0.11507640530665715 Validation Loss: 1.0737632513046265\n",
      "Epoch 8312: Training Loss: 0.11548661192258199 Validation Loss: 1.0731309652328491\n",
      "Epoch 8313: Training Loss: 0.11498395105202992 Validation Loss: 1.0732569694519043\n",
      "Epoch 8314: Training Loss: 0.11532651384671529 Validation Loss: 1.0739609003067017\n",
      "Epoch 8315: Training Loss: 0.11501672118902206 Validation Loss: 1.0746855735778809\n",
      "Epoch 8316: Training Loss: 0.11487439771493275 Validation Loss: 1.075675129890442\n",
      "Epoch 8317: Training Loss: 0.11514516919851303 Validation Loss: 1.0753923654556274\n",
      "Epoch 8318: Training Loss: 0.11583540091911952 Validation Loss: 1.0752201080322266\n",
      "Epoch 8319: Training Loss: 0.11508399496475856 Validation Loss: 1.0747495889663696\n",
      "Epoch 8320: Training Loss: 0.11475295076767604 Validation Loss: 1.0742945671081543\n",
      "Epoch 8321: Training Loss: 0.11496645212173462 Validation Loss: 1.074549913406372\n",
      "Epoch 8322: Training Loss: 0.11483600238958995 Validation Loss: 1.0755561590194702\n",
      "Epoch 8323: Training Loss: 0.11495620260636012 Validation Loss: 1.0763081312179565\n",
      "Epoch 8324: Training Loss: 0.11507878700892131 Validation Loss: 1.0779539346694946\n",
      "Epoch 8325: Training Loss: 0.11540977905193965 Validation Loss: 1.0763059854507446\n",
      "Epoch 8326: Training Loss: 0.11529971907536189 Validation Loss: 1.0762884616851807\n",
      "Epoch 8327: Training Loss: 0.11454348017772038 Validation Loss: 1.0751906633377075\n",
      "Epoch 8328: Training Loss: 0.11477346966663997 Validation Loss: 1.0750807523727417\n",
      "Epoch 8329: Training Loss: 0.11537376046180725 Validation Loss: 1.0758051872253418\n",
      "Epoch 8330: Training Loss: 0.11534244815508525 Validation Loss: 1.0765118598937988\n",
      "Epoch 8331: Training Loss: 0.11473339051008224 Validation Loss: 1.077417016029358\n",
      "Epoch 8332: Training Loss: 0.11581303924322128 Validation Loss: 1.0770360231399536\n",
      "Epoch 8333: Training Loss: 0.1149056926369667 Validation Loss: 1.0767828226089478\n",
      "Epoch 8334: Training Loss: 0.11452205230792363 Validation Loss: 1.0758426189422607\n",
      "Epoch 8335: Training Loss: 0.11488339801629384 Validation Loss: 1.075317621231079\n",
      "Epoch 8336: Training Loss: 0.11486109594504039 Validation Loss: 1.0749969482421875\n",
      "Epoch 8337: Training Loss: 0.1146405463417371 Validation Loss: 1.0744810104370117\n",
      "Epoch 8338: Training Loss: 0.11465394496917725 Validation Loss: 1.0749262571334839\n",
      "Epoch 8339: Training Loss: 0.11454799522956212 Validation Loss: 1.0752509832382202\n",
      "Epoch 8340: Training Loss: 0.11414753645658493 Validation Loss: 1.0760715007781982\n",
      "Epoch 8341: Training Loss: 0.1155180111527443 Validation Loss: 1.075859785079956\n",
      "Epoch 8342: Training Loss: 0.1147462675968806 Validation Loss: 1.0766199827194214\n",
      "Epoch 8343: Training Loss: 0.11524372299512227 Validation Loss: 1.0760846138000488\n",
      "Epoch 8344: Training Loss: 0.11461368203163147 Validation Loss: 1.076000690460205\n",
      "Epoch 8345: Training Loss: 0.114719125131766 Validation Loss: 1.075308918952942\n",
      "Epoch 8346: Training Loss: 0.1151810313264529 Validation Loss: 1.0757520198822021\n",
      "Epoch 8347: Training Loss: 0.11535748342672984 Validation Loss: 1.0752493143081665\n",
      "Epoch 8348: Training Loss: 0.11482468744119008 Validation Loss: 1.0756289958953857\n",
      "Epoch 8349: Training Loss: 0.11450231571992238 Validation Loss: 1.076663851737976\n",
      "Epoch 8350: Training Loss: 0.11480380594730377 Validation Loss: 1.0774480104446411\n",
      "Epoch 8351: Training Loss: 0.11569134642680486 Validation Loss: 1.0770915746688843\n",
      "Epoch 8352: Training Loss: 0.1143825575709343 Validation Loss: 1.0766327381134033\n",
      "Epoch 8353: Training Loss: 0.11434820046027501 Validation Loss: 1.0757005214691162\n",
      "Epoch 8354: Training Loss: 0.1146966814994812 Validation Loss: 1.0753332376480103\n",
      "Epoch 8355: Training Loss: 0.11466788997252782 Validation Loss: 1.0754666328430176\n",
      "Epoch 8356: Training Loss: 0.1139896884560585 Validation Loss: 1.0762690305709839\n",
      "Epoch 8357: Training Loss: 0.11486284931500752 Validation Loss: 1.0774860382080078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8358: Training Loss: 0.11498090624809265 Validation Loss: 1.0768569707870483\n",
      "Epoch 8359: Training Loss: 0.11435412615537643 Validation Loss: 1.0768662691116333\n",
      "Epoch 8360: Training Loss: 0.11431697010993958 Validation Loss: 1.0766692161560059\n",
      "Epoch 8361: Training Loss: 0.11453072975079219 Validation Loss: 1.0767369270324707\n",
      "Epoch 8362: Training Loss: 0.11467039088408153 Validation Loss: 1.0768585205078125\n",
      "Epoch 8363: Training Loss: 0.1145003189643224 Validation Loss: 1.0766692161560059\n",
      "Epoch 8364: Training Loss: 0.11417717983325322 Validation Loss: 1.076533317565918\n",
      "Epoch 8365: Training Loss: 0.11445423712333043 Validation Loss: 1.0768479108810425\n",
      "Epoch 8366: Training Loss: 0.11401969691117604 Validation Loss: 1.07670259475708\n",
      "Epoch 8367: Training Loss: 0.1139767939845721 Validation Loss: 1.076392412185669\n",
      "Epoch 8368: Training Loss: 0.11418107151985168 Validation Loss: 1.0757948160171509\n",
      "Epoch 8369: Training Loss: 0.11413491020600001 Validation Loss: 1.0752862691879272\n",
      "Epoch 8370: Training Loss: 0.11423370242118835 Validation Loss: 1.076101541519165\n",
      "Epoch 8371: Training Loss: 0.11430524786313374 Validation Loss: 1.0770317316055298\n",
      "Epoch 8372: Training Loss: 0.11388762791951497 Validation Loss: 1.0765867233276367\n",
      "Epoch 8373: Training Loss: 0.11425264924764633 Validation Loss: 1.076825499534607\n",
      "Epoch 8374: Training Loss: 0.11453420420487721 Validation Loss: 1.0772475004196167\n",
      "Epoch 8375: Training Loss: 0.1143341064453125 Validation Loss: 1.0772134065628052\n",
      "Epoch 8376: Training Loss: 0.11415510127941768 Validation Loss: 1.0771713256835938\n",
      "Epoch 8377: Training Loss: 0.1143149733543396 Validation Loss: 1.0773693323135376\n",
      "Epoch 8378: Training Loss: 0.11424820125102997 Validation Loss: 1.0776379108428955\n",
      "Epoch 8379: Training Loss: 0.11445717016855876 Validation Loss: 1.0779811143875122\n",
      "Epoch 8380: Training Loss: 0.11461735020081203 Validation Loss: 1.0771669149398804\n",
      "Epoch 8381: Training Loss: 0.11415779838959376 Validation Loss: 1.0765528678894043\n",
      "Epoch 8382: Training Loss: 0.11470225950082143 Validation Loss: 1.0776921510696411\n",
      "Epoch 8383: Training Loss: 0.114693450431029 Validation Loss: 1.0774800777435303\n",
      "Epoch 8384: Training Loss: 0.11390633384386699 Validation Loss: 1.077426791191101\n",
      "Epoch 8385: Training Loss: 0.11440906425317128 Validation Loss: 1.0768429040908813\n",
      "Epoch 8386: Training Loss: 0.11406050125757854 Validation Loss: 1.077249526977539\n",
      "Epoch 8387: Training Loss: 0.11371489117542903 Validation Loss: 1.0766063928604126\n",
      "Epoch 8388: Training Loss: 0.1142927035689354 Validation Loss: 1.0765354633331299\n",
      "Epoch 8389: Training Loss: 0.11498410254716873 Validation Loss: 1.0764440298080444\n",
      "Epoch 8390: Training Loss: 0.11525643865267436 Validation Loss: 1.0760018825531006\n",
      "Epoch 8391: Training Loss: 0.11455805599689484 Validation Loss: 1.0765957832336426\n",
      "Epoch 8392: Training Loss: 0.11426722258329391 Validation Loss: 1.0772446393966675\n",
      "Epoch 8393: Training Loss: 0.1141350269317627 Validation Loss: 1.076722264289856\n",
      "Epoch 8394: Training Loss: 0.11412578076124191 Validation Loss: 1.0769522190093994\n",
      "Epoch 8395: Training Loss: 0.11405237267414729 Validation Loss: 1.076933741569519\n",
      "Epoch 8396: Training Loss: 0.11440278341372807 Validation Loss: 1.077256441116333\n",
      "Epoch 8397: Training Loss: 0.11416391283273697 Validation Loss: 1.0772526264190674\n",
      "Epoch 8398: Training Loss: 0.11443894604841869 Validation Loss: 1.077874779701233\n",
      "Epoch 8399: Training Loss: 0.1145569458603859 Validation Loss: 1.0783214569091797\n",
      "Epoch 8400: Training Loss: 0.11388486375411351 Validation Loss: 1.0788910388946533\n",
      "Epoch 8401: Training Loss: 0.11393571893374126 Validation Loss: 1.079329490661621\n",
      "Epoch 8402: Training Loss: 0.11430305242538452 Validation Loss: 1.0785118341445923\n",
      "Epoch 8403: Training Loss: 0.11381811400254567 Validation Loss: 1.0781192779541016\n",
      "Epoch 8404: Training Loss: 0.11390551676352818 Validation Loss: 1.0770307779312134\n",
      "Epoch 8405: Training Loss: 0.11368034531672795 Validation Loss: 1.0769424438476562\n",
      "Epoch 8406: Training Loss: 0.11419288565715154 Validation Loss: 1.0784008502960205\n",
      "Epoch 8407: Training Loss: 0.11396848162015279 Validation Loss: 1.0789545774459839\n",
      "Epoch 8408: Training Loss: 0.11447519063949585 Validation Loss: 1.0780346393585205\n",
      "Epoch 8409: Training Loss: 0.11399143437544505 Validation Loss: 1.077082633972168\n",
      "Epoch 8410: Training Loss: 0.11420253912607829 Validation Loss: 1.0770416259765625\n",
      "Epoch 8411: Training Loss: 0.11397323509057362 Validation Loss: 1.0783002376556396\n",
      "Epoch 8412: Training Loss: 0.11563306798537572 Validation Loss: 1.078331708908081\n",
      "Epoch 8413: Training Loss: 0.11407461265722911 Validation Loss: 1.0783560276031494\n",
      "Epoch 8414: Training Loss: 0.11446134994427364 Validation Loss: 1.0784869194030762\n",
      "Epoch 8415: Training Loss: 0.11385273933410645 Validation Loss: 1.0780080556869507\n",
      "Epoch 8416: Training Loss: 0.1138211836417516 Validation Loss: 1.0773545503616333\n",
      "Epoch 8417: Training Loss: 0.11355155954758327 Validation Loss: 1.07729971408844\n",
      "Epoch 8418: Training Loss: 0.11410154898961385 Validation Loss: 1.0774075984954834\n",
      "Epoch 8419: Training Loss: 0.11410665015379588 Validation Loss: 1.0792901515960693\n",
      "Epoch 8420: Training Loss: 0.11385302742322286 Validation Loss: 1.0788689851760864\n",
      "Epoch 8421: Training Loss: 0.11378523955742519 Validation Loss: 1.0786285400390625\n",
      "Epoch 8422: Training Loss: 0.11405261854330699 Validation Loss: 1.0786464214324951\n",
      "Epoch 8423: Training Loss: 0.11439135919014613 Validation Loss: 1.0789085626602173\n",
      "Epoch 8424: Training Loss: 0.11385126908620198 Validation Loss: 1.0786160230636597\n",
      "Epoch 8425: Training Loss: 0.11503142863512039 Validation Loss: 1.0788681507110596\n",
      "Epoch 8426: Training Loss: 0.11425478508075078 Validation Loss: 1.0796924829483032\n",
      "Epoch 8427: Training Loss: 0.11392993728319804 Validation Loss: 1.0788575410842896\n",
      "Epoch 8428: Training Loss: 0.11364688475926717 Validation Loss: 1.078747034072876\n",
      "Epoch 8429: Training Loss: 0.11436857283115387 Validation Loss: 1.078372836112976\n",
      "Epoch 8430: Training Loss: 0.11427675684293111 Validation Loss: 1.0782338380813599\n",
      "Epoch 8431: Training Loss: 0.1134977216521899 Validation Loss: 1.077855110168457\n",
      "Epoch 8432: Training Loss: 0.11416082829236984 Validation Loss: 1.078413963317871\n",
      "Epoch 8433: Training Loss: 0.11377299825350444 Validation Loss: 1.0785270929336548\n",
      "Epoch 8434: Training Loss: 0.11364202449719112 Validation Loss: 1.078762412071228\n",
      "Epoch 8435: Training Loss: 0.11459514498710632 Validation Loss: 1.0788108110427856\n",
      "Epoch 8436: Training Loss: 0.1137151022752126 Validation Loss: 1.0786184072494507\n",
      "Epoch 8437: Training Loss: 0.11417852838834126 Validation Loss: 1.077449917793274\n",
      "Epoch 8438: Training Loss: 0.1136852999528249 Validation Loss: 1.0772382020950317\n",
      "Epoch 8439: Training Loss: 0.1134575034181277 Validation Loss: 1.0781781673431396\n",
      "Epoch 8440: Training Loss: 0.11324452857176463 Validation Loss: 1.0784422159194946\n",
      "Epoch 8441: Training Loss: 0.11341869334379832 Validation Loss: 1.079103708267212\n",
      "Epoch 8442: Training Loss: 0.11347311486800511 Validation Loss: 1.0795360803604126\n",
      "Epoch 8443: Training Loss: 0.11423568924268086 Validation Loss: 1.079689860343933\n",
      "Epoch 8444: Training Loss: 0.11348193138837814 Validation Loss: 1.0791802406311035\n",
      "Epoch 8445: Training Loss: 0.11350867648919423 Validation Loss: 1.0789304971694946\n",
      "Epoch 8446: Training Loss: 0.11367806047201157 Validation Loss: 1.0785362720489502\n",
      "Epoch 8447: Training Loss: 0.11359262714783351 Validation Loss: 1.0791059732437134\n",
      "Epoch 8448: Training Loss: 0.11404577394326527 Validation Loss: 1.0787652730941772\n",
      "Epoch 8449: Training Loss: 0.11348759631315868 Validation Loss: 1.0790486335754395\n",
      "Epoch 8450: Training Loss: 0.11342810839414597 Validation Loss: 1.079338550567627\n",
      "Epoch 8451: Training Loss: 0.1133450319369634 Validation Loss: 1.0789154767990112\n",
      "Epoch 8452: Training Loss: 0.11391595502694447 Validation Loss: 1.0800726413726807\n",
      "Epoch 8453: Training Loss: 0.11389774580796559 Validation Loss: 1.0793124437332153\n",
      "Epoch 8454: Training Loss: 0.11376970012982686 Validation Loss: 1.0799014568328857\n",
      "Epoch 8455: Training Loss: 0.11338245123624802 Validation Loss: 1.0800954103469849\n",
      "Epoch 8456: Training Loss: 0.11346253007650375 Validation Loss: 1.0791107416152954\n",
      "Epoch 8457: Training Loss: 0.11495488633712132 Validation Loss: 1.0785678625106812\n",
      "Epoch 8458: Training Loss: 0.11373484879732132 Validation Loss: 1.0787959098815918\n",
      "Epoch 8459: Training Loss: 0.11376757174730301 Validation Loss: 1.079198956489563\n",
      "Epoch 8460: Training Loss: 0.11357877155145009 Validation Loss: 1.0780194997787476\n",
      "Epoch 8461: Training Loss: 0.11332872758309047 Validation Loss: 1.0781981945037842\n",
      "Epoch 8462: Training Loss: 0.11394148071606953 Validation Loss: 1.079712152481079\n",
      "Epoch 8463: Training Loss: 0.11367074648539226 Validation Loss: 1.080458402633667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8464: Training Loss: 0.11438452949126561 Validation Loss: 1.0797724723815918\n",
      "Epoch 8465: Training Loss: 0.11354117840528488 Validation Loss: 1.078668236732483\n",
      "Epoch 8466: Training Loss: 0.11324885735909145 Validation Loss: 1.078640341758728\n",
      "Epoch 8467: Training Loss: 0.11385176579157512 Validation Loss: 1.0790214538574219\n",
      "Epoch 8468: Training Loss: 0.11356569826602936 Validation Loss: 1.0797533988952637\n",
      "Epoch 8469: Training Loss: 0.11321720480918884 Validation Loss: 1.0810930728912354\n",
      "Epoch 8470: Training Loss: 0.11322806030511856 Validation Loss: 1.081207275390625\n",
      "Epoch 8471: Training Loss: 0.11347621927658717 Validation Loss: 1.080438494682312\n",
      "Epoch 8472: Training Loss: 0.11316653341054916 Validation Loss: 1.0786590576171875\n",
      "Epoch 8473: Training Loss: 0.11338930080334346 Validation Loss: 1.0779364109039307\n",
      "Epoch 8474: Training Loss: 0.11431378374497096 Validation Loss: 1.078324317932129\n",
      "Epoch 8475: Training Loss: 0.11347373326619466 Validation Loss: 1.0791691541671753\n",
      "Epoch 8476: Training Loss: 0.11301550020774205 Validation Loss: 1.0802415609359741\n",
      "Epoch 8477: Training Loss: 0.11355346192916234 Validation Loss: 1.080910563468933\n",
      "Epoch 8478: Training Loss: 0.11300552388032277 Validation Loss: 1.0800272226333618\n",
      "Epoch 8479: Training Loss: 0.11320101221402486 Validation Loss: 1.079377293586731\n",
      "Epoch 8480: Training Loss: 0.11313813676436742 Validation Loss: 1.0799295902252197\n",
      "Epoch 8481: Training Loss: 0.11327069749434789 Validation Loss: 1.0803850889205933\n",
      "Epoch 8482: Training Loss: 0.11317760497331619 Validation Loss: 1.0800950527191162\n",
      "Epoch 8483: Training Loss: 0.11316330482562383 Validation Loss: 1.07980215549469\n",
      "Epoch 8484: Training Loss: 0.11319717019796371 Validation Loss: 1.0798323154449463\n",
      "Epoch 8485: Training Loss: 0.11354969690243404 Validation Loss: 1.0809836387634277\n",
      "Epoch 8486: Training Loss: 0.11374938984711964 Validation Loss: 1.0809675455093384\n",
      "Epoch 8487: Training Loss: 0.11316038419802983 Validation Loss: 1.081642508506775\n",
      "Epoch 8488: Training Loss: 0.11287204176187515 Validation Loss: 1.080514907836914\n",
      "Epoch 8489: Training Loss: 0.11322173724571864 Validation Loss: 1.0798609256744385\n",
      "Epoch 8490: Training Loss: 0.11331760883331299 Validation Loss: 1.0798369646072388\n",
      "Epoch 8491: Training Loss: 0.11330576489369075 Validation Loss: 1.079864740371704\n",
      "Epoch 8492: Training Loss: 0.11316669235626857 Validation Loss: 1.0802019834518433\n",
      "Epoch 8493: Training Loss: 0.1135944128036499 Validation Loss: 1.080776333808899\n",
      "Epoch 8494: Training Loss: 0.1131324569384257 Validation Loss: 1.0804511308670044\n",
      "Epoch 8495: Training Loss: 0.11248229692379634 Validation Loss: 1.0803335905075073\n",
      "Epoch 8496: Training Loss: 0.1145843689640363 Validation Loss: 1.0796183347702026\n",
      "Epoch 8497: Training Loss: 0.11338398357232411 Validation Loss: 1.079879879951477\n",
      "Epoch 8498: Training Loss: 0.11302848656972249 Validation Loss: 1.0798773765563965\n",
      "Epoch 8499: Training Loss: 0.112862229347229 Validation Loss: 1.0807732343673706\n",
      "Epoch 8500: Training Loss: 0.11339910576740901 Validation Loss: 1.0817381143569946\n",
      "Epoch 8501: Training Loss: 0.11295114457607269 Validation Loss: 1.082017421722412\n",
      "Epoch 8502: Training Loss: 0.1130010982354482 Validation Loss: 1.0815987586975098\n",
      "Epoch 8503: Training Loss: 0.11304424454768498 Validation Loss: 1.0799871683120728\n",
      "Epoch 8504: Training Loss: 0.11298118034998576 Validation Loss: 1.079533576965332\n",
      "Epoch 8505: Training Loss: 0.11319782088200252 Validation Loss: 1.0811312198638916\n",
      "Epoch 8506: Training Loss: 0.11356453845898311 Validation Loss: 1.0808773040771484\n",
      "Epoch 8507: Training Loss: 0.11303393046061198 Validation Loss: 1.0806005001068115\n",
      "Epoch 8508: Training Loss: 0.11204024652640025 Validation Loss: 1.0802907943725586\n",
      "Epoch 8509: Training Loss: 0.11287921667098999 Validation Loss: 1.0806695222854614\n",
      "Epoch 8510: Training Loss: 0.11255259563525517 Validation Loss: 1.0817186832427979\n",
      "Epoch 8511: Training Loss: 0.11313103636105855 Validation Loss: 1.0820895433425903\n",
      "Epoch 8512: Training Loss: 0.11304796735445659 Validation Loss: 1.081114649772644\n",
      "Epoch 8513: Training Loss: 0.11295756946007411 Validation Loss: 1.0801070928573608\n",
      "Epoch 8514: Training Loss: 0.1128627136349678 Validation Loss: 1.0806766748428345\n",
      "Epoch 8515: Training Loss: 0.11269080638885498 Validation Loss: 1.080488681793213\n",
      "Epoch 8516: Training Loss: 0.11336904764175415 Validation Loss: 1.0812374353408813\n",
      "Epoch 8517: Training Loss: 0.11314302931229274 Validation Loss: 1.0808689594268799\n",
      "Epoch 8518: Training Loss: 0.1125975822408994 Validation Loss: 1.0809460878372192\n",
      "Epoch 8519: Training Loss: 0.11275087296962738 Validation Loss: 1.081100344657898\n",
      "Epoch 8520: Training Loss: 0.11315607279539108 Validation Loss: 1.081077218055725\n",
      "Epoch 8521: Training Loss: 0.1129524012406667 Validation Loss: 1.0805549621582031\n",
      "Epoch 8522: Training Loss: 0.11309826125701268 Validation Loss: 1.0807154178619385\n",
      "Epoch 8523: Training Loss: 0.11244603743155797 Validation Loss: 1.0808889865875244\n",
      "Epoch 8524: Training Loss: 0.11314050604899724 Validation Loss: 1.0811197757720947\n",
      "Epoch 8525: Training Loss: 0.11288055777549744 Validation Loss: 1.0821945667266846\n",
      "Epoch 8526: Training Loss: 0.11277159055074056 Validation Loss: 1.0820425748825073\n",
      "Epoch 8527: Training Loss: 0.11295843621095021 Validation Loss: 1.0813833475112915\n",
      "Epoch 8528: Training Loss: 0.11274107297261556 Validation Loss: 1.080557107925415\n",
      "Epoch 8529: Training Loss: 0.11250683416922887 Validation Loss: 1.0812253952026367\n",
      "Epoch 8530: Training Loss: 0.11263065536816914 Validation Loss: 1.0809180736541748\n",
      "Epoch 8531: Training Loss: 0.11301823953787486 Validation Loss: 1.0808396339416504\n",
      "Epoch 8532: Training Loss: 0.11219078799088796 Validation Loss: 1.0812883377075195\n",
      "Epoch 8533: Training Loss: 0.11312095820903778 Validation Loss: 1.0809688568115234\n",
      "Epoch 8534: Training Loss: 0.11284001171588898 Validation Loss: 1.0813097953796387\n",
      "Epoch 8535: Training Loss: 0.11295948674281438 Validation Loss: 1.08077871799469\n",
      "Epoch 8536: Training Loss: 0.1130334734916687 Validation Loss: 1.0812371969223022\n",
      "Epoch 8537: Training Loss: 0.11290308584769566 Validation Loss: 1.0823705196380615\n",
      "Epoch 8538: Training Loss: 0.11280511071284612 Validation Loss: 1.0823547840118408\n",
      "Epoch 8539: Training Loss: 0.11238626142342885 Validation Loss: 1.082639455795288\n",
      "Epoch 8540: Training Loss: 0.11254618316888809 Validation Loss: 1.082931399345398\n",
      "Epoch 8541: Training Loss: 0.11261443793773651 Validation Loss: 1.0821038484573364\n",
      "Epoch 8542: Training Loss: 0.11273473749558131 Validation Loss: 1.0819406509399414\n",
      "Epoch 8543: Training Loss: 0.11250476042429607 Validation Loss: 1.0812127590179443\n",
      "Epoch 8544: Training Loss: 0.1128159910440445 Validation Loss: 1.0804810523986816\n",
      "Epoch 8545: Training Loss: 0.11220322052637736 Validation Loss: 1.0810502767562866\n",
      "Epoch 8546: Training Loss: 0.11281421780586243 Validation Loss: 1.0818095207214355\n",
      "Epoch 8547: Training Loss: 0.11287781596183777 Validation Loss: 1.0824730396270752\n",
      "Epoch 8548: Training Loss: 0.1122721756498019 Validation Loss: 1.0819979906082153\n",
      "Epoch 8549: Training Loss: 0.11292601376771927 Validation Loss: 1.081000566482544\n",
      "Epoch 8550: Training Loss: 0.11241495857636134 Validation Loss: 1.081395149230957\n",
      "Epoch 8551: Training Loss: 0.11328745633363724 Validation Loss: 1.0816380977630615\n",
      "Epoch 8552: Training Loss: 0.11251188566287358 Validation Loss: 1.0817757844924927\n",
      "Epoch 8553: Training Loss: 0.11247294147809346 Validation Loss: 1.082556128501892\n",
      "Epoch 8554: Training Loss: 0.11257072538137436 Validation Loss: 1.0821280479431152\n",
      "Epoch 8555: Training Loss: 0.11256551494201024 Validation Loss: 1.0827072858810425\n",
      "Epoch 8556: Training Loss: 0.11266039311885834 Validation Loss: 1.0821691751480103\n",
      "Epoch 8557: Training Loss: 0.11191220333178838 Validation Loss: 1.082374095916748\n",
      "Epoch 8558: Training Loss: 0.11237964779138565 Validation Loss: 1.0826548337936401\n",
      "Epoch 8559: Training Loss: 0.11214054624239604 Validation Loss: 1.0825477838516235\n",
      "Epoch 8560: Training Loss: 0.11223492274681728 Validation Loss: 1.0811635255813599\n",
      "Epoch 8561: Training Loss: 0.11246553560098012 Validation Loss: 1.0811585187911987\n",
      "Epoch 8562: Training Loss: 0.11266490072011948 Validation Loss: 1.0807654857635498\n",
      "Epoch 8563: Training Loss: 0.11204567303260167 Validation Loss: 1.0814405679702759\n",
      "Epoch 8564: Training Loss: 0.11295707275470097 Validation Loss: 1.0838013887405396\n",
      "Epoch 8565: Training Loss: 0.1123738835255305 Validation Loss: 1.083778977394104\n",
      "Epoch 8566: Training Loss: 0.11244208614031474 Validation Loss: 1.0832805633544922\n",
      "Epoch 8567: Training Loss: 0.11255841950575511 Validation Loss: 1.0823155641555786\n",
      "Epoch 8568: Training Loss: 0.11245685815811157 Validation Loss: 1.0812351703643799\n",
      "Epoch 8569: Training Loss: 0.11209321767091751 Validation Loss: 1.0817134380340576\n",
      "Epoch 8570: Training Loss: 0.1121814598639806 Validation Loss: 1.0822160243988037\n",
      "Epoch 8571: Training Loss: 0.11213868111371994 Validation Loss: 1.0823769569396973\n",
      "Epoch 8572: Training Loss: 0.11222392817338307 Validation Loss: 1.0828441381454468\n",
      "Epoch 8573: Training Loss: 0.11246070762475331 Validation Loss: 1.083407998085022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8574: Training Loss: 0.11294074604908626 Validation Loss: 1.0833542346954346\n",
      "Epoch 8575: Training Loss: 0.11256206780672073 Validation Loss: 1.0830429792404175\n",
      "Epoch 8576: Training Loss: 0.11230382819970448 Validation Loss: 1.0835506916046143\n",
      "Epoch 8577: Training Loss: 0.11272919923067093 Validation Loss: 1.0833103656768799\n",
      "Epoch 8578: Training Loss: 0.1119944378733635 Validation Loss: 1.0825589895248413\n",
      "Epoch 8579: Training Loss: 0.11225145310163498 Validation Loss: 1.0824666023254395\n",
      "Epoch 8580: Training Loss: 0.11226257185141246 Validation Loss: 1.0814710855484009\n",
      "Epoch 8581: Training Loss: 0.11211919287840526 Validation Loss: 1.0815088748931885\n",
      "Epoch 8582: Training Loss: 0.1125950887799263 Validation Loss: 1.0822945833206177\n",
      "Epoch 8583: Training Loss: 0.1120697632431984 Validation Loss: 1.0829704999923706\n",
      "Epoch 8584: Training Loss: 0.11213519424200058 Validation Loss: 1.0833029747009277\n",
      "Epoch 8585: Training Loss: 0.11235394328832626 Validation Loss: 1.083693027496338\n",
      "Epoch 8586: Training Loss: 0.11234281708796819 Validation Loss: 1.0844013690948486\n",
      "Epoch 8587: Training Loss: 0.11252014090617497 Validation Loss: 1.084331750869751\n",
      "Epoch 8588: Training Loss: 0.1123407706618309 Validation Loss: 1.0846389532089233\n",
      "Epoch 8589: Training Loss: 0.11221630126237869 Validation Loss: 1.084512710571289\n",
      "Epoch 8590: Training Loss: 0.112081545094649 Validation Loss: 1.0837841033935547\n",
      "Epoch 8591: Training Loss: 0.11208493014176686 Validation Loss: 1.0833405256271362\n",
      "Epoch 8592: Training Loss: 0.11239953835805257 Validation Loss: 1.0830836296081543\n",
      "Epoch 8593: Training Loss: 0.11255111545324326 Validation Loss: 1.083460807800293\n",
      "Epoch 8594: Training Loss: 0.11194183180729549 Validation Loss: 1.0832804441452026\n",
      "Epoch 8595: Training Loss: 0.1120499720176061 Validation Loss: 1.0828466415405273\n",
      "Epoch 8596: Training Loss: 0.11300529291232426 Validation Loss: 1.0819568634033203\n",
      "Epoch 8597: Training Loss: 0.11228473236163457 Validation Loss: 1.0824793577194214\n",
      "Epoch 8598: Training Loss: 0.11233612646659215 Validation Loss: 1.0834081172943115\n",
      "Epoch 8599: Training Loss: 0.11243809511264165 Validation Loss: 1.0823208093643188\n",
      "Epoch 8600: Training Loss: 0.11242420474688213 Validation Loss: 1.0822207927703857\n",
      "Epoch 8601: Training Loss: 0.11197529236475627 Validation Loss: 1.0825610160827637\n",
      "Epoch 8602: Training Loss: 0.11220757166544597 Validation Loss: 1.0838960409164429\n",
      "Epoch 8603: Training Loss: 0.11168123284975688 Validation Loss: 1.0838593244552612\n",
      "Epoch 8604: Training Loss: 0.11223167926073074 Validation Loss: 1.083959698677063\n",
      "Epoch 8605: Training Loss: 0.11188888549804688 Validation Loss: 1.0840411186218262\n",
      "Epoch 8606: Training Loss: 0.11176924655834834 Validation Loss: 1.0845799446105957\n",
      "Epoch 8607: Training Loss: 0.11178510636091232 Validation Loss: 1.0847768783569336\n",
      "Epoch 8608: Training Loss: 0.11162831137577693 Validation Loss: 1.0844900608062744\n",
      "Epoch 8609: Training Loss: 0.11200464020172755 Validation Loss: 1.0840630531311035\n",
      "Epoch 8610: Training Loss: 0.11281814177831014 Validation Loss: 1.0831917524337769\n",
      "Epoch 8611: Training Loss: 0.11216936508814494 Validation Loss: 1.082996129989624\n",
      "Epoch 8612: Training Loss: 0.11189013471206029 Validation Loss: 1.0828654766082764\n",
      "Epoch 8613: Training Loss: 0.11207613845666249 Validation Loss: 1.0839771032333374\n",
      "Epoch 8614: Training Loss: 0.11195884644985199 Validation Loss: 1.0838537216186523\n",
      "Epoch 8615: Training Loss: 0.11139387389024098 Validation Loss: 1.0837692022323608\n",
      "Epoch 8616: Training Loss: 0.11176721751689911 Validation Loss: 1.0828120708465576\n",
      "Epoch 8617: Training Loss: 0.11229575922091801 Validation Loss: 1.0833848714828491\n",
      "Epoch 8618: Training Loss: 0.11309219648440678 Validation Loss: 1.0838223695755005\n",
      "Epoch 8619: Training Loss: 0.11258569111426671 Validation Loss: 1.083340048789978\n",
      "Epoch 8620: Training Loss: 0.11188933749993642 Validation Loss: 1.0844484567642212\n",
      "Epoch 8621: Training Loss: 0.1120398963491122 Validation Loss: 1.0848876237869263\n",
      "Epoch 8622: Training Loss: 0.11349911987781525 Validation Loss: 1.0851272344589233\n",
      "Epoch 8623: Training Loss: 0.11256490151087443 Validation Loss: 1.0852748155593872\n",
      "Epoch 8624: Training Loss: 0.11154100050528844 Validation Loss: 1.0849984884262085\n",
      "Epoch 8625: Training Loss: 0.11164514472087224 Validation Loss: 1.0848771333694458\n",
      "Epoch 8626: Training Loss: 0.11344065020481746 Validation Loss: 1.0841161012649536\n",
      "Epoch 8627: Training Loss: 0.11211857199668884 Validation Loss: 1.0836434364318848\n",
      "Epoch 8628: Training Loss: 0.11134347319602966 Validation Loss: 1.0834376811981201\n",
      "Epoch 8629: Training Loss: 0.1117096021771431 Validation Loss: 1.0840082168579102\n",
      "Epoch 8630: Training Loss: 0.11188654601573944 Validation Loss: 1.0855721235275269\n",
      "Epoch 8631: Training Loss: 0.1119324763615926 Validation Loss: 1.0855045318603516\n",
      "Epoch 8632: Training Loss: 0.11209943393866222 Validation Loss: 1.0855796337127686\n",
      "Epoch 8633: Training Loss: 0.11151500294605891 Validation Loss: 1.085313320159912\n",
      "Epoch 8634: Training Loss: 0.1118462805946668 Validation Loss: 1.0837883949279785\n",
      "Epoch 8635: Training Loss: 0.11191980540752411 Validation Loss: 1.0835171937942505\n",
      "Epoch 8636: Training Loss: 0.11137946446736653 Validation Loss: 1.0839544534683228\n",
      "Epoch 8637: Training Loss: 0.11164203782876332 Validation Loss: 1.084267258644104\n",
      "Epoch 8638: Training Loss: 0.11175523946682613 Validation Loss: 1.0847305059432983\n",
      "Epoch 8639: Training Loss: 0.1114426131049792 Validation Loss: 1.0837903022766113\n",
      "Epoch 8640: Training Loss: 0.11169784267743428 Validation Loss: 1.0825202465057373\n",
      "Epoch 8641: Training Loss: 0.11124133070309956 Validation Loss: 1.0829603672027588\n",
      "Epoch 8642: Training Loss: 0.11213327944278717 Validation Loss: 1.0845341682434082\n",
      "Epoch 8643: Training Loss: 0.11219740907351176 Validation Loss: 1.0852489471435547\n",
      "Epoch 8644: Training Loss: 0.11192599684000015 Validation Loss: 1.0844422578811646\n",
      "Epoch 8645: Training Loss: 0.11062684406836827 Validation Loss: 1.084102988243103\n",
      "Epoch 8646: Training Loss: 0.11160519470771153 Validation Loss: 1.0845844745635986\n",
      "Epoch 8647: Training Loss: 0.1114150732755661 Validation Loss: 1.0845359563827515\n",
      "Epoch 8648: Training Loss: 0.11158457398414612 Validation Loss: 1.084119200706482\n",
      "Epoch 8649: Training Loss: 0.11162629475196202 Validation Loss: 1.0849215984344482\n",
      "Epoch 8650: Training Loss: 0.11179718126853307 Validation Loss: 1.086410403251648\n",
      "Epoch 8651: Training Loss: 0.11174586415290833 Validation Loss: 1.0858819484710693\n",
      "Epoch 8652: Training Loss: 0.11138162761926651 Validation Loss: 1.0857115983963013\n",
      "Epoch 8653: Training Loss: 0.11140371610720952 Validation Loss: 1.0852419137954712\n",
      "Epoch 8654: Training Loss: 0.11119835823774338 Validation Loss: 1.0848456621170044\n",
      "Epoch 8655: Training Loss: 0.11139049381017685 Validation Loss: 1.0843192338943481\n",
      "Epoch 8656: Training Loss: 0.11209355294704437 Validation Loss: 1.084529995918274\n",
      "Epoch 8657: Training Loss: 0.11164654543002446 Validation Loss: 1.0849841833114624\n",
      "Epoch 8658: Training Loss: 0.11210123697916667 Validation Loss: 1.0853068828582764\n",
      "Epoch 8659: Training Loss: 0.1114084521929423 Validation Loss: 1.085977554321289\n",
      "Epoch 8660: Training Loss: 0.11180124680201213 Validation Loss: 1.0866247415542603\n",
      "Epoch 8661: Training Loss: 0.11241918553908666 Validation Loss: 1.0861939191818237\n",
      "Epoch 8662: Training Loss: 0.11108386268218358 Validation Loss: 1.084805965423584\n",
      "Epoch 8663: Training Loss: 0.11165032039086024 Validation Loss: 1.0837829113006592\n",
      "Epoch 8664: Training Loss: 0.11134904871384303 Validation Loss: 1.0839089155197144\n",
      "Epoch 8665: Training Loss: 0.11156382163365682 Validation Loss: 1.0841368436813354\n",
      "Epoch 8666: Training Loss: 0.11136453598737717 Validation Loss: 1.0844717025756836\n",
      "Epoch 8667: Training Loss: 0.11152540644009908 Validation Loss: 1.0858495235443115\n",
      "Epoch 8668: Training Loss: 0.11155224094788234 Validation Loss: 1.085913062095642\n",
      "Epoch 8669: Training Loss: 0.11093502988417943 Validation Loss: 1.0857585668563843\n",
      "Epoch 8670: Training Loss: 0.11151115347941716 Validation Loss: 1.084889531135559\n",
      "Epoch 8671: Training Loss: 0.11140503734350204 Validation Loss: 1.0843524932861328\n",
      "Epoch 8672: Training Loss: 0.11209908872842789 Validation Loss: 1.0850775241851807\n",
      "Epoch 8673: Training Loss: 0.11255877713362376 Validation Loss: 1.0860655307769775\n",
      "Epoch 8674: Training Loss: 0.11112482349077861 Validation Loss: 1.085748314857483\n",
      "Epoch 8675: Training Loss: 0.11201793203751247 Validation Loss: 1.0861376523971558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8676: Training Loss: 0.11138402918974559 Validation Loss: 1.0857645273208618\n",
      "Epoch 8677: Training Loss: 0.11112813154856364 Validation Loss: 1.0854774713516235\n",
      "Epoch 8678: Training Loss: 0.11112900326649348 Validation Loss: 1.0856326818466187\n",
      "Epoch 8679: Training Loss: 0.11151889711618423 Validation Loss: 1.0858582258224487\n",
      "Epoch 8680: Training Loss: 0.11081868161757787 Validation Loss: 1.0857844352722168\n",
      "Epoch 8681: Training Loss: 0.11181462059418361 Validation Loss: 1.0853315591812134\n",
      "Epoch 8682: Training Loss: 0.11149701724449794 Validation Loss: 1.0852075815200806\n",
      "Epoch 8683: Training Loss: 0.11126777281363805 Validation Loss: 1.0855921506881714\n",
      "Epoch 8684: Training Loss: 0.11048879971106847 Validation Loss: 1.0859935283660889\n",
      "Epoch 8685: Training Loss: 0.11112291365861893 Validation Loss: 1.0867373943328857\n",
      "Epoch 8686: Training Loss: 0.1113153447707494 Validation Loss: 1.0870938301086426\n",
      "Epoch 8687: Training Loss: 0.11209601660569508 Validation Loss: 1.086897611618042\n",
      "Epoch 8688: Training Loss: 0.11089719583590825 Validation Loss: 1.0863840579986572\n",
      "Epoch 8689: Training Loss: 0.11204735934734344 Validation Loss: 1.0871978998184204\n",
      "Epoch 8690: Training Loss: 0.11080782115459442 Validation Loss: 1.086175799369812\n",
      "Epoch 8691: Training Loss: 0.11112482845783234 Validation Loss: 1.0861512422561646\n",
      "Epoch 8692: Training Loss: 0.11123906075954437 Validation Loss: 1.0859041213989258\n",
      "Epoch 8693: Training Loss: 0.11113176743189494 Validation Loss: 1.085565209388733\n",
      "Epoch 8694: Training Loss: 0.11094747483730316 Validation Loss: 1.0864231586456299\n",
      "Epoch 8695: Training Loss: 0.11199073990186055 Validation Loss: 1.0875146389007568\n",
      "Epoch 8696: Training Loss: 0.1111844206849734 Validation Loss: 1.0864468812942505\n",
      "Epoch 8697: Training Loss: 0.11175107459227245 Validation Loss: 1.0871927738189697\n",
      "Epoch 8698: Training Loss: 0.11066951602697372 Validation Loss: 1.0877102613449097\n",
      "Epoch 8699: Training Loss: 0.11225641270478566 Validation Loss: 1.087165355682373\n",
      "Epoch 8700: Training Loss: 0.11105457196633021 Validation Loss: 1.0867894887924194\n",
      "Epoch 8701: Training Loss: 0.11076535781224568 Validation Loss: 1.0859613418579102\n",
      "Epoch 8702: Training Loss: 0.11102757354577382 Validation Loss: 1.0859084129333496\n",
      "Epoch 8703: Training Loss: 0.11134303609530131 Validation Loss: 1.0865790843963623\n",
      "Epoch 8704: Training Loss: 0.11129643023014069 Validation Loss: 1.0871655941009521\n",
      "Epoch 8705: Training Loss: 0.11116698135932286 Validation Loss: 1.0888211727142334\n",
      "Epoch 8706: Training Loss: 0.11160365988810857 Validation Loss: 1.0870062112808228\n",
      "Epoch 8707: Training Loss: 0.11112926652034123 Validation Loss: 1.0864015817642212\n",
      "Epoch 8708: Training Loss: 0.11092796425024669 Validation Loss: 1.0864449739456177\n",
      "Epoch 8709: Training Loss: 0.11091798543930054 Validation Loss: 1.086607575416565\n",
      "Epoch 8710: Training Loss: 0.11090017110109329 Validation Loss: 1.0862789154052734\n",
      "Epoch 8711: Training Loss: 0.1117021714647611 Validation Loss: 1.0867143869400024\n",
      "Epoch 8712: Training Loss: 0.11155457546313603 Validation Loss: 1.0869331359863281\n",
      "Epoch 8713: Training Loss: 0.11104685068130493 Validation Loss: 1.0865694284439087\n",
      "Epoch 8714: Training Loss: 0.11104844013849895 Validation Loss: 1.0871270895004272\n",
      "Epoch 8715: Training Loss: 0.11085487653811772 Validation Loss: 1.08748197555542\n",
      "Epoch 8716: Training Loss: 0.11089630673329036 Validation Loss: 1.0865730047225952\n",
      "Epoch 8717: Training Loss: 0.11096107463041942 Validation Loss: 1.085935354232788\n",
      "Epoch 8718: Training Loss: 0.11094583322604497 Validation Loss: 1.086105227470398\n",
      "Epoch 8719: Training Loss: 0.1108512207865715 Validation Loss: 1.08595609664917\n",
      "Epoch 8720: Training Loss: 0.11080051958560944 Validation Loss: 1.0865707397460938\n",
      "Epoch 8721: Training Loss: 0.11108424017826717 Validation Loss: 1.0866880416870117\n",
      "Epoch 8722: Training Loss: 0.11105012645324071 Validation Loss: 1.0875296592712402\n",
      "Epoch 8723: Training Loss: 0.11090302964051564 Validation Loss: 1.0879286527633667\n",
      "Epoch 8724: Training Loss: 0.11087226867675781 Validation Loss: 1.0872831344604492\n",
      "Epoch 8725: Training Loss: 0.11079215010007222 Validation Loss: 1.086545705795288\n",
      "Epoch 8726: Training Loss: 0.11107596009969711 Validation Loss: 1.086368441581726\n",
      "Epoch 8727: Training Loss: 0.1108557681242625 Validation Loss: 1.0873357057571411\n",
      "Epoch 8728: Training Loss: 0.11065144340197246 Validation Loss: 1.0874924659729004\n",
      "Epoch 8729: Training Loss: 0.1107805569966634 Validation Loss: 1.087255835533142\n",
      "Epoch 8730: Training Loss: 0.11093752831220627 Validation Loss: 1.0871074199676514\n",
      "Epoch 8731: Training Loss: 0.10989222923914592 Validation Loss: 1.0870146751403809\n",
      "Epoch 8732: Training Loss: 0.11092402786016464 Validation Loss: 1.086669683456421\n",
      "Epoch 8733: Training Loss: 0.1110022912422816 Validation Loss: 1.0875877141952515\n",
      "Epoch 8734: Training Loss: 0.11083739995956421 Validation Loss: 1.0885084867477417\n",
      "Epoch 8735: Training Loss: 0.11073931554953258 Validation Loss: 1.087953805923462\n",
      "Epoch 8736: Training Loss: 0.11025959253311157 Validation Loss: 1.0878485441207886\n",
      "Epoch 8737: Training Loss: 0.11064606159925461 Validation Loss: 1.0873051881790161\n",
      "Epoch 8738: Training Loss: 0.11067357659339905 Validation Loss: 1.087496280670166\n",
      "Epoch 8739: Training Loss: 0.11088659117619197 Validation Loss: 1.0875036716461182\n",
      "Epoch 8740: Training Loss: 0.1110508864124616 Validation Loss: 1.08729887008667\n",
      "Epoch 8741: Training Loss: 0.11061081538597743 Validation Loss: 1.0873665809631348\n",
      "Epoch 8742: Training Loss: 0.1106746718287468 Validation Loss: 1.0879026651382446\n",
      "Epoch 8743: Training Loss: 0.11169379701217015 Validation Loss: 1.088356852531433\n",
      "Epoch 8744: Training Loss: 0.11068194111188252 Validation Loss: 1.088329553604126\n",
      "Epoch 8745: Training Loss: 0.11050565044085185 Validation Loss: 1.0881446599960327\n",
      "Epoch 8746: Training Loss: 0.11088566482067108 Validation Loss: 1.087675929069519\n",
      "Epoch 8747: Training Loss: 0.11068465809027354 Validation Loss: 1.086995005607605\n",
      "Epoch 8748: Training Loss: 0.11083180457353592 Validation Loss: 1.0877186059951782\n",
      "Epoch 8749: Training Loss: 0.1108447661002477 Validation Loss: 1.0887105464935303\n",
      "Epoch 8750: Training Loss: 0.11096083869536717 Validation Loss: 1.0890330076217651\n",
      "Epoch 8751: Training Loss: 0.11069971323013306 Validation Loss: 1.0891368389129639\n",
      "Epoch 8752: Training Loss: 0.11063381781180699 Validation Loss: 1.0880593061447144\n",
      "Epoch 8753: Training Loss: 0.11094159136215846 Validation Loss: 1.0875275135040283\n",
      "Epoch 8754: Training Loss: 0.11072224378585815 Validation Loss: 1.0878899097442627\n",
      "Epoch 8755: Training Loss: 0.11056116720040639 Validation Loss: 1.0876256227493286\n",
      "Epoch 8756: Training Loss: 0.11056717236836751 Validation Loss: 1.0868711471557617\n",
      "Epoch 8757: Training Loss: 0.110774926841259 Validation Loss: 1.087038516998291\n",
      "Epoch 8758: Training Loss: 0.11072620749473572 Validation Loss: 1.087620496749878\n",
      "Epoch 8759: Training Loss: 0.11049184948205948 Validation Loss: 1.0875754356384277\n",
      "Epoch 8760: Training Loss: 0.11044641832510631 Validation Loss: 1.0883195400238037\n",
      "Epoch 8761: Training Loss: 0.11068041374286015 Validation Loss: 1.0883818864822388\n",
      "Epoch 8762: Training Loss: 0.11102668941020966 Validation Loss: 1.088988184928894\n",
      "Epoch 8763: Training Loss: 0.11047070970137914 Validation Loss: 1.0891214609146118\n",
      "Epoch 8764: Training Loss: 0.11034584293762843 Validation Loss: 1.0879435539245605\n",
      "Epoch 8765: Training Loss: 0.11058420687913895 Validation Loss: 1.0873533487319946\n",
      "Epoch 8766: Training Loss: 0.11045853048563004 Validation Loss: 1.0881835222244263\n",
      "Epoch 8767: Training Loss: 0.11072208235661189 Validation Loss: 1.0883383750915527\n",
      "Epoch 8768: Training Loss: 0.11014620214700699 Validation Loss: 1.0878963470458984\n",
      "Epoch 8769: Training Loss: 0.1104230135679245 Validation Loss: 1.0885299444198608\n",
      "Epoch 8770: Training Loss: 0.11066366980473201 Validation Loss: 1.0887081623077393\n",
      "Epoch 8771: Training Loss: 0.11005749305089314 Validation Loss: 1.0884872674942017\n",
      "Epoch 8772: Training Loss: 0.11023985594511032 Validation Loss: 1.088399887084961\n",
      "Epoch 8773: Training Loss: 0.11085767298936844 Validation Loss: 1.089963674545288\n",
      "Epoch 8774: Training Loss: 0.11019071936607361 Validation Loss: 1.0890693664550781\n",
      "Epoch 8775: Training Loss: 0.11087059726317723 Validation Loss: 1.0888482332229614\n",
      "Epoch 8776: Training Loss: 0.11018775403499603 Validation Loss: 1.089015245437622\n",
      "Epoch 8777: Training Loss: 0.11038327713807423 Validation Loss: 1.0885038375854492\n",
      "Epoch 8778: Training Loss: 0.11013283580541611 Validation Loss: 1.0884792804718018\n",
      "Epoch 8779: Training Loss: 0.11038848509391148 Validation Loss: 1.088518500328064\n",
      "Epoch 8780: Training Loss: 0.11037547141313553 Validation Loss: 1.0884301662445068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8781: Training Loss: 0.1111120084921519 Validation Loss: 1.0884652137756348\n",
      "Epoch 8782: Training Loss: 0.11036588996648788 Validation Loss: 1.0881729125976562\n",
      "Epoch 8783: Training Loss: 0.11078555136919022 Validation Loss: 1.088010311126709\n",
      "Epoch 8784: Training Loss: 0.11008067925771077 Validation Loss: 1.0890610218048096\n",
      "Epoch 8785: Training Loss: 0.1102657417456309 Validation Loss: 1.0889692306518555\n",
      "Epoch 8786: Training Loss: 0.1101320410768191 Validation Loss: 1.088598608970642\n",
      "Epoch 8787: Training Loss: 0.11002742250760396 Validation Loss: 1.0882761478424072\n",
      "Epoch 8788: Training Loss: 0.11022793253262837 Validation Loss: 1.0875359773635864\n",
      "Epoch 8789: Training Loss: 0.11018076539039612 Validation Loss: 1.0885969400405884\n",
      "Epoch 8790: Training Loss: 0.11085915317138036 Validation Loss: 1.089167594909668\n",
      "Epoch 8791: Training Loss: 0.11018184820810954 Validation Loss: 1.090000033378601\n",
      "Epoch 8792: Training Loss: 0.11032840857903163 Validation Loss: 1.0901720523834229\n",
      "Epoch 8793: Training Loss: 0.11027639110883077 Validation Loss: 1.0902060270309448\n",
      "Epoch 8794: Training Loss: 0.11124917368094127 Validation Loss: 1.0899320840835571\n",
      "Epoch 8795: Training Loss: 0.11005428681770961 Validation Loss: 1.0884044170379639\n",
      "Epoch 8796: Training Loss: 0.11053724090258281 Validation Loss: 1.0875715017318726\n",
      "Epoch 8797: Training Loss: 0.11025618016719818 Validation Loss: 1.0881823301315308\n",
      "Epoch 8798: Training Loss: 0.10989493131637573 Validation Loss: 1.0895342826843262\n",
      "Epoch 8799: Training Loss: 0.1103559782107671 Validation Loss: 1.090731143951416\n",
      "Epoch 8800: Training Loss: 0.11123322695493698 Validation Loss: 1.091201663017273\n",
      "Epoch 8801: Training Loss: 0.11017542829116185 Validation Loss: 1.0907058715820312\n",
      "Epoch 8802: Training Loss: 0.10999438911676407 Validation Loss: 1.0891807079315186\n",
      "Epoch 8803: Training Loss: 0.11005342503388722 Validation Loss: 1.0885354280471802\n",
      "Epoch 8804: Training Loss: 0.11014668146769206 Validation Loss: 1.0871546268463135\n",
      "Epoch 8805: Training Loss: 0.11005354672670364 Validation Loss: 1.088060736656189\n",
      "Epoch 8806: Training Loss: 0.11005827287832896 Validation Loss: 1.0889513492584229\n",
      "Epoch 8807: Training Loss: 0.10997651765743892 Validation Loss: 1.0894744396209717\n",
      "Epoch 8808: Training Loss: 0.1113791565100352 Validation Loss: 1.0898057222366333\n",
      "Epoch 8809: Training Loss: 0.11019901682933171 Validation Loss: 1.0903717279434204\n",
      "Epoch 8810: Training Loss: 0.11014692733685176 Validation Loss: 1.090104579925537\n",
      "Epoch 8811: Training Loss: 0.11001211901505788 Validation Loss: 1.08940589427948\n",
      "Epoch 8812: Training Loss: 0.11032980928818385 Validation Loss: 1.0884523391723633\n",
      "Epoch 8813: Training Loss: 0.11038467039664586 Validation Loss: 1.0884662866592407\n",
      "Epoch 8814: Training Loss: 0.11037408063809077 Validation Loss: 1.0893129110336304\n",
      "Epoch 8815: Training Loss: 0.11024087419112523 Validation Loss: 1.0899205207824707\n",
      "Epoch 8816: Training Loss: 0.11018388718366623 Validation Loss: 1.0901955366134644\n",
      "Epoch 8817: Training Loss: 0.10982142140467961 Validation Loss: 1.090856671333313\n",
      "Epoch 8818: Training Loss: 0.11013325055440266 Validation Loss: 1.0911896228790283\n",
      "Epoch 8819: Training Loss: 0.11045020322004954 Validation Loss: 1.0903781652450562\n",
      "Epoch 8820: Training Loss: 0.10948706418275833 Validation Loss: 1.0904256105422974\n",
      "Epoch 8821: Training Loss: 0.10981746514638265 Validation Loss: 1.0911245346069336\n",
      "Epoch 8822: Training Loss: 0.11010229339202245 Validation Loss: 1.090243935585022\n",
      "Epoch 8823: Training Loss: 0.110367846985658 Validation Loss: 1.0893863439559937\n",
      "Epoch 8824: Training Loss: 0.10990032802025478 Validation Loss: 1.0892260074615479\n",
      "Epoch 8825: Training Loss: 0.10972707470258077 Validation Loss: 1.0895823240280151\n",
      "Epoch 8826: Training Loss: 0.10989876339832942 Validation Loss: 1.090238332748413\n",
      "Epoch 8827: Training Loss: 0.10890196512142818 Validation Loss: 1.0906137228012085\n",
      "Epoch 8828: Training Loss: 0.11032439519961675 Validation Loss: 1.0904786586761475\n",
      "Epoch 8829: Training Loss: 0.10994182030359904 Validation Loss: 1.0906076431274414\n",
      "Epoch 8830: Training Loss: 0.10991150885820389 Validation Loss: 1.0894217491149902\n",
      "Epoch 8831: Training Loss: 0.11007800946633021 Validation Loss: 1.0894001722335815\n",
      "Epoch 8832: Training Loss: 0.10986957450707753 Validation Loss: 1.088890552520752\n",
      "Epoch 8833: Training Loss: 0.10946946839491527 Validation Loss: 1.0895004272460938\n",
      "Epoch 8834: Training Loss: 0.10947768886884053 Validation Loss: 1.0894849300384521\n",
      "Epoch 8835: Training Loss: 0.10991024225950241 Validation Loss: 1.0904316902160645\n",
      "Epoch 8836: Training Loss: 0.10989433278640111 Validation Loss: 1.0899063348770142\n",
      "Epoch 8837: Training Loss: 0.10979357361793518 Validation Loss: 1.0899908542633057\n",
      "Epoch 8838: Training Loss: 0.10994896044333775 Validation Loss: 1.0898149013519287\n",
      "Epoch 8839: Training Loss: 0.10993322978417079 Validation Loss: 1.0893224477767944\n",
      "Epoch 8840: Training Loss: 0.11038404454787572 Validation Loss: 1.0900816917419434\n",
      "Epoch 8841: Training Loss: 0.10980680088202159 Validation Loss: 1.0912389755249023\n",
      "Epoch 8842: Training Loss: 0.10945818324883778 Validation Loss: 1.0914582014083862\n",
      "Epoch 8843: Training Loss: 0.1097843125462532 Validation Loss: 1.0915991067886353\n",
      "Epoch 8844: Training Loss: 0.10971113791068395 Validation Loss: 1.0914604663848877\n",
      "Epoch 8845: Training Loss: 0.10991031428178151 Validation Loss: 1.0911176204681396\n",
      "Epoch 8846: Training Loss: 0.10945122440656026 Validation Loss: 1.0902844667434692\n",
      "Epoch 8847: Training Loss: 0.11005950470765431 Validation Loss: 1.0907114744186401\n",
      "Epoch 8848: Training Loss: 0.10974380373954773 Validation Loss: 1.0914679765701294\n",
      "Epoch 8849: Training Loss: 0.1093727598587672 Validation Loss: 1.0917657613754272\n",
      "Epoch 8850: Training Loss: 0.10967592398325603 Validation Loss: 1.0922186374664307\n",
      "Epoch 8851: Training Loss: 0.1105698694785436 Validation Loss: 1.0916261672973633\n",
      "Epoch 8852: Training Loss: 0.11027119308710098 Validation Loss: 1.0913844108581543\n",
      "Epoch 8853: Training Loss: 0.11021563907464345 Validation Loss: 1.0904334783554077\n",
      "Epoch 8854: Training Loss: 0.10938109954198201 Validation Loss: 1.089645266532898\n",
      "Epoch 8855: Training Loss: 0.10979963342348735 Validation Loss: 1.0891743898391724\n",
      "Epoch 8856: Training Loss: 0.10973877211411794 Validation Loss: 1.089136004447937\n",
      "Epoch 8857: Training Loss: 0.10962774604558945 Validation Loss: 1.090250015258789\n",
      "Epoch 8858: Training Loss: 0.10977422446012497 Validation Loss: 1.0908548831939697\n",
      "Epoch 8859: Training Loss: 0.10943727940320969 Validation Loss: 1.0915836095809937\n",
      "Epoch 8860: Training Loss: 0.10994250575701396 Validation Loss: 1.0911860466003418\n",
      "Epoch 8861: Training Loss: 0.10965085277954738 Validation Loss: 1.0912646055221558\n",
      "Epoch 8862: Training Loss: 0.10935353487730026 Validation Loss: 1.0907347202301025\n",
      "Epoch 8863: Training Loss: 0.11005848397811253 Validation Loss: 1.0913856029510498\n",
      "Epoch 8864: Training Loss: 0.1098854864637057 Validation Loss: 1.091856598854065\n",
      "Epoch 8865: Training Loss: 0.10987749695777893 Validation Loss: 1.0918596982955933\n",
      "Epoch 8866: Training Loss: 0.10989679396152496 Validation Loss: 1.092118501663208\n",
      "Epoch 8867: Training Loss: 0.10964476068814595 Validation Loss: 1.091017246246338\n",
      "Epoch 8868: Training Loss: 0.10993668188651402 Validation Loss: 1.0909152030944824\n",
      "Epoch 8869: Training Loss: 0.11022708316644032 Validation Loss: 1.0914438962936401\n",
      "Epoch 8870: Training Loss: 0.10947659860054652 Validation Loss: 1.0912419557571411\n",
      "Epoch 8871: Training Loss: 0.10886466006437938 Validation Loss: 1.0907847881317139\n",
      "Epoch 8872: Training Loss: 0.10988779117663701 Validation Loss: 1.0912895202636719\n",
      "Epoch 8873: Training Loss: 0.10962615658839543 Validation Loss: 1.0911974906921387\n",
      "Epoch 8874: Training Loss: 0.10911299536625545 Validation Loss: 1.0907601118087769\n",
      "Epoch 8875: Training Loss: 0.10922637581825256 Validation Loss: 1.09099280834198\n",
      "Epoch 8876: Training Loss: 0.10975884894529979 Validation Loss: 1.0906440019607544\n",
      "Epoch 8877: Training Loss: 0.10932515313227971 Validation Loss: 1.0913738012313843\n",
      "Epoch 8878: Training Loss: 0.10962407290935516 Validation Loss: 1.091559886932373\n",
      "Epoch 8879: Training Loss: 0.10976975411176682 Validation Loss: 1.0924183130264282\n",
      "Epoch 8880: Training Loss: 0.1095878854393959 Validation Loss: 1.0933512449264526\n",
      "Epoch 8881: Training Loss: 0.10933708151181538 Validation Loss: 1.0925779342651367\n",
      "Epoch 8882: Training Loss: 0.10940740009148915 Validation Loss: 1.090816855430603\n",
      "Epoch 8883: Training Loss: 0.10938164343436559 Validation Loss: 1.0906962156295776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8884: Training Loss: 0.11029833058516185 Validation Loss: 1.0919435024261475\n",
      "Epoch 8885: Training Loss: 0.1103217750787735 Validation Loss: 1.0928435325622559\n",
      "Epoch 8886: Training Loss: 0.10981073727210362 Validation Loss: 1.0931235551834106\n",
      "Epoch 8887: Training Loss: 0.10914300630489986 Validation Loss: 1.092849612236023\n",
      "Epoch 8888: Training Loss: 0.10917418946822484 Validation Loss: 1.0922707319259644\n",
      "Epoch 8889: Training Loss: 0.10932591060797374 Validation Loss: 1.091576099395752\n",
      "Epoch 8890: Training Loss: 0.10964697599411011 Validation Loss: 1.0923620462417603\n",
      "Epoch 8891: Training Loss: 0.10930244127909343 Validation Loss: 1.0920716524124146\n",
      "Epoch 8892: Training Loss: 0.10939149310191472 Validation Loss: 1.0918798446655273\n",
      "Epoch 8893: Training Loss: 0.10943794747193654 Validation Loss: 1.090927243232727\n",
      "Epoch 8894: Training Loss: 0.10940457880496979 Validation Loss: 1.0911338329315186\n",
      "Epoch 8895: Training Loss: 0.10929234077533086 Validation Loss: 1.0912073850631714\n",
      "Epoch 8896: Training Loss: 0.10919370502233505 Validation Loss: 1.0903739929199219\n",
      "Epoch 8897: Training Loss: 0.10938989619414012 Validation Loss: 1.0923717021942139\n",
      "Epoch 8898: Training Loss: 0.10933553179105122 Validation Loss: 1.0929986238479614\n",
      "Epoch 8899: Training Loss: 0.10958721488714218 Validation Loss: 1.092970848083496\n",
      "Epoch 8900: Training Loss: 0.10913233955701192 Validation Loss: 1.0931552648544312\n",
      "Epoch 8901: Training Loss: 0.10941248635451 Validation Loss: 1.0921636819839478\n",
      "Epoch 8902: Training Loss: 0.10912646353244781 Validation Loss: 1.0916615724563599\n",
      "Epoch 8903: Training Loss: 0.10912654797236125 Validation Loss: 1.0910788774490356\n",
      "Epoch 8904: Training Loss: 0.10964523007472356 Validation Loss: 1.0911871194839478\n",
      "Epoch 8905: Training Loss: 0.10990131398042043 Validation Loss: 1.091902732849121\n",
      "Epoch 8906: Training Loss: 0.10876809805631638 Validation Loss: 1.0919432640075684\n",
      "Epoch 8907: Training Loss: 0.10890198250611623 Validation Loss: 1.0923727750778198\n",
      "Epoch 8908: Training Loss: 0.10913735379775365 Validation Loss: 1.0927258729934692\n",
      "Epoch 8909: Training Loss: 0.10904603699843089 Validation Loss: 1.0928783416748047\n",
      "Epoch 8910: Training Loss: 0.10881830006837845 Validation Loss: 1.0928961038589478\n",
      "Epoch 8911: Training Loss: 0.10905718555053075 Validation Loss: 1.0931271314620972\n",
      "Epoch 8912: Training Loss: 0.10898810625076294 Validation Loss: 1.0935200452804565\n",
      "Epoch 8913: Training Loss: 0.10952348758776982 Validation Loss: 1.0933218002319336\n",
      "Epoch 8914: Training Loss: 0.10918517659107844 Validation Loss: 1.0929772853851318\n",
      "Epoch 8915: Training Loss: 0.10910723110040028 Validation Loss: 1.0927388668060303\n",
      "Epoch 8916: Training Loss: 0.10928034037351608 Validation Loss: 1.0916897058486938\n",
      "Epoch 8917: Training Loss: 0.10912978400786717 Validation Loss: 1.0922795534133911\n",
      "Epoch 8918: Training Loss: 0.10898441572984059 Validation Loss: 1.0935529470443726\n",
      "Epoch 8919: Training Loss: 0.10915009925762813 Validation Loss: 1.0946394205093384\n",
      "Epoch 8920: Training Loss: 0.10932886103789012 Validation Loss: 1.0930986404418945\n",
      "Epoch 8921: Training Loss: 0.10897934933503468 Validation Loss: 1.0921313762664795\n",
      "Epoch 8922: Training Loss: 0.10890401651461919 Validation Loss: 1.091392993927002\n",
      "Epoch 8923: Training Loss: 0.10911353429158528 Validation Loss: 1.0923471450805664\n",
      "Epoch 8924: Training Loss: 0.10960990438858668 Validation Loss: 1.0930355787277222\n",
      "Epoch 8925: Training Loss: 0.10903402417898178 Validation Loss: 1.092815637588501\n",
      "Epoch 8926: Training Loss: 0.10883039732774098 Validation Loss: 1.0921146869659424\n",
      "Epoch 8927: Training Loss: 0.10887378702561061 Validation Loss: 1.0920592546463013\n",
      "Epoch 8928: Training Loss: 0.10956062624851863 Validation Loss: 1.0920147895812988\n",
      "Epoch 8929: Training Loss: 0.10895023743311565 Validation Loss: 1.0928668975830078\n",
      "Epoch 8930: Training Loss: 0.10899093995491664 Validation Loss: 1.0933363437652588\n",
      "Epoch 8931: Training Loss: 0.10917450487613678 Validation Loss: 1.0942962169647217\n",
      "Epoch 8932: Training Loss: 0.10897522419691086 Validation Loss: 1.093706727027893\n",
      "Epoch 8933: Training Loss: 0.10880196591218312 Validation Loss: 1.0935674905776978\n",
      "Epoch 8934: Training Loss: 0.10876219222942989 Validation Loss: 1.0929161310195923\n",
      "Epoch 8935: Training Loss: 0.10898671050866444 Validation Loss: 1.092131495475769\n",
      "Epoch 8936: Training Loss: 0.10904623319705327 Validation Loss: 1.0923118591308594\n",
      "Epoch 8937: Training Loss: 0.10887045164903005 Validation Loss: 1.0927602052688599\n",
      "Epoch 8938: Training Loss: 0.10905203223228455 Validation Loss: 1.092913031578064\n",
      "Epoch 8939: Training Loss: 0.10886107881863911 Validation Loss: 1.092980980873108\n",
      "Epoch 8940: Training Loss: 0.10866646220286687 Validation Loss: 1.093794584274292\n",
      "Epoch 8941: Training Loss: 0.10867968449989955 Validation Loss: 1.0930548906326294\n",
      "Epoch 8942: Training Loss: 0.10951022058725357 Validation Loss: 1.0934969186782837\n",
      "Epoch 8943: Training Loss: 0.10879318664471309 Validation Loss: 1.093367576599121\n",
      "Epoch 8944: Training Loss: 0.10917381197214127 Validation Loss: 1.0936529636383057\n",
      "Epoch 8945: Training Loss: 0.10887006670236588 Validation Loss: 1.0939170122146606\n",
      "Epoch 8946: Training Loss: 0.10894594341516495 Validation Loss: 1.0939059257507324\n",
      "Epoch 8947: Training Loss: 0.10869672397772472 Validation Loss: 1.0941275358200073\n",
      "Epoch 8948: Training Loss: 0.10897978643576305 Validation Loss: 1.0943094491958618\n",
      "Epoch 8949: Training Loss: 0.10910277565320332 Validation Loss: 1.0934959650039673\n",
      "Epoch 8950: Training Loss: 0.10903011014064153 Validation Loss: 1.0924601554870605\n",
      "Epoch 8951: Training Loss: 0.10874205082654953 Validation Loss: 1.0929690599441528\n",
      "Epoch 8952: Training Loss: 0.10865311324596405 Validation Loss: 1.0929275751113892\n",
      "Epoch 8953: Training Loss: 0.10851450512806575 Validation Loss: 1.093658208847046\n",
      "Epoch 8954: Training Loss: 0.10874155163764954 Validation Loss: 1.094718337059021\n",
      "Epoch 8955: Training Loss: 0.10884833584229152 Validation Loss: 1.094441294670105\n",
      "Epoch 8956: Training Loss: 0.10871656984090805 Validation Loss: 1.0942189693450928\n",
      "Epoch 8957: Training Loss: 0.10878974944353104 Validation Loss: 1.0940691232681274\n",
      "Epoch 8958: Training Loss: 0.1084970807035764 Validation Loss: 1.093712568283081\n",
      "Epoch 8959: Training Loss: 0.10898518810669582 Validation Loss: 1.0932271480560303\n",
      "Epoch 8960: Training Loss: 0.10855736335118611 Validation Loss: 1.0934810638427734\n",
      "Epoch 8961: Training Loss: 0.10853498180707295 Validation Loss: 1.0930086374282837\n",
      "Epoch 8962: Training Loss: 0.10877026617527008 Validation Loss: 1.0938247442245483\n",
      "Epoch 8963: Training Loss: 0.10960787286361058 Validation Loss: 1.094661831855774\n",
      "Epoch 8964: Training Loss: 0.10867447902758916 Validation Loss: 1.0946825742721558\n",
      "Epoch 8965: Training Loss: 0.10867690791686375 Validation Loss: 1.0946342945098877\n",
      "Epoch 8966: Training Loss: 0.10849364846944809 Validation Loss: 1.0949435234069824\n",
      "Epoch 8967: Training Loss: 0.10855226218700409 Validation Loss: 1.0952633619308472\n",
      "Epoch 8968: Training Loss: 0.10875244438648224 Validation Loss: 1.095698356628418\n",
      "Epoch 8969: Training Loss: 0.10852699478467305 Validation Loss: 1.0951615571975708\n",
      "Epoch 8970: Training Loss: 0.10850494354963303 Validation Loss: 1.0944395065307617\n",
      "Epoch 8971: Training Loss: 0.10872746258974075 Validation Loss: 1.0935862064361572\n",
      "Epoch 8972: Training Loss: 0.10839033623536427 Validation Loss: 1.09453547000885\n",
      "Epoch 8973: Training Loss: 0.10826503733793895 Validation Loss: 1.0942590236663818\n",
      "Epoch 8974: Training Loss: 0.10917011151711146 Validation Loss: 1.0945701599121094\n",
      "Epoch 8975: Training Loss: 0.10857217758893967 Validation Loss: 1.0944136381149292\n",
      "Epoch 8976: Training Loss: 0.10893295953671138 Validation Loss: 1.0931172370910645\n",
      "Epoch 8977: Training Loss: 0.1094366783897082 Validation Loss: 1.0934112071990967\n",
      "Epoch 8978: Training Loss: 0.10853016624848048 Validation Loss: 1.0927680730819702\n",
      "Epoch 8979: Training Loss: 0.10890654474496841 Validation Loss: 1.0938267707824707\n",
      "Epoch 8980: Training Loss: 0.10834807405869167 Validation Loss: 1.0942447185516357\n",
      "Epoch 8981: Training Loss: 0.10857406755288442 Validation Loss: 1.0947067737579346\n",
      "Epoch 8982: Training Loss: 0.10864981760581334 Validation Loss: 1.0952576398849487\n",
      "Epoch 8983: Training Loss: 0.10891012102365494 Validation Loss: 1.096189260482788\n",
      "Epoch 8984: Training Loss: 0.10854256898164749 Validation Loss: 1.0952420234680176\n",
      "Epoch 8985: Training Loss: 0.10826946298281352 Validation Loss: 1.0947726964950562\n",
      "Epoch 8986: Training Loss: 0.10898777842521667 Validation Loss: 1.093968391418457\n",
      "Epoch 8987: Training Loss: 0.1096260795990626 Validation Loss: 1.0935757160186768\n",
      "Epoch 8988: Training Loss: 0.10841581225395203 Validation Loss: 1.094455361366272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8989: Training Loss: 0.10877477377653122 Validation Loss: 1.095910906791687\n",
      "Epoch 8990: Training Loss: 0.10841005047162373 Validation Loss: 1.0970443487167358\n",
      "Epoch 8991: Training Loss: 0.10890243947505951 Validation Loss: 1.0949889421463013\n",
      "Epoch 8992: Training Loss: 0.10829394310712814 Validation Loss: 1.0947318077087402\n",
      "Epoch 8993: Training Loss: 0.10844123363494873 Validation Loss: 1.094801425933838\n",
      "Epoch 8994: Training Loss: 0.10848490397135417 Validation Loss: 1.095504641532898\n",
      "Epoch 8995: Training Loss: 0.10884345571200053 Validation Loss: 1.0947068929672241\n",
      "Epoch 8996: Training Loss: 0.10884292423725128 Validation Loss: 1.0944803953170776\n",
      "Epoch 8997: Training Loss: 0.10926419248183568 Validation Loss: 1.093632698059082\n",
      "Epoch 8998: Training Loss: 0.10833971699078877 Validation Loss: 1.0940238237380981\n",
      "Epoch 8999: Training Loss: 0.10828058669964473 Validation Loss: 1.0943657159805298\n",
      "Epoch 9000: Training Loss: 0.1085382451613744 Validation Loss: 1.0949424505233765\n",
      "Epoch 9001: Training Loss: 0.10915918896595637 Validation Loss: 1.094558835029602\n",
      "Epoch 9002: Training Loss: 0.10837377856175105 Validation Loss: 1.0952482223510742\n",
      "Epoch 9003: Training Loss: 0.10900822281837463 Validation Loss: 1.0957417488098145\n",
      "Epoch 9004: Training Loss: 0.10937266548474629 Validation Loss: 1.0953280925750732\n",
      "Epoch 9005: Training Loss: 0.10855598002672195 Validation Loss: 1.0947824716567993\n",
      "Epoch 9006: Training Loss: 0.10824925204118092 Validation Loss: 1.0941836833953857\n",
      "Epoch 9007: Training Loss: 0.10853444536526997 Validation Loss: 1.0943610668182373\n",
      "Epoch 9008: Training Loss: 0.10943834483623505 Validation Loss: 1.0956815481185913\n",
      "Epoch 9009: Training Loss: 0.10827877620855968 Validation Loss: 1.0970901250839233\n",
      "Epoch 9010: Training Loss: 0.10817494491736095 Validation Loss: 1.0972373485565186\n",
      "Epoch 9011: Training Loss: 0.107913243273894 Validation Loss: 1.0962016582489014\n",
      "Epoch 9012: Training Loss: 0.10784593472878139 Validation Loss: 1.0954536199569702\n",
      "Epoch 9013: Training Loss: 0.10805240273475647 Validation Loss: 1.0943163633346558\n",
      "Epoch 9014: Training Loss: 0.10793998589118321 Validation Loss: 1.0938353538513184\n",
      "Epoch 9015: Training Loss: 0.10855995366970699 Validation Loss: 1.094101905822754\n",
      "Epoch 9016: Training Loss: 0.10911230991284053 Validation Loss: 1.0944230556488037\n",
      "Epoch 9017: Training Loss: 0.10799607386191686 Validation Loss: 1.0951026678085327\n",
      "Epoch 9018: Training Loss: 0.10803962250550588 Validation Loss: 1.0958197116851807\n",
      "Epoch 9019: Training Loss: 0.10816369205713272 Validation Loss: 1.0965311527252197\n",
      "Epoch 9020: Training Loss: 0.10801323751608531 Validation Loss: 1.0965248346328735\n",
      "Epoch 9021: Training Loss: 0.10810626298189163 Validation Loss: 1.0962258577346802\n",
      "Epoch 9022: Training Loss: 0.10834471136331558 Validation Loss: 1.095948338508606\n",
      "Epoch 9023: Training Loss: 0.108406367401282 Validation Loss: 1.096262812614441\n",
      "Epoch 9024: Training Loss: 0.10799506803353627 Validation Loss: 1.0960661172866821\n",
      "Epoch 9025: Training Loss: 0.10860904802878697 Validation Loss: 1.0970795154571533\n",
      "Epoch 9026: Training Loss: 0.10858761022488277 Validation Loss: 1.096911907196045\n",
      "Epoch 9027: Training Loss: 0.1082088698943456 Validation Loss: 1.0970048904418945\n",
      "Epoch 9028: Training Loss: 0.10801233599583308 Validation Loss: 1.0961942672729492\n",
      "Epoch 9029: Training Loss: 0.10801351070404053 Validation Loss: 1.095787763595581\n",
      "Epoch 9030: Training Loss: 0.10717391222715378 Validation Loss: 1.095635175704956\n",
      "Epoch 9031: Training Loss: 0.10816655556360881 Validation Loss: 1.0965787172317505\n",
      "Epoch 9032: Training Loss: 0.10742567976315816 Validation Loss: 1.0973114967346191\n",
      "Epoch 9033: Training Loss: 0.10716591775417328 Validation Loss: 1.0964431762695312\n",
      "Epoch 9034: Training Loss: 0.10841321696837743 Validation Loss: 1.0954911708831787\n",
      "Epoch 9035: Training Loss: 0.10787322868903478 Validation Loss: 1.094682216644287\n",
      "Epoch 9036: Training Loss: 0.10822335133949916 Validation Loss: 1.0950984954833984\n",
      "Epoch 9037: Training Loss: 0.10860310743252437 Validation Loss: 1.097011685371399\n",
      "Epoch 9038: Training Loss: 0.10809271037578583 Validation Loss: 1.0985702276229858\n",
      "Epoch 9039: Training Loss: 0.1081272338827451 Validation Loss: 1.0971951484680176\n",
      "Epoch 9040: Training Loss: 0.10815307746330897 Validation Loss: 1.0967192649841309\n",
      "Epoch 9041: Training Loss: 0.10847840954860051 Validation Loss: 1.0962789058685303\n",
      "Epoch 9042: Training Loss: 0.10797408719857533 Validation Loss: 1.0956933498382568\n",
      "Epoch 9043: Training Loss: 0.1080696831146876 Validation Loss: 1.0951648950576782\n",
      "Epoch 9044: Training Loss: 0.10834018141031265 Validation Loss: 1.096259355545044\n",
      "Epoch 9045: Training Loss: 0.10784950107336044 Validation Loss: 1.0968223810195923\n",
      "Epoch 9046: Training Loss: 0.10797638197739919 Validation Loss: 1.0969514846801758\n",
      "Epoch 9047: Training Loss: 0.10862383246421814 Validation Loss: 1.0981113910675049\n",
      "Epoch 9048: Training Loss: 0.1081689124306043 Validation Loss: 1.0973737239837646\n",
      "Epoch 9049: Training Loss: 0.1079519713918368 Validation Loss: 1.0967193841934204\n",
      "Epoch 9050: Training Loss: 0.10798736910025279 Validation Loss: 1.0957175493240356\n",
      "Epoch 9051: Training Loss: 0.10841235270102818 Validation Loss: 1.096153974533081\n",
      "Epoch 9052: Training Loss: 0.10801497598489125 Validation Loss: 1.0951764583587646\n",
      "Epoch 9053: Training Loss: 0.1078980341553688 Validation Loss: 1.0956730842590332\n",
      "Epoch 9054: Training Loss: 0.10908510287602742 Validation Loss: 1.0962958335876465\n",
      "Epoch 9055: Training Loss: 0.10781293114026387 Validation Loss: 1.0976718664169312\n",
      "Epoch 9056: Training Loss: 0.10794475426276524 Validation Loss: 1.0975900888442993\n",
      "Epoch 9057: Training Loss: 0.10758532087008159 Validation Loss: 1.0985431671142578\n",
      "Epoch 9058: Training Loss: 0.10761548827091853 Validation Loss: 1.0975450277328491\n",
      "Epoch 9059: Training Loss: 0.10806213319301605 Validation Loss: 1.0965946912765503\n",
      "Epoch 9060: Training Loss: 0.10774228721857071 Validation Loss: 1.0970087051391602\n",
      "Epoch 9061: Training Loss: 0.10757205386956532 Validation Loss: 1.0969622135162354\n",
      "Epoch 9062: Training Loss: 0.10792009035746257 Validation Loss: 1.0964826345443726\n",
      "Epoch 9063: Training Loss: 0.10747712602217992 Validation Loss: 1.0961964130401611\n",
      "Epoch 9064: Training Loss: 0.107604019343853 Validation Loss: 1.0967211723327637\n",
      "Epoch 9065: Training Loss: 0.10824173440535863 Validation Loss: 1.0959893465042114\n",
      "Epoch 9066: Training Loss: 0.1077783207098643 Validation Loss: 1.0962806940078735\n",
      "Epoch 9067: Training Loss: 0.10792862127224605 Validation Loss: 1.096150279045105\n",
      "Epoch 9068: Training Loss: 0.10803590963284175 Validation Loss: 1.0964415073394775\n",
      "Epoch 9069: Training Loss: 0.10789297272761662 Validation Loss: 1.0974526405334473\n",
      "Epoch 9070: Training Loss: 0.10790276279052098 Validation Loss: 1.0971412658691406\n",
      "Epoch 9071: Training Loss: 0.10753406087557475 Validation Loss: 1.0967376232147217\n",
      "Epoch 9072: Training Loss: 0.10764110833406448 Validation Loss: 1.0968390703201294\n",
      "Epoch 9073: Training Loss: 0.10789083689451218 Validation Loss: 1.098390817642212\n",
      "Epoch 9074: Training Loss: 0.10789793978134792 Validation Loss: 1.0993001461029053\n",
      "Epoch 9075: Training Loss: 0.1082709605495135 Validation Loss: 1.09818434715271\n",
      "Epoch 9076: Training Loss: 0.10768371572097142 Validation Loss: 1.096453309059143\n",
      "Epoch 9077: Training Loss: 0.10778087625900905 Validation Loss: 1.0968137979507446\n",
      "Epoch 9078: Training Loss: 0.10672051707903545 Validation Loss: 1.0965752601623535\n",
      "Epoch 9079: Training Loss: 0.10766189048687617 Validation Loss: 1.0967620611190796\n",
      "Epoch 9080: Training Loss: 0.10807161529858907 Validation Loss: 1.0960917472839355\n",
      "Epoch 9081: Training Loss: 0.10776697099208832 Validation Loss: 1.0955278873443604\n",
      "Epoch 9082: Training Loss: 0.10758463541666667 Validation Loss: 1.096557378768921\n",
      "Epoch 9083: Training Loss: 0.10773357500632604 Validation Loss: 1.0985482931137085\n",
      "Epoch 9084: Training Loss: 0.10792500774065654 Validation Loss: 1.097363829612732\n",
      "Epoch 9085: Training Loss: 0.1075236052274704 Validation Loss: 1.097274899482727\n",
      "Epoch 9086: Training Loss: 0.10772789518038432 Validation Loss: 1.0966659784317017\n",
      "Epoch 9087: Training Loss: 0.10786257435878117 Validation Loss: 1.0966147184371948\n",
      "Epoch 9088: Training Loss: 0.10737209022045135 Validation Loss: 1.0968207120895386\n",
      "Epoch 9089: Training Loss: 0.10749014715353648 Validation Loss: 1.0981031656265259\n",
      "Epoch 9090: Training Loss: 0.1080046792825063 Validation Loss: 1.0982619524002075\n",
      "Epoch 9091: Training Loss: 0.10752882808446884 Validation Loss: 1.0977089405059814\n",
      "Epoch 9092: Training Loss: 0.1071910411119461 Validation Loss: 1.097683310508728\n",
      "Epoch 9093: Training Loss: 0.1079454372326533 Validation Loss: 1.0983654260635376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9094: Training Loss: 0.10748612880706787 Validation Loss: 1.0986452102661133\n",
      "Epoch 9095: Training Loss: 0.10772175093491872 Validation Loss: 1.0981125831604004\n",
      "Epoch 9096: Training Loss: 0.10770013431708018 Validation Loss: 1.0985418558120728\n",
      "Epoch 9097: Training Loss: 0.10839594155550003 Validation Loss: 1.097682237625122\n",
      "Epoch 9098: Training Loss: 0.1074805681904157 Validation Loss: 1.0972559452056885\n",
      "Epoch 9099: Training Loss: 0.10702468454837799 Validation Loss: 1.0976223945617676\n",
      "Epoch 9100: Training Loss: 0.10785443832476933 Validation Loss: 1.0990629196166992\n",
      "Epoch 9101: Training Loss: 0.10759860773881276 Validation Loss: 1.0986865758895874\n",
      "Epoch 9102: Training Loss: 0.10806692639986674 Validation Loss: 1.0980528593063354\n",
      "Epoch 9103: Training Loss: 0.1078197459379832 Validation Loss: 1.098239541053772\n",
      "Epoch 9104: Training Loss: 0.10714021573464076 Validation Loss: 1.0973143577575684\n",
      "Epoch 9105: Training Loss: 0.1074568231900533 Validation Loss: 1.0972646474838257\n",
      "Epoch 9106: Training Loss: 0.10774937272071838 Validation Loss: 1.0967475175857544\n",
      "Epoch 9107: Training Loss: 0.10741118341684341 Validation Loss: 1.0965547561645508\n",
      "Epoch 9108: Training Loss: 0.10773049046595891 Validation Loss: 1.097659707069397\n",
      "Epoch 9109: Training Loss: 0.10733370979626973 Validation Loss: 1.0981860160827637\n",
      "Epoch 9110: Training Loss: 0.10737742731968562 Validation Loss: 1.0983864068984985\n",
      "Epoch 9111: Training Loss: 0.10742449263731639 Validation Loss: 1.0985634326934814\n",
      "Epoch 9112: Training Loss: 0.10724272082249324 Validation Loss: 1.0992660522460938\n",
      "Epoch 9113: Training Loss: 0.10844985395669937 Validation Loss: 1.098957896232605\n",
      "Epoch 9114: Training Loss: 0.10731684664885204 Validation Loss: 1.0983521938323975\n",
      "Epoch 9115: Training Loss: 0.10749710847934087 Validation Loss: 1.0976953506469727\n",
      "Epoch 9116: Training Loss: 0.10722386588652928 Validation Loss: 1.097232699394226\n",
      "Epoch 9117: Training Loss: 0.1068482647339503 Validation Loss: 1.0975980758666992\n",
      "Epoch 9118: Training Loss: 0.10875364641348521 Validation Loss: 1.0976555347442627\n",
      "Epoch 9119: Training Loss: 0.10760289927323659 Validation Loss: 1.0980238914489746\n",
      "Epoch 9120: Training Loss: 0.10732672860225041 Validation Loss: 1.0984948873519897\n",
      "Epoch 9121: Training Loss: 0.1083567018310229 Validation Loss: 1.097568154335022\n",
      "Epoch 9122: Training Loss: 0.10773872584104538 Validation Loss: 1.0985095500946045\n",
      "Epoch 9123: Training Loss: 0.1073735033472379 Validation Loss: 1.0987037420272827\n",
      "Epoch 9124: Training Loss: 0.10719507932662964 Validation Loss: 1.0987012386322021\n",
      "Epoch 9125: Training Loss: 0.1070781871676445 Validation Loss: 1.0990318059921265\n",
      "Epoch 9126: Training Loss: 0.10725187013546626 Validation Loss: 1.0988173484802246\n",
      "Epoch 9127: Training Loss: 0.1072588786482811 Validation Loss: 1.0991536378860474\n",
      "Epoch 9128: Training Loss: 0.10719158500432968 Validation Loss: 1.0982029438018799\n",
      "Epoch 9129: Training Loss: 0.1067918340365092 Validation Loss: 1.098466396331787\n",
      "Epoch 9130: Training Loss: 0.10788519928852718 Validation Loss: 1.099128246307373\n",
      "Epoch 9131: Training Loss: 0.10725220789511998 Validation Loss: 1.0997990369796753\n",
      "Epoch 9132: Training Loss: 0.10722978909810384 Validation Loss: 1.1004362106323242\n",
      "Epoch 9133: Training Loss: 0.10708942761023839 Validation Loss: 1.0988929271697998\n",
      "Epoch 9134: Training Loss: 0.10715979834397633 Validation Loss: 1.0987043380737305\n",
      "Epoch 9135: Training Loss: 0.1076431820789973 Validation Loss: 1.0977033376693726\n",
      "Epoch 9136: Training Loss: 0.10755478590726852 Validation Loss: 1.0984857082366943\n",
      "Epoch 9137: Training Loss: 0.1070234477519989 Validation Loss: 1.0990478992462158\n",
      "Epoch 9138: Training Loss: 0.10781993468602498 Validation Loss: 1.0998958349227905\n",
      "Epoch 9139: Training Loss: 0.10698878765106201 Validation Loss: 1.0994759798049927\n",
      "Epoch 9140: Training Loss: 0.10692245761553447 Validation Loss: 1.098739504814148\n",
      "Epoch 9141: Training Loss: 0.10653900106747945 Validation Loss: 1.0988107919692993\n",
      "Epoch 9142: Training Loss: 0.10717732707659404 Validation Loss: 1.0994712114334106\n",
      "Epoch 9143: Training Loss: 0.10679344832897186 Validation Loss: 1.09891939163208\n",
      "Epoch 9144: Training Loss: 0.10693005969127019 Validation Loss: 1.0984289646148682\n",
      "Epoch 9145: Training Loss: 0.107061468064785 Validation Loss: 1.0983093976974487\n",
      "Epoch 9146: Training Loss: 0.10696373631556828 Validation Loss: 1.0983227491378784\n",
      "Epoch 9147: Training Loss: 0.10728756586710612 Validation Loss: 1.0989524126052856\n",
      "Epoch 9148: Training Loss: 0.10713511457045873 Validation Loss: 1.099236011505127\n",
      "Epoch 9149: Training Loss: 0.10706752290328343 Validation Loss: 1.098768949508667\n",
      "Epoch 9150: Training Loss: 0.10705261925856273 Validation Loss: 1.099076271057129\n",
      "Epoch 9151: Training Loss: 0.10696697980165482 Validation Loss: 1.0986440181732178\n",
      "Epoch 9152: Training Loss: 0.10703812787930171 Validation Loss: 1.098899006843567\n",
      "Epoch 9153: Training Loss: 0.10725102325280507 Validation Loss: 1.0992939472198486\n",
      "Epoch 9154: Training Loss: 0.10670924683411916 Validation Loss: 1.0997395515441895\n",
      "Epoch 9155: Training Loss: 0.10710608959197998 Validation Loss: 1.099974274635315\n",
      "Epoch 9156: Training Loss: 0.1069931834936142 Validation Loss: 1.0999298095703125\n",
      "Epoch 9157: Training Loss: 0.10703584303458531 Validation Loss: 1.0993058681488037\n",
      "Epoch 9158: Training Loss: 0.10695304224888484 Validation Loss: 1.0991125106811523\n",
      "Epoch 9159: Training Loss: 0.10708526770273845 Validation Loss: 1.0994068384170532\n",
      "Epoch 9160: Training Loss: 0.10709661742051442 Validation Loss: 1.099649429321289\n",
      "Epoch 9161: Training Loss: 0.10711670170227687 Validation Loss: 1.0990841388702393\n",
      "Epoch 9162: Training Loss: 0.10727872451146443 Validation Loss: 1.0995185375213623\n",
      "Epoch 9163: Training Loss: 0.10691351940234502 Validation Loss: 1.0999808311462402\n",
      "Epoch 9164: Training Loss: 0.10678947468598683 Validation Loss: 1.1003249883651733\n",
      "Epoch 9165: Training Loss: 0.1068676585952441 Validation Loss: 1.100768804550171\n",
      "Epoch 9166: Training Loss: 0.10695620874563853 Validation Loss: 1.0996235609054565\n",
      "Epoch 9167: Training Loss: 0.10679150621096294 Validation Loss: 1.0996280908584595\n",
      "Epoch 9168: Training Loss: 0.10657609750827153 Validation Loss: 1.0998406410217285\n",
      "Epoch 9169: Training Loss: 0.1069887379805247 Validation Loss: 1.098702311515808\n",
      "Epoch 9170: Training Loss: 0.10701485474904378 Validation Loss: 1.099115014076233\n",
      "Epoch 9171: Training Loss: 0.1066787193218867 Validation Loss: 1.099349021911621\n",
      "Epoch 9172: Training Loss: 0.10682990401983261 Validation Loss: 1.1001006364822388\n",
      "Epoch 9173: Training Loss: 0.10769945879777272 Validation Loss: 1.100583791732788\n",
      "Epoch 9174: Training Loss: 0.10672768950462341 Validation Loss: 1.1002358198165894\n",
      "Epoch 9175: Training Loss: 0.10678725192944209 Validation Loss: 1.099337100982666\n",
      "Epoch 9176: Training Loss: 0.10676037023464839 Validation Loss: 1.0987865924835205\n",
      "Epoch 9177: Training Loss: 0.10727254301309586 Validation Loss: 1.0995451211929321\n",
      "Epoch 9178: Training Loss: 0.10721710572640102 Validation Loss: 1.0998026132583618\n",
      "Epoch 9179: Training Loss: 0.10754768302043279 Validation Loss: 1.1002705097198486\n",
      "Epoch 9180: Training Loss: 0.10701189935207367 Validation Loss: 1.1007962226867676\n",
      "Epoch 9181: Training Loss: 0.10711374382177989 Validation Loss: 1.1009882688522339\n",
      "Epoch 9182: Training Loss: 0.1067298874258995 Validation Loss: 1.1014357805252075\n",
      "Epoch 9183: Training Loss: 0.10736520091692607 Validation Loss: 1.1018526554107666\n",
      "Epoch 9184: Training Loss: 0.10698078324397405 Validation Loss: 1.1018741130828857\n",
      "Epoch 9185: Training Loss: 0.10667930295070012 Validation Loss: 1.1002578735351562\n",
      "Epoch 9186: Training Loss: 0.10657945523659389 Validation Loss: 1.1002057790756226\n",
      "Epoch 9187: Training Loss: 0.10724664231141408 Validation Loss: 1.0994904041290283\n",
      "Epoch 9188: Training Loss: 0.10647944112618764 Validation Loss: 1.0998055934906006\n",
      "Epoch 9189: Training Loss: 0.10629536211490631 Validation Loss: 1.1002311706542969\n",
      "Epoch 9190: Training Loss: 0.1067593644062678 Validation Loss: 1.1002449989318848\n",
      "Epoch 9191: Training Loss: 0.10666964948177338 Validation Loss: 1.100513219833374\n",
      "Epoch 9192: Training Loss: 0.10689238955577214 Validation Loss: 1.1003401279449463\n",
      "Epoch 9193: Training Loss: 0.10683988531430562 Validation Loss: 1.101640224456787\n",
      "Epoch 9194: Training Loss: 0.10637467602888744 Validation Loss: 1.1010633707046509\n",
      "Epoch 9195: Training Loss: 0.10674130419890086 Validation Loss: 1.101611852645874\n",
      "Epoch 9196: Training Loss: 0.10679998497168224 Validation Loss: 1.1005949974060059\n",
      "Epoch 9197: Training Loss: 0.10768153766791026 Validation Loss: 1.1006990671157837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9198: Training Loss: 0.10657012462615967 Validation Loss: 1.100502848625183\n",
      "Epoch 9199: Training Loss: 0.10709135234355927 Validation Loss: 1.0998773574829102\n",
      "Epoch 9200: Training Loss: 0.10669674227635066 Validation Loss: 1.099017858505249\n",
      "Epoch 9201: Training Loss: 0.10698773463567098 Validation Loss: 1.0996934175491333\n",
      "Epoch 9202: Training Loss: 0.10662682851155598 Validation Loss: 1.1003057956695557\n",
      "Epoch 9203: Training Loss: 0.10649814456701279 Validation Loss: 1.1022197008132935\n",
      "Epoch 9204: Training Loss: 0.10676562289396922 Validation Loss: 1.102472186088562\n",
      "Epoch 9205: Training Loss: 0.10656049102544785 Validation Loss: 1.1019219160079956\n",
      "Epoch 9206: Training Loss: 0.10638695706923802 Validation Loss: 1.0998541116714478\n",
      "Epoch 9207: Training Loss: 0.10658781975507736 Validation Loss: 1.0992997884750366\n",
      "Epoch 9208: Training Loss: 0.10681172460317612 Validation Loss: 1.0999659299850464\n",
      "Epoch 9209: Training Loss: 0.10747017214695613 Validation Loss: 1.1006808280944824\n",
      "Epoch 9210: Training Loss: 0.10685133685668309 Validation Loss: 1.101132869720459\n",
      "Epoch 9211: Training Loss: 0.10637408743302028 Validation Loss: 1.1007156372070312\n",
      "Epoch 9212: Training Loss: 0.10660039881865184 Validation Loss: 1.1002812385559082\n",
      "Epoch 9213: Training Loss: 0.1064136524995168 Validation Loss: 1.1005587577819824\n",
      "Epoch 9214: Training Loss: 0.1067468672990799 Validation Loss: 1.100711464881897\n",
      "Epoch 9215: Training Loss: 0.10570273548364639 Validation Loss: 1.1017135381698608\n",
      "Epoch 9216: Training Loss: 0.10666483889023463 Validation Loss: 1.1025928258895874\n",
      "Epoch 9217: Training Loss: 0.10649473965167999 Validation Loss: 1.1022958755493164\n",
      "Epoch 9218: Training Loss: 0.10681803524494171 Validation Loss: 1.1008939743041992\n",
      "Epoch 9219: Training Loss: 0.10652121901512146 Validation Loss: 1.1008477210998535\n",
      "Epoch 9220: Training Loss: 0.10652756939331691 Validation Loss: 1.1003965139389038\n",
      "Epoch 9221: Training Loss: 0.10637762397527695 Validation Loss: 1.1006301641464233\n",
      "Epoch 9222: Training Loss: 0.10630010068416595 Validation Loss: 1.1009154319763184\n",
      "Epoch 9223: Training Loss: 0.10637141267458598 Validation Loss: 1.1008363962173462\n",
      "Epoch 9224: Training Loss: 0.10594278077284495 Validation Loss: 1.1007283926010132\n",
      "Epoch 9225: Training Loss: 0.10625813156366348 Validation Loss: 1.1010489463806152\n",
      "Epoch 9226: Training Loss: 0.10619790355364482 Validation Loss: 1.1009727716445923\n",
      "Epoch 9227: Training Loss: 0.10634461045265198 Validation Loss: 1.1012815237045288\n",
      "Epoch 9228: Training Loss: 0.1065231164296468 Validation Loss: 1.101819396018982\n",
      "Epoch 9229: Training Loss: 0.10617014020681381 Validation Loss: 1.1020455360412598\n",
      "Epoch 9230: Training Loss: 0.10692404955625534 Validation Loss: 1.10172700881958\n",
      "Epoch 9231: Training Loss: 0.10637951890627544 Validation Loss: 1.1012811660766602\n",
      "Epoch 9232: Training Loss: 0.10657111555337906 Validation Loss: 1.1013444662094116\n",
      "Epoch 9233: Training Loss: 0.10678231716156006 Validation Loss: 1.1013859510421753\n",
      "Epoch 9234: Training Loss: 0.10672931373119354 Validation Loss: 1.1022323369979858\n",
      "Epoch 9235: Training Loss: 0.10653966913620631 Validation Loss: 1.1029740571975708\n",
      "Epoch 9236: Training Loss: 0.10779087245464325 Validation Loss: 1.102327585220337\n",
      "Epoch 9237: Training Loss: 0.10631428907314937 Validation Loss: 1.1021546125411987\n",
      "Epoch 9238: Training Loss: 0.10602363447348277 Validation Loss: 1.1024073362350464\n",
      "Epoch 9239: Training Loss: 0.10676801949739456 Validation Loss: 1.1026463508605957\n",
      "Epoch 9240: Training Loss: 0.1064907933274905 Validation Loss: 1.1029213666915894\n",
      "Epoch 9241: Training Loss: 0.10622036705414455 Validation Loss: 1.1035583019256592\n",
      "Epoch 9242: Training Loss: 0.10626575102408727 Validation Loss: 1.10394287109375\n",
      "Epoch 9243: Training Loss: 0.10669782757759094 Validation Loss: 1.10312020778656\n",
      "Epoch 9244: Training Loss: 0.10667230933904648 Validation Loss: 1.102327823638916\n",
      "Epoch 9245: Training Loss: 0.10629857331514359 Validation Loss: 1.1019178628921509\n",
      "Epoch 9246: Training Loss: 0.10655216624339421 Validation Loss: 1.1016087532043457\n",
      "Epoch 9247: Training Loss: 0.10652988900740941 Validation Loss: 1.102136492729187\n",
      "Epoch 9248: Training Loss: 0.10627460728089015 Validation Loss: 1.1031241416931152\n",
      "Epoch 9249: Training Loss: 0.1068713366985321 Validation Loss: 1.1038236618041992\n",
      "Epoch 9250: Training Loss: 0.1065835952758789 Validation Loss: 1.1032757759094238\n",
      "Epoch 9251: Training Loss: 0.10710348933935165 Validation Loss: 1.1017850637435913\n",
      "Epoch 9252: Training Loss: 0.10700250416994095 Validation Loss: 1.1014091968536377\n",
      "Epoch 9253: Training Loss: 0.10668818652629852 Validation Loss: 1.1020509004592896\n",
      "Epoch 9254: Training Loss: 0.10620757440725963 Validation Loss: 1.1025882959365845\n",
      "Epoch 9255: Training Loss: 0.10634733984867732 Validation Loss: 1.1023101806640625\n",
      "Epoch 9256: Training Loss: 0.10604388515154521 Validation Loss: 1.102710247039795\n",
      "Epoch 9257: Training Loss: 0.10634087026119232 Validation Loss: 1.1034306287765503\n",
      "Epoch 9258: Training Loss: 0.10633229464292526 Validation Loss: 1.1034624576568604\n",
      "Epoch 9259: Training Loss: 0.10579173266887665 Validation Loss: 1.102102518081665\n",
      "Epoch 9260: Training Loss: 0.1070361038049062 Validation Loss: 1.1012717485427856\n",
      "Epoch 9261: Training Loss: 0.10620712240537007 Validation Loss: 1.1026965379714966\n",
      "Epoch 9262: Training Loss: 0.10627767940362294 Validation Loss: 1.1022775173187256\n",
      "Epoch 9263: Training Loss: 0.10589121282100677 Validation Loss: 1.10288405418396\n",
      "Epoch 9264: Training Loss: 0.10690465072790782 Validation Loss: 1.102237343788147\n",
      "Epoch 9265: Training Loss: 0.10637050370375316 Validation Loss: 1.1031135320663452\n",
      "Epoch 9266: Training Loss: 0.10575825721025467 Validation Loss: 1.102224588394165\n",
      "Epoch 9267: Training Loss: 0.10602464030186336 Validation Loss: 1.1014937162399292\n",
      "Epoch 9268: Training Loss: 0.10733071217934291 Validation Loss: 1.1009843349456787\n",
      "Epoch 9269: Training Loss: 0.10641867419083913 Validation Loss: 1.1021345853805542\n",
      "Epoch 9270: Training Loss: 0.10610578209161758 Validation Loss: 1.1027671098709106\n",
      "Epoch 9271: Training Loss: 0.1066914622982343 Validation Loss: 1.1039495468139648\n",
      "Epoch 9272: Training Loss: 0.10624504337708156 Validation Loss: 1.1031051874160767\n",
      "Epoch 9273: Training Loss: 0.10636167724927266 Validation Loss: 1.1026047468185425\n",
      "Epoch 9274: Training Loss: 0.10594095289707184 Validation Loss: 1.1025980710983276\n",
      "Epoch 9275: Training Loss: 0.10619045048952103 Validation Loss: 1.102944016456604\n",
      "Epoch 9276: Training Loss: 0.10643664002418518 Validation Loss: 1.102594017982483\n",
      "Epoch 9277: Training Loss: 0.10664867609739304 Validation Loss: 1.102464199066162\n",
      "Epoch 9278: Training Loss: 0.1063601424296697 Validation Loss: 1.101559042930603\n",
      "Epoch 9279: Training Loss: 0.10619595895210902 Validation Loss: 1.1014736890792847\n",
      "Epoch 9280: Training Loss: 0.10611975193023682 Validation Loss: 1.102570652961731\n",
      "Epoch 9281: Training Loss: 0.1065534974137942 Validation Loss: 1.1037507057189941\n",
      "Epoch 9282: Training Loss: 0.10640288144350052 Validation Loss: 1.1059645414352417\n",
      "Epoch 9283: Training Loss: 0.10607001185417175 Validation Loss: 1.1053706407546997\n",
      "Epoch 9284: Training Loss: 0.10581962019205093 Validation Loss: 1.1035146713256836\n",
      "Epoch 9285: Training Loss: 0.10613365471363068 Validation Loss: 1.102648377418518\n",
      "Epoch 9286: Training Loss: 0.10612921665112178 Validation Loss: 1.1024974584579468\n",
      "Epoch 9287: Training Loss: 0.10622269908587138 Validation Loss: 1.102765679359436\n",
      "Epoch 9288: Training Loss: 0.10616509368022282 Validation Loss: 1.1030936241149902\n",
      "Epoch 9289: Training Loss: 0.10555336127678554 Validation Loss: 1.1036765575408936\n",
      "Epoch 9290: Training Loss: 0.10637880116701126 Validation Loss: 1.103874921798706\n",
      "Epoch 9291: Training Loss: 0.10598394026358922 Validation Loss: 1.1034411191940308\n",
      "Epoch 9292: Training Loss: 0.1059708371758461 Validation Loss: 1.1031630039215088\n",
      "Epoch 9293: Training Loss: 0.10574303815762202 Validation Loss: 1.1038448810577393\n",
      "Epoch 9294: Training Loss: 0.10612813134988149 Validation Loss: 1.1038941144943237\n",
      "Epoch 9295: Training Loss: 0.10592916111151378 Validation Loss: 1.103737473487854\n",
      "Epoch 9296: Training Loss: 0.10621424267689387 Validation Loss: 1.103996992111206\n",
      "Epoch 9297: Training Loss: 0.10584944486618042 Validation Loss: 1.1036101579666138\n",
      "Epoch 9298: Training Loss: 0.1061611498395602 Validation Loss: 1.1043267250061035\n",
      "Epoch 9299: Training Loss: 0.10610561817884445 Validation Loss: 1.104434847831726\n",
      "Epoch 9300: Training Loss: 0.10566163063049316 Validation Loss: 1.1040830612182617\n",
      "Epoch 9301: Training Loss: 0.10641025751829147 Validation Loss: 1.1028317213058472\n",
      "Epoch 9302: Training Loss: 0.1057255615790685 Validation Loss: 1.1024575233459473\n",
      "Epoch 9303: Training Loss: 0.10656531155109406 Validation Loss: 1.1025644540786743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9304: Training Loss: 0.10583644111951192 Validation Loss: 1.102107048034668\n",
      "Epoch 9305: Training Loss: 0.10577946404616038 Validation Loss: 1.1030479669570923\n",
      "Epoch 9306: Training Loss: 0.1060411309202512 Validation Loss: 1.103155493736267\n",
      "Epoch 9307: Training Loss: 0.10620121906201045 Validation Loss: 1.103203535079956\n",
      "Epoch 9308: Training Loss: 0.10626727590958278 Validation Loss: 1.1036146879196167\n",
      "Epoch 9309: Training Loss: 0.10646880914767583 Validation Loss: 1.1039241552352905\n",
      "Epoch 9310: Training Loss: 0.1060220996538798 Validation Loss: 1.1044986248016357\n",
      "Epoch 9311: Training Loss: 0.10528424630562465 Validation Loss: 1.1045341491699219\n",
      "Epoch 9312: Training Loss: 0.10589494307835896 Validation Loss: 1.1040959358215332\n",
      "Epoch 9313: Training Loss: 0.10556062559286754 Validation Loss: 1.1045550107955933\n",
      "Epoch 9314: Training Loss: 0.10591111332178116 Validation Loss: 1.1050467491149902\n",
      "Epoch 9315: Training Loss: 0.10606019198894501 Validation Loss: 1.1040637493133545\n",
      "Epoch 9316: Training Loss: 0.10581352065006892 Validation Loss: 1.1039478778839111\n",
      "Epoch 9317: Training Loss: 0.10582676778237025 Validation Loss: 1.104061484336853\n",
      "Epoch 9318: Training Loss: 0.10580601791540782 Validation Loss: 1.104614019393921\n",
      "Epoch 9319: Training Loss: 0.10564149916172028 Validation Loss: 1.104896903038025\n",
      "Epoch 9320: Training Loss: 0.1057271088163058 Validation Loss: 1.1052312850952148\n",
      "Epoch 9321: Training Loss: 0.10562491416931152 Validation Loss: 1.1047176122665405\n",
      "Epoch 9322: Training Loss: 0.10556060820817947 Validation Loss: 1.103886365890503\n",
      "Epoch 9323: Training Loss: 0.10502257694800694 Validation Loss: 1.1034364700317383\n",
      "Epoch 9324: Training Loss: 0.10593112806479137 Validation Loss: 1.1035341024398804\n",
      "Epoch 9325: Training Loss: 0.10574838519096375 Validation Loss: 1.1036145687103271\n",
      "Epoch 9326: Training Loss: 0.10587551693121593 Validation Loss: 1.103386640548706\n",
      "Epoch 9327: Training Loss: 0.10552497208118439 Validation Loss: 1.1044626235961914\n",
      "Epoch 9328: Training Loss: 0.10549558699131012 Validation Loss: 1.1048601865768433\n",
      "Epoch 9329: Training Loss: 0.10560512294371922 Validation Loss: 1.1051197052001953\n",
      "Epoch 9330: Training Loss: 0.10557275762160619 Validation Loss: 1.104622483253479\n",
      "Epoch 9331: Training Loss: 0.10596278061469395 Validation Loss: 1.1047133207321167\n",
      "Epoch 9332: Training Loss: 0.10532174756129582 Validation Loss: 1.1043763160705566\n",
      "Epoch 9333: Training Loss: 0.10553515205780666 Validation Loss: 1.1042906045913696\n",
      "Epoch 9334: Training Loss: 0.10639166831970215 Validation Loss: 1.1054401397705078\n",
      "Epoch 9335: Training Loss: 0.10549586514631908 Validation Loss: 1.1045382022857666\n",
      "Epoch 9336: Training Loss: 0.10550044476985931 Validation Loss: 1.1040325164794922\n",
      "Epoch 9337: Training Loss: 0.105565145611763 Validation Loss: 1.1044055223464966\n",
      "Epoch 9338: Training Loss: 0.10553542524576187 Validation Loss: 1.1040371656417847\n",
      "Epoch 9339: Training Loss: 0.10558538138866425 Validation Loss: 1.103523850440979\n",
      "Epoch 9340: Training Loss: 0.10566654304663341 Validation Loss: 1.1027427911758423\n",
      "Epoch 9341: Training Loss: 0.10563958436250687 Validation Loss: 1.1037286520004272\n",
      "Epoch 9342: Training Loss: 0.10545028497775395 Validation Loss: 1.1053766012191772\n",
      "Epoch 9343: Training Loss: 0.10548334568738937 Validation Loss: 1.1056861877441406\n",
      "Epoch 9344: Training Loss: 0.10543640454610188 Validation Loss: 1.1059975624084473\n",
      "Epoch 9345: Training Loss: 0.10542717576026917 Validation Loss: 1.1060291528701782\n",
      "Epoch 9346: Training Loss: 0.10526753216981888 Validation Loss: 1.1060158014297485\n",
      "Epoch 9347: Training Loss: 0.1055268148581187 Validation Loss: 1.1046265363693237\n",
      "Epoch 9348: Training Loss: 0.10619028160969417 Validation Loss: 1.1050283908843994\n",
      "Epoch 9349: Training Loss: 0.104874387383461 Validation Loss: 1.1044567823410034\n",
      "Epoch 9350: Training Loss: 0.10587129493554433 Validation Loss: 1.1048903465270996\n",
      "Epoch 9351: Training Loss: 0.1053937350710233 Validation Loss: 1.1062532663345337\n",
      "Epoch 9352: Training Loss: 0.10525703678528468 Validation Loss: 1.1057772636413574\n",
      "Epoch 9353: Training Loss: 0.10533758997917175 Validation Loss: 1.1053682565689087\n",
      "Epoch 9354: Training Loss: 0.1053847074508667 Validation Loss: 1.1047327518463135\n",
      "Epoch 9355: Training Loss: 0.10553860912720363 Validation Loss: 1.1045191287994385\n",
      "Epoch 9356: Training Loss: 0.10611693561077118 Validation Loss: 1.1042265892028809\n",
      "Epoch 9357: Training Loss: 0.10544164727131526 Validation Loss: 1.1045109033584595\n",
      "Epoch 9358: Training Loss: 0.1059192419052124 Validation Loss: 1.1049141883850098\n",
      "Epoch 9359: Training Loss: 0.10538781434297562 Validation Loss: 1.1052281856536865\n",
      "Epoch 9360: Training Loss: 0.10663372526566188 Validation Loss: 1.1061880588531494\n",
      "Epoch 9361: Training Loss: 0.10561780383189519 Validation Loss: 1.106400489807129\n",
      "Epoch 9362: Training Loss: 0.1051804746190707 Validation Loss: 1.1073393821716309\n",
      "Epoch 9363: Training Loss: 0.10518468668063481 Validation Loss: 1.1062047481536865\n",
      "Epoch 9364: Training Loss: 0.10520017147064209 Validation Loss: 1.1052215099334717\n",
      "Epoch 9365: Training Loss: 0.10537935545047124 Validation Loss: 1.1045160293579102\n",
      "Epoch 9366: Training Loss: 0.10579454650481541 Validation Loss: 1.1052066087722778\n",
      "Epoch 9367: Training Loss: 0.10506621996561687 Validation Loss: 1.104299783706665\n",
      "Epoch 9368: Training Loss: 0.10522155463695526 Validation Loss: 1.10354483127594\n",
      "Epoch 9369: Training Loss: 0.10501099129517873 Validation Loss: 1.1046640872955322\n",
      "Epoch 9370: Training Loss: 0.10564110924800237 Validation Loss: 1.1054103374481201\n",
      "Epoch 9371: Training Loss: 0.10517931977907817 Validation Loss: 1.105624794960022\n",
      "Epoch 9372: Training Loss: 0.10528084884087245 Validation Loss: 1.1056095361709595\n",
      "Epoch 9373: Training Loss: 0.10573355108499527 Validation Loss: 1.1054930686950684\n",
      "Epoch 9374: Training Loss: 0.10509279370307922 Validation Loss: 1.1055841445922852\n",
      "Epoch 9375: Training Loss: 0.10547595222791036 Validation Loss: 1.1065226793289185\n",
      "Epoch 9376: Training Loss: 0.10513294239838918 Validation Loss: 1.1062639951705933\n",
      "Epoch 9377: Training Loss: 0.10525020211935043 Validation Loss: 1.106088638305664\n",
      "Epoch 9378: Training Loss: 0.10507507870594661 Validation Loss: 1.1062724590301514\n",
      "Epoch 9379: Training Loss: 0.10520125677188237 Validation Loss: 1.1059571504592896\n",
      "Epoch 9380: Training Loss: 0.10571850587924321 Validation Loss: 1.106050968170166\n",
      "Epoch 9381: Training Loss: 0.1051889459292094 Validation Loss: 1.1064097881317139\n",
      "Epoch 9382: Training Loss: 0.10498684147993724 Validation Loss: 1.1065096855163574\n",
      "Epoch 9383: Training Loss: 0.10543313374121983 Validation Loss: 1.1072378158569336\n",
      "Epoch 9384: Training Loss: 0.10567120711008708 Validation Loss: 1.1063967943191528\n",
      "Epoch 9385: Training Loss: 0.10525689522425334 Validation Loss: 1.1068228483200073\n",
      "Epoch 9386: Training Loss: 0.10528288036584854 Validation Loss: 1.1069667339324951\n",
      "Epoch 9387: Training Loss: 0.10536932945251465 Validation Loss: 1.106353521347046\n",
      "Epoch 9388: Training Loss: 0.10534833868344624 Validation Loss: 1.1060189008712769\n",
      "Epoch 9389: Training Loss: 0.10543854037920634 Validation Loss: 1.1054694652557373\n",
      "Epoch 9390: Training Loss: 0.10512970139582951 Validation Loss: 1.1055985689163208\n",
      "Epoch 9391: Training Loss: 0.10504958033561707 Validation Loss: 1.1050795316696167\n",
      "Epoch 9392: Training Loss: 0.10568244258562724 Validation Loss: 1.1061345338821411\n",
      "Epoch 9393: Training Loss: 0.10481121887763341 Validation Loss: 1.105581283569336\n",
      "Epoch 9394: Training Loss: 0.10511220246553421 Validation Loss: 1.1050827503204346\n",
      "Epoch 9395: Training Loss: 0.10496376951535542 Validation Loss: 1.106334924697876\n",
      "Epoch 9396: Training Loss: 0.10486144324143727 Validation Loss: 1.1069509983062744\n",
      "Epoch 9397: Training Loss: 0.1047370582818985 Validation Loss: 1.1070476770401\n",
      "Epoch 9398: Training Loss: 0.10533849646647771 Validation Loss: 1.1073518991470337\n",
      "Epoch 9399: Training Loss: 0.10576138645410538 Validation Loss: 1.1070703268051147\n",
      "Epoch 9400: Training Loss: 0.1050728311141332 Validation Loss: 1.1065129041671753\n",
      "Epoch 9401: Training Loss: 0.10511700809001923 Validation Loss: 1.1051490306854248\n",
      "Epoch 9402: Training Loss: 0.1052878772219022 Validation Loss: 1.1047638654708862\n",
      "Epoch 9403: Training Loss: 0.10493586460749309 Validation Loss: 1.106372356414795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9404: Training Loss: 0.1048684020837148 Validation Loss: 1.1067581176757812\n",
      "Epoch 9405: Training Loss: 0.10529692222674687 Validation Loss: 1.1081331968307495\n",
      "Epoch 9406: Training Loss: 0.1051219254732132 Validation Loss: 1.10750412940979\n",
      "Epoch 9407: Training Loss: 0.10547090818484624 Validation Loss: 1.1065982580184937\n",
      "Epoch 9408: Training Loss: 0.10511987408002217 Validation Loss: 1.105961799621582\n",
      "Epoch 9409: Training Loss: 0.10531713316837947 Validation Loss: 1.1061211824417114\n",
      "Epoch 9410: Training Loss: 0.10509086400270462 Validation Loss: 1.1064774990081787\n",
      "Epoch 9411: Training Loss: 0.10505930582682292 Validation Loss: 1.106522560119629\n",
      "Epoch 9412: Training Loss: 0.10480571786562602 Validation Loss: 1.1069190502166748\n",
      "Epoch 9413: Training Loss: 0.10495760788520177 Validation Loss: 1.1067811250686646\n",
      "Epoch 9414: Training Loss: 0.10497922201951344 Validation Loss: 1.1073373556137085\n",
      "Epoch 9415: Training Loss: 0.10489230106274287 Validation Loss: 1.1082165241241455\n",
      "Epoch 9416: Training Loss: 0.10529597600301106 Validation Loss: 1.1065095663070679\n",
      "Epoch 9417: Training Loss: 0.1048361286520958 Validation Loss: 1.1057723760604858\n",
      "Epoch 9418: Training Loss: 0.10466525952021281 Validation Loss: 1.1053106784820557\n",
      "Epoch 9419: Training Loss: 0.1048976331949234 Validation Loss: 1.1058982610702515\n",
      "Epoch 9420: Training Loss: 0.10480679323275884 Validation Loss: 1.1063652038574219\n",
      "Epoch 9421: Training Loss: 0.10454349468151729 Validation Loss: 1.1063246726989746\n",
      "Epoch 9422: Training Loss: 0.10476050029198329 Validation Loss: 1.106703758239746\n",
      "Epoch 9423: Training Loss: 0.1044234757622083 Validation Loss: 1.1064276695251465\n",
      "Epoch 9424: Training Loss: 0.10491608331600825 Validation Loss: 1.1060030460357666\n",
      "Epoch 9425: Training Loss: 0.10480781396230061 Validation Loss: 1.1063207387924194\n",
      "Epoch 9426: Training Loss: 0.10486121972401936 Validation Loss: 1.1073541641235352\n",
      "Epoch 9427: Training Loss: 0.10500077406565349 Validation Loss: 1.1079360246658325\n",
      "Epoch 9428: Training Loss: 0.10510368645191193 Validation Loss: 1.1090247631072998\n",
      "Epoch 9429: Training Loss: 0.10490427911281586 Validation Loss: 1.1082011461257935\n",
      "Epoch 9430: Training Loss: 0.1048954650759697 Validation Loss: 1.107637882232666\n",
      "Epoch 9431: Training Loss: 0.10496302445729573 Validation Loss: 1.1067044734954834\n",
      "Epoch 9432: Training Loss: 0.10483726610740025 Validation Loss: 1.1064369678497314\n",
      "Epoch 9433: Training Loss: 0.10487521191438039 Validation Loss: 1.1068135499954224\n",
      "Epoch 9434: Training Loss: 0.10474923998117447 Validation Loss: 1.1082326173782349\n",
      "Epoch 9435: Training Loss: 0.10470144202311833 Validation Loss: 1.1088075637817383\n",
      "Epoch 9436: Training Loss: 0.1050317386786143 Validation Loss: 1.1079065799713135\n",
      "Epoch 9437: Training Loss: 0.10550803691148758 Validation Loss: 1.1077297925949097\n",
      "Epoch 9438: Training Loss: 0.10505134115616481 Validation Loss: 1.1074376106262207\n",
      "Epoch 9439: Training Loss: 0.1051393672823906 Validation Loss: 1.1072697639465332\n",
      "Epoch 9440: Training Loss: 0.10477493206659953 Validation Loss: 1.1070066690444946\n",
      "Epoch 9441: Training Loss: 0.10556324571371078 Validation Loss: 1.1071410179138184\n",
      "Epoch 9442: Training Loss: 0.104612131913503 Validation Loss: 1.1071454286575317\n",
      "Epoch 9443: Training Loss: 0.10509342451890309 Validation Loss: 1.1074460744857788\n",
      "Epoch 9444: Training Loss: 0.10477115710576375 Validation Loss: 1.1075153350830078\n",
      "Epoch 9445: Training Loss: 0.10471434891223907 Validation Loss: 1.1085355281829834\n",
      "Epoch 9446: Training Loss: 0.10594041645526886 Validation Loss: 1.108742594718933\n",
      "Epoch 9447: Training Loss: 0.10461912304162979 Validation Loss: 1.1079957485198975\n",
      "Epoch 9448: Training Loss: 0.10487856715917587 Validation Loss: 1.108223557472229\n",
      "Epoch 9449: Training Loss: 0.10481830934683482 Validation Loss: 1.1075026988983154\n",
      "Epoch 9450: Training Loss: 0.1053176149725914 Validation Loss: 1.1066988706588745\n",
      "Epoch 9451: Training Loss: 0.10473661373058955 Validation Loss: 1.1069697141647339\n",
      "Epoch 9452: Training Loss: 0.10477386663357417 Validation Loss: 1.1084465980529785\n",
      "Epoch 9453: Training Loss: 0.10520302007595699 Validation Loss: 1.1086716651916504\n",
      "Epoch 9454: Training Loss: 0.10451658566792806 Validation Loss: 1.1090422868728638\n",
      "Epoch 9455: Training Loss: 0.10474174469709396 Validation Loss: 1.1088613271713257\n",
      "Epoch 9456: Training Loss: 0.10459395498037338 Validation Loss: 1.1083028316497803\n",
      "Epoch 9457: Training Loss: 0.10559883217016856 Validation Loss: 1.1067469120025635\n",
      "Epoch 9458: Training Loss: 0.10473902026812236 Validation Loss: 1.1068122386932373\n",
      "Epoch 9459: Training Loss: 0.10453317314386368 Validation Loss: 1.10698664188385\n",
      "Epoch 9460: Training Loss: 0.104593130449454 Validation Loss: 1.107915997505188\n",
      "Epoch 9461: Training Loss: 0.10487860441207886 Validation Loss: 1.108663558959961\n",
      "Epoch 9462: Training Loss: 0.10404289762179057 Validation Loss: 1.109160304069519\n",
      "Epoch 9463: Training Loss: 0.10427132248878479 Validation Loss: 1.1081178188323975\n",
      "Epoch 9464: Training Loss: 0.10451198369264603 Validation Loss: 1.1081901788711548\n",
      "Epoch 9465: Training Loss: 0.10503504673639934 Validation Loss: 1.107730746269226\n",
      "Epoch 9466: Training Loss: 0.10509080439805984 Validation Loss: 1.1084975004196167\n",
      "Epoch 9467: Training Loss: 0.10449027270078659 Validation Loss: 1.108100175857544\n",
      "Epoch 9468: Training Loss: 0.10429478933413823 Validation Loss: 1.1089913845062256\n",
      "Epoch 9469: Training Loss: 0.10389666010936101 Validation Loss: 1.1090052127838135\n",
      "Epoch 9470: Training Loss: 0.10441496223211288 Validation Loss: 1.1090916395187378\n",
      "Epoch 9471: Training Loss: 0.1045527458190918 Validation Loss: 1.1085911989212036\n",
      "Epoch 9472: Training Loss: 0.10461532572905223 Validation Loss: 1.1086931228637695\n",
      "Epoch 9473: Training Loss: 0.1048712432384491 Validation Loss: 1.1084102392196655\n",
      "Epoch 9474: Training Loss: 0.1058929314215978 Validation Loss: 1.108266830444336\n",
      "Epoch 9475: Training Loss: 0.10499364386002223 Validation Loss: 1.1091649532318115\n",
      "Epoch 9476: Training Loss: 0.10460684448480606 Validation Loss: 1.1083375215530396\n",
      "Epoch 9477: Training Loss: 0.10505281140406926 Validation Loss: 1.1095913648605347\n",
      "Epoch 9478: Training Loss: 0.10462530950705211 Validation Loss: 1.1094378232955933\n",
      "Epoch 9479: Training Loss: 0.10461858411629994 Validation Loss: 1.1102690696716309\n",
      "Epoch 9480: Training Loss: 0.10446511209011078 Validation Loss: 1.1110670566558838\n",
      "Epoch 9481: Training Loss: 0.10451135287682216 Validation Loss: 1.1095507144927979\n",
      "Epoch 9482: Training Loss: 0.10423474262158076 Validation Loss: 1.1088377237319946\n",
      "Epoch 9483: Training Loss: 0.10452939073244731 Validation Loss: 1.109055995941162\n",
      "Epoch 9484: Training Loss: 0.10487028708060582 Validation Loss: 1.1093270778656006\n",
      "Epoch 9485: Training Loss: 0.10429159551858902 Validation Loss: 1.1089396476745605\n",
      "Epoch 9486: Training Loss: 0.10422403613726298 Validation Loss: 1.109188437461853\n",
      "Epoch 9487: Training Loss: 0.10477485756079356 Validation Loss: 1.1090196371078491\n",
      "Epoch 9488: Training Loss: 0.10428975770870845 Validation Loss: 1.1074262857437134\n",
      "Epoch 9489: Training Loss: 0.10415344685316086 Validation Loss: 1.1069046258926392\n",
      "Epoch 9490: Training Loss: 0.10409380247195561 Validation Loss: 1.107075810432434\n",
      "Epoch 9491: Training Loss: 0.10437935342391332 Validation Loss: 1.1075420379638672\n",
      "Epoch 9492: Training Loss: 0.10459099461634953 Validation Loss: 1.1096051931381226\n",
      "Epoch 9493: Training Loss: 0.10439697653055191 Validation Loss: 1.1110135316848755\n",
      "Epoch 9494: Training Loss: 0.10439463704824448 Validation Loss: 1.111535906791687\n",
      "Epoch 9495: Training Loss: 0.1041211908062299 Validation Loss: 1.1107738018035889\n",
      "Epoch 9496: Training Loss: 0.10454855859279633 Validation Loss: 1.108647346496582\n",
      "Epoch 9497: Training Loss: 0.10428713510433833 Validation Loss: 1.1078569889068604\n",
      "Epoch 9498: Training Loss: 0.10440169771512349 Validation Loss: 1.1076130867004395\n",
      "Epoch 9499: Training Loss: 0.10487879067659378 Validation Loss: 1.1084394454956055\n",
      "Epoch 9500: Training Loss: 0.10422170907258987 Validation Loss: 1.1092520952224731\n",
      "Epoch 9501: Training Loss: 0.10517203311125438 Validation Loss: 1.110654354095459\n",
      "Epoch 9502: Training Loss: 0.10443763931592305 Validation Loss: 1.110465168952942\n",
      "Epoch 9503: Training Loss: 0.104596513013045 Validation Loss: 1.1096447706222534\n",
      "Epoch 9504: Training Loss: 0.10397735983133316 Validation Loss: 1.1096669435501099\n",
      "Epoch 9505: Training Loss: 0.10417354106903076 Validation Loss: 1.109397053718567\n",
      "Epoch 9506: Training Loss: 0.10402233898639679 Validation Loss: 1.1090664863586426\n",
      "Epoch 9507: Training Loss: 0.1041531835993131 Validation Loss: 1.110519289970398\n",
      "Epoch 9508: Training Loss: 0.10371801008780797 Validation Loss: 1.1105109453201294\n",
      "Epoch 9509: Training Loss: 0.10430842638015747 Validation Loss: 1.1100291013717651\n",
      "Epoch 9510: Training Loss: 0.10400610913832982 Validation Loss: 1.1102116107940674\n",
      "Epoch 9511: Training Loss: 0.10407208154598872 Validation Loss: 1.1092098951339722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9512: Training Loss: 0.10388804227113724 Validation Loss: 1.1091537475585938\n",
      "Epoch 9513: Training Loss: 0.1040443554520607 Validation Loss: 1.1095449924468994\n",
      "Epoch 9514: Training Loss: 0.10421766092379887 Validation Loss: 1.109650731086731\n",
      "Epoch 9515: Training Loss: 0.10458523780107498 Validation Loss: 1.1107542514801025\n",
      "Epoch 9516: Training Loss: 0.10416340827941895 Validation Loss: 1.1099961996078491\n",
      "Epoch 9517: Training Loss: 0.10414092242717743 Validation Loss: 1.109268069267273\n",
      "Epoch 9518: Training Loss: 0.10387364526589711 Validation Loss: 1.1091564893722534\n",
      "Epoch 9519: Training Loss: 0.10431657979885738 Validation Loss: 1.10980224609375\n",
      "Epoch 9520: Training Loss: 0.10493249197800954 Validation Loss: 1.109333872795105\n",
      "Epoch 9521: Training Loss: 0.10513534396886826 Validation Loss: 1.1092473268508911\n",
      "Epoch 9522: Training Loss: 0.10381914178530376 Validation Loss: 1.1099863052368164\n",
      "Epoch 9523: Training Loss: 0.10445735603570938 Validation Loss: 1.1115401983261108\n",
      "Epoch 9524: Training Loss: 0.10435988008975983 Validation Loss: 1.112121343612671\n",
      "Epoch 9525: Training Loss: 0.10457334170738856 Validation Loss: 1.1114987134933472\n",
      "Epoch 9526: Training Loss: 0.10385676721731822 Validation Loss: 1.1101791858673096\n",
      "Epoch 9527: Training Loss: 0.10446650286515553 Validation Loss: 1.1096755266189575\n",
      "Epoch 9528: Training Loss: 0.10440958539644878 Validation Loss: 1.1096147298812866\n",
      "Epoch 9529: Training Loss: 0.1043145681420962 Validation Loss: 1.1091160774230957\n",
      "Epoch 9530: Training Loss: 0.10480732967456181 Validation Loss: 1.1093367338180542\n",
      "Epoch 9531: Training Loss: 0.10433874279260635 Validation Loss: 1.1090517044067383\n",
      "Epoch 9532: Training Loss: 0.10406278570493062 Validation Loss: 1.1101267337799072\n",
      "Epoch 9533: Training Loss: 0.10401308039824168 Validation Loss: 1.1114267110824585\n",
      "Epoch 9534: Training Loss: 0.10403313239415486 Validation Loss: 1.110332727432251\n",
      "Epoch 9535: Training Loss: 0.10393956800301869 Validation Loss: 1.1100000143051147\n",
      "Epoch 9536: Training Loss: 0.1035721202691396 Validation Loss: 1.109632134437561\n",
      "Epoch 9537: Training Loss: 0.1045527458190918 Validation Loss: 1.10941743850708\n",
      "Epoch 9538: Training Loss: 0.1039437750975291 Validation Loss: 1.109189510345459\n",
      "Epoch 9539: Training Loss: 0.10359484950701396 Validation Loss: 1.108959674835205\n",
      "Epoch 9540: Training Loss: 0.10445020099480946 Validation Loss: 1.1100844144821167\n",
      "Epoch 9541: Training Loss: 0.10360829780499141 Validation Loss: 1.1102921962738037\n",
      "Epoch 9542: Training Loss: 0.10413868725299835 Validation Loss: 1.110249638557434\n",
      "Epoch 9543: Training Loss: 0.10409234464168549 Validation Loss: 1.1107416152954102\n",
      "Epoch 9544: Training Loss: 0.10432431101799011 Validation Loss: 1.1111613512039185\n",
      "Epoch 9545: Training Loss: 0.10383269439140956 Validation Loss: 1.1109914779663086\n",
      "Epoch 9546: Training Loss: 0.10412982106208801 Validation Loss: 1.11173415184021\n",
      "Epoch 9547: Training Loss: 0.10390252868334453 Validation Loss: 1.1113864183425903\n",
      "Epoch 9548: Training Loss: 0.103700357178847 Validation Loss: 1.1103930473327637\n",
      "Epoch 9549: Training Loss: 0.10403013477722804 Validation Loss: 1.1106977462768555\n",
      "Epoch 9550: Training Loss: 0.10396717737118404 Validation Loss: 1.1109602451324463\n",
      "Epoch 9551: Training Loss: 0.10373100886742274 Validation Loss: 1.1105782985687256\n",
      "Epoch 9552: Training Loss: 0.10371889173984528 Validation Loss: 1.1104998588562012\n",
      "Epoch 9553: Training Loss: 0.10350090265274048 Validation Loss: 1.1101205348968506\n",
      "Epoch 9554: Training Loss: 0.10386075576146443 Validation Loss: 1.109631896018982\n",
      "Epoch 9555: Training Loss: 0.10388655463854472 Validation Loss: 1.109649896621704\n",
      "Epoch 9556: Training Loss: 0.10373675326506297 Validation Loss: 1.109955906867981\n",
      "Epoch 9557: Training Loss: 0.10362533231576283 Validation Loss: 1.1111880540847778\n",
      "Epoch 9558: Training Loss: 0.10391900191704433 Validation Loss: 1.1113367080688477\n",
      "Epoch 9559: Training Loss: 0.10410189131895702 Validation Loss: 1.1111122369766235\n",
      "Epoch 9560: Training Loss: 0.10383659601211548 Validation Loss: 1.1111501455307007\n",
      "Epoch 9561: Training Loss: 0.1038350115219752 Validation Loss: 1.111769199371338\n",
      "Epoch 9562: Training Loss: 0.10412475963433583 Validation Loss: 1.1104353666305542\n",
      "Epoch 9563: Training Loss: 0.10376972953478496 Validation Loss: 1.1100648641586304\n",
      "Epoch 9564: Training Loss: 0.10386727005243301 Validation Loss: 1.109721302986145\n",
      "Epoch 9565: Training Loss: 0.10382000108559926 Validation Loss: 1.1105480194091797\n",
      "Epoch 9566: Training Loss: 0.10427966217199962 Validation Loss: 1.1112282276153564\n",
      "Epoch 9567: Training Loss: 0.10369013249874115 Validation Loss: 1.111043095588684\n",
      "Epoch 9568: Training Loss: 0.10377151270707448 Validation Loss: 1.1118518114089966\n",
      "Epoch 9569: Training Loss: 0.10402641942103703 Validation Loss: 1.111790657043457\n",
      "Epoch 9570: Training Loss: 0.10381420701742172 Validation Loss: 1.1118086576461792\n",
      "Epoch 9571: Training Loss: 0.10392457991838455 Validation Loss: 1.1124298572540283\n",
      "Epoch 9572: Training Loss: 0.10380070159832637 Validation Loss: 1.1117066144943237\n",
      "Epoch 9573: Training Loss: 0.1037689099709193 Validation Loss: 1.11064875125885\n",
      "Epoch 9574: Training Loss: 0.1040606548388799 Validation Loss: 1.1105406284332275\n",
      "Epoch 9575: Training Loss: 0.1037318507830302 Validation Loss: 1.11125648021698\n",
      "Epoch 9576: Training Loss: 0.10373796274264653 Validation Loss: 1.111177682876587\n",
      "Epoch 9577: Training Loss: 0.10391319294770558 Validation Loss: 1.1107629537582397\n",
      "Epoch 9578: Training Loss: 0.10366901010274887 Validation Loss: 1.1111005544662476\n",
      "Epoch 9579: Training Loss: 0.10373105605443318 Validation Loss: 1.1114068031311035\n",
      "Epoch 9580: Training Loss: 0.1036561851700147 Validation Loss: 1.111761450767517\n",
      "Epoch 9581: Training Loss: 0.10345456749200821 Validation Loss: 1.111161231994629\n",
      "Epoch 9582: Training Loss: 0.10353174060583115 Validation Loss: 1.1113097667694092\n",
      "Epoch 9583: Training Loss: 0.10370497405529022 Validation Loss: 1.1125260591506958\n",
      "Epoch 9584: Training Loss: 0.1040414571762085 Validation Loss: 1.1126322746276855\n",
      "Epoch 9585: Training Loss: 0.10356022914250691 Validation Loss: 1.1128610372543335\n",
      "Epoch 9586: Training Loss: 0.10306218514839809 Validation Loss: 1.111537218093872\n",
      "Epoch 9587: Training Loss: 0.10440719624360402 Validation Loss: 1.1120669841766357\n",
      "Epoch 9588: Training Loss: 0.10321542620658875 Validation Loss: 1.112213373184204\n",
      "Epoch 9589: Training Loss: 0.10410648832718532 Validation Loss: 1.1112112998962402\n",
      "Epoch 9590: Training Loss: 0.10367610057195027 Validation Loss: 1.1113168001174927\n",
      "Epoch 9591: Training Loss: 0.10364902516206105 Validation Loss: 1.1110769510269165\n",
      "Epoch 9592: Training Loss: 0.10368466377258301 Validation Loss: 1.111013650894165\n",
      "Epoch 9593: Training Loss: 0.103226604561011 Validation Loss: 1.1112723350524902\n",
      "Epoch 9594: Training Loss: 0.10358894616365433 Validation Loss: 1.1113817691802979\n",
      "Epoch 9595: Training Loss: 0.1034192144870758 Validation Loss: 1.1118416786193848\n",
      "Epoch 9596: Training Loss: 0.10427917540073395 Validation Loss: 1.111862063407898\n",
      "Epoch 9597: Training Loss: 0.10352908323208491 Validation Loss: 1.1115565299987793\n",
      "Epoch 9598: Training Loss: 0.1036158949136734 Validation Loss: 1.112233281135559\n",
      "Epoch 9599: Training Loss: 0.10401740670204163 Validation Loss: 1.1122573614120483\n",
      "Epoch 9600: Training Loss: 0.1033379261692365 Validation Loss: 1.1113708019256592\n",
      "Epoch 9601: Training Loss: 0.10351731876532237 Validation Loss: 1.111832857131958\n",
      "Epoch 9602: Training Loss: 0.10354499518871307 Validation Loss: 1.1117712259292603\n",
      "Epoch 9603: Training Loss: 0.10357477019230525 Validation Loss: 1.11122727394104\n",
      "Epoch 9604: Training Loss: 0.10351759443680446 Validation Loss: 1.1120061874389648\n",
      "Epoch 9605: Training Loss: 0.10283628354469936 Validation Loss: 1.1118991374969482\n",
      "Epoch 9606: Training Loss: 0.1035165364543597 Validation Loss: 1.111907958984375\n",
      "Epoch 9607: Training Loss: 0.10369752844174702 Validation Loss: 1.1106454133987427\n",
      "Epoch 9608: Training Loss: 0.10357565184434255 Validation Loss: 1.1105986833572388\n",
      "Epoch 9609: Training Loss: 0.1033279150724411 Validation Loss: 1.1113687753677368\n",
      "Epoch 9610: Training Loss: 0.10325475285450618 Validation Loss: 1.1124701499938965\n",
      "Epoch 9611: Training Loss: 0.10395560165246327 Validation Loss: 1.1130191087722778\n",
      "Epoch 9612: Training Loss: 0.10344269871711731 Validation Loss: 1.1125150918960571\n",
      "Epoch 9613: Training Loss: 0.10328550885121028 Validation Loss: 1.112447738647461\n",
      "Epoch 9614: Training Loss: 0.10396872957547505 Validation Loss: 1.1140626668930054\n",
      "Epoch 9615: Training Loss: 0.10351479550202687 Validation Loss: 1.1134566068649292\n",
      "Epoch 9616: Training Loss: 0.10364563018083572 Validation Loss: 1.1115878820419312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9617: Training Loss: 0.10330404589573543 Validation Loss: 1.1113536357879639\n",
      "Epoch 9618: Training Loss: 0.10373551150163014 Validation Loss: 1.1113721132278442\n",
      "Epoch 9619: Training Loss: 0.10377739618221919 Validation Loss: 1.113229751586914\n",
      "Epoch 9620: Training Loss: 0.10470240066448848 Validation Loss: 1.1135741472244263\n",
      "Epoch 9621: Training Loss: 0.10343793282906215 Validation Loss: 1.1128573417663574\n",
      "Epoch 9622: Training Loss: 0.10332850863536198 Validation Loss: 1.1128698587417603\n",
      "Epoch 9623: Training Loss: 0.10420407851537068 Validation Loss: 1.112816333770752\n",
      "Epoch 9624: Training Loss: 0.1031523123383522 Validation Loss: 1.112534999847412\n",
      "Epoch 9625: Training Loss: 0.10273579756418864 Validation Loss: 1.1128287315368652\n",
      "Epoch 9626: Training Loss: 0.10392653445402782 Validation Loss: 1.1119623184204102\n",
      "Epoch 9627: Training Loss: 0.10361086080471675 Validation Loss: 1.111404299736023\n",
      "Epoch 9628: Training Loss: 0.10348449895779292 Validation Loss: 1.1137497425079346\n",
      "Epoch 9629: Training Loss: 0.103170412282149 Validation Loss: 1.1146548986434937\n",
      "Epoch 9630: Training Loss: 0.10322246203819911 Validation Loss: 1.1138336658477783\n",
      "Epoch 9631: Training Loss: 0.10292530804872513 Validation Loss: 1.11246657371521\n",
      "Epoch 9632: Training Loss: 0.10316494852304459 Validation Loss: 1.1115236282348633\n",
      "Epoch 9633: Training Loss: 0.10328803459803264 Validation Loss: 1.1116276979446411\n",
      "Epoch 9634: Training Loss: 0.10421594480673473 Validation Loss: 1.111772894859314\n",
      "Epoch 9635: Training Loss: 0.10302452246348064 Validation Loss: 1.1129875183105469\n",
      "Epoch 9636: Training Loss: 0.10339101900657018 Validation Loss: 1.113131046295166\n",
      "Epoch 9637: Training Loss: 0.10315278172492981 Validation Loss: 1.1133666038513184\n",
      "Epoch 9638: Training Loss: 0.10302685697873433 Validation Loss: 1.1128886938095093\n",
      "Epoch 9639: Training Loss: 0.10396109769741695 Validation Loss: 1.1144529581069946\n",
      "Epoch 9640: Training Loss: 0.10372435301542282 Validation Loss: 1.1144367456436157\n",
      "Epoch 9641: Training Loss: 0.10355659574270248 Validation Loss: 1.1157827377319336\n",
      "Epoch 9642: Training Loss: 0.1027838687102 Validation Loss: 1.1143550872802734\n",
      "Epoch 9643: Training Loss: 0.10309848189353943 Validation Loss: 1.1137677431106567\n",
      "Epoch 9644: Training Loss: 0.10300467163324356 Validation Loss: 1.1129144430160522\n",
      "Epoch 9645: Training Loss: 0.10364214827617009 Validation Loss: 1.1123124361038208\n",
      "Epoch 9646: Training Loss: 0.10335317999124527 Validation Loss: 1.1126651763916016\n",
      "Epoch 9647: Training Loss: 0.10315386950969696 Validation Loss: 1.113335132598877\n",
      "Epoch 9648: Training Loss: 0.10361238320668538 Validation Loss: 1.1134763956069946\n",
      "Epoch 9649: Training Loss: 0.10333520670731862 Validation Loss: 1.1138676404953003\n",
      "Epoch 9650: Training Loss: 0.10283566763003667 Validation Loss: 1.1143848896026611\n",
      "Epoch 9651: Training Loss: 0.10351396103700002 Validation Loss: 1.1128990650177002\n",
      "Epoch 9652: Training Loss: 0.10306003441413243 Validation Loss: 1.1131726503372192\n",
      "Epoch 9653: Training Loss: 0.10319845875104268 Validation Loss: 1.113080620765686\n",
      "Epoch 9654: Training Loss: 0.10349111755688985 Validation Loss: 1.113118052482605\n",
      "Epoch 9655: Training Loss: 0.10312578578790028 Validation Loss: 1.1128214597702026\n",
      "Epoch 9656: Training Loss: 0.10323494424422582 Validation Loss: 1.1136468648910522\n",
      "Epoch 9657: Training Loss: 0.10317608217398326 Validation Loss: 1.1134523153305054\n",
      "Epoch 9658: Training Loss: 0.10302053640286128 Validation Loss: 1.1133778095245361\n",
      "Epoch 9659: Training Loss: 0.1035551130771637 Validation Loss: 1.1134248971939087\n",
      "Epoch 9660: Training Loss: 0.10277011493841808 Validation Loss: 1.1141842603683472\n",
      "Epoch 9661: Training Loss: 0.1029345045487086 Validation Loss: 1.1137009859085083\n",
      "Epoch 9662: Training Loss: 0.10270869980255763 Validation Loss: 1.1137303113937378\n",
      "Epoch 9663: Training Loss: 0.10310260206460953 Validation Loss: 1.1139857769012451\n",
      "Epoch 9664: Training Loss: 0.10296442608038585 Validation Loss: 1.1143152713775635\n",
      "Epoch 9665: Training Loss: 0.10299394031365712 Validation Loss: 1.114294171333313\n",
      "Epoch 9666: Training Loss: 0.10275691250960033 Validation Loss: 1.1134614944458008\n",
      "Epoch 9667: Training Loss: 0.10309661428133647 Validation Loss: 1.1127876043319702\n",
      "Epoch 9668: Training Loss: 0.10370319336652756 Validation Loss: 1.112436294555664\n",
      "Epoch 9669: Training Loss: 0.10293834904829662 Validation Loss: 1.1134051084518433\n",
      "Epoch 9670: Training Loss: 0.1028217002749443 Validation Loss: 1.114057183265686\n",
      "Epoch 9671: Training Loss: 0.10388851910829544 Validation Loss: 1.114806056022644\n",
      "Epoch 9672: Training Loss: 0.10263214508692424 Validation Loss: 1.115110158920288\n",
      "Epoch 9673: Training Loss: 0.10302529980738957 Validation Loss: 1.1142888069152832\n",
      "Epoch 9674: Training Loss: 0.10428338994582494 Validation Loss: 1.1135203838348389\n",
      "Epoch 9675: Training Loss: 0.10303184390068054 Validation Loss: 1.1126842498779297\n",
      "Epoch 9676: Training Loss: 0.10273784895737965 Validation Loss: 1.1137659549713135\n",
      "Epoch 9677: Training Loss: 0.10301477462053299 Validation Loss: 1.1139172315597534\n",
      "Epoch 9678: Training Loss: 0.10300325105587642 Validation Loss: 1.1150660514831543\n",
      "Epoch 9679: Training Loss: 0.10289493451515834 Validation Loss: 1.115586519241333\n",
      "Epoch 9680: Training Loss: 0.10288681586583455 Validation Loss: 1.1150338649749756\n",
      "Epoch 9681: Training Loss: 0.10277219365040462 Validation Loss: 1.1143579483032227\n",
      "Epoch 9682: Training Loss: 0.10273014008998871 Validation Loss: 1.1136549711227417\n",
      "Epoch 9683: Training Loss: 0.102885402739048 Validation Loss: 1.1150990724563599\n",
      "Epoch 9684: Training Loss: 0.10359606643517812 Validation Loss: 1.1138476133346558\n",
      "Epoch 9685: Training Loss: 0.10277829070885976 Validation Loss: 1.1140773296356201\n",
      "Epoch 9686: Training Loss: 0.10223449518283208 Validation Loss: 1.1135015487670898\n",
      "Epoch 9687: Training Loss: 0.10267297923564911 Validation Loss: 1.114622712135315\n",
      "Epoch 9688: Training Loss: 0.10293541351954143 Validation Loss: 1.1139581203460693\n",
      "Epoch 9689: Training Loss: 0.10294785847266515 Validation Loss: 1.1151485443115234\n",
      "Epoch 9690: Training Loss: 0.10257442047198613 Validation Loss: 1.1154439449310303\n",
      "Epoch 9691: Training Loss: 0.10294155776500702 Validation Loss: 1.114804983139038\n",
      "Epoch 9692: Training Loss: 0.10355573395888011 Validation Loss: 1.1144723892211914\n",
      "Epoch 9693: Training Loss: 0.10257019350926082 Validation Loss: 1.1151694059371948\n",
      "Epoch 9694: Training Loss: 0.10275210936864217 Validation Loss: 1.1154321432113647\n",
      "Epoch 9695: Training Loss: 0.10323407252629598 Validation Loss: 1.1151447296142578\n",
      "Epoch 9696: Training Loss: 0.10324850678443909 Validation Loss: 1.1139233112335205\n",
      "Epoch 9697: Training Loss: 0.10340755929549535 Validation Loss: 1.114357590675354\n",
      "Epoch 9698: Training Loss: 0.10278814285993576 Validation Loss: 1.1143304109573364\n",
      "Epoch 9699: Training Loss: 0.10279430449008942 Validation Loss: 1.1141302585601807\n",
      "Epoch 9700: Training Loss: 0.10345881680647533 Validation Loss: 1.1152650117874146\n",
      "Epoch 9701: Training Loss: 0.10288443913062413 Validation Loss: 1.1153908967971802\n",
      "Epoch 9702: Training Loss: 0.10324972619613011 Validation Loss: 1.1167216300964355\n",
      "Epoch 9703: Training Loss: 0.10299802323182423 Validation Loss: 1.1162264347076416\n",
      "Epoch 9704: Training Loss: 0.10325587789217631 Validation Loss: 1.1143847703933716\n",
      "Epoch 9705: Training Loss: 0.10294797519842784 Validation Loss: 1.1140249967575073\n",
      "Epoch 9706: Training Loss: 0.10270129144191742 Validation Loss: 1.1138110160827637\n",
      "Epoch 9707: Training Loss: 0.10262359430392583 Validation Loss: 1.1151176691055298\n",
      "Epoch 9708: Training Loss: 0.10359555979569753 Validation Loss: 1.1154239177703857\n",
      "Epoch 9709: Training Loss: 0.10302580396334331 Validation Loss: 1.1149406433105469\n",
      "Epoch 9710: Training Loss: 0.10330108304818471 Validation Loss: 1.1152424812316895\n",
      "Epoch 9711: Training Loss: 0.10244321326414745 Validation Loss: 1.1155751943588257\n",
      "Epoch 9712: Training Loss: 0.10341836760441463 Validation Loss: 1.1154389381408691\n",
      "Epoch 9713: Training Loss: 0.10219753533601761 Validation Loss: 1.1160088777542114\n",
      "Epoch 9714: Training Loss: 0.10286720593770345 Validation Loss: 1.1149927377700806\n",
      "Epoch 9715: Training Loss: 0.1027085930109024 Validation Loss: 1.1154507398605347\n",
      "Epoch 9716: Training Loss: 0.10274942219257355 Validation Loss: 1.1148622035980225\n",
      "Epoch 9717: Training Loss: 0.10253538439671199 Validation Loss: 1.115350365638733\n",
      "Epoch 9718: Training Loss: 0.10257381945848465 Validation Loss: 1.1159048080444336\n",
      "Epoch 9719: Training Loss: 0.1029397373398145 Validation Loss: 1.1155180931091309\n",
      "Epoch 9720: Training Loss: 0.10270755489667256 Validation Loss: 1.1149286031723022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9721: Training Loss: 0.10283944755792618 Validation Loss: 1.1153219938278198\n",
      "Epoch 9722: Training Loss: 0.10261218746503194 Validation Loss: 1.1159112453460693\n",
      "Epoch 9723: Training Loss: 0.1028043602903684 Validation Loss: 1.115970253944397\n",
      "Epoch 9724: Training Loss: 0.10264357676108678 Validation Loss: 1.1153913736343384\n",
      "Epoch 9725: Training Loss: 0.10242116202910741 Validation Loss: 1.1158894300460815\n",
      "Epoch 9726: Training Loss: 0.10250718891620636 Validation Loss: 1.115515947341919\n",
      "Epoch 9727: Training Loss: 0.10256565858920415 Validation Loss: 1.1152206659317017\n",
      "Epoch 9728: Training Loss: 0.10325963298479716 Validation Loss: 1.1149605512619019\n",
      "Epoch 9729: Training Loss: 0.10267135004202525 Validation Loss: 1.1157619953155518\n",
      "Epoch 9730: Training Loss: 0.10312193632125854 Validation Loss: 1.1160759925842285\n",
      "Epoch 9731: Training Loss: 0.102622223397096 Validation Loss: 1.1159662008285522\n",
      "Epoch 9732: Training Loss: 0.1023837352792422 Validation Loss: 1.1152516603469849\n",
      "Epoch 9733: Training Loss: 0.10244274884462357 Validation Loss: 1.115175724029541\n",
      "Epoch 9734: Training Loss: 0.1029306302467982 Validation Loss: 1.114920973777771\n",
      "Epoch 9735: Training Loss: 0.10192315528790157 Validation Loss: 1.1157904863357544\n",
      "Epoch 9736: Training Loss: 0.10272821535666783 Validation Loss: 1.1172475814819336\n",
      "Epoch 9737: Training Loss: 0.10241117825110753 Validation Loss: 1.116182804107666\n",
      "Epoch 9738: Training Loss: 0.10235707710186641 Validation Loss: 1.116886854171753\n",
      "Epoch 9739: Training Loss: 0.10239784171183904 Validation Loss: 1.116940975189209\n",
      "Epoch 9740: Training Loss: 0.1024010603626569 Validation Loss: 1.116610050201416\n",
      "Epoch 9741: Training Loss: 0.10247711340586345 Validation Loss: 1.1151822805404663\n",
      "Epoch 9742: Training Loss: 0.102240189909935 Validation Loss: 1.114724040031433\n",
      "Epoch 9743: Training Loss: 0.10264100382725398 Validation Loss: 1.1150950193405151\n",
      "Epoch 9744: Training Loss: 0.10256441434224446 Validation Loss: 1.1160948276519775\n",
      "Epoch 9745: Training Loss: 0.10249030341704686 Validation Loss: 1.1165179014205933\n",
      "Epoch 9746: Training Loss: 0.10282640159130096 Validation Loss: 1.1157159805297852\n",
      "Epoch 9747: Training Loss: 0.10269876569509506 Validation Loss: 1.1162710189819336\n",
      "Epoch 9748: Training Loss: 0.10259062300125758 Validation Loss: 1.1175040006637573\n",
      "Epoch 9749: Training Loss: 0.10262832542260487 Validation Loss: 1.1155270338058472\n",
      "Epoch 9750: Training Loss: 0.10288219650586446 Validation Loss: 1.1158698797225952\n",
      "Epoch 9751: Training Loss: 0.10237131019433339 Validation Loss: 1.1166307926177979\n",
      "Epoch 9752: Training Loss: 0.103268563747406 Validation Loss: 1.1167744398117065\n",
      "Epoch 9753: Training Loss: 0.10238500436147054 Validation Loss: 1.1162253618240356\n",
      "Epoch 9754: Training Loss: 0.10236682494481404 Validation Loss: 1.1167036294937134\n",
      "Epoch 9755: Training Loss: 0.10277389734983444 Validation Loss: 1.1164451837539673\n",
      "Epoch 9756: Training Loss: 0.10245085507631302 Validation Loss: 1.1166740655899048\n",
      "Epoch 9757: Training Loss: 0.10245000571012497 Validation Loss: 1.1174261569976807\n",
      "Epoch 9758: Training Loss: 0.10300911714633305 Validation Loss: 1.1176936626434326\n",
      "Epoch 9759: Training Loss: 0.10268289099136989 Validation Loss: 1.1175700426101685\n",
      "Epoch 9760: Training Loss: 0.10249018420775731 Validation Loss: 1.1171969175338745\n",
      "Epoch 9761: Training Loss: 0.10246624052524567 Validation Loss: 1.1167422533035278\n",
      "Epoch 9762: Training Loss: 0.10302197436491649 Validation Loss: 1.1174262762069702\n",
      "Epoch 9763: Training Loss: 0.10323477784792583 Validation Loss: 1.1172791719436646\n",
      "Epoch 9764: Training Loss: 0.10258482644955318 Validation Loss: 1.116400122642517\n",
      "Epoch 9765: Training Loss: 0.10228143632411957 Validation Loss: 1.116503357887268\n",
      "Epoch 9766: Training Loss: 0.10349445044994354 Validation Loss: 1.1158403158187866\n",
      "Epoch 9767: Training Loss: 0.10229814052581787 Validation Loss: 1.115807294845581\n",
      "Epoch 9768: Training Loss: 0.1024588073293368 Validation Loss: 1.11652672290802\n",
      "Epoch 9769: Training Loss: 0.10243724534908931 Validation Loss: 1.1173667907714844\n",
      "Epoch 9770: Training Loss: 0.10256226609150569 Validation Loss: 1.1178574562072754\n",
      "Epoch 9771: Training Loss: 0.10228203733762105 Validation Loss: 1.1173168420791626\n",
      "Epoch 9772: Training Loss: 0.10275461276372273 Validation Loss: 1.1160318851470947\n",
      "Epoch 9773: Training Loss: 0.1022378479441007 Validation Loss: 1.1163605451583862\n",
      "Epoch 9774: Training Loss: 0.10243796805540721 Validation Loss: 1.1168237924575806\n",
      "Epoch 9775: Training Loss: 0.10247763991355896 Validation Loss: 1.1168745756149292\n",
      "Epoch 9776: Training Loss: 0.10264868537584941 Validation Loss: 1.1168828010559082\n",
      "Epoch 9777: Training Loss: 0.10355951637029648 Validation Loss: 1.1166915893554688\n",
      "Epoch 9778: Training Loss: 0.10277822862068813 Validation Loss: 1.1172480583190918\n",
      "Epoch 9779: Training Loss: 0.10244481513897578 Validation Loss: 1.117817997932434\n",
      "Epoch 9780: Training Loss: 0.10200700412193935 Validation Loss: 1.117860198020935\n",
      "Epoch 9781: Training Loss: 0.10252625743548076 Validation Loss: 1.1168150901794434\n",
      "Epoch 9782: Training Loss: 0.10222721348206203 Validation Loss: 1.1169874668121338\n",
      "Epoch 9783: Training Loss: 0.10223116725683212 Validation Loss: 1.1171420812606812\n",
      "Epoch 9784: Training Loss: 0.10317867249250412 Validation Loss: 1.117649793624878\n",
      "Epoch 9785: Training Loss: 0.10267536342144012 Validation Loss: 1.1174030303955078\n",
      "Epoch 9786: Training Loss: 0.10222779214382172 Validation Loss: 1.1180678606033325\n",
      "Epoch 9787: Training Loss: 0.10183208684126537 Validation Loss: 1.1178523302078247\n",
      "Epoch 9788: Training Loss: 0.10197025040785472 Validation Loss: 1.1177306175231934\n",
      "Epoch 9789: Training Loss: 0.10214709490537643 Validation Loss: 1.1172523498535156\n",
      "Epoch 9790: Training Loss: 0.10317749033371608 Validation Loss: 1.1174317598342896\n",
      "Epoch 9791: Training Loss: 0.10205207765102386 Validation Loss: 1.1168535947799683\n",
      "Epoch 9792: Training Loss: 0.10208191970984141 Validation Loss: 1.1163874864578247\n",
      "Epoch 9793: Training Loss: 0.1020830621321996 Validation Loss: 1.1168147325515747\n",
      "Epoch 9794: Training Loss: 0.10216032713651657 Validation Loss: 1.116919755935669\n",
      "Epoch 9795: Training Loss: 0.10218434035778046 Validation Loss: 1.1181421279907227\n",
      "Epoch 9796: Training Loss: 0.10218173265457153 Validation Loss: 1.1180140972137451\n",
      "Epoch 9797: Training Loss: 0.10246732085943222 Validation Loss: 1.1182743310928345\n",
      "Epoch 9798: Training Loss: 0.10222481439510982 Validation Loss: 1.1170215606689453\n",
      "Epoch 9799: Training Loss: 0.10211998969316483 Validation Loss: 1.115497350692749\n",
      "Epoch 9800: Training Loss: 0.10193220277627309 Validation Loss: 1.1161659955978394\n",
      "Epoch 9801: Training Loss: 0.1023833230137825 Validation Loss: 1.1168566942214966\n",
      "Epoch 9802: Training Loss: 0.10200638075669606 Validation Loss: 1.118277907371521\n",
      "Epoch 9803: Training Loss: 0.10207278033097585 Validation Loss: 1.1186567544937134\n",
      "Epoch 9804: Training Loss: 0.10176939765612285 Validation Loss: 1.1174498796463013\n",
      "Epoch 9805: Training Loss: 0.10175686329603195 Validation Loss: 1.117595911026001\n",
      "Epoch 9806: Training Loss: 0.10188140720129013 Validation Loss: 1.1177926063537598\n",
      "Epoch 9807: Training Loss: 0.10194654762744904 Validation Loss: 1.118159294128418\n",
      "Epoch 9808: Training Loss: 0.10214528938134511 Validation Loss: 1.119341492652893\n",
      "Epoch 9809: Training Loss: 0.1024126335978508 Validation Loss: 1.1178675889968872\n",
      "Epoch 9810: Training Loss: 0.10181090484062831 Validation Loss: 1.1174416542053223\n",
      "Epoch 9811: Training Loss: 0.10197963068882625 Validation Loss: 1.1172350645065308\n",
      "Epoch 9812: Training Loss: 0.10201626271009445 Validation Loss: 1.1176881790161133\n",
      "Epoch 9813: Training Loss: 0.10212203363577525 Validation Loss: 1.1179511547088623\n",
      "Epoch 9814: Training Loss: 0.1018500030040741 Validation Loss: 1.1186803579330444\n",
      "Epoch 9815: Training Loss: 0.10192707926034927 Validation Loss: 1.1181282997131348\n",
      "Epoch 9816: Training Loss: 0.1019863486289978 Validation Loss: 1.1179203987121582\n",
      "Epoch 9817: Training Loss: 0.10212926069895427 Validation Loss: 1.1179451942443848\n",
      "Epoch 9818: Training Loss: 0.10186149924993515 Validation Loss: 1.1190279722213745\n",
      "Epoch 9819: Training Loss: 0.10178005198637645 Validation Loss: 1.1185755729675293\n",
      "Epoch 9820: Training Loss: 0.10210656623045604 Validation Loss: 1.1185485124588013\n",
      "Epoch 9821: Training Loss: 0.10164730002482732 Validation Loss: 1.119407296180725\n",
      "Epoch 9822: Training Loss: 0.1020993913213412 Validation Loss: 1.1183884143829346\n",
      "Epoch 9823: Training Loss: 0.10161863764127095 Validation Loss: 1.1185616254806519\n",
      "Epoch 9824: Training Loss: 0.10185052702824275 Validation Loss: 1.1192729473114014\n",
      "Epoch 9825: Training Loss: 0.10251433153947194 Validation Loss: 1.1186445951461792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9826: Training Loss: 0.10157692680756251 Validation Loss: 1.1176023483276367\n",
      "Epoch 9827: Training Loss: 0.10178012400865555 Validation Loss: 1.1179412603378296\n",
      "Epoch 9828: Training Loss: 0.1019646426041921 Validation Loss: 1.1183252334594727\n",
      "Epoch 9829: Training Loss: 0.10208881894747417 Validation Loss: 1.117923378944397\n",
      "Epoch 9830: Training Loss: 0.10179054488738377 Validation Loss: 1.1176693439483643\n",
      "Epoch 9831: Training Loss: 0.1021680807073911 Validation Loss: 1.1192570924758911\n",
      "Epoch 9832: Training Loss: 0.10165675977865855 Validation Loss: 1.1188539266586304\n",
      "Epoch 9833: Training Loss: 0.1020900160074234 Validation Loss: 1.1186240911483765\n",
      "Epoch 9834: Training Loss: 0.10214891036351521 Validation Loss: 1.118853211402893\n",
      "Epoch 9835: Training Loss: 0.10176257292429607 Validation Loss: 1.118031620979309\n",
      "Epoch 9836: Training Loss: 0.1021015743414561 Validation Loss: 1.1180733442306519\n",
      "Epoch 9837: Training Loss: 0.10154743740955989 Validation Loss: 1.1182557344436646\n",
      "Epoch 9838: Training Loss: 0.102266825735569 Validation Loss: 1.118315577507019\n",
      "Epoch 9839: Training Loss: 0.10172769178946812 Validation Loss: 1.118492603302002\n",
      "Epoch 9840: Training Loss: 0.10183786352475484 Validation Loss: 1.1185133457183838\n",
      "Epoch 9841: Training Loss: 0.10261355837186177 Validation Loss: 1.1189117431640625\n",
      "Epoch 9842: Training Loss: 0.1018329660097758 Validation Loss: 1.119226336479187\n",
      "Epoch 9843: Training Loss: 0.10201597462097804 Validation Loss: 1.1198694705963135\n",
      "Epoch 9844: Training Loss: 0.10146650423606236 Validation Loss: 1.117769718170166\n",
      "Epoch 9845: Training Loss: 0.10333682845036189 Validation Loss: 1.117438793182373\n",
      "Epoch 9846: Training Loss: 0.10197950154542923 Validation Loss: 1.1169772148132324\n",
      "Epoch 9847: Training Loss: 0.1019724930326144 Validation Loss: 1.1171157360076904\n",
      "Epoch 9848: Training Loss: 0.1018018548687299 Validation Loss: 1.118164300918579\n",
      "Epoch 9849: Training Loss: 0.10149714599053065 Validation Loss: 1.1187732219696045\n",
      "Epoch 9850: Training Loss: 0.1019276628891627 Validation Loss: 1.1196942329406738\n",
      "Epoch 9851: Training Loss: 0.10134083529313405 Validation Loss: 1.1202102899551392\n",
      "Epoch 9852: Training Loss: 0.1016487330198288 Validation Loss: 1.1201841831207275\n",
      "Epoch 9853: Training Loss: 0.10151212910811107 Validation Loss: 1.1195447444915771\n",
      "Epoch 9854: Training Loss: 0.10173322012027104 Validation Loss: 1.1201484203338623\n",
      "Epoch 9855: Training Loss: 0.10188301652669907 Validation Loss: 1.1194766759872437\n",
      "Epoch 9856: Training Loss: 0.10238366822401683 Validation Loss: 1.1186569929122925\n",
      "Epoch 9857: Training Loss: 0.10191486775875092 Validation Loss: 1.1186000108718872\n",
      "Epoch 9858: Training Loss: 0.10160384823878606 Validation Loss: 1.1177966594696045\n",
      "Epoch 9859: Training Loss: 0.10172603776057561 Validation Loss: 1.1173127889633179\n",
      "Epoch 9860: Training Loss: 0.10212246825297673 Validation Loss: 1.118977427482605\n",
      "Epoch 9861: Training Loss: 0.10246247549851735 Validation Loss: 1.1195286512374878\n",
      "Epoch 9862: Training Loss: 0.10161242137352626 Validation Loss: 1.1193463802337646\n",
      "Epoch 9863: Training Loss: 0.10163766394058864 Validation Loss: 1.1201142072677612\n",
      "Epoch 9864: Training Loss: 0.10150655110677083 Validation Loss: 1.1202130317687988\n",
      "Epoch 9865: Training Loss: 0.10172817359368007 Validation Loss: 1.1205346584320068\n",
      "Epoch 9866: Training Loss: 0.10141675174236298 Validation Loss: 1.1189243793487549\n",
      "Epoch 9867: Training Loss: 0.10180601229270299 Validation Loss: 1.1187562942504883\n",
      "Epoch 9868: Training Loss: 0.10180027037858963 Validation Loss: 1.1191076040267944\n",
      "Epoch 9869: Training Loss: 0.1011198063691457 Validation Loss: 1.1182737350463867\n",
      "Epoch 9870: Training Loss: 0.10163861761490504 Validation Loss: 1.118095874786377\n",
      "Epoch 9871: Training Loss: 0.10170314957698186 Validation Loss: 1.119818091392517\n",
      "Epoch 9872: Training Loss: 0.10160955786705017 Validation Loss: 1.1198652982711792\n",
      "Epoch 9873: Training Loss: 0.10201089332501094 Validation Loss: 1.119287133216858\n",
      "Epoch 9874: Training Loss: 0.10194768259922664 Validation Loss: 1.1186259984970093\n",
      "Epoch 9875: Training Loss: 0.10157307734092076 Validation Loss: 1.119922161102295\n",
      "Epoch 9876: Training Loss: 0.10134694973627727 Validation Loss: 1.1207327842712402\n",
      "Epoch 9877: Training Loss: 0.10140150785446167 Validation Loss: 1.1209560632705688\n",
      "Epoch 9878: Training Loss: 0.10177407910426457 Validation Loss: 1.1213700771331787\n",
      "Epoch 9879: Training Loss: 0.10154453416665395 Validation Loss: 1.1196949481964111\n",
      "Epoch 9880: Training Loss: 0.10147128005822499 Validation Loss: 1.119284749031067\n",
      "Epoch 9881: Training Loss: 0.1018614595135053 Validation Loss: 1.1195532083511353\n",
      "Epoch 9882: Training Loss: 0.10182085384925206 Validation Loss: 1.1188404560089111\n",
      "Epoch 9883: Training Loss: 0.10104674597581227 Validation Loss: 1.1189578771591187\n",
      "Epoch 9884: Training Loss: 0.10138342032829921 Validation Loss: 1.1197144985198975\n",
      "Epoch 9885: Training Loss: 0.10174106309811275 Validation Loss: 1.1193898916244507\n",
      "Epoch 9886: Training Loss: 0.10105184217294057 Validation Loss: 1.120012879371643\n",
      "Epoch 9887: Training Loss: 0.10163804392019908 Validation Loss: 1.1197603940963745\n",
      "Epoch 9888: Training Loss: 0.10198789338270824 Validation Loss: 1.119663953781128\n",
      "Epoch 9889: Training Loss: 0.10146740823984146 Validation Loss: 1.1207332611083984\n",
      "Epoch 9890: Training Loss: 0.10214874645074208 Validation Loss: 1.1203325986862183\n",
      "Epoch 9891: Training Loss: 0.10129456718762715 Validation Loss: 1.1194840669631958\n",
      "Epoch 9892: Training Loss: 0.10200292120377223 Validation Loss: 1.1200206279754639\n",
      "Epoch 9893: Training Loss: 0.10216226428747177 Validation Loss: 1.121006727218628\n",
      "Epoch 9894: Training Loss: 0.10139962534109752 Validation Loss: 1.1202757358551025\n",
      "Epoch 9895: Training Loss: 0.10152823477983475 Validation Loss: 1.1202619075775146\n",
      "Epoch 9896: Training Loss: 0.10132037103176117 Validation Loss: 1.1212016344070435\n",
      "Epoch 9897: Training Loss: 0.10130458076794942 Validation Loss: 1.1216752529144287\n",
      "Epoch 9898: Training Loss: 0.10179431488116582 Validation Loss: 1.1209766864776611\n",
      "Epoch 9899: Training Loss: 0.10124255220095317 Validation Loss: 1.1199328899383545\n",
      "Epoch 9900: Training Loss: 0.10187983512878418 Validation Loss: 1.1190590858459473\n",
      "Epoch 9901: Training Loss: 0.10142566760381062 Validation Loss: 1.1197620630264282\n",
      "Epoch 9902: Training Loss: 0.10138413806756337 Validation Loss: 1.120848298072815\n",
      "Epoch 9903: Training Loss: 0.10210417459408443 Validation Loss: 1.121602177619934\n",
      "Epoch 9904: Training Loss: 0.10151890913645427 Validation Loss: 1.1209251880645752\n",
      "Epoch 9905: Training Loss: 0.10154809306065242 Validation Loss: 1.1207516193389893\n",
      "Epoch 9906: Training Loss: 0.101225346326828 Validation Loss: 1.1207091808319092\n",
      "Epoch 9907: Training Loss: 0.10122471799453099 Validation Loss: 1.1201326847076416\n",
      "Epoch 9908: Training Loss: 0.1011512925227483 Validation Loss: 1.1196610927581787\n",
      "Epoch 9909: Training Loss: 0.10097895314296086 Validation Loss: 1.1197007894515991\n",
      "Epoch 9910: Training Loss: 0.1018336961666743 Validation Loss: 1.1204601526260376\n",
      "Epoch 9911: Training Loss: 0.10116873929897945 Validation Loss: 1.1206626892089844\n",
      "Epoch 9912: Training Loss: 0.10143763820330302 Validation Loss: 1.1203925609588623\n",
      "Epoch 9913: Training Loss: 0.10157241920630138 Validation Loss: 1.1202031373977661\n",
      "Epoch 9914: Training Loss: 0.1022914449373881 Validation Loss: 1.1212363243103027\n",
      "Epoch 9915: Training Loss: 0.10121724754571915 Validation Loss: 1.1210273504257202\n",
      "Epoch 9916: Training Loss: 0.10112819820642471 Validation Loss: 1.121044397354126\n",
      "Epoch 9917: Training Loss: 0.10106028864781062 Validation Loss: 1.121250867843628\n",
      "Epoch 9918: Training Loss: 0.10117707898219426 Validation Loss: 1.1210689544677734\n",
      "Epoch 9919: Training Loss: 0.10169040163358052 Validation Loss: 1.1202384233474731\n",
      "Epoch 9920: Training Loss: 0.1012319028377533 Validation Loss: 1.1205894947052002\n",
      "Epoch 9921: Training Loss: 0.10110598802566528 Validation Loss: 1.1208772659301758\n",
      "Epoch 9922: Training Loss: 0.10130439698696136 Validation Loss: 1.1210658550262451\n",
      "Epoch 9923: Training Loss: 0.10097220291694005 Validation Loss: 1.120116949081421\n",
      "Epoch 9924: Training Loss: 0.10152309636274974 Validation Loss: 1.1201704740524292\n",
      "Epoch 9925: Training Loss: 0.10165728876988094 Validation Loss: 1.121097207069397\n",
      "Epoch 9926: Training Loss: 0.10098263869682948 Validation Loss: 1.1206022500991821\n",
      "Epoch 9927: Training Loss: 0.10145070155461629 Validation Loss: 1.120194673538208\n",
      "Epoch 9928: Training Loss: 0.10189565519491832 Validation Loss: 1.121613621711731\n",
      "Epoch 9929: Training Loss: 0.10150233904520671 Validation Loss: 1.1208691596984863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9930: Training Loss: 0.10143472502628963 Validation Loss: 1.1203216314315796\n",
      "Epoch 9931: Training Loss: 0.10146487007538478 Validation Loss: 1.120042324066162\n",
      "Epoch 9932: Training Loss: 0.10147996495167415 Validation Loss: 1.1202319860458374\n",
      "Epoch 9933: Training Loss: 0.10159668574730556 Validation Loss: 1.122043490409851\n",
      "Epoch 9934: Training Loss: 0.10109289735555649 Validation Loss: 1.1228841543197632\n",
      "Epoch 9935: Training Loss: 0.1019148975610733 Validation Loss: 1.122135877609253\n",
      "Epoch 9936: Training Loss: 0.10140995184580485 Validation Loss: 1.1217546463012695\n",
      "Epoch 9937: Training Loss: 0.10094068944454193 Validation Loss: 1.121462106704712\n",
      "Epoch 9938: Training Loss: 0.10195059329271317 Validation Loss: 1.1216580867767334\n",
      "Epoch 9939: Training Loss: 0.10096620519955952 Validation Loss: 1.1214215755462646\n",
      "Epoch 9940: Training Loss: 0.10114504645268123 Validation Loss: 1.1209321022033691\n",
      "Epoch 9941: Training Loss: 0.1013074591755867 Validation Loss: 1.1213184595108032\n",
      "Epoch 9942: Training Loss: 0.10082129885752995 Validation Loss: 1.1218006610870361\n",
      "Epoch 9943: Training Loss: 0.10124931236108144 Validation Loss: 1.1232037544250488\n",
      "Epoch 9944: Training Loss: 0.1010799656311671 Validation Loss: 1.1225742101669312\n",
      "Epoch 9945: Training Loss: 0.10254852225383122 Validation Loss: 1.1217455863952637\n",
      "Epoch 9946: Training Loss: 0.10127418984969457 Validation Loss: 1.121928095817566\n",
      "Epoch 9947: Training Loss: 0.10152553518613179 Validation Loss: 1.1219656467437744\n",
      "Epoch 9948: Training Loss: 0.1015758216381073 Validation Loss: 1.122389793395996\n",
      "Epoch 9949: Training Loss: 0.10103076199690501 Validation Loss: 1.1212319135665894\n",
      "Epoch 9950: Training Loss: 0.10118207335472107 Validation Loss: 1.1213148832321167\n",
      "Epoch 9951: Training Loss: 0.10148203869660695 Validation Loss: 1.1212996244430542\n",
      "Epoch 9952: Training Loss: 0.1015191450715065 Validation Loss: 1.12093985080719\n",
      "Epoch 9953: Training Loss: 0.10099728405475616 Validation Loss: 1.1207923889160156\n",
      "Epoch 9954: Training Loss: 0.10213916003704071 Validation Loss: 1.1217325925827026\n",
      "Epoch 9955: Training Loss: 0.10085853685935338 Validation Loss: 1.1213287115097046\n",
      "Epoch 9956: Training Loss: 0.10088999321063359 Validation Loss: 1.121706247329712\n",
      "Epoch 9957: Training Loss: 0.10113893946011861 Validation Loss: 1.12105393409729\n",
      "Epoch 9958: Training Loss: 0.10062847286462784 Validation Loss: 1.12169349193573\n",
      "Epoch 9959: Training Loss: 0.10103851556777954 Validation Loss: 1.1221680641174316\n",
      "Epoch 9960: Training Loss: 0.10103399803241093 Validation Loss: 1.1215227842330933\n",
      "Epoch 9961: Training Loss: 0.1010386695464452 Validation Loss: 1.1226145029067993\n",
      "Epoch 9962: Training Loss: 0.10118954877058665 Validation Loss: 1.1224021911621094\n",
      "Epoch 9963: Training Loss: 0.10224643846352895 Validation Loss: 1.1230151653289795\n",
      "Epoch 9964: Training Loss: 0.10107136269410451 Validation Loss: 1.1222110986709595\n",
      "Epoch 9965: Training Loss: 0.1011436755458514 Validation Loss: 1.1203951835632324\n",
      "Epoch 9966: Training Loss: 0.10183462500572205 Validation Loss: 1.1207351684570312\n",
      "Epoch 9967: Training Loss: 0.10143099228541057 Validation Loss: 1.1221740245819092\n",
      "Epoch 9968: Training Loss: 0.1019454871614774 Validation Loss: 1.123552918434143\n",
      "Epoch 9969: Training Loss: 0.10091789811849594 Validation Loss: 1.1231788396835327\n",
      "Epoch 9970: Training Loss: 0.10082969069480896 Validation Loss: 1.1227585077285767\n",
      "Epoch 9971: Training Loss: 0.10087033361196518 Validation Loss: 1.1218817234039307\n",
      "Epoch 9972: Training Loss: 0.1008619690934817 Validation Loss: 1.1207184791564941\n",
      "Epoch 9973: Training Loss: 0.10079589237769444 Validation Loss: 1.1213151216506958\n",
      "Epoch 9974: Training Loss: 0.10125358651081721 Validation Loss: 1.1218962669372559\n",
      "Epoch 9975: Training Loss: 0.10151736934979756 Validation Loss: 1.1228935718536377\n",
      "Epoch 9976: Training Loss: 0.10100316007932027 Validation Loss: 1.1233701705932617\n",
      "Epoch 9977: Training Loss: 0.10078881432612737 Validation Loss: 1.1227314472198486\n",
      "Epoch 9978: Training Loss: 0.1011199230949084 Validation Loss: 1.121581792831421\n",
      "Epoch 9979: Training Loss: 0.10073260714610417 Validation Loss: 1.1212332248687744\n",
      "Epoch 9980: Training Loss: 0.10146470616261165 Validation Loss: 1.1200954914093018\n",
      "Epoch 9981: Training Loss: 0.10067842900753021 Validation Loss: 1.1206943988800049\n",
      "Epoch 9982: Training Loss: 0.10126671691735585 Validation Loss: 1.121829867362976\n",
      "Epoch 9983: Training Loss: 0.10098296403884888 Validation Loss: 1.123352289199829\n",
      "Epoch 9984: Training Loss: 0.10062253475189209 Validation Loss: 1.1238017082214355\n",
      "Epoch 9985: Training Loss: 0.10177508741617203 Validation Loss: 1.1233587265014648\n",
      "Epoch 9986: Training Loss: 0.10068333645661671 Validation Loss: 1.1229838132858276\n",
      "Epoch 9987: Training Loss: 0.10138181348641713 Validation Loss: 1.1235252618789673\n",
      "Epoch 9988: Training Loss: 0.10081290205319722 Validation Loss: 1.123625636100769\n",
      "Epoch 9989: Training Loss: 0.10034517447153728 Validation Loss: 1.1227750778198242\n",
      "Epoch 9990: Training Loss: 0.10081725070873897 Validation Loss: 1.1224406957626343\n",
      "Epoch 9991: Training Loss: 0.10071881363789241 Validation Loss: 1.1222891807556152\n",
      "Epoch 9992: Training Loss: 0.100581593811512 Validation Loss: 1.1229077577590942\n",
      "Epoch 9993: Training Loss: 0.10041040927171707 Validation Loss: 1.1230990886688232\n",
      "Epoch 9994: Training Loss: 0.10066676884889603 Validation Loss: 1.123206615447998\n",
      "Epoch 9995: Training Loss: 0.10061560074488322 Validation Loss: 1.123685359954834\n",
      "Epoch 9996: Training Loss: 0.10100190589825313 Validation Loss: 1.1220283508300781\n",
      "Epoch 9997: Training Loss: 0.10074999680121739 Validation Loss: 1.1217894554138184\n",
      "Epoch 9998: Training Loss: 0.10070752104123433 Validation Loss: 1.1219897270202637\n",
      "Epoch 9999: Training Loss: 0.10107149928808212 Validation Loss: 1.1215909719467163\n",
      "Epoch 10000: Training Loss: 0.10075883318980534 Validation Loss: 1.122118592262268\n",
      "Epoch 10001: Training Loss: 0.10145064940055211 Validation Loss: 1.1217511892318726\n",
      "Epoch 10002: Training Loss: 0.10083290934562683 Validation Loss: 1.1230087280273438\n",
      "Epoch 10003: Training Loss: 0.10123644024133682 Validation Loss: 1.1241811513900757\n",
      "Epoch 10004: Training Loss: 0.10165366282065709 Validation Loss: 1.1226764917373657\n",
      "Epoch 10005: Training Loss: 0.1008409708738327 Validation Loss: 1.123687744140625\n",
      "Epoch 10006: Training Loss: 0.10060481975475948 Validation Loss: 1.1237154006958008\n",
      "Epoch 10007: Training Loss: 0.10064369191726048 Validation Loss: 1.1225312948226929\n",
      "Epoch 10008: Training Loss: 0.10094949851433437 Validation Loss: 1.1232846975326538\n",
      "Epoch 10009: Training Loss: 0.10053489357233047 Validation Loss: 1.123734474182129\n",
      "Epoch 10010: Training Loss: 0.10072296857833862 Validation Loss: 1.122509479522705\n",
      "Epoch 10011: Training Loss: 0.10020094861586888 Validation Loss: 1.1228660345077515\n",
      "Epoch 10012: Training Loss: 0.10054745773474376 Validation Loss: 1.1235991716384888\n",
      "Epoch 10013: Training Loss: 0.10119420289993286 Validation Loss: 1.1241666078567505\n",
      "Epoch 10014: Training Loss: 0.10070940603812535 Validation Loss: 1.123355746269226\n",
      "Epoch 10015: Training Loss: 0.10064581781625748 Validation Loss: 1.1223503351211548\n",
      "Epoch 10016: Training Loss: 0.10080079237620036 Validation Loss: 1.122368574142456\n",
      "Epoch 10017: Training Loss: 0.10071418186028798 Validation Loss: 1.1224205493927002\n",
      "Epoch 10018: Training Loss: 0.1008124053478241 Validation Loss: 1.1230378150939941\n",
      "Epoch 10019: Training Loss: 0.10072578738133113 Validation Loss: 1.1234862804412842\n",
      "Epoch 10020: Training Loss: 0.10014480352401733 Validation Loss: 1.123555064201355\n",
      "Epoch 10021: Training Loss: 0.10057824353377025 Validation Loss: 1.1232131719589233\n",
      "Epoch 10022: Training Loss: 0.10062759617964427 Validation Loss: 1.1231215000152588\n",
      "Epoch 10023: Training Loss: 0.10080716510613759 Validation Loss: 1.1240164041519165\n",
      "Epoch 10024: Training Loss: 0.10079897940158844 Validation Loss: 1.125203013420105\n",
      "Epoch 10025: Training Loss: 0.10088467846314113 Validation Loss: 1.1242084503173828\n",
      "Epoch 10026: Training Loss: 0.10082164903481801 Validation Loss: 1.123856544494629\n",
      "Epoch 10027: Training Loss: 0.10051131248474121 Validation Loss: 1.1236974000930786\n",
      "Epoch 10028: Training Loss: 0.10124186674753825 Validation Loss: 1.1234136819839478\n",
      "Epoch 10029: Training Loss: 0.10061077773571014 Validation Loss: 1.123733639717102\n",
      "Epoch 10030: Training Loss: 0.10062789171934128 Validation Loss: 1.1237103939056396\n",
      "Epoch 10031: Training Loss: 0.10077577084302902 Validation Loss: 1.1239908933639526\n",
      "Epoch 10032: Training Loss: 0.10061594595511754 Validation Loss: 1.1234869956970215\n",
      "Epoch 10033: Training Loss: 0.10090913623571396 Validation Loss: 1.1231287717819214\n",
      "Epoch 10034: Training Loss: 0.10048053165276845 Validation Loss: 1.1233125925064087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10035: Training Loss: 0.10121134171883266 Validation Loss: 1.1236600875854492\n",
      "Epoch 10036: Training Loss: 0.09993734707434972 Validation Loss: 1.1240824460983276\n",
      "Epoch 10037: Training Loss: 0.10068239271640778 Validation Loss: 1.1229497194290161\n",
      "Epoch 10038: Training Loss: 0.100523146490256 Validation Loss: 1.1238569021224976\n",
      "Epoch 10039: Training Loss: 0.10045354813337326 Validation Loss: 1.124522089958191\n",
      "Epoch 10040: Training Loss: 0.10161539415518443 Validation Loss: 1.1256386041641235\n",
      "Epoch 10041: Training Loss: 0.10049464801947276 Validation Loss: 1.1247236728668213\n",
      "Epoch 10042: Training Loss: 0.1005194087823232 Validation Loss: 1.1255342960357666\n",
      "Epoch 10043: Training Loss: 0.10062567392985027 Validation Loss: 1.1245808601379395\n",
      "Epoch 10044: Training Loss: 0.10079167286554973 Validation Loss: 1.1241956949234009\n",
      "Epoch 10045: Training Loss: 0.10008367151021957 Validation Loss: 1.1235765218734741\n",
      "Epoch 10046: Training Loss: 0.10070056468248367 Validation Loss: 1.1233800649642944\n",
      "Epoch 10047: Training Loss: 0.10039496173461278 Validation Loss: 1.1237436532974243\n",
      "Epoch 10048: Training Loss: 0.10055635124444962 Validation Loss: 1.1238919496536255\n",
      "Epoch 10049: Training Loss: 0.10007929801940918 Validation Loss: 1.1232054233551025\n",
      "Epoch 10050: Training Loss: 0.10096120337645213 Validation Loss: 1.123488426208496\n",
      "Epoch 10051: Training Loss: 0.10084535926580429 Validation Loss: 1.1238517761230469\n",
      "Epoch 10052: Training Loss: 0.10044057418902715 Validation Loss: 1.1242125034332275\n",
      "Epoch 10053: Training Loss: 0.10063692679007848 Validation Loss: 1.1250896453857422\n",
      "Epoch 10054: Training Loss: 0.10118570427099864 Validation Loss: 1.1261537075042725\n",
      "Epoch 10055: Training Loss: 0.10040779411792755 Validation Loss: 1.1254700422286987\n",
      "Epoch 10056: Training Loss: 0.10079892228047053 Validation Loss: 1.124534249305725\n",
      "Epoch 10057: Training Loss: 0.10019066681464513 Validation Loss: 1.1241575479507446\n",
      "Epoch 10058: Training Loss: 0.10031172384818395 Validation Loss: 1.12421452999115\n",
      "Epoch 10059: Training Loss: 0.10001037766536076 Validation Loss: 1.1245555877685547\n",
      "Epoch 10060: Training Loss: 0.10009356091419856 Validation Loss: 1.1254926919937134\n",
      "Epoch 10061: Training Loss: 0.10060600191354752 Validation Loss: 1.1266010999679565\n",
      "Epoch 10062: Training Loss: 0.10028643906116486 Validation Loss: 1.1266099214553833\n",
      "Epoch 10063: Training Loss: 0.10017192612091701 Validation Loss: 1.1251578330993652\n",
      "Epoch 10064: Training Loss: 0.10039886583884557 Validation Loss: 1.1240721940994263\n",
      "Epoch 10065: Training Loss: 0.10005221764246623 Validation Loss: 1.1231058835983276\n",
      "Epoch 10066: Training Loss: 0.10033406068881352 Validation Loss: 1.1228197813034058\n",
      "Epoch 10067: Training Loss: 0.1007011408607165 Validation Loss: 1.1245174407958984\n",
      "Epoch 10068: Training Loss: 0.10021261622508366 Validation Loss: 1.125669002532959\n",
      "Epoch 10069: Training Loss: 0.10004036873579025 Validation Loss: 1.1257901191711426\n",
      "Epoch 10070: Training Loss: 0.10005768885215123 Validation Loss: 1.125209093093872\n",
      "Epoch 10071: Training Loss: 0.10034003108739853 Validation Loss: 1.1249439716339111\n",
      "Epoch 10072: Training Loss: 0.1003153348962466 Validation Loss: 1.1251184940338135\n",
      "Epoch 10073: Training Loss: 0.10020069032907486 Validation Loss: 1.123802661895752\n",
      "Epoch 10074: Training Loss: 0.10029391447703044 Validation Loss: 1.123990774154663\n",
      "Epoch 10075: Training Loss: 0.10004201779762904 Validation Loss: 1.124005913734436\n",
      "Epoch 10076: Training Loss: 0.10066220164299011 Validation Loss: 1.1245791912078857\n",
      "Epoch 10077: Training Loss: 0.10035647948582967 Validation Loss: 1.1253715753555298\n",
      "Epoch 10078: Training Loss: 0.10020005454619725 Validation Loss: 1.1246650218963623\n",
      "Epoch 10079: Training Loss: 0.10016798973083496 Validation Loss: 1.1248836517333984\n",
      "Epoch 10080: Training Loss: 0.1004768709341685 Validation Loss: 1.1249592304229736\n",
      "Epoch 10081: Training Loss: 0.10017433762550354 Validation Loss: 1.1242271661758423\n",
      "Epoch 10082: Training Loss: 0.10021884491046269 Validation Loss: 1.1238996982574463\n",
      "Epoch 10083: Training Loss: 0.10089048246542613 Validation Loss: 1.1247261762619019\n",
      "Epoch 10084: Training Loss: 0.10022710512081782 Validation Loss: 1.1249037981033325\n",
      "Epoch 10085: Training Loss: 0.10010497272014618 Validation Loss: 1.1254442930221558\n",
      "Epoch 10086: Training Loss: 0.10042899350325267 Validation Loss: 1.1254703998565674\n",
      "Epoch 10087: Training Loss: 0.10037252306938171 Validation Loss: 1.1266050338745117\n",
      "Epoch 10088: Training Loss: 0.10049155602852504 Validation Loss: 1.1262649297714233\n",
      "Epoch 10089: Training Loss: 0.10024619847536087 Validation Loss: 1.1251345872879028\n",
      "Epoch 10090: Training Loss: 0.10094329218069713 Validation Loss: 1.1247979402542114\n",
      "Epoch 10091: Training Loss: 0.10052134344975154 Validation Loss: 1.1249394416809082\n",
      "Epoch 10092: Training Loss: 0.1008297602335612 Validation Loss: 1.1261214017868042\n",
      "Epoch 10093: Training Loss: 0.10001837958892186 Validation Loss: 1.1262974739074707\n",
      "Epoch 10094: Training Loss: 0.10080510626236598 Validation Loss: 1.1261317729949951\n",
      "Epoch 10095: Training Loss: 0.09983668476343155 Validation Loss: 1.124507188796997\n",
      "Epoch 10096: Training Loss: 0.0997077946861585 Validation Loss: 1.1229101419448853\n",
      "Epoch 10097: Training Loss: 0.10037778566281001 Validation Loss: 1.1227071285247803\n",
      "Epoch 10098: Training Loss: 0.10095245639483134 Validation Loss: 1.1249326467514038\n",
      "Epoch 10099: Training Loss: 0.10017456859350204 Validation Loss: 1.1266390085220337\n",
      "Epoch 10100: Training Loss: 0.10026635229587555 Validation Loss: 1.1273174285888672\n",
      "Epoch 10101: Training Loss: 0.10012409090995789 Validation Loss: 1.1268162727355957\n",
      "Epoch 10102: Training Loss: 0.10030618061621983 Validation Loss: 1.1254469156265259\n",
      "Epoch 10103: Training Loss: 0.1001564587155978 Validation Loss: 1.1253561973571777\n",
      "Epoch 10104: Training Loss: 0.09974778691927592 Validation Loss: 1.125354290008545\n",
      "Epoch 10105: Training Loss: 0.10000475496053696 Validation Loss: 1.1255854368209839\n",
      "Epoch 10106: Training Loss: 0.10005963097016017 Validation Loss: 1.1266342401504517\n",
      "Epoch 10107: Training Loss: 0.10045590500036876 Validation Loss: 1.1278791427612305\n",
      "Epoch 10108: Training Loss: 0.10076701641082764 Validation Loss: 1.1273863315582275\n",
      "Epoch 10109: Training Loss: 0.1000207116206487 Validation Loss: 1.1257766485214233\n",
      "Epoch 10110: Training Loss: 0.10035965591669083 Validation Loss: 1.1245038509368896\n",
      "Epoch 10111: Training Loss: 0.10127300024032593 Validation Loss: 1.1254937648773193\n",
      "Epoch 10112: Training Loss: 0.1001142015059789 Validation Loss: 1.126617431640625\n",
      "Epoch 10113: Training Loss: 0.0997848908106486 Validation Loss: 1.1267930269241333\n",
      "Epoch 10114: Training Loss: 0.10052915910879771 Validation Loss: 1.12557852268219\n",
      "Epoch 10115: Training Loss: 0.09964371720949809 Validation Loss: 1.1255426406860352\n",
      "Epoch 10116: Training Loss: 0.10055069625377655 Validation Loss: 1.1249428987503052\n",
      "Epoch 10117: Training Loss: 0.09973012159268062 Validation Loss: 1.125030755996704\n",
      "Epoch 10118: Training Loss: 0.09988457709550858 Validation Loss: 1.1261420249938965\n",
      "Epoch 10119: Training Loss: 0.09992680698633194 Validation Loss: 1.1275932788848877\n",
      "Epoch 10120: Training Loss: 0.09967784335215886 Validation Loss: 1.1275372505187988\n",
      "Epoch 10121: Training Loss: 0.09941278646389644 Validation Loss: 1.1273441314697266\n",
      "Epoch 10122: Training Loss: 0.10005005697409312 Validation Loss: 1.126432180404663\n",
      "Epoch 10123: Training Loss: 0.09974822402000427 Validation Loss: 1.1255203485488892\n",
      "Epoch 10124: Training Loss: 0.10025542229413986 Validation Loss: 1.1255812644958496\n",
      "Epoch 10125: Training Loss: 0.09966342647870381 Validation Loss: 1.1261515617370605\n",
      "Epoch 10126: Training Loss: 0.09975316375494003 Validation Loss: 1.1265461444854736\n",
      "Epoch 10127: Training Loss: 0.09996316333611806 Validation Loss: 1.126674771308899\n",
      "Epoch 10128: Training Loss: 0.10004158814748128 Validation Loss: 1.1272825002670288\n",
      "Epoch 10129: Training Loss: 0.09985330204168956 Validation Loss: 1.1272624731063843\n",
      "Epoch 10130: Training Loss: 0.0997025469938914 Validation Loss: 1.1265056133270264\n",
      "Epoch 10131: Training Loss: 0.09992540379365285 Validation Loss: 1.125770926475525\n",
      "Epoch 10132: Training Loss: 0.10031184554100037 Validation Loss: 1.1260157823562622\n",
      "Epoch 10133: Training Loss: 0.09993645052115123 Validation Loss: 1.1262403726577759\n",
      "Epoch 10134: Training Loss: 0.09999097386995952 Validation Loss: 1.1263396739959717\n",
      "Epoch 10135: Training Loss: 0.09923670689264934 Validation Loss: 1.1272735595703125\n",
      "Epoch 10136: Training Loss: 0.09988832970460255 Validation Loss: 1.1279160976409912\n",
      "Epoch 10137: Training Loss: 0.09987333168586095 Validation Loss: 1.127078652381897\n",
      "Epoch 10138: Training Loss: 0.09991253664096196 Validation Loss: 1.1264256238937378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10139: Training Loss: 0.10013704250256221 Validation Loss: 1.1267050504684448\n",
      "Epoch 10140: Training Loss: 0.09983461101849873 Validation Loss: 1.1261390447616577\n",
      "Epoch 10141: Training Loss: 0.09964747726917267 Validation Loss: 1.1269245147705078\n",
      "Epoch 10142: Training Loss: 0.09998111675182979 Validation Loss: 1.1272622346878052\n",
      "Epoch 10143: Training Loss: 0.09965783109267552 Validation Loss: 1.1274415254592896\n",
      "Epoch 10144: Training Loss: 0.10068957010904948 Validation Loss: 1.1279383897781372\n",
      "Epoch 10145: Training Loss: 0.10042307525873184 Validation Loss: 1.1278321743011475\n",
      "Epoch 10146: Training Loss: 0.10009256998697917 Validation Loss: 1.126616358757019\n",
      "Epoch 10147: Training Loss: 0.09995251893997192 Validation Loss: 1.1266738176345825\n",
      "Epoch 10148: Training Loss: 0.0996525560816129 Validation Loss: 1.1270438432693481\n",
      "Epoch 10149: Training Loss: 0.09952540695667267 Validation Loss: 1.1266673803329468\n",
      "Epoch 10150: Training Loss: 0.09976605822642644 Validation Loss: 1.1262116432189941\n",
      "Epoch 10151: Training Loss: 0.10058257232109706 Validation Loss: 1.1267930269241333\n",
      "Epoch 10152: Training Loss: 0.09976003567377727 Validation Loss: 1.1272329092025757\n",
      "Epoch 10153: Training Loss: 0.09986939032872517 Validation Loss: 1.12741219997406\n",
      "Epoch 10154: Training Loss: 0.10020434608062108 Validation Loss: 1.1280988454818726\n",
      "Epoch 10155: Training Loss: 0.09964937716722488 Validation Loss: 1.127169132232666\n",
      "Epoch 10156: Training Loss: 0.09956684460242589 Validation Loss: 1.127690076828003\n",
      "Epoch 10157: Training Loss: 0.0996367484331131 Validation Loss: 1.1278560161590576\n",
      "Epoch 10158: Training Loss: 0.10004340360562007 Validation Loss: 1.1274772882461548\n",
      "Epoch 10159: Training Loss: 0.09986405322949092 Validation Loss: 1.1271755695343018\n",
      "Epoch 10160: Training Loss: 0.09940425058205922 Validation Loss: 1.1273607015609741\n",
      "Epoch 10161: Training Loss: 0.09962369998296101 Validation Loss: 1.127753734588623\n",
      "Epoch 10162: Training Loss: 0.09992198397715886 Validation Loss: 1.1274651288986206\n",
      "Epoch 10163: Training Loss: 0.09975965321063995 Validation Loss: 1.1270132064819336\n",
      "Epoch 10164: Training Loss: 0.09968110422293346 Validation Loss: 1.1275701522827148\n",
      "Epoch 10165: Training Loss: 0.09990155200163524 Validation Loss: 1.127618432044983\n",
      "Epoch 10166: Training Loss: 0.09982382257779439 Validation Loss: 1.1272410154342651\n",
      "Epoch 10167: Training Loss: 0.09909635037183762 Validation Loss: 1.1276376247406006\n",
      "Epoch 10168: Training Loss: 0.09947540114323299 Validation Loss: 1.1275733709335327\n",
      "Epoch 10169: Training Loss: 0.09943343202273051 Validation Loss: 1.1276737451553345\n",
      "Epoch 10170: Training Loss: 0.09951340158780415 Validation Loss: 1.1274560689926147\n",
      "Epoch 10171: Training Loss: 0.09981999049584071 Validation Loss: 1.127322793006897\n",
      "Epoch 10172: Training Loss: 0.09984750548998515 Validation Loss: 1.1273151636123657\n",
      "Epoch 10173: Training Loss: 0.0996870423356692 Validation Loss: 1.1278560161590576\n",
      "Epoch 10174: Training Loss: 0.09953440229098003 Validation Loss: 1.1286414861679077\n",
      "Epoch 10175: Training Loss: 0.09985397259394328 Validation Loss: 1.1290016174316406\n",
      "Epoch 10176: Training Loss: 0.10011130074659984 Validation Loss: 1.128463625907898\n",
      "Epoch 10177: Training Loss: 0.09989179174105327 Validation Loss: 1.1276863813400269\n",
      "Epoch 10178: Training Loss: 0.0996940756837527 Validation Loss: 1.1283009052276611\n",
      "Epoch 10179: Training Loss: 0.09944652020931244 Validation Loss: 1.1283499002456665\n",
      "Epoch 10180: Training Loss: 0.09937156985203426 Validation Loss: 1.1278997659683228\n",
      "Epoch 10181: Training Loss: 0.10268939286470413 Validation Loss: 1.127346396446228\n",
      "Epoch 10182: Training Loss: 0.09957982351382573 Validation Loss: 1.127267837524414\n",
      "Epoch 10183: Training Loss: 0.09984018156925838 Validation Loss: 1.1274726390838623\n",
      "Epoch 10184: Training Loss: 0.09975294023752213 Validation Loss: 1.1278268098831177\n",
      "Epoch 10185: Training Loss: 0.09985381364822388 Validation Loss: 1.1281664371490479\n",
      "Epoch 10186: Training Loss: 0.09946286926666896 Validation Loss: 1.1289528608322144\n",
      "Epoch 10187: Training Loss: 0.10000029702981313 Validation Loss: 1.1288543939590454\n",
      "Epoch 10188: Training Loss: 0.09953287740548451 Validation Loss: 1.128247857093811\n",
      "Epoch 10189: Training Loss: 0.09904197851816814 Validation Loss: 1.1272633075714111\n",
      "Epoch 10190: Training Loss: 0.09938385834296544 Validation Loss: 1.1271833181381226\n",
      "Epoch 10191: Training Loss: 0.09935811907052994 Validation Loss: 1.1282888650894165\n",
      "Epoch 10192: Training Loss: 0.09952038526535034 Validation Loss: 1.1284233331680298\n",
      "Epoch 10193: Training Loss: 0.09946674853563309 Validation Loss: 1.1283512115478516\n",
      "Epoch 10194: Training Loss: 0.09950673580169678 Validation Loss: 1.1274737119674683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b17409d5a916>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytra\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-2ffcc1ef844f>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(X, Y, Xt, Yt, net, optimizer, error, n_epochs, n_batches, device, PATH, min_val_loss)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mval_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-2ffcc1ef844f>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc11\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc12\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rober\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rober\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rober\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1352\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1353\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(Xtra, ytra, Xtes, ytes, net, optimizer, criterion, n_epochs, n_batches, device, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'Data\\\\model_checkpoint.pt'\n",
    "device = torch.device('cpu')\n",
    "net = Net1()\n",
    "net.load_state_dict(torch.load(PATH, map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9386579990386963\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    inputs = torch.FloatTensor(Xtes)\n",
    "    labels = torch.tensor(ytes, dtype=torch.long)\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = net.forward(inputs)\n",
    "    loss = error(outputs, labels) \n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {}\n",
    "for v,p in enumerate(net.parameters()):\n",
    "    param_dict[v] = p.data.numpy()\n",
    "    \n",
    "W0 = param_dict[0].T\n",
    "b0 = param_dict[1]\n",
    "W1 = param_dict[2].T\n",
    "b1 = param_dict[3]\n",
    "\n",
    "\n",
    "h0 = np.matmul(Xtes, W0) + b0\n",
    "h1 = np.tanh(h0)\n",
    "h2 = np.matmul(h1, W1) + b1\n",
    "h3 = np.exp(h2)\n",
    "o = h3/np.sum(h3,axis=1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 13)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data\\\\param_dict.npy', param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(prob, topics):    \n",
    "    return topics[np.argmax(prob)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = []\n",
    "topics = ['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
    "for c in o:\n",
    "    pred_labels.append(y_hat(c, topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = data.Pregunta[btes]\n",
    "labels = data.Tema[btes]\n",
    "ambig = data.ambiguedad[btes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtes = np.column_stack((preguntas, labels, pred_labels, ambig, Xtes, o))\n",
    "#Xtes = np.column_stack((preguntas, labels, pred_labels, Xtes, o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(Xtes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.load('Data\\\\vocab.npy', allow_pickle=True)\n",
    "colnames = np.concatenate((['pregunta', 'label', 'pred_label', 'ambig'], vocab, ['n_token', 'perc_greet', 'capacitacion', 'polarity'], ['Jubilacion Patronal', 'Consultoria', \\\n",
    "                                                                                                     'Renuncia/Despido/Desahucio', 'IESS', 'Greeting', \\\n",
    "                                                                                                         'Contacto', 'No Topic', 'Queja', 'Otros servicios', \\\n",
    "                                                                                                 'Charlas/Capacitaciones', 'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']))\n",
    "data2.columns = colnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregunta</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>ambig</th>\n",
       "      <th>buen</th>\n",
       "      <th>dia</th>\n",
       "      <th>mi</th>\n",
       "      <th>nombr</th>\n",
       "      <th>llam</th>\n",
       "      <th>empres</th>\n",
       "      <th>...</th>\n",
       "      <th>IESS</th>\n",
       "      <th>Greeting</th>\n",
       "      <th>Contacto</th>\n",
       "      <th>No Topic</th>\n",
       "      <th>Queja</th>\n",
       "      <th>Otros servicios</th>\n",
       "      <th>Charlas/Capacitaciones</th>\n",
       "      <th>Hi Five</th>\n",
       "      <th>job seeker</th>\n",
       "      <th>Facturacion/Retencion/Cobros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hola !</td>\n",
       "      <td>Greeting</td>\n",
       "      <td>Greeting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00171698</td>\n",
       "      <td>0.964344</td>\n",
       "      <td>0.00396915</td>\n",
       "      <td>0.00604601</td>\n",
       "      <td>0.000971784</td>\n",
       "      <td>0.00451093</td>\n",
       "      <td>0.00112638</td>\n",
       "      <td>0.00662948</td>\n",
       "      <td>0.000665403</td>\n",
       "      <td>0.00167011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uno de mis lugares favoritos de mi hogar es mi...</td>\n",
       "      <td>No Topic</td>\n",
       "      <td>No Topic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353248</td>\n",
       "      <td>1.12381e-05</td>\n",
       "      <td>0.00200821</td>\n",
       "      <td>0.463934</td>\n",
       "      <td>0.0258064</td>\n",
       "      <td>0.00481347</td>\n",
       "      <td>0.0126399</td>\n",
       "      <td>2.5435e-05</td>\n",
       "      <td>0.000254181</td>\n",
       "      <td>0.00900774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El 13 de noviembre de 2015, el Consejo Directi...</td>\n",
       "      <td>IESS</td>\n",
       "      <td>IESS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978761</td>\n",
       "      <td>0.000617609</td>\n",
       "      <td>0.000746195</td>\n",
       "      <td>0.00221438</td>\n",
       "      <td>0.00123249</td>\n",
       "      <td>0.000384678</td>\n",
       "      <td>0.000200026</td>\n",
       "      <td>2.09142e-05</td>\n",
       "      <td>3.9227e-05</td>\n",
       "      <td>0.000572202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  591 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            pregunta     label pred_label  \\\n",
       "0                                             Hola !  Greeting   Greeting   \n",
       "1  Uno de mis lugares favoritos de mi hogar es mi...  No Topic   No Topic   \n",
       "2  El 13 de noviembre de 2015, el Consejo Directi...      IESS       IESS   \n",
       "\n",
       "  ambig buen dia mi nombr llam empres  ...        IESS     Greeting  \\\n",
       "0   NaN    0   0  0     0    0      0  ...  0.00171698     0.964344   \n",
       "1   NaN    0   0  0     0    0      0  ...    0.353248  1.12381e-05   \n",
       "2   NaN    0   0  0     0    0      0  ...    0.978761  0.000617609   \n",
       "\n",
       "      Contacto    No Topic        Queja Otros servicios  \\\n",
       "0   0.00396915  0.00604601  0.000971784      0.00451093   \n",
       "1   0.00200821    0.463934    0.0258064      0.00481347   \n",
       "2  0.000746195  0.00221438   0.00123249     0.000384678   \n",
       "\n",
       "  Charlas/Capacitaciones      Hi Five   job seeker  \\\n",
       "0             0.00112638   0.00662948  0.000665403   \n",
       "1              0.0126399   2.5435e-05  0.000254181   \n",
       "2            0.000200026  2.09142e-05   3.9227e-05   \n",
       "\n",
       "  Facturacion/Retencion/Cobros  \n",
       "0                   0.00167011  \n",
       "1                   0.00900774  \n",
       "2                  0.000572202  \n",
       "\n",
       "[3 rows x 591 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_csv('Data\\\\Xtes.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
