{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from pattern.es import parsetree\n",
    "import unidecode\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "#from googletrans import Translator\n",
    "from translate import Translator\n",
    "from classifier import SentimentClassifier as sf\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_funcionales = nltk.corpus.stopwords.words(\"spanish\")    \n",
    "stemmer = SnowballStemmer('spanish')\n",
    "#translator = Translator()\n",
    "#translator = Translator(\"spanish\", \"english\")\n",
    "clf = sf() \n",
    "\n",
    "def trim_sent(sentence):    \n",
    "    return ' '.join(sentence.split())\n",
    "\n",
    "def prepare_text(text): \n",
    "    try:\n",
    "        text = trim_sent(text).lower()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print('Exception en prepare_text: {0}'.format(e))\n",
    "        return None       \n",
    "\n",
    "\n",
    "def hasNumbers(string):\n",
    "    return bool(re.search(r'\\d', string))\n",
    "\n",
    "\n",
    "def hasBC(string):\n",
    "    i = string.find('/')\n",
    "    return bool(i != -1)\n",
    "\n",
    "\n",
    "def other_check(token):    \n",
    "    b1 = not hasNumbers(token)\n",
    "    b2 = not hasBC(token)\n",
    "    return (b1 and b2)\n",
    "\n",
    "def remove_accent(word):\n",
    "    return unidecode.unidecode(word)\n",
    "\n",
    "def stem_lemma(word):     \n",
    "    word = parsetree(word, lemmata=True)[0].lemmata[0]\n",
    "    word = stemmer.stem(word) \n",
    "    return word\n",
    "\n",
    "\n",
    "def token_and_clean(texto): \n",
    "    tokens = nltk.word_tokenize(texto, \"spanish\")\n",
    "    token_list = []\n",
    "    for token in tokens:        \n",
    "        if token not in palabras_funcionales:\n",
    "            token = stem_lemma(token)\n",
    "            token = remove_accent(token)\n",
    "            if len(token) >= 2 and other_check(token):\n",
    "                token_list.append(token)                \n",
    "        \n",
    "    return token_list   \n",
    "\n",
    "\n",
    "def vectorize_phrase(texto, vocab):\n",
    "    try:\n",
    "        tokens = token_and_clean(texto)\n",
    "        vector = np.zeros(len(vocab))\n",
    "        for t in tokens:\n",
    "            if t in vocab:\n",
    "                vector[vocab.index(t)] = 1\n",
    "        return vector\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception en vectorize_phrase: {0}'.format(e))\n",
    "        return None  \n",
    "    \n",
    "\n",
    "def n_token(sentence):\n",
    "    token_list = token_and_clean(sentence) \n",
    "    return len(token_list) \n",
    "\n",
    "\n",
    "def polarity_and_lang(message): #blob has a limit on api calls\n",
    "    \n",
    "    try:\n",
    "        if len(message) > 2:\n",
    "    \n",
    "            #blob = TextBlob(message)    \n",
    "        \n",
    "            #leng = blob.detect_language()\n",
    "            leng = 'es'\n",
    "            text = ''\n",
    "            if leng == 'es':\n",
    "                #blob = blob.translate(to='en').lower() \n",
    "                \n",
    "                #trans = translator.translate(message)\n",
    "                \n",
    "                #trans = translator.translate(message)\n",
    "                \n",
    "                \n",
    "                \n",
    "                #blob = TextBlob(trans.text).lower()\n",
    "                #blob = TextBlob(trans).lower()\n",
    "                pol = clf.predict(message)\n",
    "                text = message.lower()\n",
    "            else:\n",
    "                #blob = blob.lower() \n",
    "                #text = blob.translate(to='es').lower().raw \n",
    "                pass #not going to happen\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            print('Se paso a polarity_and_lang un texto menor que 3 caracters')\n",
    "            pol = 0\n",
    "            text = message.lower()          \n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "            print('Exception en polarity_and_lang: {0}'.format(e))\n",
    "            pol = 0\n",
    "            text = None            \n",
    "    \n",
    "    return (pol, text)\n",
    "\n",
    "\n",
    "def percent_greet(sentence):\n",
    "    tgreet = ['hol', 'buen', 'tard', 'dia', 'noch']\n",
    "    count = 0\n",
    "    tokens = token_and_clean(sentence)\n",
    "    for w in tokens:       \n",
    "        if w in tgreet:\n",
    "            count += 1  \n",
    "    if len(tokens) > 0:\n",
    "        return count/len(tokens)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def ex_capac(sentence):\n",
    "    tcap = ['charl', 'curs', 'capacit', 'seminari', 'formacion', 'capacitacion']\n",
    "    count = 0\n",
    "    tokens = token_and_clean(sentence)\n",
    "    for w in tokens:       \n",
    "        if w in tcap:\n",
    "            count += 1  \n",
    "    return (count > 0)*1\n",
    "\n",
    "\n",
    "def pred_prob(text):\n",
    "    try:        \n",
    "    \n",
    "        vocab = np.load('Data\\\\vocab.npy', allow_pickle=True)\n",
    "        vocab = list(vocab)\n",
    "    \n",
    "        ldata = np.load('Data\\\\param_dict.npy', allow_pickle=True)\n",
    "        param_dict = ldata.item() \n",
    "        W0 = param_dict[0].T\n",
    "        b0 = param_dict[1]\n",
    "        W1 = param_dict[2].T\n",
    "        b1 = param_dict[3]\n",
    "        \n",
    "        pol, text = polarity_and_lang(text)\n",
    "        \n",
    "        \n",
    "        if text:\n",
    "    \n",
    "            x = vectorize_phrase(text, vocab)\n",
    "            if x.any():\n",
    "                x = np.append(x, n_token(text))\n",
    "                x = np.append(x, percent_greet(text))\n",
    "                x = np.append(x, ex_capac(text))\n",
    "                x = np.append(x, pol)   \n",
    "    \n",
    "                h0 = np.matmul(x, W0) + b0\n",
    "                h1 = np.tanh(h0)\n",
    "                h2 = np.matmul(h1, W1) + b1\n",
    "                h3 = np.exp(h2)\n",
    "                prob = h3/np.sum(h3)\n",
    "        \n",
    "                return (prob, pol, text)  \n",
    "            else:\n",
    "                return(np.zeros(13), pol, text)\n",
    "        \n",
    "        else:\n",
    "            return (np.zeros(13), pol, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception en predTop_prob: {0}'.format(e))\n",
    "        return (np.zeros(13), None, None)\n",
    "    \n",
    "    \n",
    "def predict_topic(sentence):\n",
    "    topics = ['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
    "    \n",
    "    sentence = prepare_text(sentence)\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        if sentence:\n",
    "            prob, pol, text = pred_prob(sentence)\n",
    "            if prob.all():\n",
    "                return (topics[np.argmax(prob)], pol, text)\n",
    "            else:\n",
    "                return('No Topic', 0, text) \n",
    "        else:\n",
    "            return('No Topic', 0, None) \n",
    "            \n",
    "    except Exception as e:\n",
    "        print('Exception en predict_topic: {0}'.format(e))\n",
    "        return('No Topic', 0, None)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', 'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros']\n",
      "\n",
      "\n",
      "(array([1.06052497e-03, 1.11177672e-03, 1.83658504e-04, 3.65919019e-03,\n",
      "       6.34876149e-05, 9.80936761e-01, 9.25394943e-03, 3.25372738e-04,\n",
      "       3.80385562e-04, 5.00611990e-05, 1.25495621e-04, 2.48210813e-03,\n",
      "       3.67228362e-04]), 0.06501434725554706, 'estoy llamando a sus numeros en guayaquil pero no me responden con quien puedo conversar')\n"
     ]
    }
   ],
   "source": [
    "text = ('estoy llamando a sus numeros en guayaquil pero no me responden con quien puedo conversar')\n",
    "print(['Jubilacion Patronal', 'Consultoria', 'Renuncia/Despido/Desahucio', 'IESS', \n",
    "                 'Greeting', 'Contacto', 'No Topic', 'Queja', 'Otros servicios', 'Charlas/Capacitaciones', \n",
    "                      'Hi Five', 'job seeker', 'Facturacion/Retencion/Cobros'])\n",
    "print('\\n')\n",
    "print(pred_prob(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'estoy llamando a sus numeros en guayaquil pero no me responden con quien puedo conversar'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Contacto',\n",
       " 0.06501434725554706,\n",
       " 'estoy llamando a sus numeros en guayaquil pero no me responden con quien puedo conversar')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_topic(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
