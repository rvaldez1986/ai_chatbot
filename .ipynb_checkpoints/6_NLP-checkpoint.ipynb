{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from pattern.es import parsetree\n",
    "from nltk.stem import SnowballStemmer\n",
    "import numpy as np\n",
    "\n",
    "from urls import u_IESS, u_RDD, u_JP, u_CONS, u_OS\n",
    "from urls import h_IESS, h_RDD, h_JP, h_CONS, h_OS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url, begin_t, end_t):\n",
    "    try:\n",
    "        html = urllib.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html)\n",
    "\n",
    "        # kill all script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()    # rip it out\n",
    "\n",
    "        # get text\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "        begin = text.find(begin_t)   #the beginning and ending have common expressions\n",
    "        end = text.find(end_t) + len(end_t)\n",
    "\n",
    "        proc_text = text[begin:end]\n",
    "        return proc_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Exception en get_text, url:{0} y codigo: {1}'.format(url, e))\n",
    "        return None     \n",
    "\n",
    "\n",
    "def hasNumbers(string):\n",
    "    return bool(re.search(r'\\d', string))\n",
    "\n",
    "\n",
    "def hasBC(string):\n",
    "    i = string.find('/')\n",
    "    return bool(i != -1)\n",
    "\n",
    "\n",
    "def other_check(token):    \n",
    "    b1 = not hasNumbers(token)\n",
    "    b2 = not hasBC(token)\n",
    "    return (b1 and b2)\n",
    "\n",
    "\n",
    "def token_and_clean(texto):\n",
    "    try:\n",
    "        palabras_funcionales = nltk.corpus.stopwords.words(\"spanish\")    \n",
    "        tokens = nltk.word_tokenize(texto, \"spanish\")\n",
    "        tokens_limpios=[]\n",
    "        for token in tokens:        \n",
    "            if token not in palabras_funcionales:\n",
    "                if len(token) > 2 and token not in tokens_limpios:  #>2 helps in filtering out common\n",
    "                    if other_check(token):\n",
    "                        tokens_limpios.append(token)\n",
    "        return tokens_limpios\n",
    "    except Exception as e:\n",
    "        print('Exception en token and clean: texto: {0} y codigo {1}'.format(texto, e))\n",
    "        return None    \n",
    "\n",
    "\n",
    "def stem_lemma(word):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    word = parsetree(word, lemmata=True)[0].lemmata[0]\n",
    "    word = stemmer.stem(word) \n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "def get_score(q, u, td):\n",
    "    try:\n",
    "        dt = td[u][1]\n",
    "        dt2 = td[u][2]    \n",
    "        q_tokens =  token_and_clean(q.lower())\n",
    "        puntaje = 0\n",
    "        k = 10  #factor for header\n",
    "        for t in q_tokens:  \n",
    "            t = stem_lemma(t)\n",
    "            if t in dt:\n",
    "                puntaje += dt[t]\n",
    "            if (t in dt2) and (t in dt):\n",
    "                puntaje += dt[t] * k\n",
    "        \n",
    "        return puntaje/len(q_tokens)  \n",
    "    except Exception as e:\n",
    "        print('Exception en get_score: q: {0}, u: {1} y codigo: {2}'.format(q, u,  e))\n",
    "        return None      \n",
    "\n",
    "\n",
    "\n",
    "def suggest_url(question, topic):    \n",
    "    \n",
    "    try:    \n",
    "        texts_data = json.load(open(\"Data\\\\texts_data.txt\"))        \n",
    "        ranking = []\n",
    "        \n",
    "        \n",
    "        if topic == \"Jubilacion Patronal\":\n",
    "            ulist = u_JP            \n",
    "        elif topic == \"Renuncia/Despido/Desahucio\":\n",
    "            ulist = u_RDD \n",
    "        elif topic == \"IESS\":\n",
    "            ulist = u_IESS            \n",
    "        elif topic == \"Consultoria\":\n",
    "            ulist = u_CONS\n",
    "        else: #otros servicios\n",
    "            ulist = u_OS\n",
    "        \n",
    "        for u in ulist:\n",
    "            score = get_score(question, u, texts_data)\n",
    "            ranking.append(score)\n",
    "            \n",
    "        if topic == \"Consultoria\":\n",
    "            np.max(ranking) >= 4:\n",
    "            url_res = ulist[np.argmax(ranking)]            \n",
    "            return url_res        \n",
    "        elif np.max(ranking) > 0:\n",
    "            url_res = ulist[np.argmax(ranking)]            \n",
    "            return url_res\n",
    "        else:\n",
    "            return None           \n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Exception en suggest_url: {0}'.format(e))\n",
    "        return None      \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urls import u_IESS, u_RDD, u_JP, u_CONS, u_OS\n",
    "from urls import h_IESS, h_RDD, h_JP, h_CONS, h_OS \n",
    "\n",
    "\n",
    "topics = [u_IESS, u_RDD, u_JP, u_CONS, u_OS]\n",
    "headers = [h_IESS, h_RDD, h_JP, h_CONS, h_OS]\n",
    "texts_data =  defaultdict(lambda: [None, None, None])  #[original_text, depurated_tokens,  header_tokens]\n",
    "\n",
    "for t,lh in zip(topics,headers):\n",
    "    for u,h in zip(t,lh):\n",
    "        text = get_text(u, 'Publicado por:', 'Compartir\\nAuthor').lower()\n",
    "        texts_data[u][0] = text\n",
    "        texts_data[u][1] = token_and_clean(text)\n",
    "        texts_data[u][2] = token_and_clean(h.lower())\n",
    "        \n",
    "#correct for others\n",
    "u = 'https://actuaria.com.ec/servicio/mercado-asegurador-y-medicina-prepagada/'\n",
    "text = get_text(u, 'Mercado', 'riesgo.').lower()\n",
    "texts_data[u][0] = text\n",
    "texts_data[u][1] = token_and_clean(text)\n",
    "texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "u = 'https://actuaria.com.ec/servicio/data-analytics/'\n",
    "text = get_text(u, 'Data', 'estadísticos/predictivos.').lower()\n",
    "texts_data[u][0] = text\n",
    "texts_data[u][1] = token_and_clean(text)\n",
    "texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "u = 'https://actuaria.com.ec/servicio/asesoria-actuarial-empresarial/'\n",
    "text = get_text(u, 'Asesoría', 'flujos').lower()\n",
    "texts_data[u][0] = text\n",
    "texts_data[u][1] = token_and_clean(text)\n",
    "texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "u = 'https://actuaria.com.ec/servicio/fondos-de-jubilacion-y-cesantia/'\n",
    "text = get_text(u, 'Fondos', 'parámetros').lower()\n",
    "texts_data[u][0] = text\n",
    "texts_data[u][1] = token_and_clean(text)\n",
    "texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "u = 'https://actuaria.com.ec/servicio/modelos-estadisticos-matematicos/'\n",
    "text = get_text(u, 'Modelos', 'otros)').lower()\n",
    "texts_data[u][0] = text\n",
    "texts_data[u][1] = token_and_clean(text)\n",
    "texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "u = 'https://actuaria.com.ec/servicio/gestion-humana/'\n",
    "text = get_text(u, 'Gestión', 'otros.').lower()\n",
    "texts_data[u][0] = text\n",
    "texts_data[u][1] = token_and_clean(text)\n",
    "texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "u = 'https://actuaria.com.ec/servicio/asesoria-financiera/'\n",
    "text = get_text(u, 'Asesoría', 'otros.').lower()\n",
    "texts_data[u][0] = text\n",
    "texts_data[u][1] = token_and_clean(text)\n",
    "texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#TF - IDF \n",
    "#w_ij = tf_ij * log (N/df_i) \n",
    "#w_ij : weight for word i in document j\n",
    "#tf_ij : number of occurrences of word i in document j\n",
    "#N : total number of corpuses\n",
    "# df_i : number of corpuses containing word i\n",
    "        \n",
    "\n",
    "\n",
    "N = len(topics[0]) + len(topics[1]) + len(topics[2]) + len(topics[3])   \n",
    "\n",
    "for u in texts_data.keys():\n",
    "    text = texts_data[u][0]\n",
    "    tokens = texts_data[u][1]\n",
    "    weights = {}    \n",
    "    for i in tokens:\n",
    "        tf_ij = text.count(i)\n",
    "        df_i = 0\n",
    "        for u2 in texts_data.keys():\n",
    "            if i in texts_data[u2][0]:\n",
    "                df_i += 1 \n",
    "                \n",
    "        if df_i/N < 0.8:\n",
    "            weights[stem_lemma(i)] = tf_ij * math.log(N/df_i)       \n",
    "        \n",
    "    texts_data[u][1] = weights\n",
    "    texts_data[u][2] = [stem_lemma(x) for x in texts_data[u][2]]\n",
    "    \n",
    "#u = 'https://actuaria.com.ec/que-es-la-bonificacion-por-desahucio/'    \n",
    "#texts_data[u][1]     \n",
    "    \n",
    "td = dict(texts_data)\n",
    "json.dump(td, open(\"Data\\\\texts_data.txt\",'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.98308036250376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://actuaria.com.ec/aspectos-importantes-en-el-calculo-de-finiquito-de-jubilacion-patronal/'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'Buenas tardes, como calculo el valor que debo pagar de jubilacion patronal'\n",
    "suggest_url(q, \"Jubilacion Patronal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mi interés está relacionado con consultas sobre: Desarrollo de Productos de Seguro Diseño y acompañamiento en la aprobación de nuevos productos y planes de seguros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
