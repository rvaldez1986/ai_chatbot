{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "from pattern.es import parsetree\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    html = urllib.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    begin = text.find('Publicado por:')   #the beginning and ending have common expressions\n",
    "    end = text.find('Compartir\\nAuthor')\n",
    "\n",
    "    proc_text = text[begin:end]\n",
    "    return proc_text\n",
    "\n",
    "\n",
    "def hasNumbers(string):\n",
    "    return bool(re.search(r'\\d', string))\n",
    "\n",
    "\n",
    "def hasBC(string):\n",
    "    i = string.find('/')\n",
    "    return bool(i != -1)\n",
    "\n",
    "\n",
    "def other_check(token):    \n",
    "    b1 = not hasNumbers(token)\n",
    "    b2 = not hasBC(token)\n",
    "    return (b1 and b2)\n",
    "\n",
    "\n",
    "def token_and_clean(texto):\n",
    "    palabras_funcionales = nltk.corpus.stopwords.words(\"spanish\")    \n",
    "    tokens = nltk.word_tokenize(texto, \"spanish\")\n",
    "    tokens_limpios=[]\n",
    "    for token in tokens:        \n",
    "        if token not in palabras_funcionales:\n",
    "            if len(token) > 2 and token not in tokens_limpios:  #>2 helps in filtering out common\n",
    "                if other_check(token):\n",
    "                    tokens_limpios.append(token)\n",
    "    return tokens_limpios\n",
    "\n",
    "\n",
    "def stem_lemma(word):\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    word = parsetree(word, lemmata=True)[0].lemmata[0]\n",
    "    word = stemmer.stem(word) \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\rober\\\\Desktop\\\\chatbot\\\\chatbot_server')\n",
    "\n",
    "from urls import u_IESS, u_RDD, u_JP, u_CONS\n",
    "from urls import h_IESS, h_RDD, h_JP, h_CONS \n",
    "\n",
    "\n",
    "topics = [u_IESS, u_RDD, u_JP, u_CONS]\n",
    "headers = [h_IESS, h_RDD, h_JP, h_CONS]\n",
    "texts_data =  defaultdict(lambda: [None, None, None])  #[original_text, depurated_tokens,  header_tokens]\n",
    "\n",
    "for t,lh in zip(topics,headers):\n",
    "    for u,h in zip(t,lh):\n",
    "        text = get_text(u).lower()\n",
    "        texts_data[u][0] = text\n",
    "        texts_data[u][1] = token_and_clean(text)\n",
    "        texts_data[u][2] = token_and_clean(h.lower())\n",
    "\n",
    "\n",
    "#TF - IDF \n",
    "#w_ij = tf_ij * log (N/df_i) \n",
    "#w_ij : weight for word i in document j\n",
    "#tf_ij : number of occurrences of word i in document j\n",
    "#N : total number of corpuses\n",
    "# df_i : number of corpuses containing word i\n",
    "        \n",
    "\n",
    "\n",
    "N = len(topics[0]) + len(topics[1]) + len(topics[2]) + len(topics[3])   \n",
    "\n",
    "for u in texts_data.keys():\n",
    "    text = texts_data[u][0]\n",
    "    tokens = texts_data[u][1]\n",
    "    weights = {}    \n",
    "    for i in tokens:\n",
    "        tf_ij = text.count(i)\n",
    "        df_i = 0\n",
    "        for u2 in texts_data.keys():\n",
    "            if i in texts_data[u2][0]:\n",
    "                df_i += 1 \n",
    "                \n",
    "        if df_i/N < 0.8:\n",
    "            weights[stem_lemma(i)] = tf_ij * math.log(N/df_i)       \n",
    "        \n",
    "    texts_data[u][1] = weights\n",
    "    texts_data[u][2] = [stem_lemma(x) for x in texts_data[u][2]]\n",
    "    \n",
    "#u = 'https://actuaria.com.ec/que-es-la-bonificacion-por-desahucio/'    \n",
    "#texts_data[u][1]    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "td = dict(texts_data)\n",
    "json.dump(td, open(\"texts_data.txt\",'w'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
